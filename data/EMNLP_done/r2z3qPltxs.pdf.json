{
    "abstractText": "With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter (Marcus, 2023), signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office (Copyright-Office, 2023) released a statement stating that \u201cIf a work\u2019s traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it\u201d. Furthermore, both the US (White-House, 2023) and the EU (EuropeanParliament, 2023) governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policymaking for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to \u2020Work does not relate to position at Amazon. establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a higher ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. 1 Proposed AI-Generated Text Detection Techniques (AGTD) \u2013 A Review Recently, six methods and their combinations have been proposed for AGTD: (i) watermarking, (ii) perplexity estimation, (iii) burstiness estimation, (iv) negative log-likelihood curvature, (v) stylometric variation, and (vi) classifier-based approaches. This paper focuses on critiquing their robustness and presents empirical evidence demonstrating their brittleness. Watermarking: Watermarking AI-generated text, first proposed by Wiggers (2022), entails the incorporation of an imperceptible signal to establish the authorship of a specific text with a high degree of certainty. This approach is analogous to encryption and decryption. Kirchenbauer et al. (2023a) (wv1) were the first to present operational watermarking models for LLMs, but their initial proposal faced criticism. Sadasivan et al. (2023) shared their initial studies suggesting that paraphrasing can efficiently eliminate watermarks. In a subsequent paper (Kirchenbauer et al., 2023b) (wv2), the authors put forth evidently more resilient watermarking techniques, asserting that paraphrasing does not significantly disrupt watermark signals in this iteration of their research. By conducting extensive experiments (detailed in Section 3), our study provides a thorough investigation of the dewatermarking techniques wv1 and wv2, demonstrating that the watermarked texts generated by both methods can be circumvented, albeit with a slight decrease in de-watermarking accuracy observed with wv2. These results further strengthen our contention that text watermarking is fragile and lacks reliability for real-life applications. Perplexity Estimation: The hypothesis related to perplexity-based AGTD methods is that humans exhibit significant variation in linguistic constraints, syntax, vocabulary, and other factors (aka perplexity) from one sentence to another. In contrast, LLMs display a higher degree of consistency in their linguistic style and structure. Employing this hypothesis, GPTZero (Tian, 2023) devised an AGTD tool that posited the overall perplexity human-generated text should surpass that of AI-generated text, as in the equation: logp\u0398(htext)\u2212 logp\u0398(AItext) \u2265 0 (Appendix C). Furthermore, GPTZero assumes that the variations in perplexity across sentences would also be lower for AI-generated text. This phenomenon could potentially be quantified by estimating the entropy for sentence-wise perplexity, as depicted in the equation: Eperp = logp\u0398[\u03a3k=1(|sh \u2212 s k+1 h |)]\u2212 logp\u0398[\u03a3k=1(|sAI \u2212 s k+1 AI |)] \u2265 0; where sh and sAI represent kth sentences of human and AI-written text respectively. Burstiness Estimation: Burstiness refers to the patterns observed in word choice and vocabulary size. GPTZero (Tian, 2023) was the first to introduce burstiness estimation for AGTD. In this context, the hypothesis suggests that AI-generated text displays a higher frequency of clusters or bursts of similar words or phrases within shorter sections of the text. In contrast, humans exhibit a broader variation in their lexical choices, showcasing a more extensive range of vocabulary. Let \u03c3\u03c4 denote the Figure 1: (Top) The negative log-curvature hypothesis proposed by Mitchell et al. (2023). According to their claim, any perturbations made to the AI-generated text should predominantly fall within a region of negative curvature. (Bottom) Our experiments using 15 LLMs with 20 perturbations indicate that the text generated by GPT 3.0 and variants do not align with this hypothesis. Moreover, for the other LLMs, the variance in the negative log-curvature was so minimal that it had to be disregarded as a reliable indication. and represent fake and real sample respectively, whereas and depict perturbed fake and real sample. standard deviation of the language spans and m\u03c4 the mean of the language spans. Burstiness (b) is calculated as b = (\u03c3\u03c4/m\u03c4\u22121 \u03c3\u03c4/m\u03c4+1) and is bounded within the interval [-1, 1]. Therefore the hypothesis is bH \u2212 bAI \u2265 0, where bH is the mean burstiness of human writers and bAI is the mean burstiness of AI aka a particular LLM. Corpora with antibursty, periodic dispersions of switch points take on burstiness values closer to -1. In contrast, corpora with less predictable patterns of switching take on values closer to 1. It is worth noting that burstiness could also be calculated sentence-wise and/or text fragment-wise and then their entropy could be defined as: Eburst = logp\u03b2 [\u03a3k=1(|sAIb \u2212 sk+1 AIb |)\u2212 logp\u03b2 [\u03a3k=1(|shb \u2212 s k+1 hb |)]]\u2265 0. Nevertheless, our comprehensive experiments involving 15 LLMs indicate that this hypothesis does not consistently provide a discernible signal. Furthermore, recent LLMs like GPT-3.5/4, MPT (OpenAI, 2023a; Team, 2023) have demonstrated the utilization of a wide range of vocabulary, challenging the hypothesis. Section 4 discusses our experiments on perplexity and burstiness estimation. Negative Log-Curvature (NLC): DetectGPT (Mitchell et al., 2023) introduced the concept of Negative Log-Curvature (NLC) to detect AIgenerated text. The hypothesis is that text generated by the the model tends to lie in the negative curvature areas of the model\u2019s log probability, i.e. a text generated by a source LLM p\u03b8 typically lies in the areas of negative curvature of the log probability function of p\u03b8 , unlike human-written text. In other words, we apply small perturbations to a passage x \u223c p\u03b8 , producing x\u0303. Defining PNLC \u03b8 as the quantity logp\u03b8 (x)\u2212 logp\u03b8 (x\u0303), PNLC \u03b8 should be larger on average for AI-generated samples than human-written text (see an example in Table 1 and the visual intuition of the hypothesis in Fig. 1). Expressed mathematically: PNLC AI \u2212PNLC H \u2265 0. It is important to note that DetectGPT\u2019s findings were derived from text-snippet analysis, but there is potential to reevaluate this approach by examining smaller fragments, such as sentences. This would enable the calculation of averages or entropies, akin to how perplexity and burstiness are measured. Finally, the limited number of perturbation patterns per sentence in (Mitchell et al., 2023) affect the reliability of results (cf. Section 5 for details).",
    "authors": [
        {
            "affiliations": [],
            "name": "Megha Chakraborty"
        },
        {
            "affiliations": [],
            "name": "S.M Towhidul Islam Tonmoy"
        },
        {
            "affiliations": [],
            "name": "S M Mehedi Zaman"
        },
        {
            "affiliations": [],
            "name": "Krish Sharma"
        },
        {
            "affiliations": [],
            "name": "Niyar R Barman"
        },
        {
            "affiliations": [],
            "name": "Chandan Gupta"
        },
        {
            "affiliations": [],
            "name": "Shreya Gautam"
        },
        {
            "affiliations": [],
            "name": "Tanay Kumar"
        },
        {
            "affiliations": [],
            "name": "Vinija Jain"
        },
        {
            "affiliations": [],
            "name": "Aman Chadha"
        },
        {
            "affiliations": [],
            "name": "Amit P. Sheth"
        },
        {
            "affiliations": [],
            "name": "Amitava Das"
        }
    ],
    "id": "SP:06356807e36277a017137e74c4e318e79f6f5bcb",
    "references": [
        {
            "authors": [
                "Rishi Bommasani",
                "Kevin Klyman",
                "Daniel Zhang",
                "Percy Liang"
            ],
            "title": "Do foundation model providers comply with the eu ai act",
            "year": 2023
        },
        {
            "authors": [
                "Dennis D. Boos",
                "Cavell Brownie."
            ],
            "title": "Bootstrap methods for testing homogeneity of variances",
            "venue": "Technometrics, 31(1):69\u201382.",
            "year": 1989
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326.",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Lucien Le Cam"
            ],
            "title": "Asymptotic methods in statistical decision theory",
            "year": 1986
        },
        {
            "authors": [
                "Souradip Chakraborty",
                "Amrit Singh Bedi",
                "Sicheng Zhu",
                "Bang An",
                "Dinesh Manocha",
                "Furong Huang"
            ],
            "title": "On the possibilities of ai-generated text detection",
            "year": 2023
        },
        {
            "authors": [
                "Xuanting Chen",
                "Junjie Ye",
                "Can Zu",
                "Nuo Xu",
                "Rui Zheng",
                "Minlong Peng",
                "Jie Zhou",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "title": "How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks. arXiv preprint arXiv:2303.00293",
            "year": 2023
        },
        {
            "authors": [
                "Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Copyright-Office."
            ],
            "title": "Copyright registration guidance: Works containing material generated by artificial intelligence",
            "venue": "Library of Congress.",
            "year": 2023
        },
        {
            "authors": [
                "Ronan Cummins."
            ],
            "title": "Modelling word burstiness in natural language: a generalised polya process for document language models in information retrieval",
            "venue": "arXiv preprint arXiv:1708.06011.",
            "year": 2017
        },
        {
            "authors": [
                "Tri Dao",
                "Dan Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9."
            ],
            "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
            "venue": "Advances in Neural Information Processing Systems, 35:16344\u201316359.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Hendrik Strobelt",
                "Alexander M Rush."
            ],
            "title": "Gltr: Statistical detection and visualization of generated text",
            "venue": "arXiv preprint arXiv:1906.04043.",
            "year": 2019
        },
        {
            "authors": [
                "Tae Kim."
            ],
            "title": "T test as a parametric statistic",
            "venue": "Korean Journal of Anesthesiology, 68:540.",
            "year": 2015
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein"
            ],
            "title": "2023a. A watermark for large language models",
            "year": 2023
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Manli Shu",
                "Khalid Saifullah",
                "Kezhi Kong",
                "Kasun Fernando",
                "Aniruddha Saha",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "On the reliability of watermarks for large language models",
            "year": 2023
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Yixiao Song",
                "Marzena Karpinska",
                "John Wieting",
                "Mohit Iyyer."
            ],
            "title": "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "venue": "arXiv preprint arXiv:2303.13408.",
            "year": 2023
        },
        {
            "authors": [
                "Tharindu Kumarage",
                "Joshua Garland",
                "Amrita Bhattacharjee",
                "Kirill Trapeznikov",
                "Scott Ruston",
                "Huan Liu"
            ],
            "title": "Stylometric detection of aigenerated text in twitter timelines",
            "year": 2023
        },
        {
            "authors": [
                "Ksenia Lagutina",
                "Nadezhda Lagutina",
                "Elena Boychuk",
                "Inna Vorontsova",
                "Elena Shliakhtina",
                "Olga Belyaeva",
                "Ilya Paramonov",
                "P.G. Demidov."
            ],
            "title": "A survey on stylometric text features",
            "venue": "2019 25th Conference of Open Innovations Association (FRUCT),",
            "year": 2019
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "year": 2020
        },
        {
            "authors": [
                "Weixin Liang",
                "Mert Yuksekgonul",
                "Yining Mao",
                "Eric Wu",
                "James Zou"
            ],
            "title": "Gpt detectors are biased against non-native english writers",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Kiwan Maeng",
                "Alexei Colin",
                "Brandon Lucia."
            ],
            "title": "Alpaca: Intermittent execution without checkpoints",
            "venue": "Proceedings of the ACM on Programming Languages, 1(OOPSLA):1\u201330.",
            "year": 2017
        },
        {
            "authors": [
                "Gary Marcus"
            ],
            "title": "Pause giant ai experiments: An open letter",
            "year": 2023
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D. Manning",
                "Chelsea Finn"
            ],
            "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "year": 2023
        },
        {
            "authors": [
                "Tempestt Neal",
                "Kalaivani Sundararajan",
                "Aneez Fatima",
                "Yiming Yan",
                "Yingfei Xiang",
                "Damon Woodard."
            ],
            "title": "Surveying stylometry techniques and applications",
            "venue": "ACM Computing Surveys (CSUR), 50(6):86.",
            "year": 2018
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "New ai classifier for indicating aiwritten text",
            "venue": "(Accessed on Feb 1 2023).",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Ofir Press",
                "Noah Smith",
                "Mike Lewis."
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Pavel Rychl\u1ef3."
            ],
            "title": "Words\u2019 burstiness in language models",
            "venue": "RASLAN, pages 131\u2013137.",
            "year": 2011
        },
        {
            "authors": [
                "Vinu Sankar Sadasivan",
                "Aounon Kumar",
                "Sriram Balasubramanian",
                "Wenxiao Wang",
                "Soheil Feizi"
            ],
            "title": "Can ai-generated text be reliably detected",
            "year": 2023
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "ArXiv, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Noam Shazeer."
            ],
            "title": "Fast transformer decoding: One write-head is all you need",
            "venue": "arXiv preprint arXiv:1911.02150.",
            "year": 2019
        },
        {
            "authors": [
                "Noam Shazeer."
            ],
            "title": "Glu variants improve transformer",
            "venue": "arXiv preprint arXiv:2002.05202.",
            "year": 2020
        },
        {
            "authors": [
                "Irene Solaiman",
                "Miles Brundage",
                "Jack Clark",
                "Amanda Askell",
                "Ariel Herbert-Voss",
                "Jeff Wu",
                "Alec Radford",
                "Gretchen Krueger",
                "Jong Wook Kim",
                "Sarah Kreps"
            ],
            "title": "Release strategies and the social impacts of language models",
            "year": 2019
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu."
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "venue": "arXiv preprint arXiv:2104.09864.",
            "year": 2021
        },
        {
            "authors": [
                "Edward Tian."
            ],
            "title": "Gptzero",
            "venue": "[Online; accessed 202301-02].",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Eduard Tulchinskii",
                "Kristian Kuznetsov",
                "Laida Kushnareva",
                "Daniil Cherniavskii",
                "Serguei Barannikov",
                "Irina Piontkovskaya",
                "Sergey Nikolenko",
                "Evgeny Burnaev"
            ],
            "title": "Intrinsic dimension estimation for robust detection of ai-generated texts",
            "year": 2023
        },
        {
            "authors": [
                "Robert A Wagner",
                "Michael J Fischer."
            ],
            "title": "The string-to-string correction problem",
            "venue": "Journal of the ACM (JACM), 21(1):168\u2013173.",
            "year": 1974
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Wiggers."
            ],
            "title": "Openai\u2019s attempts to watermark ai text hit limits",
            "venue": "[Online; accessed 2023-01-02].",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Ye",
                "Xuanting Chen",
                "Nuo Xu",
                "Can Zu",
                "Zekai Shao",
                "Shichun Liu",
                "Yuhan Cui",
                "Zeyang Zhou",
                "Chao Gong",
                "Yang Shen",
                "Jie Zhou",
                "Siming Chen",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "title": "A comprehensive capability analysis of gpt-3 and gpt-3.5 series",
            "year": 2023
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Franziska Roesner",
                "Yejin Choi"
            ],
            "title": "Defending against neural fake news",
            "year": 2020
        },
        {
            "authors": [
                "Biao Zhang",
                "Rico Sennrich."
            ],
            "title": "Root mean square layer normalization",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pretrained transformer language models",
            "year": 2022
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny."
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592.",
            "year": 2023
        },
        {
            "authors": [
                "Krishna"
            ],
            "title": "2023), watermarked texts can be relatively easily de-watermarked. Even with the implementation of the newer, more robust watermarking scheme presented by Kirchenbauer et al. (2023b)",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "\u2020Work does not relate to position at Amazon.\nestablish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a higher ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making."
        },
        {
            "heading": "1 Proposed AI-Generated Text Detection Techniques (AGTD) \u2013 A Review",
            "text": "Recently, six methods and their combinations have been proposed for AGTD: (i) watermarking, (ii) perplexity estimation, (iii) burstiness estimation, (iv) negative log-likelihood curvature, (v) stylometric variation, and (vi) classifier-based approaches. This paper focuses on critiquing their robustness and presents empirical evidence demonstrating their brittleness. Watermarking: Watermarking AI-generated text, first proposed by Wiggers (2022), entails the incorporation of an imperceptible signal to establish the authorship of a specific text with a high degree of certainty. This approach is analogous to encryption and decryption. Kirchenbauer et al. (2023a) (wv1) were the first to present operational watermarking models for LLMs, but their initial proposal faced criticism. Sadasivan et al. (2023) shared their initial studies suggesting that paraphrasing can efficiently eliminate watermarks. In a subsequent paper (Kirchenbauer et al., 2023b) (wv2), the\nauthors put forth evidently more resilient watermarking techniques, asserting that paraphrasing does not significantly disrupt watermark signals in this iteration of their research. By conducting extensive experiments (detailed in Section 3), our study provides a thorough investigation of the dewatermarking techniques wv1 and wv2, demonstrating that the watermarked texts generated by both methods can be circumvented, albeit with a slight decrease in de-watermarking accuracy observed with wv2. These results further strengthen our contention that text watermarking is fragile and lacks reliability for real-life applications. Perplexity Estimation: The hypothesis related to perplexity-based AGTD methods is that humans exhibit significant variation in linguistic constraints, syntax, vocabulary, and other factors (aka perplexity) from one sentence to another. In contrast, LLMs display a higher degree of consistency in their linguistic style and structure. Employing this hypothesis, GPTZero (Tian, 2023) devised an AGTD tool that posited the overall perplexity human-generated text should surpass that of AI-generated text, as in the equation: logp\u0398(htext)\u2212 logp\u0398(AItext) \u2265 0 (Appendix C). Furthermore, GPTZero assumes that the variations in perplexity across sentences would also be lower for AI-generated text. This phenomenon could potentially be quantified by estimating the entropy for sentence-wise perplexity, as depicted in the equation: Eperp = logp\u0398[\u03a3nk=1(|skh \u2212 s k+1 h |)]\u2212 logp\u0398[\u03a3nk=1(|skAI \u2212 s k+1 AI |)] \u2265 0; where skh and skAI represent kth sentences of human and AI-written text respectively. Burstiness Estimation: Burstiness refers to the patterns observed in word choice and vocabulary size. GPTZero (Tian, 2023) was the first to introduce burstiness estimation for AGTD. In this context, the hypothesis suggests that AI-generated text displays a higher frequency of clusters or bursts of similar words or phrases within shorter sections of the text. In contrast, humans exhibit a broader variation in their lexical choices, showcasing a more extensive range of vocabulary. Let \u03c3\u03c4 denote the\nit had to be disregarded as a reliable indication. and represent fake and real sample respectively, whereas and depict perturbed fake and real sample. standard deviation of the language spans and m\u03c4 the mean of the language spans. Burstiness (b) is calculated as b = (\u03c3\u03c4/m\u03c4\u22121\u03c3\u03c4/m\u03c4+1) and is bounded within the interval [-1, 1]. Therefore the hypothesis is bH \u2212 bAI \u2265 0, where bH is the mean burstiness of human writers and bAI is the mean burstiness of AI aka a particular LLM. Corpora with antibursty, periodic dispersions of switch points take on burstiness values closer to -1. In contrast, corpora with less predictable patterns of switching take on values closer to 1. It is worth noting that burstiness could also be calculated sentence-wise and/or text fragment-wise and then their entropy could be defined as: Eburst = logp\u03b2 [\u03a3nk=1(|skAIb \u2212 sk+1AIb |)\u2212 logp\u03b2 [\u03a3nk=1(|skhb \u2212 s k+1 hb |)]]\u2265 0. Nevertheless, our comprehensive experiments involving 15 LLMs indicate that this hypothesis does not consistently provide a discernible signal. Furthermore, recent LLMs like GPT-3.5/4, MPT (OpenAI, 2023a; Team, 2023) have demonstrated the utilization of a wide range of vocabulary, challenging the hypothesis. Section 4 discusses our experiments on perplexity and burstiness estimation.\nNegative Log-Curvature (NLC): DetectGPT (Mitchell et al., 2023) introduced the concept of Negative Log-Curvature (NLC) to detect AIgenerated text. The hypothesis is that text generated by the the model tends to lie in the negative curvature areas of the model\u2019s log probability, i.e. a text generated by a source LLM p\u03b8 typically lies in the areas of negative curvature of the log probability function of p\u03b8 , unlike human-written text. In other words, we apply small perturbations to a passage x \u223c p\u03b8 , producing x\u0303. Defining PNLC\u03b8 as the quantity logp\u03b8 (x)\u2212 logp\u03b8 (x\u0303), PNLC\u03b8 should be larger on average for AI-generated samples than human-written text (see an example in Table 1 and the visual intuition of the hypothesis in Fig. 1). Expressed mathematically: PNLCAI \u2212PNLCH \u2265 0. It is important to note that DetectGPT\u2019s findings were derived from text-snippet analysis, but there is potential to reevaluate this approach by examining smaller fragments, such as sentences. This would enable the calculation of averages or entropies, akin to how perplexity and burstiness are measured. Finally, the limited number of perturbation patterns per sentence in (Mitchell et al., 2023) affect the reliability of results (cf. Section 5 for details).\nStylometric variation: Stylometry is dedicated to analyzing the linguistic style of text in order to differentiate between various writers. Kumarage et al. (2023) investigated the examination of stylistic features of AI-generated text in order to distinguish it from human-written text. The authors reported impressive results for text detection generated from RoBERTa. However, we observe limitations in applying such methods to newer advanced models (cf. Section 6). Classification-based approach: This problem formulation involves training classifiers to differentiate between AI-written and human-written text,\nand is relatively straightforward. OpenAI initially developed its own text classifier (OpenAI, 2023b), which reported an accuracy of only 26% on true positives. Due to its weaker performance among the proposed methods, we did not further investigate this strategy.\nOUR CONTRIBUTIONS: A Counter Turing Test (CT2) and AI Detectability Index (ADI).\n\u27a0 Introducing the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of prevalent AGTD techniques.\n\u27a0 Empirically showing that the popular AGTD methods are brittle and relatively easy to circumvent.\n\u27a0 Introducing AI Detectability Index (ADI) as a measure for LLMs to infer whether their generations are detectable as AI-generated or not.\n\u27a0 Conducting a thorough examination of 15 contemporary LLMs to establish the aforementioned points.\n\u27a0 Both benchmarks \u2013 CT2 and ADI \u2013 will be published as open-source leaderboards.\n\u27a0 Curated datasets will be made publicly available.\n2 Design Choices for CT2 and ADI Study This section discusses our selected LLMs and elaborates on our data generation methods. More details in Appendix A."
        },
        {
            "heading": "2.1 LLMs: Rationale and Coverage",
            "text": "We chose a wide gamut of 15 LLMs that have exhibited exceptional results on a wide range of NLP tasks. They are: (i) GPT 4 (OpenAI, 2023a); (ii) GPT 3.5 (Chen et al., 2023); (iii) GPT 3 (Brown et al., 2020); (iv) GPT 2 (Radford et al., 2019); (v) MPT (Team, 2023); (vi) OPT (Zhang et al., 2022); (vii) LLaMA (Touvron et al., 2023); (viii) BLOOM (Scao et al., 2022); (ix) Alpaca (Maeng et al., 2017); (x) Vicuna (Zhu et al., 2023); (xi) Dolly (Wang et al., 2022); (xii) StableLM (Tow et al.); (xiii) XLNet (Yang et al., 2019); (xiv) T5 (Raffel et al., 2020); (xv) T0 (Sanh et al., 2021). Given that the field is ever-evolving, we admit that this process will never be complete but rather continue to expand. Hence, we plan to keep the\nCT2 benchmark leaderboard open to researchers, allowing for continuous updates and contributions."
        },
        {
            "heading": "2.2 Datasets: Generation and Statistics",
            "text": "To develop CT2 and ADI, we utilize parallel data comprising both human-written and AI-generated text on the same topic. We select The New York Times (NYT) Twitter handle as our prompt source for the following reasons. Firstly, the handle comprises approximately 393K tweets that cover a variety of topics. For our work, we chose a subset of 100K tweets. Secondly, NYT is renowned for its reliability and credibility. The tweets from NYT exhibit a high level of word-craftsmanship by experienced journalists, devoid of grammatical mistakes. Thirdly, all the tweets from this source include URLs that lead to the corresponding human-written news articles. These tweets serve as prompts for the 15 LLMs, after eliminating hashtags and mentions during pre-processing. Appendix G offers the generated texts from 15 chosen LLMs when given the prompt \"AI generated text detection is not easy.\""
        },
        {
            "heading": "3 De-Watermarking: Discovering its Ease and Efficiency",
            "text": "In the realm of philosophy, watermarking is typically regarded as a source-side activity. It is highly plausible that organizations engaged in the development and deployment of LLMs will progressively adopt this practice in the future. Additionally, regulatory mandates may necessitate the implementation of watermarking as an obligatory measure. The question that remains unanswered is the level of difficulty in circumventing watermarking, i.e., de-watermarking, when dealing with watermarked AI-generated text. In this section, we present our rigorous experiments that employ three methods capable of de-watermarking an AI-generated text that has been watermarked: (i) spotting high entropy words and replacing them, (ii) paraphrasing, (iii) paraphrasing + replacing high-entropy words Table 2 showcases an instance of de-watermarking utilizing two techniques for OPT as target LLM."
        },
        {
            "heading": "3.1 De-watermarking by Spotting and",
            "text": "Replacing High Entropy Words (DeW1)\nThe central concept behind the text watermarking proposed by Kirchenbauer et al. (2023a) is to identify high entropy words and replace them with alternative words that are contextually plausible. The replacement is chosen by an algorithm (analogous to an encryption key) known only to the LLM\u2019s creator. Hence, if watermarking has been implemented, it has specifically focused on those words. High entropy words are the content words in a linguistic construct. In contrast, low entropy words, such as function words, contribute to the linguistic structure and grammatical coherence of a given text. Replacing low entropy words can disrupt the quality of text generation. Appendix B provides more details on high entropy vs. low entropy words. Challenges of detecting high entropy words: High entropy words aid in discerning ambiguity in LLM\u2019s as observed through the probability differences among predicted candidate words. While detecting high entropy words may seem technically feasible, there are two challenges in doing so: (i) many modern LLMs are not open-source. This restricts access to the LLM\u2019s probability distribution over the vocabulary; (ii) assuming a text snippet is AI-generated, in real-world scenarios, the specific LLM that generated it is challenging to determine unless explicitly stated. This lack of information makes it difficult to ascertain the origin and underlying generation process of a text.\nSpotting high-entropy words: Closed-source LLMs conceal the log probabilities of generated text, thus rendering one of the most prevalent AGTD methods intractable. To address this, we utilize open-source LLMs to identify high-entropy words in a given text. As each LLM is trained on a distinct corpus, the specific high-entropy words identified may vary across different LLMs. To mitigate this, we adopt a comparative approach by employing multiple open-source LLMs. Replacing high-entropy words: We can employ\nany LLM to replace the previously identified highentropy words, resulting in a de-watermarked text. To achieve this, we tried various LLMs and found that BERT-based models are best performing to generate replacements for the masked text. Winning combination: The results of experiments on detecting and replacing high entropy words are presented in Table 3 for OPT. The findings indicate that ALBERT (albert-large-v2) (Lan et al., 2020) and DistilRoBERTa (distilrobertabase) perform exceptionally well in identifying high entropy words in text generated by the OPT model for both versions, v1 and v2. On the other hand, DistilRoBERTa (distilrobertabase) (Sanh et al., 2019) and BERT (bert-baseuncased) (Devlin et al., 2019) demonstrate superior performance in substituting the high entropy words for versions v1 and v2 of the experiments. Therefore, the optimal combination for Kirchenbauer et al. (2023a) (wv1) is (albert-large-v2, distilroberta-base), achieving a 75% accuracy in removing watermarks, while (distilrobertabase, bert-base-uncased) performs best for (Kirchenbauer et al., 2023b) (wv2), attaining 72% accuracy in de-watermarking. The results for the remaining 14 LLMs are reported in Appendix B. 3.2 De-watermarking by Paraphrasing (DeW2) We have used paraphrasing as yet another technique to remove watermarking from LLMs. Idea 1) Feed textual input to a paraphraser model such as Pegasus, T5, GPT-3.5 and evaluate watermarking for the paraphrased text. Idea 2) Replace the high entropy words, which are likely to be the watermarked tokens, and then paraphrase the text to ensure that we have eliminated the watermarks. We perform a comprehensive analysis of both qualitative and quantitative aspects of automatic paraphrasing for the purpose of de-watermarking. We chose three SoTA paraphrase models: (a) Pegasus (Zhang et al., 2020), (b) T5 (Flan-t5-xxl\nvariant) (Chung et al., 2022), and (c) GPT-3.5 (gpt-3.5-turbo-0301 variant) (Brown et al., 2020). We seek answers to the following questions: (i) What is the accuracy of the paraphrases generated? (ii) How do they distort the original content? (iii) Are all the possible candidates generated by the paraphrase models successfully de-watermarked? (iv) Which paraphrase module has a greater impact on the de-watermarking process? To address these questions, we evaluate the paraphrase modules based on three key dimensions: (i) Coverage: number of considerable paraphrase generations, (ii) Correctness: correctness of the generations, (iii) Diversity: linguistic diversity in the generations. Our experiments showed that GPT-3.5 (gpt-3.5-turbo-0301 variant) is the most suitable paraphraser (Fig. 2). Please see details of experiments in Appendix B.3.\nFor a given text input, we generate multiple paraphrases using various SoTA models. In the process of choosing the appropriate paraphrase model based on a list of available models, the primary question we asked is how to make sure the generated paraphrases are rich in diversity while still being linguistically correct. We delineate the process followed to achieve this as follows. Let\u2019s say we have a claim c. We generate n paraphrases us-\ning a paraphrasing model. This yields a set of pc1, . . ., pcn. Next, we make pair-wise comparisons of these paraphrases with c, resulting in c\u2212 pc1, . . ., and c\u2212 pcn. At this step, we identify the examples which are entailed, and only those are chosen. For the entailment task, we have utilized RoBERTa Large (Liu et al., 2019) \u2013 a SoTA model trained on the SNLI task (Bowman et al., 2015). Key Findings from De-Watermarking Experiments: As shown in Table 3 and Table 5, our experiments provide empirical evidence suggesting that the watermarking applied to AI-generated text can be readily circumvented (cf. Appendix B)."
        },
        {
            "heading": "4 Reliability of Perplexity and Burstiness as AGTD Signals",
            "text": "In this section, we extensively investigate the reliability of perplexity and burstiness as AGTD signals. Based on our empirical findings, it is evident that the text produced by newer LLMs is nearly\nindistinguishable from human-written text from a statistical perspective.\nThe hypothesis assumes that AI-generated text displays a higher frequency of clusters or bursts of similar words or phrases within shorter sections of the text. In contrast, humans exhibit a broader variation in their lexical choices, showcasing a more extensive range of vocabulary. Moreover, sentence-wise human shows more variety in terms of length, and structure in comparison with AIgenerated text. To measure this we have utilized entropy. The entropy pilogpi of a random variable is the average level of surprise, or uncertainty."
        },
        {
            "heading": "4.1 Estimating Perplexity \u2013 Human vs. AI",
            "text": "Perplexity is a metric utilized for computing the probability of a given sequence of words in natural language. It is computed as e\u2212 1 N \u2211 N i=1 log2 p(wi), where N represents the length of the word sequence, and p(wi) denotes the probability of the individual word wi. As discussed previously, GPTZero (Tian, 2023) assumes that humangenerated text exhibits more variations in both overall perplexity and sentence-wise perplexity as compared to AI-generated text. To evaluate the strength of this proposition, we compare text samples generated by 15 LLMs with corresponding human-generated text on the same topic. Our empirical findings indicate that larger LLMs, such as GPT-3+, closely resemble human-generated text and exhibit minimal distinctiveness. However, relatively smaller models such as XLNet, BLOOM, etc. are easily distinguishable from human-generated text. Fig. 3 demonstrates a side-by-side comparison of the overall perplexity of GPT4 and T5. We report results for 3 LLMs in Table 4 (cf. Table 22 in Appendix C for results over all 15 LLMs)."
        },
        {
            "heading": "4.2 Estimating Burstiness \u2013 Human vs. AI",
            "text": "In Section 1, we discussed the hypothesis that explores the contrasting burstiness patterns between human-written text and AI-generated text. Previous studies that have developed AGTD techniques based on burstiness include (Rychly\u0300, 2011) and (Cummins, 2017). Table 4 shows that there is less distinction in the standard deviation of burstiness scores between AI-generated and human text for OPT. However, when it comes to XLNet, the difference becomes more pronounced. From several such examples, we infer that larger and more complex LLMs gave similar burstiness scores as humans. Hence, we conclude that as the size or complexity of the models increases, the deviation in burstiness scores diminishes. This, in turn, reinforces our claim that perplexity or burstiness estimations cannot be considered as reliable for AGTD (cf. Appendix C)."
        },
        {
            "heading": "5 Negative Log-Curvature (NLC)",
            "text": "In Section 1, we discussed the NLC-based AGTD hypothesis (Mitchell et al., 2023). Our experimental results, depicted in Fig. 1, demonstrate that we are unable to corroborate the same NLC pattern for GPT4. To ensure the reliability of our experiments, we performed 20 perturbations per sentence. Fig. 1 (bottom) presents a comparative analysis of 20 perturbation patterns observed in 2000 samples of OPTgenerated text and human-written text on the same topic. Regrettably, we do not see any discernible pattern. To fortify our conclusions, we compute the standard deviation, mean, and entropy, and conduct a statistical validity test using bootstrapping, which is more appropriate for non-Gaussian distributions (Kim, 2015; Boos and Brownie, 1989). Table 22 documents the results (cf. Appendix C). Based on our experimental results, we argue that NLC is not a robust method for AGTD."
        },
        {
            "heading": "6 Stylometric Variation",
            "text": "Stylometry analysis is a well-studied subject (Lagutina et al., 2019; Neal et al., 2018) where scholars have proposed a comprehensive range of lexical, syntactic, semantic, and structural characteristics for\nthe purpose of authorship attribution. Our investigation, which differs from the study conducted by Kumarage et al. (2023), represents the first attempt to explore the stylometric variations between humanwritten text and AI-generated text. Specifically, we assign 15 LLMs as distinct authors, whereas text composed by humans is presumed to originate from a hypothetical 16th author. Our task involves identifying stylometric variations among these 16 authors. After examining other alternatives put forth in previous studies such as (Tulchinskii et al., 2023), we encountered difficulties in drawing meaningful conclusions regarding the suitability of these methods for AGTD. Therefore, we focus our investigations on a specific approach that involves using perplexity (as a syntactic feature) and burstiness (as a lexical choice feature) as density functions to identify a specific LLM. By examining the range of values produced by these functions, we aim to pinpoint a specific LLM associated with a given text. Probability density such as LplxH = \u2211\u221ek=0\n\u2223\u2223\u2223Pr(Skplx)\u2212 \u03bb kn e\u2212\u03bbnk! \u2223\u2223\u2223 and LbrstyH = \u2211\u221ek=0\n\u2223\u2223\u2223Pr(Skbrsty)\u2212 \u03bb kn e\u2212\u03bbnk! \u2223\u2223\u2223 are calculated using Le Cam\u2019s lemma (Cam, 1986-2012), which gives the total variation distance between the sum of independent Bernoulli variables and a Poisson random variable with the same mean. Where Pr(Skplx) is the perplexity and Pr(Skbrsty) is the brustiness of the of kth sentence respectively. In particular, it tells us that the sum is approximately Poisson in a specific sense (see more in Appendix E). Our experiment suggests stylistic feature estimation may not be very distinctive, with only broad ranges to group LLMs: (i) Detectable (80%+): T0 and T5, (ii) Hard to detect (70%+): XLNet, StableLM, and Dolly, and (iii) Impossible to detect (<50%): LLaMA, OPT, GPT, and variations.\nOur experiment yielded intriguing results. Given that our stylometric analysis is solely based on density functions, we posed the question: what would happen if we learned the search density for one LLM and applied it to another LLM? To explore this, we generated a relational matrix, as depicted in Fig. 7. As previously described and illustrated\nin Fig. 5, the LLMs can be classified into three groups: (i) easily detectable, (ii) hard to detect, and (iii) not detectable. Fig. 7 demonstrates that Le Cam\u2019s lemma learned for one LLM is only applicable to other LLMs within the same group. For instance, the lemma learned from GPT 4 can be successfully applied to GPT-3.5, OPT, and GPT-3, but not beyond that. Similarly, Vicuna, StableLM, and LLaMA form the second group. Fig. 4 offers a visual summary."
        },
        {
            "heading": "7 AI Detectability Index (ADI)",
            "text": "As new LLMs continue to emerge at an accelerated pace, the usability of prevailing AGTD techniques might not endure indefinitely. To align with the ever-changing landscape of LLMs, we introduce the AI Detectability Index (ADI), which identifies the discernable range for LLMs based on SoTA AGTD techniques. The hypothesis behind this proposal is that both LLMs and AGTD techniques\u2019 SoTA benchmarks can be regularly updated to adapt to the evolving landscape. Additionally, ADI serves as a litmus test to gauge whether contemporary LLMs have surpassed the ADI benchmark and are thereby rendering themselves impervious to detection, or whether new methods for AI-generated text detection will require the ADI standard to be reset and re-calibrated.\nAmong the various paradigms of AGTD, we select perplexity and burstiness as the foundation for quantifying the ADI. We contend that NLC is a derivative function of basic perplexity and burstiness, and if there are distinguishable patterns in NLC within AI-generated text, they should be well captured by perplexity and burstiness. We present a summary in Fig. 4 that illustrates the detectable and non-detectable sets of LLMs based on ADI scores obtained using stylometry and classification methods. It is evident that the detectable LLM set is relatively small for both paradigms, while the combination of perplexity and burstiness consistently provides a stable ADI spectrum. Furthermore, we argue that both stylistic features and classification are also derived functions of basic per-\nplexity and burstiness. ADI serves to encapsulate the overall distinguishability between AI-written and human-written text, employing the formula:\nADIx = 100U\u00d72 \u2217 [\u2211 U x=1{\u03b41(x)\u2217\n( Pt\u2212LplxH ) (\n1\u2212\u00b5 plxH ) }+{\u03b42(x)\u2217\n( Bt\u2212LbrstyH ) (\n1\u2212\u00b5brstyH ) }] (1)\nwhere, Pt = 1U \u2217{\u2211Ux=1 ( logpiu \u2212 logpi+1u ) } and Bt = 1U \u2217{\u2211 U x=1 ( logpi+(i+1)+(i+2)u \u2212 logp (i+3)+(i+4)+(i+5) u ) }.\nWhen confronted with a random input text, it is difficult to predict its resemblance to humanwritten text on the specific subject. Therefore, to calculate ADI we employ the mean perplexity (\u00b5 plxH ) and burstiness (\u00b5 brsty H ) derived from human-written text. Furthermore, to enhance the comparison between the current text and human text, Le Cam\u2019s lemma has been applied using precalculated values (LplxH and L brsty H ) as discussed in Section 6. To assess the overall contrast a summation has been used over all the 100K data points as depicted here by U . Lastly, comparative measures are needed to rank LLMs based on their detectability. This is achieved using multiplicative damping factors, \u03b41(x) and \u03b42(x), which are calculated based on \u00b5 \u00b1 rankx \u00d7\u03c3 . Initially, we calculate the ADI for all 15 LLMs, considering \u03b41(x) and \u03b42(x) as 0.5. With these initial ADIs, we obtain the mean (\u00b5) and standard deviation (\u03c3 ), allowing us to recalculate the ADIs for all the LLMs. The resulting ADIs are then ranked and scaled providing a comparative spectrum as presented in Fig. 4. This scaling process is similar to Z-Score Normalization and/or Min-max normalization (Wikipedia,\n2019). However, having damping factors is an easier option for exponential smoothing while we have a handful of data points. Finally, for better human readability ADI is scaled between 0\u2212100.\nFrom the methods we considered, it is unlikely that any of them would be effective for models with high ADI, as shown by our experiments and results. As LLMs get more advanced, we assume that the current AGTD methods would become even more unreliable. With that in mind, ADI will remain a spectrum to judge which LLM is detectable and vs. which is not. Please refer to Appendix F for more discussion.\nThe ADI spectrum reveals the presence of three distinct groups. T0 and T5 are situated within the realm of detectable range, while XLNet, StableLM, Dolly, and Vicuna reside within the difficult-to-detect range. The remaining LLMs are deemed virtually impervious to detection through the utilization of prevailing SoTA AGTD techniques. It is conceivable that forthcoming advancements may lead to improved AGTD techniques and/or LLMs imbued with heightened human-like attributes that render them impossible to detect. Regardless of the unfolding future, ADI shall persist in serving the broader AI community and contribute to AI-related policy-making by identifying non-detectable LLMs that necessitate monitoring through policy control measures."
        },
        {
            "heading": "8 Conclusion",
            "text": "Our proposition is that SoTA AGTD techniques exhibit fragility. We provide empirical evidence to substantiate this argument by conducting experiments on 15 different LLMs. We proposed AI Detectability Index (ADI), a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels. The excitement and success of LLMs have resulted in their extensive proliferation, and this trend is anticipated to persist regardless of the future course they take. In light of this, the CT2 benchmark and the ADI will continue to play a vital role in catering to the scientific community."
        },
        {
            "heading": "9 Ethical Considerations",
            "text": "Our experiments show the limitations of AGTD methods and how to bypass them. We develop ADI with the hope that it could be used for guiding further research and policies. However, it can be misused by bad actors for creating AI-generated text, particularly fake news, that cannot be distinguished from human-written text. We strongly advise against such use of our work."
        },
        {
            "heading": "10 Limitations",
            "text": "Discussion: On June 14th, 2023, the European Parliament successfully passed its version of the EU AI Act (European-Parliament, 2023). Subsequently, a team of researchers from the Stanford Institute for Human-Centered Artificial Intelligence (HAI) embarked on investigating the extent to which Foundation Model Providers comply with the EU AI Act. Their initial findings are presented in the publication by (Bommasani et al., 2023). In this study, the authors put forward a grading system consisting of 12 aspects for evaluating Language Models (LLMs). These aspects include (i) data sources, (ii) data governance, (iii) copyrighted data, (iv) compute, (v) energy, (vi) capabilities & limitations, (vii) risk & mitigations, (viii) evaluation, (ix) testing, (x) machine-generated content, (xi) member states, and (xii) downstream documentation. The overall grading of each LLM can be observed in Fig. 5. While this study is commendable, it appears to be inherently incomplete due to the ever-evolving nature of LLMs. Since all scores are assigned manually, any future changes will require a reassessment of this rubric, while ADI is auto-computable. Furthermore, we propose that ADI should be considered the most suitable metric for assessing risk and mitigations."
        },
        {
            "heading": "10.1 Addressing Opposing Views by Chakraborty et al. (2023)",
            "text": "It is important to note that a recent study (Chakraborty et al., 2023) contradicts our findings and claims otherwise. The study postulates that given enough sample points, whether the output\nwas derived from a human vs an LLM is detectable, irrespective of the LLM used for AI-generated text. The sample size of this dataset is a function of the difference in the distribution of human text vs AItext, with a smaller sample size enabling detection if the distributions show significant differences. However, the study does not provide empirical evidence or specify the required sample size, thus leaving the claim as a hypothesis at this stage.\nFurthermore, the authors propose that employing techniques such as watermarking can change the distributions of AI text, making it more separable from human-text distribution and thus detectable. However, the main drawback of this argument is that given a single text snippet (say, an online article or a written essay), detecting whether it is AI-generated is not possible. Also, the proposed technique may not be cost-efficient compute-wise, especially as new LLMs emerge. However, the authors did not provide any empirical evidence to support this hypothesis.\nLimitations: This paper delves into the discussion of six primary methods for AGTD and their potential combinations. These methods include (i) watermarking, (ii) perplexity estimation, (iii) burstiness estimation, (iv) negative loglikelihood curvature, (v) stylometric variation, and (vi) classifier-based approaches.\nOur empirical research strongly indicates that the proposed methods are vulnerable to tampering or manipulation in various ways. We provide extensive empirical evidence to support this argument. However, it is important to acknowledge that there may still exist potential deficiencies in our experiments. In this section, we explore and discuss further avenues for investigation in order to address these potential shortcomings. In the subsequent paragraph, we outline the potential limitations associated with each of the methods we have previously investigated."
        },
        {
            "heading": "10.2 Watermarking",
            "text": "Although Kirchenbauer et al. (2023a) was the pioneering paper to introduce watermarking for AI-\ngenerated text, this research has encountered numerous criticisms since its inception. A major concern raised by several fellow researchers (Sadasivan et al., 2023) is that watermarking can be easily circumvented through machine-generated paraphrasing. In our experiment, we have presented two potential de-watermarking techniques. Subsequently, the same group of researchers published a follow-up paper (Kirchenbauer et al., 2023b) in which they asserted the development of a more advanced and robust watermarking technique. We assessed this claim as well and discovered that de-watermarking remains feasible. However, although the overall accuracy of de-watermarking has decreased, it still retains considerable strength. As the paper was published on June 9th, 2023, we will include the complete experiment details in the final version of our report.\nIn their work, Kirchenbauer et al. (2023b) put forward improved watermarking techniques by enhancing the hashing mechanism for selecting watermarking keys and introducing more effective watermark detection techniques. They conducted\nextensive testing on de-watermarking possibilities, considering both machine-generated paraphrasing and human paraphrasing, and observed dilution in the strength of the watermark, which aligns with their findings.\nAlthough paraphrasing is a powerful technique for attacking watermark text, we argue that highentropy-based word replacement offers a superior approach. When using high-entropy word replacements, it becomes exceedingly difficult for watermark detection modules to identify the newly generated text, even after paraphrasing. We will now elaborate on our rationale. In their work, Kirchenbauer et al. (2023b) identify content words such as nouns, verbs, adjectives, and adverbs as suitable candidates for replacement. However, any advanced techniques employed to select replacement watermark keys for these positions will result in high-entropy words. Consequently, these replacements will always remain detectable, regardless of the strength of the hashing mechanism."
        },
        {
            "heading": "10.3 Perplexity and Burstiness Estimation",
            "text": "Liang et al. (2023) and Chakraborty et al. (2023) among others have shown perplexity and burstiness are often not reliable indicators of human written text. The fallibility of these metrics become especially prominent in academic writing or text generated in a low-resource language. Our experiments have also pointed towards similar findings. Moreover, in our experiments, we computed perplexity and burstiness metrics both at the overall text level and the sentence level. It is also feasible to calculate perplexity at smaller fragment levels. Since each language model has a unique attention mechanism and span, these characteristics can potentially manifest in the generated text, making them detectable. However, determining the precise fragment size for a language model necessitates extensive experimentation, which we have not yet conducted."
        },
        {
            "heading": "10.4 Negative Log Curvature",
            "text": "Although we discussed earlier, it is crucial to reemphasize the significant limitations of DetectGPT (Mitchell et al., 2023). One of its major limitations is that it relies on access to the log probabilities of the texts, which necessitates the use of a specific LLM. However, it is unlikely that we would know in advance which LLM was employed to generate a particular text, and the log-likelihood calculated by different LLMs for the same text would yield significantly different results. In reality, one would need to compare the results with all available LLMs in existence, which would require a computationally expensive brute-force search. In our experiments, we empirically demonstrate that the hypothesis of log-probability #2 < logprobability #1 can be easily manipulated using simple [MASK]-based post-fixing techniques."
        },
        {
            "heading": "10.5 Stylometric Variation",
            "text": "In this experiment, we made a simplifying assumption that all the human-written text was authored by a single individual, which is certainly not re-\nflective of reality. Furthermore, texts composed by different authors inevitably leave behind their unique traces and characteristics. Furthermore, a recent paper by Tulchinskii et al. (2023) introduced the concept of intrinsic dimensionality estimation, which can be described as a stylometric analysis. However, this paper is currently available only on arXiv and lacks an implemented solution. We are currently working on replicating the theory and evaluating the robustness of the approach."
        },
        {
            "heading": "10.6 Classifier-based Approaches",
            "text": "Numerous classifiers have been proposed in the literature (Zellers et al., 2020; Gehrmann et al., 2019; Solaiman et al., 2019). However, the majority of these classifiers are specifically created to identify instances generated by individual models. They achieve this by either utilizing the model itself (as demonstrated by Mitchell et al. (2023)) or by training on a dataset consisting of the generated samples from that particular model. For example, RoBERTa-Large-Detector developed by OpenAI (OpenAI, 2023b) is trained or fine-tuned specifically for binary classification tasks. These detectors are trained using datasets that consist of both human-generated and AI-generated texts. Consequently, their ability to effectively classify data from new models and unfamiliar domains is severely limited."
        },
        {
            "heading": "A LLM Selection Criteria",
            "text": "Beyond the primary criteria for choosing performant LLMs, our selection was meant to cover a wide gamut of LLMs that utilize a repertoire of recent techniques under the hood that have enabled their exceptional capabilities, namely: FlashAttention (Dao et al., 2022) for memory-efficient exact attention, Multi-Query Attention (Shazeer, 2019) for memory bandwidth efficiency, SwiGLU (Shazeer, 2020) as the activation function instead of ReLU (Agarap, 2019), ALiBi (Press et al., 2022) for larger context width, RMSNorm (Zhang and Sennrich, 2019) for per-normalization, RoPE (Su et al., 2021) to improve the expressivity of positional embeddings, etc."
        },
        {
            "heading": "B De-Watermarking",
            "text": "As also shown by Krishna et al. (2023), watermarked texts can be relatively easily de-watermarked. Even with the implementation of the newer, more robust watermarking scheme presented by Kirchenbauer et al. (2023b), we were still able to circumvent the watermarks to a significant extent. Here we discuss the methods in detail, concluding with Table 21 showing de-watermarking accuracies across 15 LLMs after paraphrasing.\nB.1 De-watermarking by spotting high entropy words and replacing them The pivotal proposal made by the watermarking paper is to spot high entropy words and replace them with a random word from the vocabulary, so it is evident that if watermarking has been done, it has been done on those words.\nWhat are high entropy words? High entropy words refer to words that are less predictable and occur less frequently in a corpus. These words have a higher degree of randomness and uncertainty and thus, pose a challenge for LLMs because they require a greater amount of training for accurate prediction. High entropy words can include domain-specific jargon or technical terms. Based on the observed patterns and frequencies of the training data, language models assign probabilities to words. Words with a high entropy tend to have lower probabilities because they are less common or have a more diverse contextual usage. These words are frequently uncommon or specialized terms, uncommon proper nouns, or words that are highly topic- or domain-specific. An example of such a high entropy word used in a sentence is as follows: \"The adventurous child clambered up the gnarled tree, seeking the thrill of climbing to its lofty branches.\" In this sentence, the word \"gnarled\" is a high entropy word. It describes something that is twisted, rough, or knotted, typically referring to tree branches or old, weathered objects. In different language models, alternative words that might occur instead of \"gnarled\" could be \"twisted,\" \"knotty,\" or \"weathered.\" These alternatives convey a similar meaning with more commonly used vocabulary. For instance, consider a masked input sentence: \"Paris is the [MASK] of France.\" In this scenario, an LLM might predict candidate words with corresponding probabilities as follows: (i) \u201ccapital\u201d [0.99], (ii) \u201ccity\u201d [0.0], (iii) \u201cmetropolis\u201d [0.0]. Here, the LLM demonstrates a high level of certainty regarding the word \u201ccapital\u201d to fill the mask. Now, consider another sentence: \"I saw a [MASK] last night.\" The LLM\u2019s predicted candidate words and their corresponding probabilities are: (i) \u201cghost\u201d [0.096], (ii) \u201cUFO\u201d [0.083], (iii) \u201cvampire\u201d [0.045]. In this case, the LLM exhibits uncertainty in choosing the appropriate candidate word.\nB.2 Dewatemarking on 14 LLMs Here we present performance evaluation of all the models\u2019 combination for the rest of the 14 LLMs. The \"Pre\" column shows the accuracy scores for the text that was successfully de-watermarked without any paraphrasing techniques. The \"Post\" column shows the accuracy scores for a text that was not successfully de-watermarked in the initial attempt but was able to be de-watermarked more successfully after paraphrasing methods were applied.\nB.3 De-watermarking by paraphrasing A recent paper (Krishna et al., 2023) talks about the DIPPER paraphrasing technique and how it can easily bypass the watermarking technique. However, their de-watermarking strategy can reduce the detection accuracy of the watermark detector tool to a certain extent. It can\u2019t fully de-watermark all the texts.\nAnother paper (Sadasivan et al., 2023) also uses the DIPPER paraphrasing technique but a slightly modified version in which they use parallel paraphrasing of multiple sentences. However, in this paper, they came up with how to bypass the paraphrasing technique so that even after paraphrasing, the detector can tell if the text is in fact AI-generated. This bypassing technique was named Retrieval and it uses the semantic sequence to detect AI-generated text even after paraphrasing (Krishna et al., 2023).\nBoth these papers also talk about the negative log-likelihood and perplexity score and they have tried on GPT and OPT models.\nBased on empirical observations, we concluded that GPT-3.5 outperformed all the other models. To offer transparency around our experiment process, we detail the aforementioned evaluation dimensions as follows.\nCoverage - number of considerable paraphrase generations: We intend to generate up to 5 paraphrases per given claim. Given all the generated claims, we perform a minimum edit distance (MED) (Wagner and Fischer, 1974) - units are words instead of alphabets). If MED is greater than \u00b12 for any given paraphrase candidate (for e.g., c\u2212 pc1) with the claim, then we further consider that paraphrase, otherwise discarded. We evaluated all three models based on this setup that what model is generating the maximum number of considerable paraphrases.\nModels Paraphrase\nGPT-3.5-Turbo Pegasus Flan-T5-XXL wv1 wv2 wv1 wv2 wv1 wv2\nGPT 4 88% 73% 79% 69% 78% 68% GPT 3.5 89% 72% 78% 68% 79% 69% OPT 90% 70% 79% 67% 80% 72% GPT 3 91% 70% 82% 68% 81% 73% Vicuna 93% 74% 85% 70% 82% 75% StableLM 95.0% 98.0% 96.4% 87.0% 83.0% 42.5% MPT 96.0% 99.0% 88.5% 90.1% 85.0% 68.7% LLaMA 95.0% 98.7% 89.3% 99.1% 98.0% 98.9% Alpaca 95.0% 99.0% 95.5% 99.0% 70.5% 66.7% GPT 2 70.3% 91.0% 89.0% 79.5% 68.0% 99.0% Dolly 98.0% 96.0% 95.6% 91.6% 98.0% 70.9% BLOOM 97.0% 97.0% 87.2% 92.9% 85.5% 76.8% T0 98.0% 99.0% 96.8% 96.0% 83.9% 80.0% XLNet 91.3% 88.0% 98.3% 89.7% 63.3% 63.0% T5 97.7% 99.1% 99.0% 98.4% 98.9% 99.2%\nTable 21: A summary of the effectiveness of the three paraphrasing methods - a) Pegasus (Zhang et al., 2020), (b) Flan-t5-xxl (Chung et al., 2022), and (c) GPT-3.5 (gpt-3.5-turbo-0301 variant) (Ye et al., 2023) for de-watermarking.\nCorrectness - correctness in those generations: After the first level of filtration we have performed pairwise entailment and kept only those paraphrase candidates, are marked as entailed by the (Liu et al., 2019) (Roberta Large), SoTA trained on SNLI (Bowman et al., 2015).\nDiversity - linguistic diversity in those generations: We were interested in choosing that model can produce linguistically more diverse paraphrases. Therefore we are interested in the dissimilarities check between generated paraphrase claims. For e.g., c\u2212 pcn, pc1 \u2212 pcn, pc2 \u2212 pcn, . . . , pcn\u22121 \u2212 pcn and repeat this process for all the other paraphrases and average out the dissimilarity score. There is no such metric to measure dissimilarity, therefore we use the inverse of the BLEU score (Papineni et al., 2002). This gives us an understanding of how linguistic diversity is produced by a given model. Based on these experiments, we found that gpt-3.5-turbo-0301 performed the best. The results of the experi-\nment are reported in the following table. Furthermore, we were more interested to choose a model that\ncan maximize the linguistic variations, and gpt-3.5-turbo-0301 performs on this parameter of choice as well. A plot on diversity vs. all the chosen models is reported in Fig. 2.\nTable 21 provides a summary of the effectiveness of the three paraphrasing methods for dewatermarking. Among them, the GPT3.5 based method demonstrated the highest performance. Additionally, it is worth noting that the de-watermarking accuracy for wv2, the watermarking technique proposed in (Kirchenbauer et al., 2023b), showed a slight decrease compared to wv1, the watermarking technique proposed in (Kirchenbauer et al., 2023a)."
        },
        {
            "heading": "C Perplexity and Burstiness Estimation",
            "text": "We have conducted an analysis to determine the perplexity and burstiness of an LLM, as well as calculate sentence-wise entropy. In order to evaluate the statistical significance of our findings, we employed the bootstrap method. Results of these experiments on all 15 models are reported in Table 22.\nBrief on bootstrap method: Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples, illustrated in Fig. 6. This process allows for the calculation of standard errors, confidence intervals, and hypothesis testing. A bootstrapping approach is an extremely useful alternative to the traditional method of hypothesis testing as it is fairly simple and it mitigates some of the pitfalls encountered within the traditional approach. As with the traditional approach, a sample of size n is drawn from the population within the bootstrapping approach. Let us call this sample S. Then, rather than using theory to determine all possible estimates, the sampling distribution is created by resampling observations with replace-\nment from S, m times, with each resampled set having n observations. Now, if sampled appropriately, S should be representative of the population. Therefore, by resampling S m times with replacement, it would be as if m samples were drawn from the original population, and the estimates derived would be representative of the theoretical distribution under the traditional approach. It must be noted that increasing the number of resamples, m, will not increase the amount of information in the data. That is, resampling the original set 100,000 times is not more useful than only resampling it 1,000 times. The amount of information within the set is dependent on the sample size, n, which will remain constant throughout each resample. The benefit of more resamples, then, is to derive a better estimate of the sampling distribution. The traditional procedure requires one to have a test statistic that satisfies particular assumptions in order to achieve valid results, and this is largely dependent on the experimental design. The traditional approach also uses theory to tell what the sampling distribution should look like, but the results fall apart if the assumptions of the theory are not met. The bootstrapping method, on the other hand, takes the original sample data and then resamples it to create many [simulated] samples. This approach does not rely on the theory since the sampling distribution can simply be observed, and one does not have to worry about any assumptions. This technique allows for accurate estimates of statistics, which is crucial when using data to make decisions.\nC.1 Reliability of Perplexity, Burstiness and NLC as AGT Signals for all LLMs Here we present the complete table showing results after performing experiments on Perplexity estimation (Section 4.1), Burstiness estimation (Section 4.2) and NLC (Section 5) over all 15 LLMs.\nC.2 Plots for 15 LLMs across the ADI spectrum Here we present the histogram plots and negative log-curvature line plots for all 15 LLMs. Arranged as per the ADI spectrum, it is evident that higher ADI models come much closer to generating text similar to humans that models that fall lower on the spectrum.\nGPT4 0 100 200 300 400 Human-GPT4 Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text GPT-4_text\n0 20 40 60 80 100 Human-GPT4 Generated Text\n0\n50\n100\n150\n200\n250\n300\n350\nPe rp\nle xi\nty\nHuman_text gpt-4_text\nGPT3.5 0 100 200 300 400 Human-GPT3.5 Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text GPT-3.5_text\n0 20 40 60 80 100 Human-GPT3.5 Generated Text\n50\n100\n150\n200\n250\n300\n350\nPe rp\nle xi\nty\nHuman_text GPT-3.5_text\nOPT 0 100 200 300 400 Human-OPT Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text OPT_text\n0 20 40 60 80 100 Human-OPT Generated Text\n0\n100\n200\n300\n400\n500\nPe rp\nle xi\nty\nHuman_text OPT_text\nModel Histplot Lineplot\nContinued on next page Table 23: Histogram and Line plots for perplexity estimation and NLC.\nGPT3 0 100 200 300 400 Human-GPT3 Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005 0.006 D en si ty Histplot of Human-AI Perplexity Human_text GPT-3_text\n0 20 40 60 80 100 Human-GPT3 Generated Text\n0\n100\n200\n300\n400\nPe rp\nle xi\nty\nHuman_text GPT-3_text\nVicuna 0 100 200 300 400 Human-Vicuna Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text Vicuna_text\n0 20 40 60 80 100 Human-Vicuna Generated Text\n50\n100\n150\n200\n250\n300\n350\nPe rp\nle xi\nty\nHuman_text Vicuna_text\nStableLM 0 100 200 300 400 Human-StableLM Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text stableLM_text\n0 20 40 60 80 100 Human-StableLM Generated Text\n50\n100\n150\n200\n250\n300\nPe rp\nle xi\nty\nHuman_text StableLM_text\nMPT 0 100 200 300 400 Human-MPT Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text MPT_text\n0 20 40 60 80 100 Human-MPT Generated Text\n0\n100\n200\n300\n400\nPe rp\nle xi\nty\nHuman_text MPT_text\nModel Histplot Lineplot\nContinued on next page Table 23: Histogram and Line plots for perplexity estimation and NLC. (Continued)\nLLaMA 0 100 200 300 400 Human-LLaMA Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text LLaMA_text\n0 20 40 60 80 Human-LLaMA Generated Text\n0\n50\n100\n150\n200\n250\n300\nPe rp\nle xi\nty\nHuman_text LLaMA_text\nAlpaca 100 0 100 200 300 400 Human-Alpaca Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text Alpaca_text\n0 20 40 60 80 100 Human-Alpaca Generated Text\n50\n100\n150\n200\n250\n300\nPe rp\nle xi\nty\nHuman_text Alpaca_text\nGPT2 0 100 200 300 400 500 Human-GPT2 Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text GPT-2_text\n0 20 40 60 80 100 Human-GPT2 Generated Text\n0\n100\n200\n300\n400\n500\n600\nPe rp\nle xi\nty\nHuman_text GPT-2_text\nDolly 0 100 200 300 400 500 Human-Dolly Text Avg Perplexity\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nD en\nsi ty\nHistplot of Human-AI Perplexity Human_text Dolly_text\n0 20 40 60 80 100 Human-Dolly Generated Text\n50\n100\n150\n200\n250\n300\nPe rp\nle xi\nty\nHuman_text Dolly_text\nModel Histplot Lineplot\nContinued on next page Table 23: Histogram and Line plots for perplexity estimation and NLC. (Continued)"
        },
        {
            "heading": "D Negative Log-Curvature (NLC)",
            "text": "DetectGPT (Mitchell et al., 2023) utilizes the generation of log-probabilities for textual analysis. It leverages the difference in perturbation discrepancies between machine-generated and human-written text to detect the origin of a given piece of text. When a language model produces text, each individual token is assigned a conditional probability based on the preceding tokens. These conditional probabilities are then multiplied together to derive the joint probability for the entire text. To determine the origin of the text, DetectGPT introduces perturbations. If the probability of the perturbed text significantly decreases compared to the original text, it is deemed to be AI-generated. Conversely, if the probability remains roughly the same, the text is considered to be human-generated.\nThe hypothesis put forward by Mitchell et al. (2023) suggests that the perturbation patterns of AIwritten text should align with the negative log-likelihood region. However, this observation is not supported by the results presented here. To strengthen our conclusions, we calculated the standard deviation, mean, and entropy, and performed a statistical validity test in the form of a p-test. The findings are reported in Table 22."
        },
        {
            "heading": "E Stylometric variation",
            "text": "The field of stylometry analysis has been extensively researched, with scholars proposing a wide range of lexical, syntactic, semantic, and structural features for authorship attribution. In our study, we employed Le Cam\u2019s lemma (Cam, 1986-2012) as a perplexity density estimation method. However, there are several alternative approaches that can be suggested, such as kernel density estimation (Wikipedia_KDE), mean integrated squared error (Wikipedia_MISE), kernel embedding of distributions (Wikipedia_KED), and spectral density estimation (Wikipedia_SDE). While we have not extensively explored these variations in our current study, we express interest in investigating them in future research."
        },
        {
            "heading": "F AI Detectability Index (ADI) \u2013 other possible variations",
            "text": "In our previous discussions, we have advocated for utilizing perplexity and burstiness as the fundamental metrics to quantify ADI within the context of various paradigms of AGTD. However, it is important to acknowledge that alternative features, such as stylistics, can also be employed to calculate the ADI. For instance, if we consider stylistic features like syntactic variation (LsynH ) and lexicon variations (L lex H ), the ADI can be reformulated as follows:\nADIx = 100U\u00d72 \u2217 [\u2211 U x=1{\u03b41(x)\u2217 (Pt\u2212LsynH ) (1\u2212\u00b5synH ) }+{\u03b42(x)\u2217 (Bt\u2212LlexH ) (1\u2212\u00b5 lexH ) }] (2)\nwhere, Pt = 1U \u2217{\u2211Ux=1 ( logpiu(syn)\u2212 logpi+1u (syn) ) and Pt = 1U \u2217{\u2211Ux=1 ( logpiu(lex)\u2212 logpi+1u (lex) ) Similarly, it is worth noting that in the future, other potential features such as NLC and any novel features that may be proposed could also be incorporated within the framework of ADI."
        },
        {
            "heading": "G Generated Samples",
            "text": "Text generated by each of the 15 LLMs given the same prompt \u201cAI generated text detection is not easy\u201d are displayed in Table 24.\nGPT 4 AI-generated text detection has become more popular in recent years as technology advances and the demand for automated text analysis increases. While it offers many benefits, such as\nbeing faster and less error-prone than manual data entry, there are also drawbacks to consider. For\nexample, AI-generated text detection often requires a large amount of training data to be accurate,\nwhich can be expensive and time consuming to collect. Additionally, AI algorithms may not\nalways detect the nuances or cultural references within the text that humans might understand\nbetter. Overall, while AI-generated text detection is certainly an important tool in...\nGPT-3.5 AI-generated text detection is an intricate challenge that continues to perplex researchers and developers alike. The rapid advancements in AI language models, such as GPT-3, have ushered in\nan era of sophisticated text generation that blurs the line between human and machine-generated\ncontent. Detecting AI-generated text requires a comprehensive understanding of the nuances and\npatterns exhibited by these models. It demands an amalgamation of statistical analysis, linguistic\nanalysis, and machine learning techniques. The dynamic nature of AI models, coupled with\ntheir ability to adapt and improve, poses an ongoing hurdle in creating robust detection systems.\nStriving to stay one step ahead, researchers tirelessly explore innovative methodologies, such as\nadversarial testing and anomaly detection, to combat the ever-evolving landscape of AI-generated\ntext. While progress is being made, the road to reliable and foolproof AI-generated text detection\nremains a complex and formidable journey.\nGPT-3 AI generated text detection is not easy to find. i\u2019ve got a number of messages and text messages going around here today. you should probably sit down and look at some of them. \" he said he\nhad been listening to the messages of the...\nGPT-2 AI generated text detection is a challenging task. While AI algorithms have made great strides in recent years, there are several factors that make text detection difficult. One of the main\nissues is the variability in text itself, both in terms of font and style. Additionally, text may\nappear in different orientations and lighting conditions, making it more difficult for algorithms\nto accurately detect and identify. Finally, background noise and other visual clutter can further\ncomplicate the task. These challenges require AI developers to continually develop and improve\ntheir algorithms to reliably detect and interpret text.\nOPT I\u2019m not sure what the topic is here. What is he trying to say? I think he is saying that it is too easy to detect AI generated text (as many real AI generated text is very detectable) and so the\ntext detection is not that good. That\u2019s not really what I was getting at... I\u2019m not trying to say that\nit\u2019s too easy to detect AI generated text, I\u2019m trying to say that it\u2019s hard to do. I\u2019m not trying to\nsay that it\u2019s not hard to detect AI generated text.\nPrompt AI generated text detection is not easy\nContinued on next page Table 24: AI generated text for a sample prompt given to all 15 LLMs\nLLaMA There are two ways to tackle the task of detecting whether an article was written by machine or human. One way would be to use a technique called Natural Language Understanding, which\ninvolves generating some kind of semantic model that can identify the properties and relations\nbetween words in order to determine if one particular word-string is likely to have been produced\nby a person or an algorithm. The other approach might involve using something like natural\nlanguage generation (see this paper) where you could generate some kind of \u201creverse\u201d sentence\nfrom a piece of raw data. But while there\u2019s a lot we still don\u2019t know about how our brains work\nwhen it comes to understanding sentences - such as why certain parts of speech seem easier for\nus to understand than others - it seems fairly safe to say that the way machines currently learn to\nwrite prose isn\u2019t yet able to match up with what humans do naturally. If I were writing an essay\ntoday, my biggest concern would probably revolve around being understood rather than trying to\nwrite really well. That said, people definitely do care about both things at once; just look at all\nthose bloggers out there obsess...\nBLOOM I agree that AI generated text detection is not easy, especially given the fact that there are many tools and resources available on the internet that can be used to generate text. But it\u2019s\nnot impossible. There are several ways to detect AI generated text, such as using natural\nlanguage processing (NLP) techniques, analyzing the writing style and patterns, and checking\nfor inconsistencies and errors in grammar and syntax. Additionally, it\u2019s important to note that\nnot all AI generated text is bad or negative, and some can be beneficial and helpful, such as in\nthe case of chatbots and AI assistants.\nMPT AI-generated content identification can be difficult, as it requires complex algorithms to identify patterns in large amounts of data and determine whether they are human or machine written.\nThis process becomes even more challenging when dealing with short texts such as tweets,\nwhich often contain slang words and abbreviations unique to certain communities. Despite these\nchallenges, researchers have made significant progress towards developing accurate methods for\ndetecting AI-generated text.\nPrompt AI generated text detection is not easy\nContinued on next page Table 24: AI generated text for a sample prompt given to all 15 LLMs (Continued)\nAlpaca AI generated text detection is not an easy task. Text detection is the process of identifying and extracting text from an image, video, or document. It is a challenging task due to the wide\nvariety of text styles, sizes, colors, and backgrounds that can be found in an image or document.\nAdditionally, AI generated text detection must be able to accurately detect text even if it is\npartially obscured by other elements in the image or document. AI generated text detection must\nalso be able to accurately detect text even if it is in a different language than the language of\nthe image or document. AI generated text detection must also be able to accurately detect text\neven if it is in a different font or font size than the rest of the text in the image or document. AI\ngenerated text detection must also be able to accurately detect text even if it is in a different color\nthan the rest of the text in the image or document. AI generated text detection must also be able\nto accurately detect text even if it is in a different orientation than the rest of the text in the image\nor document. AI generated text detection must also be able to accurately detect text even if it is\nin a different shape than the rest...\nVicuna Detecting AI-generated text can be a challenging task. Unlike human-written content, which often contains unique phrasing and grammatical errors, AI-generated text is designed to be\nflawless. As a result, it can be difficult for humans to differentiate between the two. Additionally,\nAI algorithms can produce vast amounts of text quickly, making it increasingly important to\ndevelop reliable detection methods. With advancements in technology and machine learning\ntechniques, detecting AI-generated text will continue to evolve and become more sophisticated\nover time.\nDolly Generating text in an image is not an easy task. There are several steps that need to be done in order to generate text from an image. The first step is to pre-process the image so that it can be\nbetter understood by the computer. One of the first steps in this process is to detect the text in the\nimage. There are many libraries and frameworks that can be used to do this text detection. Some\nof these libraries include python-OCR, OpenText, and Tesseract. Once the text has been detected,\nthe next step is to tokenize the text. Tokenization is the process of splitting the detected text up\ninto individual words. Some libraries that can be used for tokenization are NLTK and SpaCy.\nAfter the text has been tokenized, you then need to filter out all the stopwords. Stopwords are\ncommon words that are not considered to be part of the text. Once all the stopwords have been\nremoved, you then need to lemmatize the words that have been removed. Lemmatization is the\nprocess of removing the inflection from words and treating them as if they were roots. The last\nstep in this process is to sentence piece the tokens that have been created from the tokenization\nstep.\nPrompt AI generated text detection is not easy\nContinued on next page Table 24: AI generated text for a sample prompt given to all 15 LLMs (Continued)"
        }
    ],
    "title": "Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think \u2013 Introducing AI Detectability Index",
    "year": 2023
}