{
    "abstractText": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jianqiao Lu"
        },
        {
            "affiliations": [],
            "name": "Wenyong Huang"
        },
        {
            "affiliations": [],
            "name": "Nianzu Zheng"
        },
        {
            "affiliations": [],
            "name": "Xingshan Zeng"
        },
        {
            "affiliations": [],
            "name": "Yu Ting Yeung"
        },
        {
            "affiliations": [],
            "name": "Xiao Chen"
        }
    ],
    "id": "SP:63ea0a89da5f758c76df8e25a138045a823c6f06",
    "references": [
        {
            "authors": [
                "Junyi Ao",
                "Rui Wang",
                "Long Zhou",
                "Shujie Liu",
                "Shuo Ren",
                "Yu Wu",
                "Tom Ko",
                "Qing Li",
                "Yu Zhang",
                "Zhihua Wei"
            ],
            "title": "SpeechT5: Unified-modal encoderdecoder pre-training for spoken language processing",
            "venue": "arXiv preprint arXiv:2110.07205",
            "year": 2021
        },
        {
            "authors": [
                "Gaurav Arora",
                "Chirag Jain",
                "Manas Chaturvedi",
                "Krupal Modi."
            ],
            "title": "HINT3: Raising the bar for intent detection in the wild",
            "venue": "Proceedings of the First",
            "year": 2020
        },
        {
            "authors": [
                "Siddhant Arora",
                "Siddharth Dalmia",
                "Pavel Denisov",
                "Xuankai Chang",
                "Yushi Ueda",
                "Yifan Peng",
                "Yuekai Zhang",
                "Sujay Kumar",
                "Karthik Ganesan",
                "Brian Yan"
            ],
            "title": "ESPnet-SLU: Advancing spoken language understanding through ESPnet",
            "year": 2022
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ankur Bapna",
                "Yu-an Chung",
                "Nan Wu",
                "Anmol Gulati",
                "Ye Jia",
                "Jonathan H Clark",
                "Melvin Johnson",
                "Jason Riesa",
                "Alexis Conneau",
                "Yu Zhang."
            ],
            "title": "SLAM: A unified encoder for speech and language modeling via speech-text joint pre-training",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Emanuele Bastianelli",
                "Andrea Vanzo",
                "Pawel Swietojanski",
                "Verena Rieser."
            ],
            "title": "Slurp: A spoken language understanding resource package",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Braun",
                "Adrian Hernandez-Mendez",
                "Florian Matthes",
                "Manfred Langen."
            ],
            "title": "Evaluating natural language understanding services for conversational question answering systems",
            "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse",
            "year": 2017
        },
        {
            "authors": [
                "Xavier Carreras",
                "Llu\u00eds M\u00e0rquez."
            ],
            "title": "Introduction to the CoNLL-2004 shared task: Semantic role labeling",
            "venue": "Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL2004) at HLT-NAACL 2004, pages 89\u201397.",
            "year": 2004
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Temcinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vulic."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on NLP for ConvAI ACL 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J Weiss",
                "Mohammad Norouzi",
                "Najim Dehak",
                "William Chan."
            ],
            "title": "WaveGrad 2: Iterative refinement for text-tospeech synthesis",
            "venue": "arXiv preprint arXiv:2106.09660.",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Chen",
                "Weiyi Lu",
                "Alejandro Mottini",
                "Li Erran Li",
                "Jasha Droppo",
                "Zheng Du",
                "Belinda Zeng."
            ],
            "title": "Top-down attention in end-to-end spoken language understanding",
            "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2021
        },
        {
            "authors": [
                "Zhehuai Chen",
                "Yu Zhang",
                "Andrew Rosenberg",
                "Bhuvana Ramabhadran",
                "Pedro Moreno",
                "Ankur Bapna",
                "Heiga Zen."
            ],
            "title": "Maestro: Matched speech text representations through modality matching",
            "venue": "arXiv preprint arXiv:2204.03409.",
            "year": 2022
        },
        {
            "authors": [
                "Zhehuai Chen",
                "Yu Zhang",
                "Andrew Rosenberg",
                "Bhuvana Ramabhadran",
                "Pedro Moreno",
                "Gary Wang"
            ],
            "title": "2022b. Tts4pretrain 2.0: Advancing the use of text and speech in asr pretraining with consistency and contrastive losses",
            "year": 2022
        },
        {
            "authors": [
                "Yu-An Chung",
                "Wei-Hung Weng",
                "Schrasing Tong",
                "James Glass."
            ],
            "title": "Unsupervised cross-modal alignment of speech and text embedding spaces",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Leon Derczynski",
                "Eric Nichols",
                "Marieke van Erp",
                "Nut Limsopatham."
            ],
            "title": "Results of the WNUT2017 shared task on novel and emerging entity recognition",
            "venue": "Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 140\u2013147.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM, 63(11):139\u2013144.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Graves."
            ],
            "title": "Sequence transduction with recurrent neural networks",
            "venue": "arXiv preprint arXiv:1211.3711.",
            "year": 2012
        },
        {
            "authors": [
                "Wei Han",
                "Zhengdong Zhang",
                "Yu Zhang",
                "Jiahui Yu",
                "Chung-Cheng Chiu",
                "James Qin",
                "Anmol Gulati",
                "Ruoming Pang",
                "Yonghui Wu."
            ],
            "title": "ContextNet: Improving convolutional neural networks for automatic speech recognition with global context",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel."
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851.",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans."
            ],
            "title": "Classifierfree diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598.",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed."
            ],
            "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE/ACM Transactions on Audio,",
            "year": 2021
        },
        {
            "authors": [
                "Wenyong Huang",
                "Wenchao Hu",
                "Yu Ting Yeung",
                "Xiao Chen."
            ],
            "title": "Conv-Transformer Transducer: Low latency, low frame rate, streamable end-to-end speech recognition",
            "venue": "Interspeech 2020, pages 5001\u2013 5005.",
            "year": 2020
        },
        {
            "authors": [
                "Wenyong Huang",
                "Zhenhe Zhang",
                "Yu Ting Yeung",
                "Xin Jiang",
                "Qun Liu"
            ],
            "title": "SPIRAL: Self-supervised perturbation-invariant representation learning",
            "year": 2022
        },
        {
            "authors": [
                "Yinghui Huang",
                "Hong-Kwang Kuo",
                "Samuel Thomas",
                "Zvi Kons",
                "Kartik Audhkhasi",
                "Brian Kingsbury",
                "Ron Hoory",
                "Michael Picheny."
            ],
            "title": "Leveraging unpaired text data for training end-to-end speech-tointent systems",
            "venue": "2020 IEEE International Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine."
            ],
            "title": "Elucidating the design space of diffusion-based generative models",
            "venue": "arXiv preprint arXiv:2206.00364.",
            "year": 2022
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Damien Vincent",
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Sertan Girgin",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Marco Tagliasacchi",
                "Neil Zeghidour."
            ],
            "title": "Speak, read and prompt: High-fidelity textto-speech with minimal supervision",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Minjeong Kim",
                "Gyuwan Kim",
                "Sang-Woo Lee",
                "JungWoo Ha."
            ],
            "title": "ST-BERT: Cross-modal language model pre-training for end-to-end spoken language understanding",
            "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2021
        },
        {
            "authors": [
                "Aleksandr Laptev",
                "Roman Korostik",
                "Aleksey Svischev",
                "Andrei Andrusenko",
                "Ivan Medennikov",
                "Sergey Rybin."
            ],
            "title": "You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation",
            "venue": "arXiv preprint arXiv:2005.07157.",
            "year": 2020
        },
        {
            "authors": [
                "Stefan Larson",
                "Kevin Leach."
            ],
            "title": "Redwood: Using collision detection to grow a large-scale intent classification dataset",
            "venue": "arXiv preprint arXiv:2204.05483.",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Larson",
                "Anish Mahendran",
                "Joseph J Peper",
                "Christopher Clarke",
                "Andrew Lee",
                "Parker Hill",
                "Jonathan K Kummerfeld",
                "Kevin Leach",
                "Michael A Laurenzano",
                "Lingjia Tang"
            ],
            "title": "An evaluation dataset for intent classification and out-of-scope",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Liu",
                "Yan Xu",
                "Tiezheng Yu",
                "Wenliang Dai",
                "Ziwei Ji",
                "Samuel Cahyawijaya",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "CrossNER: Evaluating crossdomain named entity recognition",
            "venue": "ArXiv preprint arXiv:2012.04373.",
            "year": 2020
        },
        {
            "authors": [
                "Yi Luan",
                "Luheng He",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Loren Lugosch",
                "Brett H Meyer",
                "Derek Nowrouzezahrai",
                "Mirco Ravanelli."
            ],
            "title": "Using speech synthesis to train end-to-end spoken language understanding models",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP",
            "year": 2020
        },
        {
            "authors": [
                "Christoph L\u00fcscher",
                "Eugen Beck",
                "Kazuki Irie",
                "Markus Kitza",
                "Wilfried Michel",
                "Albert Zeyer",
                "Ralf Schl\u00fcter",
                "Hermann Ney."
            ],
            "title": "RWTH asr systems for LibriSpeech: Hybrid vs Attention",
            "venue": "Interspeech 2019, pages 231\u2013235.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen."
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Librispeech: An ASR corpus based on public domain audio books",
            "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206\u20135210.",
            "year": 2015
        },
        {
            "authors": [
                "D.S. Park",
                "Y. Zhang",
                "C. Chiu",
                "Y. Chen",
                "B. Li",
                "W. Chan",
                "Q.V. Le",
                "Y. Wu."
            ],
            "title": "SpecAugment on large scale datasets",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2020), pages 6879\u20136883.",
            "year": 2020
        },
        {
            "authors": [
                "Daniel S. Park",
                "Yu Zhang",
                "Ye Jia",
                "Wei Han",
                "ChungCheng Chiu",
                "Bo Li",
                "Yonghui Wu",
                "Quoc V. Le."
            ],
            "title": "Improved noisy student training for automatic speech recognition",
            "venue": "Interspeech 2020, pages 2817\u2013 2821.",
            "year": 2020
        },
        {
            "authors": [
                "Vijayaditya Peddinti",
                "Yiming Wang",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Low latency acoustic modeling using temporal convolution and LSTMs",
            "venue": "IEEE Signal Processing Letters, 25(3):373\u2013377.",
            "year": 2018
        },
        {
            "authors": [
                "Baolin Peng",
                "Chenguang Zhu",
                "Chunyuan Li",
                "Xiujun Li",
                "Jinchao Li",
                "Michael Zeng",
                "Jianfeng Gao."
            ],
            "title": "Few-shot natural language generation for taskoriented dialog",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Vadim Popov",
                "Ivan Vovk",
                "Vladimir Gogoryan",
                "Tasnima Sadekova",
                "Mikhail Kudinov."
            ],
            "title": "Grad-TTS: A diffusion probabilistic model for text-to-speech",
            "venue": "International Conference on Machine Learning (ICML), pages 8599\u20138608.",
            "year": 2021
        },
        {
            "authors": [
                "Yao Qian",
                "Ximo Bianv",
                "Yu Shi",
                "Naoyuki Kanda",
                "Leo Shen",
                "Zhen Xiao",
                "Michael Zeng."
            ],
            "title": "Speechlanguage pre-training for end-to-end spoken language understanding",
            "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "arXiv preprint arXiv:1909.05855.",
            "year": 2019
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Schemaguided dialogue state tracking task at dstc8",
            "venue": "arXiv preprint arXiv:2002.01359.",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox."
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241.",
            "year": 2015
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language",
            "year": 2022
        },
        {
            "authors": [
                "Hiroaki Sato",
                "Tomoyasu Komori",
                "Takeshi Mishima",
                "Yoshihiko Kawai",
                "Takahiro Mochizuki",
                "Shoei Sato",
                "Tetsuji Ogawa."
            ],
            "title": "Text-only domain adaptation based on intermediate ctc",
            "venue": "Proceedings of the Annual Conference of the International Speech",
            "year": 2022
        },
        {
            "authors": [
                "Bidisha Sharma",
                "Maulik Madhavi",
                "Haizhou Li."
            ],
            "title": "Leveraging acoustic and linguistic embeddings from pretrained speech and language models for intent classification",
            "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Process-",
            "year": 2021
        },
        {
            "authors": [
                "Yilin Shen",
                "Yen-Chang Hsu",
                "Avik Ray",
                "Hongxia Jin."
            ],
            "title": "Enhancing the generalization for intent classification and out-of-domain detection in slu",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through",
            "year": 2020
        },
        {
            "authors": [
                "Marcin Sowa\u0144ski",
                "Artur Janicki."
            ],
            "title": "Leyzer: A dataset for multilingual virtual assistants",
            "venue": "International Conference on Text, Speech, and Dialogue, pages 477\u2013486.",
            "year": 2020
        },
        {
            "authors": [
                "Guangzhi Sun",
                "Yu Zhang",
                "Ron J Weiss",
                "Yuan Cao",
                "Heiga Zen",
                "Andrew Rosenberg",
                "Bhuvana Ramabhadran",
                "Yonghui Wu"
            ],
            "title": "Generating diverse and natural text-to-speech samples using a quantized fine-grained VAE and autoregressive prosody prior",
            "year": 2020
        },
        {
            "authors": [
                "Samuel Thomas",
                "Brian Kingsbury",
                "George Saon",
                "Hong-Kwang J Kuo."
            ],
            "title": "Integrating text inputs for training and adapting rnn transducer asr models",
            "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Thomas",
                "Hong-Kwang J Kuo",
                "George Saon",
                "Zolt\u00e1n T\u00fcske",
                "Brian Kingsbury",
                "Gakuto Kurata",
                "Zvi Kons",
                "Ron Hoory."
            ],
            "title": "Rnn transducer models for spoken language understanding",
            "venue": "ICASSP 20212021 IEEE International Conference on Acoustics,",
            "year": 2021
        },
        {
            "authors": [
                "Yusheng Tian",
                "Philip John Gorinski."
            ],
            "title": "Improving end-to-end speech-to-intent classification with Reptile",
            "venue": "Interspeech 2020, pages 891\u2013895.",
            "year": 2020
        },
        {
            "authors": [
                "Zhengkun Tian",
                "Jiangyan Yi",
                "Jianhua Tao",
                "Ye Bai",
                "Zhengqi Wen."
            ],
            "title": "Self-attention transducers for end-to-end speech recognition",
            "venue": "Interspeech 2019, pages 4395\u20134399.",
            "year": 2019
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013",
            "year": 2003
        },
        {
            "authors": [
                "Gokhan Tur",
                "Dilek Hakkani-T\u00fcr",
                "Larry Heck"
            ],
            "title": "What is left to be understood in ATIS",
            "venue": "In 2010 IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2010
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30.",
            "year": 2017
        },
        {
            "authors": [
                "Pengwei Wang",
                "Liangchen Wei",
                "Yong Cao",
                "Jinghui Xie",
                "Zaiqing Nie."
            ],
            "title": "Large-scale unsupervised pre-training for end-to-end spoken language understanding",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP",
            "year": 2020
        },
        {
            "authors": [
                "Yingzhi Wang",
                "Abdelmoumene Boumadane",
                "Abdelwahab Heba"
            ],
            "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
            "venue": "arXiv preprint arXiv:2111.02735",
            "year": 2021
        },
        {
            "authors": [
                "Ralph Weischedel",
                "Martha Palmer",
                "Mitchell Marcus",
                "Eduard Hovy",
                "Sameer Pradhan",
                "Lance Ramshaw",
                "Nianwen Xue",
                "Ann Taylor",
                "Jeff Kaufman",
                "Michelle Franchini"
            ],
            "title": "Linguistic Data Consortium",
            "venue": "Ontonotes release",
            "year": 2013
        },
        {
            "authors": [
                "Ching-Feng Yeh",
                "Jay Mahadeokar",
                "Kaustubh Kalgaonkar",
                "Yongqiang Wang",
                "Duc Le",
                "Mahaveer Jain",
                "Kjell Schubert",
                "Christian Fuegen",
                "Michael L Seltzer."
            ],
            "title": "Transformer-Transducer: End-to-end speech recognition with self-attention",
            "venue": "arXiv preprint",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoxue Zang",
                "Abhinav Rastogi",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Jianguo Zhang",
                "Jindong Chen"
            ],
            "title": "MultiWOZ 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines",
            "venue": "In Proceedings of the 2nd Workshop on Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Dong Zhang",
                "Shimin Li",
                "Xin Zhang",
                "Jun Zhan",
                "Pengyu Wang",
                "Yaqian Zhou",
                "Xipeng Qiu."
            ],
            "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
            "venue": "arXiv preprint arXiv:2305.11000.",
            "year": 2023
        },
        {
            "authors": [
                "Q. Zhang",
                "H. Lu",
                "H. Sak",
                "A. Tripathi",
                "E. McDermott",
                "S. Koo",
                "S. Kumar."
            ],
            "title": "Transformer Transducer: A streamable speech recognition model with transformer encoders and RNN-T loss",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and",
            "year": 2020
        },
        {
            "authors": [
                "Ziqiang Zhang",
                "Sanyuan Chen",
                "Long Zhou",
                "Yu Wu",
                "Shuo Ren",
                "Shujie Liu",
                "Zhuoyuan Yao",
                "Xun Gong",
                "Lirong Dai",
                "Jinyu Li"
            ],
            "title": "2022a. SpeechLM: Enhanced speech pre-training with unpaired textual data. arXiv preprint arXiv:2209.15329",
            "year": 2022
        },
        {
            "authors": [
                "Ziqiang Zhang",
                "Long Zhou",
                "Junyi Ao",
                "Shujie Liu",
                "Lirong Dai",
                "Jinyu Li",
                "Furu Wei."
            ],
            "title": "SpeechUT: Bridging speech and text with hiddenunit for encoder-decoder based speech-text pretraining",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the data-centric artificial intelligence era, large quantity and high quality training data are essential for good performance of natural language processing (NLP) models including speech processing models. A conventional speech processing system is usually cascaded with an automatic speech recognition (ASR) module and an NLP module. For example, in spoken language understanding (SLU) which predicts semantic information from speech input, the system first transcribes input speech into\n\u2217Leading co-authors with equal contribution. \u2020Work done during an internship at Huawei.\ntext with ASR, then pipes the text output to the natural language understanding (NLU) model for text analysis. An end-to-end (E2E) speech processing system leverages a single model which takes the input speech and performs spoken language processing tasks simultaneously. E2E models draw increasing attention due to less computational complexity and error propagation mitigation (Shen et al., 2021; Tian and Gorinski, 2020; Sharma et al., 2021; Lugosch et al., 2020; Wang et al., 2020; Chen et al., 2021b). However, a challenge of E2E model training is the collection of enormous annotated spoken data, which are significantly more expensive to collect compared with the text-only counterpart. In contrast, for a cascaded system, the ASR module and NLP module are trained separately with paired speech-transcription data and annotated textual data respectively. Separated types of data are usually more readily available and thus lower data collection costs. As the amount of high quality training data is critical for an E2E model, a strategy to alleviate the inadequate spoken data problem with more abundant textual data.\nTwo approaches have been proposed for utilizing textual data for E2E speech models in the literature. The first is modality conversion which utilizes a text-to-speech (TTS) system to convert text into speech (Laptev et al., 2020). The disadvantage is the requirement for a high-quality expressive TTS system. Another approach is unified representation learning for matching latent representations of speech and text with alignment losses (Bapna et al., 2021; Chen et al., 2022a). Given the significant difference between speech and text, aligning the hidden latent space of the two modalities is challenging.\nWe propose Latent Synthesis (LaSyn), a method to utilize text-only data for E2E speech processing models. LaSyn can be seen as an integration of the above two ideas. We train a latent synthesis model which synthesizes textual data into an intermediate\nlatent representation of a pre-trained speech model. Compared to modality conversion, speech latent representation contains fewer details and redundancy than the original speech signal, thus is easier to synthesize. Compared to unified representation learning, instead of aligning two modalities of huge difference, LaSyn learns to map the text into the latent representation of speech directly.\nWe evaluate LaSyn on low-resource ASR and SLU tasks. Low-resource ASR has gained big progress with the advancement of self-supervised speech pre-training (Baevski et al., 2020; Hsu et al., 2021; Huang et al., 2022). Further performance improvement still relies on external language models (Baevski et al., 2020). We show that LaSyn allows an E2E ASR model to utilize text-only data effectively without external language models, and outperforms ASR models with external language models. We further evaluate LaSyn on two publicly available datasets for SLU tasks, namely SLURP (Bastianelli et al., 2020) and Spoken Task Oriented Semantic Parsing (STOP) (Tomasello et al., 2022). LaSyn achieves comparable performance to the state-of-the-art (SOTA) SLU models but with significantly fewer model parameters. We summarize our contributions as follows:\n\u2022 We propose LaSyn, an efficient textual data utilization framework for E2E speech processing models. The framework enables cross-modal knowledge transfer from text to E2E speech processing models through latent synthesis.\n\u2022 We design 2 implementations for latent synthesizer which is the core of LaSyn framework: a fixed-projection latent synthesizer, and a diffusion latent synthesizer which applies recent progress of generative model, diffusion probabilistic model (Ho et al., 2020; Song et al., 2020).\n\u2022 By improving an E2E ASR model through textual data utilization with LaSyn, we achieve competitive results on a low-resource ASR setup than published supervised ASR models which utilize textual data through an external language model.\n\u2022 With LaSyn, we demonstrate E2E SLU models can be improved with a diverse set of textual NLP tasks, including NLU, information extraction (IE), named entity recognition (NER), and masked language modeling (MLM). We achieve competitive results to published SOTA works on two publicly available SLU datasets, with significantly fewer model parameters.\nThis paper is organized as follows. In the next section, we discuss related works of LaSyn. In Section 3, we discuss the model structure and training of LaSyn. We present experimental setup and results in Section 4, and ablation studies on SLU tasks in Section 5. Finally, we conclude our work in Section 6."
        },
        {
            "heading": "2 Related Works",
            "text": "In this section, we discuss the prior works of modality conversion and unified representation learning related to LaSyn.\nModality conversion: Laptev et al. (2020) shows that TTS data augmentation improves ASR performance in a low-resource setting. Sun et al. (2020) further shows that the diversity and quality of the TTS system are important for ASR data augmentation. Chen et al. (2022b) demonstrates similar representations derived from synthesized speech help downstream ASR tasks. Lugosch et al. (2020) confirms the effectiveness of speech synthesis for E2E SLU models, either as a sole source of training data or as a form of data augmentation. Thomas et al. (2021) utilizes artificially synthesized speech to adapt a SLU model based on a recurrent neural network transducer. Huang et al. (2020b) demonstrates the effectiveness of a multispeaker TTS system under a low-resource SLU setting. Kharitonov et al. (2023) decouples the textto-semantic and semantic-to-acoustic tasks to realize a multi-speaker text-to-speech system. LaSyn generates pseudo acoustic representations from text without requiring a vocoder for speech waveform generation.\nUnified representation learning: Ao et al. (2021) extends the idea of T5 (Raffel et al., 2020) and proposes Speech-T5 with a cross-modal vector quantization in a shared discrete latent space. Kim et al. (2021) learns multi-modal alignment with two cross-modal pre-training tasks of masked language modeling and conditioned language modeling. Qian et al. (2021) unifies a pre-trained ASR encoder for speech and a pre-trained language model encoder for text into a transformer decoder. Sato et al. (2022) introduces an adaptation branch to embed acoustic and linguistic information in the same latent space. Thomas et al. (2022) trains an RNN-T model both on speech and text inputs. Zhang et al. (2022a) introduces two alternative discrete phoneme-unit and hidden-unit tokenizers to\nbridge speech and text modalities. MAESTRO (Chen et al., 2022a) learns unified representations of text and speech through sequence matching and duration prediction. Chung et al. (2018) attempts to align the individually learned text and speech embedding via adversarial training and a refinement procedure. SpeechUT (Zhang et al., 2022b) leverages hidden units as the bridge between the speech encoder and the text decoder. SpeechGPT(Zhang et al., 2023) applies modality-adaptation pertaining and cross-modal instruction fine-tuning to perceive and generate multi-model content. LaSyn connects text and speech information by mapping text representation directly into the pseudo acoustic latent space of a pre-trained speech model."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Architecture",
            "text": "The LaSyn framework is illustrated in Fig. 1. The framework has 3 components: a speech latent encoder which maps speech data to corresponding speech latent representation, a latent synthesizer that projects text into the speech latent space, and a backbone model which is trained with either speech latent representations or pseudo acoustic latent representations from text."
        },
        {
            "heading": "3.2 Training procedure",
            "text": ""
        },
        {
            "heading": "3.2.1 Speech Latent Encoder",
            "text": "Speech latent encoder is obtained from a pretrained speech processing model, which is a supervised ASR model as illustrated in Fig. 2 in this work. The parameters of speech latent encoder are frozen in the latter training stages to fix the speech latent space."
        },
        {
            "heading": "3.2.2 Latent Synthesizer",
            "text": "We then train a latent synthesizer to project textual data into the same speech latent space of the speech latent encoder. Latent synthesizer allows utilizing training samples from textual data, which is the core of the LaSyn framework. We explore two implementations of the latent synthesizer.\nFixed-projection Latent Synthesizer: We train a fixed-projection latent synthesizer with the help of a guiding net. The guiding net is also obtained from the pre-trained ASR model as illustrated in Fig. 2. Note that the guiding net is frozen in this stage. The training procedure is illustrated in Fig. 3. We optimize a fixed-projection latent synthesizer to generate latent representations which are recognizable as input of the guiding net. As the name suggests, the fixed-projection latent synthesizer learns a fixed one-to-one projection between\ntext data and speech latent representation. The training objective is defined as follows,\nargmin \u03d5\nLASR ( G\u03b8 ( P\u03d5 ( G2P(t) )) , t ) (1)\nwhere G\u03b8 and P\u03d5 represent the guiding network and the fixed-projection latent synthesizer respectively, \u03d5 represents the parameters of the latent synthesizer, t is the text input, and G2P is a graphemeto-phoneme module. LASR is the same loss function of the pre-trained ASR model, such as transducer loss (Graves, 2012) or cross-entropy loss for attention-based encoder-decoder (AED) (Vaswani et al., 2017).\nDiffusion Latent Synthesizer: We also experiment with diffusion probabilistic models (DPM) (Ho et al., 2020) as the latent synthesizer. DPMs have achieved great success in TTS (Popov et al., 2021; Chen et al., 2021a) and text-conditioned image synthesis (Nichol et al., 2021; Saharia et al., 2022) recently. We use the formulation of DPM proposed in Karras et al. (2022). Diffusion latent synthesizer generates latent representations by sampling an initial latent representation from a noise distribution and iteratively denoising the sample using a denoising model D(hnoisy; e, \u03c3) where hnoisy represents the noisy latent at the current step, e denotes the conditional text. The denoising model is composed of an UNet (Ronneberger et al., 2015) and a text encoder as shown in Fig. 4. To reduce the complexity of the diffusion model, we train an autoencoder to compress the latent representation and use the lower-dimensional latent representation as the target of the diffusion latent synthesizer, similar to Rombach et al. (2022). For succinctness, we do not depict the training of autoencoder in Fig. 4. The training objective is to minimize,\nEp(h,e),p(\u03f5),p(\u03c3) [ \u03bb(\u03c3) \u2225\u2225D(h+ \u03c3\u03f5; e, \u03c3)\u2212 h\u2225\u22252 2 ] (2)\nwhere h is clean latent representation, p(h, e) represents the training data distribution of latent-text pairs. The latent-text pairs are derived from a paired speech-text dataset and a speech latent encoder which converts the speeches into latent representations. p(\u03c3) is the distribution of noise levels that defines the corruption schedule (Karras et al., 2022). p(\u03f5) \u2208 N (0,1) is the standard normal\ndistribution, \u03bb(\u03c3) is the weighting factor of noise levels. We employ classifier-free diffusion guidance (Ho and Salimans, 2022) to control latent quality and text alignment when sampling from the diffusion latent synthesizer."
        },
        {
            "heading": "3.2.3 Backbone Model and Dual-modality Training",
            "text": "After we train the latent synthesizer, we train the backbone model. We freeze the speech latent encoder and the latent synthesizer during backbone model training. We utilize both text and speech data in training. The backbone model takes input latent features from either speech latent encoder or latent synthesizer. We formulate both text-to-text and speech-to-text tasks as a unified sequence-tosequence problem and refer to as dual-modality training. The training loss is specific to each task, i.e., transducer loss for ASR, and cross-entropy loss for SLU. The amount of textual data is usually significantly larger than speech data. We first train the backbone model with textual data. Then we train the backbone model with both text and speech data."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Training Data",
            "text": ""
        },
        {
            "heading": "4.1.1 ASR",
            "text": "We apply a 100-hour subset (train-clean-100) of LibriSpeech (Panayotov et al., 2015) as lowresource labeled speech data. We use the transcription of the whole 960-hour LibriSpeech training split (LS-960) as text-only data."
        },
        {
            "heading": "4.1.2 SLU",
            "text": "We evaluate LaSyn on two challenging SLU datasets, SLURP (Bastianelli et al., 2020) and\nSTOP (Tomasello et al., 2022). SLURP is substantially larger and linguistically more diverse than previous SLU datasets. STOP is a recently released dataset that is the largest and the most complex SLU dataset. We also leverage a diverse set of NLP text datasets from different tasks, including natural language understanding (NLU), named entity recognition (NER), and information extraction (IE). The extra NLP text datasets are listed in Table 1."
        },
        {
            "heading": "4.2 Model and Training Setups",
            "text": ""
        },
        {
            "heading": "4.2.1 ASR",
            "text": "For ASR pre-training, we use a Transformer Transducer model (Tian et al., 2019; Yeh et al., 2019; Zhang et al., 2020). We apply a 128-dimensional log-mel filterbank with 20 ms window length and 10 ms frame rate as input acoustic feature. We interleave strided-convolutions in the encoder to gradually down-sample the input speech as illustrated in Fig. 5, which reduces computation effectively with negligible performance degradation (Peddinti et al., 2018; Han et al., 2020; Huang et al., 2020a). This model is pre-trained with train-clean100. SpecAugment (Park et al., 2020) is applied to avoid overfitting. This pre-trained model is also our E2E ASR baseline. We obtain a speech latent encoder from this pre-trained model.\nFor latent synthesizers, we evaluate both fixedprojection latent synthesizer and diffusion latent synthesizer. The fixed-projection latent synthesizer is composed of 4 1-D convolutional layers of 512 filters with a kernel size of 5. We observe that a simple model structure is sufficient. We train the diffusion latent synthesizer with train-clean-100. The text encoder is composed of two convolution layers followed by the two-layer transformer. The number of channels is 256. The UNet model is adapted for 1-D sequence processing. The hyper-parameters of the UNet model are listed in Table 2. We use a small model such that the latent synthesizer generates the pseudo acoustic latent representations on the fly during dual-modality training.\nThe backbone model is the same as the guiding net in Fig. 2. To utilize textual data in dualmodality training of the backbone model, we design a task similar to masked language modeling\n(MLM) (Devlin et al., 2018) as illustrated in Fig. 6. We randomly mask 30% of input phonemes converted by g2pE1 according to CMUDict2, and train the backbone model to predict the corresponding words.\nWe note that the parameters of the guiding net are frozen in latent synthesizer training. If we do not provide textual data for backbone model training, we just update the E2E baseline with extra epochs with a frozen speech latent encoder."
        },
        {
            "heading": "4.2.2 SLU",
            "text": "We apply an attention-based encoder-decoder model for ASR pre-training. The pre-trained ASR model is trained with LS-960 and SLURP speech data. The structure of the encoder is similar to the one in ASR experiments described in section 4.1.2. We apply a 6-layer, 256-dimensional Transformer as the decoder. We evaluate the two implementations of the latent synthesizer similar to ASR experiments. For fixed-projection latent synthesizer, the configuration is the same as ASR experiments. We apply text transcription of LS-960 for training. For diffusion latent synthesizer, we use LS-960 as paired speech-text training data. The backbone model shares the same model structure as the guiding net in Fig. 2. We also initialize the parameters from the guiding net. We train the backbone model with multiple tasks, including SLU, NLU, NER, and IE. We convert the annotation of all the datasets to a text-sequence format as illustrated in Fig. 7. We formulate all the tasks as a unified sequence-to-sequence problem.\nWe note that the model structure of the E2E baseline model is the same as the LaSyn model, but the latent synthesizer is disabled. The E2E baseline model does not train with any additional textual data. We fine-tune the E2E baseline model with SLU task after ASR pre-training."
        },
        {
            "heading": "4.3 ASR Results",
            "text": "The experimental results of ASR are shown in Table 3. We first compare LaSyn models with our E2E baseline which achieves comparable performance to conformer-based models. The only difference is that the LaSyn models are trained with additional textual data. The LaSyn-Diffusion model, which uses a diffusion latent synthesizer, achieves 40.5% and 22.3% relative WER reductions on testclean and test-other of Librispeech test sets com-\n1https://github.com/Kyubyong/g2p 2https://github.com/cmusphinx/cmudict\npared to the E2E baseline. We notice that the improvement on test-clean is more significant than test-other. Both the fixed-projection latent synthesizer and the diffusion latent synthesizer are trained with train-clean-100 which contains only clean speech. We speculate that the limited variety of training data train-clean-100 biases ASR performance toward clean speech.\nWe also observe that the performance of the model with fixed-projection latent synthesizer (LaSyn-FixedProj) is only slightly worse than LaSyn-Diffusion. The result is surprising, as the fixed-projection latent synthesizer is simpler than the diffusion latent synthesizer. The diffusion latent synthesizer may need further hyper-parameter tuning, or may need more training data for better performance. The LaSyn-FixedProj-LFR model utilizes a low frame rate speech latent encoder as illustrated in Fig. 5. The performance is slightly worse than the LaSyn-FixedProj on test-other.\nCompared to published supervised ASR mod-\nels that utilize text data through external language models, LaSyn models perform better without an external language model (LM). Compared to the published methods using TTS for data augmentation, the performance of LaSyn models are significantly better without an external LM. Given the existence of real-world scenarios with limited labeled speech data, such as minority languages and specific domains, our proposed method offers a novel approach to developing ASR applications."
        },
        {
            "heading": "4.4 SLU Results",
            "text": ""
        },
        {
            "heading": "4.4.1 SLURP",
            "text": "The experimental results of SLURP are shown in Table 4. We report accuracy for intent classification (IC), and SLU-F1(Bastianelli et al., 2020) for slot filling (SF).\nWe first compare LaSyn models with our E2E baseline. Compared to the E2E baseline, LaSynFixedProj improves IC accuracy and SF SLU-F1 by absolute 4.1% and 3.8% respectively. The result suggests that knowledge of textual NLP data is effectively transferred to SLU model. LaSynDiffusion performs slightly worse than LaSynFixedProj. We believe that with further hyperparameter tuning and more training data, the performance of diffusion latent synthesizer should be further improved.\nWe further compare the LaSyn models with previously published E2E SLU results. The published models are fine-tuned from HuBERT (Hsu et al., 2021) Base (95 M parameters) or Large (300 M parameters). The performance of LaSyn-FixedProj is comparable to ESPnet-SLU (Arora et al., 2022) and PF-hbt-base (Wang et al., 2021). The IC accuracy of LaSyn-FixedProj is slightly worse than EF-hbt-large (Wang et al., 2021), but the number of parameters is 8 times fewer.\nTo understand how LaSyn improves our baseline E2E SLU model, we further analyze samples from\nthe test set that LaSyn performs better than our baseline. An example is shown in Fig. 8. Our E2E baseline model fails for the slot \"Oldies Station\", as this phrase never occurs in the SLURP training set. In contrast, LaSyn model correctly predicts the slot value. This phrase is included in the textual corpora. The text knowledge is transferred to SLU model with the LaSyn framework. The baseline E2E SLU model does not get the proprietary term \u2018Oldies Station\u2019 while LaSyn predicts this unique vocabulary successfully."
        },
        {
            "heading": "4.4.2 STOP",
            "text": "We present our results of STOP in Table 5. Compared to our E2E baseline, LaSyn-FixedProj improves EM accuracy and EM-Tree accuracy on the test set by absolute 4.49% and 2.25% respectively, again suggesting that there is effective crossmodality text knowledge transfer.\nWe further compare our results with STOPE2E and STOP-Cascaded (Tomasello et al., 2022). STOP-E2E is an encoder-decoder based Transformer model fine-tuned from an E2E ASR model. The E2E ASR model is fine-tuned from HuBERT Base (Hsu et al., 2021). STOP-Cascaded is a cascaded system composed of an ASR system finetuned from wav2vec2.0 Base (Baevski et al., 2020) and an NLU model fine-tuned from a BART Base model (Lewis et al., 2019). LaSyn-FixedProj performs slightly better than STOP-E2E with 0.25% and 2.63% absolute improvement of EM and EMTree accuracies on the test set respectively. However, compared to STOP-Cascaded on the test set, while LaSyn-FixedProj is competitive on EM-Tree accuracy, EM accuracy is slightly inferior. The number of parameters in LaSyn models is much fewer. We expect performance improvement with more model parameters."
        },
        {
            "heading": "5 Ablation Study",
            "text": ""
        },
        {
            "heading": "5.1 Training with Unlabeled Textual Data",
            "text": "Plain text data without annotation are more abundant than annotated NLP data. We experiment with SLU training with unlabelled textual data. We prepare the unlabelled text data by striping the annotation labels of the NLP datasets and keeping the input text. We apply the MLM task described in section 4.1.2 to utilize the unlabeled textual data. We evaluate LaSyn models with fixed-projection latent synthesizer. The results are listed in Table 6.\nThe results show that LaSyn still benefits from unlabeled text, compared to our E2E baseline on both SLURP and STOP datasets. With unlabeled text and MLM tasks, LaSyn achieves an absolute improvement of 1.6 % and 0.9 % on IC and SF tasks on SLURP dataset, 2.19 %, and 0.46% on EM and EM-Tree on STOP test set. While the improvement is not as significant as using labeled textual data, data collection is further simplified with unlabelled textual data."
        },
        {
            "heading": "5.2 Training with Diverse NLP Tasks",
            "text": "We do an ablation to observe the effect of training LaSyn with textual data from a diverse set of NLP tasks. The results are shown in Table 7. We observe that including each NLP task brings substantial improvement over the E2E baseline. As the NLU task is the most relevant to SLU, performance improvement is the most significant. When we combine all the NLP tasks, there is marginal further performance improvement.\nModel Text Training data STOP (dev) STOP (test)(EM / EM-Tree) (EM / EM-Tree)"
        },
        {
            "heading": "5.3 Latent Synthesizer as Acoustic Augmentation",
            "text": "We experiment with using the fixed-projection latent synthesizer for acoustic augmentation. We extract the transcription and the annotation from the SLU dataset to form an NLU dataset. When training the backbone model, we apply both the SLU and the NLU datasets in dual-modality training. As the NLU dataset is derived from the SLU dataset, the latent synthesizer does not introduce extra textual content. Pseudo speech latent representations from the latent synthesizer are considered as an augmentation of the original speech latent representation.\nAs shown in Table 8, SLU performance improves significantly over the E2E baseline but does not reach the level of Table 7 which utilizes extra NLP datasets. Further enriching the diversity of pseudo acoustic latent is the potential to improve SLU performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present LaSyn, a framework which enables efficient textual data utilization for E2E speech processing. By converting text into pseudo acoustic latent representation with a latent synthesizer, crossmodality knowledge transfer from textual data to E2E speech processing models is achieved. For the low-resource ASR task with Librispeech, LaSyn achieves relative WER reduction from 22.3% to 40.5% on test sets, compared to our E2E baseline with the same model structure. The results are competitive to published works which utilize textual data through external language models. For SLU\ntasks, LaSyn improves over our E2E baseline by absolute 4.1% and 3.8% for for IC accuracy and SF SLU-F1 on SLURP, and absolute 4.49% and 2.25% of EM and EM-Tree accuracies on STOP. The results are competitive to published SOTA works with much fewer model parameters. Future improvement of latent synthesizer should further bridge the gap between speech and textual modality, which we leave as next step.\nLimitations\nThe core of our method is the generation of pseudo acoustic representation from text input. We focus on generating consistent latent sequences effectively. We only evaluate two latent synthesis methods, including fixed-projection and diffusion latent synthesizers. There are other probable methods for latent generation, such as generative adversarial networks (GAN) (Goodfellow et al., 2020). Compared with TTS which generates audible speech suitable for human judgment, there is no subjective method to evaluate the quality and intelligibility of generated pseudo acoustic representation from the proposed framework, which is a main limitation. The design of reasonable quality indicators of acoustic representation would be meaningful for future work. Moreover, we have not evaluated the proposed latent synthesis framework on other phonological systems such as tonal languages like Chinese. The effectiveness of the framework on tonal languages is not guaranteed.\nEthics Statement\nIn this paper, we only use publicly available datasets for experiments. Our experiments do not involve any subjective tests or human data annotations. In the experiments, the latent synthesis framework does not produce any audible speech content. We do not apply any specific speaker information during training and inference."
        }
    ],
    "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
    "year": 2023
}