{
    "abstractText": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al. (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, TREE OF CLARIFICATIONS (TOC): It recursively constructs a tree of disambiguations for the AQ\u2014via few-shot prompting leveraging external knowledge\u2014and uses it to generate a long-form answer. TOC outperforms existing baselines on ASQA in a fewshot setup across all metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at github.com/gankim/tree-of-clarifications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gangwoo Kim"
        },
        {
            "affiliations": [],
            "name": "Sungdong Kim"
        },
        {
            "affiliations": [],
            "name": "Byeongguk Jeon"
        },
        {
            "affiliations": [],
            "name": "Joonsuk Park"
        },
        {
            "affiliations": [],
            "name": "Jaewoo Kang"
        }
    ],
    "id": "SP:dda0c3d4d01dcadaadddc7ef670ea4ce2d66df62",
    "references": [
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Kellie Webster",
                "Michael Collins",
                "Dipanjan Das",
                "Shashi Narayan."
            ],
            "title": "Query refinement prompts for closed-book long-form question answering",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading wikipedia to answer opendomain questions",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879.",
            "year": 2017
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy R Cole",
                "Michael JQ Zhang",
                "Daniel Gillick",
                "Julian Martin Eisenschlos",
                "Bhuwan Dhingra",
                "Jacob Eisenstein."
            ],
            "title": "Selectively answering ambiguous questions",
            "venue": "arXiv preprint arXiv:2305.14613.",
            "year": 2023
        },
        {
            "authors": [
                "Yifan Gao",
                "Henghui Zhu",
                "Patrick Ng",
                "Cicero dos Santos",
                "Zhiguo Wang",
                "Feng Nan",
                "Dejiao Zhang",
                "Ramesh Nallapati",
                "Andrew O Arnold",
                "Bing Xiang."
            ],
            "title": "Answering ambiguous questions through generative evidence fusion and round-trip prediction",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Siddhant Garg",
                "Thuy Vu",
                "Alessandro Moschitti."
            ],
            "title": "Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7780\u20137788.",
            "year": 2020
        },
        {
            "authors": [
                "Meiqi Guo",
                "Mingda Zhang",
                "Siva Reddy",
                "Malihe Alikhani."
            ],
            "title": "Abg-coqa: Clarifying ambiguity in conversational question answering",
            "venue": "3rd Conference on Automated Knowledge Base Construction.",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "\u00c9douard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2019
        },
        {
            "authors": [
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open",
            "year": 2020
        },
        {
            "authors": [
                "ton Lee"
            ],
            "title": "Natural questions: A benchmark",
            "year": 2019
        },
        {
            "authors": [
                "t\u00e4schel"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Sewon Min",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova",
                "Hannaneh Hajishirzi."
            ],
            "title": "Joint passage ranking for diverse multi-answer retrieval",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Ambigqa: Answering ambiguous open-domain questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for squad",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789.",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Zhihong Shao",
                "Minlie Huang."
            ],
            "title": "Answering open-domain multi-answer questions via a recallthen-verify framework",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1825\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Ivan Stelmakh",
                "Yi Luan",
                "Bhuwan Dhingra",
                "MingWei Chang."
            ],
            "title": "ASQA: Factoid questions meet long-form answers",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8273\u20138288, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou"
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint arXiv:2305.10601.",
            "year": 2023
        },
        {
            "authors": [
                "2020). A"
            ],
            "title": "Implementation Details A large portion of our implementation is based on the DSP library (Khattab et al., 2022). To dynamically find few-shot examples with the nearest neigh",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In open-domain question answering (ODQA), users often ask ambiguous questions (AQs), which can be interpreted in multiple ways. To handle AQs, several approaches have been proposed, such as providing individual answers to disambiguated questions (DQs) for all plausible interpretations of the given AQ (Min et al., 2020) or asking a clarification question (Guo et al., 2021). Among them, we adopt that of Stelmakh et al. (2022), which provides a comprehensive response without bothering the user for clarification: The task is to identify all DQs of the given AQ and generate a long-form answer addressing all the DQs (See Figure 1).\nThere are two main challenges to this task: (1) the AQ may need to be clarified by considering mul-\n\u2020 Corresponding author\ntiple dimensions of ambiguity. For example, the AQ \u201cwhat country has the most medals in Olympic history\u201d in Figure 1 can be clarified with respect to the type of medals\u2014gold, silver, or bronze\u2014or Olympics\u2014summer or winter; and (2) substantial knowledge is required to identify DQs and respective answers. For example, it requires knowledge to be aware of the existence of different types of medals and the exact counts for each country.\nTo address the challenges and provide a longform answer to AQ, we propose a novel framework, TREE OF CLARIFICATIONS (TOC): It recursively constructs a tree of DQs for the AQ\u2014via few-shot\nprompting leveraging external knowledge\u2014and uses it to generate a long-form answer. More specifically, first, relevant passages for the AQ are retrieved. Then, leveraging the passages, DQs for the AQ are recursively generated via few-shot prompting and pruned as necessary. Lastly, a long-form answer addressing all DQs is generated. The tree structure promotes exploring DQs in targeting particular dimensions of clarification, addressing the first challenge, and the external sources offer additional knowledge to cope with the second challenge.\nExperiments demonstrate that our proposed use of LLMs with retrieval-augmentation and guidance to pursue diverse paths of clarification results in the new state-of-the-art on ASQA (Stelmakh et al., 2022)\u2014a long-form QA benchmark for AQs. TOC outperforms existing baselines on ASQA in a few-shot setup across all metrics. In addition, this 5-shot performance surpasses that of the fullysupervised baselines trained on the whole training set by 7.3 and 2.9 in terms of Disambig-F1 and Disambig-ROUGE, respectively.\nThe main contribution of this work is proposing a novel framework, TREE OF CLARIFICATIONS (TOC), for generating long-form answers to AQs in ODQA, advancing the state-of-the-art on the ASQA benchmark. TOC introduces two main innovations:\n\u2022 It guides LLMs to explore diverse paths of clarification of the given AQ in a tree structure with the ability to prune unhelpful DQs.\n\u2022 To the best of our knowledge, it is the first to combine retrieval systems with LLM for generating long-form answers to AQs."
        },
        {
            "heading": "2 Related Work",
            "text": "A line of studies (Min et al., 2020, 2021; Gao et al., 2021; Shao and Huang, 2022) extends retrieve-andread frameworks dominant in ODQA task (Chen et al., 2017; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) to clarify AQ and generate DQs with corresponding answers to them. However, their approaches require fine-tuning models on the large-scale train set. On the other hand, our framework enables LLM to generate a comprehensive response addressing all DQs via few-shot prompting.\nRecent studies introduce LLM-based methods to generate a long-form answer to the AQ. Amplayo\net al. (2023) suggest optimal prompts specifically engineered for the task. Kuhn et al. (2022) prompt LLMs to clarify ambiguous questions selectively. However, the studies do not utilize external information to ensure the factual correctness of the disambiguations, thereby potentially increasing the risk of hallucinations from LLMs. Moreover, the results could be bounded by inherent parametric knowledge of LLM. Concurrently, Lee et al. (2023) automatically generate clarifying questions to resolve ambiguity.\nOur framework involves the recursive tree architecture, inspired by several prior studies. Min et al. (2021) propose the tree-decoding algorithm to autoregressively rerank passages in ambiguous QA. Gao et al. (2021) iteratively explore additional interpretations and verify them in a round-trip manner. Concurrently, extending chain of thoughts (Wei et al., 2022) prompting, Yao et al. (2023) apply the tree architecture to reasoning tasks for deductive or mathematical problems. On the contrary, TOC recursively clarifies questions and introduces a self-verification method to prune unhelpful DQs."
        },
        {
            "heading": "3 Tree of Clarifications",
            "text": "We introduce a novel framework, TREE OF CLARIFICATIONS (TOC), as illustrated in Figure 1. We first devise retrieval-augmented clarification (RAC; Sec. 3.1), a basic component that clarifies AQ and generates DQs based on relevant passages. TOC explores various fine-grained interpretations, represented as a tree structure (TS; Sec. 3.2) by recursively performing RAC and pruning unhelpful DQs. Lastly, it aggregates the tree and generates a long-form answer addressing all valid interpretations."
        },
        {
            "heading": "3.1 Retrieval-Augmented Clarification (RAC)",
            "text": "We first retrieve relevant Wikipedia documents for the AQ by using two retrieval systems, ColBERT (Khattab and Zaharia, 2020) and Bing search engine1. ColBERT is a recent dense retriever that has effective and efficient zero-shot search quality. Following Khattab et al. (2022), we use the off-the-shelf model pre-trained on MS-Marco (Bajaj et al., 2016). We additionally include the Bing search engine to promote the diversity of retrieved Wikipedia passages. Finally, we obtain over 200 passages by combining passages retrieved by each system.\n1https://www.microsoft.com/bing\nAfter collecting a passage set for the AQ, we rerank and choose top-k passages and augment them to a prompt. We use SentenceBERT (Reimers and Gurevych, 2019) pre-trained on MS-Marco as the reranker backbone. For in-context learning setup, we dynamically choose k-shot examples with the nearest neighbor search2 and add them to the prompt. We initiate with the instruction of Amplayo et al. (2023) and revise it for our setup. Given the prompt with relevant passages and AQs, LLM generates all possible DQs and their corresponding answers3."
        },
        {
            "heading": "3.2 Tree Structure (TS)",
            "text": "To effectively explore the diverse dimensions of ambiguity, we introduce a recursive tree structure of clarifications. Starting from the root node with AQ, it progressively inserts child nodes by recursively performing RAC, each of which contains a disambiguated question-answer pair. In each expansion step, passages are reranked again regarding the current query. It allows each step to focus on its own DQ, encouraging TOC to comprehend a wider range of knowledge. Exploration of a tree ends when it satisfies termination conditions; it reaches the maximum number of valid nodes or the maximum depth. We choose the breadth-first search (BFS) by default, hence the resulting tree could cover the broader interpretations4.\nPruning with Self-Verification To remove unhelpful nodes, we design a pruning method, inspired by current studies for self-verification (Kadavath et al., 2022; Cole et al., 2023). Specifically, we check the factual coherency between the answers in a target node and the AQ in the root node. By doing so, we discard the generated DQs that ask different or irrelevant facts from the original one. For example, given an AQ \u201cWho will host the next world cup 2022?\u201d, a generated disambiguation \u201cDQ: Who hosted the world cup 2018? A: Russia\u201d is a factually consistent question-answer pair but it changes the original scope of the AQ5. We perform self-verification by prompting LLMs to determine whether the current node would be pruned or not. Prompted with AQ, the answer to the target DQ, and the answer-containing passage, LLM identifies\n2See Appendix A.3 for detailed implementation 3See Appendix C.2 for example prompts 4It is suboptimal to adopt the depth-first search since it would encounter unambiguous questions more frequently. See Appendix 7 for failure cases.\n5See Appendix C.3 for more detailed case studies\nif the given answer could be a correct answer to AQ.\nAnswer Generation Once constructing the tree of clarifications, TOC aggregates all valid nodes and generates a comprehensive long-form answer to AQ. It selects the disambiguations in retained nodes of the resulting tree with the relevant passages. If the number of nodes is insufficient, we undo the pruning steps from closer nodes to the root node in BFS order. Passages that contain the answers of valid nodes are prioritized. It finally generates a long-form answer, encoding AQ, selected disambiguations, and relevant passages6."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets All baselines and our framework are evaluated on ASQA (Stelmakh et al., 2022). It is a long-form QA dataset built upon the 6K ambiguous questions identified from AmbigNQ (Min et al., 2020). More details are in Appendix A.1\nEvaluation Metrics We use three evaluation metrics, following Stelmakh et al. (2022). (1) Disambig-F1 (D-F1) measures the factual correctness of generated predictions. It extracts short answers to each DQ and computes their F1 accuracy. (2) ROUGE-L (R-L) measures the lexical overlap between long-form answers from references and predictions. (3) DR score is the geometric mean of\n6See Appendix C.4 for an example prompt\ntwo scores, which assesses the overall performance. For validating intermediate nodes, we additionally use Answer-F1 that measures the accuracy of generated short answers in disambiguation. Further details are in Appendix A.2.\nBaselines Stelmakh et al. (2022) propose finetuned baselines. They fine-tune T5-large (Raffel et al., 2020) to generate long-form answers on the whole train set. Models are evaluated in the closed-book setup or combined with JPR (Min et al., 2021), task-specific dense retriever for ambiguous QA by enhancing DPR (Karpukhin et al., 2020). On the other hand, Amplayo et al. (2023) propose a prompt engineering method to adapt LLMs to the ASQA benchmark. They employ PaLM (Chowdhery et al., 2022) and InstructGPT (Ouyang et al., 2022) that learn the soft prompts or adopt in-context learning with few-shot examples. They conduct experiments in the closedbook setup. Note that they share the same backbone with our models, GPT-3 with 175B parameters (text-davinci-002)."
        },
        {
            "heading": "4.2 Experimental Results",
            "text": "TOC outperforms fully-supervised and few-shot prompting baselines. Table 1 shows the long-form QA performance of baselines and TOC on the development set of ASQA. Among baselines, using the whole training set (Fully-supervised) achieves greater performances than Few-shot Prompting in all metrics. It implies that long-form QA task is challenging in the few-shot setup. In the closed-book setup, GPT-3 shows competitive performances with T5-large with JPR in D-F1 score, showing LLM\u2019s strong reasoning ability over its inherent knowledge.\nAmong our models, LLM with RAC outperforms all other baselines in D-F1 and DR scores. It indicates the importance of leveraging external knowledge in clarifying AQs. Employing the tree structure (TS) helps the model to explore diverse interpretations, improving D-F1 and DR scores by\n1.3 and 0.9. When pruning the tree with our proposed self-verification (TS w/ Pruning), the model achieves state-of-the-art performance in D-F1 and DR score, surpassing the previous few-shot baseline by 8.4 and 7.0. Notably, it outperforms the best model in a fully-supervised setup (T5-large with JPR) by 7.3 and 2.9. In the experiment, T5-Large in a closed-book setup achieves comparable performance with LLM baselines in ROUGE-L score despite its poor D-F1 scores. It reconfirms the observation from Krishna et al. (2021) that shows the limitations of the ROUGE-L metric.\nIntegrating retrieval systems largely contributes to accurate and diverse disambiguations. Table 2 displays the ablation study for measuring the contributions of each proposed component. When removing disambiguations from fewshot training examples, the ROUGE-L score is significantly degraded, which shows the importance of the intermediate step to provide the complete answer. Integrating retrieval systems (i.e., Bing search engine and ColBERT) largely improves the model performance, especially in the D-F1 score. It indicates using external knowledge is key to enhancing the factual correctness of clarification. We report intrinsic evaluation for each retrieval system in Appendix B.\nOur pruning method precisely identifies helpful disambiguations from the tree. Table 3 shows intrinsic evaluation for generated disambiguations, where all baselines are evaluated with Answer-F1 score that measures the F1 accuracy of the answer to the target DQ. Compared to the baseline, the valid nodes that pass self-verification contain more accurate disambiguations, achieving much higher Answer-F1 score (+18.4). On the other hand, solely using deduplication does not advance the accuracy, indicating the efficacy of our proposed self-verification method."
        },
        {
            "heading": "5 Discussion",
            "text": "Ambiguity Detection TOC is designed to clarify AQs without bothering users; hence does not explicitly identify whether the given question is ambiguous or not. It tries to perform clarification even if the question cannot be disambiguated anymore, often resulting in generating duplicate or irrelevant DQs7. However, we could presume a question to be unambiguous if it can no longer be disambiguated8. In TOC, when it fails to disambiguate the given question or all generated disambiguations are pruned, the question could be regarded as unambiguous.\nComputational Complexity Although TOC requires multiple LLM calls, its maximum number is less than 20 times per question. Exploration of the tree ends when it obtains the pre-defined number of valid nodes (10 in our experiments). Since the clarification process generates from two to five disambiguations for each question, it satisfies the termination condition in a few steps without the pruning method. Failing to expand three times in a row also terminates the exploration. Pruning steps consume a smaller amount of tokens since they encode a single passage without few-shot exemplars. Compared to the existing ensemble methods such as self-consistency (Wei et al., 2022) which cannot be directly adopted to the generative task, ToC achieves a state-of-the-art performance with a comparable number of LLM calls.\nGeneralizability The key idea of ToC could be potentially generalized to other tasks and model architectures. It has a model-agnostic structure that could effectively explore diverse paths of recursive reasoning, which would be helpful for tasks that require multi-step reasoning, such as multi-hop QA. Future work might investigate the generalizability of TOC to diverse tasks, datasets, and LM architectures."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we propose a novel framework, TREE OF CLARIFICATIONS. It recursively builds a tree of disambiguations for the AQ via few-shot prompting with external knowledge and utilizes it to generate a\n7See Appendix 7 for failure cases 8The idea is aligned with the annotation process of AmbigQA (Min et al., 2020), in which the target question is classified as ambiguous if multiple distinct answers to it were observed.\nlong-form answer. Our framework explores diverse dimensions of interpretations of ambiguity. Experimental results demonstrate TOC successfully guide LLMs to traverse diverse paths of clarification for a given AQ within tree structure and generate comprehensive answers. We hope this work could shed light on building robust clarification models, which can be generalized toward real-world scenarios."
        },
        {
            "heading": "Limitations",
            "text": "Although TOC is a model-agnostic framework that could be combined with other components, our study is limited in demonstrating the generalizability of different kinds or sizes of LLMs. In addition, the experiments are only conducted on a benchmark, ASQA (Stelmakh et al., 2022). Although TOC enables LLM to explore diverse reasoning paths by iteratively prompting LLM, the cost of multiple prompting is not negligible.\nWe tried the recent prompting method, chain of thoughts (Wei et al., 2022), but failed to enhance the performance in our pilot experiments. It might indicate the disambiguation process requires external knowledge, which shows the importance of document-grounded or retrieval-augmented systems. Future work could suggest other pruning methods that identify unhelpful DQs more effectively. The performance could be further enhanced by using the state-of-the-art reranker in the answer sentence selection task, as proposed by recent works (Garg et al., 2020; Lauriola and Moschitti, 2021)."
        },
        {
            "heading": "Acknowledgements",
            "text": "The first author, Gangwoo Kim, has been supported by the Hyundai Motor Chung Mong-Koo Foundation. This research was supported by the National Research Foundation of Korea (NRF2023R1A2C3004176, RS-2023-00262002), the MSIT (Ministry of Science and ICT), Korea, under the ICT Creative Consilience program (IITP-20222020-0-01819) supervised by the IITP (Institute for Information & communications Technology Planning & Evaluation), and the Electronics and Telecommunications Research Institute (RS-202300220195)."
        },
        {
            "heading": "A Experimental Setup Details",
            "text": ""
        },
        {
            "heading": "A.1 Ambiguous QA Datasets",
            "text": "All baselines and our framework are evaluated on ASQA benchmark (Stelmakh et al., 2022). It is a long-form QA dataset built on the subset of ambiguous questions identified from AmbigNQ dataset (Min et al., 2020). It contains opendomain questions collected from Natural Questions (Kwiatkowski et al., 2019). ASQA consists of 6,316 ambiguous questions and their long-form answers with disambiguations, split into 4,353, 948, and 1,015 train, development, and test set, respectively."
        },
        {
            "heading": "A.2 Evaluation Metrics",
            "text": "Following Stelmakh et al. (2022), we use three evaluation metrics on ASQA. First, ROUGE-L (R-L) measures the lexical overlap between long-form answers from references and system-generated predictions. Since the benchmark provides two groundtruth answers, we report the maximum ROUGE-L score. Disambig-F1 (D-F1) measures the factual correctness of generated predictions. A reading comprehension model, RoBERTa (Liu et al., 2019) trained on SQuADv2 (Rajpurkar et al., 2018), finds short answers to the ground-truth DQs from the generated long-form response. Then, F1 accuracy of the detected answer is calculated to check if the long-form answer contains accurate information. Disambiguation-ROUGE (DR) score is computed as the geometric mean of ROUGE-L and DisambigF1 to measure the overall performance. We additionally use Answer-F1 to validate the disambiguations. It computes the maximum F1 accuracy of answers to a single DQ. We use ground-truth disambiguations provided by AmbigNQ (Min et al., 2020).\nA.3 Implementation Details A large portion of our implementation is based on the DSP library (Khattab et al., 2022). To dynamically find few-shot examples with the nearest neighbor search, we use pre-trained MiniLM (Wang et al., 2020) to obtain hidden representations of questions and compute similarity scores with Faiss library (Johnson et al., 2019). We add 5-shot training examples to the prompt, following Amplayo et al. (2023). It was the optimal number in our pilot experiment.\nFor prompting LLM to perform RAC, we use top-5 relevant passages. To determine whether to\nprune the target node or not, we rerank and pick the most relevant passage among those containing the answer in the target node. In the answer generation process, we took ten valid disambiguations in BFS order and five answer-containing passages. We use API served by OpenAI9 to employ GPT-3 as our backbone. We set max tokens as 300 and top-p as 1.0."
        },
        {
            "heading": "B Additional Experiment",
            "text": "B.1 Intrinsic Evaluation for Retrieval Systems\nWe randomly sample 100 examples from ASQA dataset and report intrinsic evaluation results for retrieval systems. Since a single AQ has multiple ground-truth DQs and their answers, it is not trivial to check how many answers are covered by retrieved passages. Inspired by Min et al. (2021), we devise an evaluation proxy, answer coverage, for measuring the quality of retrieved passages in ambiguous QA tasks. We consider the retrieval as successful if the retrieved passages contain one of the answers to the target DQ. We calculate the proportion of success among DQs for a single AQ to check overall answer coverage.\nTable 4 compares retrieval systems in answer coverage (AC@k) of top-k passages. Bing search engine without reranker performs worst among baselines in AC@10 and @30. However, with reranker, its performances are greatly enhanced, outperforming ColBERT baselines. When combining two retrieval systems, it shows the best performances across all evaluation metrics; hence two results are complementary. It achieves 80.1 in AC@100 scores, which indicates the passage set has sufficient information if properly explored.\n9https://platform.openai.com/docs/api-reference"
        },
        {
            "heading": "C Qualitative Analysis",
            "text": ""
        },
        {
            "heading": "C.1 Prompt Format",
            "text": "We add format descriptions to our prompt following Khattab et al. (2022). Table 5 displays the format specifically designed to generate disambiguations for a given question based on external documents. The format description is augmented to prompts of both RAC and the answer generation. By using it, we encouarge LLM to comply with the format."
        },
        {
            "heading": "C.2 Question Clarification",
            "text": "Table 6 shows an example of RAC for the AQ. Retrieval systems provide the external knowledge. Leveraging it, LLM generates disambiguated question-answer pairs. In RAC, long-form answers are also generated to follow the format but we do not use them in the later steps.\nIn Table 7, we observe the cases where TOC encounters unambiguous questions and fails to clarify them. It often asks different or irrelevant facts from them of original AQ."
        },
        {
            "heading": "C.3 Self Verification",
            "text": "Table 8, 9 show examples of self-verification prompt. We prompt LLM to verify the current answer is factually coherent with AQ based on the relevant passage. It generates \u2018True\u2019 or \u2018False\u2019 to determine whether the node would be discarded or not. We do not provide few-shot training examples or formats."
        },
        {
            "heading": "C.4 Answer Generation",
            "text": "Table 10 depicts an example of answer generation prompt. We use a similar prompt to that of RAC except disambiguations are given as inputs. It encodes up to ten disambiguations and five relevant passages.\nFollow the following format.\nContext: ${sources that may contain relevant content}\nQuestion: ${ambiguous question to be disambiguated}\nDisambiguations: ${the disambiguated pairs of questions and answers, each is separated by a new line.} DQ i: ${(i)-th disambiguated question that clarifies the ambiguous question} DA i: ${short factoid answers separated by semi-colon (;) to (i)-th disambiguated question, often between 1 and 5 words}\nAnswer: ${a thorough, detailed answer that explains the multiple interpretations of the original question and includes the appropriate disambiguations, at least three sentences.}\nTable 5: Format description for both RAC and the answer generation.\nI will provide ambiguous questions that can have multiple answers based on their different possible interpretations. Clarify the given question into several disambiguated questions and provide short factoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at least three sentences. Here are some examples.\nContext: [1] Fred and George Weasley | Fred and George Weasley are fictional characters in the \"Harry Potter\" book series written by J. K. Rowling. The characters are the identical twin brothers ... The twins were played by identical twin brothers James and Oliver Phelps in the film adaptations [2] James and Oliver Phelps | James Andrew Eric Phelps and Oliver Martyn John Phelps (born 25 February 1986) are English actors and identical twin brothers. They are known for playing Fred and George Weasley in the \"Harry Potter\" film series from 2001 to 2011 ... \u00b7 \u00b7 \u00b7 [5] Chris Rankin | plays of \"Bugsy Malone\" and \"The Lion, The Witch and The Wardrobe\". His professional acting career began when he won the role of Percy Weasley in September 2000 ... after his character\u2019s absence from \"Harry Potter and the Goblet\"\nQuestion: Who played the weasley brothers in harry potter?\nDisambiguations: DQ 1: Who played the fictional characters Fred and George Weasley in the \"Harry"
        },
        {
            "heading": "Potter\" book series?",
            "text": "DA 1: James and Oliver Phelps DQ 2: Who are the English actors and identical twin brothers known for playing Fred and George Weasley in the \"Harry Potter\" film series? DA 2: James and Oliver Phelps DQ 3: Who is the actor that plays Percy Weasley in the Harry Potter series? DA 3: Chris Rankin\nAnswer: The Weasley brothers in the Harry Potter series were played by identical twin brothers James and other Oliver Phelps. The Phelps brothers are English actors who have appeared in films and TV shows together as a duo. Chris Rankin is the actor who played Percy Weasley in the Harry Potter series. Rankin is a British actor who has appeared in theatre, film, and TV.\nTable 6: Example prompt and output in RAC. Few-shot training examples and format descriptions are omitted for simplicity.\nI will provide ambiguous questions that can have multiple answers based on their different possible interpretations. Clarify the given question into several disambiguated questions and provide short factoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at least three sentences. Here are some examples.\nFollow the following format. Context: [1] 1991 Major League Baseball All-Star Game | The 1991 Major League Baseball All-Star Game ... \u00b7 \u00b7 \u00b7 [5] Venues of the 1996 Summer Olympics | would serve as host to the Peach Bowl from ...\nQuestion: When did Toronto host the MLB All-Star Game in 1991?\nDisambiguations: DQ 1: When was the 1991 Major League Baseball All-Star Game played? DA 1: July 9, 1991 DQ 2: What was the outcome of the 1991 Major League Baseball All-Star Game? DA 2: American League defeated the National League\nAnswer: The 1991 Major League Baseball All-Star Game was ...\nI will provide ambiguous questions that can have multiple answers based on their different possible interpretations. Clarify the given question into several disambiguated questions and provide short factoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at least three sentences. Here are some examples.\nFollow the following format. Context: [1] Highest-paid NBA players by season | Highest-paid NBA players by season The highest-paid NBA players by season over ... \u00b7 \u00b7 \u00b7 [5] Highest-paid NBA players by season | Highest-paid NBA players ...\nQuestion: Who was the highest-paid NBA player in the 2017-2018 season?\nDisambiguations: DQ 1: Who was the highest-paid NBA player in the 2017-2018 season by salary? DA 1: LeBron James DQ 2: Who was the highest-paid NBA player in the 2017-2018 season by total earnings? DA 2: LeBron James\nAnswer: LeBron James was the highest-paid NBA player in the 2017-2018 season ...\nTable 7: Failure case where the model encounters and clarifies unambiguous questions. Few-shot training examples and format descriptions are omitted for simplicity."
        },
        {
            "heading": "Correct Case 1",
            "text": "DQ: Who was selected to host the 2018 FIFA World Cup?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed answer could be correct answers or not with only \u2018True\u2019 or \u2018False\u2019\nContext: 2018 and 2022 FIFA World Cup bids | FIFA\u2019s headquarters in Zurich. Russia was chosen to host the 2018 World Cup, and Qatar was chosen to host the 2022 World Cup. This made Russia the first Eastern European country to host the World Cup, while Qatar would be the first Middle Eastern country to host the World Cup. Blatter noted that the committee had decided to \u201cgo to new lands\u201d and reflected a desire to \u201cdevelop football\u201d by bringing it to more countries. In each round a majority of twelve votes was needed. If no bid received 12 votes in a round, the bid with the fewest votes\nQuestion: Who is hosting the next world cup 2022?\nProposed Answer: Russia"
        },
        {
            "heading": "False",
            "text": ""
        },
        {
            "heading": "Correct Case 2",
            "text": "DQ: Which player has won the most World Series in baseball?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed answer could be correct answers or not with only \u2018True\u2019 or \u2018False\u2019\nContext: World Series ring | on World Series rings. The New York Yankees Museum, located in Yankee Stadium, has an exhibit with replicas of all Yankees\u2019 World Series rings, including the pocket watch given after the 1923 World Series. Yogi Berra won the most World Series rings with 10, as a player. Frankie Crosetti won 17 as a player and as a coach. Yogi Berra Museum and Learning Center. World Series ring A World Series ring is an award given to Major League Baseball players who win the World Series. Since only one Commissioner\u2019s Trophy is awarded to the team, a World Series ring is\nQuestion: Who\u2019s won the most world series in baseball?\nProposed Answer: Yogi Berra"
        },
        {
            "heading": "True",
            "text": "Table 8: Correct cases of pruning method. Few-shot training examples or formats are not augmented to the prompt. Generated texts are colored green."
        },
        {
            "heading": "Incorrect Case 1",
            "text": "DQ: Who is the highest goalscorer in world football in a single game?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed answer could be correct answers or not with only \u2018True\u2019 or \u2018False\u2019\nContext: List of footballers with the most goals in a single game | This is a list of players with the most goals in a football game. The list only includes players who have scored the most multiple goals in first class or fully professional matches for country or club.The current world record for an international is held by Archie Thompson, who scored 13 goals against American Samoa in Australia\u2019s 31\u20130 victory during the 2002 FIFA World Cup qualification. David Zdrilic scored 8 goals.In November 2022, Shokhan Nooraldin Salihi scored 15 goals in the match of Al-Hilal against Sama in the 2022\u201323 Saudi Women\u2019s Premier League. In this match, Al-Hilal beat Sama 18-0.\nQuestion: Who has the highest goals in world football?\nProposed Answer: Archie Thompson"
        },
        {
            "heading": "False",
            "text": ""
        },
        {
            "heading": "Incorrect Case 2",
            "text": "DQ: When was episode 113 of Dragon Ball Super released in the US?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed answer could be correct answers or not with only \u2018True\u2019 or \u2018False\u2019\nContext: Dragon Ball Super | would be available in the United States in summer 2017. Bandai has also announced the updated \u201cDragon Ball Super Card Game\u201d that starts with one starter deck, one special pack containing 4 booster packs and a promotional Vegeta card and a booster box with 24 packs. It was released on July 28, 2017. A line of six \u201cDragon Ball Super\u201d Happy Meal toys were made available at Japanese McDonald\u2019s restaurants in May 2017. The average audience TV rating in Japan was 5.6% (Kanto region). The maximum audience rating was 8.4% (Episode 47) and the lowest rating was 3.5% (Episodes 109-110).\nQuestion: When is episode 113 of dragon ball super coming out?\nProposed Answer: November 5, 2017"
        },
        {
            "heading": "False",
            "text": "Table 9: Incorrect cases of self-verification. Generated texts are colored green.\nI will provide ambiguous questions that can have multiple answers based on their different possible interpretations. Clarify the given question into several disambiguated questions and provide short factoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at least three sentences. Here are some examples.\nContext: [1] Game of Thrones | Game of Thrones Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss. ... and its seventh season ended on August 27, 2017. The series will conclude with its eighth season [2] Game of Thrones | Game of Thrones is an American fantasy drama television series created by David Benioff and for HBO. It is an adaptation of \"A Song of Ice and Fire\", ... Set on the fictional continents of Westeros and Essos, \"Game of Thrones\" has a large ensemble cast \u00b7 \u00b7 \u00b7 [5] A Game of Thrones (comics) | A Game of Thrones (comics) A Game of Thrones is the comic book adaptation of George R. R. Martin\u2019s fantasy novel \"A Game of Thrones\", . . . It is intended to follow the story and atmosphere of the novel closely, at a rate of about a page of art for each page of text, and\nQuestion: What kind of series is game of thrones?\nDisambiguations: DQ 1: What is the genre of the American television series Game of Thrones? DA 1: fantasy drama DQ 2: What is the genre of the comic book series A Game of Thrones? DA 2: fantasy \u00b7 \u00b7 \u00b7 DQ 10: What is the genre of the board game A Game of Thrones? DA 10: strategy\nAnswer: There are multiple works that share the title Game of Thrones. The first is a television series that is a fantasy drama, the second is a comic book series that is fantasy, the third is a book series that is fantasy, and the fourth is a board game that is a strategy game.\nTable 10: Example prompt for the answer generation process. Generated texts are colored green."
        }
    ],
    "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
    "year": 2023
}