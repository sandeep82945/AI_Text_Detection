{
    "abstractText": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our largescale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zorik GekhmanT"
        },
        {
            "affiliations": [],
            "name": "Jonathan Herzig"
        },
        {
            "affiliations": [],
            "name": "Roee Aharoni"
        },
        {
            "affiliations": [],
            "name": "Chen Elkind"
        },
        {
            "affiliations": [],
            "name": "Idan Szpektor"
        }
    ],
    "id": "SP:e04fa0b2545e41195a1560ef6772bc949c05c0ab",
    "references": [
        {
            "authors": [
                "Priyanka Agrawal",
                "Chris Alberti",
                "Fantine Huot",
                "Joshua Maynez",
                "Ji Ma",
                "Sebastian Ruder",
                "Kuzman Ganchev",
                "Dipanjan Das",
                "Mirella Lapata."
            ],
            "title": "Qameleon: Multilingual QA with only 5 examples",
            "venue": "CoRR, abs/2211.08264.",
            "year": 2022
        },
        {
            "authors": [
                "Roee Aharoni",
                "Shashi Narayan",
                "Joshua Maynez",
                "Jonathan Herzig",
                "Elizabeth Clark",
                "Mirella Lapata."
            ],
            "title": "mface: Multilingual summarization with factual consistency evaluation",
            "venue": "CoRR, abs/2212.10622.",
            "year": 2022
        },
        {
            "authors": [
                "Rachith Aiyappa",
                "Jisun An",
                "Haewoon Kwak",
                "YongYeol Ahn"
            ],
            "title": "Can we trust the evaluation on chatgpt? CoRR, abs/2303.12767",
            "year": 2023
        },
        {
            "authors": [
                "Vidhisha Balachandran",
                "Hannaneh Hajishirzi",
                "William W. Cohen",
                "Yulia Tsvetkov."
            ],
            "title": "Correcting diverse factual errors in abstractive summarization via post-editing and language model infilling",
            "venue": "Proceedings of the 2022 Conference",
            "year": 2022
        },
        {
            "authors": [
                "Michele Banko",
                "Michael J. Cafarella",
                "Stephen Soderland",
                "Matthew Broadhead",
                "Oren Etzioni."
            ],
            "title": "Open information extraction from the web",
            "venue": "IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, In-",
            "year": 2007
        },
        {
            "authors": [
                "Yonatan Bitton",
                "Shlomi Cohen-Ganor",
                "Ido Hakimi",
                "Yoad Lewenberg",
                "Roee Aharoni",
                "Enav Weinreb"
            ],
            "title": "q2d: Turning questions into dialogs to teach models how to search",
            "year": 2023
        },
        {
            "authors": [
                "Shiqi Chen",
                "Siyang Gao",
                "Junxian He."
            ],
            "title": "Evaluating factual consistency of summaries with large language models",
            "venue": "CoRR, abs/2305.14069.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "CoRR, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei."
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "CoRR, abs/2210.11416.",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel R. Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: evaluating cross-lingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Bosheng Ding",
                "Chengwei Qin",
                "Linlin Liu",
                "Lidong Bing",
                "Shafiq R. Joty",
                "Boyang Li"
            ],
            "title": "Is GPT-3 a good data annotator? CoRR, abs/2212.10450",
            "year": 2022
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "Summeval: Reevaluating summarization evaluation",
            "venue": "arXiv preprint arXiv:2007.12626.",
            "year": 2020
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo F.R. Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "Proceedings of the 57th Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo F.R. Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "Proceedings of the 57th Annual",
            "year": 2019
        },
        {
            "authors": [
                "Ben Goodrich",
                "Vinay Rao",
                "Mohammad Saleh",
                "Peter J. Liu."
            ],
            "title": "Assessing the factual accuracy of generated text",
            "venue": "CoRR, abs/1905.13322.",
            "year": 2019
        },
        {
            "authors": [
                "Tanya Goyal",
                "Junyi Jessy Li",
                "Greg Durrett."
            ],
            "title": "News summarization and evaluation in the era of GPT-3",
            "venue": "CoRR, abs/2209.12356.",
            "year": 2022
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Samin Mubasshir",
                "Yuan-Fang Li",
                "YongBin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "Xl-sum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "Find-",
            "year": 2021
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1s Kocisk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference",
            "year": 2015
        },
        {
            "authors": [
                "Or Honovich",
                "Roee Aharoni",
                "Jonathan Herzig",
                "Hagai Taitelbaum",
                "Doron Kukliansy",
                "Vered Cohen",
                "Thomas Scialom",
                "Idan Szpektor",
                "Avinatan Hassidim",
                "Yossi Matias."
            ],
            "title": "TRUE: re-evaluating factual consistency evaluation",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend."
            ],
            "title": "$q\u02c62$: Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering",
            "venue": "Proceedings of the 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "CoRR, abs/2212.10403.",
            "year": 2022
        },
        {
            "authors": [
                "Tushar Khot",
                "Ashish Sabharwal",
                "Peter Clark."
            ],
            "title": "Scitail: A textual entailment dataset from science question answering",
            "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Arti-",
            "year": 2018
        },
        {
            "authors": [
                "Tom Kocmi",
                "Christian Federmann."
            ],
            "title": "Large language models are state-of-the-art evaluators of translation quality",
            "venue": "CoRR, abs/2302.14520.",
            "year": 2023
        },
        {
            "authors": [
                "Huan Yee Koh",
                "Jiaxin Ju",
                "Ming Liu",
                "Shirui Pan."
            ],
            "title": "An empirical survey on long document summarization: Datasets, models, and metrics",
            "venue": "ACM Comput. Surv., 55(8):154:1\u2013154:35.",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Neural text summarization: A critical evaluation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N. Bennett",
                "Marti A. Hearst."
            ],
            "title": "Summac: Re-visiting nlibased models for inconsistency detection in summarization",
            "venue": "Trans. Assoc. Comput. Linguistics, 10:163\u2013 177.",
            "year": 2022
        },
        {
            "authors": [
                "Faisal Ladhak",
                "Esin Durmus",
                "Claire Cardie",
                "Kathleen R. McKeown."
            ],
            "title": "Wikilingua: A new benchmark dataset for cross-lingual abstractive summarization",
            "venue": "CoRR, abs/2010.03093.",
            "year": 2020
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "WANLI: worker and AI collaboration for natural language inference dataset creation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "venue": "CoRR, abs/2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou."
            ],
            "title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "venue": "CoRR, abs/2303.15621.",
            "year": 2023
        },
        {
            "authors": [
                "Clark."
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "CoRR, abs/2303.17651.",
            "year": 2023
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan T. McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July",
            "year": 2020
        },
        {
            "authors": [
                "Anshuman Mishra",
                "Dhruvesh Patel",
                "Aparna Vijayakumar",
                "Xiang Lorraine Li",
                "Pavan Kapanipathi",
                "Kartik Talamadupula."
            ],
            "title": "Looking beyond sentencelevel natural language inference for question answering and text summarization",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela"
            ],
            "title": "Adversarial NLI: A new benchmark for natural lan",
            "year": 2020
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the As-",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Vitaly Nikolaev",
                "Matthew Lamm",
                "Michael Collins",
                "Dipanjan Das",
                "Slav Petrov",
                "Gaurav Singh Tomar",
                "Iulia Turc",
                "David Reitter."
            ],
            "title": "Measuring attribution in natural language generation models",
            "venue": "CoRR, abs/2112.12870.",
            "year": 2021
        },
        {
            "authors": [
                "Tal Schuster",
                "Sihao Chen",
                "Senaka Buthpitiya",
                "Alex Fabrikant",
                "Donald Metzler."
            ],
            "title": "Stretching sentence-pair NLI models to reason over long documents and clusters",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano",
                "Alex Wang",
                "Patrick Gallinari."
            ],
            "title": "Questeval: Summarization asks for fact-based evaluation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in",
            "year": 2021
        },
        {
            "authors": [
                "Amir Soleimani",
                "Christof Monz",
                "Marcel Worring."
            ],
            "title": "NonFactS: NonFactual summary generation for factuality evaluation in document summarization",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 6405\u20136419, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Prasetya Utama",
                "Joshua Bambrick",
                "Nafise Sadat Moosavi",
                "Iryna Gurevych."
            ],
            "title": "Falsesum: Generating document-level NLI examples for recognizing factual inconsistency in summarization",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis."
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
            "year": 2020
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Is chatgpt a good NLG evaluator? A preliminary study",
            "venue": "CoRR, abs/2303.04048.",
            "year": 2023
        },
        {
            "authors": [
                "Shuohang Wang",
                "Yang Liu",
                "Yichong Xu",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Want to reduce labeling cost? GPT-3 can help",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,",
            "year": 2021
        },
        {
            "authors": [
                "Yixuan Weng",
                "Minjun Zhu",
                "Fei Xia",
                "Bin Li",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Large language models are better reasoners with self-verification",
            "venue": "CoRR, abs/2212.09561.",
            "year": 2023
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi."
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Wenhao Wu",
                "Wei Li",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Sujian Li",
                "Yajuan Lv."
            ],
            "title": "Wecheck: Strong factual consistency checker via weakly supervised learning",
            "venue": "Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics, ACL 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Wenpeng Yin",
                "Dragomir R. Radev",
                "Caiming Xiong."
            ],
            "title": "Docnli: A large-scale dataset for document-level natural language inference",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-",
            "year": 2021
        },
        {
            "authors": [
                "Parliament member Matteo"
            ],
            "title": "Salvini, the leader of Italy\u2019s far-right Northern League. In the first three months of 2015, Italy registered more than 10,000 migrants arriving, the International Organization for Migration said, and about",
            "year": 2015
        },
        {
            "authors": [
                "Weng"
            ],
            "title": "2023), we use a self verification prompt. If the LLM classified the summary as consistent, we prompt it again and ask it for its certainty",
            "year": 2023
        },
        {
            "authors": [
                "Rashkin"
            ],
            "title": "2021), with the participants asked \"Is all the information in the summary fully attributable",
            "year": 2021
        },
        {
            "authors": [
                "Utama"
            ],
            "title": "2022)\u2019s human evaluation",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Generative summarization models are prone to generate summaries that are factually inconsistent with respect to the corresponding input documents (Goodrich et al., 2019; Kryscinski et al., 2019), limiting their applicability in real-world scenarios.\n\u2217Work done during an internship at Google Research. 1Our dataset and model are available at: https://github.com/google-research/\ngoogle-research/tree/master/true_teacher\nSince factual consistency evaluation could be cast as a Natural Language Inference (NLI) task, NLI models are often used to evaluate consistency (Falke et al., 2019a; Maynez et al., 2020; Laban et al., 2022). However, NLI models exhibit limited success in evaluating factual consistency in summarization (Falke et al., 2019b; Kryscinski et al., 2020), since NLI datasets lack the entailment phenomena that naturally arise in abstractive summarization (Khot et al., 2018). For example, single-sentence premise-hypothesis pairs are shorter than document-summary pairs (Mishra et al., 2021; Schuster et al., 2022).\nTo address this domain mismatch, previous work proposed various approaches for generating synthetic training data (Kryscinski et al., 2020; Yin et al., 2021; Utama et al., 2022; Balachandran et al., 2022). The data is typically generated by perturb-\ning human-written summaries to introduce factual inconsistencies. While these perturbations are effective, they are limited to factual error categories that can be covered by the perturbation logic. In addition, since simulating factual errors is challenging, such perturbations may fail to introduce factual errors, leading to incorrect labels.2 Finally, since the synthetic summaries are based on humanwritten summaries, they may differ in style from real model-generated summaries, which can reduce the effectiveness of the synthetic data.\nAn alternative approach to augmenting NLI models with synthetic data, is to directly prompt large language models (LLMs) to evaluate factual consistency. Recently, there has been a growing evidence for the effectiveness of LLMs in evaluating generative tasks (Kocmi and Federmann, 2023; Wang et al., 2023; Liu et al., 2023), including factual consistency in summarization (Chen et al., 2023). However, LLMs are still too computationally expensive to be heavily used in practice.\nTo make the best of both worlds we propose TrueTeacher, a simple and effective synthetic data generation method that leverages model-generated summaries and the reasoning abilities of LLMs (Huang and Chang, 2022). In TrueTeacher, we first train a diverse collection of summarization models with different capacities. Next, we use these models to summarize each document in a given corpus (Figure 1). The resulting document-summary pairs are then annotated by prompting a LLM to predict the corresponding factual consistency label.\nWe apply TrueTeacher using FLAN-PaLM 540B (Chung et al., 2022) to generate a large-scale synthetic dataset, which is used to train a student model. Experiments on the summarization subset of the TRUE benchmark (Honovich et al., 2022) show that augmenting existing NLI data with TrueTeacher data improves a state-of-the-art model\u2019s ROC-AUC from 82.7 to 87.8, while maintaining similar model capacity. The resulting model even outperforms its LLM teacher, despite the latter having a \u00d750 larger capacity.\nWe also compare TrueTeacher to existing synthetic data generation methods. To this end, we design a systematic study to re-evaluate existing methods with a \"fair comparison\" in a challenging setting. Our results indicate that existing approaches fail to generalize to documents derived from a distribution different from the one used for\n2As we also demonstrate in \u00a74.3.\nsynthetic data generation. In contrast, TrueTeacher demonstrates robustness by successfully generalizing to documents from new domains.\nFinally, we apply TrueTeacher to generate multilingual synthetic data. While existing data generation methods are often limited to English (Utama et al., 2022; Balachandran et al., 2022), TrueTeacher can use a multilingual LLM. Results on the mFACE dataset (Aharoni et al., 2022), show improvements on 35 out of 45 languages when using our method. This demonstrates the usefulness of multilingual synthetic data and the effectiveness of TrueTeacher in generating such data.\nTo summarize, this work includes the following contributions:\n\u2022 We introduce TrueTeacher, a synthetic data generation approach based on annotating model-generated summaries with LLMs, and demonstrate its effectiveness and robustness.\n\u2022 We evaluate FLAN-PaLM 540B on the task of factual consistency evaluation and show that its knowledge can be distilled into a significantly smaller model using our method.\n\u2022 We conduct a systematic study, re-evaluating existing synthetic data generation methods for the task in an apples-to-apples comparison and identify their limitations.\n\u2022 We perform the first experiment in generating multilingual synthetic data for factual consistency, and demonstrate its usefulness.\n\u2022 We release a large-scale dataset comprised of 1.4 million TrueTeacher examples, and verify its quality with human evaluation. We additionally release a state-of-the-art consistency evaluation model trained on this data.1"
        },
        {
            "heading": "2 TrueTeacher",
            "text": "In this section we describe TrueTeacher, our approach for generating synthetic examples for the task of factual consistency evaluation in summarization. Our main motivation is to use factual inconsistencies that occur in real model-generated summaries, instead of relying on perturbed human-written summaries. To this end, we generate a diverse set of summaries using generative summarization models of different capacities, and leverage a LLM to label them for factual consistency. Some of the generated summaries are expected to contain factual errors, and we hypothesize\nthat a strong-performing LLM can generalize to the task and label them with sufficient quality to be useful for training. The usage of model-generated summaries not only yields more realistic texts, but also allows to potentially include rare errors, which can be harder to incorporate with perturbation logic.\nOur data generation process is illustrated in Figure 2. First, we train a variety of summarization models (upper diagram). We use a collection of one or more summarization training sets T = {sd1, sd2, . . . , sdn} and different pretrained LMs = {lm1, lm2, . . . , lmm} to finetune a collection of summarization models SM = {sm1, sm2, . . . , smk}, where k = n\u00d7m.3 Using different pretrained LMs allows to diversify the expected consistency errors, e.g., errors made by large or small models. The choice of summarization training sets allows to control for the nature of the resulting summaries, e.g., focusing on abstrative training sets to increase output abstractiveness.\nNext, we obtain model-generated summaries and annotate them (lower diagram). We choose a documents corpus D = {d1, d2, . . . , dr} and use all the summarization models in SM to summarize all the documents in D, resulting in a collection of modelgenerated output summaries O = {s1,1, . . . sr,k}, where si,j is the summary of document di generated by summarization model smj . TrueTeacher\n3We note that the pretrained LMs here refer to the models that we are fine tuning for summarization, and they are different from the LLM that we use as the teacher.\ndoes not require gold summaries, which allows it to be used with any collection of documents D, and makes it more scalable than previous methods (Yin et al., 2021; Utama et al., 2022; Balachandran et al., 2022).\nFinally, a LLM is prompted to label all the summaries in O for consistency w.r.t. their source documents, resulting with labels {l1,1, . . . , l1,k, . . . lr,k}.4 Figure 1 illustrates a real example of this process for a single document di \u2208 D. Each document, summary, and label (di, si,j , li,j) are then used as a synthetic example for training a factual consistency classifier. Since we leverage LLMs for labeling, our approach is likely to benefit from the ongoing progress in LLMs quality. Furthermore, previous approaches often rely on language-specific components (e.g., Information Extraction), which limits their applicability in multiple languages. Since recent LLMs are pretrained on multilingual data, our method can be easily applied to non-English languages, as we show in \u00a75."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "We use TrueTeacher to generate a synthetic dataset for factual consistency evaluation in summarization (\u00a73.1), and experiment with it to evaluate the effectiveness and usefulness of our method (\u00a74)."
        },
        {
            "heading": "3.1 TrueTeacher Instantiation",
            "text": "To apply TrueTeacher, we instantiate the summarization datasets T , the pre-trained LMs and the documents corpus D. We use XSum (Narayan et al., 2018) as T , T5 pre-trained models (Raffel et al., 2020) as LMs = {T5-small, T5-base, T5-large, T5-3B, T5-11B}, and documents from CNN/DailyMail (Hermann et al., 2015) as D.\nAs our teacher model, we employ FLAN-PaLM 540B (Chung et al., 2022). This model was instruction fine-tuned, including training on the closelyrelated NLI task.5 Therefore, we expect it to generalize well to factual consistency evaluation.6 We use zero-shot prompting for simplicity, and since applying few-shot or chain-of-thought prompting did not improve performance in early experiments.7\n4See \u00a73.1 and \u00a7A.1 for our prompting implementation. 5https://github.com/google-research/FLAN/blob/\ne9e4ec6e2701182c7a91af176f705310da541277/flan/ task_splits.py#L109\n6We validate this expectation in \u00a74.1 and \u00a74.4. 7In \u00a7A.1 we discuss potential reasons to this.\nExtensive implementation details about our FLANPaLM usage are provided in \u00a7A.1 and \u00a7A.2.\nApplying TrueTeacher in this setup resulted in \u223c1.4M synthetic training examples (Table 1), which we use to train a student model for factual consistency evaluation.8 In \u00a74, we provide evidence for the dataset\u2019s quality through human evaluation (\u00a74.4), its usefulness for improving NLI models in a challenging setting (\u00a74.1), and its superiority over other existing synthetic datasets (\u00a74.2).\nIn early experiments, we also explored data filtering based on prompting FLAN-PaLM for selfverification (details in \u00a7A.5). This resulted in an increase in the labeling accuracy. Yet, surprisingly, training the student model on the filtered data did not improve performance in comparison to training on the full dataset.9 Thus, for simplicity, we conduct experiments using the full dataset."
        },
        {
            "heading": "3.2 Evaluation",
            "text": "To compare between consistency evaluation models, we use the TRUE benchmark (Honovich et al., 2022), focusing on its summarization subset: MNBM (Maynez et al., 2020), FRANK (Pagnoni et al., 2021), SummEval (Fabbri et al., 2020), QAGS-X and QAGS-C (Wang et al., 2020). For additional details about these datasets, we refer the reader to Honovich et al. (2022). Following Honovich et al., we use ROC-AUC in a binary classification setting as our evaluation metric."
        },
        {
            "heading": "3.3 Baselines",
            "text": "We compare the performance of factual consistency evaluation models trained on TrueTeacher data, against the top performing models on the TRUE benchmark: QuestEval (Scialom et al., 2021), Q2 (Honovich et al., 2021), SUMMACZS (Laban et al., 2022), T5-11B fine tuned on ANLI (Honovich\n8Implementation details for our trained models are in \u00a7A.3. 9This could be attributed to the high-quality of the initial\nlabels and the student model\u2019s robustness to noise.\net al., 2022), WeCheck (Wu et al., 2023), and the Ensemble from Honovich et al. (2022).10\nWe also compare TrueTeacher data generation mechanism to existing methods for synthetic data generation. We consider the following approaches:\nDocNLI (Yin et al., 2021). Reformatted NLI, question answering and summarization datasets, including the CNN/DM corpus. The summarizationbased positive examples are based on concatenated gold summaries. The negative examples are then generated using word/entity replacements.\nFactCC (Kryscinski et al., 2020). The documents are from CNN/DM. The consistent summaries are randomly sampled sentences from the document, which are optionally injected with noise or paraphrased. The inconsistent summaries are obtained by rule-based transformations, such as sentence negation and entity/pronoun/number swaps.\nFactEdit (Balachandran et al., 2022). The positive examples are based on gold summaries from CNN/DM. For the negative examples, an infilling model is trained using sentences from the documents, employing the OpenIE framework (Banko et al., 2007) to mask predicates and arguments. Each predicate and argument phrase in the summary is then iterativelly masked and infilled with the model\u2019s lower order beam candidates.\nFalsesum (Utama et al., 2022). The positive examples are based on gold summaries from CNN/DM. For the negative examples, predicates and arguments are detected in the document and the summary using the OpenIE (Banko et al., 2007) framework. Randomly selected predicates and arguments from the summary are then masked and infilled using predicates and arguments from the document, or by \"hallucinating\" new content. For this purpose a dedicated infilling model is trained."
        },
        {
            "heading": "4 Experiments and Analysis",
            "text": "Our main experiments are in \u00a74.1 and \u00a74.2, followed by various analyses and ablations in \u00a74.3, \u00a74.4, \u00a74.5 and \u00a74.6. We design our experiments to address the following research questions (RQs):\n\u2022 RQ1: What is the performance of FLAN-PaLM 540B in factual consistency evaluation in summarization? Is it a good choice for a teacher? 10We discuss WeCheck in \u00a76, and refer the reader to Honovich et al. (2022) for a detailed description of other baselines.\n\u2022 RQ2: Can TrueTeacher facilitate training of a competitive model w.r.t. state-of-the-art models?\n\u2022 RQ3: What is the quality of the data generated using TrueTeacher compared to existing synthetic data generation methods?\nWe address RQ1 and RQ2 in \u00a74.1. To address RQ1, we evaluate FLAN-PaLM 540B against competitive models for factual consistency evaluation. To address RQ2, we use our full dataset from \u00a73.1 to train our best-performing model, and evaluate it in the exact same setting. Finally, RQ3 is addressed in \u00a74.2, where we conduct a systematic study, comparing existing methods to TrueTeacher, while controlling for factors such as the synthetic data size and the documents used for data synthesis."
        },
        {
            "heading": "4.1 Main Results on the TRUE Benchmark",
            "text": "We address RQ1 by evaluating FLAN-PaLM 540B on the task and present the results in Table 2. FLAN-PaLM 540B achieves an impressive performance, with an average ROC-AUC of 84.9 compared to 83.0 of the best single-model baseline, and performs on-par with the Ensemble. This demonstrates the chosen LLM\u2019s capability for the task, and its potential as a teacher for smaller models.\nTo address RQ2, we fine-tune T5-11B (Raffel et al., 2020) over our full dataset (\u00a73.1) mixed with ANLI (Nie et al., 2020). Table 2 shows that including TrueTeacher data in the training set, substantially improves the strong-performing T5-11B w. ANLI baseline from an average ROCAUC of 82.7 to 87.8 (+5.1), while maintaining exactly the same model capacity. This strong result demonstrates the high effectiveness of TrueTeacher in a challenging setup. Notably, our model sets the new state-of-the-art result on the benchmark, outperforming the \u00d750 times larger LLM that we used as the teacher (84.9 \u2192 87.8). This can be\nattributed to large-scale knowledge distillation on a specific task, while the LLM is trained to perform many tasks. Additionally, the smaller model is trained on target-domain data (documents and model-generated summaries) which can further improve performance (Gururangan et al., 2020)."
        },
        {
            "heading": "4.2 Re-evaluating Synthetic Data Generation Methods \u2013 A Study",
            "text": "Previous studies on synthetic data generation have used different experimental setups, making it difficult to compare their results. In this section, we design a systematic study to re-evaluate existing methods in a standardized setup. We first discuss our study design choices followed by the results.\nPrevious work has demonstrated that synthetic data can improve NLI-based models. However, they typically used relatively small-capacity models, whereas Honovich et al. (2022) recently demonstrated significant performance gains by scaling up to T5-11B fine-tuned on ANLI. We therefore adopt this competitive baseline, to which we add synthetic data from each method. For ablation, we include variants trained solely on synthetic data (without ANLI), and also repeat our study using the smaller-capacity T5-base model.\nTo preform a fair comparison, we restrict the number of examples from each evaluated method to 100k, randomly sampled with balanced labels.\nTo evaluate domain-shift robustness, we further restrict the synthetic training examples to ones that were generated only based on CNN/DM documents,11 and then consider the XSum-based evaluation sets as out-of-domain.12\n11Some methods are based exclusively on CNN/DM while others use additional datasets, more details in \u00a73.3.\n12SummEval and QAGS-C are based on documents from CNN/DM, MNBM and QAGS-X use documents from XSum, and FRANK has documents from both CNN/DM and XSum. We split FRANK to FRANK-C and FRANK-X which contain its CNN/DN based and XSum based subsets respectively.\nTable 3 presents the results of our study. We calculate three average scores: for in-domain test sets based on CNN/DM documents, for out-of-domain test sets based on XSum documents, and for the original datasets from TRUE.\nIn-Domain Results Most methods outperform the corresponding ANLI-only baseline, demonstrating the usefulness of synthetic data. Predictably, all methods improve with larger models and a complementary effect is often observed when mixing synthetic data with ANLI. The best results are obtained by mixing ANLI with Falsesum or TrueTeacher data and using T5-11B, with a substantial improvement over the corresponding ANLI-only baseline (in-domain score increase from 81.1 to 87.9).\nOut-of-domain Results While most methods perform well in-domain, their performance drops significantly on the out-of-domain test sets. Most of the evaluated methods underperform the corresponding ANLI-only baseline with similar model capacity. For some methods, performance deteriorates dramatically; e.g. Falsesum \u2013 despite its impressive in-domain performance, its out-ofdomain score falls significantly below the ANLIonly baseline. This suggests that some methods overfit to documents from the distribution used to generate the synthetic data. Based on this finding, we encourage future research to prioritize outof-domain evaluation. Interestingly, even though TrueTeacher\u2019s relative improvement is smaller com-\npared to the in-domain setup, it is still the only method with higher out-of-domain score compared to the corresponding ANLI-only baseline. This demonstrates the robustness of TrueTeacher to domain shift, which may be due to the use of modelgenerated summaries that increase the variability of the resulting synthetic data.\nOverall Results on TRUE Due to the poor outof-domain performance of the existing methods, TrueTeacher is the only method that consistently outperforms the ANLI-only baseline on the TRUE benchmark. Notably, TrueTeacher + ANLI with T5base (81.9) performs on par with the ANLI-only baseline using T5-11B (82.0). Additionally, the TrueTeacher-based variant using T5-11B (85.2) already performs on-par with the 540B LLM teacher (84.9, Table 2), even though we used only 100k synthetic examples in this experiment, and did not use ANLI data. When comparing TrueTeacher + ANLI with T5-11B and 100k examples (Table 3) to the equivalent variant using the full dataset (Table 2), we observe a performance increase (86.4\u2192 87.8), which demonstrates TrueTeacher\u2019s scalability. We conclude that TrueTeacher yields high quality data and generalizes well for new domains, which we attribute to the usage of model-generated summaries."
        },
        {
            "heading": "4.3 Qualitative Analysis",
            "text": "Figure 3 presents a case study with a randomly sampled document, and the corresponding inconsistent summaries generated with each of the evaluated\nmethods. FactEdit used the second gold-summary and replaced \"to flooding call\" with \"rescue\", introducing a grammatical error rather than a factual error, demonstrating the potential problems with using lower-beam completions as proxy for factual errors. DocNLI uses all the gold summaries concatenated. While replacing \"morning\" with \"night\" introduces a factual error, three other edits fail to introduce factual errors, demonstrating the limitations of using simple word/entity replacements. FactCC used the first sentence from the article and successfully introduced factual error by an entity swap from \"firetruck\" to \"fire engine\". The paraphrase highlighted in green increases the abstractiveness, but the paraphrase in orange introduces a grammatical error that is less likely to be made by a strong summarization model. The noise injection used by FactCC (duplicating or removing random tokens) is colored in red, but its usefulness is questionable. Falsesum uses the first gold summary, and its perturbation model predicts the removal of \"Tuesday morning\" and the replacement of the \"sinkhole\" argument with \"water\", failing to introduce a factual error, since the sinkhole is referred to as \"water-logged sinkhole\" in the article. Finally, TrueTeacher uses an abstractive summary generated by a real summarization model.\nIt introduces a nuanced factual error by replacing \"Los Angeles firefighters\" with A firefighter and also by hallucinating new content (the text in bold red font). This case study further illustrates the challenges of perturbing texts to introduce factual inconsistencies and re-iterates the importance in using model-generated summaries."
        },
        {
            "heading": "4.4 Human Evaluation",
            "text": "To further assess the quality of the synthetic data produced by TrueTeacher, we perform human evaluation carried out by domain experts.13 We evaluate 100 examples from our dataset,14 using binary judgements based on the attribution definition from Rashkin et al. (2021). The labeling accuracy of the sampled examples from our data stands at 89%, which demonstrates its high quality. Table 4 further presents the precision, recall and F1 scores for the consistent and inconsistent classes. More details on the human evaluation are available in \u00a7A.8."
        },
        {
            "heading": "4.5 Ablating Summary Distribution and Label Correctness",
            "text": "There are two key differences between TrueTeacher and perturbation-based synthetic data generation methods: (1) the distribution of the summaries15 and (2) the correctness of the generated labels.16 Each of these differences may lead to the better quality of TrueTeacher w.r.t the baselines. To measure the impact of each difference, we isolate them in a controlled ablation study. We create 2 ablated variants, using Falsesum as a recent baseline method for synthetic data generation. The results are presented in Table 5. LabelAblation is an ablation created by labeling the document-summary pairs from Falsesum\u2019s data using FLAN-PaLM 540B.17 Comparing\n1310 NLP researchers, each with at least one year of experience in factual consistency evaluation.\n14We randomly sampled 50 positively and 50 negatively labeled examples from our synthetic dataset.\n15Model-generated vs. human-written perturbed. 16Both methods may yield wrong labels. Perturbations might not introduce inconsistencies, as seen in \u00a74.3, while TrueTeacher can have errors due to LLM mislabeling.\n17We used the same 100k examples as Falsesum + ANLI baseline, and the same LLM prompt as in TrueTeacher.\nLabelAblation to Falsesum + ANLI allows us to examine the effect of using FLAN-PaLM labels instead of the original Falsesum labels, while controlling for the summaries distribution. LabelAblation outperforms Falsesum + ANLI by 5.6%, which shows that performance gains can be obtained using summaries generated with existing synthetic data generation methods combined with second-stage improved labeling quality. However, TrueTeacher is substantially simpler and also results in better performance. SummaryAblation is an ablation created by flipping labels on a random portion of TrueTeacher\u2019s data, such that the expected labeling accuracy is similar to Falsesum (More details in \u00a7A.9). Comparing SummaryAblation to Falsesum + ANLI allows us to examine the effect of changing the summary distribution from human-written perturbed to model-generated, while controlling for the labeling quality. SummaryAblation outperforms Falsesum + ANLI by 5.8%, a similar improvement as observed for LabelAblation (5.6%). This demonstrates that label correctness and summary distribution have a similar effect on the performance, but they also have a complimentary effect as the best performance of 86.4 ROC-AUC is obtained only when they are combined together."
        },
        {
            "heading": "4.6 Abstractiveness Analysis",
            "text": "Advances in large scale pretraining (Devlin et al., 2019; Lewis et al., 2020) and the availability of relevant datasets (Narayan et al., 2018), enabled rapid progress in abstractive summarization, which better imitates the way humans summarize (Koh et al., 2023) and is also preferred by humans (Goyal et al., 2022). This motivates us to focus on generating abstractive synthetic summaries.\nWe compare the abstractiveness degree of different methods using the extractive fragment coverage and density measures from Grusky et al. (2018). Following Utama et al. (2022) we multiply these\nmeasures to obtain a combined score.18 Table 6 presents the abstractiveness scores, and a density plot is available in the Appendix (Figure 5). We observe higher abstractiveness for model-based methods (FactEdit, Falsesum and TrueTeacher), suggesting that rule-based methods might be less useful with the recent shift towards abstractive summarization. TrueTeacher produces the most abstractive summaries with lowest combined score."
        },
        {
            "heading": "5 Multi-Lingual Data Generation for Factual Consistency Evaluation",
            "text": "Utilizing a multilingual LLM enables a straightforward application of TrueTeacher to multiple languages. This contrasts with recent approaches that rely on NLP components only available for highresource languages, e.g., information extraction (Utama et al., 2022; Balachandran et al., 2022). In this section, we examine TrueTeacher\u2019s usefulness for multilingual factual consistency evaluation.\nWe first generate multilingual synthetic data using TrueTeacher. This time we train a single summarization model by fine tuning mT5-XXL (Xue et al., 2021) on XLSum (Hasan et al., 2021) and use it to summarize documents from WikiLingua (Ladhak et al., 2020), which we then label for consistency with our LLM. For the purposes of this experiment we focus on a subset of WikiLingua documents in 4 languages: English (en), French\n18We provide additional technical details in \u00a7A.6.\n(fe), Spanish (es) and German (de).19. After generating the dataset for these 4 languages, we sample 100k examples, by randomly sampling 25k in each language with balanced labels (as illustrated in Table 9 in the Appendix). For ablation, we also create an English-only variant, by randomly sampling 100k English examples with balanced labels.20\nWe use the resulted data to train multilingual consistency evaluation models and evaluate them on the mFace test set (Aharoni et al., 2022), containing 3150 examples in 45 languages. As a strong baseline we follow Aharoni et al. and fine-tune mT5XXL (Xue et al., 2021) on the ANLI (Nie et al., 2020) and XNLI (Conneau et al., 2018) datasets. We then assess whether adding our synthetic data to the training set can improve this model.\nTable 7 presents the results overview, full results in all 45 languages are available in Table 10 (Appendix). Adding English-only summarizationbased synthetic data, already improves results on 32 out of 45 languages and increases the avg. ROCAUC from 71.6 to 73.8. Yet, using the same amount of multi-lingual examples improved the performance even more, with avg. ROC AUC of 75.3. This demonstrates the added value in generating multi-lingual synthetic examples using TrueTeacher, laying the ground for future work."
        },
        {
            "heading": "6 Related Work",
            "text": "Previous work proposed methods for generating synthetic training data for factual consistency evaluation, by perturbing gold summaries (Yin et al., 2021; Kryscinski et al., 2020; Balachandran et al., 2022; Utama et al., 2022; Soleimani et al., 2023).21 A key advantage of TrueTeacher, is the ability to leverage real model-generated summaries, leading to superior performance and robustness. The utility of model-generated outputs was also highlighted by Wu et al. (2023), who proposed a weakly super-\n19They are the most prevalent languages in PaLM\u2019s pretraining data (Chowdhery et al., 2022)\n20Also based on WikiLingua, generated with the same process like the 25k English subset of our multilingual dataset.\n21We provide extensive review of these methods in \u00a73.3.\nvised consistency evaluation model that leverages probabilistic labels derived from aggregated scores of other consistency evaluation models. Our work proposes a simpler solution, that is also inherently multilingual.\nAnother line of work for adapting NLI-based models for summarization, focuses on better processing of long texts, splitting the documents into sentences to create shorter premise-hypothesis pairs (Laban et al., 2022; Schuster et al., 2022).\nRecent work attempts to assess LLMs\u2019 capability for evaluating generative tasks (Kocmi and Federmann, 2023; Wang et al., 2023; Liu et al., 2023). Luo et al. (2023) evaluated ChatGPT (OpenAI, 2022) speciffically on the task of factual consistency evaluation in summarization. Yet, Aiyappa et al. (2023) argued that ChatGPT\u2019s \"closed\" nature risks data leakage (training-test contamination).22 Chen et al. (2023) performed a study of LLMs as factual consistency evaluators, using a variety of prompting methods.\nPrevious work also attempted to distill knowledge from LLMs (West et al., 2022; Hsieh et al., 2023), as well as to leverage LLMs for data annotation (Wang et al., 2021; Ding et al., 2022), and synthetic data generation (Agrawal et al., 2022; Liu et al., 2022; Bitton et al., 2023). As far as we aware, our work is the first to leverage LLMs for data generation for factual consistency evaluation."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduced TrueTeacher, a simple and highly effective method for generating synthetic data for factual consistency evaluation. Instead of perturbation of human-written summaries like done in previous work, TrueTeacher leverages realistic model-generated summaries, which are annotated by prompting a large language model.\nUsing our method, we generate a large-scale synthetic dataset, which we are making publicly available. Our experimental results show that this dataset substantially enhances the performance of a state-of-the-art model. In our systematic study, we compare TrueTeacher to existing approaches and further demonstrate its effectiveness and robustness. Our study highlights the importance of out-ofdomain evaluation, which we hope will be adopted in future work. Lastly, we show that TrueTeacher generalizes well to multilingual scenarios, presenting additional advantage over existing methods.\n22While FLAN\u2019s instruction fine-tuning data is public."
        },
        {
            "heading": "8 Limitations",
            "text": "Noisy synthetic data TrueTeacher relies on a LLM for labeling model generated summaries. This process may result in some frequency of noisy synthetic examples for which the label is incorrect. This can affect the overall quality of the student model that trains on this data. In our experiments we validated the quality of our synthetic data with human evaluation, however this should be reexamined when generating data for new domains. In addition, we experimented with different filtering approaches, but found that training on filtered data with higher labeling accuracy, did not improve the performance of the student model. We encourage future work to further examine such automatic filtering.\nReliance on LLMs In this work we use a 540B LLM to label 1.4M model generated summaries. This requires non-negligible resources that may not be available to the whole community. To mitigate this, we release our collected synthetic data and the corresponding model checkpoint. In addition, the decreasing inference cost of proprietary LLMs, and the availability of open-source LLMs (Touvron et al., 2023) can further assist.\nEffect of low-resource languages Our multilingual experiments (\u00a75) focus on a subset of WikiLingua documents in only 4 languages: English (en), French (fe), Spanish (es) and German (de), that are the most prevalent in our LLM\u2019s pre-training data. As can be seen in our full results (Table 9 in the Appendix), our multilingual data successfully improves low-resource languages as well. We did not fully explore the effect of adding additional languages to our synthetic data, especially lowresource ones. We believe that there is a tradeoff between language coverage and labeling quality. i.e, while generating the synthetic data in lowresource languages will increase language coverage, it can lead to poor labeling quality by our LLM. We did not fully explore the exact sweet-spot for how many languages to include in our synthetically labeled training data, leaving this for future work."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 FLAN-PaLM Prompt Design To apply FLAN-PaLM for factual consistency evaluation, we experimented with zero-shot, few-shot and chain-of-thought prompting strategies, and various formats for each strategy. We chose the best performing strategy and format, based on the accuracy on a development set.23 Table 8 presents the accuracy of each prompt type on the development set. We observed only minor performance differences, and thus we opted for the simplest solution that is the zero-shot prompt. While we cannot know the exact reasons for why few-shot and chain-ofthought did not improve performance, we can offer potential explanations. (1) Since the model was fine-tuned on NLI datasets, it is able to effectively generalize to factual consistency evaluation, making further demonstrations via few-shot prompting unnecessary in this case. (2) The performance with the zero-shot prompt is already notably high (89%, \u00a74.4) and thus our particular LLM is less likely to benefit from chain-of-thought prompting. (3) It could be the case that only a few reasoning steps are needed to evaluate consistency in our particular setup and thus chain-of-thought is not necessarily better in this case.\nBelow, we describe our top-performing zeroshot, few-shot and chain-of-thought prompts.\nZero-shot Prompt Since FLAN-PaLM was instruction fine-tuned on NLI, we designed our prompt to resemble an NLI prompt (e.g. using \"premise\" and \"hypothesis\" instead of \"document\" and \"summary\"). Our final prompt is as follows:\nPremise: {document} Hypothesis: {summary} Can the hypothesis be inferred from the premise? Answer using \"Yes\" or \"No\" only.\nFew-shot Prompt We use two few-shot examples, one \"consistent\" and one \"inconsistent\". We randomly sample these examples from the development set examples shorter than 200 words.23 We limit ourselves to two short examples since summarization examples can include long documents, and thus few-shot may lead to too long context length. Our final prompt is as follows:\n23For development set we use the FactCC dataset (Kryscinski et al., 2020) with 1,431 examples containing summaries of documents from CNN/DailyMail, manually annotated for factual correctness. Following (Utama et al., 2022), we merge the dev and test sets.\nPremise: (CNN) Desperate migrants from Africa and the Middle East keep heading to Europe, with 978 rescued Friday in the Mediterranean Sea, the Italian Coast Guard said Saturday via Twitter. The migrants were picked up 30 miles off the coast of Libya, said European Parliament member Matteo Salvini, the leader of Italy\u2019s far-right Northern League. In the first three months of 2015, Italy registered more than 10,000 migrants arriving, the International Organization for Migration said, and about 2,000 were rescued at sea during the first weekend of April in the Channel of Sicily. Most migrants recorded this year come from countries in West Africa as well as Somalia and Syria, the IMO said. They use Libya as a country of transit. At least 480 migrants have died while crossing the Mediterranean since the beginning of the year, often because of bad weather and overcrowded vessels used by smugglers, the IMO said. Sometimes the captains and crews abandon the ships, leaving passengers to fend for themselves. At this time last year, there were fewer than 50 deaths reported, the IMO said. Most of the migrants are asylum seekers, victims of trafficking or violence, unaccompanied children and pregnant women. Hypothesis: the migrants were picked up 30 miles off the coast of libya. Can the hypothesis be inferred from the premise? Answer using \"Yes\" or \"No\" only. Answer: Yes\nPremise: (CNN) A nuclear submarine being repaired at a Russian shipyard has caught on fire, according to a law enforcement source speaking to Russia\u2019s state-run news agency ITAR-Tass. \"The submarine is in a dry dock,\" Tass reports, citing the source, and there is no ammunition on board. \"The rubber insulation between the submarine\u2019s light and pressure hull is on fire,\" Tass reported. Russia\u2019s RIA Novosti news agency says insulation caught on fire as welding work was being done on the submarine. Tass reported that the fire began on a sub in the Zvyozdochka shipyard in northwestern Russia. Zvyozdochka spokesman Yevgeny Gladyshev told the news agency that the sub had been undergoing repairs since November 2013. \"Nuclear fuel from the sub\u2019s reactor has been unloaded,\" he reportedly said. \"There are no armaments or chemically active, dangerous substances, fissionable materials on it,\" Gladyshev said to Tass. \"The enterprise\u2019s personnel left the premises when the submarine caught fire, no one has been injured. The fire presents no threat to people and the shipyard.\" Hypothesis: \"the rubber insulation between the submarine\u2019s light and pressure hull is on fire,\" russia\u2019s ria novosti news agency says. Can the hypothesis be inferred from the premise? Answer using \"Yes\" or \"No\" only. Answer: No\nPremise: {document} Hypothesis: {summary} Can the hypothesis be inferred from the premise? Answer using \"Yes\" or \"No\" only. Answer:\nChain-of-thought Prompt Following Kojima et al. (2022) we append \"Let\u2019s think step by step\" to the prompt to facilitate a step-by-step reasoning before predicting the label. Our final prompt is as follows:\nPremise: {document} Hypothesis: {summary} Q: Can the hypothesis be inferred from the premise? Answer using \"Yes\" or \"No\" only. A: Let\u2019s think step by step\nThis prompt successfully unlocked a step by step reasoning. Below is FLAN-PaLM\u2019s response format, where {answer} is either \"yes\" or \"no\".\n{reasoning steps}. So, the answer is {answer}.\nExample input:\nPremise: (CNN) Georgia Southern University was in mourning Thursday after five nursing students were killed the day before in a multivehicle wreck near Savannah. Caitlyn Baggett, Morgan Bass, Emily Clark, Abbie Deloach and Catherine (McKay) Pittman \u2013 all juniors \u2013 were killed in the Wednesday morning crash as they were traveling to a hospital in Savannah, according to the school website. Fellow nursing students Brittney McDaniel and Megan Richards were injured as was another person, who was not identified by the Georgia State Patrol. The young women were on their way to finish their first set of clinical rotations ... ... ... Hypothesis: georgia southern university was in mourning after five nursing students died. Q: Can the hypothesis be inferred from the premise? Answer using \"Yes\" or \"No\" only. A: Let\u2019s think step by step\nThe output for this example is:\nGeorgia Southern University was in mourning Thursday after five nursing students were killed the day before in a multivehicle wreck near Savannah. So, the answer is yes.\nA.2 Inference with FLAN-PaLM\nWe used the zero-shot prompt (see \u00a7A.1). The vast majority of FLAN-PaLM\u2019s responses were either \"Yes\" or \"No\", and a tiny fraction of the responses were \"It\u2019s impossible to say\".\nDuring the labeling phase, we let FLAN-PaLM generate the output (predict mode), and label as \"consistent\" if the generated output is \"Yes\" and \"inconsistent\" in case the output is \"No\". We discard the \"It\u2019s impossible to say\" examples. In order to measure ROC-AUC in a binary classification setting, we compute the model\u2019s probability of generating \"Yes\" (score mode) and use it as the example-level factual consistency score.\nA.3 Fine tuning T5\nWe fine tune our T5 models for factual consistency evaluation using the following input format:\nPrompt type Dev accuracy\nzero-shot 93.6 few-shot 93.2 chain-of-thought 93.8\nThe model is trained to predict \"1\" if the summary is factually consistent and \"0\" otherwise. We use a learning rate of 10\u22124 and a batch size of 32. During training, we use a maximum input length of 512 tokens and truncate the premise if needed.24 During inference we use a maximum input length of 2048 tokens. We train for a maximum of 20 epochs, evaluate a checkpoint every 1k steps and choose the checkpoint with the best ROC-AUC on a development set.23 In our study we make sure to use the same training regime for all baselines.\nThe ANLI-only results in Table 3 are from our experiments, while in Table 2 we use the results reported in previous work.\nFor the summarization models we fine tune the corresponding T5 models on the XSum training set (Narayan et al., 2018) in a similar fashion and use the ROUGE score on the XSum development set as a stopping criteria.\nA.4 Additional Details About Our Dataset\nAs mentioned in \u00a73.1, we create the dataset based on documents from CNN/DailyMail (Hermann et al., 2015). We do not use the gold summaries, and we only use examples from the training set.\nIn our experiments with the full dataset (\u00a74.1), we balance the labels by randomly sampling 475,563 positive examples (see Table 1).\n24In early experiments we saw that training with longer maximum input length resulted with comparable performance.\nA.5 Data Filtering with Self-verification As mentioned in \u00a73 we also explored data filtering based on prompting FLAN-PaLM for selfverification. Our proccess is based on 3 steps. (1) Detect potential examples in our dataset that are likely to be labeled incorrectly by the LLM. (2) Prompt the LLM to self-verify its earlier prediction and filter out examples that the model is uncertain of. This leads to a smaller dataset with improved labeling accuracy. (3) Train the factual consistency evaluation model on the filtered dataset. This approach is based on 2 observations:\n1. In early experiments, we saw that our LLM has extremely high precision for the inconsistent class. This can also be seen in our human evaluation (Table 4). This means that almost all the errors occur when the LLM predicts that the summary is consistent. Following this, we only consider filtering examples classified as consistent by the LLM.\n2. Inspired by the work of Weng et al. (2023) and Madaan et al. (2023), we use a self verification prompt. If the LLM classified the summary as consistent, we prompt it again and ask it for its certainty. If the answer is \u201cYes\u201d (i.e. it is consistent with the original reasoning path), we keep the example, otherwise we filter it out. This proccess is illustrated in Figure 4.\nThe self-verification prompt is as follows:\nPremise: {document} Hypothesis: {summary} Are you sure that the summary can be inferred from the document? Answer using \"Yes\" or \"No\" only.\nThis approach filtered-out 15% of the dataset.\nWhen we qualitatively analyzed the filtered examples, it seems that the majority of the filtered examples indeed had a wrong label, and that applying this filtering mechanism increases the labeling accuracy by approximately 5%.\nWhile this filtering mechanism results in higher labeling accuracy, we did not observe a performance gain when filtering the training data in this way. For TrueTeacher + ANLI with T5-11B (on a sample of 100k examples) we got an average of 86 ROC-AUC on TRUE using the filtered data, slightly below the 86.4 using the unfiltered data (Table 3). As mentioned in Footnote 9, we attribute this to the fact that the labeling accuracy is high to begin with (89%, section 4.4) and that the model is likely robust to some amount of labeling noise. Following this, for simplicity, our official method does not use filtering.\nA.6 Abstractiveness Analysis: Additional Details\nAs our backbone metrics we use the Extractive Fragment Coverage and Density measures defined by Grusky et al. (2018). Coverage measures the percentage of words in the summary that are part of an extractive fragment with the article, quantifying the extent to which a summary is derivative of a text. Density measures the average length of the extractive fragment to which each word in the summary belongs, quantifying how well the word sequence of a summary can be described as a series of extractions. Our Combined score is obtained by multiplyng the Coverage and the Density scores, similar to Utama et al. (2022). To further illustrated the differences in the abstractiveness of different methods, we include a visualization of the density of the combined abstractivness score in Figure 5.\nA.7 Using the mFace dataset\nIn \u00a75 we report results on the mFace dataset (Aharoni et al., 2022). Aharoni et al. performed large scale human evaluation of summaries of documents from the XLSum corpus (Hasan et al., 2021), produced by different summarization models. Each summary was rated for quality, attribution and informativeness. We use the attribution scores in our work. The attribution evaluation is based on the attribution definition provided in Rashkin et al. (2021), with the participants asked \"Is all the information in the summary fully attributable to the\narticle?\". In our work we use the average attribution score (between 0 to 1) and treat summaries as factually consistent if the score is larger than 0.5. We focus on the test split of XLSum containing 3150 examples in 45 languages (i.e., 70 examples in each language). In \u00a75 we refer to Table 7 with the results overview, and we provide the full results for all languages in Table 10.\nA.8 Human Evaluation We instructed the participants to review the document and its corresponding summary, and to evaluate the summary based on the attribution definition provided by Rashkin et al. (2021), using binary judgements. To avoid a common confusion between factual inconsistency and contradiction, we also provided the following instruction:\nIn this task you will evaluate the factual consistency of a system-generated summary. The system\u2019s goal is to summarize the original source document, while remaining truthful to it. Your goal is to evaluate whether the system-generated summary is consistent w.r.t. the source document. Summary will be considered consistent if all of the information in the summary can be verified from the source document (i.e., for the summary to be inconsistent, the document does not necessarily need to contradict it, it can also fail to support some facts).\nIn an early experiment, we found that using crowd workers without domain expertise and substantial time investments resulted in extremely lowquality ratings. Following this, all our raters were NLP researchers, each with at least one year of specific experience in the task of factual consistency evaluation, with significant time allocation and no more than 10 examples per rater.25 These steps ensured high quality ratings.\nA.9 Adding noise to TrueTeacher In \u00a74.5 we create SummaryAblation by flipping labels to a random portion of TrueTeacher\u2019s data, such that the expected labeling accuracy is similar to Falsesum. Falsesum\u2019s labeling method is coupled with the data generation, thus we need an approximation for its labeling quality. We estimate Falesum\u2019s labeling accuracy as 83.5%, according to Utama et al. (2022)\u2019s human evaluation (we average the Intrinsic and Extrinsic results), while ours is 89% (\u00a74.4). So to mimic Falsesum\u2019s quality we flipped TrueTeacher\u2019s labels in order to add additional 5.5% errors.\n25We found that it is sufficient to use one rater per example (unlike in our experiments with the crowd workers)."
        }
    ],
    "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
    "year": 2023
}