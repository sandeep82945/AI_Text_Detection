{
    "abstractText": "Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced MTAM, a Multimodal Transformer Alignment Model, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide interpretations of the performance improvement: (1) feature distribution shows the effectiveness of the alignment module for discovering and encoding the relationship between EEG and language; (2) alignment weights show the influence of different language semantics as well as EEG frequency features; (3) brain topographical maps provide an intuitive demonstration of the connectivity in the brain regions. Our code is available at https://github.com/ Jason-Qiu/EEG_Language_Alignment.",
    "authors": [
        {
            "affiliations": [],
            "name": "William Han"
        },
        {
            "affiliations": [],
            "name": "Jielin Qiu"
        },
        {
            "affiliations": [],
            "name": "Jiacheng Zhu"
        },
        {
            "affiliations": [],
            "name": "Mengdi Xu"
        },
        {
            "affiliations": [],
            "name": "Douglas Weber"
        },
        {
            "affiliations": [],
            "name": "Bo Li"
        },
        {
            "affiliations": [],
            "name": "Ding Zhao"
        }
    ],
    "id": "SP:fd62c4acf638ce13fa0ae55d1c8a236bd827f2e7",
    "references": [
        {
            "authors": [
                "Bao-Liang Lu"
            ],
            "title": "Investigating sex",
            "year": 2019
        },
        {
            "authors": [
                "Khayrul Bashar"
            ],
            "title": "Ecg and eeg based multimodal",
            "year": 2018
        },
        {
            "authors": [
                "Rhee"
            ],
            "title": "Improving bi-encoder document ranking",
            "year": 2021
        },
        {
            "authors": [
                "J. Correia",
                "E. Formisano",
                "G. Valente",
                "L. Hausfeld",
                "B. Jansma",
                "M. Bonte."
            ],
            "title": "Brain-based translation: fmri decoding of spoken words in bilinguals reveals language-independent semantic representations in anterior temporal lobe",
            "venue": "Journal of Neuroscience,",
            "year": 2013
        },
        {
            "authors": [
                "Aron Culotta",
                "Andrew McCallum",
                "Jonathan Betz."
            ],
            "title": "Integrating probabilistic extraction models and data mining to discover relations and patterns in text",
            "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages",
            "year": 2006
        },
        {
            "authors": [
                "Pinar Demetci",
                "Rebecca Santorella",
                "Bj\u00f6rn Sandstede",
                "William Stafford Noble",
                "Ritambhara Singh."
            ],
            "title": "Gromov-wasserstein optimal transport to align singlecell multi-omics data",
            "venue": "bioRxiv.",
            "year": 2020
        },
        {
            "authors": [
                "Fatma Deniz",
                "Christine Tseng",
                "Leila Wehbe",
                "Jack L. Gallant."
            ],
            "title": "Semantic representations during language comprehension are affected by context",
            "venue": "bioRxiv.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805.",
            "year": 2019
        },
        {
            "authors": [
                "Vipula Dissanayake",
                "Sachith Seneviratne",
                "Rajib Rana",
                "Elliott Wen",
                "Tharindu Kaluarachchi",
                "Suranga Nanayakkara."
            ],
            "title": "Sigrep: Toward robust wearable emotion recognition with contrastive representation learning",
            "venue": "IEEE Access, 10:18105\u201318120.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Du",
                "Yongling Xu",
                "Xiaoan Wang",
                "Li Liu",
                "Pengcheng Ma"
            ],
            "title": "Etst: Eeg transformer for person identification",
            "year": 2022
        },
        {
            "authors": [
                "Javier Fdez",
                "Nicholas Guttenberg",
                "Olaf Witkowski",
                "Antoine Pasquali."
            ],
            "title": "Cross-subject eeg-based emotion recognition through neural networks with stratified normalization",
            "venue": "Frontiers in Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Chris Foster",
                "Chad C. Williams",
                "Olave E. Krigolson",
                "Alona Fyshe"
            ],
            "title": "Using eeg to decode semantics during an artificial language learning",
            "venue": "task. Brain and Behavior,",
            "year": 2021
        },
        {
            "authors": [
                "Jon Gauthier",
                "Anna A. Ivanova."
            ],
            "title": "Does the brain represent words? an evaluation of brain decoding studies of language understanding",
            "venue": "ArXiv, abs/1806.00591.",
            "year": 2018
        },
        {
            "authors": [
                "Yunchao Gong",
                "Qifa Ke",
                "Michael Isard",
                "Svetlana Lazebnik."
            ],
            "title": "A multi-view embedding space for modeling internet images, tags, and their semantics",
            "venue": "International Journal of Computer Vision, 106:210\u2013 233.",
            "year": 2013
        },
        {
            "authors": [
                "Alex Graves",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
            "venue": "Neural networks : the official journal of the International Neural Network Society, 18 5-6:602\u201310.",
            "year": 2005
        },
        {
            "authors": [
                "Jiang-Jian Guo",
                "Rong Zhou",
                "Li-Ming Zhao",
                "BaoLiang Lu"
            ],
            "title": "Multimodal emotion recognition from eye image, eye movement and eeg using deep neural networks",
            "year": 2019
        },
        {
            "authors": [
                "John Hale",
                "Adhiguna Kuncoro",
                "Keith B. Hall",
                "Chris Dyer",
                "Jonathan Brennan."
            ],
            "title": "Text genre and training data size in human-like parsing",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "X. Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Maria Barrett",
                "Lisa Beinborn."
            ],
            "title": "Towards best practices for leveraging human language processing signals for natural language processing",
            "venue": "LINCR.",
            "year": 2020
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Maria Barrett",
                "Marius Troendle",
                "Francesco Bigiolli",
                "Nicolas Langer",
                "Ce Zhang."
            ],
            "title": "Advancing nlp with cognitive language processing signals",
            "venue": "ArXiv, abs/1904.02682.",
            "year": 2019
        },
        {
            "authors": [
                "Nora Hollenstein",
                "C\u00e9dric Renggli",
                "Benjamin James Glaus",
                "Maria Barrett",
                "Marius Troendle",
                "Nicolas Langer",
                "Ce Zhang."
            ],
            "title": "Decoding eeg brain activity for multi-modal natural language processing",
            "venue": "Frontiers in Human Neuroscience, 15.",
            "year": 2021
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Jonathan Rotsztejn",
                "Marius Troendle",
                "Andreas Pedroni",
                "Ce Zhang",
                "Nicolas Langer."
            ],
            "title": "Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading",
            "venue": "Scientific Data,",
            "year": 2018
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Marius Troendle",
                "Ce Zhang",
                "Nicolas Langer"
            ],
            "title": "2020b. Zuco 2.0: A dataset of physiological recordings during natural reading and annotation. arXiv:1912.00903 [cs",
            "year": 1912
        },
        {
            "authors": [
                "Shenda Hong",
                "Yanbo Xu",
                "Alind Khare",
                "Satria Priambada",
                "Kevin Maher",
                "Alaa Aljiffry",
                "Jimeng Sun",
                "Alexey Tumanov."
            ],
            "title": "Holmes: Health online model ensemble serving for deep learning models in intensive care units",
            "venue": "Proceedings of the 26th ACM",
            "year": 2020
        },
        {
            "authors": [
                "Shih-Cheng Huang",
                "Anuj Pareek",
                "Roham Zamanian",
                "Imon Banerjee",
                "Matthew P. Lungren."
            ],
            "title": "Multimodal fusion with deep neural networks for leveraging ct imaging and electronic health record: a casestudy in pulmonary embolism detection",
            "venue": "Scientific",
            "year": 2020
        },
        {
            "authors": [
                "Lorenz H\u00fcbschle-Schneider",
                "Peter Sanders."
            ],
            "title": "Parallel weighted random sampling",
            "venue": "ACM Transactions on Mathematical Software (TOMS).",
            "year": 2019
        },
        {
            "authors": [
                "Elizabeth A. Kensinger."
            ],
            "title": "Remembering the details: Effects of emotion",
            "venue": "Emotion Review, 1:113 \u2013 99.",
            "year": 2009
        },
        {
            "authors": [
                "John Lee",
                "Max Dabagia",
                "Eva L. Dyer",
                "Christopher J. Rozell."
            ],
            "title": "Hierarchical optimal transport for multimodal distribution alignment",
            "venue": "ArXiv, abs/1906.11768.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Mu Li",
                "Bao-Liang Lu."
            ],
            "title": "Emotion classification based on gamma-band eeg",
            "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,",
            "year": 2009
        },
        {
            "authors": [
                "Wei Liu",
                "Jie-Lin Qiu",
                "Wei-Long Zheng",
                "BaoLiang Lu."
            ],
            "title": "Multimodal emotion recognition using deep canonical correlation analysis",
            "venue": "ArXiv, abs/1908.05349.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Liu",
                "Jie-Lin Qiu",
                "Wei-Long Zheng",
                "Bao-Liang Lu."
            ],
            "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
            "venue": "IEEE Transactions on Cognitive and Developmental Systems, 14:715\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Murphy",
                "Bernd Bohnet",
                "Ryan T. McDonald",
                "Uta Noppeney."
            ],
            "title": "Decoding part-of-speech from human eeg signals",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Grigorios Nasios",
                "Efthymios Dardiotis",
                "Lambros Messinis."
            ],
            "title": "From broca and wernicke to the neuromodulation era: Insights of brain language networks for neurorehabilitation",
            "venue": "Behavioural Neurology, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Ole Numssen",
                "Danilo Bzdok",
                "Gesa Hartwigsen."
            ],
            "title": "Functional specialization within the inferior parietal lobes across cognitive domains",
            "venue": "eLife, 10.",
            "year": 2021
        },
        {
            "authors": [
                "Pablo Ortega",
                "A. Aldo Faisal."
            ],
            "title": "Deep learning multimodal fnirs and eeg signals for bimanual grip force decoding",
            "venue": "Journal of Neural Engineering, 18.",
            "year": 2021
        },
        {
            "authors": [
                "Cheul Young Park",
                "Narae Cha",
                "Soowon Kang",
                "Auk Kim",
                "Ahsan Habib Khandoker",
                "Leontios Hadjileontiadis",
                "Alice Oh",
                "Yong Jeong",
                "Uichin Lee"
            ],
            "title": "K-emocon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
            "year": 2020
        },
        {
            "authors": [
                "Jie-Lin Qiu",
                "W. Liu",
                "Bao-Liang Lu."
            ],
            "title": "Multiview emotion recognition using deep canonical correlation analysis",
            "venue": "ICONIP.",
            "year": 2018
        },
        {
            "authors": [
                "Jielin Qiu",
                "Jiacheng Zhu",
                "Mengdi Xu",
                "Franck Dernoncourt",
                "Trung Bui",
                "Zhaowen Wang",
                "Bo Li",
                "Ding Zhao",
                "Hailin Jin."
            ],
            "title": "Mhms: Multimodal hierarchical multimedia summarization",
            "venue": "ArXiv, abs/2204.03734.",
            "year": 2022
        },
        {
            "authors": [
                "Jingyu Quan",
                "Yoshihiro Miyake",
                "Takayuki Nozawa."
            ],
            "title": "Incorporating interpersonal synchronization features for automatic emotion recognition from visual and audio data during communication",
            "venue": "Sensors, 21:5317.",
            "year": 2021
        },
        {
            "authors": [
                "Aniketh Janardhan Reddy",
                "Leila Wehbe"
            ],
            "title": "Can fmri reveal the representation of syntactic structure in the brain? bioRxiv",
            "year": 2021
        },
        {
            "authors": [
                "Amna Rehman",
                "Yasir Al Khalili"
            ],
            "title": "Neuroanatomy, occipital lobe",
            "year": 2019
        },
        {
            "authors": [
                "David Ruppert."
            ],
            "title": "The elements of statistical learning: Data mining, inference, and prediction",
            "venue": "Journal of the American Statistical Association, 99:567 \u2013 567.",
            "year": 2004
        },
        {
            "authors": [
                "Enas S. Yousif",
                "Azmi Shawkat Abdulbaqi",
                "Abduladheem Zaily Hameed",
                "Saif Al-din M.N."
            ],
            "title": "Electroencephalogram signals classification based on feature normalization",
            "venue": "IOP Conference Series: Materials Science and Engineering, 928:032028.",
            "year": 2020
        },
        {
            "authors": [
                "Maham Saeidi",
                "Waldemar Karwowski",
                "Farzad Vasheghani Farahani",
                "Krzysztof Fiok",
                "Redha Taiar",
                "Peter A. Hancock",
                "Awad Al-Juaid."
            ],
            "title": "Neural decoding of eeg signals with machine learning: A systematic review",
            "venue": "Brain Sciences, 11.",
            "year": 2021
        },
        {
            "authors": [
                "Dan Schwartz",
                "Mariya Toneva",
                "Leila Wehbe."
            ],
            "title": "Inducing brain-relevant bias in natural language processing models",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Brigitte Stemmer",
                "John Connolly"
            ],
            "title": "The eeg/erp technologies in linguistic research",
            "year": 2012
        },
        {
            "authors": [
                "Mariya Toneva",
                "Tom. Mitchell",
                "Leila Wehbe."
            ],
            "title": "Combining computational controls with natural text reveals new aspects of meaning composition",
            "venue": "bioRxiv.",
            "year": 2020
        },
        {
            "authors": [
                "Mariya Toneva",
                "Leila Wehbe."
            ],
            "title": "Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)",
            "venue": "ArXiv, abs/1905.11833.",
            "year": 2019
        },
        {
            "authors": [
                "Zhenhailong Wang",
                "Heng Ji."
            ],
            "title": "Open vocabulary electroencephalography-to-text decoding and zeroshot sentiment classification",
            "venue": "ArXiv, abs/2112.02690.",
            "year": 2021
        },
        {
            "authors": [
                "Leila Wehbe",
                "Idan Asher Blank",
                "Cory Shain",
                "Richard Futrell",
                "Roger Philip Levy",
                "Titus von der Malsburg",
                "Nathaniel J. Smith",
                "Edward Gibson",
                "Evelina Fedorenko"
            ],
            "title": "Incremental language comprehension difficulty predicts activity in the language network",
            "year": 2020
        },
        {
            "authors": [
                "Leila Wehbe",
                "Ashish Vaswani",
                "Kevin Knight",
                "Tom Mitchell"
            ],
            "title": "Aligning context-based statistical models of language with brain activity",
            "year": 2014
        },
        {
            "authors": [
                "Jennifer Williams",
                "Leila Wehbe."
            ],
            "title": "Behavior measures are predicted by how information is encoded in an individual\u2019s brain",
            "venue": "ArXiv, abs/2112.06048.",
            "year": 2021
        },
        {
            "authors": [
                "Kangning Yang",
                "Benjamin Tag",
                "Yue Gu",
                "Chaofan Wang",
                "Tilman Dingler",
                "Greg Wadley",
                "Jorge Goncalves."
            ],
            "title": "Mobile emotion recognition via multiple physiological signals using convolution-augmented transformer",
            "venue": "Proceedings of the 2022 International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime G. Carbonell",
                "Ruslan Salakhutdinov",
                "Quoc V. Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Siyang Yuan",
                "Ke Bai",
                "Liqun Chen",
                "Yizhe Zhang",
                "Chenyang Tao",
                "Chunyuan Li",
                "Guoyin Wang",
                "Ricardo Henao",
                "Lawrence Carin."
            ],
            "title": "Advancing weakly supervised cross-domain alignment with optimal transport",
            "venue": "ArXiv, abs/2008.06597.",
            "year": 2020
        },
        {
            "authors": [
                "P. Zhou",
                "Wei Shi",
                "Jun Tian",
                "Zhenyu Qi",
                "Bingchen Li",
                "Hongwei Hao",
                "Bo Xu."
            ],
            "title": "Attention-based bidirectional long short-term memory networks for relation classification",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational",
            "year": 2016
        },
        {
            "authors": [
                "Jiacheng Zhu",
                "Jielin Qiu",
                "Zhuolin Yang",
                "Douglas Weber",
                "Michael A. Rosenberg",
                "Emerson Liu",
                "Bo Li",
                "Ding Zhao."
            ],
            "title": "Geoecg: Data augmentation via wasserstein geodesic perturbation for robust electrocardiogram prediction",
            "venue": "MLHC.",
            "year": 2022
        },
        {
            "authors": [
                "Hollenstein"
            ],
            "title": "The train/test/validation splitting is (80",
            "venue": "Yousif et al.,",
            "year": 2021
        },
        {
            "authors": [
                "ACC BVP",
                "EDA",
                "NA TEMP"
            ],
            "title": "Convolution-augmented Transformer (Yang et al., 2022) BCP, EDA, HR, SKT",
            "venue": "NA Transformer (Yang et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2019) extracted correlated features between EEG and eye movement data for emotion classification, showing transformed features are more homogeneous and discriminative. Guo et al. (2019) collected eye images, eye movement data, and EEG signals, and encoded the three modalities with a bimodal deep autoencoder",
            "year": 2019
        },
        {
            "authors": [
                "2020a). Wehbe"
            ],
            "title": "used a recurrent neural network to perform word alignment between MEG activity and the generated word embeddings. Toneva and Wehbe (2019) utilized word-level MEG and fMRI recordings to compare word embeddings from large language models. Schwartz et al. (2019) used MEG and fMRI data to fine-tune a BERT language model (Devlin et al., 2019",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Brain activity is an important parameter in furthering our knowledge of how human language is represented and interpreted (Toneva et al., 2020; Williams and Wehbe, 2021; Reddy and Wehbe, 2021; Wehbe et al., 2020; Deniz et al., 2021). Researchers from domains such as linguistics, psychology, cognitive science, and computer science\n\u2217*marked as equal contribution\nhave made large efforts in using brain-recording technologies to analyze cognitive activity during language-related tasks and observed that these technologies added value in terms of understanding language (Stemmer and Connolly, 2012).\nBasic linguistic rules seem to be effortlessly understood by humans in contrast to machinery. Recent advances in natural language processing (NLP) models (Vaswani et al., 2017) have enabled computers to maintain long and contextual information through self-attention mechanisms. This attention mechanism has been maneuvered to create robust language models but at the cost of tremendous amounts of data (Devlin et al., 2019; Liu et al., 2019b; Lewis et al., 2020; Brown et al., 2020; Yang et al., 2019). Although performance has significantly improved by using modern NLP models, they are still seen to be suboptimal compared to the human brain. In this study, we explore the relationship and dependencies of EEG and language. We apply EEG, a popularized routine in cognitive research, for its accessibility and practicality, along with language to discover connectivity.\nOur contributions are summarized as follows:\n\u2022 To the best of our knowledge, this is the first work to explore the fundamental relationship and connectivity between EEG and language through computational multimodal methods.\n\u2022 We introduced MTAM, a Multimodal Transformer Alignment Model, that learns coordinated representations by hierarchical transformer encoders. The transformed representations showed tremendous performance improvements and state-of-the-art results in downstream applications, i.e., sentiment analysis and relation detection, on two datasets, ZuCo 1.0/2.0 and K-EmoCon.\n\u2022 We carried out experiments with multiple alignment mechanisms, i.e., canonical correlation analysis and Wasserstein distance, and\nproved that relation-seeking loss functions are helpful in downstream tasks.\n\u2022 We provided interpretations of the performance improvement by visualizing the original & transformed feature distribution, showing the effectiveness of the alignment module for discovering and encoding the relationship between EEG and language.\n\u2022 Our findings on word-level and sentence-level EEG-language alignment showed the influence of different language semantics as well as EEG frequency features, which provided additional explanations.\n\u2022 The brain topographical maps delivered an intuitive demonstration of the connectivity of EEG and language response in the brain regions, which issues a physiological basis for our discovery."
        },
        {
            "heading": "2 Related Work",
            "text": "Multimodal Learning of Language and Other Brain Signals Wehbe et al. (2014) used a recurrent neural network to perform word alignment between MEG activity and the generated word embeddings. Toneva and Wehbe (2019) utilized wordlevel MEG and fMRI recordings to compare word embeddings from large language models. Schwartz et al. (2019) used MEG and fMRI data to fine-tune a BERT language model (Devlin et al., 2019) and found the relationships between these two modalities were generalized across participants. Huang et al. (2020) leveraged CT images and text from electronic health records to classify pulmonary embolism cases and observed that the multimodal model with late fusion achieved the best performance. However, the relationship between language and EEG has not been explored before."
        },
        {
            "heading": "Multimodal Learning of EEG and Language",
            "text": "Foster et al. (2021) applied EEG signals to pre-\ndict specific values of each dimension in a word vector through regression models. Wang and Ji (2021) used word-level EEG features to decode corresponding text tokens through an open vocabulary, sequence-to-sequence framework. Hollenstein et al. (2021) focused on a multimodal approach by utilizing a combination of EEG, eye-tracking, and text data to improve NLP tasks, but did not explore the relationship between EEG and language. More related work can be found in Appendix E."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 Overview of Model Architecture",
            "text": "The architecture of our model is shown in Fig. 1. The bi-encoder architecture is helpful in projecting embeddings into vector space for methodical analysis (Liu et al., 2019a; Hollenstein et al., 2021; Choi et al., 2021). Thus in our study, we adopt the bi-encoder approach to effectively reveal hidden relations between language and EEG. The MTAM, Multimodal Transformer Alignment Model, contains several modules. We use a dual-encoder architecture, where each view contains hierarchical transformer encoders. The inputs of each encoder are EEG and language, respectively. For EEG hierarchical encoders, each encoder shares the same architecture as the encoder module in Vaswani et al. (2017). In the current literature, researchers assume that the brain acts as an encoder for highdimensional semantic representations (Wang and Ji, 2021; Gauthier and Ivanova, 2018; Correia et al., 2013). Based on this assumption, the EEG signals act as low-level embeddings. By feeding it into its respective hierarchical encoder, we extract transformed EEG embeddings as input for the cross alignment module. As for the language path, the language encoder is slightly different from the EEG encoder. We first process the text with a pretrained large language model (LLM) to extract text em-\nbeddings and then use hierarchical transformer encoders to transform the raw text embeddings into high-level features. The mechanism of the cross alignment module is to explore the inner relationship between EEG and language through a connectivity-based loss function. In our study, we investigate two alignment methods, i.e., Canonical Correlation Analysis (CCA) and Wasserstein Distance (WD). The output features from the cross alignment module can be used for downstream applications. The details of each part are introduced in Appendix B.3."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Results and Discussions",
            "text": "In this study, we evaluate our method on two downstream tasks: Sentiment Analysis (SA) and Relation Detection (RD) of two datasets: K-EmoCon (Park et al., 2020) and ZuCo 1.0/2.0 Dataset (Hollenstein et al., 2018, 2020b). Given a succession of word-level or sentence-level EEG features and their corresponding language, Sentiment Analysis (SA) task aims to predict the sentiment label. For Relation Detection (RD), the goal is to extract semantic relations between entities in a given text. More details about the tasks, data processing, and experimental settings can be found in Appendix C.\nIn Table 1, we show the comparison results of the ZuCo dataset for Sentiment Analysis and Relation Detection, respectively. Our method outperforms all baselines, and the multimodal approach outperforms unimodal approaches, which further demonstrates the importance of exploring the inner\nalignment between EEG and language. The results of the K-EmoCon dataset are listed in Appendix D"
        },
        {
            "heading": "4.2 Ablation Study",
            "text": "To further investigate the performance of different mechanisms in the CAM, we carried out ablation experiments on the Zuco dataset, and the results are shown in Table 6 in Appendix D.2. The combination of CCA and WD performed better compared to using only one mechanism for sentiment analysis and relation detection in all model settings. We also conducted experiments on word-level, sentencelevel, and concat word-level inputs, and the results are also shown in Table 6. We observe that word-level EEG features paired with their respective word generally outperform sentence-level and concat word-level in both tasks."
        },
        {
            "heading": "4.3 Analysis",
            "text": "To understand the alignment between language and EEG, we visualize the alignment weights of wordlevel EEG-language alignment on the ZuCo dataset. Fig. 2 and Fig. 3 show examples of negative & positive sentence word-level alignment, respectively. The sentence-level alignment visualizations are shown in Appendix D.5.\nFrom word level alignment in Fig. 2 and 3, beta2 and gamma1 waves are most active. This is consistent with the literature, which showed that gamma waves are seen to be active in detecting emotions (Li and Lu, 2009) and beta waves have been involved in higher-order linguistic functions (e.g., discrimination of word categories). Hollenstein et al.\n(2021) found that beta and theta waves were most useful in terms of model performance in sentiment analysis. In Kensinger (2009), Kensinger explained that generally, negative events are more likely to be remembered than positive events. Building off of Kensinger (2009), negative words can embed a more significant and long-lasting memory than positive words, and thus may have higher activation in the occipital and inferior parietal lobes.\nWe performed an analysis of which EEG feature refined the model\u2019s performance since different neurocognitive factors during language processing are associated with brain oscillations at miscellaneous frequencies. The beta and theta bands have positively contributed the most, which is due to the theta band power expected to rise with increased language processing activity and the band\u2019s relation to semantic memory retrieval (Kosch et al., 2020; Hollenstein et al., 2021). The beta\u2019s contribution can be best explained by the effect of emotional connotations of the text (Bastiaansen et al., 2005; Hollenstein et al., 2021).\nIn Fig. 4, we visualized the brain topologies with word-level EEG features for important and unimportant words from positive and negative sentences in the ZuCo dataset. We deemed a word important if the definition had a positive or negative connotation. \u2018Upscale\u2019 and \u2018lame\u2019 are important positive and negative words, respectively, while \u2018will\u2019 and \u2018someone\u2019 are unimportant positive and negative words, respectively. There are two areas in the brain that are heavily associated with language processing: Broca\u2019s area and Wernicke\u2019s area. Broca\u2019s area is assumed to be located in the left frontal lobe, and this region is concerned with the production of speech (Nasios et al., 2019). The left posterior superior temporal gyrus is typically assumed as Wernicke\u2019s area, and this locale is involved with the comprehension of speech (Nasios et al., 2019).\nSimilar to Fig. 2,3, we can observe that beta2, gamma1, and gamma2 frequency bands have the most powerful signals for all words. In Fig. 4, ac-\ntivity in Wernicke\u2019s area is seen most visibly in the beta2, gamma1, and gamma2 bands for the words \u2018Upscale\u2019 and \u2018Will\u2019. For the word \u2018Upscale,\u2019 we also saw activity around Broca\u2019s area for alpha1, alpha2, beta1, beta2, theta1, and theta2 bands. An interesting observation is that for the negative words, \u2018Lame\u2019 and \u2018Someone\u2019, we see very low activation in Broca\u2019s and Wernicke\u2019s areas. Instead, we see most activity in the occipital lobes and slightly over the inferior parietal lobes. The occipital lobes are noted as the visual processing area of the brain and are associated with memory formation, face recognition, distance and depth interpretation, and visuospatial perception (Rehman and Khalili, 2019). The inferior parietal lobes are generally found to be key actors in visuospatial attention and semantic memory (Numssen et al., 2021)."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this study, we explore the relationship between EEG and language. We propose MTAM, a Multimodal Transformer Alignment Model, to observe coordinated representations between the two modalities and employ the transformed representations for downstream applications. Our method achieved state-of-the-art performance on sentiment analysis and relation detection tasks on two public datasets, ZuCo and K-EmoCon. Furthermore, we carried out a comprehensive study to analyze the connectivity and alignment between EEG and language. We observed that the transformed features show less randomness and sparsity. The word-level language-EEG alignment clearly demonstrated the importance of the explored connectivity. We also provided brain topologies as an intuitive understanding of the corresponding activity regions in the brain, which could build the empirical neuropsychological basis for understanding the relationship between EEG and language through computational models."
        },
        {
            "heading": "6 Limitations",
            "text": "Since we proposed a new task of exploring the relationship between EEG and language, we believe there are several limitations that can be focused on in future work.\n\u2022 The size of the datasets may not be large enough. Due to the difficulty and timeconsumption of collecting human-related data (in addition, to privacy concerns), there are few publicly available datasets that have EEG recordings with corresponding natural language. When compared to other mature tasks, (i.e. image classification, object detection, etc), datasets that have a combination of EEG signals and different modalities are rare. In the future, we would like to collect more data on EEG signals with natural language to enhance innovation in this direction.\n\u2022 The computational architecture, the MTAM model, is relatively straightforward. We agree the dual-encoder architecture is one of the standard paradigms in multimodal learning. Since our target is to explore the connectivity and relationship between EEG and language, we used a straightforward paradigm. Our model\u2019s architecture may be less complex compared to others in different tasks, such as image-text pre-training. However, we purposely avoid complicating the model\u2019s structure due to the size of the training data. We noticed when adding more layers of complexity, the model was more prone to overfitting.\n\u2022 The literature lacks available published baselines. As shown in our paper, since the task is new, there are not enough published works that provide comparable baselines. We understand that the comparison is important, so we implemented several baselines by ourselves, including MLP, Bi-LSTM, Transformer, and ResNet, to provide more convincing judgment and support future work in this area."
        },
        {
            "heading": "7 Ethics Statement",
            "text": "The goal of our study is to explore the connectivity between EEG and language, which involves human subjects\u2019 data and may inflect cognition in the brain, so we would like to provide an ethics discussion.\nFirst, all the data used in our paper are publicly available datasets: K-EmoCon and Zuco. We did\nnot conduct any human-involved experiments by ourselves. Additionally, we do not implement any technologies on the human brain. The datasets can be found in Park et al. (2020); Hollenstein et al. (2018, 2020b)\nWe believe this study can empirically provide findings about the connection between natural language and the human brain. To our best knowledge, we do not foresee any harmful uses of this scientific study."
        },
        {
            "heading": "A Three paradigms of EEG and language alignment",
            "text": ""
        },
        {
            "heading": "B More Details about our Model",
            "text": ""
        },
        {
            "heading": "B.1 Hierarchical Transformer Encoders",
            "text": "Let Xe \u2208 RDe and Xt \u2208 RDt be the two normalized input feature matrices for EEG and text, respectively, where De and Dt describes the dimensions of the feature matrices. To encode the two feature vectors, we feed them to their hierarchical transformer encoders: Ve = Ee(Xe;We);Vt = Et(Xt;Wt), where Ee and Et denotes the separate encoders, Ve and Vt symbolizes the outputs for the transformed low-level features and We and Wt denotes the trainable weights for EEG and text respectively. The outputs of these two encoders can be further expanded by stating Ve = [v1e , v 2 e , v 3 e , ..., v n e ] \u2208 Rn and Vt = [v1t , v2t , v3t , ..., vkt ] \u2208 Rk, where n and k denotes the number of instances in a given output vector and vne and vkt denotes the instance itself. The details about Transformer encoders are introduced in the section below."
        },
        {
            "heading": "B.2 Transformer Encoders",
            "text": "The transformer is based on the attention mechanism and outperforms previous models in accuracy and performance. The original transformer model is composed of an encoder and a decoder. The encoder maps an input sequence into a latent representation, and the decoder uses the representation along with other inputs to generate a target sequence. Our model only adopts the encoder, since we aim at learning the representations of features.\nFirst, we feed out the input into an embedding layer, which is a learned vector representation. Then we inject positional information into the embeddings by:\nPE(pos,2i) = sin ( pos/100002i/dmodel ) , PE(pos,2i+1) = cos ( pos/100002i/dmodel ) (1)\nThe attention model contains two sub-modules, a multi-headed attention model and a fully connected network. The multi-headed attention computes the attention weights for the input and produces an output vector with encoded information on how each feature should attend to all other features in the sequence. There are residual connections around each of the two sub-layers followed by a layer normalization, where the residual connection means adding the multi-headed attention output vector to the original positional input embedding, which helps the network train by allowing gradients to flow through the networks directly. Multi-headed attention applies a self-attention mechanism, where the input goes into three distinct fully connected layers to create the query, key, and value vectors. The output of the residual connection goes through layer normalization.\nIn our model, our attention model contains N same layers, and each layer contains two sub-layers, which are a multi-head self-attention model and a fully connected feed-forward network. Residual connection and normalization are added in each sub-layer. So the output of the sub-layer can be expressed as: Output = LayerNorm(x+ (SubLayer(x))), For the Multi-head self-attention module, the attention can be expressed as: attention = Attention(Q,K, V ), where multi-head attention uses h different linear transformations to project query, key, and value, which are Q, K, and V , respectively, and finally\nconcatenate different attention results:\nMultiHead(Q,K,V) = Concat(head1, ..., headh)WO (2)\nheadi = Attention(QW Q i ,KW K i , V W V i ) (3)\nwhere the projections are parameter matrices:\nWQi \u2208 R dmodel dk , WKi \u2208 Rdmodel dk ,W Vi \u2208 Rdmodel dv , WOi \u2208 Rhdv\u00d7dmodel (4)\nwhere the computation of attention adopted scaled dot-product: Attention(Q,K, V ) = softmax(QK T\n\u221a dk )V\nFor the output, we use a 1D convolutional layer and softmax layer to calculate the final output."
        },
        {
            "heading": "B.3 Cross Alignment Module",
            "text": "As shown in Fig. 5, there are three paradigms of EEG and language alignment. For word level, the EEG features are divided by each word, and the objective of the alignment is to find the connectivity of different frequencies with the corresponding word. For the concat-word level, the 8 frequencies\u2019 EEG features are concatenated as a whole, and then concatenated again to match the corresponding sentence, so the alignment is to find out the relationship within the sentence. As for sentence level, the EEG features are calculated as an average over the word-level EEG features. There is no boundary for the word, so the alignment module tries to encode the embeddings as a whole, and explore the general representations. In the Cross Alignment Module (CAM), we introduced a new loss function in addition to the original cross-entropy loss. The new loss is based on Canonical Correlation Analysis (CCA) (Andrew et al., 2013) and Optimal Transport (Wasserstein Distance). As in Andrew et al. (2013), CCA aims to concurrently learn the parameters of two networks to maximize the correlation between them. Wasserstein Distance (WD), which originates from Optimal Transport (OT), has the ability to align embeddings from different domains to explore the relationship (Chen et al., 2020).\nCanonical Correlation Analysis (CCA) is a method for exploring the relationships between two multivariate sets of variables. It learns the linear transformation of two vectors to maximize the correlation between them, which is used in many multimodal problems (Andrew et al., 2013; Qiu et al., 2018; Gong et al., 2013). In this work, we apply CCA to capture the cross-domain relationship. Let low-level transformed EEG features be Ve and low-level language features be Lt. We assume (Ve, Vt) \u2208 Rn1 \u00d7Rn2 has covariances (\u03a311,\u03a322) and cross-covariance \u03a312. CCA finds pairs of linear projections of the two views, (w\u20321Ve, w \u2032 2Vt) that are maximally correlated:\n(w\u22171, w \u2217 2) = argmax\nw1,w2 corr\n( w\u20321Ve, w \u2032 2Vt ) = argmax\nw1,w2 w\u20321\u03a312w2\u221a w\u20321\u03a311w1w \u2032 2\u03a322w2\n(5)\nIn our study, we modified the structure of Andrew et al. (2013) while honoring its duty by replacing the neural networks with Transformer encoders. w\u22171 and w \u2217 2 denote the high-level, transformed weights from the low-level text and EEG features, respectively.\nWasserstein Distance (WD) is introduced in Optimal Transport (OT), which is a natural type of divergence for registration problems as it accounts for the underlying geometry of the space, and has been used for multimodal data matching and alignment tasks (Chen et al., 2020; Yuan et al., 2020; Lee et al., 2019; Demetci et al., 2020; Qiu et al., 2022; Zhu et al., 2022). In Euclidean settings, OT introduces WD W(\u00b5, \u03bd), which measures the minimum effort required to \u201cdisplace\u201d points across measures \u00b5 and \u03bd, where \u00b5 and \u03bd are values observed in the empirical distribution. In our setting, we compute the temporalpairwise Wasserstein Distance on EEG features and language features, which are (\u00b5, \u03bd) = (Ve, Vt). For simplicity without loss of generality, assume \u00b5 \u2208 P (X) and \u03bd \u2208 P (Y) denote the two discrete distributions, formulated as \u00b5 = \u2211n i=1 ui\u03b4xi and \u03bd = \u2211m j=1 vj\u03b4yj , with \u03b4x as the Dirac function centered on x. \u03a0(\u00b5, \u03bd) denotes all the joint distributions \u03b3(x, y), with marginals \u00b5(x) and \u03bd(y). The weight vectors\nu = {ui}ni=1 \u2208 \u2206n and v = {vi} m i=1 \u2208 \u2206m belong to the n\u2212 and m\u2212dimensional simplex, respectively. The WD between the two discrete distributions \u00b5 and \u03bd is defined as:\nWD(\u00b5, \u03bd) = inf \u03b3\u2208\u03a0(\u00b5,\u03bd) E(x,y)\u223c\u03b3 [c(x, y)] = min T\u2208\u03a0(u,v) n\u2211 i=1 m\u2211 j=1 Tij \u00b7 c (xi, yj) (6)\nwhere \u03a0(u,v)={T\u2208Rn\u00d7m+ |T1m=u,T\u22a41n=v}, 1n denotes an n\u2212dimensional all-one vector, and c (xi, yj) is the cost function evaluating the distance between xi and yj .\nLoss Objective The loss objective for the CAM module can be formalized as: Loss = lCE +\u03b11lCCA+ \u03b12lWD, where \u03b1i \u2208 {0, 1}, i \u2208 (1, 2) controls the weights of different parts of alignment-based loss objective."
        },
        {
            "heading": "C Experimental Settings",
            "text": ""
        },
        {
            "heading": "C.1 Downstream Tasks",
            "text": "In this study, we evaluate our method on two downstream tasks: Sentiment Analysis (SA) and Relation Detection (RD) of two datasets: K-EmoCon (Park et al., 2020) and ZuCo 1.0/2.0 Dataset (Hollenstein et al., 2018, 2020b).\nSentiment Analysis (SA) Given a succession of word-level or sentence-level EEG features and their corresponding language, the task is to predict the sentiment label. The ZuCo 1.0 dataset consists of sentences from the Stanford Sentiment Treebank, which contains movie reviews and their corresponding sentiment label (i.e., positive, neutral, negative) (Socher et al., 2013). The K-EmoCon dataset categorizes emotion annotations as valence, arousal, happy, sad, nervous, and angry. For each emotion, the participant labeled the extent of the given emotion felt by following a Likert-scale paradigm. Arousal and valence are rated 1 to 5 (1: very low; 5: very high). Happy, sad, nervous, and angry emotions are rated 1 to 4, where 1 means very low and 4 means very high. The ratings are dominantly labeled as very low and neutral. Therefore to combat class imbalance, we collapse the labels to binary and ternary settings.\nRelation Detection (RD) The goal of relation detection (also known as relation extraction or entity association) is to extract semantic relations between entities in a given text. For example, in the sentence, \"June Huh won the 2022 Fields Medal.\", the relation AWARD connects the two entities \"June Huh\" and \"Fields Medal\" together. The ZuCo 1.0/2.0 datasets provide the ground truth labels and texts for this task. We use texts from the Wikipedia relation extraction dataset (Culotta et al., 2006) that has 10 relation categories: award, control, education, employer, founder, job title, nationality, political affiliation, visited, and wife (Hollenstein et al., 2018, 2020b)."
        },
        {
            "heading": "C.2 Datasets and Data Processing",
            "text": "K-EmoCon Dataset K-EmoCon (Park et al., 2020) is a multimodal dataset including videos, speech audio, accelerometer, and physiological signals during a naturalistic conversation. After the conversation, each participant watched a recording of themselves and annotated their own and partner\u2019s emotions. Five external annotators were recruited to annotate both parties\u2019 emotions, six emotions in total (Arousal, Valence, Happy, Sad, Angry, Nervous). The NeuroSky MindWave headset captured EEG signals from the left prefrontal lob (FP1) at a sampling rate of 125 Hz in 8 frequency bands: delta (0.5\u20132.75Hz), theta (3.5\u20136.75Hz), low-alpha (7.5\u20139.25Hz), high-alpha (10\u201311.75Hz), low-beta (13\u201316.75Hz), high-beta (18\u201329.75Hz), low-gamma (31\u201339.75Hz), and middle-gamma (41\u201349.75Hz). We used Google Cloud\u2019s Speech-to-Text API to transcribe the audio data into text.\nZuCo Dataset The ZuCo Dataset (Hollenstein et al., 2018, 2020b) is a corpus of EEG signals and eye-tracking data during natural reading. The tasks during natural reading can be separated into three categories: sentiment analysis, natural reading, and task-specified reading. During sentiment analysis, the participant was presented with 400 positive, neutral, and negative labeled sentences from the Stanford Sentiment Treebank (Socher et al., 2013). The EEG data used in this study can be categorized into\nsentence-level and word-level features. The sentence-level features are the averaged word-level EEG features for the entire sentence duration. The word-level EEG features are for the first fixation duration (FFD) of a specific word, meaning when the participant\u2019s eye met the word, the EEG signals were recorded. For both word and sentence-level features, 8 frequency bands were recorded at a sampling frequency of 500 Hz and denoted as the following: theta1 (4-6Hz), theta2 (6.5\u20138Hz), alpha1 (8.5\u201310Hz), alpha2 (10.5\u201313Hz), beta1 (13.5\u201318Hz), beta2 (18.5\u201330Hz), gamma1 (30.5\u201340Hz), and gamma2 (40\u201349.5Hz)."
        },
        {
            "heading": "C.3 Experimental Setup",
            "text": "The hierarchical transformer encoders follow the standard skeleton from Vaswani et al. (2017), excluding its complexity. To avoid overfitting, we adopt the oversampling strategy for data augmentation (H\u00fcbschleSchneider and Sanders, 2019), which ensures a balanced distribution of classes included in each batch. The train/test/validation splitting is (80%, 10%, 10%) as in Hollenstein et al. (2021). The EEG features are extracted from the datasets in 8 frequency bands and normalized with Z-score according to previous work (S. Yousif et al., 2020; Fdez et al., 2021; Du et al., 2022) over each frequency band. To preserve relatability, the word and sentence embeddings are also normalized with Z-scores. We use pre-trained language models to generate text features (Devlin et al., 2019), where all texts are tokenized and embedded using the BERT-uncased-base model. Each sentence has an average length of 20 tokens, so we instantiate a max length of 32 with padding. In the case of word-level, we use an average length of 4 tokens for each word and establish a max length of 10 with padding. The token vectors\u2019 from the four last hidden layers of the pre-trained model are withdrawn and averaged to get a final sentence or word embedding. These embeddings are used during the sentence-level and word-level settings. For concat word-level, we simply concatenate the word embeddings for their respective sentence. All the experimental parameters are listed in Appendix C.4."
        },
        {
            "heading": "C.4 Experiment Parameters, Code and Dataset",
            "text": "Our model\u2019s parameters used in the experiments are listed in Table 2. Parameters with the best performance are marked in bold. Our anonymous code is available at https://anonymous.4open.science/r/EMNLP_ 2023-B08D/."
        },
        {
            "heading": "C.5 Baselines",
            "text": "The area of multimodal learning of EEG and language is not well explored, and to the best of our knowledge, only Hollenstein et al. (2021)\u2019s approach was directly comparable to our study. However, to make a fair evaluation, we implemented the following state-of-the-art representative approaches as baselines for verification: MLP (Ruppert, 2004), Bi-LSTM (Graves and Schmidhuber, 2005; Zhou et al., 2016), Transformer (Vaswani et al., 2017), and ResNet (He et al., 2016).\nIn this section, we present implementation details for our multilayer perceptron (MLP), ResNet, and BiLSTM models during baseline retrieval. Throughout all baseline results, we used a pre-trained BERTuncased-base model to extract useful features for text. In the case of EEG features, we used the signals as is. Both text and EEG features were normalized with a Z-score before inputting them into the models. We also used the cross-entropy loss function for all baseline results. We configure the MLP with 6 hidden layers. At every step before the last output layer, we established a rectified linear unit activation function and a dropout rate of 0.3. Starting from the input layer, we use a hidden layer sizes of 256, 128, and 64 for our baseline results. Our 1D ResNet architecture has 34 layers (Hong et al., 2020). The BiLSTM\nmodel has 4 layers with a size of 128 and 64, respectively. Once the initial embedding is fed into the BiLSTM model, we use a pack padded sequence function to ignore the padded elements. The comparison of implementation details of baseline methods and our methods is shown in Table 3"
        },
        {
            "heading": "D Additional Experimental Results",
            "text": ""
        },
        {
            "heading": "D.1 Experimental Results on K-EmoCon dataset",
            "text": "To the best of our knowledge, there is no existing work where EEG or text is used for the K-EmoCon dataset. However, other modalities such as audio, video, blood volume pulse (BVP), electrodermal activity (EDA), body temperature (TEMP), skin temperature (SKT), accelerometer (ACC) and heart rate (HR) have been used to perform sentiment analysis. As shown in Table 4, our model outperforms previous method, with even less domains\u2019 data, showing the connectivity between EEG and language and also the advantages of exploring them for downstream applications.\nIn Table 5, we show the comparison results of different methods on the K-EmoCon dataset. From Table 5, we can see that our method outperforms the other baselines, and the multimodal approach outperforms the unimodal approach, which also demonstrates the effectiveness of our method.\nD.2 Ablation results on the components in the CAM module\nTo further investigate the performance of different mechanisms in the CAM, we carried out ablation experiments on the Zuco dataset, and the results are shown in Table 6 in Appendix D.2. The combination of CCA and WD performed better compared to using only one mechanism for sentiment analysis and relation detection in all model settings. We also conducted experiments on word-level, sentence-level, and concat word-level inputs, and the results are also shown in Table 6. We observe that word-level EEG features paired with their respective word generally outperform sentence-level and concat word-level in both tasks."
        },
        {
            "heading": "D.3 Full Brain Topological Maps",
            "text": "In the paper, we only showed the brain topological maps for three frequency bands due to page limit, here we provide full brain topological maps for all the eight frequency bands, and the results are shown in Figure 6. We can observe the beta2, gamma1, and gamma2 frequency bands having the most powerful signals for all words."
        },
        {
            "heading": "D.4 t-SNE Feature Projections",
            "text": "In order to interpret the performance improvement, we visualized the original feature distribution and the transformed feature distribution. As shown in Fig. 7, the transformed feature distribution makes better clusters than the original one. The features learned by CAM can be more easily separable, showing the effectiveness of discovering and encoding the relationship between EEG and language. Figures 8,9,10 show more t-SNE projection results of the K-EmoCon dataset on Sentiment Analysis task."
        },
        {
            "heading": "D.5 Sentence-level Alignment",
            "text": "Figure 11 shows the negative and positive sentence-level alignment weights of ZuCo dataset. In Figure 11, we can find that alpha1, beta1,and gamma1 frequency bands show larger different response between negative and positive sentences."
        },
        {
            "heading": "D.6 Baseline Results",
            "text": "In this section, we provided baseline results that directly used either EEG, language, or fusion as input for the downstream applications. The results are shown in Table 7 and Table 8."
        },
        {
            "heading": "E More Related Work",
            "text": "Multimodal Learning of EEG and Other Domains EEG signal is a popular choice as a modality in multimodal learning. Ben Said et al. (2017) used EMG signals jointly with EEG in a bi-autoencoder architecture and increased accuracies for sentiment analysis. Bashar (2018) integrated ECG and EEG signals in a human identification task, where fused classifiers produced the highest score. Liu et al. (2019a, 2022); Bao et al. (2019) extracted correlated features between EEG and eye movement data for emotion classification, showing transformed features are more homogeneous and discriminative. Guo et al. (2019) collected eye images, eye movement data, and EEG signals, and encoded the three modalities with a bimodal deep autoencoder. Ortega and Faisal (2021) fed fNIRS and EEG to decode bimanual grip force and resulted in increased performance, compared to single modality models. There are also efforts to find correlations between EEG and visual stimulus frequencies (Saeidi et al., 2021). A common theme occurring among these works showed EEG paired with other domains could boost performance.\nMultimodal Learning of Language and Other Brain Signals Recently, language and cognitive data were also used together in multimodal settings to complete desirable tasks (Wang and Ji, 2021; Hollenstein et al., 2019, 2021, 2020a). Wehbe et al. (2014) used a recurrent neural network to perform word alignment between MEG activity and the generated word embeddings. Toneva and Wehbe (2019) utilized word-level MEG and fMRI recordings to compare word embeddings from large language models. Schwartz et al. (2019) used MEG and fMRI data to fine-tune a BERT language model (Devlin et al., 2019) and found that the relationships between these two modalities were generalized across participants. Huang et al. (2020) leveraged CT images and text from electronic health records to classify pulmonary embolism cases and observed that the multimodal model with late fusion achieved the best performance. Murphy et al. (2022) found semantic categories between MEG and language. However, the relationship between language and EEG has not been explored before.\nMultimodal Learning of EEG and Language Hale et al. (2019) related EEG signals to the states of a neural phrase structure parser and showed that through EEG signals, models were correlating syntactic properties to a specific genre of text. Foster et al. (2021) applied EEG signals to predict specific values of each dimension in a word vector through regression models. Wang and Ji (2021) used word-level EEG features to decode corresponding text tokens through an open vocabulary, sequence-to-sequence framework. Hollenstein et al. (2021) focused on a multimodal approach by utilizing a combination of EEG, eye-tracking, and text data to improve NLP tasks. They used a variation of LSTM and CNN to decode the EEG features, but did not explore the relationship between EEG and language. Their proposed multimodal framework follows the bi-encoder approach (Choi et al., 2021) where the two modalities are encoded separately (Hollenstein et al., 2021)."
        }
    ],
    "title": "Can Brain Signals Reveal Inner Alignment with Human Languages?",
    "year": 2023
}