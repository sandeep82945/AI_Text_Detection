{
    "abstractText": "The increasing number of Indian language users on the internet necessitates the development of Indian language technologies. In response to this demand, our paper presents a generalized representation vector for diverse text characteristics, including native scripts, transliterated text, multilingual, code-mixed, and social media-related attributes. We gather text from both social media and well-formed sources and utilize the FastText model to create the \"IndiSocialFT\" embedding. Through intrinsic and extrinsic evaluation methods, we compare IndiSocialFT with three popular pretrained embeddings trained over Indian languages. Our findings show that the proposed embedding surpasses the baselines in most cases and languages, demonstrating its suitability for various NLP applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Saurabh Kumar"
        },
        {
            "affiliations": [],
            "name": "Sanasam Ranbir Singh"
        },
        {
            "affiliations": [],
            "name": "Sukumar Nandi"
        }
    ],
    "id": "SP:3f48886af68e37017b3d46391956e65668625dc9",
    "references": [
        {
            "authors": [
                "Anmol Agarwal",
                "Jigar Gupta",
                "Rahul Goel",
                "Shyam Upadhyay",
                "Pankaj Joshi",
                "Rengarajan Aravamudhan."
            ],
            "title": "Cst5: Data augmentation for code-switched semantic parsing",
            "venue": "arXiv preprint arXiv:2211.07514.",
            "year": 2022
        },
        {
            "authors": [
                "Syed Sarfaraz Akhtar",
                "Arihant Gupta",
                "Avijit Vajpayee",
                "Arjit Srivastava",
                "Manish Shrivastava."
            ],
            "title": "Word similarity datasets for Indian languages: Annotation and baseline systems",
            "venue": "Proceedings of the 11th Linguistic Annotation Workshop, pages 91\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the Association for Computational Linguistics, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Edouard Grave",
                "Piotr Bojanowski",
                "Prakhar Gupta",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Learning word vectors for 157 languages",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki,",
            "year": 2018
        },
        {
            "authors": [
                "Divyanshu Kakwani",
                "Anoop Kunchukuttan",
                "Satish Golla",
                "Gokul N.C",
                "Avik Bhattacharyya",
                "Mitesh M. Khapra",
                "Pratyush Kumar"
            ],
            "title": "IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian",
            "year": 2020
        },
        {
            "authors": [
                "Gagandeep Kaur",
                "Abhishek Kaushik",
                "Shubham Sharma."
            ],
            "title": "Cooking is creating emotion: A study on hinglish sentiments of youtube cookery channels using semi-supervised approach",
            "venue": "Big Data and Cognitive Computing, 3(3).",
            "year": 2019
        },
        {
            "authors": [
                "Simran Khanuja",
                "Diksha Bansal",
                "Sarvesh Mehtani",
                "Savya Khosla",
                "Atreyee Dey",
                "Balaji Gopalan",
                "Dilip Kumar Margam",
                "Pooja Aggarwal",
                "Rajiv Teja Nagipogu",
                "Shachi Dave"
            ],
            "title": "Muril: Multilingual representations for indian",
            "year": 2021
        },
        {
            "authors": [
                "Lenin Laitonjam",
                "Sanasam Ranbir Singh"
            ],
            "title": "Manipuri\u2013english comparable corpus for cross",
            "year": 2023
        },
        {
            "authors": [
                "Ruba Priyadharshini",
                "Bharathi Raja Chakravarthi",
                "Sajeetha Thavareesan",
                "Dhivya Chinnappa",
                "Durairaj Thenmozhi",
                "Rahul Ponnusamy."
            ],
            "title": "Overview of the dravidiancodemix 2021 shared task on sentiment detection in tamil, malayalam, and kannada",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Raghavan",
                "Anoop Kunchukuttan",
                "Pratyush Kumar",
                "Mitesh Shantadevi Khapra"
            ],
            "title": "Samanantar: The largest publicly available parallel corpora collection for 11 indic languages",
            "year": 2021
        },
        {
            "authors": [
                "Brian Roark",
                "Lawrence Wolf-Sonkin",
                "Christo Kirov",
                "Sabrina J. Mielke",
                "Cibu Johny",
                "I\u015fin Demir\u015fahin",
                "Keith Hall."
            ],
            "title": "Processing South Asian languages written in the Latin script: the Dakshina dataset",
            "venue": "Proceedings of The 12th Language Resources and",
            "year": 2020
        },
        {
            "authors": [
                "Bin Wang",
                "Angela Wang",
                "Fenxiao Chen",
                "Yuncheng Wang",
                "C.-C. Jay Kuo."
            ],
            "title": "Evaluating word embedding models: methods and experimental results",
            "venue": "APSIPA Transactions on Signal and Information Processing, 8:e19.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Considering the growing interest of developing language technologies (with Indian languages support) due to vase Indian language base internet users (expected to cross 650 millions1), development of pre-trained Indian language text representation suitable for various NLP applications is becoming an important task. Though India is home to a vast linguistic landscapes, encompassing 1,369 rationalized languages and dialects (INDIA, 2011), the majority of the Indian language related NLP research focus primarily on few scheduled languages (only 22 languages are scheduled). Further, in the context of developing Indian language technology, the widespread use of transliterated text, creative acronyms, multilingual, code-mixed text etc. on social media should also be taken into account. To mitigate the above challenges, few studies (Kakwani et al., 2020; Conneau et al., 2020; Khanuja et al., 2021) have initiated the development of word embedding for Indian languages. However, these\n1Statistica-2022 www.statista.com\nstudies mostly focus on monolingual corpora with limited reach to social media setups. Motivated by these observations, this paper focuses on developing a more generalized representation vector suitable for the text with different characteristics - written in native scripts, transliterated text, multilingual, code-mixed, and other social media related characteristics.\nWith a target to incorporate diverse characteristics of user-generated contents on social media and also well formed text, we consider texts collected from two forms of sources \u2013 social media text (Twitter, Facebook and YouTube), and well-formed text collected from Samanantar Dataset (Ramesh et al., 2021), Dakshina Dataset (Roark et al., 2020), Manipuri-English comparable corpus (Laitonjam and Singh, 2023) and Wikipedia (including 20 schedule languages written in respective native scripts). Due to computing resource constraints at our end, the embedding of the words present in the corpus has been built using FastText (Bojanowski et al., 2017) model named IndiSocialFT2. To evaluate the quality of obtained embedding, we compare it with three popular publicly available works for Indian languages namely Facebook\u2019s FastText (Wiki+CommonCrawl) (Grave et al., 2018), two models by IndicNLPSuite (Kakwani et al., 2020)\u2013IndicFT and IndicBERT , and Google\u2019s MuRIL (Khanuja et al., 2021) using both intrinsic and extrinsic evaluation methods. From various experimental observations, the proposed embedding outperforms all the baseline embedding for almost all the cases and languages."
        },
        {
            "heading": "2 Related Work",
            "text": "Facebook FastText project (Grave et al., 2018) has published monolingual word embedding of 157 languages. Their FastText-based approach utilized Wiki+CommonCrawl data to create high-quality\n2Model will be made available upon decision of the paper\nword embeddings that encompasses both semantic and syntactic information.\nExpanding on the growing interest in Indian language representation, authors of IndicNLPSuite (Kakwani et al., 2020) have developed two different pre-trained models, IndicFT \u2013 a set of 11 monolingual pre-trained FastText embedding models, and IndicBERT \u2013 a multilingual ALBERT model trained on their corpora, referred to as IndicCorp.\nTo address the code-mixed cross-lingual transfer tasks, a few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data.\nA more recent development in this field is the MuRIL (Khanuja et al., 2021), which focuses on multilingual representations for 17 Indian languages. MuRIL is a pre-trained multilingual language model based on the BERT framework, specifically built for Indian languages. Its effectiveness has been demonstrated across a wide range of NLP tasks for Indian languages."
        },
        {
            "heading": "3 Dataset",
            "text": ""
        },
        {
            "heading": "3.1 Data Collection",
            "text": "We have crawled tweets, retweets, and replies using Twitter\u2019s API, focusing on Indian languages of a three-year duration spanning from 2019 to 2022. Our curated dataset primarily comprises text sourced from Twitter, totaling 0.6 billion tweets, including quoted retweets and replies. These tweets are filtered by location (India) and amount to 5.5 billion tokens. Additionally, we have collected posts and comments from Facebook profiles of well-known Indian individuals and news media, as well as comments on videos uploaded by popular news and entertainment channels on YouTube. The content from Facebook includes a total of 0.8 million posts, which also encompass comments and\nnested comments, resulting in 14.8 million tokens. The dataset also incorporates 0.4 million comments from YouTube, comprising 3.8 million tokens.\nTo ensure balanced distribution, we have also included 20 Indian languages in their native scripts from various publicly available datasets. We have incorporated Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Malayalam (ml), Marathi (mr), Oriya (or), Punjabi (pn), Tamil (ta), and Telugu (te) from the Samanantar dataset (Ramesh et al., 2021), Sindhi (sd), Sinhala (si), and Urdu (ur) from the Dakshina Dataset (Roark et al., 2020), and Manipuri (mni) from the Manipuri-English comparable corpus (Laitonjam and Singh, 2023). We have also included Sanskrit (sa), Bhojpuri (bho), Nepali (ne), Maithili (mai), and Angika (ang) languages in their native script text, which we have crawled from Wikipedia. Additionally, we have also included English (en) language text in our dataset. These native script datasets have added 0.3 billion sentences to the social media dataset. Summarization of dataset is presented in Table 1."
        },
        {
            "heading": "3.2 Model Pre-training Details",
            "text": "With the curated dataset, we have trained a 300- dimensional embeddings model using FastText. We have selected FastText for this task due to its ability to handle morphologically rich languages, such as Indian languages, by incorporating subword information in the form of character n-gram embeddings during the training process.\nWe have run the training for 15 epochs, utilized a window size of 5, and set a minimum token count of 5 for each instance. These hyperparameters are chosen to optimize the performance of the embeddings while considering the specific linguistic characteristics of the Indian languages in our dataset.\nThe resulting multilingual word embeddings are\nexpected to capture semantic and syntactic similarities across the various Indian languages present in the dataset, thereby enabling the development of more effective natural language processing applications tailored to this diverse set of languages."
        },
        {
            "heading": "4 Evaluation",
            "text": ""
        },
        {
            "heading": "4.1 Evaluation on Texts with Native Scripts",
            "text": "In the native script setting, we have compared our embeddings (referred to as IndiSocialFT) with two pre-trained embeddings: one released by the FastText project, trained on Wiki+CommonCrawl (FTWC) (Grave et al., 2018), and the other released by IndicNLPSuite, IndicFT. Evaluation is done with two setups \u2013 intrinsic and extrinsic.\nIntrinsic Evaluation: For intrinsic evaluation, we have created a set of semantically related word pairs (antonyms and synonyms word pairs) for six languages (five Indian and English) - Assamese, Bengali, Manipuri, Urdu(only antonyms), Tamil,\nand English each of set having 100-150 pairs, and performed a ranking-based intrinsic evaluation. Let Simk(wi) be the set of the top most similar k words of wi. We have used cosine similarity to estimate similarity between embedding of two words. In the ranking-based approach, given a word pair (wi, wj), the rank of word wj is defined as the position of word wj in the set Simk(wi). As the words in the antonyms pairs and the synonyms pairs are semantically similar to each other, their ranks are expected to be low. The average rank (k = 50) for different language datasets of antonyms and synonyms pairs, considering various pre-trained FastText-based models, is tabulated in Table 2. For most of the languages the average rank result produced by our embedding for pairs of words is lower than the result produced by the other monolingual embeddings. The lower ranked result reveals that our embedding is better at representing the semantically similar words.\nWe have also conducted another word similarity based intrinsic evaluation using the IIIT-Hyderabad word similarity dataset (Akhtar et al., 2017). The word similarity assessment examines the relationship between the distances of word embedding and the semantic similarity perceived by humans (Wang et al., 2019). This helps to determine how well the word embedding representations capture human-like understanding of similarity, and supports the idea that a word\u2019s meaning is connected to the context it appears in. Higher the word similarity score generated by the word embedding model for semantic similar words implies the better word embedding representations. Following the similar-\nity evaluation method outlined by Kakwani et al. (2020), we have assessed the similarity scores on the IIIT-Hyderabad word similarity dataset. The word similarity scores are presented in Table 3.Our findings demonstrate that across various languages, our embedding model outperforms the other models in terms of word similarity scores. These higher scores suggest that the word embeddings generated by our model in the native script are more effective at capturing a human-like perception of similarity.\nExtrinsic Evaluation: We have further conducted extrinsic evaluation of our model by employing text classification tasks on the IndicGLUE Datasets (Kakwani et al., 2020). IndicGLUE is a comprehensive dataset containing news articles classified into various categories, covering nine different Indian languages, each represented in its native script, offering a diverse linguistic landscape for evaluation and analysis. We have adopted the k-NN (k = 4) classifier based evaluation module outlined by Kakwani et al. (2020) to assess our embeddings. As this approach is non-parametric, the classification performance directly illustrates the efficacy of the embedding space in capturing the semantic and contextual information of of each word in the text. The accuracy score of the trained classifier on IndicGLUE Datasets is presented in Table 4. We have got an average accuracy score of 96.69%. This remarkable accuracy score highlights the effectiveness of our embedding model in\nhandling the semantic and contextual information of text in the native script."
        },
        {
            "heading": "4.2 Evaluation on Multilingual Code-Mixed Texts",
            "text": "We have assessed our word embeddings in a codemixed multilingual environment by conducting various text classification tasks, including (a) sentiment analysis, (b) offensive language detection, and (c) domain classification, using publicly available code-mixed datasets.\nDataset\u2013 For evaluation, we have utilized the Dravidian-CodeMix-FIRE 2021 (Priyadharshini et al., 2021) dataset, YouTube cookery channels viewer comments in Hinglish (Kaur et al., 2019), and the human-annotated code-switched semantic parsing dataset (Hinglish-TOP Dataset) (Agarwal et al., 2022). The statistics of these datasets are provided in Table 5.\nClassifier Training\u2013 In line with Kakwani et al. (2020), we have employed k-NN (k = 4) classifier, ensuring that classification performance directly reflects the effectiveness of the embedding space in capturing text semantics and contextual information.\nResults\u2013 As our embedding model is trained over a large dataset containing text from social media platforms, it inherently contains the code-mixed and multilingual text. We have compared our classification results with a baseline model trained using a TF-IDF vectorizer, FastText, IndicFT, IndicBERT and MuRIL (Khanuja et al., 2021). The text classification results in terms of accuracy and Macro F1 score are presented in Table 6. As our model is trained over the both native and code-mixed text, we can observe that in most of the dataset our model outperform all other models. The higher average accuracy score of 0.691 and higher Macro F1 score of 0.504 for various text classification tasks on the code-mixed multilingual texts, using our embedding model, demonstrates that our model is more proficient in managing contextual information in code-mixed and multilingual settings also."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we have addressed the challenge of representing text in a multilingual code-mixed social media environment by developing a FastTextbased embedding model. Our model is trained on a diverse dataset collected from various social media platforms, supplemented with native script text\nfrom publicly available corpora to ensure balance. We have assessed the performance of our trained embeddings in both monolingual native script and code-mixed multilingual environments, employing a range of intrinsic and extrinsic evaluation techniques. The results demonstrate the effectiveness of our model\u2019s embedding space in capturing the semantic and syntactic information of text in both native monolingual and code-mixed multilingual contexts. This trained embedding model can be utilized to address various NLP challenges in the social media context in the Indian region.\nAs for future work, we plan to further improve the quality of our embeddings by incorporating additional data sources and exploring transformerbased pre-training techniques. We also aim to extend the applicability of our embeddings to a wider range of NLP tasks and evaluate their performance in more diverse linguistic scenarios."
        }
    ],
    "title": "IndiSocialFT: Multilingual Word Representation for Indian languages in code-mixed environment",
    "year": 2023
}