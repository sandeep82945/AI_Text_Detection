{
    "abstractText": "Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich information from multiple sources (e.g., language, video, and audio), the potential sentiment-irrelevant and conflicting information across modalities may hinder the performance from being further improved. To alleviate this, we present Adaptive Language-guided Multimodal Transformer (ALMT), which incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. With the obtained hypermodality representation, the model can obtain a complementary and joint representation through multimodal fusion for effective MSA. In practice, ALMT achieves state-of-the-art performance on several popular datasets (e.g., MOSI, MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and necessity of our irrelevance/conflict suppression mechanism.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Yu"
        }
    ],
    "id": "SP:10352f7686739546e5cf5fb701ab98fd148c8570",
    "references": [
        {
            "authors": [
                "Tadas Baltrusaitis",
                "Amir Zadeh",
                "Yao Chong Lim",
                "Louis-Philippe Morency"
            ],
            "title": "Openface 2.0: Facial behavior analysis toolkit",
            "venue": "IEEE International Conference on Automatic Face & Gesture Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko."
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "Proceedings of the 16th European Conference on Computer Vision, volume 12346,",
            "year": 2020
        },
        {
            "authors": [
                "Weidong Chen",
                "Xiaofen Xing",
                "Xiangmin Xu",
                "Jianxin Pang",
                "Lan Du."
            ],
            "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
            "venue": "Proceedings of the 23rd Annual Conference of the International Speech Communica-",
            "year": 2022
        },
        {
            "authors": [
                "Jiwei Guo",
                "Jiajia Tang",
                "Weichen Dai",
                "Yu Ding",
                "Wanzeng Kong."
            ],
            "title": "Dynamically adjust word representations using unaligned multimodal information",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, MM \u201922, page 3394\u20133402.",
            "year": 2022
        },
        {
            "authors": [
                "Wei Han",
                "Hui Chen",
                "Soujanya Poria."
            ],
            "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Devamanyu Hazarika",
                "Roger Zimmermann",
                "Soujanya Poria."
            ],
            "title": "MISA: modality-invariant and -specific representations for multimodal sentiment analysis",
            "venue": "Proceedings of the 28th ACM international conference on multimedia, pages 1122\u20131131.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Huang",
                "Jianhua Tao",
                "Bin Liu",
                "Zheng Lian",
                "Mingyue Niu."
            ],
            "title": "Multimodal transformer fusion for continuous emotion recognition",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), pages 3507\u20133511.",
            "year": 2020
        },
        {
            "authors": [
                "Yingying Jiang",
                "Wei Li",
                "M. Shamim Hossain",
                "Min Chen",
                "Abdulhameed Alelaiwi",
                "Muneer Al-Hammadi."
            ],
            "title": "A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition",
            "venue": "Information Fusion, 53:209\u2013221.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Yuanyuan Liu",
                "Wenbin Wang",
                "Chuanxu Feng",
                "Haoyu Zhang",
                "Zhe Chen",
                "Yibing Zhan."
            ],
            "title": "Expression snippet transformer for robust video-based facial expression recognition",
            "venue": "Pattern Recognition, 138:109368.",
            "year": 2023
        },
        {
            "authors": [
                "Yuanyuan Liu",
                "Haoyu Zhang",
                "Yibing Zhan",
                "Zijing Chen",
                "Guanghao Yin",
                "Lin Wei",
                "Zhe Chen."
            ],
            "title": "Noise-resistant multimodal transformer for emotion recognition",
            "venue": "arXiv preprint arXiv:2305.02814.",
            "year": 2023
        },
        {
            "authors": [
                "Zhun Liu",
                "Ying Shen",
                "Varun Bharadhwaj Lakshminarasimhan",
                "Paul Pu Liang",
                "AmirAli Bagher Zadeh",
                "Louis-Philippe Morency."
            ],
            "title": "Efficient lowrank multimodal fusion with modality-specific factors",
            "venue": "Proceedings of the 56th Annual Meeting of",
            "year": 2018
        },
        {
            "authors": [
                "Fengmao Lv",
                "Xiang Chen",
                "Yanyong Huang",
                "Lixin Duan",
                "Guosheng Lin."
            ],
            "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2021
        },
        {
            "authors": [
                "Huisheng Mao",
                "Ziqi Yuan",
                "Hua Xu",
                "Wenmeng Yu",
                "Yihe Liu",
                "Kai Gao."
            ],
            "title": "M-sena: An integrated platform for multimodal sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2022
        },
        {
            "authors": [
                "Brian McFee",
                "Colin Raffel",
                "Dawen Liang",
                "Daniel P.W. Ellis",
                "Matt McVicar",
                "Eric Battenberg",
                "Oriol Nieto."
            ],
            "title": "librosa: Audio and music signal analysis in python",
            "venue": "Proceedings of the 14th Python in Science Conference 2015 (SciPy 2015), Austin, Texas, July 6",
            "year": 2015
        },
        {
            "authors": [
                "Yongfeng Qian",
                "Yin Zhang",
                "Xiao Ma",
                "Han Yu",
                "Limei Peng."
            ],
            "title": "EARS: emotion-aware recommender system based on hybrid information fusion",
            "venue": "Information Fusion, 46:141\u2013146.",
            "year": 2019
        },
        {
            "authors": [
                "Wasifur Rahman",
                "Md Kamrul Hasan",
                "Sangwu Lee",
                "Amir Zadeh",
                "Chengfeng Mao",
                "Louis-Philippe Morency",
                "Ehsan Hoque."
            ],
            "title": "Integrating multimodal information in large pretrained transformers",
            "venue": "Proceedings of the conference. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Shaojie Bai",
                "Paul Pu Liang",
                "J. Zico Kolter",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Multimodal transformer for unaligned multimodal language sequences",
            "venue": "Proceedings of the 57th Annual Meeting of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Paul Pu Liang",
                "Amir Zadeh",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Learning factorized multimodal representations",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Dingkang Yang",
                "Shuai Huang",
                "Haopeng Kuang",
                "Yangtao Du",
                "Lihua Zhang."
            ],
            "title": "Disentangled representation learning for multimodal emotion recognition",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, pages 1642\u20131651. ACM.",
            "year": 2022
        },
        {
            "authors": [
                "Wenmeng Yu",
                "Hua Xu",
                "Fanyang Meng",
                "Yilin Zhu",
                "Yixiao Ma",
                "Jiele Wu",
                "Jiyun Zou",
                "Kaicheng Yang."
            ],
            "title": "CH-SIMS: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Wenmeng Yu",
                "Hua Xu",
                "Ziqi Yuan",
                "Jiele Wu."
            ],
            "title": "Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages",
            "year": 2021
        },
        {
            "authors": [
                "Ziqi Yuan",
                "Wei Li",
                "Hua Xu",
                "Wenmeng Yu."
            ],
            "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pages 4400\u20134407. ACM.",
            "year": 2021
        },
        {
            "authors": [
                "Amir Zadeh",
                "Minghai Chen",
                "Soujanya Poria",
                "Erik Cambria",
                "Louis-Philippe Morency."
            ],
            "title": "Tensor fusion network for multimodal sentiment analysis",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Amir Zadeh",
                "Paul Pu Liang",
                "Soujanya Poria",
                "Erik Cambria",
                "Louis-Philippe Morency."
            ],
            "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
            "venue": "Proceedings of the 56th Annual Meeting of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Amir Zadeh",
                "Rowan Zellers",
                "Eli Pincus",
                "LouisPhilippe Morency."
            ],
            "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
            "venue": "IEEE Intelligent Systems, 31(6):82\u201388.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multimodal Sentiment Analysis (MSA) focuses on recognizing the sentiment attitude of humans from various types of data, such as video, audio, and language. It plays a central role in several applications, such as healthcare and human-computer interaction (Jiang et al., 2020; Qian et al., 2019). Compared with unimodal methods, MSA methods are generally more robust by exploiting and exploring the relationships between different modalities, showing significant advantages in improving the understanding of human sentiment.\nMost recent MSA methods can be grouped into two categories: representation learning-centered methods (Hazarika et al., 2020; Yang et al., 2022;\n\u2217*Corresponding author\nI don\u2019t need anything.\nModel\nI don\u2019t need anything. NegativeModel \u2714\nPositive \u00d7\nPositiveModel \u00d7\nYu et al., 2021; Han et al., 2021; Guo et al., 2022) and multimodal fusion-centered methods (Zadeh et al., 2017; Liu et al., 2018; Tsai et al., 2019a; Huang et al., 2020). The representation learningcentered methods mainly focus on learning refined modality semantics that contains rich and varied human sentiment clues, which can further improve the efficiency of multimodal fusion for relationship modelling. On the other hand, the multimodal fusion-centered methods mainly focus on directly designing sophisticated fusion mechanisms to obtain a joint representation of multimodal data. In addition, some works and corresponding ablation studies (Hazarika et al., 2020; Rahman et al., 2020; Guo et al., 2022) further imply that various modalities contribute differently to recognition, where language modality stands out as the dominant one. We note, however, information from different modalities may be ambiguous and conflicting due to sentiment-irrelevance, especially from non-dominating modalities (e.g., lighting and head pose in video and background noise in audio). Such\ndisruptive information can greatly limit the performance of MSA methods. We have observed this phenomenon in several datasets (see Section 4.5.1) and an illustration is in Figure 1. To the best of our knowledge, there has never been prior work explicitly and actively taking this factor into account.\nMotivated by the above observation, we propose a novel Adaptive Language-guided Multimodal Transformer (ALMT) to improve the performance of MSA by addressing the adverse effects of disruptive information in visual and audio modalities. In ALMT, each modality is first transformed into a unified form by using a Transformer with initialized tokens. This operation not only suppresses the redundant information across modalities, but also compresses the length of long sequences to facilitate efficient model computation. Then, we introduce an Adaptive Hyper-modality Learning (AHL) module that uses different scales of language features with dominance to guide the visual and audio modalities to produce the intermediate hyper-modality token, which contains less sentiment-irrelevant information. Finally, we apply a cross-modality fusion Transformer with language features serving as query and hyper-modality features serving as key and value. In this sense, the complementary relations between language and visual and audio modalities are implicitly reasoned, achieving robust and accurate sentiment predictions. In summary, the major contributions of our work can be summarized as:\n\u2022 We present a novel multimodal sentiment analysis method, namely Adaptive Languageguided Multimodal Transformer (ALMT), which for the first time explicitly tackles the adverse effects of redundant and conflicting information in auxiliary modalities (i.e., visual and audio modalities), achieving a more robust sentiment understanding performance.\n\u2022 We devise a novel Adaptive Hyper-modality Learning (AHL) module for representation learning. The AHL uses different scales of language features to guide the visual and audio modalities to form a hyper modality that complements the language modality.\n\u2022 ALMT achieves state-of-the-art performance in several public and widely adopted datasets. We further provide in-depth analysis with rich empirical results to demonstrate the validity and necessity of the proposed approach."
        },
        {
            "heading": "2 Related Work",
            "text": "In this part, we briefly review previous work from two perspectives: multimodal sentiment analysis and Transformers."
        },
        {
            "heading": "2.1 Multimodal Sentiment Analysis",
            "text": "As mentioned in the section above, most previous MSA methods are mainly classified into two categories: representation learning-centered methods and multimodal fusion-centered methods.\nFor representation learning-centered methods, Hazarika et al. (2020) and Yang et al. (2022) argued representation learning of multiple modalities as a domain adaptation task. They respectively used metric learning and adversarial learning to learn the modality-invariant and modality-specific subspaces for multimodal fusion, achieving advanced performance in several popular datasets. Han et al. (2021) proposed a framework named MMIM that improves multimodal fusion with hierarchical mutual information maximization. Rahman et al. (2020) and Guo et al. (2022) devised different architectures to enhance language representation by incorporating multimodal interactions between language and non-verbal behavior information. However, these methods do not pay enough attention to sentiment-irrelevant redundant information that is more likely to be present in visual and audio modalities, which limits the performance of MSA.\nFor multimodal fusion-centered methods, Zadeh et al. (2017) proposed a fusion method (TFN) using a tensor fusion network to model the relationships between different modalities by computing the cartesian product. Tsai et al. (2019a) and Huang et al. (2020) introduced a multimodal Transformer to align the sequences and model long-range dependencies between elements across modalities. However, these methods directly fuse information from uni-modalities, which is more accessible to the introduction of sentiment-irrelevant information, thus obtaining sub-optimal results."
        },
        {
            "heading": "2.2 Transformer",
            "text": "Transformer is an attention-based building block for machine translation introduced by Vaswani et al. (2017). It learns the relationships between tokens by aggregating data from the entire sequence, showing an excellent modeling ability in various tasks, such as natural language processing, speech processing, and computer vision, etc. (Kenton and\nToutanova, 2019; Carion et al., 2020; Chen et al., 2022; Liu et al., 2023a). In MSA, this technique has been widely used for feature extraction, representation learning, and multimodal fusion (Tsai et al., 2019a; Huang et al., 2020; Liu et al., 2023b; Yuan et al., 2021)."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "The overall processing pipeline of the proposed Adaptive Language-guided Multimodal Transformer (ALMT) for robust multimodal sentiment analysis is in Figure 2. As shown, ALMT first extracts unified modality features from the input. Then, Adaptive Hyper-Modality Learning (AHL) module is employed to learn the adaptive hypermodality representation with the guidance of language features at different scales. Finally, we apply a Cross-modality Fusion Transformer to synthesize the hyper-modality features with language features as anchors, thus obtaining a language-guided hypermodality network for MSA."
        },
        {
            "heading": "3.2 Multimodal Input",
            "text": "Regarding the multimodal input, each sample consists of language (l), audio (a), and visual (v) sources. Referring to previous works, we first obtain pre-computed sequences calculated by BERT (Kenton and Toutanova, 2019), Librosa (McFee et al., 2015), and OpenFace (Baltrusaitis et al., 2018), respectively. Then, we denote these sequence inputs as Um \u2208 RTm\u00d7dm , where m \u2208 {l, v, a}, Tm is the sequence length and dm is the vector dimension of each modality. In practice, Tm and dm are different on different datasets. For example, on the MOSI dataset, Tv, Ta, Tl, da, dv and dl are 50, 50, 50, 5, 20, and 768, respectively."
        },
        {
            "heading": "3.3 Modality Embedding",
            "text": "With multimodal input Um, we introduce three Transformer layers to unify features of each modality, respectively. More specifically, we randomly initialize a low-dimensional token H0m \u2208 RT\u00d7dm for each modality and use the Transformer to embed the essential modality information to these to-\nkens :\nH1m = E 0 m(concat(H 0 m, Um), \u03b8E0m) \u2208 R T\u00d7d (1)\nwhere H1m is the unified feature of each modality m with a size of T \u00d7 d, E0m and \u03b8E0m respectively represent the modality feature extractor and corresponding parameters, concat(\u00b7) represent the concatenation operation.\nIn practice, T and d are set to 8 and 128, respectively. The structure of the transformer layer is designed as the same as the Vision Transformer (VIT) (Dosovitskiy et al., 2021) with a depth setting of 1. Moreover, it is worth noting that transferring the essential modality information to initialized lowdimensional tokens is beneficial to decrease the redundant information that is irrelevant to human sentiment, thus achieving higher efficiency with lesser parameters."
        },
        {
            "heading": "3.4 Adaptive Hyper-modality Learning",
            "text": "After modality embedding, we further employ an Adaptive Hyper-modality Learning (AHL) module to learn a refined hyper-modality representation that contains relevance/conflict-suppressing information and highly complements language features. The AHL module consists of two Transformer layers and three AHL layers, which aim to learn language features at different scales and adaptively learn a hyper-modality feature from visual and audio modalities under the guidance of\nlanguage features. In practice, we found that the language features significantly impact the modeling of hyper-modality (with more details in section 4.5.4)."
        },
        {
            "heading": "3.4.1 Construction of Two-scale Language Features",
            "text": "We define the feature H1l as low-scale language feature. With the feature, we introduce two Transformer layers to learn language features at middlescale and high-scale (i.e. H2l and H 3 l ). Different from the Transformer layer in the modality embedding stage that transfers essential information to an initialized token, layers in this stage directly model the language features:\nH il = E i l(H i\u22121 l , \u03b8Eil ) \u2208 RT\u00d7d (2)\nwhere i \u2208 {2, 3}, H il is language features at different scales with a size of T \u00d7 d, Eil and \u03b8Eil represents the i-th Transformer layer for language features learning and corresponding parameters. In practice, we used 8-head attention to model the information of each modality."
        },
        {
            "heading": "3.4.2 Adaptive Hyper-modality Learning Layer",
            "text": "With the language features of different scales H il , we first initialize a hyper-modality feature H0hyper \u2208 RT\u00d7d, then update H0hyper by calculating the relationship between obtained language\nfeatures and two remaining modalities using multihead attention (Vaswani et al., 2017). As shown in Figure 3, using the extracted H il as query and H1a as key, we can obtain the similarity matrix \u03b1 between language features and audio features :\n\u03b1 = softmax( QlK T a\u221a\ndk )\n= softmax( H ilWQlW T Ka H1Ta\u221a dk\n) \u2208 RT\u00d7T (3)\nwhere softmax represents weight normalization operation, WQl \u2208 Rd\u00d7dk and WKa \u2208 Rd\u00d7dk are learnable parameters, dk is the dimension of each attention head. In practice, we used 8-head attention and set dk to 16.\nSimilar to \u03b1, \u03b2 represents the similarity matrix between language modality and visual modality:\n\u03b2 = softmax( QlK T v\u221a\ndk )\n= softmax( H ilWQlW T Kv H1Tv\u221a dk\n) \u2208 RT\u00d7T (4)\nwhere WKv \u2208 Rd\u00d7dk is learnable. Then the hyper-modality features Hjhyper can be updated by weighted audio features and weighted visual features as:\nHjhyper = H j\u22121 hyper + \u03b1Va + \u03b2Vv\n= Hj\u22121hyper + \u03b1H 1 aWVa + \u03b2H 1 vWVv\n(5)\nwhere j \u2208 {1, 2, 3} and Hjhyper \u2208 R T\u00d7d respectively represent the j-th AHL layer and corresponding output hyper-modality features, WVa \u2208 Rd\u00d7dk and WVv \u2208 Rd\u00d7dk are learnable parameters."
        },
        {
            "heading": "3.5 Multimodal Fusion and Output",
            "text": "In the Multimodal Fusion, we first obtain a new language feature Hl and Hhyper and a new hypermodality feature by respectively concatenating initialized a token H0 \u2208 R1\u00d7d with H3hyper and H3l . Then we apply Cross-modality Fusion Transformer to transfer the essential joint and complementary information to these tokens. In practice, the Crossmodality Fusion Transformer fuse the language features Hl (serving as the query) and hyper-modality features Hhyper (serving as the key and value), thus obtaining a joint multimodal representation H \u2208 R1\u00d7d for final sentiment analysis. We denote the Cross-modality Fusion Transformer as\nCrossTrans, so the fusion process can be written as:\nHl = Concat(H0, H 3 l ) \u2208 R(T+1)\u00d7d (6)\nHhyper = Concat(H0, H 3 hyper) \u2208 R(T+1)\u00d7d (7)\nH = CrossTrans(Hl, Hhyper) \u2208 R1\u00d7d (8)\nAfter the multimodal fusion, we obtain the final sentiment analysis output y\u0302 by applying a classifier on the outputs of Cross-modality Fusion Transformer H . In practice, we also used 8-head attention to model the relationships between language modality and hyper-modality. For more details of the Cross-modality Fusion Transformer, we refer readers to Tsai et al. (2019a)."
        },
        {
            "heading": "3.6 Overall Learning Objectives",
            "text": "To summarize, our method only involves one learning objective, i.e., the sentiment analysis learning loss L, which is:\nL = 1 Nb Nb\u2211 n=0 \u2225yn \u2212 y\u0302n\u222522 (9)\nwhere Nb is the number of samples in the training set, yn is the sentiment label of the n-th sample. y\u0302n is the prediction of our ALMT.\nIn addition, thanks to our simple optimization goal, compared with advanced methods (Hazarika et al., 2020; Yu et al., 2021) with multiple optimization goals, ALMT is much easier to train without tuning extra hyper-parameters. More details are shown in section 4.5.10."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conducted extensive experiments on three popular trimodal datasets (i.e., MOSI (Zadeh et al., 2016), MOSEI (Zadeh et al., 2018), and CHSIMS (Yu et al., 2020)).\nMOSI. The dataset comprises 2,199 multimodal samples encompassing visual, audio, and language modalities. Specifically, the training set consists of 1,284 samples, the validation set contains 229 samples, and the test set encompasses 686 samples. Each individual sample is assigned a sentiment score ranging from -3 (indicating strongly negative) to 3 (indicating strongly positive).\nMOSEI. The dataset comprises 22,856 video clips collected from YouTube with a diverse factors (e.g., spontaneous expressions, head poses,\nocclusions, illuminations). This dataset has been categorized into 16,326 training instances, 1,871 validation instances, and 4,659 test instances. Each instance is meticulously labeled with a sentiment score ranging from -3 to 3. And the sentiment scores from -3 to 3 indicate most negative to most positive.\nCH-SIMS. It is a Chinese multimodal sentiment dataset that comprises 2,281 video clips collected from variuous sources, such as different movies and TV serials with spontaneous expressions, various head poses, etc. It is divided into 1,368 training samples, 456 validation samples, and 457 test samples. Each sample is manually annotated with a sentiment score from -1 (strongly negative) to 1 (strongly positive)."
        },
        {
            "heading": "4.2 Evaluation Criteria",
            "text": "Following prior works (Yu et al., 2020), we used several evaluation metrics, i.e., binary classification accuracy (Acc-2), F1, three classification accuracy\n1https://github.com/thuiar/MMSA/blob/master/ results/result-stat.md\n(Acc-3), five classification accuracy (Acc-5), seven classification accuracy (Acc-7), mean absolute error (MAE), and the correlation of the model\u2019s prediction with human (Corr). Moreover, on MOSI and MOSEI, agreeing with prior works (Hazarika et al., 2020), we calculated Acc-2 and F1 in two ways: negative/non-negative and negative/positive on MOSI and MOSEI datasets, respectively."
        },
        {
            "heading": "4.3 Baselines",
            "text": "To comprehensively validate the performance of our ALMT, we make a fair comparison with the several advanced and state-of-the-art methods, i.e., TFN (Zadeh et al., 2017), LMF (Liu et al., 2018), MFM (Tsai et al., 2019b), MuLT (Tsai et al., 2019a), MISA (Hazarika et al., 2020), PMR (Lv et al., 2021), MAG-BERT (Rahman et al., 2020), Self-MM (Yu et al., 2021), MMIM (Han et al., 2021), FDMER (Yang et al., 2022) and CHFN (Guo et al., 2022)."
        },
        {
            "heading": "4.4 Performance Comparison",
            "text": "Table 1 and Table 2 list the comparison results of our proposed method and state-of-the-art methods on the MOSI, MOSEI, and CH-SIMS, respectively.\nAs shown in the Table 1, the proposed ALMT obtained state-of-the-art performance in almost all metrics. On the task of more difficult and finegrained sentiment classification (Acc-7), our model achieves remarkable improvements. For example, on the MOSI dataset, ALMT achieved a relative improvement of 1.69% compared to the secondbest result obtained by CHFN. It demonstrates that eliminating the redundancy of auxiliary modalities is essential for effective MSA.\nMoreover, it is worth noting that the scenarios in\nSIMS are more complex than MOSI and MOSEI. Therefore, it is more challenging to model the multimodal data. However, as shown in the Table 2, ALMT achieved state-of-the-art performance in all metrics compared to the sub-optimal approach. For example, compared to Self-MM, it achieved relative improvements with 1.44% on Acc-2 and 1.40% on the corresponding F1, respectively. Achieving such superior performance on SIMS with more complex scenarios demonstrates ALMT\u2019s ability to extract effective sentiment information from various scenarios."
        },
        {
            "heading": "4.5 Ablation Study and Analysis",
            "text": ""
        },
        {
            "heading": "4.5.1 Effects of Different Modalities",
            "text": "To better understand the influence of each modality in the proposed ALMT, Table 3 reports the ablation results of the subtraction of each modality to the ALMT on the MOSI and CH-SIMS datasets, respectively. It is shown that, if the AHL is removed based on the subtraction of each modality, the performance decreases significantly in all metrics. This phenomenon demonstrates that AHL is beneficial in reducing the sentiment-irrelevant redundancy of visual and audio modalities, thus improving the robustness of MSA.\nIn addition, we note that after removing the video and audio inputs, the performance of ALMT remains relatively high. Therefore, in the MSA task, we argue that eliminating the sentimentirrelevant information that appears in auxiliary modalities (i.e., visual and audio modalities) and improving the contribution of auxiliary modalities in performance should be paid more attention to."
        },
        {
            "heading": "4.5.2 Effects of Different Components",
            "text": "To verify the effectiveness of each component of our ALMT, in Table 4, we present the ablation result of the subtraction of each component on the MOSI and CH-SIMS datasets, respectively. We observe that deactivating the AHL (replaced with\nfeature concatenation) greatly decreases the performance, demonstrating the language-guided hypermodality representation learning strategy is effective. Moreover, after the removal of the fusion Transformer and Modality Embedding, the performance drops again, also supporting that the fusion Transformer and Modality embedding can effectively improve the ALMT\u2019s ability to explore the sentiment information in each modality."
        },
        {
            "heading": "4.5.3 Effects of Different Query, Key, and Value Settings in Fusion Transformer",
            "text": "Table 5 presents the experimental results of different query, key, and value settings in Transformer on the MOSI and MOSEI datasets, respectively. We observed that ALMT can obtain better performance when aligning hyper-modality features to language features (i.e., using H3l as query and using H3hyper as key and value). We attribute this phenomenon to the fact that language information is relatively clean and can provide more sentimentrelevant information for effective MSA."
        },
        {
            "heading": "4.5.4 Effects of the Guidance of Different Language Features in AHL",
            "text": "To discuss the effect of the guidance of different language features in AHL, we show the ablation result of different guidance settings on MOSI and CH-SIMS in Table 6. In practice, we replace the AHL layer that do not require language guidance with MLP layer. Obviously, we can see that the ALMT can obtain the best performance when all scals of language features (i.e., H1l , H 2 l , H 3 l ) involve the guidance of hyper-modality learning.\nIn addition, we found that the model is more difficult to converge when AHL is removed. It indicates that sentiment-irrelevant and conflicting information visual and audio modalities may limit the improvement of the model."
        },
        {
            "heading": "4.5.5 Effects of Different Fusion Techniques",
            "text": "To analyze the effects of different fusion techniques, we conducted some experiments, whose results are shown in the table 7. Obviously, on the MOSI dataset, the use of our Cross-modality Fusion Transformer to fuse language features and hyper-modality features is the most effective. On the CH-SIMS dataset, although TFN achieves better performance on the MAE metric, its Acc-5 is lower. Overall, using Transformer for feature fusion is an effective way."
        },
        {
            "heading": "4.5.6 Analysis on Model Complexity",
            "text": "As shown in Table 8, we compare the parameters of ALMT with other state-of-the-art Transformer-based methods. Due to the different hyper-parameter configurations for each dataset may lead to a slight difference in the number of parameters calculated. We calculated the model parameters under the hyper-parameter settings on the MOSI. Obviously, our ALMT obtains the best performance (Acc-7 of 49.42 %) with a second\ncomputational cost (2.50M). It shows that ALMT achieves a better trade-off between accuracy and computational burden."
        },
        {
            "heading": "4.5.7 Visualization of Attention in AHL",
            "text": "In Figure 4, we present the average attention matrix (i.e., \u03b1 and \u03b2) on CH-SIMS. As shown, ALMT pays more attention to the visual modality, indicating that the visual modality provides more complementary information than the audio modality. In addition, from Table 3, compared to removing audio input, the performance of ALMT decreases more obviously when the video input is removed. It also demonstrates that visual modality may provide more complementary information."
        },
        {
            "heading": "4.5.8 Visualization of Robustness of AHL",
            "text": "To test the AHL\u2019s ability to perceive sentimentirrelevant information, as shown in Figure 5, we visualize the attention weights (\u03b2) of the last AHL layer between language features (H3l ) and visual features (H1v ) on CH-SIMS. More specifically, we first randomly selected a sample from the test set. Then we added random noise to a peak frame\n(marked by the black dashed boxes) of H1v , and finally observed the change of attention weights between H3l and H 1 v . It is seen that when the random noise is added to the peak frame, the attention weights between language and the corresponding peak frame show a remarkable decrease. This phenomenon demonstrates that AHL can suppress sentiment-irrelevant information, thus obtaining a more robust hyper-modality representation for multimodal fusion.\nhigh\nlowla n\ngu ag\ne fe\nat u\nre s\no f\ne ac\nh f\nra m\ne\nvisual features of each frame\nla n\ngu ag\ne fe\nat u\nre s\no f\ne ac\nh f\nra m\ne\nvisual features of each frame\n(a) without adding noise (b) with adding noise\nnoisy frame\n(a) (b)\nFigure 5: Visualization of the attention weights between language and visual modalities learned by the AHL for a randomly selected sample with and without a random noise on the CH-SIMS dataset. (a) The attention weights without a random noise; (b) the attention weights with a random noise. Note: darker colors indicate higher attention weights for learning."
        },
        {
            "heading": "4.5.9 Visualization of Different Representations",
            "text": "In Figure 6, we visualized the hyper-modality representation H3hyper, visual representation H v 1 and audio representation Ha1 in a 3D feature space by using t-SNE (Van der Maaten and Hinton, 2008) on CH-SIMS. Obviously, there is a modality distribution gap existing between audio and visual features, as well as within their respective modalities. However, the hyper-modality representations learned from audio and visual features converge in the same distribution, indicating that the AHL can narrow the difference of inter-/intra modality distribution of audio and visual representations, thus reducing the difficulty of multimodal fusion."
        },
        {
            "heading": "4.5.10 Visualization of Convergence Performance",
            "text": "In Figure 7, we compared the convergence hehavior of ALMT with three state-of-the-art methods (i.e., MulT, MISA and Self-MM) on CH-SIMS. We choose the MAE curve for comparison as MAE indicates the model\u2019s ability to predict fine-grained sentiment. Obviously, on the training set, although\nSelf-MM converges the fastest, its MAE of convergence is larger than ALMT at the end of the epoch. On the validation set, ALMT seems more stable compared to other methods, while the curves of other methods show relatively more dramatic fluctuations. It demonstrates that the ALMT is easier to train and has a better generalization capability."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, a novel Adaptive Language-guided Multimodal Transformer (ALMT) is proposed to better model sentiment cues for robust Multimodal Sentiment Analysis (MSA). Due to effectively suppressing the adverse effects of redundant information in visual and audio modalities, the proposed method achieved highly improved performance on several popular datasets. We further present rich indepth studies investigating the reasons behind the effectiveness, which may potentially advise other researchers to better handle MSA-related tasks.\nLimitations\nOur AMLT which is a Transformer-based model usually has a large number of parameters. It requires comprehensive training and thus can be subjected to the size of the training datasets. As current sentiment datasets are typically small in size, the performance of AMLT may be limited. For example, compared to classification metrics, such as Acc-7 and Acc-2, the more fine-grained regression metrics (i.e., MAE and Corr) may need more data for training, resulting in relatively small improvements compared to other advanced methods."
        },
        {
            "heading": "A Hyper-parameters",
            "text": "In this section, we show the selection of some key hyper-parameters on the validation set of CHSIMS.\nA.1 Overview\nWe used PyTorch to implement our method. The experiments were conducted on a PC with Intel(R) Xeon(R) 6240C CPU at 2.6GHz and 128GB memory and NVIDIA GeForce RTX 3090. The key parameters are shown in Table 9. We see that most hyper-parameters are the same across these datasets, demonstrating the ALMT does not require complex hyper-parameters adjustment.\nA.2 Effects of Length Settings of Modality Feature\nIn Figure 8, we show the effect of the sequence length T of the token H0m in modality embedding on the CH-SIMS dataset. It is observed that there are significant performance changes when the hyper-parameter is changed. And a similar phenomenon occurred on the MOSI and MOSEI datasets. Although the MAE is not the best when the Acc-5 is highest, e.g., T is set to 32. Considering that the model computation rises when the T increases, we set T to 8, which is beneficial for ALMT to obtain the best performance with a relatively lower computational cost.\nA.3 Effects of Depth Settings of AHL\nFigure 9 presents the effect of AHL depth settings for MSA. Obviously, the ALMT achieves the best performance on the two most difficult evaluation metrics, i.e., Acc-5 and MAE. Hence, in this study, we set the depth of AHL to 3. Moreover, on the MOSI and MOSEI datasets, we set it to 3 as the similar phenomenon is also observed.\nA.4 Effects of Depth Settings of Fusion Transformer\nIn Figure 10, we presents the effects of depth settings of cross-modality fusion Transformer on CH-SIMS. We observed that the ALMT can obtain the best result on Acc-5 and MAE when the depth is set to 3 and 5, respectively. However, to balance performance and model computation, we set the depth to 4 on the CH-SIMS. Following the similar rule, we set the depth of the cross-modality fusion Transformer to 2 and 4 on the MOSI and MOSEI, respectively."
        }
    ],
    "title": "Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis",
    "year": 2023
}