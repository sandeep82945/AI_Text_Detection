{
    "abstractText": "Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the \u2018Big Five\u2019. Categorizations like the \u2018Big Five\u2019 are commonly used to assess and diagnose personality types. In this work, we explore whether text generated from large language models exhibits consistency in it\u2019s perceived \u2018Big Five\u2019 personality traits. For example, is a language model such as GPT2 likely to respond in a consistent way if asked to go out to a party? We also show that when exposed to different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as BERT and GPT2 consistently identify and mirror personality markers in those contexts. This behavior illustrates an ability to be manipulated in a predictable way (with correlations up to 0.84 between intended and realized changes in personality traits), and frames them as tools for controlling personas in applications such as dialog systems. We contribute two data-sets of personality descriptions of humans subjects.",
    "authors": [
        {
            "affiliations": [],
            "name": "Graham Caron"
        },
        {
            "affiliations": [],
            "name": "Shashank Srivastava"
        }
    ],
    "id": "SP:b7fcde5d4d31cd68bbec896ee5b99d212a27777d",
    "references": [
        {
            "authors": [
                "Abubakar Abid",
                "Maheen Farooqi",
                "James Zou."
            ],
            "title": "Large language models associate muslims with violence",
            "venue": "Nature Machine Intelligence, 3(6):461\u2013463.",
            "year": 2021
        },
        {
            "authors": [
                "Lisa P Argyle",
                "Ethan C Busby",
                "Nancy Fulda",
                "Joshua Gubler",
                "Christopher Rytting",
                "David Wingate."
            ],
            "title": "Out of one, many: Using language models to simulate human samples",
            "venue": "arXiv preprint arXiv:2209.06899.",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Bayne"
            ],
            "title": "The myers-briggs type indicator: A critical review and practical guide",
            "year": 1997
        },
        {
            "authors": [
                "Emily M. Bender",
                "Alexander Koller."
            ],
            "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185\u20135198, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Shikha Bordia",
                "Samuel R. Bowman."
            ],
            "title": "Identifying and reducing gender bias in word-level language models",
            "venue": "Proceedings of the 2019 Conference of the North, Minneapolis, Minnesota.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Hans Christian",
                "Derwin Suhartono",
                "Andry Chowanda",
                "Kamal Z. Zamli."
            ],
            "title": "Text based personality prediction from multiple social media data sources using pre-trained language model and model averaging",
            "venue": "Journal of Big Data, 8(1).",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Lewis R. Goldberg."
            ],
            "title": "An alternative \"description of personality\": The big-five factor structure",
            "venue": "Journal of Personality and Social Psychology, 59(6):1216\u20131229.",
            "year": 1990
        },
        {
            "authors": [
                "Lewis R. Goldberg."
            ],
            "title": "The structure of phenotypic personality traits",
            "venue": "American Psychologist, 48(1):26\u2013",
            "year": 1993
        },
        {
            "authors": [
                "Kotaro Hara",
                "Abigail Adams",
                "Kristy Milland",
                "Saiph Savage",
                "Chris Callison-Burch",
                "Jeffrey P. Bigham"
            ],
            "title": "A data-driven analysis of workers",
            "venue": "earnings on amazon mechanical turk. Proceedings of the 2018 CHI Conference on Human Factors in Computing",
            "year": 2018
        },
        {
            "authors": [
                "Po-Sen Huang",
                "Huan Zhang",
                "Ray Jiang",
                "Robert Stanforth",
                "Johannes Welbl",
                "Jack Rae",
                "Vishal Maini",
                "Dani Yogatama",
                "Pushmeet Kohli."
            ],
            "title": "Reducing sentiment bias in language models via counterfactual evaluation",
            "venue": "Findings of the Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "HuggingFace."
            ],
            "title": "the ai community building the future",
            "venue": "Last accessed 22 March 2022.",
            "year": 2022
        },
        {
            "authors": [
                "IPIP."
            ],
            "title": "Administering IPIP measures, with a 50item sample questionnaire",
            "venue": "Last accessed 22 March 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Guangyuan Jiang",
                "Manjie Xu",
                "Song-Chun Zhu",
                "Wenjuan Han",
                "Chi Zhang",
                "Yixin Zhu."
            ],
            "title": "MPI: evaluating and Inducing personality in pre-trained language models",
            "venue": "arXiv preprint arXiv:2206.07550.",
            "year": 2022
        },
        {
            "authors": [
                "Oliver P. John",
                "Laura P. Naumann",
                "Christopher J. Soto."
            ],
            "title": "Paradigm shift to the integrative big-five trait taxonomy: History, measurement, and conceptual issues",
            "venue": "Oliver P. John, Richard W. Robins,",
            "year": 2008
        },
        {
            "authors": [
                "Oliver P. John",
                "Sanjay Srivastava."
            ],
            "title": "The big five trait taxonomy: History, measurement, and theoretical perspectives",
            "venue": "Lawrence A. Pervin and Oliver P. John, editors, Handbook of personality: Theory and research, pages 102\u2013138. The Guilford Press, New",
            "year": 1999
        },
        {
            "authors": [
                "Saketh Reddy Karra",
                "Son Nguyen",
                "Theja Tulabandhula."
            ],
            "title": "AI Personification: Estimating the personality of language models",
            "venue": "arXiv preprint arXiv:2204.12000.",
            "year": 2022
        },
        {
            "authors": [
                "Anastasia Kuzminykh",
                "Jenny Sun",
                "Nivetha Govindaraju",
                "Jeff Avery",
                "Edward Lank."
            ],
            "title": "Genie in the bottle: Anthropomorphized perceptions of conversational agents",
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y Masui",
                "Y Gondo",
                "H Inagaki",
                "N Hirose."
            ],
            "title": "Do personality characteristics predict longevity? findings from the tokyo centenarian study",
            "venue": "Age, 28(4):353\u2013 361.",
            "year": 2006
        },
        {
            "authors": [
                "Yash Mehta",
                "Samin Fatehi",
                "Amirmohammad Kazameini",
                "Clemens Stachl",
                "Erik Cambria",
                "Sauleh Eetemadi."
            ],
            "title": "Bottom-up and top-down: Predicting personality with psycholinguistic and language model features",
            "venue": "2020 IEEE International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Maril\u00f9 Miotto",
                "Nicola Rossberg",
                "Bennett Kleinberg."
            ],
            "title": "Who is gpt-3? an exploration of personality, values and demographics",
            "venue": "arXiv preprint arXiv:2209.14338.",
            "year": 2022
        },
        {
            "authors": [
                "Shane T Mueller."
            ],
            "title": "Cognitive anthropomorphism of ai: How humans and computers classify images",
            "venue": "Ergonomics in Design, 28(3):12\u201319.",
            "year": 2020
        },
        {
            "authors": [
                "Ha Thanh Nguyena",
                "Randy Goebelb",
                "Francesca Tonic",
                "Kostas Stathisd",
                "Ken Satoha"
            ],
            "title": "A negation detection assessment of gpts: analysis with the xnot360 dataset",
            "year": 2023
        },
        {
            "authors": [
                "Open-Psychometrics."
            ],
            "title": "Open-source psychometrics project: Answers to the IPIP big five factor markers",
            "venue": "Last accessed 29 August 2022.",
            "year": 2018
        },
        {
            "authors": [
                "Joe O\u2019Connor",
                "Jacob Andreas"
            ],
            "title": "What context features can transformer language models use",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Melissa C O\u2019Connor",
                "Sampo V Paunonen"
            ],
            "title": "Big five personality predictors of post-secondary academic performance",
            "venue": "Personality and Individual differences,",
            "year": 2007
        },
        {
            "authors": [
                "Pierre Pureur",
                "Murat Erder."
            ],
            "title": "8, page 187\u2013213",
            "venue": "Morgan Kaufmann Publishers.",
            "year": 2016
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Arleen Salles",
                "Kathinka Evers",
                "Michele Farisco."
            ],
            "title": "Anthropomorphism in ai",
            "venue": "AJOB neuroscience, 11(2):88\u201395.",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg",
            "year": 2022
        },
        {
            "authors": [
                "Jerome P Wagner",
                "Ronald E Walker."
            ],
            "title": "Reliability and validity study of a sufi personality typology: The enneagram",
            "venue": "Journal of clinical psychology, 39(5):712\u2013717.",
            "year": 1983
        },
        {
            "authors": [
                "Jason K White",
                "Susan S Hendrick",
                "Clyde Hendrick."
            ],
            "title": "Big five personality variables and relationship constructs",
            "venue": "Personality and individual differences, 37(7):1519\u20131530.",
            "year": 2004
        },
        {
            "authors": [
                "Marty J Wolf",
                "Keith W Miller",
                "Frances S Grodzinsky."
            ],
            "title": "Why we should have seen that coming: comments on microsoft\u2019s tay \u201cexperiment,\u201d and wider implications",
            "venue": "The ORBIT Journal, 1(2):1\u201312.",
            "year": 2017
        },
        {
            "authors": [
                "Feifan Yang",
                "Tao Yang",
                "Xiaojun Quan",
                "Qinliang Su."
            ],
            "title": "Learning to answer psychological questionnaire for personality detection",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the meteoric rise of AI systems based on language models, there is an increasing need to understand the \u2018personalities\u2019 of these models. As communication with AI systems increases, so does the tendency to anthropomorphize them (Salles et al., 2020; Mueller, 2020; Kuzminykh et al., 2020). Thus, even though language models encode probability distributions over text and the tendency to assign cognitive abilities to them has been criticized (Bender and Koller, 2020), the way users perceive these systems can have significant consequences. If the perceived personality traits of these models can be better understood, their behavior can be tailored for specific applications. For instance, when suggesting email auto-completes, it may be\nuseful for a model to mirror the personality of the user. In contrast, for a dialog agent in a clinical setting, it may be desirable to manipulate a model interacting with a depressed individual such that it does not reinforce depressive behavior. Additionally, since such models are subject to biases in the text they are trained on, some may be prone to interact with users in hostile ways (Wolf et al., 2017). Manipulating these models can enable smoother and more amiable interactions with users.\nLanguage-based questionnaires have long been\nused in psychological assessments for measuring personality traits in humans (John et al., 2008). We apply the same principle to language models, and investigate the personality traits of these models through the text that they generate in response to such questions. As previously mentioned, we do not posit that these models have actual cognitive abilities, but are focused on exploring how their personality may be perceived through the lens of human psychology. Since language models are subject to influence from the context they see (O\u2019Connor and Andreas, 2021), we also explore how specific context could be used to manipulate the perceived personality of the models without controlling sources of bias or the models themselves (i.e., pretraining, parameter finetuning). Figure 1 shows an example illustrating this approach.\nOur analysis reveals that personality traits of language models are influenced by ambient context, and that this behavior can be manipulated in a highly predictable way. In general, we observe high correlations (median Pearson correlation coefficients of up to 0.84 and 0.81 for BERT and GPT2) between the expected and observed changes in personality traits across different contexts. The models\u2019 affinity to be affected by context positions them as potential tools for characterizing personality traits in humans. In further experiments, we find that, when using context from self-reported text descriptions of human subjects, language models can predict the subject\u2019s personality traits to a surprising degree (correlation up to 0.48 between predicted and actual human subject scores). We also confirm that the measured personality of a model reflects the personality seen in the text that the model generates. Together, these results frame language models as tools for identifying personality traits and controlling personas in applications such as dialog systems. Our contributions are:\n\u2022 We introduce the use of psychometric questionnaires for probing the personalities of language models. \u2022 We demonstrate that the personality traits of common language models can be predictably controlled using textual contexts. \u2022 We contribute two data-sets: 1) self-reported personality descriptions of human subjects paired with their psychometric assessment data, 2) personality descriptions collated from Reddit. (See project Git repository)"
        },
        {
            "heading": "2 Related Work",
            "text": "In recent years, research has looked at multiple forms of biases (i.e., racial, gender) in language models (Bordia and Bowman, 2019; Huang et al., 2020; Abid et al., 2021). However, the issue of measuring and controlling for biases in personas of language models is under-explored. A substantial body of research has explored the ways language models can be used to predict personality traits of humans. Mehta et al. (2020) and Christian et al. (2021) apply language models to such personality prediction tasks. Similar to our methodology, Argyle et al. (2022) contextualize large language models on a data-set of socio-economic back-stories to show that they model socio-cultural attitudes in broad human populations, and Yang et al. (2021) develop a new model designed to better detect personalty in user based context, using question based answering. Most relevant to our work are contemporaneous unpublished works by Karra et al. (2022), Miotto et al. (2022), and Jiang et al. (2022), who also explore aspects of personality in the language models themselves. However, these works substantially diverge from our approach and, along with Yang et al. (2021), do not attempt to characterize or manipulate the perceived personality of the models as we do."
        },
        {
            "heading": "3 \u2018Big Five\u2019 Preliminaries",
            "text": "The \u2018Big Five\u2019 is a seminal grouping of personality traits in psychological trait theory (Goldberg, 1990, 1993), and remains the most widely used taxonomy of personality traits (John and Srivastava, 1999; Pureur and Erder, 2016). These traits are: \u2022 Extroversion (E): People with a strong tendency\nin this trait are outgoing and energetic. They obtain energy from the company of others. \u2022 Agreeableness (A): People with a strong tendency in this trait are compassionate and kind. They value getting along with others. \u2022 Conscientiousness (C): People with a strong tendency in this trait are goal focused and organized. They follow rules and plan their actions. \u2022 Emotional Stability (ES): People with a strong tendency in this trait are not anxious or impulsive. They experience negative emotions less easily. \u2022 Openness to Experience (OE): People with a strong tendency in this trait are imaginative and creative. They are open to new ideas. While there are other personality groupings such as MBTI and the Enneagram (Bayne, 1997; Wag-\nner and Walker, 1983), we use the Big Five as the basis of our analyses, because the Big Five remains the most used taxonomy for personality assessment, and has been shown to be predictive of outcomes such as educational attainment (O\u2019Connor and Paunonen, 2007), longevity (Masui et al., 2006) and relationship satisfaction (White et al., 2004). Further, it is relatively natural to cast as an assessment for language models."
        },
        {
            "heading": "4 Experiment Design",
            "text": "We experiment with two language models, BERTbase (Devlin et al., 2019) and GPT2 (124M parameters) (Radford et al., 2019), to answer questions from a standard 50-item \u2018Big Five\u2019 personality assessment (IPIP, 2022) 1. Each item consists of a statement beginning with the prefix \u201cI\u201d or \u201cI am\u201d (e.g., I am the life of the party). Acceptable answers lie on a 5-point Likert scale where the answer choices disagree, slightly disagree, neutral, slightly agree, and agree correspond to numerical scores of 1, 2, 3, 4, and 5, respectively. To make the questionnaire more conducive to answering by language models, items were modified to a sentence completion format. For instance, the item \u201cI am the life of the party\u201d was changed to \u201cI am {blank} the life of the party\u201d, where the model is expected to select the answer choice that best fits the blank (see Appendix B for a complete list of items and their corresponding traits). To avoid complexity due to variable number of tokens, the answer choices were modified to the adverbs never, rarely, sometimes, often, and always, corresponding to numerical scores 1, 2, 3, 4, and 5, respectively. It is noteworthy that in this framing, an imbalance in the number of occurrences of each answer choice in the pretraining data might cause natural biases toward certain answer choices. However, while this factor might affect the absolute scores of the models, this is unlikely to affect the consistent overall patterns of changes in scores that we observe in our experiments by incorporating different contexts.\nFor assessment with BERT, the answer choice with the highest probability in place of the masked blank token was selected as the response. For assessment with GPT2, the procedure was modified, since GPT2 is an autoregressive model, and hence not directly conducive to fill-in-the-blank tasks. In this case, the probability of the sentence with each\n1BERT & GPT2 were selected because of their availability as open-source, pretrained models.\ncandidate answer choice was evaluated, and the answer choice from the sentence with the highest probability was selected.\nFinally, for each questionnaire (consisting of model responses to 50 questions), personality scores for each of the \u2018Big Five\u2019 personality traits were calculated according to a standard scoring procedure defined by the International Personality Item Pool (IPIP, 2022). Specifically, each of the five personality traits is associated with ten questions in the questionnaire. The numerical values associated with the response for these items were entered into a formula for the trait in which the item was assigned, leading to an overall integer score for each trait. To interpret model scores, we estimated the distribution of \u2018Big Five\u2019 personality traits in the human population. For this, we used data from a large-scale survey of \u2018Big Five\u2019 personality scores in 1,015,000 individuals (OpenPsychometrics, 2018). In the following sections, we report model scores in percentile terms of these human population distributions. Statistics for the human distributions and details of the IPIP scoring procedure are included in Appendix B."
        },
        {
            "heading": "5 Base Model Trait Evaluation",
            "text": "Table 1 shows the results of the base personality assessment for GPT2 and BERT for each of the five traits in terms of numeric values and corresponding human population percentiles. In the table, E stands for extroversion, A for agreeableness, C for conscientiousness, ES for emotional stability and OE for openness to experience. None of the base scores from BERT or GPT2, which we refer to as Xbase, diverge from the spread of the population distributions (TOST equivalence test at \u03b1 = 0.05). All scores were within 26 percentile points of the human population medians. This suggests that the pretraining data reflected the population distribution of the personality markers to some extent. However, percentiles for BERT\u2019s openness to experience (24) and GPT2\u2019s agreeableness (25) are substantially lower and GPT2\u2019s conscientiousness (73) and emotional stability (71) are significantly higher than the population median."
        },
        {
            "heading": "6 Manipulating Personality Traits",
            "text": "In this section, we explore manipulating the base personality traits of language models. Our exploration focuses on using prefix contexts to influence the personas of language models. For example,\nif we include a context where the first person is seen to engage in extroverted behavior, the idea is that language models might pick up on such cues and modify their language generation (e.g., to generate language that also reflects extrovert behavior). We investigate using three types of context: (1) answers to personality assessment items, (2) descriptions of personality from Reddit, and (3) self-reported personality descriptions from human users. In the following subsections, we describe these experiments in detail."
        },
        {
            "heading": "6.1 Analysis With Assessment Item Context",
            "text": "To investigate whether the personality traits of models can be manipulated predictably, the models are first evaluated on the \u2018Big Five\u2019 assessment (\u00a74) with individual questionnaire items serving as context. When used as context, we refer to the answer choices as modifiers and the items themselves as context items. For example, for extroversion, the context item \u201cI am {blank} the life of the party\" paired with the modifier always yields the context \u201cI am always the life of the party\", which precedes each extroversion questionnaire item.\nTo calculate the model scores, Xcm, for each trait, the models are evaluated on all ten items assigned to the trait, with each item serving as context once. This is done for each of the five modifiers, resulting in 10 (context items per trait) \u00d7 5 (modifiers per context item) \u00d7 10 (questionnaire items to be answered by the model) = 500 responses per trait and 10 (context items per trait) \u00d7 5 (modifiers per context item) = 50 scores (Xcm) per trait (one for each context). Context/modifier ratings (rcm) are calculated to quantify the models\u2019 expected behavior in response to context. First, each modifier is assigned a modifier rating between -2 and\n2 with -2 = never, -1 = rarely, 0 = sometimes, 1 = often and 2 = always. Because this experiment examines correlation between models scores and ratings, the magnitude of the modifier rating is arbitrary, so long as the ratings increase linearly from never (strongest negative connotation) to always (strongest positive connotation). Context items are given a context rating of -1 if the item negatively affected the trait score based on the IPIP scoring procedure, and 1 otherwise. The context ratings are multiplied by the modifier ratings to get the rcm. This value represents the expected relative change in trait score (expected behavior) when the corresponding context/modifier pair was used as context.\nNext, the differences, \u2206cm, between Xcm and Xbase values are calculated and the Pearson correlation with the rcm ratings measured (see Table 2 for the context/modifier pairs with the largest \u2206cm). One would expect Xcm evaluated on more positive rcm to increase relative to Xbase and vice versa. This is what we observe for BERT (see Figure 2) and GPT2, both of which show significant correlations (0.40 and 0.54) between \u2206cm and rcm (p < 0.01, t-test).\nFurther, to examine at the effect of individual context items as the strength of the modifier changes, we compute the correlation, \u03c1, between \u2206cm and rcm for individual context items (correlation computed from 5 data points per context item, one for each modifier). Table 3 reports the mean and median values of these correlations. These results indicate a strong relationship between \u2206cm and rcm. The mean values are significantly less than the medians, suggesting a left skew. For further analysis, the data was broken down by trait.\nThe histograms in Figure 3 depict \u03c1 by trait and include summary statistics for this data.\nMean and median \u03c1 from Figure 3 plots suggest a positive linear correlation between \u2206cm and rcm amongst context item plots, with conscientiousness and emotional stability having the strongest correlation for both BERT and GPT2. Groupings of \u03c1 around 1 in conscientiousness and emotional stability plots from Figure 3 demonstrate this correlation. GPT2 extroversion, BERT & GPT2 agreeableness and BERT openness to experience show large left skews. A possible explanation for for this is that models may have had difficulty distinguishing between the double negative statements created by some context/modifier pairs (i.e. item 36 with modifier never: \u201cI never don\u2019t like to draw attention to myself.\"). This may have caused \u2206cm to be negatively correlated with rcm, leading to an accumulation of \u03c1 values near -1.\nTable 2 shows the contexts that lead to the largest change for each of the personality traits for BERT and GPT2. We observe that all 10 contexts consist of the high-polarity quantifiers (either always or never), which is consistent with the correlation results. Further, we note that for four of the five traits, the item context that leads to the largest change is common between the two models.\nIt is important to note a possible weakness with our approach of using questionnaire items as context. Since our evaluation includes a given questionnaire item as context to itself during scoring,\na language model could achieve a spurious correlation, simply by copying the modifier choice mentioned in the context item. We experimented with adjustments 2 that would account for this issue and saw similar trends, with slightly lower but consistent correlation numbers (mean correlations of 0.25 and 0.40 for BERT and GPT2, compared with 0.40 and 0.54, statistically significant at p < 0.05, t-test).\nAlternate Framing: Another possible concern is the altering of the Big Five personality assessment framing to involve quantifiers. We experimented with an alternate fill-in-the-blank framing (e.g., I {blank} that I am the life of the party) that uses the same answer choices as the original test. Note that neutral was excluded because it fails to form a grammatical sentence. Despite the differences in token count amongst these answers, the greater frequency imbalance of these answers in the pretraining data compared to the altered answers, and the added sentence complexity of the assessment items, we saw similar trends. BERT extroversion and emotional stability had mean correlations of 0.22 & 0.29 respectively, and GPT2 agreeableness, conscientiousness, emotional stability and openness to experience had mean correlations of 0.10, 0.14, 0.61 & 0.40. These results suggest that our results are robust to our modification of the wording of the answer choices."
        },
        {
            "heading": "6.2 Analysis With Reddit Context",
            "text": "Next, we qualitatively analyze how personality traits of language models react to user-specific contexts. To acquire such context data, we curated data from Reddit threads asking individuals about their personality (see Appendix D for a list of sources). 1119 responses were collected, the majority of which were first person. Table 4 shows two examples. 3 Because GPT2 & BERT tokenizers can\u2019t accept more than 512 tokens, responses longer than this were truncated. The models were evaluated on the \u2018Big Five\u2019 assessment (\u00a74) using each of the 1119 responses as context (Reddit\n2We replaced the model responses where the questionnaire and context items matched with the base model\u2019s response for the item. This means that the concerning context item can no longer contribute to \u2206. However, this also means that numbers with this adjustment cannot be directly compared with those without since there are fewer sources of variation.\n3In qualitative analysis of a random sample of 200 responses, 3.5% of sampled responses were found to be hostile, harmfully biased or offensive, while 71.5% were found to be relevant to the topic of personality.\ncontext). For each Reddit context, scores, Xreddit, were calculated for all 5 traits. The difference between Xreddit and Xbase was calculated as \u2206reddit.\nTo interpret what phrases in the contexts affect the language models\u2019 personality traits, we train regression models on bag-of-words and n-gram (with n = 2 and n = 3) representations of the Reddit contexts as input, and \u2206reddit values as labels. Since the goal is to analyze attributes in the contexts that caused substantial shifts in trait scores, we only consider contexts with \u2225\u2206reddit\u2225 \u2265 1. Next, we extract the ten most positive and most negative feature weights for each trait. We note that for extroversion, phrases such as \u2018friendly\u2019, \u2018great\u2019 and \u2018no problem\u2019 are among the highest positively weighted phrases, whereas phrases such as \u2018stubborn\u2019 and \u2018don\u2019t like people\u2019 are among the most negatively weighted. For agreeableness, phrases like \u2018love\u2019 and \u2018loyal\u2019 are positively weighted, whereas phrases such as \u2018lazy\u2019, \u2018asshole\u2019 and expletives are weighted highly negative. On the whole, changes in personality scores for most traits conformed with a human understanding of the most highly weighted features. As further examples, phrases such as \u2018hang out with\u2019 caused a positive shift in trait score for openness to experience, while \u2018lack of motivation\u2019 causes a negative shift for con-\nscientiousness. There were fewer phrases for GPT2 openness to experience, GPT2 negatively weighted agreeableness, and GPT2 negatively weighted extroversion that caused shifts in the expected direction. This was consistent with results from \u00a76.1, where these traits exhibited the weakest relative positive correlations. Appendix D contains the full lists of highly weighted features for each trait."
        },
        {
            "heading": "6.3 Analysis With Psychometric Survey Data",
            "text": "The previous sections indicate that language models can pick up on personality traits from context. This raises the question of whether they can be used to estimate an individual\u2019s personality. In theory, this would be done by evaluating on the \u2018Big Five\u2019 personality assessment using context describing the individual, which could aid in personality characterization in cases where it is not feasible for a subject to manually undergo a personality assessment. We investigate this with the following experiment. The experimental design for this study\nwas vetted and approved by an Institutional Review Board (IRB) at the authors\u2019 home institution.\nUsing Amazon Mechanical Turk, subjects were asked to complete the 50-item \u2018Big Five\u2019 personality assessment outlined in \u00a74 (the assessment was not modified to a sentence completion format as was done for model testing) and provide a 75-150 word description of their personality (see Appendix E for survey instructions). Responses were manually filtered and low effort attempts discarded, resulting in 404 retained responses. Two variations of the study were adopted: the subjects for 199 of the responses were provided a brief summary of the \u2018Big Five\u2019 personality traits and asked to consider, but not specifically reference, these traits in their descriptions. We refer to these responses as the Directed Responses data set. The remaining 205 subjects were not provided this summary and their responses make up the Undirected Responses data set. Table 5 shows examples of collected descriptions. Despite asking for personality descriptions upwards of 75 words, around a fourth of the responses fell below this limit. The concern was that data with low word counts may not provide enough context. Thus, we experiment with filtering the responses by removing outliers (based on the interquartile ranges of measured correlations) and including minimum thresholds on the description length (75 and 100).\nHuman subject scores, Xsubject, were calculated for each assessment, using the same scoring procedure as previously described in \u00a74. The models were subsequently evaluated on the \u2018Big Five\u2019 personality assessment using the subjects\u2019 personality descriptions as context, yielding Xsurvey scores corresponding to each subject. Table 6 shows a summary of the correlation statistics for the two data sets and the different filters. There are strong correlations (0.48 for GPT2 and 0.44 for BERT for Directed Responses) between predicted scores from personality descriptions and the actual psychometric assessment scores. We note that there are only marginal differences in correlations between the two datasets, in spite of their different characteristics. While more specific testing is required to determine causal factors that explain these observed correlation values, they suggest the potential for using language models as probes for personality traits in free text.\nFigure 4 plots the correlations (\u03c1, outliers removed) for the individual personality traits, and\nincludes correlation coefficients from \u00a76.1. While the correlations from both sections are measured for different variables, they both represent a general relationship between observed personality traits of language models and the expected behavior (from two different types of contexts). While there are positive correlations for all ten scenarios, correlations from survey contexts are smaller than those from item contexts. This is not surprising since item contexts are specifically handpicked by domain experts to be relevant to specific personality traits, while survey contexts are free texts from open-ended prompts."
        },
        {
            "heading": "6.4 Observed Ranges of Personality Traits",
            "text": "In the previous subsections, we investigated priming language models with different types of contexts to manipulate their personality traits. Figure 5 summarizes the observed ranges of personality trait scores for different contexts, grouped by context type. The four columns for each trait represent the scores achieved by the base model (no context), and the ranges of scores achieved by the different types of contexts. The minimum, median and maximum scores for each context type are indicated by different shades on each bar. We observe that the different contexts lead to a remarkable range\nof scores for all five personality traits. In particular, for two of the traits (conscientiousness and emotional stability), the models actually achieve the full range of human scores (nearly 0 to 100 percentile). Curiously, for all five traits, different contexts are able to achieve very low scores (< 10 percentile). However, the models particularly struggle with achieving high scores for agreeableness."
        },
        {
            "heading": "7 Effects on Text Generation",
            "text": "While the previous sections strongly suggest that the perceived personality traits of language models can be influenced for fill-in-the-blank personality questionnaires, it is important to understand whether these influences also translate to text generated by these language models in downstream applications. To answer this question, we created \u2018text generation contexts\u2019 by concatenating each context/modifier pair from \u00a76.1 with each of six neutrally framed prompts (e.g., \"I am always the life of the party\" + \"When I talk to others, I...\", see Appendix F for complete list of prompts). For this experiment, GPT2 4 was used to generate a 50 token text for each text generation context.\nTable 7 gives examples of some text generation contexts and corresponding generated texts. Example 1 in Table 7 corresponds to a text generation context that asserts that the model is \u201calways interested in people\"; the generated text matches this in\n4Since BERT is trained for masked language modeling, and is not well suited for text generation\nboth sentiment and topic, describing an individual who is both curious about people and who enjoys spending time in an interactive environment like a job. While there are some generated texts with no apparent relation to text generation contexts, we found that most of the generated texts qualitatively mirror the personality in text generation context.\nWe also quantitatively evaluate how well the personality traits in the generated texts matches corresponding text generation contexts. For this, each generated text is, itself, used as context for a Big Five assessment (as previously shown in Figure 1, panel C). We measure the Pearson correlation between the resulting scores, Xgen, and the scores for the context/item pair (Xcm) from \u00a76.1 that were\nused in the corresponding text generation context. Figure 6 gives the results from this analysis, and shows an overall Pearson correlation of 0.49 between Xgen and Xcm.\nThis suggests that the personality scores of the model, measured using the Big Five assessment, are a good indication of the personality that might be seen in text generated from the contextualized language models."
        },
        {
            "heading": "8 Conclusion",
            "text": "We have presented a simple approach for measuring and controlling the perceived personality traits of language models. Further, we show that such models can predict personality traits of human users, possibly enabling assessment in cases where participation is difficult to attain. Future work can explore the use of alternate personality taxonomies. Similarly, there is a large and growing variety of language models. It is unclear to what extent our findings generalize to other language models, particularly those with significantly more parameters (Brown et al., 2020; Smith et al., 2022). Finally, the role that pretraining data plays on personality traits is an another important question for exploration.\nLimitations\nOur exploration has some notable limitations. These include answer bias due to variable token count and frequency imbalance in pretraining data and the presence of double negative statements in questionnaire items (\u00a74). The later might be addressed by experimentation with other language models. For instance, GPT2\u2019s closed source successors, GPT3 and GPT4, are shown to handle double negatives better than GPT2 ( (Nguyena et al., 2023)). Concerns with the altered questionnaire framing and the context item evaluation procedure were partially addressed in follow up experiments in \u00a76.1. As mentioned in the Conclusions section, whether and how our results generalize to other language models remains an open question.\nEthics and Broader Impact\nThe \u2018Big Five\u2019 assessment items and scoring procedure used in this study were drawn from free public resources and open source implementations of BERT and GPT2 (HuggingFace, 2022) were used. Reddit data was scraped from public threads and no usernames or other identifiable markers were\ncollated. The crowd-sourced survey data was collected using Amazon Mechanical Turk (AMT) with the permission of all participants, following IRB approval of the study design. No personally identifiable markers were stored and participants were compensated fairly, with a payment rate ($2.00/task w/ est. completion time of 15 min) significantly higher than AMT averages (Hara et al., 2018).\nThe broader goal of this line of research is to investigate aspects of personality in language models, which are increasingly being used in a number of NLP applications. Since AI systems that use these technologies are growing ever pervasive, and as humans tend to anthropomorphize such systems (i.e., Siri and Alexa), understanding and controlling their perceived personalities can have both broad and deep consequences. This is especially true for applications in domains such as education and mental health, where interactions with these systems can have lasting personal impacts on their users.\nFinally, if the personalities of AI systems can be manipulated in the ways that our research suggests, there is a serious risk of such systems being manipulated, through targeted attacks, to be hostile or disagreeable to their users. Developing methods through which language models could be made immune to such attacks would then be a necessary consideration before fielding such systems."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by NSF grant DRL2112635. The authors also thank anonymous reviewers for suggestions and feedback."
        },
        {
            "heading": "Appendix A Model Background",
            "text": "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a transformerbased deep learning model for natural language processing (Devlin et al., 2019). The model is pretrained on unlabeled data from the 800M word BooksCorpus and 2500M word English Wikipedia corpora. While BERT can be fine-tuned for autoregressive language modeling tasks, it is pretrained for masked language modeling. This study uses a BERT model from HuggingFaces\u2019s Transformer Python Library with a language model head for masked language modeling. No fine-tuning was done to the model. GPT2, which stands for Generative Pretrained Transformer 2, is a general-purpose learning transformer model developed by OpenAI in 2018 (Radford et al., 2019). Like BERT, this model is also pretrained on unlabeled data from the 800M word BooksCorpus. The study used Hugginface\u2019s GPT2 model with a language model head for autoregressive language modeling. As with BERT, no fine-tuning took place."
        },
        {
            "heading": "Appendix B Experiment Design Items",
            "text": "Item Associated Trait I am {blank} the life of the party. E I {blank} feel little concern for others. A I am {blank} prepared. C I {blank} get stressed out easily. ES I {blank} have a rich vocabulary. OE I {blank} don\u2019t talk a lot. E I am {blank} interested in people. A I {blank} leave my belongings around. C I am {blank} relaxed most of the time. ES I {blank} have difficulty understanding abstract ideas. OE I {blank} feel comfortable around people. E I {blank} insult people. A I {blank} pay attention to details. C I {blank} worry about things. ES I {blank} have a vivid imagination. OE I {blank} keep in the background. E I {blank} sympathize with others\u2019 feelings. A I {blank} make a mess of things. C I {blank} seldom feel blue. ES I am {blank} not interested in abstract ideas. OE I {blank} start conversations. E I am {blank} not interested in other people\u2019s problems. A I {blank} get chores done right away. C I am {blank} easily disturbed. ES I {blank} have excellent ideas. OE I {blank} have little to say. E I {blank} have a soft heart. A I {blank} forget to put things back in their proper place. C I {blank} get upset easily. ES I {blank} do not have a good imagination. OE I {blank} talk to a lot of different people at parties. E I am {blank} not really interested in others. A I {blank} like order. C I {blank} change my mood a lot. ES I am {blank} quick to understand things. OE I {blank} don\u2019t like to draw attention to myself. E I {blank} take time out for others. A I {blank} shirk my duties. C I {blank} have frequent mood swings. ES I {blank} use difficult words. OE I {blank} don\u2019t mind being the center of attention. E I {blank} feel others\u2019 emotions. A I {blank} follow a schedule. C I {blank} get irritated easily. ES I {blank} spend time reflecting on things. OE I am {blank} quiet around strangers. E I {blank} make people feel at ease. A I am {blank} exacting in my work. C I {blank} feel blue. ES I am {blank} full of ideas. OE\nTable B1: Adjusted \u2018Big Five\u2019 Personality Assessment Items.\nTrait Median Mean (\u00b5) SD (\u03c3) E 20 19.60 9.10 A 29 27.74 7.29 C 24 23.66 7.37 ES 19 19.33 8.59 OE 29 28.99 6.30\nTable B2: Human Population Distribution of \u2018Big Five\u2019 Personality Traits.\nTrait Base Value Positively Scored Item # Negatively Scored Item # E 20 1, 11, 21, 31, 41 6, 16, 26, 36, 46 A 14 7, 17, 27, 37, 42, 47 2, 12, 22, 32 C 14 3, 13, 23, 33, 43, 48 8, 18, 28, 38 ES 38 9, 19 4, 14, 24, 29, 34, 39, 44, 49 OE 8 5, 15, 25, 35, 40, 45, 50 10, 20, 30\nTable B3: \u2018Big Five\u2019 Personality Item Scoring Procedure."
        },
        {
            "heading": "Appendix C Item Context Evaluation Tables",
            "text": "rcm Mean \u2206cm Med \u2206cm \u2206cm SD Confidence Interval BERT\n-2 -3.36 -2.0 7.49 [-5.51, -1.21] -1 -3.18 -3.50 4.81 [-4.56, -1.80] 0 -0.02 0.00 4.51 [-1.32, 1.28] 1 2.42 2.00 6.17 [0.648, 4.19] 2 3.96 3.00 8.33 [1.57, 6.35] GPT2 -2 -7.34 -8.0 6.38 [-9.17, -5.51] -1 -4.58 -4.0 4.32 [-5.82, -3.34] 0 -2.06 -1.0 4.24 [-3.28, -0.84] 1 0.0 0.0 3.13 [-0.90, 0.90] 2 1.56 1.0 5.78 [-0.10, 3.22]\nTable C1: Statistics from \u2206cm vs rcm plots containing data from all traits. Statistics include mean, median, standard deviation and a confidence interval for \u2206cm at each rcm."
        },
        {
            "heading": "Appendix D Reddit Context Evaluation Tables",
            "text": "Reddit Context Sources reddit.com/r/AskReddit/comments/k3dhnt/how_would_you_describe_your_personality/ reddit.com/r/AskReddit/comments/q4ga1j/redditors_what_is_your_personality/ reddit.com/r/AskReddit/comments/68jl8g/how_can_you_describe_your_personality/ reddit.com/r/AskReddit/comments/ayjgyz/whats_your_personality_like/ reddit.com/r/AskReddit/comments/9xjahw/how_would_you_describe_your_personality/ reddit.com/r/AskWomen/comments/c1gr4a/how_would_you_describe_your_personality/ reddit.com/r/AskWomen/comments/7x23zg/what_are_your_most_defining_personalitycharacter/ reddit.com/r/CasualConversation/comments/5xtckg/how_would_you_describe_your_personality/ reddit.com/r/AskReddit/comments/aewroe/how_would_you_describe_your_personality/ reddit.com/r/AskMen/comments/c0grgv/how_would_you_describe_your_personality/ reddit.com/r/AskReddit/comments/pzm3in/how_would_you_describe_your_personality/ reddit.com/r/AskReddit/comments/bem0ro/how_would_you_describe_your_personality/ reddit.com/r/AskReddit/comments/1w9yp0/what_is_your_best_personality_trait/ reddit.com/r/AskReddit/comments/a499ng/what_is_your_worst_personality_trait/ reddit.com/r/AskReddit/comments/6onwek/what_is_your_worst_personality_trait/ reddit.com/r/AskReddit/comments/2d7l2i/serious_reddit_what_is_your_worst_character_trait/ reddit.com/r/AskReddit/comments/449cu7/serious_how_would_you_describe_your_personality/\nTable D1: Domain names of threads that were scraped to collect Reddit context.\nTrait Mean \u2206reddit Med \u2206reddit \u2206reddit SD 5 Max \u2206reddit 5 Min \u2206reddit BERT E -2.28 -2 4.04 8, 7, 7, 6, 5 -14, -13, -13, -13, -13 A -2.02 -1 3.38 2, 2, 2, 2, 2 -19, -18, -15, -15, -15 C 3.77 4 5.17 15, 15, 15, 15, 13 -17, -17, -16, -14, -13 ES 1.71 2 2.29 14, 14, 13, 13, 12 -12, -10, -10, -10, -10 OE 1.74 1 2.17 9, 7, 7, 7, 7 -11, -11, -8, -8, -7 GPT2 E -3.73 -4 3.33 7, 5, 5, 4, 4 -14, -10, -10, -10, -10 A -0.98 -1 4.26 13, 10, 8, 7, 7 -17, -15, -15, -15, -14 C -0.27 0 4.27 11, 11, 11, 11, 9 -20, -16, -16, -16, -15 ES -3.83 -3 6.27 8, 8, 8, 8, 8 -21, -21, -21, -21, -21 OE -1.91 -2 3.21 4, 4, 4, 4, 4 -15, -12, -12, -12, -12\nTable D2: \u2206reddit summary statistics. Statistics include mean, median and standard deviation, as well as 5 largest and 5 smallest \u2206reddit.\nBERT Extroversion\n\u2022 Notable Positively Weighted Phrases: \u2018friendly\u2019, \u2018great\u2019, \u2018good\u2019, \u2018quite\u2019, \u2018laugh\u2019, \u2018please\u2019, \u2018sense of\u2019, \u2018thanks for\u2019, \u2018really good\u2019, \u2018and friendly\u2019, \u2018no problem\u2019, \u2018to please\u2019, \u2018my sense of\u2019, \u2018finish everything start\u2019, \u2018enthusiastic but sensitive\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018question\u2019, \u2018stubborn\u2019, \u2018why\u2019, \u2018lack\u2019, \u2018fuck\u2019, \u2018fucking\u2019, \u2018hate\u2019, \u2018not\u2019, \u2018lack of\u2019, \u2018too much\u2019, \u2018don know\u2019, \u2018don like\u2019, \u2018too easily\u2019, \u2018way too\u2019, \u2018don like people\u2019, \u2018you go out\u2019, \u2018don know how\u2019, \u2018don[\u2019t] know what\u2019\nAgreeableness\n\u2022 Notable Positively Weighted Phrases: \u2018will\u2019, \u2018friendly\u2019, \u2018lol\u2019, \u2018love\u2019, \u2018loyal\u2019, \u2018calm\u2019, \u2018yup\u2019, \u2018does\u2019, \u2018honesty\u2019, \u2018laid back\u2019, \u2018go out\u2019, \u2018thanks for\u2019, \u2018really good\u2019, \u2018out with me\u2019, \u2018friendly polite and\u2019, \u2018really good listener\u2019, \u2018true to myself\u2019, \u2018my sense of\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018lack\u2019, \u2018didn[\u2019t]\u2019, \u2018won[\u2019t], \u2018lazy\u2019, \u2018fucking\u2019, \u2018self\u2019, \u2018worst\u2019, \u2018lack of\u2019, \u2018too easily\u2019, \u2018don like\u2019, \u2018the worst\u2019, \u2018being too\u2019, \u2018have no\u2019, \u2018don like people\u2019, \u2018lack of motivation\u2019, \u2018don know how\u2019, \u2018my worst trait\u2019, \u2018also my worst\u2019, \u2018too honest sometimes\u2019, \u2018doesn[\u2019t] talk much\u2019\nConscientiousness\n\u2022 Notable Positively Weighted Phrases: \u2018am\u2019, \u2018friendly\u2019, \u2018just\u2019, \u2018calm\u2019, \u2018believe\u2019, \u2018can be\u2019, \u2018of people\u2019, \u2018tend to\u2019, \u2018feel like\u2019, \u2018the most humble\u2019, \u2018most humble person\u2019, \u2018my sense of\u2019, \u2018get to know\u2019, \u2018friendly polite and\u2019, \u2018get along with\u2019, \u2018people like me\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018lack\u2019, \u2018no\u2019, \u2018lazy\u2019, \u2018inability\u2019, \u2018fucks\u2019, \u2018half\u2019, \u2018lack of\u2019, \u2018fuck off\u2019, \u2018don like\u2019, \u2018inability to\u2019, \u2018don like people\u2019, \u2018you go out\u2019, \u2018lack of motivation\u2019, \u2018don even know\u2019, \u2018monotonous and impulsive\u2019\nEmotional Stability\n\u2022 Notable Positively Weighted Phrases: \u2018will\u2019, \u2018feel\u2019, \u2018out with me\u2019, \u2018go out with\u2019, \u2018will you go\u2019, \u2018the most humble\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018no\u2019, \u2018off\u2019, \u2018hypercritical\u2019, \u2018overthinking\u2019, \u2018lack of\u2019, \u2018easily distracted\u2019, \u2018doesn[\u2019t] talk\u2019, \u2018don even\u2019, \u2018too easily distracted\u2019, \u2018lack of motivation\u2019, \u2018doesn[\u2019t] talk much\u2019, \u2018don even know\u2019, \u2018unrelatable is strange\u2019, \u2018is strange one\u2019, \u2018this said foreskin\u2019\nOpenness to Experience\n\u2022 Notable Positively Weighted Phrases: \u2018most\u2019, \u2018like\u2019, \u2018me to\u2019, \u2018out with\u2019, \u2018like me\u2019, \u2018like to\u2019, \u2018want to\u2019, \u2018with me\u2019, \u2018out with me\u2019, \u2018will you go\u2019, \u2018want to be\u2019, \u2018all the time\u2019, \u2018for me to\u2019, \u2018hang out with\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018lack\u2019, \u2018never\u2019, \u2018fucks\u2019, \u2018sad\u2019, \u2018nothing\u2019, \u2018lack empathy\u2019, \u2018the complainer\u2019, \u2018no confidence\u2019, \u2018lack of\u2019, \u2018easily distracted\u2019, \u2018blame helicopter\u2019, \u2018helicopter parents\u2019, \u2018never say sorry\u2019, \u2018blame helicopter parents\u2019, \u2018too easily distracted\u2019, \u2018finish projects after\u2019, \u2018never finish projects\u2019, \u2018procrastination out of\u2019, \u2018my lack of\u2019, \u2018lack of personality\u2019, \u2018too many fucks\u2019\nTable D3: Analysis of highest weighted phrases from BERT logistic regression.\nGPT2 Extroversion\n\u2022 Notable Positively Weighted Phrases: \u2018believe\u2019, \u2018loyal\u2019, \u2018curious\u2019, \u2018best\u2019, \u2018passionate\u2019, \u2018enjoy\u2019, \u2018bright\u2019, \u2018hard working\u2019, \u2018no problem\u2019, \u2018am nice\u2019, \u2018my amazing modesty\u2019, \u2018smooth bright epic\u2019, \u2018patient and flexible\u2019, \u2018great with children\u2019, \u2018calm cool collected\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018introverted\u2019, \u2018lack of\u2019, \u2018laid back\u2019, \u2018don know how\u2019\nAgreeableness\n\u2022 Notable Positively Weighted Phrases: \u2018friendly\u2019, \u2018loyal\u2019, \u2018honest\u2019, \u2018gay\u2019, \u2018humor\u2019, \u2018like people\u2019, \u2018thanks for\u2019, \u2018to please\u2019, \u2018and friendly\u2019, \u2018no problem\u2019, \u2018friendly polite and\u2019, \u2018patient and flexible\u2019, \u2018calm cool collected\u2019, \u2018honesty being straightforward\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018too easily\u2019, \u2018too much\u2019, \u2018lack of\u2019, \u2018you go out\u2019, \u2018don know what\u2019, \u2018self\u2019, \u2018asshole\u2019\nConscientiousness\n\u2022 Notable Positively Weighted Phrases: \u2018smile\u2019, \u2018thanks for\u2019, \u2018no problem\u2019, \u2018friendly polite and\u2019, \u2018really good listener\u2019, \u2018true to myself\u2019, \u2018patient and flexible\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018stop\u2019, \u2018jealousy\u2019, \u2018lazy\u2019, \u2018hate\u2019, \u2018lack\u2019, \u2018fuck\u2019, \u2018worst\u2019, \u2018lack of\u2019, \u2018too easily\u2019, \u2018fuck off\u2019, \u2018too nice\u2019, \u2018don know\u2019, \u2018don know how\u2019, \u2018lack of motivation\u2019, \u2018don even know\u2019, \u2018my worst trait\u2019, \u2018damn it uncle\u2019, \u2018depressed as shit\u2019\nEmotional Stability\n\u2022 Notable Positively Weighted Phrases: \u2018friendly\u2019, \u2018calm\u2019, \u2018easy\u2019, \u2018honesty\u2019, \u2018laid back\u2019, \u2018hard working\u2019, \u2018calm and\u2019, \u2018humble am\u2019, \u2018polite and\u2019, \u2018no problem\u2019, \u2018out with me\u2019, \u2018the most humble\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018lack\u2019, \u2018anxious\u2019, \u2018lazy\u2019, \u2018jealousy\u2019, \u2018lack of\u2019, \u2018don know\u2019, \u2018too easily\u2019, \u2018don like\u2019, \u2018don like people\u2019, \u2018don know how\u2019, \u2018lack of motivation\u2019, \u2018don even know\u2019\nOpenness to Experience\n\u2022 Notable Positively Weighted Phrases: \u2018understand\u2019, \u2018having\u2019, \u2018wanting\u2019, \u2018thoughts\u2019, \u2018thanks for\u2019, \u2018too nice\u2019, \u2018no problem\u2019, \u2018can relate\u2019, \u2018being too nice\u2019, \u2018that just confidence\u2019\n\u2022 Notable Negatively Weighted Phrases: \u2018fuck\u2019, \u2018myself\u2019, \u2018cynical\u2019, \u2018lack\u2019, \u2018boring\u2019, \u2018lack of\u2019, \u2018don like people\u2019\nTable D4: Analysis of highest weighted phrases from GPT2 logistic regression."
        },
        {
            "heading": "Appendix E Survey Context Evaluation Tables",
            "text": "Part 1 Instruction There are two parts to this questionnaire. In the first part (on this page), you will be shown 50 questions, and need to choose a response which best matches your personality. In the second part (on the next page), you will be asked to write a short (75-150 word) description of your personality in free text. Participants will only be compensated if they respond to all questions. Part 2 Instruction In between 75 and 150 words, please describe your personality [Directed responses: as it relates to the 5 personality traits outlined above. Be sure not to use the name of the personality traits themselves in your response].\nTable E1: Data collection survey instructions."
        },
        {
            "heading": "Appendix F Generated Text Evaluation Tables",
            "text": "Text Generation Prompts When I go to a gathering, I ... Others say that I am ... When I am around people, I ... When I have work to do, I ... When I have free time, I ... When I talk to others, I ...\nTable F1: List of prompts used in text generation context."
        }
    ],
    "title": "Manipulating the Perceived Personality Traits of Language Models",
    "year": 2023
}