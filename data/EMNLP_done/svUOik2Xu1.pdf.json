{
    "abstractText": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework , which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group.",
    "authors": [
        {
            "affiliations": [],
            "name": "Moxin Li"
        },
        {
            "affiliations": [],
            "name": "Wenjie Wang"
        },
        {
            "affiliations": [],
            "name": "Fuli Feng"
        },
        {
            "affiliations": [],
            "name": "Yixin Cao"
        },
        {
            "affiliations": [],
            "name": "Jizhi Zhang"
        },
        {
            "affiliations": [],
            "name": "Tat-Seng Chua"
        }
    ],
    "id": "SP:74e6544eded4cd4b63ba331c944fd701dec23c73",
    "references": [
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Jianfeng Gao",
                "Yejin Choi"
            ],
            "title": "Piqa: Reasoning about physical commonsense in natural language",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Xuanting Chen",
                "Junjie Ye",
                "Can Zu",
                "Nuo Xu",
                "Rui Zheng",
                "Minlong Peng",
                "Jie Zhou",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "title": "How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks. arXiv preprint arXiv:2303.00293",
            "year": 2023
        },
        {
            "authors": [
                "Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Leyang Cui",
                "Yu Wu",
                "Shujie Liu",
                "Yue Zhang",
                "Ming Zhou."
            ],
            "title": "Mutual: A dataset for multi-turn dialogue reasoning",
            "venue": "arXiv preprint arXiv:2004.04494.",
            "year": 2020
        },
        {
            "authors": [
                "Mingkai Deng",
                "Jianyu Wang",
                "Cheng-Ping Hsieh",
                "Yihan Wang",
                "Han Guo",
                "Tianmin Shu",
                "Meng Song",
                "Eric P Xing",
                "Zhiting Hu."
            ],
            "title": "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
            "venue": "arXiv preprint arXiv:2205.12548.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Shizhe Diao",
                "Zhichao Huang",
                "Ruijia Xu",
                "Xuechun Li",
                "Yong Lin",
                "Xiao Zhou",
                "Tong Zhang."
            ],
            "title": "Blackbox prompt learning for pre-trained language models",
            "venue": "arXiv preprint arXiv:2201.08531.",
            "year": 2022
        },
        {
            "authors": [
                "Dheeru Dua",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Gabriel Stanovsky",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "venue": "arXiv preprint arXiv:1903.00161.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Yuxian Gu",
                "Xu Han",
                "Zhiyuan Liu",
                "Minlie Huang."
            ],
            "title": "Ppt: Pre-trained prompt tuning for few-shot learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8410\u20138423.",
            "year": 2022
        },
        {
            "authors": [
                "Chulaka Gunasekara",
                "Jonathan K Kummerfeld",
                "Lazaros Polymenakos",
                "Walter Lasecki."
            ],
            "title": "Dstc7 task 1: Noetic end-to-end response selection",
            "venue": "Proceedings of the First Workshop on NLP for Conversational AI, pages 60\u201367.",
            "year": 2019
        },
        {
            "authors": [
                "Xu Guo",
                "Boyang Li",
                "Han Yu."
            ],
            "title": "Improving the sample efficiency of prompt tuning with domain adaptation",
            "venue": "arXiv preprint arXiv:2210.02952.",
            "year": 2022
        },
        {
            "authors": [
                "Bairu Hou",
                "Joe O\u2019Connor",
                "Jacob Andreas",
                "Shiyu Chang",
                "Yang Zhang"
            ],
            "title": "Promptboosting: Black-box text classification with ten forward passes. arXiv preprint arXiv:2212.09257",
            "year": 2022
        },
        {
            "authors": [
                "Shengding Hu",
                "Ning Ding",
                "Huadong Wang",
                "Zhiyuan Liu",
                "Jingang Wang",
                "Juanzi Li",
                "Wei Wu",
                "Maosong Sun."
            ],
            "title": "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models. arXiv preprint arXiv:2211.09110",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "William B Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022)",
            "venue": "The 3rd Workshop on Knowledge Extrac-",
            "year": 2022
        },
        {
            "authors": [
                "Ryan Lowe",
                "Nissan Pow",
                "Iulian Serban",
                "Joelle Pineau."
            ],
            "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
            "venue": "arXiv preprint arXiv:1506.08909.",
            "year": 2015
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "R Thomas McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "arXiv preprint arXiv:1902.01007.",
            "year": 2019
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "arXiv preprint arXiv:1809.02789.",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Archiki Prasad",
                "Peter Hase",
                "Xiang Zhou",
                "Mohit Bansal."
            ],
            "title": "Grips: Gradient-free, edit-based instruction search for prompting large language models",
            "venue": "arXiv preprint arXiv:2203.07281.",
            "year": 2022
        },
        {
            "authors": [
                "Reid Pryzant",
                "Dan Iter",
                "Jerry Li",
                "Yin Tat Lee",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Automatic prompt optimization with\" gradient descent\" and beam search",
            "venue": "arXiv preprint arXiv:2305.03495.",
            "year": 2023
        },
        {
            "authors": [
                "Chengwei Qin",
                "Shafiq Joty",
                "Qian Li",
                "Ruochen Zhao"
            ],
            "title": "Learning to initialize: Can meta learning improve cross-task generalization in prompt tuning? arXiv preprint arXiv:2302.08143",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Laria Reynolds",
                "Kyle McDonell."
            ],
            "title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan LeBras",
                "Yejin Choi."
            ],
            "title": "Socialiqa: Commonsense reasoning about social interactions",
            "venue": "arXiv preprint arXiv:1904.09728.",
            "year": 2019
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "It\u2019s not just size that matters: Small language models are also fewshot learners",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro Stolfo",
                "Zhijing Jin",
                "Kumar Shridhar",
                "Bernhard Sch\u00f6lkopf",
                "Mrinmaya Sachan."
            ],
            "title": "A causal framework to quantify the robustness of mathematical reasoning with language models",
            "venue": "arXiv preprint arXiv:2210.12023.",
            "year": 2022
        },
        {
            "authors": [
                "Yusheng Su",
                "Xiaozhi Wang",
                "Yujia Qin",
                "Chi-Min Chan",
                "Yankai Lin",
                "Huadong Wang",
                "Kaiyue Wen",
                "Zhiyuan Liu",
                "Peng Li",
                "Juanzi Li"
            ],
            "title": "On transferability of prompt tuning for natural language processing",
            "venue": "In Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Hong Qian",
                "Xuanjing Huang",
                "Xipeng Qiu."
            ],
            "title": "Black-box tuning for language-model-as-a-service",
            "venue": "International Conference on Machine Learning, pages 20841\u201320855. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Weng Lam Tam",
                "Xiao Liu",
                "Kaixuan Ji",
                "Lilong Xue",
                "Xingjian Zhang",
                "Yuxiao Dong",
                "Jiahua Liu",
                "Maodi Hu",
                "Jie Tang."
            ],
            "title": "Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers",
            "venue": "arXiv preprint arXiv:2207.07087.",
            "year": 2022
        },
        {
            "authors": [
                "Ruixiang Tang",
                "Dehan Kong",
                "Longtao Huang",
                "Hui Xue."
            ],
            "title": "Large language models can be lazy learners: Analyze shortcuts in in-context learning",
            "venue": "arXiv preprint arXiv:2305.17256.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Nirali Vaghani",
                "Mansi Thummar."
            ],
            "title": "BFlipkart product reviews with sentiment dataset",
            "venue": "https:// www.kaggle.com/dsv/4940809.",
            "year": 2023
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer."
            ],
            "title": "Spot: Better frozen model adaptation through soft prompt transfer",
            "venue": "arXiv preprint arXiv:2110.07904.",
            "year": 2021
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461.",
            "year": 2018
        },
        {
            "authors": [
                "Jindong Wang",
                "Xixu Hu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Haojun Huang",
                "Wei Ye",
                "Xiubo Geng"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Atharva Naik",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Anjana Arunkumar",
                "David Stap"
            ],
            "title": "Supernaturalinstructions: Generalization via declarative",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Linyi Yang",
                "Shuibai Zhang",
                "Libo Qin",
                "Yafu Li",
                "Yidong Wang",
                "Hanmeng Liu",
                "Jindong Wang",
                "Xing Xie",
                "Yue Zhang."
            ],
            "title": "Glue-x: Evaluating natural language understanding models from an outof-distribution generalization perspective",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Wentao Ye",
                "Mingfeng Ou",
                "Tianyi Li",
                "Xuetao Ma",
                "Yifan Yanggong",
                "Sai Wu",
                "Jie Fu",
                "Gang Chen",
                "Junbo Zhao"
            ],
            "title": "Assessing hidden risks of llms: An empirical study",
            "year": 2023
        },
        {
            "authors": [
                "Yunhu Ye",
                "Binyuan Hui",
                "Min Yang",
                "Binhua Li",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
            "venue": "arXiv preprint arXiv:2301.13808.",
            "year": 2023
        },
        {
            "authors": [
                "Tianjun Zhang",
                "Xuezhi Wang",
                "Denny Zhou",
                "Dale Schuurmans",
                "Joseph E Gonzalez."
            ],
            "title": "Tempera: Test-time prompt editing via reinforcement learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Kaijie Zhu",
                "Jindong Wang",
                "Jiaheng Zhou",
                "Zichen Wang",
                "Hao Chen",
                "Yidong Wang",
                "Linyi Yang",
                "Wei Ye",
                "Neil Zhenqiang Gong",
                "Yue Zhang"
            ],
            "title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "LLMs have gained significant attention for their remarkable performance in a broad range of Natural Language Processing (NLP) tasks (Ouyang et al., 2022; Chung et al., 2022; Brown et al., 2020; Touvron et al., 2023). This success has led to a shift in the paradigm of solving NLP tasks, moving away from training task-specific deep models towards developing task-specific strategies to effectively utilize LLMs (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022a; Ye et al., 2023b). In the new paradigm, the prompt becomes a crucial factor in ensuring the effectiveness of LLM on the NLP task, since even slight variations in prompt phrasing can largely affect LLM output (Reynolds and\n\u2217Corresponding author.\nMcDonell, 2021; Gao et al., 2021), making prompt optimization a promising research direction.\nExisting research has explored automatic prompt optimization methods to eliminate manual effort in identifying effective prompts for a given task. These methods can be gradient-based or gradientfree, depending on the availability of model gradients. Gradient-based methods optimize the prompt by calculating its gradients through the LLM (Schick and Sch\u00fctze, 2021b,a; Hu et al., 2022). Gradient-free methods update prompts based on LLM outputs using techniques such as an iterative search-and-select over the prompt space (Zhou et al., 2023; Prasad et al., 2022; Pryzant et al., 2023). This work focuses on gradient-free prompt optimization as LLMs are evolving into black-box API services (Sun et al., 2022).\nCurrent gradient-free prompt optimization methods ignore distribution shifts (Wang et al., 2023),\nwhere the data an LLM serves may differ from the labeled data for prompt optimization. Realworld NLP applications often encounter distribution shifts, such as new user groups with distinct linguistic habits in customer review analysis. It is unclear if prompts hinder the robustness of LLMs against distribution shifts. To answer this question, we conduct experiments with the representative gpt3.5-turbo-0301 model and prompts optimized by APE (Zhou et al., 2023) over paired data groups with distribution shifts. Results on 30 pairs of data groups from six tasks show the risk of significant performance gaps under certain distribution shifts.\nBased on this finding, we propose a new robust prompt optimization problem, which aims to optimize task-specific prompts with consideration of performance on both source and target groups under different distributions. Given an NLP task such as sentiment analysis, our problem setting has a labeled source group similar as the conventional prompt optimization setting and a unlabeled target group. We keep the target group unlabeled for the consideration that distribution shifts happen along time in practice. Labeling the newly coming target group will cause unnecessary labor cost and latency. Accordingly, the main challenge for solving this robust prompt optimization problem is incorporating unlabeled data into prompt optimization.\nTo this end, we propose the Generalized Prompt Optimization (GPO) framework to obtain a taskspecific prompt for both source and target groups. To jointly considering the two groups in prompt optimization, the key lies in labeling the target group in an automatic and reliable manner by adapting knowledge from the labeled source group. Towards this goal, we leverage the strong power of LLM in zero-shot labeling, and prompt ensemble to enhance the labeling robustness. Experimental results on three tasks demonstrate the effectiveness of our framework in improving the performance on the target group and simultaneously preserving a comparable performance on the source group. To sum up, our contributions are threefold:\n\u2022 We reveal the robustness issue of prompt optimization against distribution shifts and propose a new robust prompt optimization problem.\n\u2022 We propose the Generalized Prompt Optimization framework, which generates robust prompts considering both labeled and unlabeled data.\n\u2022 We conduct extensive experiments on three NLP\ntasks, validating the rationality and effectiveness of our proposed framework."
        },
        {
            "heading": "2 Preliminary Experiments",
            "text": "Prompt optimization aims to find the best prompt p that can instruct LLMs to predict the output y based on the concatenation of p and task input x, where x, y and p are all sequences of tokens. Formally, given an NLP task with a dataset {(x, y)} following a distribution P , the goal is to obtain\npo = argmax p\u2208Z E(x,y)\u223cP [r(LLM(p, x), y)], (1)\nwhere Z denotes the prompt optimization space and r is the evaluation metric to compare the LLM output with the ground truth output y, e.g., Accuracy. Existing studies usually leverage gradientbased or gradient-free methods to automatically optimize the prompts. Since LLMs are evolving into black-box API services, gradient-free methods become increasingly important. However, they ignore distribution shifts between training and testing data. In this light, we conduct controlled experiments to answer the following research question:\nAre prompts optimized by existing gradient-free methods robust to distribution shifts?"
        },
        {
            "heading": "2.1 Evaluation Protocol",
            "text": "We conduct the controlled experiments between a pair of data groups with distribution shifts, i.e., a source group {(xs, ys)} following a distribution Ps, and a target group {(xt, yt)} with a distribution Pt, where Pt \u0338= Ps. We intend to examine whether the prompt ps optimized on the source group can generalize to the target group. Specifically, given ps and pt optimized on the target group, we compare the performance of ps on the target group E(x,y)\u223cPt [r(LLM(ps, x), y)] with that of pt E(x,y)\u223cPt [r(LLM(pt, x), y)].\nDatasets. We select 16 datasets from six popular NLP tasks, where each pair of groups under the same task is treated as the source and target groups. Following recent out-of-distribution (OOD) research (Yang et al., 2022), we take each dataset as a group and regard different backgrounds and topics across the datasets as the distribution shifts. For the sentiment analysis task, we adopt Yelp (Zhang et al., 2015), Flipkart (Vaghani and Thummar, 2023), IMDB (Maas et al., 2011) and Amazon (Zhang et al., 2015) of different topics.\nFor the natural language inference task, we utilize MNLI (Williams et al., 2018), and ANLI (Nie et al., 2020) which is an adversarial dataset for MNLI. For the textual entailment, we use RTE (Wang et al., 2018) and its OOD dataset HANS (McCoy et al., 2019). For commonsense QA, we use SocialIQA (Sap et al., 2019), PIQA (Bisk et al., 2020), and OpenbookQA (Mihaylov et al., 2018), which focus on different types of commonsense knowledge. For the multi-turn dialog reasoning, we use DSTC7 (Gunasekara et al., 2019), Ubuntu Dialog (Lowe et al., 2015), and MuTual (Cui et al., 2020). Besides, for the numerical QA task, we use the samples of two different answer types (i.e., numerical values and text spans) in DROP (Dua et al., 2019) as two groups. See Appendix A.1 for details.\nExperimental Setup. We adopt APE (Zhou et al., 2023), an effective gradient-free prompt optimization method, for prompt generalization analysis. To highlight the effect of prompts, we conduct experiments under the zero-shot setting without incontext examples. For the backbone LLMs, we leverage gpt-3.5-turbo-0301 by calling the OpenAI API1. For all classification tasks (all tasks except for DROP), we use accuracy as the evaluation metric. For DROP, we utilize its standard evaluation metric \u2014 F1. Following the setting of APE, we randomly sample N -shot training and N -shot validation samples for prompt optimization, and repeat the experiments for five runs with different sampled\n1https://chat.openai.com/.\ndata to report the averaged results. More implementation details can be found in Appendix A.2."
        },
        {
            "heading": "2.2 Experimental Results",
            "text": ""
        },
        {
            "heading": "Demonstration of Generalization Performance",
            "text": "Gap. Table 1 shows the tasks without a large generalization gap between the performance of prompts ps and pt, and Table 2 shows the tasks with large gaps (Accuracy gap>8.0) on some groups. The row headers refer to the source groups for prompt optimization while the column headers show the target groups to test optimized prompts. The generalization performance gap between ps and pt can be observed by comparing the values in the same column.\nFrom the tables, we can observe: 1) The generalization performance gap may not exist for previously studied OOD and adversarial groups (see Table 1), including the groups of the natural language inference and the textual entailment tasks. This is possibly attributed to the strong generalization ability of LLMs. 2) However, under some data groups of Table 2 such as the sentiment analysis datasets (e.g., Flipkart and Yelp) and the commonsense QA datasets with different topics (e.g., PIQA and OpenbookQA), and the DROP groups with different answer types, there are still significant generalization performance gaps, demonstrating the existence of the generalization issue of prompt optimization. 3) Surprisingly, the prompt ps op-\n(a) Label distribution shifts. Smaller values indicate less distri-\ntimized from the source group does not always perform worse than the prompt pt optimized on the target group. In Table 2(b), ps from OpenbookQA performs even better than pt for SocialIQA. Besides, for DROP in Table 2(c), ps from Spans also performs better than pt from Number. In the following section, we try to explore the reasons for the above three observations."
        },
        {
            "heading": "Exploration on the Factors Affecting Prompt",
            "text": "Robustness. Based on the above observations, we further explore two research questions. Q1: Why do the prompts optimized on source groups perform differently on a target group? Q2: Why does the prompt optimized on the source group perform even better than the prompt optimized on the target group in some cases?\nFor Q1, we conjecture that the varied performance gaps are attributed to different distribution shifts between the source and target groups. To verify this, we examine two metrics to measure two kinds of distribution shifts: 1) the label shifts measured by the KL divergence, and 2) the input similarity quantified by the n-gram similarity of the input corpora of the two groups. Their detailed implementation is illustrated in Appendix A.3. We show the results of the sentiment analysis task as an example in Table 3. We can observe that the smallest label distribution shifts and the largest input similarity in Table 3 generally coincide with the best generalization performance on each target group in Table 2, indicating the correlation between distribution shifts and generalization per-\nformance. Nevertheless, the two metrics cannot perfectly explain the performance on all tasks (cf. Appendix A.3). Therefore, Q1 is still a challenging research question, requiring further exploration in future work.\nFor Q2, we conjecture that the outstanding generalization performance is because a source group with large diversity covers heterogeneous patterns in the target group, leading to a more robust prompt ps than pt. To explore this, we measure the heterogeneity of source and target groups by calculating the percentage of unique n-grams, and the percentage of n-grams of the target group covered by the source group. For illustration, we present the results of the commonsense QA task in Table 4. From Table 4(a), we can observe that OpenbookQA has the most diverse input according to the n-gram statistics. Moreover, OpenbookQA covers a large proportion of n-grams of SocialIQA and PIQA. These partly explain the superiority of the prompts optimized on OpenbookQA (see Table 2)."
        },
        {
            "heading": "3 Robust Prompt Optimization",
            "text": "In this section, we first formulate a robust prompt optimization problem and propose a GPO framework to enhance the robustness of the prompts."
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "To enhance the generalization ability of prompts, we propose a robust prompt optimization prob-\nlem. Specifically, given an NLP task such as sentiment analysis, it aims to optimize a task-specific prompt for the data groups with different distributions. We consider the popular scenario where a source group Gs = {(xs, ys)} following a distribution Ps and {xt} in a unlabeled target group Gt = {(xt, yt)} \u223c Pt (Pt \u0338= Ps) are available while {yt} is unseen during prompt optimization. The objective becomes utilizing Gs = {(xs, ys)} and {xt} to optimize a task-specific prompt robust to the samples from either Ps or Pt."
        },
        {
            "heading": "Reasons for Access to Unlabeled Target Group.",
            "text": "In a real-world deployment, LLMs continually encounter the testing data with distribution shifts. Collecting the input features {xt} of the target group is feasible. For example, when using LLMs as web services to solve user queries of certain NLP tasks, it is easy to collect extensive user queries as unlabeled target groups. However, labeling {xt} may be time-consuming and costly, and thus we intend to optimize robust prompts without the labels of the target group."
        },
        {
            "heading": "A Task-Specific Prompt vs. One Prompt for",
            "text": "Each Group. To tackle the generalization issue of optimized prompts, an intuitive approach is to optimize a separate prompt for each data group, yet this simplistic approach faces several limitations in real scenarios. In real-world deployment, it not only requires additional computation costs to construct more prompts, but also needs to accurately classify each testing sample into the appropriate group of the same distribution, thereby resulting in increased computation costs, latency, and new challenges for precise group classification. Further-\nmore, the collected source group data cannot cover all potential target groups, and the prompts optimized on the source groups may inevitably test on the examples from previously unseen groups. Thus, we aim at improving the generalization ability of one task-specific prompt across different groups."
        },
        {
            "heading": "3.2 GPO Framework",
            "text": "To obtain a robust prompt for both the source and target groups, it is natural to jointly consider Gs and Gt for prompt optimization. However, Gt lacks the labels {yt} that are commonly required by the gradient-free optimization methods (refer to Table 5 for the inferior results without labeling). With the impressive capabilities of LLMs on zero-shot labeling, we propose to utilize LLMs to label {xt}. Considering that noisy labels may damage the quality of optimized prompts, we further present two strategies to improve labeling accuracy.\nAs illustrated in Figure 2, we first propose a Meta Prompt to instruct LLMs to acquire knowledge from the labeled source group and generate a series of prompts. Thereafter, we utilize a prompt ensemble labeling strategy to apply generated prompts to an LLM for precise labeling of {xt}. In detail, we derive a three-step framework to perform the labeling with two strategies, and then conduct joint prompt optimization as shown in Figure 2.\n1. Prompt Generation via Meta Prompt. Following APE, we utilize a Meta Prompt to ask LLM to generate prompts for labeling by feeding the examples of Gs (see an example in Figure 2). Based on strong language understanding and reasoning abilities, LLMs can infer the relationships between the inputs and outputs of the examples and provide general and precise task prompts. We use different splits of Gs to generate K different prompts in total.\n2. Prompt Ensemble Labeling Strategy. Given K prompts, we utilize each of them to label {xt} with an LLM, and thus obtain K candidate labels for each example. We adopt an ensembling strategy and select the label with the highest consistency among the K candidate labels for each example. Besides, inspired from Wang et al. (2022a), we set a consistency threshold T \u2208 [0, 1] to only accept the labeled examples that have more than T percent of prompts agreed on the label. Eventually, we obtain a filtered labeled set G\u2217t for the target group.\n3. Joint Prompt Optimization. Finally, we mix Gs and G\u2217t to run APE for joint prompt optimization and obtain the final optimized prompt. As G\u2217t may have fewer samples than Gs after filtering with T , we perform a random upsampling on G\u2217t to have the same data number as Gs before running APE. A brief illustration about APE can be found in Appendix A.2."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Setup",
            "text": "Datasets. We experiment GPO with three tasks: sentiment analysis, commonsense QA, and DROP. For each task, we select a pair of groups with generalization performance gap as source and target groups, and ablate the labels for the target groups. Compared Methods. We adopt the following baseline methods: 1) APE; 2) APO (Pryzant et al., 2023), the state-of-the-art gradient-free prompt optimization method for LLM; 3) APE-ut, a naive generalization solution by incorporating the unlabeled target group input into APE; 4) the Upper Bound, which represents the performance of the prompt optimized on the target group data with ground-truth labels by APE; and 5) our proposed GPO; We also show the results of simple humanwritten prompts that are general for the task, and the revised versions by PromptPerfect2 which is an automatic prompt engineering website. Evaluation Protocol. We utilize two strategies for testing: Top 1 and Ensemble. Top 1 refers to using the single optimized prompt with the best validation performance, while Ensemble refers to labeling with all obtained K prompts and accept the output with the most agreement on the prompts. We utilize the same N -shot data as the preliminary experiments and also report the averaged results for five runs. More implementation details are illustrated in Appendix A.4."
        },
        {
            "heading": "4.2 Performance Comparison",
            "text": "Compare to Generated Prompts. From Table 5, we can observe the followings: 1) GPO achieves superior performance for all target groups in both Top 1 and Ensemble testing, validating its effectiveness. However, there is still space for improvement towards the Upper Bound for all tasks, showing the challenge of the generalization problem. 2) GPO achieves comparable source group performance for all tasks, showing its improvement on the target\n2https://promptperfect.jina.ai.\ngroup does not largely hinder the source group. Compared with APE, GPO shows increased performance on the source groups of SocialIQA and Number by incorporating the target group data, which is in line with the finding in Table 2. 3) Across baselines, APO outperforms APE on the source groups of the last two tasks and achieve comparable performance on sentiment analysis, showing its effectiveness for prompt optimization. However, the generalization ability is only comparable to APE since APO performs worse than APE on several target groups. 4) APE-ut achieves improved target group performance for the first two task, indicating the benefit of incorporating unlabeled target group data for generalization. However, for Spans where obtaining accurate target labels is challenging (as shown by the low F1 values), APE-ut largely underperforms GPO, showing the importance of target group labeling especially for difficult tasks.\nCompare to Human-written Prompts. From Table 6, we further observe that GPO outperforms human-written prompts and PromptPerfect for sentiment analysis and commonsense QA tasks. However, on the most difficult task DROP, GPO underperforms human-written prompts. This is poten-\ntially because the inaccurate labels for Spans hinder the prompt optimization. Similarly, PromptPerfect also fail to optimize human-written prompts for DROP."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "We study the effect of prompt ensemble labeling and joint prompt optimization by evaluating two modifications of GPO: (1) setting the consistency threshold as 0, denoted as w/o cons; and (2) removing the target group training data during the final prompt generation, denoted as w/o cons+t-train. From Table 7, we can observe that: 1) In all cases except for Flipkart with Top 1 evaluation, GPO performs better than w/o cons on target groups, showing the effectiveness of the consistency threshold. 2) Among the three tasks, DROP has large improvement between w/o cons and GPO on both source and target groups then the other two tasks. We hypothesis that this discrepancy is related to the different degrees of improvement in the labeling accuracy by the consistency threshold, which will be further discussed in Section 4.4. 3) Comparing\nw/o cons and w/o cons+t-train, removing the target group training data benefits the Top 1 results of the source group, but harms the Ensemble results of the target groups. It has less effect on the target group Top 1 results since the two methods still use target group validation data."
        },
        {
            "heading": "4.4 In-depth Analysis",
            "text": "Analysis on the Effect of the Consistency Threshold. To further reveal the effect of consistency threshold, we first show the labeling accuracy of the target group training and validation data for GPO and w/o cons in Table 8. We can observe that applying the consistency threshold can improve the labeling accuracy for all target groups. By examining the relationship between this labeling accuracy improvement and the performance difference between GPO and w/o cons in Table 7, it can be explained that for Flipkart and OpenbookQA, where the labeling accuracy is already high under w/o cons, further improving the labeling accuracy by the consistency threshold is unlikely to achieve large performance gain. Conversely, in the case of Spans with low labeling accuracy, even a minor improvement can result in significant performance gains. To explore the connection between labeling accuracy and target group performance further, we conducted an experiment where we manually assigned incorrect labels to varying proportions (0%, 50%, and 90%) of the target training and validation data. The results are illustrated in Figure 3. It can be observed that as the percentage of incorrect labels increases, the overall performance on the target group generally decreases, emphasizing the importance of labeling accuracy for achieving effective generalization.\nTop 1 Ensemble APE GPO APE GPO\nVicuna-7B 38.4 \u00b1 25.3 63.5 \u00b1 15.6 43.9 \u00b1 21.3 71.9 \u00b1 13.1 Vicuna-13B 66.8 \u00b1 18.4 68.3 \u00b1 13.7 60.7 \u00b1 9.5 70.7 \u00b1 10.8 GPT-3.5 78.4 \u00b1 1.9 80.5 \u00b1 2.1 81.3 \u00b1 1.4 84.5 \u00b1 2.0 GPT-4 77.5 \u00b1 13.7 85.3 \u00b1 2.7 83.3 \u00b1 0.0 85.4 \u00b1 2.4\nTable 9: Performance comparison of APE and GPO on Flipkart of different backbone LLMs.\nGPO with Different Backbone LLMs. We also conducted experiments with GPO using different backbone LLMs, including Vicuna 7B and 13B (Chiang et al., 2023) which are notable smallersized LLMs, and GPT-4 (OpenAI, 2023). Table 9 shows the generalization results on Flipkart with Yelp as the source group for APE and GPO on different backbone LLMs. Due to the small sizes of the Vicuna models, generating the exact sentiment label as the answer can be challenging. Therefore, we extract the sentiment labels from their outputs before calculating the accuracy. The results show that there is room for enhancing the generalization performance in APE across various LLMs, and GPO consistently outperforms APE in all cases. Notably, when applying GPO to the smaller Vicuna 7B model, there is a significant improvement that allows it to reach the same performance level as the Vicuna 13B model. Across LLMs, the smallersized Vicuna models achieve relatively worse performance, and the powerful GPT-4 achieves the best performance on GPO."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "Generalization Ability and Robustness of LLM.",
            "text": "Researchers have been investigating the generalization ability and robustness of LLMs since their recent breakthrough. LLMs like ChatGPT have shown significant improvement in out-ofdistribution (OOD) and adversarial tasks (Wang et al., 2023), although they are still imperfect (Chen et al., 2023). Some LLMs still rely on shortcuts\nand spurious correlation (Tang et al., 2023; Stolfo et al., 2022). Moreover, LLMs remain vulnerable to adversarial perturbations and achieve inconsistent results (Wang et al., 2023; Ye et al., 2023a; Liang et al., 2022). Additionally, LLMs demonstrate high sensitivity to the prompt (Reynolds and McDonell, 2021; Zhu et al., 2023) and the selection of in-context examples (Liu et al., 2022; Rubin et al., 2022). Lastly, instruction tuning allows LLMs to generalize to novel tasks (Ouyang et al., 2022; Wang et al., 2022b,a). We specifically focus on the generalization issue of prompt optimization on the distribution shifts within one task.\nPrompt Optimization. Obtaining effective prompts for applying LLM in NLP tasks is a popular research area. Prompt tuning methods (Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021; Gu et al., 2022) learn soft continuous vectors as prompts in the LLM input using gradients from the task objective. Recent studies have also focused on gradient-free prompt optimization for black-box LLM, such as reinforcement learningbased methods (Zhang et al., 2023; Deng et al., 2022; Diao et al., 2022), search-based methods (Brown et al., 2020; Prasad et al., 2022; Pryzant et al., 2023), and other gradient-free optimization techniques like evolutionary algorithms (Sun et al., 2022) and boosting (Hou et al., 2022). Among them, the state-of-the-art methods leverage the power of LLMs for prompt optimization, such as prompt generation and evaluation by LLM (APE (Zhou et al., 2023)) and prompt editing following critiques (APO (Pryzant et al., 2023)), where we mainly compare with them. Notably, while some previous work on prompt tuning has addressed generalization across tasks and models (Su et al., 2022; Vu et al., 2021; Qin et al., 2023), and domain adaptation (Tam et al., 2022; Guo et al., 2022), this paper specifically focuses on the generalization issue of gradient-free prompt optimization."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we revealed the generalization issue of prompt optimization for LLMs under distribution shifts. We observed that the prompt optimized on the source data group may have a performance drop on the target group with distribution shifts. We performed an initial analysis aiming at identifying the factors that correlate to the varied generalization performance across groups, including label distribution shift and input distribution sim-\nilarity. To enhance the generalization ability of LLMs, we proposed a Generalized Prompt Optimization framework to jointly consider the source and target groups for robust prompt optimization. Experimental results validated the effectiveness of our proposed framework in boosting the robustness of the prompts on the source and target groups. In future work, we plan to study the prompt generalization to unseen target groups without available inputs {xt}, and explore prompt generalization ability with in-context examples from different groups."
        },
        {
            "heading": "Limitations",
            "text": "Firstly, this work discusses the generalization ability of prompts while ignoring the effect of other LLM inputs such as in-context examples. The choice of in-context examples might also affect the robustness of LLMs. Future work can look into the generalization issue of the prompt in combination with in-context examples. Secondly, this work assumes the availability of the inputs {xt} of the target group. It is under-explored how to achieve generalized prompt optimization to completely unseen groups without {xt}. To improve the robustness on these groups, we believe it is helpful to extend this work toward robust prompt optimization on multiple heterogeneous groups. Thirdly, we acknowledge that the scope of our research is limited to black-box LLMs capable of understanding instructions, where gradient-free prompt optimization with instructing LLM is a suitable choice. For smaller LMs without instruction understanding abilities, e.g., BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), they are generally not black-box and are more advantageous to utilize gradient-based prompt optimization methods."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by NExT Research Center, and the National Natural Science Foundation of China (62272437). We thank the reviewers for their constructive feedback."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Dataset Details",
            "text": "For each dataset, we use the original training set to split into training and validation sets, and randomly sample a subset from the original validation set as our test set as sometimes the labels for the original test set are not available. Following the official implementation of APE 3, we split the original training set with 1000 training samples, and the rest as validation samples. For MNLI, we sample the same number of matched and mismatched validation data as the test set. For ANLI, we use R2. For Yelp and Flipkart, we assign the review scores of 0 and 1 as negative, 3 as neutral, and 4, 5 as positive. For multi-turn dialog reasoning, we select the instances of MuTual within 5 dialog turns, Ubuntu and DSTC7 within 7 dialog turns, and reduce the number of choices to 4 for all three datasets. We show an example of LLM input for each task in Table 11, and the dataset statistics in Table 10."
        },
        {
            "heading": "A.2 Additional Implementation Details for Preliminary Experiments",
            "text": "The APE performs prompt optimization by iteratively generating and selecting the prompts leveraging LLM. For prompt generation, it utilizes a meta prompt to instruct LLM to infer prompts from given input-output examples. Then, the generated prompts are evaluated on validation data to select the prompts with good task performance. After that, APE leverages LLM to perform Monte Carlo search by iteratively paraphrasing the current effective prompts and performing evaluation on them to obtain optimized prompts.\nFollowing the official implementation, for prompt generation, the sampled N -shot training data are divided into K splits to generate K\n3https://github.com/keirp/automatic_prompt_ engineer/tree/main.\nprompts by LLM for further selection. For each task, we try the value of N as 9, 16, 25, 36, and K as N \u2019s factors, to ensure obtaining effective prompts, where APE is not very parameter sensitive. Moreover, we ablate the Monte Carlo search since it is optional and not significant for our tasks.\nGiven the randomness of the backbone LLM, we set the temperature of the LLM as 0, top p as 1.0. We set the max tokens for prompt generation as 100 to try to ensure no truncation, and keep other LLM parameters the same as the official APE implementation. The parameters N and K are shown in Table 10."
        },
        {
            "heading": "A.3 Additional Details and Results for the Exploration on the Factors Affecting Prompt Robustness.",
            "text": "Calculation of Q1 Metrics. The label distribution shift quantifies the divergence of the label distributions between two groups for classification tasks, calculated by the KL divergence of their label distributions,\nDKL = \u2211 y\u2208Y Prs(y)log( Prs(y) Prt(y) )\nwhere Y is the label space of the task, and Prs(y) and Prt(y) denote the probability of the label y in the source and target groups, respectively.\nThe input similarity quantifies the n-gram similarity of the input corpuses of the two groups. Suppose that we sample M inputs from the source and target groups respectively, denoted as xs = {xs1 , ..., xsM } and xt = {xt1 , ..., xtM }, we calculate the Spearman\u2019s rank order correlation between the bag-of-word vectors of xs and xt,\n\u03c1 = cov(Vs, Vt)\n\u03b4(Vs)\u03b4(Vt)\nwhere Vs and Vt denotes the ranked bag-of-word vectors of xs and xt on the vocabulary of xt.\nCalculation of Q2 Metrics. We sample the same amount of inputs from SocialIQA, PIQA and OpenbookQA, and denote the input corpuses as x1, x2 and x3. Firstly, we calculate the proportion of unique n-grams for each group against the number of all n-grams for the three corpuses as\n|n-grams(xi)| |n-grams({x1, x2, x3})| , i = 1, 2, 3\nwhere n-gram(\u00b7) returns the set of unique n-grams, and the braces denotes mixing the inputs.\nSecondly, we think the source group that has already covered a larger proportion of n-grams of the target group may promote better generalization, and we calculate the proportion of n-gram coverage between the source and target groups as\n|n-grams(xs) \u2229 n-grams(xt)| |n-grams(xt)|\nFor both metrics, the n-gram(\u00b7) is calculated as both word 1-gram and character 4-gram using scikit-learn.\nQ1 Metrics for More Tasks. Table 12 and Table 13 show the two Q1 metric results for commonsense QA and Dialog tasks. Linking the results with the generalization performance in Table 1 and Table 2, we have the following observations. 1) For each target group of the commonsense QA task, the largest value for input similarity coheres with the best generalization performance, but the smallest value of label distribution shifts does not correlate to the best generalization performance. 2) For the Dialog groups, the zero label distribution shifts and the close input similarities cohere with the subtle generalization performance difference on each target group. 3) The evaluation metrics cannot be compared across target groups nor across tasks. e.g., the source group SocialIQA performs better on PIQA than OpenbookQA (cf. Table 2), but the input similarity is higher for OpenbookQA. Also, MuTual has smaller input similarity with Ubuntu (input similarity is 0.56, and generalization performance is 74.7) but better generalization performance than PIQA generalizing to SocialIQA (input similarity is 0.57, and generalization performance is 68.9) (cf. Section 2). These findings reveals the benefits and limitations of the Q1 metrics."
        },
        {
            "heading": "A.4 Details for Baseline Implementation",
            "text": "For all compared methods, the LLM parameters such as temperature, top p, max tokens are the same as in Appendix A.2. The implementation and results for APE follow the preliminary experiments as illustrated in Appendix A.2 and Section 2. For APO, we follow the original parameter setting except for number of optimization step as 1 because the three tasks do not need multi-round optimization. For GPO, the value K is unchanged from APE. The consistency threshold for GPO are 0.83 (5 out of 6 prompts) for sentiment analysis and commonsense QA, and 0.33 (2 out of 6 prompts) for DROP. Note that APE and APO are not designed to utilize the unlabeled target group data so we only observe the direct generalization performance, while APE-ut and GPO utilize the N -shot source group data and N -shot target group data. All of the above methods do not need to apply Monte Carlo search following the official implementation of APE. We use one 32GB GPU to perform in-\nference for Vicuna models. We present the meta prompt of APE and APE-ut, the initial prompt for APO, the human-written prompts, the revised versions by PromptPerfect here.\n\u2022 APE meta prompt: I provide my friend with an instruction. Based on the instruction, I gave him several inputs, and he generated the corresponding outputs. Here are the input-output examples:[DEMO]. Please briefly illustrate the instruction and describe the output format. The instruction is to\n\u2022 APE-ut meta prompt: I provide my friend with an instruction. Based on the instruction, I gave him several inputs, and he generated the corresponding outputs. Here are the input-output examples:[Source]. Here are also some unlabeled examples. Please consider these examples as well for prompt generation:[Unlabeled Target].Please briefly illustrate the instruction and describe the output format. The instruction is to\n\u2022 APO initial Prompts: For Yelp: Provide a sentiment analysis of the following text. Answer Positive Neutral or Negative as labels. For SocialIQA: Give answer to the following multi choice question. Provide only the single letter as labels. For Number: Answer the following question based on the context which involves numerical calculation. Provide only the numerical value that directly answers the question.\n\u2022 Human Prompts: For sentiment analysis: Provide a sentiment analysis of a given input text. The output format is a single word indicating whether the sentiment is positive, negative, or neutral. For commonsense QA: Give answer to the following multi choice question which involves commonsense knowledge. Provide only the single letter (a, b, c, or d) as labels. For DROP: Answer the following question based on the context which involves numerical reasoning. Provide only the direct answer the question, which can be a numerical value or a short string.\n\u2022 PromptPerfect: For sentiment analysis: Your task is to perform a sentiment analysis on a given input text and provide a single word indicating whether the sentiment is positive, negative, or neutral. The input text may contain any language or style of writing. Please ensure that your analysis takes into account the overall tone and context of the text.Your response should be concise and clear, providing a single word that accurately reflects the sentiment of the input text. If there are multiple sentiments present in the text, please choose the one that best represents the overall feeling conveyed by the author.Please note that your analysis should take into account all relevant factors, such as tone, language use, and content. Your response should also be flexible enough to allow for various types of input texts. For commonsense QA: Please choose the best answer for the following multiple choice question. Choose the one answer that best fits the given scenario. Please provide only the single letter (a, b, c, or d) as labels. For DROP: Your task is to answer a numerical question based on a given context involving numerical reasoning. Please provide a direct answer to the question, which can be a numerical value or a short string.Please note that your response should be concise and directly answer the question. The question may involve various numerical data, such as percentages, averages, or counts. You should focus on identifying the relevant information and providing a clear and accurate answer.Additionally, please ensure that your response is flexible enough to allow for various relevant and creative answers based on the context provided."
        },
        {
            "heading": "A.5 Case Study",
            "text": "We present a case study by presenting the best prompt among the five runs for sentiment analysis and DROP as shown in Table 14. We can observe that the optimized prompt for a single group often contains group-specific background information as highlighted by underline which may hinder robust prompt generalization. On the contrary, the optimized prompts of GPO are more general and thus performs well on both groups. Note that for Spans, the optimized prompt is also general enough and thus can generalize well to Number as shown in\nTable 2."
        },
        {
            "heading": "A.6 Study on the Impact of the Number of Candidate Prompts",
            "text": "We examine the effect of varying the number of candidate prompts K on GPO performance in our 36-shot sentiment analysis task. We test the K values in {3, 6, 9, 12, 18}. The results on the target group Flipkart are shown in Table 15. We observe that the generalization performance stabilizes as K reaches a specific value, in this case is 6, indicating that further generating more prompts are unlikely to yield significant improvements in performance."
        }
    ],
    "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
    "year": 2023
}