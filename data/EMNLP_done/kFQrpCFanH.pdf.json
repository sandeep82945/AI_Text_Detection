{
    "abstractText": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPTseries) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose DYNOSAUR, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, DYNOSAUR offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than ALPACA and FLAN on SUPER-NI and LONGFORM with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https:// github.com/WadeYin9712/Dynosaur.",
    "authors": [
        {
            "affiliations": [],
            "name": "Da Yin"
        },
        {
            "affiliations": [],
            "name": "Xiao Liu"
        },
        {
            "affiliations": [],
            "name": "Fan Yin"
        },
        {
            "affiliations": [],
            "name": "Ming Zhong"
        },
        {
            "affiliations": [],
            "name": "Hritik Bansal"
        },
        {
            "affiliations": [],
            "name": "Jiawei Han"
        },
        {
            "affiliations": [],
            "name": "Kai-Wei Chang"
        },
        {
            "affiliations": [],
            "name": "\u00a7UCLA \u2663Peking"
        }
    ],
    "id": "SP:f29e30d15f1d331073798ef22145fda7fa3d4d71",
    "references": [
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma"
            ],
            "title": "A general language assistant as a laboratory",
            "year": 2021
        },
        {
            "authors": [
                "jan Chhablani",
                "Han Wang",
                "Jason Fries",
                "Maged Alshaibani",
                "Shanya Sharma",
                "Urmish Thakker",
                "Khalid Almubarak",
                "Xiangru Tang",
                "Dragomir Radev",
                "Mike Tian-jian Jiang",
                "Alexander Rush"
            ],
            "title": "PromptSource: An integrated development environment",
            "year": 2022
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Magdalena Biesialska",
                "Katarzyna Biesialska",
                "Marta R. Costa-juss\u00e0."
            ],
            "title": "Continual lifelong learning in natural language processing: A survey",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6523\u20136541,",
            "year": 2020
        },
        {
            "authors": [
                "Databricks."
            ],
            "title": "Databricks\u2019 dolly, a large language model trained on the databricks machine learning platform",
            "venue": "https://github.com/ databrickslabs/dolly.",
            "year": 2023
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charlie Snell",
                "Xinyang Geng",
                "Hao Liu",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "The false promise of imitating proprietary llms",
            "venue": "arXiv preprint arXiv:2305.15717.",
            "year": 2023
        },
        {
            "authors": [
                "Or Honovich",
                "Thomas Scialom",
                "Omer Levy",
                "Timo Schick."
            ],
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "venue": "arXiv preprint arXiv:2212.09689.",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Uri Shaham",
                "Samuel R Bowman",
                "Omer Levy."
            ],
            "title": "Instruction induction: From few examples to natural language task descriptions",
            "venue": "arXiv preprint arXiv:2205.10782.",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Po-Nien Kung",
                "Fan Yin",
                "Di Wu",
                "Kai-Wei Chang",
                "Nanyun Peng."
            ],
            "title": "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks",
            "venue": "The 2023 Conference on Empirical Methods in Natural Language Processing",
            "year": 2023
        },
        {
            "authors": [
                "Abdullatif K\u00f6ksal",
                "Timo Schick",
                "Anna Korhonen",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Longform: Optimizing instruction tuning for long text generation with corpus extraction",
            "year": 2023
        },
        {
            "authors": [
                "Lagunas",
                "Alexander Rush",
                "Thomas Wolf."
            ],
            "title": "Datasets: A community library for natural language processing",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175\u2013184, Online",
            "year": 2021
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Sida Wang",
                "Xi Victoria Lin",
                "Robin Jia",
                "Lin Xiao",
                "Xiang Ren",
                "Wen-tau Yih."
            ],
            "title": "On continual model refinement in out-of-distribution data streams",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V Le",
                "Barret Zoph",
                "Jason Wei"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "venue": "arXiv preprint arXiv:2301.13688",
            "year": 2023
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Chatgpt",
            "venue": "https://openai.com/blog/ chatgpt/. Accessed on May 3, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Arun Raja",
                "Manan Dey"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Shailaja Keyur Sampat",
                "Siddhartha Mishra",
                "Sujan Reddy A",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen."
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Wu",
                "Abdul Waheed",
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed",
                "Alham Fikri Aji."
            ],
            "title": "Lamini-lm: A diverse herd of distilled models from large-scale instructions",
            "venue": "arXiv preprint arXiv:2304.14402.",
            "year": 2023
        },
        {
            "authors": [
                "Fan Yin",
                "Jesse Vig",
                "Philippe Laban",
                "Shafiq Joty",
                "Caiming Xiong",
                "Chien-Sheng Jason Wu."
            ],
            "title": "Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning",
            "venue": "arXiv preprint arXiv:2306.01150.",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric Xing"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "arXiv preprint arXiv:2211.01910.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "By leveraging the existing annotated datasets, DYNOSAUR offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than ALPACA and FLAN on SUPER-NI and LONGFORM with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better.\nCode and data are available at https:// github.com/WadeYin9712/Dynosaur."
        },
        {
            "heading": "1 Introduction",
            "text": "Instruction tuning (Sanh et al., 2022; Ouyang et al., 2022; Wei et al., 2022) enables large language models (LLMs) (Raffel et al., 2020; Brown et al., 2020; Touvron et al., 2023) to provide appropri-\n\u2217These four authors contributed equally.\nate output according to input instructions. Existing approaches compile instruction-tuning datasets mainly by 1) manual annotations or 2) distillate from a larger size of LLM. For example, SUPERNATURALINSTRUCTION (SUPER-NI) (Wang et al., 2022b) and DOLLY (Databricks, 2023) recruit experts to manually annotate task instructions and related task data. Despite their high quality, this approach is labor-intensive and costly (Honovich et al., 2022a). Recent efforts (Wang et al., 2022a; Taori et al., 2023) leverage GPT-series to distill instruction tuning data to train smaller models. However, subsequent studies (Gudibande et al., 2023) argue that these methods merely help smaller models learn to mimic the style of teacher LLMs without inheriting their true capabilities, such as factuality and problem solving skills. We suspect it is mainly due to the instructions not ground to actual data.\nIn this paper, we propose DYNOSAUR, a dynamic growth paradigm to convert high quality annotations from dataset repositories into instructiontuning data. In particular, DYNOSAUR generates instructions based on the metadata of existing datasets in the dynamically growing Huggingface Datasets Platform (Lhoest et al., 2021). As shown in Figure 1, metadata covers essential information about a dataset, including dataset description (\u201cA collection of ... ebooks ...\u201d), dataset name (\u201cGutenburg_English\u201d), data fields (\u201ctitle\u201d, \u201ctext\u201d, ..., \u201cissued\u201d) and dataset annotations. Guided by metadata, our method can generate multiple tasks applicable for forming instruction-tuning data with instances in NLP datasets. We leverage LLMs to harvest task instructions and their corresponding input/output fields with a single prompt. Prompted with dataset description involving ebooks and data fields about the book published information, LLMs can synthesize instructions such as \u201cGiven a Gutenburg passage, generate its title\u201d and \u201cPredict the year when the book is published based\non book title and authors\u201d. These instructions reflect the original data domain and use multiple dataset components.\nIn the meantime, LLMs also determine which data fields should be used to construct corresponding task inputs/outputs according to generated instructions. As illustrated in Figure 1, LLMs capture corresponding input fields \u201ctitle\u201d and \u201cauthor\u201d and output field \u201cissued\u201d for the generated task about predicting issued years given book title and authors. Subsequently, all the data under \u201ctitle\u201d and \u201cauthor\u201d fields are used as the final inputs of the generated task, and the data under \u201cissued\u201d are treated as final outputs. Suppose that we generate N instructions based on the metadata of a dataset which contains M instances, our method can synthesize N \u00d7M instruction-tuning data.\nDYNOSAUR offers several advantages:\nLow Conversion Cost. As DYNOSAUR leverages existing annotated data, it reduces the number of queries to larger LLMs for generating instructions. For example, it costs only $11.5 USD to query GPT-3.5-turbo (OpenAI, 2023) and generate 800K instruction-tuning data based on annotated datasets. In contrast, both ALPACA and INSTRUCTION GPT-4 cost around $500 USD to generate a significantly smaller dataset of 52K instances. Despite the lower cost of querying LLMs, DYNOSAUR generates high-quality data by effectively leveraging existing annotations.\nEffectiveness of Instruction-Tuning Data. We evaluate the data effectiveness by studying whether models trained with DYNOSAUR can achieve competitive performance on SUPER-NI, LONGFORM (K\u00f6ksal et al., 2023) and USERINSTRUCTION-252 (Wang et al., 2022a). On SUPER-NI, both T5-3B and LLAMA-7B models fine-tuned with DYNOSAUR outperform ALPACA, INSTRUCTION GPT-4 and DOLLY that are much more expensive to be collected. In particular, training T5-3B with DYNOSAUR brings 2.5-22 ROUGE-L improvement than other datasets. On LONGFORM, training T5-3B with DYNOSAUR is 2.8-12.8 METEOR better than training with other human-curated instruction data such as PROMPTSOURCE (Sanh et al., 2022) and FLAN (Wei et al., 2022). On USER-INSTRUCTION-252, DYNOSAUR can be exploited as additional training data to achieve higher performance than solely training with either ALPACA or INSTRUCTION GPT-4.\nSupporting Continuously Improving Models with New Instruction Data. Statistics show that an average of 143.6 datasets are added to Huggingface Datasets daily in 2023. Because of the low conversion cost, DYNOSAUR can grow dynamically as the platform expands without much effort.\nAn ever-growing instruction-tuning dataset provides an opportunity to continuously improve instruction-following models. Suppose we have a model trained with K tasks (MK) and newly obtain L training tasks. How can we train MK with\nthe L new tasks to 1) achieve better generalization on unseen tasks and the new L tasks and 2) suffer less from forgetting the previous K training tasks? We propose several continual learning strategies specifically for instruction tuning which select replay tasks based on the diversity of instruction and data representations. Experiments with SUPER-NI and DYNOSAUR show that replaying is effective to improve generalization and mitigate forgetting. Besides, once L new tasks are used for training, replaying previous tasks with the least similar instructions to the L tasks performs the best."
        },
        {
            "heading": "2 Collection of DYNOSAUR Data",
            "text": "In this section, we introduce how to construct the DYNOSAUR dataset. As shown in Figure 1, we first collect metadata from existing datasets, then prompt LLM to create tasks based on the metadata, and filter out invalid ones."
        },
        {
            "heading": "2.1 Metadata Collection",
            "text": "Metadata contains key information about an NLP dataset that contributes to instruction-tuning data generation. It covers the following elements:\nDataset Name. Dataset name sometimes provides useful information to help us identify the domain and task category of a dataset. For example, dataset names with \u201cbio\u201d usually indicate that the dataset is in the biological domain; names with \u201cnli\u201d may suggest that the dataset is originally designed for natural language inference tasks.\nDataset Description. Dataset description offers more detailed information about the motivation for building a dataset, the summary of dataset contents, and its supported tasks. It facilitates LLM to create instructions by supplying extra information about the dataset domain and initial dataset design.\nData Fields and Dataset Annotations. Data fields are the keys included in dataset annotations. For example, given an instance {\u201ctitle\u201d: ..., \u201ctext\u201d: ..., \u201cauthor\u201d: ..., \u201csubjects\u201d: ..., \u201cissued\u201d: ...}, the data fields are \u201ctitle\u201d, \u201ctext\u201d, \u201cauthor\u201d, \u201csubjects\u201d and \u201cissued\u201d. When LLM generates task instructions, it needs to determine which fields can be used as task inputs/outputs according to the semantics of data field names and contents of the data fields.\nAll the metadata components are collected from the Huggingface Datasets Platform. We only col-\nlect the metadata from datasets whose licenses allow adaptation. More details are in Appendix A."
        },
        {
            "heading": "2.2 Instruction and Input/Output Field Generation",
            "text": "For each dataset accompanied by processed metadata, we then deploy LLM to generate multiple tasks associated with it. For each task, LLM generates a specific task instruction and designates its input/output fields simultaneously. As exemplified in Figure 1, LLM is expected to generate an instruction \u201cGiven a Gutenburg passage, generate its title\u201d, its input field \u201ctext\u201d, and the output field \u201ctitle\u201d.\nTo accomplish this, we harness the power of incontext learning (Brown et al., 2020). Concretely, we wrap the information of each dataset into a dictionary format and construct four demonstrations manually. Due to the length limitation of the LLM, we use two of them each time as part of the input. Depending on whether or not the incorporating dataset descriptions in the input prompt, we consider the following two configurations:\nDescription-Aware Generation. To maximize the utilization of information present in the dataset description, we incorporate metadata of the two demonstration datasets as well as the new dataset where we plan to generate new tasks as input. The benefit is that LLM can infer the underlying purpose of the dataset creation, thereby generating the most aligned tasks with the original intent. In this setup, LLM generates new tasks, with the input prompt being: \u201cNow given a dictionary as input, please help us to generate new tasks. You may stop when there is no more plausi ble task.\u201d and requirements being \u201cNote that the input and output fields should not be duplicated and should both appear in [data fields]. Each task should still be a dictio nary, containing no text or explanations outside the dictionary.\u201d The full prompt is shown in Appendix B. This setting, however, still has limitations: firstly, comprehensive metadata may not be available for certain datasets; secondly, LLM exhibits a proclivity towards dataset descriptions, leading to homogenization of the generated tasks. To mitigate these issues, we additionally introduce the following setup.\nDescription-Unaware Generation. To fully exploit the annotations and distinct data fields, we exclude the dataset description from the\ninput, thereby allowing the LLM to freely generate diverse task instructions and input/output fields. In this scenario, the dataset can be perceived as a description-less database, with the LLM generating diverse potential tasks based on the valid fields within it. For instance, the data fields in the Wikipedia-based QA dataset may encompass \u201ctitle\u201d, \u201ccontext\u201d, \u201cquestion\u201d, and \u201canswers\u201d. Possible new tasks could include Wikipedia article generation (\u201ctitle\u201d\u21d2\u201ccontext\u201d), Wikipedia title generation (\u201ccontext\u201d\u21d2\u201ctitle\u201d), and open-domain QA question generation (\u201canswer\u201d\u21d2\u201cquestion\u201d).\nBy integrating these two settings, we ensure the preservation of the original intent of all datasets, while leveraging the creativity of LLM to delve deeper into the inherent potential in existing data."
        },
        {
            "heading": "2.3 Post-Processing",
            "text": "Filtering Invalid Tasks. Even though we describe the requirements for a valid task in the prompt, LLM sometimes neglects the requirements and generate invalid tasks. We filter out tasks with three criteria: 1) tasks with non-existent data fields (for instance, a task with the output field \u201ccontent\u201d is invalid given the data in Figure 1); 2) tasks with more than one output fields; 3) tasks whose input/output fields overlap. Moreover, we remove duplicate tasks created during both the descriptionaware and -unaware generation.\nOrganizing Instruction Data. We organize the instruction data in the form of \u201cinstruction\u201d, \u201cinput\u201d, and \u201coutput\u201d. Given an instance of a dataset and a generated task containing the instruction, input fields, and the output field, the \u201cinstruction\u201d is the generated instruction and the \u201coutput\u201d is the value of the output field. If there is only one input field, the \u201cinput\u201d is the value of the input field; otherwise, the \u201cinput\u201d describes all the input fields with the format \u201cThe [field name] is [value of the field].\u201d"
        },
        {
            "heading": "Adding Label Spaces for Classification Tasks.",
            "text": "As we only showcase several dataset instances to LLMs, it does not know the entire label space when generating a classification task. As a result, the generated instruction may not contain the label space knowledge adequately. To overcome this issue, we automatically add the label space information in the instruction of classification tasks. We simply treat a task with less than 10 distinct outputs as\na classification task, and add \u201cAnswers must be one of [distinct outputs].\u201d to the end of the instruction. We also discard classification tasks with extremely imbalanced distributions (e.g., only one distinct output value) in this step."
        },
        {
            "heading": "2.4 Statistics and Cases",
            "text": "In total, we collect 2,911 English datasets from the Huggingface Datasets Platform as of Feb 23, 2023. We then feed them to GPT-3.5-turbo (OpenAI, 2023) and generate 13,610 tasks, of which 5,740 are valid and distinct. For each task, we sample up to 200 instances, ending in 801,900 instances that form the DYNOSAUR dataset. The diversity of the instructions is shown in Figure 3. Following the approach of Wang et al. (2022a), we plot the top 20 most prevalent root verbs and their top 4 direct nouns, each of which appears at least 5 times. The instructions are quite diverse, especially when considering we only have a total of 4 demonstrations.\nFigure 2 demonstrates examples of datasets and corresponding tasks. The dataset name, dataset description, data fields, and annotations are all used by LLM to design the tasks. LLM infers from the dataset name that it is about anaphor agreement and include this information in the instruction. In Example 2, LLM creates the task of paraphrase identification by understanding the relationship between the fields \u201csentence1\u201d and \u201csentence2\u201d implied in the dataset description. Under the descriptionunaware setting like Example 3, tasks can be generated based on the names of data fields."
        },
        {
            "heading": "3 Experiments",
            "text": "We conduct two sets of experiments to evaluate the quality of DYNOSAUR. We first evaluate models trained with DYNOSAUR on SUPER-NI and LONGFORM to examine its ability to solve NLP tasks. Then we run a human evaluation to examine if DYNOSAUR helps in user-oriented situations."
        },
        {
            "heading": "3.1 Automatic Evaluation on SUPER-NI and",
            "text": "LONGFORM\nExperimental Settings. We fine-tune T5-3B and LLAMA-7B with a variety of instruction-tuning datasets, including DYNOSAUR, SUPER-NI training set, ALPACA, etc. LLAMA-7B is fine-tuned with LORA (Hu et al., 2022), an efficient finetuning approach. We also compare with larger models, including models based on T5-11B, T0 and T0++ (Sanh et al., 2022), Tk-Instruct (Wang"
        },
        {
            "heading": "Models Data Size ROUGE-L",
            "text": ""
        },
        {
            "heading": "Models Data Size ROUGE-L",
            "text": "et al., 2022b) and GPT-3 fine-tuned on PROMPTSOURCE (Bach et al., 2022) and SELF-INSTRUCT.\nTo alleviate the effect of data size disparity, instead of training models with the entire DYNOSAUR, we sample a subset that shares a similar data scale with other instruction-tuning datasets. Specifically, we select 681 tasks from DYNOSAUR as training tasks and sample mostly 100 instances for each selected task, resulting in 66,695 instances in total. For SUPER-NI training set, we also select 681 tasks which are 90% out of all SUPER-NI training tasks and 67,825 instances. The rest 10% tasks are left as the validation set for SUPER-NI evaluation experiments. We also sample 67K data from PROMPTSOURCE and FLAN.\nDuring task selection of SUPER-NI , we ensure that all the selected tasks have distinct categories from SUPER-NI test tasks. Concretely, we use\nGPT-3.5-turbo as task category classifier1 to categorize each task into one of 76 task categories in SUPER-NI and avoid selecting tasks with test task categories. Details about fine-tuning hyperparameters and training task selection are shown in Appendix C, E.1 and E.2. Following the original evaluation on SUPER-NI and LONGFORM, we leverage ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) as the metrics.\nFor all the evaluation experiments, we follow the Self-Instruct paper\u2019s setting and exclude all the positive and negative examples written in SUPERNI instructions. It is for fair comparison with the datasets that contain instructions without any examples, such as ALPACA, INST. GPT-4 and DOLLY.\n1We evaluate the GPT-3.5-turbo classifier upon the human evaluation from Amazon MTurk on 200 classification results. The performance is 96.5%, suggesting the preciseness of removing tasks belonging to test task categories.\nDYNOSAUR + ALPACA Tie ALPACA\nHelpfulness 18.7% 59.1% 22.2% Honesty 17.5% 65.4% 17.1% Harmlessness 15.5% 70.6% 13.9%\nDYNOSAUR + INST. GPT-4 Tie INST. GPT-4\nHelpfulness 27.8% 42.9% 29.3% Honesty 21.0% 59.9% 19.1% Harmlessness 19.8% 62.3% 17.9%\n(a) DYNOSAUR as a supplement to automatically generated instructions ALPACA and INST. GPT-4.\nDYNOSAUR Tie SUPER-NI Helpfulness 19.5% 61.5% 19.0% Honesty 15.5% 71.8% 12.7% Harmlessness 13.5% 73.8% 12.7%\nDYNOSAUR SUPER-NI + ALPACA Tie + ALPACA\nHelpfulness 17.1% 65.5% 17.4% Honesty 19.5% 59.9% 20.6% Harmlessness 15.5% 73.4% 11.1%\nDYNOSAUR SUPER-NI + INST. GPT-4 Tie + INST. GPT-4\nHelpfulness 18.2% 63.9% 17.9% Honesty 17.9% 68.6% 13.5% Harmlessness 16.7% 70.2% 13.1%\n(b) Comparing DYNOSAUR and SUPER-NI.\nDYNOSAUR vs. Other Instruction-Tuning Datasets on SUPER-NI. As shown in Table 1, models trained with DYNOSAUR outperform the same models trained with ALPACA, SELFINSTRUCT, INSTRUCTION GPT-4 and DOLLY. In particular, training T5-3B with DYNOSAUR surpasses the variants trained with other datasets by a significant margin around 2.5-22 ROUGEL score. Also, we notice that fine-tuning smaller models with DYNOSAUR also achieves comparable performance than fine-tuning GPT-3 with SELFINSTRUCT and PROMPTSOURCE data.\nDYNOSAUR + SUPER-NI Training Set vs. SUPER-NI Training Set. The combination of DYNOSAUR and SUPER-NI training set can lead to higher performance than training with SUPERNI training set. We first find that integrating DYNOSAUR with SUPER-NI performs better than solely training with SUPER-NI around 1.2 ROUGE-L score in Table 1. This suggests that DYNOSAUR can be considered as a useful supplement for existing instruction-tuning data to further\nenhance model generalizability.\nDYNOSAUR vs. Other Instruction-Tuning Datasets on LONGFORM. To further compare DYNOSAUR and other instruction-tuning datasets that are constructed with existing data, we evaluate them on LONGFORM, a recently released instruction tuning benchmark for evaluating models\u2019 instruction-following ability on long text generation tasks. LONGFORM is equally unseen to all these datasets. As shown in Table 3, DYNOSAUR largely outperforms the other three datasets SUPER-NI, PROMPTSOURCE, and FLAN. In particular, with LLAMA-7B as the base model, DYNOSAUR outperforms the other datasets with 7.5-12.8 METEOR score. LLAMA7B trained with DYNOSAUR even surpasses other 11B instruction-tuned models such as T0++ and Flan-T5 by a large margin.\nAblation Studies. We first evaluate how well models perform when only using either descriptionaware or -unaware instructions as training data. As shown in Table 4, considering both types of in-\nstructions can produce better results than merely relying on description-aware/unaware instructions. We also study if there exists performance drop after we remove the label space descriptions in the instructions. From Table 4, the performance drops 2.6 and 4.2 ROUGE-L for T5-3B and LLAMA-7B.\nDYNOSAUR vs. Larger Models. From Table 1, we observe that T5-3B and LLAMA-7B with DYNOSAUR are comparable with some greater models. For example, our models are competitive with T0++ trained with orders of magnitude more data and 175B GPT-3 w/ SELFINSTRUCT. This further shows the effectiveness brought from DYNOSAUR and implies decent quality of DYNOSAUR."
        },
        {
            "heading": "3.2 Human Evaluation on User Instructions",
            "text": "Experimental Settings. We conduct human evaluation on USER-INSTRUCTION-252, a user-\noriented dataset to test the generation quality in practical domains such as email writing. As there is no test category constraint, we resample 67K data from all the task categories in DYNOSAUR. We fine-tune LLAMA-7B with the resampled data, and keep fine-tuning hyperparameters the same as SUPER-NI evaluation. We recruit annotators from Amazon Mechanical Turk, and ask them to compare two models\u2019 outputs from helpfulness, honesty, and harmless (three criteria proposed by Askell et al. (2021)). See details about sampling tasks for USER-INSTRUCTION-252 evaluation and human evaluation interface in Appendix E.3 and F.\nDYNOSAUR as Augmentation Data to Automatically Generated Instructions. Admittedly, compared to automatically generated instructions whose seed tasks are closer to the ones for daily usage, DYNOSAUR is built upon data from existing NLP tasks and is less involved in user scenarios. However, DYNOSAUR can be used as a supplement to the automatically generated instructions. As shown in Table 2a, training together with DYNOSAUR data outperforms solely trained on ALPACA or INSTRUCTION GPT-4 in the majority of aspects. In particular harmlessness gains a steady boost after incorporating DYNOSAUR.\nDYNOSAUR vs. SUPER-NI. We also compare DYNOSAUR with SUPER-NI, as both of them are constructed from existing task data. Table 2b manifests that the model trained with DYNOSAUR exceeds SUPER-NI on all the three aspects. Moreover, DYNOSAUR is an effective addition to automatically generated instructions like INST. GPT-4 than SUPER-NI."
        },
        {
            "heading": "3.3 Unveiling More Benefits of DYNOSAUR",
            "text": "Beyond the evident advantages in data quality, which correspondingly enhance model performance, we elucidate the additional merits of DYNOSAUR from three perspectives: the validity of data, the cost-efficiency in data construction, and the potential for dynamic data expansion.\nData Validity. We conduct human evaluation to scrutinize the validity of DYNOSAUR. We randomly select 200 task instructions and recruit evaluators from Amazon Mechanical Turk to confirm the data validity. Each evaluator is instructed to choose from four options for each sample: \u201ccompletely reasonable\u201d, \u201cincorrect input\u201d, \u201cincorrect\noutput\u201d, or \u201cincorrect instruction\u201d. In situations where a sample contains multiple errors, the evaluators are directed to highlight the most critical one. Remarkably, 84% of generated instances is completely correct. It is a substantial improvement over the 54% reported in SELF-INSTRUCT.\nData Construction Cost. On average, the cost to formulate a valid task encompassing the generation of the instruction and input/output fields is approximate $0.002. Regarding the subset of our data, DYNOSAUR-sub, utilized in SUPER-NI experiments, we sample 681 tasks and randomly select around 100 instances per task, resulting in a total cost of $1.36. Notably, the full version of DYNOSAUR achieves a data size of 800K instances via generating 5,740 tasks at a total cost of $11.5. This further reveals that our method is cost-efficient, thereby enabling the production of large-scale instruction-tuning datasets.\nDynamic Growth of Data. The inherent design of DYNOSAUR fosters a capacity for dynamic growth, aligning seamlessly with the ongoing expansion of the Huggingface Datasets Platform. As confirmed by statistics, as of May 20, an average of 143.6 datasets were incorporated into Huggingface daily in 2023, serving continuously as a rich data resource for DYNOSAUR."
        },
        {
            "heading": "4 Continual Learning with Dynamically Growing Datasets",
            "text": "As DYNOSAUR can expand over time as new tasks come in, an important question is how to adapt an instruction-tuned model to new tasks without suffering from catastrophic forgetting. In this section, we examine continual learning as an approach for learning instruction-following models with dynamically growing datasets. We focus on one of the common continual learning techniques (Biesialska et al., 2020), replay methods, which select previously trained tasks for further training stage. We\naim to provide an analysis of how to most effectively select the tasks to replay. We want to answer the following questions: 1) Do we need to replay history tasks? 2) Shall we replay tasks based on instructions or data? 3) Which tasks to replay?.2\nReplay Methods. We compare the following replay strategies: 1) No Replay: Train models without any replay tasks; 2) Instr. Diverse: Replay last stage\u2019s tasks that diverge most from ones in the current stage based on instruction representations; 3) Instr. Similar: Replay last stage\u2019s tasks that are most similar to tasks in the current stage; 4) Instr. Support: Replay the most representative tasks in the last stage; 5) Data Diverse: Replay diverse tasks based on similarity of example data.\nSuppose there are L tasks in the current stage, and K tasks in the previous stage, we use Sentence Transformer (Reimers and Gurevych, 2019) based on RoBERTa-large (Liu et al., 2019) to obtain the instruction representation matrix Ic \u2208 RL\u00d7d for the current stage and Ip \u2208 RK\u00d7d for the previous stage, where d is the representation dimension. Then, we compute the cosine similarity between Ic and Ip, and Ip itself: Scp = cos (Ic, Ip) \u2208 RL\u00d7K , Spp= cos (Ip, Ip) \u2208 RK\u00d7K . Then, Instr. Diverse replays the tasks with the least column sum in Scp. Instr. Similar replays the tasks with the largest column sum in Scp. Instr. Support replays the tasks with the largest row sum in Spp.\nMetrics. Inspired by CL literature (Biesialska et al., 2020; Lin et al., 2022), we design three metrics to quantify to what extent models generalize to new tasks, how well models perform on the training tasks in current stage, and how much models forget the previously trained tasks - Test: ROUGE-L on the test set of SUPER-NI, which represents unseen tasks; Holdout: ROUGE-L on the holdout data of training tasks in current stage; Previous: ROUGEL on the holdout data of training tasks in previous stages. As mentioned in \u00a73.3, 16% of DYNOSAUR data are invalid. To avoid evaluating models on invalid holdout data, we do not report Holdout and Previous results for DYNOSAUR experiments.\nExperimental Settings. We evaluate replay strategies by training T5-3B with SUPER-NI and\n2A concurrent work (Kung et al., 2023) discusses the role of task active learning in effectively improving the generalization ability on unseen tasks. We highlight here that the difference between the two settings is that we consider not only the generalization performance but the performance on history data as well.\nStage 1. Stage 2. Stage 3.\nMethods Test Holdout Test Holdout Previous Test Holdout Previous Full 43.4\nNo Replay 40.6 53.3 40.5 56.3 50.9 43.3 60.1 58.2 / 49.3\nData Diverse\n40.6 53.3 43.0 58.9 53.3 42.8 60.3 60.7 / 52.7 Instr. Diverse 43.6 59.8 54.2 44.8 63.5 59.5 / 53.3 Instr. Similar 42.9 59.2 53.6 41.0 60.0 60.9 / 53.0 Instr. Support 43.4 59.6 54.6 44.6 61.3 59.3 / 53.0\n(a) Continual learning results of T5-3B trained with SUPER-NI. We divide the training set into three stages. For each stage, we report ROUGE-L on the test set, holdout data in current stage, and holdout data in previous stages.\nMethods Stage1 2 3\nFull 40.4\nNo Replay 36.5 37.5 38.2Instr. Diverse 37.9 39.9\n(b) Continual learning results of T5-3B trained with DYNOSAUR on SUPER-NI test set. For simplicity, we only compare no replay with Instr. Diverse, the best replay strategy based on SUPER-NI.\nTable 6: Continual learning results of T5-3B trained with SUPER-NI and DYNOSAUR. \u201cFull\u201d denotes training with entire SUPER-NI and DYNOSAUR at once.\nDYNOSAUR. To simulate continual learning scenarios, we first randomly split both datasets into three groups. Then we train T5-3B for three stages, each stage on one of the groups and 50 replayed tasks from the last stage. For each task, we sample 100 instances for training and another 100 instances for holdout evaluation.\nResults. Displayed in Table 6a, we find that replaying previous tasks not only mitigates forgetting issues, but also helps better generalize to unseen tasks. For example, in Stage 3, No Replay gets 43.3 on test set and 60.1 on the holdout set of Stage 1, while Instr. Diverse achieves 44.8 and 63.5. Further, comparing Instr. Diverse and Data Diverse, we notice that selecting replay tasks based on the diversity of instruction representations better improves unseen task performance (+0.6/+2.0 at Stage 2/3). Besides, Instr. Diverse can even perform better on test set than training with full SUPER-NI data at once.\nWe see similar trends on DYNOSAUR. We select the best replay strategy, Instr. Diverse, based on results on SUPER-NI and compare it with No Replay. As shown in Table 6b, Instr. Diverse outperforms No Replay by 1.7 at Stage 3. Overall, a proper replay strategy can bridge performance gap or even help surpass training with full dataset."
        },
        {
            "heading": "5 Related Works",
            "text": "LLMs can be empowered to follow instructions via instruction tuning (Sanh et al., 2022; Ouyang et al., 2022; Wei et al., 2022; Mishra et al., 2022; Wang et al., 2022b; Chung et al., 2022; OpenAI, 2023; Wang et al., 2022a; Longpre et al., 2023; Taori et al., 2023; Peng et al., 2023; Wu et al., 2023). They fine-tune LLMs with the training data and instructions of diverse upstream training tasks and enable them to do inference on unseen tasks.\nOne branch of instruction-tuning data are constructed with existing human annotations. The instructions in PROMPTSOURCE (Bach et al., 2022) and FLAN (Wei et al., 2022) are created with human-designed templates for limited task categories. NI (Mishra et al., 2022) and SUPER-NI (Wang et al., 2022b) are annotated by NLP practitioners from GitHub and NLP courses. Most recent attempts distill instruction-tuning data from LLMs. The methods proposed in SelfInstruct (Wang et al., 2022a) and Unnatural Instruction (Honovich et al., 2022a) generate novel tasks by prompting LLMs with seed instructiontuning tasks. Other works (Honovich et al., 2022b; Zhou et al., 2022) study instruction generation upon input/output data. There are another type of works simply using structured metadata as instructions (Yin et al., 2023). Different from those works, when we generate DYNOSAUR instructions, the inputs/outputs for the generated tasks are unknown to LLMs. LLMs need to generate instructions from metadata and determine which part of the dataset annotations are task inputs/outputs simultaneously."
        },
        {
            "heading": "6 Conclusions",
            "text": "We propose DYNOSAUR, an automatic paradigm for instruction data construction. We utilize metadata from existing NLP datasets and generate various tasks upon them. DYNOSAUR generation costs significantly lower than other methods, while models trained on DYNOSAUR data outperform models trained on existing human-curated and machinegenerated instruction datasets on SUPER-NI and LONGFORM. Taking advantage of the dynamic growth nature of DYNOSAUR, we further explore specific replay methods for instruction tuning that are effective in mitigating forgetting."
        },
        {
            "heading": "Limitations",
            "text": "Limited Language Scope. DYNOSAUR is only built upon English datasets in Huggingface Datasets. Whereas, multilingual NLP datasets take up a large proportion in the platform. We plan to further curate a multilingual version of DYNOSAUR and conduct comprehensive experiments for evaluating generalization in multilingual settings.\nErrors in Generated Instruction Data. Although the data validity of DYNOSAUR is high, there are still 16% invalid data present in DYNOSAUR. We conduct error analysis (Appendix D) on the 200 instances used for human evaluation in \u00a73.3 and notice that there are still multiple types of errors that have not been resolved yet. We expect to seek better methods to improve the quality of generated instruction data in future works.\nLimited Sampled Dataset Instances. Due to the limits of data storage, we only sample at most 200 instances from each dataset for instruction-tuning data generation. We plan to consider more available instances from selected datasets and further scale up DYNOSAUR.\nDifficulty in Evaluation. It is hard to comprehensively assess the capabilities of instruction-tuned models (Zheng et al., 2023). We make our best efforts to evaluate models on a large-scale benchmark SUPER-NI with diverse tasks, along with human evaluation of user instructions."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our work is based on annotations of existing datasets. As these data may contain selection bias or annotation bias, the bias may be inherited in our paradigm. We recruit annotators for human evaluation of data validity and task category classification from Amazon Mechanical Turk. All annotators are fairly paid approximately $12 per hour."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank UCLA-NLP lab members and anonymous reviewers for their valuable feedback. The research is supported in part by ONR grant N0001423-1-2780, DARPA MCS program under contract number N660011924032, and an Amazon AWS credit award. Da Yin was supported by an Amazon Fellowship, Hritik was supported in part by AFOSR MURI grant FA9550-22-1-0380, Fan was\nsupported in part by CISCO, and Kai-Wei was supported as a Sloan Fellow."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Details of Metadata Collection",
            "text": ""
        },
        {
            "heading": "A.1 Extracting Dataset Name and Description",
            "text": "There are many datasets assumed as a subtask under a parent dataset. For example, SST-2 is included as part of GLUE dataset. Then we concatenate the parent dataset name and the dataset\u2019s own name as final dataset name in collected metadata.\nDataset description is extracted from dataset card. To shorten the input prompt for generating instructions, we only capture the contents in the \u201cDataset Summary\u201d."
        },
        {
            "heading": "A.2 Selecting Licensed Datasets",
            "text": "To properly leverage datasets at Huggingface Datasets, our metadata collection process does not apply on the datasets without any licenses, and the ones with cc-by-nc-nd-4.0, cc-by-nd-4.0, cc-by-nc-nd-3.0, ofl, other, or unknown licenses. The instances of the tasks eventually included in DYNOSAUR are subject to the licenses under which the original dataset was released."
        },
        {
            "heading": "A.3 Removing Index and Nested Fields",
            "text": "Index field frequently exists in datasets, but it should not be considered as a part of task annotations. We remove this field to reduce the effect of irrelevant information. Also, for simplicity, we also remove nested fields whose corresponding values are in a hierarchical dictionary structure."
        },
        {
            "heading": "B Example Prompt of Instruction Generation",
            "text": "We provide an example of the description-aware instruction generation in Table 7. For descriptionunaware generation, the \u201csummary\u201d field is removed from the prompt."
        },
        {
            "heading": "C Details of Fine-Tuning Hyperparameters",
            "text": "We fine-tune T5-3B with every studied instruction dataset for 2 epoches, with batch size 16 and learning rate 1e \u2212 5. We truncate all the input texts to 1024 tokens and limit the maximum output length as 128 tokens. The number of linear warmup steps is set to 600. We follow the hyperparameters of ALPACA in finetuning LLAMA. Models are trained for 3 epoches with batch size 128 and the max length is 512 tokens. Due to memory limit, we apply LORA (low-rank adaptation) in finetuning with learning\nrate 3e \u2212 4, lorar = 8, and loraalpha = 16. All the instruction datasets are finetuned with the same hyperparameters. All the fine-tuning experiments are performed with 48GB NVIDIA A6000 GPUs and 40GB NVIDIA A100 GPUs."
        },
        {
            "heading": "D Error Analysis for DYNOSAUR Data",
            "text": "We conduct error analysis to investigate the error types of generated DYNOSAUR data. Among all 200 instances we evaluate in human evaluation, we find that 4% of the human evaluated instances have incorrect instructions, 5% of the evaluated instances have incorrect inputs, and rest 7% have incorrect outputs. Representative wrong cases are shown in Table 9. The cases with incorrect outputs usually do not follow the format requirements mentioned in instructions. The cases with incorrect inputs are unclear and do not meet the requirements noted in instructions. The incorrect instructions are typically irrelevant with the input/output contents."
        },
        {
            "heading": "E Details of Sampling Strategies",
            "text": ""
        },
        {
            "heading": "E.1 Sampling Tasks for Evaluation on SUPER-NI",
            "text": "Classification tasks take up a great proportion of SUPER-NI test tasks. Meanwhile, most tasks of DYNOSAUR belong to generation tasks. Therefore, we sample classification tasks with a higher probability to enforce models to learn more classification tasks. There are 300 classification tasks among the 681 selected tasks.\nWe also notice that the tasks produced from BigScience and Flax Stack Exchange datasets are more frequently selected. To promote the diversity of training tasks, our sampling method is constrained to select at most 70 tasks with regard to the two datasets. Besides, we discard programming language tasks to mitigate their negative effect on natural language tasks.\nTo make a fair comparison, we also sample 67K training data for PROMPTSOURCE and FLAN, and exclude the task categories of SUPER-NI test tasks. Specifically, we exclude the tasks that belong to Structure-to-text, Natural Language Inference, Coreference Resolution, and the task COPA from both datasets."
        },
        {
            "heading": "E.2 Sampling Tasks for Evaluation on",
            "text": "LONGFORM\nSimilar to the sampling strategy described in Appendix E.1, we also sample 681 tasks from the\nfull version of DYNOSAUR, each with at most 100 instances. As the evaluation tasks in LONGFORM usually have long outputs, we only sample the DYNOSAUR tasks that have the average output length above 50 words."
        },
        {
            "heading": "E.3 Sampling and Processing DYNOSAUR Tasks Tasks for Evaluation on",
            "text": "USER-INSTRUCTION-252\nSimilar to sampling tasks for SUPER-NI evaluation, we also limit the maximum number of selected tasks regarding BigScience and Flax Stack Exchange datasets to 70.\nWe observe that the instructions of many useroriented instruction data are not paired with any input data. For example, the instruction \u201cWhat is the sum of 3 and 5?\u201d does not need any additional input. Thus, after task sampling, we choose part of the instruction data in the sampled tasks and integrate their instructions and corresponding input data to emulate the style of user-oriented instructions. For example, assume that there is an instruction \u201cPlease tell me a book written by the given author\u201d and its corresponding input data \u201cVictor Hugo\u201d. Through prompting GPT-3.5-turbo, we can harvest a new instruction, e.g., \u201cPlease tell me a book written by Victor Hugo\u201d, by combining its input data. We only perform integration on the instruction data whose corresponding input text is shorter than 50 characters. It aims at preventing task instructions from carrying overwhelmed information. The prompt for integration is shown in Table 8."
        },
        {
            "heading": "F Human Evaluation Interface for",
            "text": "USER-INSTRUCTION-252\nWe show the screenshot of human evaluation interface for USER-INSTRUCTION-252 in Figure 4.\nGiven a dictionary containing a dataset description and a few examples, our goal is to design up to three different tasks based on this dataset. Each task should still be a dictionary, including the instruction, input fields and one output field. The following are two examples.\nExample 1: Input: {\u2018task_name\u2019: \u2018squad\u2019, \u2018selected_data\u2019: [{\u2018title\u2019: \u2018University_of_Notre_Dame\u2019, \u2018context\u2019: \u2019Architecturally, the school has a Catholic character. Atop the Main Building\u2019s gold dome is a golden statue of the Virgin Mary. ...\u2019, \u2018question\u2019: \u2018To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\u2019},\n{\u2018title\u2019: \u2018University_of_Notre_Dame\u2019, \u2018context\u2019: \u2018Architecturally, the school has a Catholic character. Atop the Main Building\u2019s gold dome is a golden statue of the Virgin Mary. ...\u2019, \u2018question\u2019: \u2018What is in front of the Notre Dame Main Building?\u2019}], \u2018summary\u2019: \u2018Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\u2019}\nTasks: {\u2018task1\u2019: {\u2018instruction\u2019: \u2018Please answer the question based on the Wikipedia article. The answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\u2019, \u2018input_fields\u2019: [\u2018title\u2019, \u2018context\u2019, \u2018question\u2019], \u2018output_field\u2019: [\u2018answers\u2019]}, \u2018task2\u2019: {\u2018instruction\u2019: \u2018Create a question provided the article.\u2019, \u2018input_fields\u2019: [\u2018context\u2019], \u2018output_field\u2019: [\u2018question\u2019]}, \u2018task3\u2019: {\u2018instruction\u2019: \u2018Can you write a title for the passage?\u2019, \u2018input_fields\u2019: [\u2018context\u2019], \u2018output_field\u2019: [\u2018title\u2019]}}\nExample 2: ...\nNow given a dictionary as input, please help us to generate new tasks. You may stop when there is no more plausible task.\nWe plan to infuse the text inputs into the user instructions. Here\u2019re two examples:\nInstruction: Given a sentiment label, generate a movie review. Input: positive New Instruction: Generate a positive movie review.\nInstruction: Give some examples of what people usually say in the given social situation. Input: when someone arrives safely New Instruction: Give some examples of what people usually say when someone arrives safely.\nNow please do the same thing for new instruction data:"
        }
    ],
    "title": "DYNOSAUR: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
    "year": 2023
}