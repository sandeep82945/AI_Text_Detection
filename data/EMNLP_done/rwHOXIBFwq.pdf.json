{
    "abstractText": "Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall. This paper extends an effective stopping rule using information derived from a text classifier that can be trained without the need for any additional annotation. Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal and RCV1) showed that the proposed approach consistently improves performance and outperforms several alternative methods.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Reem Bin-Hezam"
        },
        {
            "affiliations": [],
            "name": "Mark Stevenson"
        },
        {
            "affiliations": [],
            "name": "Nourah bint Abdulrahman"
        }
    ],
    "id": "SP:9fa14b69092e1562d8e808228dbe64e128f79a25",
    "references": [
        {
            "authors": [
                "Max W Callaghan",
                "Finn M\u00fcller-Hansen."
            ],
            "title": "Statistical Stopping Criteria for Automated Screening in Systematic Reviews",
            "venue": "Systematic Reviews, 9(1):1\u201314.",
            "year": 2020
        },
        {
            "authors": [
                "Gordon Cormack",
                "Maura Grossman."
            ],
            "title": "Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review",
            "venue": "arXiv preprint arXiv:1504.06868.",
            "year": 2015
        },
        {
            "authors": [
                "Gordon V. Cormack",
                "Maura R. Grossman."
            ],
            "title": "Engineering quality and reliability in technologyassisted review",
            "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 75\u201384.",
            "year": 2016
        },
        {
            "authors": [
                "Gordon V Cormack",
                "Maura R Grossman."
            ],
            "title": "Scalability of Continuous Active Learning for Reliable High-Recall Text Classification",
            "venue": "Proceedings of the 25th ACM International Conference on Information and Knowledge Management, pages 1039\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Gordon V. Cormack",
                "Maura R. Grossman",
                "Bruce Hedin",
                "Douglas W. Oard."
            ],
            "title": "Overview of the TREC 2010 Legal Track",
            "venue": "Proceedings of The Nineteenth Text REtrieval Conference, TREC 2010, Gaithersburg, Maryland, USA, November 16-19, 2010, vol-",
            "year": 2010
        },
        {
            "authors": [
                "David R. Cox",
                "Valerie Isham."
            ],
            "title": "Point Processes, volume 12",
            "venue": "CRC Press.",
            "year": 1980
        },
        {
            "authors": [
                "Maura R. Grossman",
                "Gordon V. Cormack",
                "Adam Roegiest."
            ],
            "title": "TREC 2016 Total Recall Track Overview",
            "venue": "Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016, volume 500-321 of NIST Special Publication. National Institute of",
            "year": 2016
        },
        {
            "authors": [
                "Yu Gu",
                "Robert Tinn",
                "Hao Cheng",
                "Michael Lucas",
                "Naoto Usuyama",
                "Xiaodong Liu",
                "Tristan Naumann",
                "Jianfeng Gao",
                "Hoifung Poon."
            ],
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "venue": "ACM Transactions on Computing",
            "year": 2020
        },
        {
            "authors": [
                "Noah Hollmann",
                "Carsten Eickhoff."
            ],
            "title": "Ranking and Feedback-based Stopping for Recall-Centric Document Retrieval",
            "venue": "Working Notes of CLEF 2017 - Conference and Labs of the Evaluation Forum, pages 7\u20138.",
            "year": 2017
        },
        {
            "authors": [
                "Evangelos Kanoulas",
                "Dan Li",
                "Leif Azzopardi",
                "Rene Spijker."
            ],
            "title": "CLEF 2017 Technologically Assisted Reviews in Empirical Medicine Overview",
            "venue": "CEUR workshop proceedings, volume 1866.",
            "year": 2017
        },
        {
            "authors": [
                "Evangelos Kanoulas",
                "Dan Li",
                "Leif Azzopardi",
                "Rene Spijker."
            ],
            "title": "CLEF 2018 Technologically Assisted Reviews in Empirical Medicine Overview",
            "venue": "CEUR workshop proceedings, volume 2125.",
            "year": 2018
        },
        {
            "authors": [
                "Evangelos Kanoulas",
                "Dan Li",
                "Leif Azzopardi",
                "Rene Spijker."
            ],
            "title": "CLEF 2019 Technology Assisted Reviews in Empirical Medicine Overview",
            "venue": "CEUR workshop proceedings, volume 2380.",
            "year": 2019
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "David D Lewis",
                "Yiming Yang",
                "Tony Russell-Rose",
                "Fan Li."
            ],
            "title": "RCV1: A New Benchmark Collection for Text Categorization Research",
            "venue": "Journal of Machine Learning Research, 5:361\u2013397.",
            "year": 2004
        },
        {
            "authors": [
                "Dan Li",
                "Evangelos Kanoulas."
            ],
            "title": "When to Stop Reviewing in Technology-Assisted Reviews",
            "venue": "ACM Transactions on Information Systems, 38(4):1\u201336.",
            "year": 2020
        },
        {
            "authors": [
                "Charles X Ling",
                "Victor S Sheng."
            ],
            "title": "CostSensitive Learning and the Class Imbalance Problem",
            "venue": "Encyclopedia of machine learning, 2011:231\u2013235.",
            "year": 2008
        },
        {
            "authors": [
                "Douglas W Oard",
                "Fabrizio Sebastiani",
                "Jyothi K Vinjumur."
            ],
            "title": "Jointly Minimizing the Expected Costs of Review for Responsiveness and Privilege in E-discovery",
            "venue": "ACM Transactions on Information Systems (TOIS), 37(1):1\u201335.",
            "year": 2018
        },
        {
            "authors": [
                "Nima Sadri",
                "Gordon V Cormack."
            ],
            "title": "Continuous Active Learning Using Pretrained Transformers",
            "venue": "arXiv preprint arXiv:2208.06955.",
            "year": 2022
        },
        {
            "authors": [
                "Ian Shemilt",
                "Antonia Simon",
                "Gareth J Hollands",
                "Theresa M Marteau",
                "David Ogilvie",
                "Alison O\u2019MaraEves",
                "Michael P Kelly",
                "James Thomas"
            ],
            "title": "Pinpointing Needles in Giant Haystacks: Use of Text",
            "year": 2014
        },
        {
            "authors": [
                "Alison Sneyd",
                "Mark Stevenson."
            ],
            "title": "Modelling Stopping Criteria for Search Results using Poisson Processes",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Alison Sneyd",
                "Mark Stevenson."
            ],
            "title": "Stopping Criteria for Technology Assisted Reviews Based on Counting Processes",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201921,",
            "year": 2021
        },
        {
            "authors": [
                "Eugene Yang",
                "David Lewis",
                "Ophir Frieder."
            ],
            "title": "Heuristic Stopping Rules for Technology-Assisted Review",
            "venue": "ACM Symposium on Document Engineering 2021 (DocEng \u201921), pages 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "Eugene Yang",
                "David D Lewis",
                "Ophir Frieder."
            ],
            "title": "On Minimizing Cost in Legal Document Review Workflows",
            "venue": "DocEng 2021 - Proceedings of the 2021 ACM Symposium on Document Engineering, pages 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "Eugene Yang",
                "Sean MacAvaney",
                "David D. Lewis",
                "Ophir Frieder."
            ],
            "title": "Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review",
            "venue": "Proceedings of the 44th European Conference on Information Retrieval, ECIR 2022, page 502\u2013517.",
            "year": 2022
        },
        {
            "authors": [
                "Zhe Yu",
                "Tim Menzies."
            ],
            "title": "FAST2: An Intelligent Assistant for Finding Relevant Papers",
            "venue": "Expert Systems with Applications, 120:57\u201371.",
            "year": 2019
        },
        {
            "authors": [
                "Justin Zobel"
            ],
            "title": "How Reliable Are the Results of Large-Scale Information Retrieval Experiments",
            "venue": "In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 1998
        }
    ],
    "sections": [
        {
            "heading": "1 Background",
            "text": "Information Retrieval (IR) systems often return large numbers of documents in response to user queries and screening them for relevance represents a significant effort. This problem is particularly acute in applications where it is important to identify most (or all) of the relevant documents, for example, systematic reviews of scientific literature (Higgins et al., 2019) and legal eDiscovery (Oard et al., 2018). Technology Assisted Review (TAR) develops methods that aim to reduce the effort required to screen a collection of documents for relevance, such as stopping rules that inform reviewers that a desired level of recall has been reached (and no further documents need to be examined) (Li and Kanoulas, 2020; Yang et al., 2021a).\nOne of the most commonly applied approaches to developing stopping rules is to identify when a particular level of recall has been reached by estimating the total number of documents in the collection (either directly or indirectly). The majority of algorithms using this approach operate by obtaining manual judgements for a sample of the unobserved documents and inferring the total number of relevant documents, e.g. (Shemilt\n1Code to reported results available from https://github. com/ReemBinHezam/TAR_Stopping_CP_CLF\net al., 2014; Howard et al., 2020; Callaghan and M\u00fcller-Hansen, 2020). However, these approaches may not account for the fact that most relevant documents appear early in the ranking, information shown to be useful for stopping algorithms (Cormack and Grossman, 2016a; Li and Kanoulas, 2020), so they may examine more documents than necessary. Counting Processes Counting processes, stochastic models of the number of occurrences of an event within some interval (Cox and Isham, 1980), can naturally model changes to the frequency of relevant document occurances within a ranking. They have been used to develop stopping rules that ensure that a desired level of recall is reached while also minimising the number of documents that need to be reviewed and found to outperform a range of alternative approaches (Sneyd and Stevenson, 2019, 2021; Stevenson and Bin-Hezam, to appear). However, in these methods the estimate of the total number of relevant documents is only based on the examined initial portion of the ranking and no information is used from the remaining documents. Classification Another approach has been to use a text classifier to estimate the total number of relevant documents (Yu and Menzies, 2019; Yang et al., 2021a). This method has the advantage of using information about all documents in the ranking by observing the classifier\u2019s predictions for the documents that have not yet been examined. However, the classifier alone does not consider or model the occurrence rate at which relevant documents have already been observed. These methods were found to be effective, although each was only tested on a single dataset.\nThis paper proposes an extension to stopping algorithms based on counting processes by using a text classifier to inform the estimate of the total number of relevant documents. This approach makes use of information about the relevance of documents from the entire ranking without increas-\ning the number of documents that need to be examined for relevance, since the text classifier is trained using information already available."
        },
        {
            "heading": "2 Approach",
            "text": "We begin by describing the existing counting process approach and then explain how the classifier is integrated. Counting Process Stopping Rule (Sneyd and Stevenson, 2019, 2021) The approach starts by obtaining relevance judgements for the n highest ranked documents. A rate function is then fitted to model the probability of relevant documents being encountered in the (as yet) unexamined documents later in the ranking. The counting process uses this rate function to estimate the total number of relevant documents remaining. Based on this estimate, the algorithm stops if enough relevant documents have been found to reach a desired recall. If not then more documents are examined and the process repeated until either the stopping point is reached or all documents have been examined. Fitting an appropriate rate function is an important factor in the success of this approach. A set of sample points are extracted from the documents for which the relevance judgements are available (i.e. top n), and the probability of a document being relevant at each point is estimated. This set of points is then provided to a non-linear least squares curve fitting algorithm to produce the fitted rate function. For each sample point, the probability of a document being relevant is computed by simply examining a window of documents around it and computing the proportion that is relevant, i.e.\n1\nn \u2211 d\u2208W 1(d) (1)\nwhere W is the set of documents in the window and 1(d) returns 1 if d has been labelled as relevant. Integrating Text Classification Since the existing counting process approach only examines the top-ranked documents, it relies on the assumption that the examined portion of documents provides a reliable indication of the rate at which relevant documents occur within the ranking. However, this may not always be the case, particularly when only a small number of documents have been manually examined or when relevant documents unexpectedly appear late in the ranking. This problem can be avoided by applying a text classifier to the unexamined documents at each stage. The information\nit provides helps to ensure that the rate function is appropriate and that the estimate of the total number of relevant documents provided by the counting process is accurate.\nThe text classifier is integrated into the stopping algorithm in a straightforward way. We begin by assuming that the ranking contains a total of N documents. As before, relevance judgements are obtained for the first n. These judgements are used to train a text classifier which is applied to all remaining documents without relevance judgements (i.e. n + 1 . . . N ). The rate function is now fitted by examining all N documents in the ranking (rather than only the first n), using manual relevance judgements for the first n and the classifier\u2019s predictions for the remainder (i.e. n + 1 . . . N ). As before, the algorithm stops if it has been estimated that enough relevant documents are contained within the first n to achieve the desired recall. Otherwise, the value of n is increased and the process repeated. See Figure 1.\nIt is possible to use information from the classifier in various ways when computing the probability of document relevance during the curve fitting process. Two were explored:\nClassLabel: the class labels output by the classifier are used directly, i.e. using eq. 1 with relevance determined by the classifier.\nClassScore: Alternatively, the scores output by the classifier is used, subject to a threshold of 0.5, i.e. eq. 1 is replaced by\n1\nn \u2211 d\u2208W { score(d) if score(d) \u2265 0.5 0 otherwise (2)\nwhere score(d) is the score the classifier assigned to d. (Using the class score without thresholding was found to overestimate the number of relevant documents.)"
        },
        {
            "heading": "3 Experiments",
            "text": "Experiments compared three approaches. CP: Baseline counting process approach without text classification, i.e. (Sneyd and Stevenson, 2021). CP+ClassLabel and CP+ClassScore: Counting process combined with the text classifier (see \u00a72).\nThe counting process uses the most effective approach previously reported (Sneyd and Stevenson, 2021). A power law (Zobel, 1998) was used as the rate function with an inhomogeneous Poisson process since its performance is comparable with a Cox process while being more computationally efficient. Parameters were set as follows: the confidence level was set to 0.95 while the sample size was initialised to 2.5% and incremented by 2.5% for each subsequent iteration. An existing reference implementation was used (Sneyd and Stevenson, 2021).\nThe text classifier uses logistic regression which has been shown to be effective for TAR problems (Yang et al., 2021a; Li and Kanoulas, 2020). The classifier was based on scikit-learn using TF-IDF scores of each document\u2019s content as features, the content is title and abstract for CLEF datasets, email messages for TREC datasets (including title and message body), and news articles content for the RCV1 dataset. The classification threshold was set to 0.5 (model default) by optimising F-measure for relevant documents using a held-out portion of the dataset. Text classifiers based on BioBERT (Lee et al., 2020) and PubMedBERT (Gu et al., 2020) were also developed. However, neither outperformed the linear logistic regression model on\nCLEF datasets, a result consistent with previous findings for TAR models (Yang et al., 2022; Sadri and Cormack, 2022).\nTAR problems are often highly imbalanced, with very few relevant documents. However, TAR methods successfully place relevant documents early in the ranking (Cormack and Grossman, 2015, 2016b; Li and Kanoulas, 2020). Consequently, the prevalence of relevant documents in the training data is unlikely to be representative of the entire collection, particularly early in the ranking. To account for this cost-sensitive learning was used during classifier training. The weight of the minority class within the training data was set to 1, while the weight of the majority class was set to the imbalance ratio (i.e. count of the minority over the majority class in training data) (Ling and Sheng, 2008)."
        },
        {
            "heading": "3.1 Datasets",
            "text": "Performance was evaluated on six diverse datasets widely used in previous work on TAR. CLEF e-Health (CLEF2017, CLEF2018, CLEF2019) (Kanoulas et al., 2017, 2018, 2019): A collection of systematic reviews from the Conference and Labs of the Evaluation Forum (CLEF) 2017, 2018, and 2019 e-Health lab Task 2: Technology-Assisted Reviews in Empirical Medicine. The datasets contain 42, 30, and 31 reviews respectively. TREC Total Recall (TR) (Grossman et al., 2016) A collection of 290,099 emails with 34 topics related to Jeb Bush\u2019s eight-year tenure as Governor of Florida (athome4). TREC Legal (Legal) (Cormack et al., 2010) A collection of 685,592 Enron emails made available by the Federal Energy Review Commission during their investigation into the company\u2019s collapse. RCV1 (Lewis et al., 2004) A collection of Reuters news articles labelled with categories. Following previous work (Yang et al., 2021b,a), 45 categories were used to represent a range of topics, and the collections downsampled to 20%.\nThe ranking used by the stopping model needs to be the same for all datasets used in the experiments to ensure fair and meaningful comparison. The rankings produced by AutoTAR (Cormack and Grossman, 2015) were used since they allow comparison with a range of alternative approaches (Li and Kanoulas, 2020) and can be generated using their reference implementation."
        },
        {
            "heading": "3.2 Evaluation Metrics",
            "text": "Approaches were evaluated using the metrics commonly used in previous work on TAR stopping\ncriteria, e.g. (Yang et al., 2021a; Li and Kanoulas, 2020), and calculated using the tar_eval opensource evaluation script.2 Average scores across all topics in each collection are reported.\nRecall: proportion of relevant documents identified by the method. Following Li and Kanoulas (2020), results closest to the desired recall are considered best, rather than the highest overall recall.\nReliability (Rel.): percentage of topics where the desired recall was reached (or exceeded). For each topic, reliability is 1 if the desired recall was reached, and 0 otherwise.\nCost: percentage of documents examined. Excess cost (Excess): we introduce this measure which quantifies the additional documents that have to be examined compared with optimal stopping (i.e. stopping immediately when the desired recall has been reached). It is computed as follows:\nexcess cost = cost(method)\u2212 cost(optimal)\n1\u2212 cost(optimal) (3)\nwhere cost(method) and cost(optimal) are the cost of the method being evaluated and stopping at the optimal point, respectively. This metric indicates the proportion of the documents that need to be examined after the desired recall has been reached.\n2https://github.com/CLEF-TAR/tar"
        },
        {
            "heading": "4 Results and Analysis",
            "text": "Experiments were carried out using a range of desired recalls {0.9, 0.8, 0.7} with results shown in Table 1. Results show that combining the classifier with the counting process (CP+ClassLabel and CP+ClassScore) is more effective than using the counting process alone. The improvement in the performance of both approaches compared with CP is statistically significant across topics (p < 0.05, paired t-test with Bonferroni correction). There is a significant reduction in cost which is often substantial (e.g. Legal collection with a desired recall of 0.9). This is achieved with a minimal reduction to the number of relevant documents identified, although the reliability remains unaffected in the majority of cases, and when it is affected (RCV1 collection with a desired recall of 0.9), the reduction is minimal and not statistically significant.\nThe performance of CP+ClassScore and CP+ClassLabel are comparable. The scores for CP+ClassScore may be marginally better, although the difference is only significant in limited circumstances. (Differences in the cost and excess scores were significant for the CLEF 2019 collection with desired recalls of 0.8 and 0.7.) Overall, the way in which the classifier output is integrated into the approach seems less important than the fact it is used at all. The average recall for all approaches exceeds the desired recall, indicating a tendency to be somewhat conservative in proposing the stopping\npoint. However, the classifier allows the algorithm to identify this point earlier, presumably because it has indicated a low probability of relevant documents being found later in the ranking."
        },
        {
            "heading": "4.1 Effect of Cost-sensitive Learning",
            "text": "The effect of using cost-sensitive learning during classifier training was explored by comparing it with the performance obtained when it was not used, see Table 3. These results demonstrate the importance of accounting for class imbalance when training the classifier. The low prevalence of relevant documents can lead to the classifier being unable to identify them resulting in the approach stopping too early, particularly when the desired recall is high."
        },
        {
            "heading": "4.2 Baseline Comparison",
            "text": "Direct comparison of stopping methods is made difficult by the range of rankings and evaluation metrics used in previous work. However, Li and Kanoulas (2020) reported the performance of several approaches using the same rankings as our experiments, allowing benchmarking against several methods. SCAL (Cormack and Grossman, 2016b), and AutoStop (Li and Kanoulas, 2020) sample documents from the entire ranking to estimate the number of relevant documents, SD-training/SDsampling (Hollmann and Eickhoff, 2017) use the scores assigned by the ranking. Results for the\ntarget method (Cormack and Grossman, 2016a) are also reported. Table 2 compares the proposed approach against these baselines for the TR collection for the desired recall levels for which results are available. (Results for other datasets were similar and available in Li and Kanoulas (2020).) CP+ClassScore has a lower cost than the baselines, requiring only a fraction of the documents to be examined, while achieving a higher recall. Although the average recall of some baselines is closer to the desired recall than the proposed approach, their reliability is also lower which indicates that they failed to identify enough relevant documents to achieve the desired recall for multiple topics."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper explored the integration of a text classifier into an existing TAR stopping algorithm. Experiments on six collections indicated that the proposed approach was able to achieve the desired recall level with a statistically significant lower cost than the existing method based on counting processes alone. Integrating the classifier output in the form of labels or scores made little difference, with improvements observed using either approach. The text classifier provides the stopping algorithm with information about the likely relevance of the documents that have not yet been manually examined, allowing it to better estimate the number remaining.\nLimitations\nWe presented a stopping rule for ranked lists. Comparison against multiple datasets requires the same ranking model for all datasets, which may not always be available. In addition, reported results have been limited to a single rate function (power law) and confidence level (0.95). However, the pattern of results for other hyperparameters is similar to those reported.\nEthics Statement\nThis work presents a TAR method designed to reduce the amount of effort required to identify information of interest within large document collections. The most common applications of this technology are literature reviews and identification of documents for legal disclosure. We do not foresee any ethical issues associated with these uses."
        }
    ],
    "title": "Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review",
    "year": 2023
}