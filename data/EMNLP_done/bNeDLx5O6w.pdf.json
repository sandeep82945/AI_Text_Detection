{
    "abstractText": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LMgenerated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment. We release our source code, prompts, and data1 to accelerate future research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Deepak Nathani"
        },
        {
            "affiliations": [],
            "name": "David Wang"
        },
        {
            "affiliations": [],
            "name": "Liangming Pan"
        },
        {
            "affiliations": [],
            "name": "William Yang Wang"
        }
    ],
    "id": "SP:1957533b7edb6bbc458631bd501d8de0fbfd57fe",
    "references": [
        {
            "authors": [
                "Afra Feyza Akyurek",
                "Ekin Aky\u00fcrek",
                "Aman Madaan",
                "A. Kalyan",
                "Peter Clark",
                "D. Wijaya",
                "Niket Tandon."
            ],
            "title": "Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs",
            "venue": "ArXiv, abs/2305.08844. 1, 3, 4",
            "year": 2023
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen."
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "3, 6",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman"
            ],
            "title": "Training verifiers to solve math word",
            "year": 2021
        },
        {
            "authors": [
                "Bhavana Dalvi",
                "Peter Jansen",
                "Oyvind Tafjord",
                "Zhengnan Xie",
                "Hannah Smith",
                "Leighanna Pipatanangkura",
                "Peter Clark."
            ],
            "title": "Explaining answers with entailment trees",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Ryan J. Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "ArXiv, abs/2203.02155. 1",
            "year": 2022
        },
        {
            "authors": [
                "Liangming Pan",
                "Michael Saxon",
                "Wenda Xu",
                "Deepak Nathani",
                "Xinyi Wang",
                "William Yang Wang."
            ],
            "title": "Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies",
            "venue": "3",
            "year": 2023
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott Lundberg",
                "Sameer Singh",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro."
            ],
            "title": "Art: Automatic multistep reasoning and tool-use for large language models",
            "venue": "1",
            "year": 2023
        },
        {
            "authors": [
                "Shishir G. Patil",
                "Tianjun Zhang",
                "Xin Wang",
                "Joseph E. Gonzalez."
            ],
            "title": "Gorilla: Large language model connected with massive apis",
            "venue": "ArXiv, abs/2305.15334. 3",
            "year": 2023
        },
        {
            "authors": [
                "Debjit Paul",
                "Mete Ismayilzada",
                "Maxime Peyrard",
                "Beatriz Borges",
                "Antoine Bosselut",
                "Robert West",
                "Boi Faltings."
            ],
            "title": "Refiner: Reasoning feedback on intermediate representations",
            "venue": "ArXiv, abs/2304.01904. 1, 3, 4",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feed",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "1, 3",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Zhengbao Jiang",
                "Fabio Petroni",
                "Patrick Lewis",
                "Gautier Izacard",
                "Qingfei You",
                "Christoforos Nalmpantis",
                "Edouard Grave",
                "Sebastian Riedel."
            ],
            "title": "Peer: A collaborative language model",
            "venue": "ArXiv, abs/2208.11663. 3",
            "year": 2022
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dong Sheng Li",
                "Weiming Lu",
                "Yue Ting Zhuang."
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face",
            "venue": "ArXiv, abs/2303.17580. 3",
            "year": 2023
        },
        {
            "authors": [
                "Freda Shi",
                "Xinyun Chen",
                "Kanishka Misra",
                "Nathan Scales",
                "David Dohan",
                "Ed Chi",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Large language models can be easily distracted by irrelevant context",
            "venue": "2, 5",
            "year": 2023
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Beck Labash",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao."
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "venue": "1, 3",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca. 5",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Huai hsin Chi",
                "Denny Zhou."
            ],
            "title": "Selfconsistency improves chain of thought reasoning in language models",
            "venue": "ArXiv, abs/2203.11171. 3",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "3, 6",
            "year": 2023
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A. Smith",
                "Mari Ostendorf",
                "Hanna Hajishirzi."
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "3, 4",
            "year": 2023
        },
        {
            "authors": [
                "Rui Yang",
                "Lin Song",
                "Yanwei Li",
                "Sijie Zhao",
                "Yixiao Ge",
                "Xiu Li",
                "Ying Shan."
            ],
            "title": "Gpt4tools: Teaching large language model to use tools via self-instruction",
            "venue": "ArXiv, abs/2305.18752. 3",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "3",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent research in Language Models has focused on augmenting them with external tools (Schick et al., 2023; Paranjape et al., 2023), learning from feedback (Ouyang et al., 2022; Akyurek et al., 2023) and iterative-refinement (Madaan et al., 2023; Paul et al., 2023; Shinn et al., 2023). Iterative-Refinement has been a necessary tool in human evolution and problem-solving. Moreover, humans seek feedback from domain-specific knowledge sources. For instance, an architect tasked with creating an environmentally friendly and structurally sound building design will require targeted feedback from civil engineers for structural integrity, and sustainability experts for eco-friendly\n1Our source code can be found here\ndesign principles. However, previous works on iterative refinement overlook this requirement and collect generic feedback from multiple sources or collect a single one from the Language Model itself.\nIn this work, we first investigate whether the use of generic feedback is a bottleneck in addressing the diverse range of errors present in the reasoning chains generated by LMs. We posit that utilizing generic feedback for a broad spectrum of errors may result in vague or non-actionable feedback for specific errors. This is due to two main factors: (i) the inclusion of multiple error categories within a single prompt, which not only increases the size of the prompt but also poses a challenge to current models that have a limited context length and struggle with long texts, and (ii) the model is burdened with identifying a multitude of error types in a single instance, which degrades the quality of the generated feedback.\nTo surmount these challenges, we introduce Multi-Aspect Feedback (MAF), a novel generalpurpose iterative refinement framework that employs a collection of specialized feedback modules, including pre-trained LMs and external tools, each tailored to address a specific error category. Our framework includes a base model to generate an initial solution, multiple feedback modules, and a refiner model to revise the solution using feedback. Contrary to previous works, our feedback modules can use one of two refinement strategies: Eager Refinement and Lazy Refinement \u00a73.4. When using the Eager-Refine mode, the solution is revised immediately before moving on to the next feedback, however, for Lazy Refinement, we first collect feedback from multiple feedback sources and then refine the solution using this collective feedback.\nIn devising MAF, we first classified errors in LMgenerated reasoning chains based on Golovneva et al. (2022) and also identified some new error\ncategories. Subsequently, we decoupled our feedback modules such that each module focuses on a single error category. This strategy not only elevates performance but also allows one to leverage specialized tools tailored for distinct error types, such as utilizing a code interpreter for syntax errors in generated Python programs instead of relying on generic LM feedback. Through a modular design, we ensure that each module of our framework is replaceable depending on the task. We conduct extensive experimentation on a variety of complex reasoning tasks such as Mathematical Reasoning (Shi et al., 2023), Logical Reasoning (Dalvi et al., 2021), and Multi-Hop Question Answering (Dua et al., 2019), and establish that MAF effectively addresses the challenges previously discussed, and significantly enhances the performance of Language Models in complex reasoning tasks. We compare our method against the Base LMs and Self-Refine (Madaan et al., 2023), which also implements iterative refinement. During our experiments, we find that Over-Refining \u00a74 the solution can lead to worse performance, and thus we treat the number of iterations as a hyperparameter. We also propose an Oracle Verifier setting \u00a74, where we assume we have access to an \"answer verifier\" and stop the refining process once we have reached the correct answer. To avoid unfair comparisons, we test Self-Refine under this setting and find that MAF provides actionable and specific feedback. To summarize, the main contributions of this work are as follows: 1. We propose a novel framework that decouples the feedback generation process for different error categories. This allows us to use error-specific tools and LMs. Moreover, the proposed framework is modular and all the modules are Plug-and-Play. This allows for better flexibility on new tasks and the usage of advanced models as they become available. 2. Additionally, we propose two different types of refinement strategies, Eager Refinement and Lazy Refinement. Eager Refinement allows immediate revision of a solution by a feedback module to avoid conflicts, while Lazy Refinement improves efficiency by performing revisions from multiple feedback modules together. 3. We show that our framework outperforms the base Language Models and similar Iterative Refinement baselines on several complex reasoning tasks such as Math reasoning (GSMIC), Logical Reasoning (Entailment Bank), and Multi-hop Question Answering (DROP)."
        },
        {
            "heading": "2 Related Work",
            "text": "Chain-of-Thought Reasoning There has been a plethora of research on prompting Large Language Models to improve their reasoning capabilities. Wei et al. (2023) found that prompting the models to generate a reasoning chain in few-shot setting before solving a problem improves the performance by a huge margin. Further, Kojima et al. (2022) found that in zero-shot setting prefixing the solution generation with Let\u2019s think step-by-step has the same effect as generating intermediate reasoning chain and improves the performance. Following this work, Wang et al. (2022) proposed sampling multiple outputs from the model and selecting the final answer by majority voting. Zhou et al. (2023) also showed that decomposing the original question into smaller sub-problems can give a performance boost. Madaan et al. (2022) and Chen et al. (2022) showed that models trained on code can be used to generate a program-of-thought similar to a chain-of-thought, which enables the model to use a language interpreter for translating mathematical calculations into code. In this work, we use LMs\u2019 in-context learning abilities to implement the LM-based feedback and refiner modules.\nTool Augmented LLMs However, even with these advanced prompting techniques, the LMs still fail for problems that require external knowledge and are still prone to problems such as hallucination and incorrect reasoning. To circumvent these issues, several recent works have introduced the use of external tools such as calculator, code interpreter, knowledge retrieval (Karpas et al., 2022; Chen et al., 2022; Schick et al., 2023). Yang et al. (2023); Patil et al. (2023); Shen et al. (2023) show that one can teach LLMs to use tools by generating API calls. Li et al. (2023) also released a large dataset to enable research in the field of augmented Language Models. Hao et al. (2023) propose to represent each tool as a token and learn tool-specific embeddings to increase the robustness of these language models for using external APIs. However, reasoning is an iterative task and it is difficult to find a plausible answer for several problems in oneshot even when these LLMs are augmented with tools. This has inspired some recent works to use iterative refinement frameworks. In our work, we use external tools for generating feedback for certain error categories such as Programming Syntax Errors, Calculator for mathematical equations etc.\nTask Feedback Type Type ER\nProgramming Syntax Interpreter ! Variable Naming OpenAI !\nMath Redundancy OpenAI # Reasoning Commonsense OpenAI #\nMissing Step OpenAI #\nLogical Redundancy OpenAI # Reasoning Repetition OpenAI #\nHallucination OpenAI #\nRedundancy OpenAI # Question Factuality OpenAI # Answering Commonsense OpenAI #\nMissing Step OpenAI #\ncient to address the diverse range of potential errors in language model responses. To overcome this limitation, we propose a suite of feedback modules, each specifically targeting a particular error category and providing detailed feedback to improve both reasoning and solution quality."
        },
        {
            "heading": "3 MAF: Multi-Aspect Feedback",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "In this work, we present an iterative refinement framework with an explicit focus on decoupling feedback generation for different error categories. This approach allows us to systematically address different error types in the generated solutions. Our proposed framework has three crucial components: a base language model M that generates an initial solution O. A collection of n feedback modules {f0, f1, f2...fn}, each focusing on a single error category, collectively these modules generate a multi-aspect feedback F. And a Refiner R that generates refined solution O\u2019 based on initial solution O and feedback F.\nWhile there are other works that also use iterative refinement (Madaan et al., 2023; Akyurek et al., 2023; Peng et al., 2023), to the best of our knowledge, we are the first to explore the effect of decoupling different types of feedbacks. Taking inspiration from ROSCOE (Golovneva et al., 2022), we categorize feedback into ten distinct categories: arithmetic, programming syntax, variable naming, missing step, coherency, redundancy, repetition, hallucination, commonsense, and factuality. Definitions of these error categories are provided in Appendix A. Moreover, a feedback module can be a tool such as a code interpreter for syntax feedback, a calculator for arithmetic feedback, a knowledge graph for factuality or commonsense feedback, a Language Model, or even a fine-tuned model. It is important to note that the feedback generation process is not limited to these categories and can be extended to include other categories as well. The overall process is illustrated in Figure 1 and Algorithm 1."
        },
        {
            "heading": "3.2 Initial Generation",
            "text": "We use a large model such as GPT3.52, GPT4 (OpenAI, 2023), to generate an initial solution. However, generating just one solution isn\u2019t ideal for the reasoning process since reasoning is an iterative\n2https://openai.com/blog/chatgpt\nprocess. The solution often needs to be refined over time with every iteration bringing us closer to the correct answer. We follow the same principle in this work, where we initially produce a solution and then proceed to refine it based on actionable feedback."
        },
        {
            "heading": "3.3 Feedback Modules",
            "text": "Feedback generation is an involved task, providing a comprehensive list of common issues encountered in model outputs. The feedback generation is accomplished through a variety of tools known as feedback modules, which may include external tools, frozen LLMs, fine-tuned models, and scorers. These modules are used to provide actionable feedback based on the initial solution.\nEach type of feedback module is suited to address specific types of errors. For instance, an external tool like a code interpreter would be ideal for providing feedback on syntax errors in code, while a fine-tuned model could provide more nuanced feedback on issues such as redundancy or hallucination. This decoupled approach allows us to address errors in a more targeted manner, improving the overall quality of the refined solution. Table 1 shows all the feedback modules used in our work.\nFurthermore, it\u2019s worth noting that merely specifying the error categories in a single prompt doesn\u2019t yield satisfactory results. We see two main reasons for this, firstly because the model is tasked with focusing on multiple errors simultaneously and secondly, as the number of error categories increase, the context length increases as well which results in high-quality feedback for the first few error categories and a steep decline in quality for the rest. While some of the previous works (Wu et al., 2023; Paul et al., 2023) have explored using fine-grained feedback and achieved promising results.\nMoreover, our feedback modules can choose between two refinement strategies: Eager-Refine or Lazy-Refine. We discuss this distinction in more detail in the next section."
        },
        {
            "heading": "3.4 Refinement",
            "text": "In this work, we reuse the initial model M as a Refiner. During the refining phase, the refiner is given a solution and multi-aspect feedback and asked to revise the solution according to feedback. We found that these large models are proficient at the refining task, demonstrating a marked improvement in the quality of the final solution compared\nto the initial output. As mentioned in the previous section, we use\ntwo refinement strategies: Eager-Refinement and Lazy-Refinement. The eager-Refinement approach is used for feedback types that can cause conflicts during refinement. An example of such as feedback module would be Variable Naming (VN), which is used to correct variable names in the generated code. This module can cause conflicts with others because when the other modules are referencing a variable that is supposed to be changed according to the VN feedback and can render the program inexecutable if refined incorrectly. Whereas, in the lazy refinement strategy feedback from the multiple modules is concatenated together along with the appropriate error categories to make a single multiaspect feedback. This collective feedback is then passed to the Refiner model in order to get a revised solution. Another advantage of this approach is the increased efficiency by refining once for multiple errors and the added flexibility.\nHowever, we also found that the smaller opensource models like LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023) and Vicuna3 often fail to adhere to the feedback provided thus making the refinement process ineffective. These smaller models also have smaller context lengths which\n3https://lmsys.org/blog/2023-03-30-vicuna/\nmake it difficult to include all the feedback in the prompt. To address this issue, we use a Selective Summarization approach. We only select parts of feedback that point to a problem. Figure 2 shows the \"summarized\" feedback generated by our approach. This simple selective summarization approach makes the feedback succinct and also proves to be less distractive to the models during the refining phase. This approach allows us to effectively combine all feedback together and use models with smaller context lengths to some extent."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Metrics",
            "text": "Mathematical Reasoning The GSM8K dataset, presented by Cobbe et al. (2021), is a comprehensive compilation of high-quality grade school math problems. GSM8K has proven to be a great resource for testing LLMs on mathematical word problems, however, the performance on this dataset has been saturated for a while.\nTo avoid that problem, we conduct experiments on a harder variant of this dataset, GSM-IC (GradeSchool Math with Irrelevant Context), that was introduced by Shi et al. (2023). We run our main experiments on a randomly sampled subset of 500 problems from GSM-IC and use % solve rate as\nAlgorithm 1 MAF algorithm Require: Input x, model M, Refiner R, number of iterations T Require: n Eager-refine {s1, s2, ...sn} and m Lazy-Refine {p1, p2, ...pm} Feedback Modules\n1: Initialize output y0 from M 2: while i < T do 3: for j \u2190 1 to n do \u25b7 Eager-refine Feedbacks 4: fsj \u2190 generate_feedback(sj , yi) \u25b7 Generate Feedback 5: if fsj indicates revision is required then 6: yi \u2190 revise(R, fsj , yi) \u25b7 Revise solution 7: F\u2190 \u201d\u201d \u25b7 Initialize empty feedback 8: for k \u2190 1 to m do \u25b7 Lazy-refine Feedbacks 9: fpk \u2190 generate_feedback(pk, yi)\n10: if fpj indicates revision is required then 11: F\u2190 F + fpk 12: if F not empty then 13: yi+1 \u2190 revise(R, fpk, yi) 14: else 15: yi+1 = yi\nthe metric.\nLogical Reasoning EntailmentBank, as described by Dalvi et al. (2021), is a dataset that contains multistep entailment trees. We use the validation set of Task 1 provided by the authors for our experimentation. For the metrics, we do not use the automated metrics provided by the original work because these metrics expect the trees to be similar in structure to gold trees. However, that is not a fair comparison because we find that there exist multiple correct entailment trees for a given hypothesis and information. Thus, we conduct a human evaluation and ask humans to evaluate if the hypothesis can be entailed from the predicted tree.\nQuestion Answering DROP (Dua et al., 2019) is a question-answering dataset. The Discrete Reasoning Over the Content of Paragraphs (DROP) dataset is designed for complex question-answering tasks that require multi-step reasoning over text passages. It presents a valuable benchmark for our iterative refinement framework, as the multi-step nature of the questions offers ample opportunities for generating feedback and refining to guide the model toward a correct solution. We use output parsing for answers and use % correct answers as the final metric."
        },
        {
            "heading": "4.2 Baselines",
            "text": "In this work, we focus on comparing our method with the Base LMs using the Ope-\nnAI API and a recently proposed iterative refinement framework, Self-Refine (Madaan et al., 2023). To provide a fair comparison and avoid randomness in the generated answer, we used Greedy Decoding for all our experiments.\nWe follow (Madaan et al., 2023) and use 8-shot Program of Thought (Chen et al., 2022) for GSMIC since the Python program written by the model acts as a \"calculator\" for mathematical equations.\nThe input for Entailment Bank includes a hypothesis and the supporting text, and we are limited by the context length of the current models. Hence, we use 4-shot prompting for this dataset. We use standard few-shot prompting (Brown et al., 2020), because the Entailment Tree itself acts like a reasoning chain.\nSimilarly, the examples in the DROP dataset have a passage and an accompanying question, so we use a 3-shot Chain of Thought Wei et al. (2023) prompting. We also provide an instruction specifying that the model should select either a number, date, or span from the passage to answer the question as shown in Appendix C.\nFor Self-Refine, we use the prompts provided by the authors for GSM8K in their work for GSM-IC and write our own prompts for DROP and Entailment Bank. Self-Refine has three modules, initial generation, feedback, and refiner. We use the same parameters and prompting strategy as the corresponding baseline for the initial generation. The feedback module and refiner module are imple-\n+SR 65.8 \u21915.4 76.0\u21914.0 74.6 \u21912.8 45.5\u219325.2\n0 1 2 3 4 Iterations\n72\n74\n76\n78\n80\n82\n84\nAc cu\nra cy\nBase LM Self-Refine Self-Refine (Oracle) MAF MAF (Oracle)\nmented using the standard few-shot prompting with 3 in-context examples for all datasets. Further details about the implementation of these baselines can be found in Appendix C."
        },
        {
            "heading": "4.3 Implementation",
            "text": "We implement MAF (Multi-Aspect Feedback) following the basic structure defined in \u00a73 and Algorithm 1. The iterative refinement process continues until we reach the desired output quality or a taskspecific stop criterion, up to a maximum of 4 iterations. Our method includes an initial generation, refiner, and feedback modules as shown in Table 1. The initial generation module uses the same parameters and prompting strategies as the corresponding baseline. Our Eager-Refine module uses 3-shot standard prompting and our Lazy-Refiner, which includes feedback from multiple modules, uses a 2-shot standard prompting approach. Our OpenAIbased feedback modules also use 3-shot standard prompting following (Madaan et al., 2023) with an error-specific instruction.\nTo provide a fair comparison, we use the same prompts for Baselines and Self-Refine wherever possible. We use Greedy Decoding for all our OpenAI-based4 modules to avoid any randomness. As mentioned in \u00a73.3, we use a selective summarization approach to fit all the lazy-refine feedbacks in our refiner module. We use a basic rule-based\n4Our experiments with GPT3.5 (Text-Davinci-003) and ChatGPT (gpt-3.5-turbo) are conducted using OpenAI API between April-2023 and August-2023."
        },
        {
            "heading": "4.4 Results",
            "text": "even though ChatGPT is generally considered to be a better model than GPT3.5.\nOver-Refining Due to the behavior described above, we face the problem of Over-Refining. This means that once we have reached an optimal solution for a reasoning problem, forcing the LM to refine it further deteriorates the performance and the quality of the reasoning chain. As shown in Table 2 and Figure 3, Self-Refine (Madaan et al., 2023) also suffers from this problem in the DROP dataset. Moreover, all the other iterative refinement framework such as Self-Refine uses a stop condition and stop the refining process if the generated feedback does not point out any problem. We however cannot take advantage of this stop condition because of the interplay between multiple feedback modules. For example, let\u2019s say our Missing Step module stops at iteration k due to no missing steps at that point in time since the other feedback modules continue to refine the solution and self-refinement is an imperfect process, it might introduce a Missing Step error in further iterations.\nHence, we treat the number of iterations as a hyperparameter and find that 2 iterations work best for our method.\nOracle Verifier Since the main focus of this work is to improve the incorrect initial generations from the model, we also evaluate the models under an Oracle Verifier setting. In this setting, we assume access to an \"Oracle Verifier\" which can judge the final answer generated by the model. If the final answer is correct, we stop the refinement process for that test sample, otherwise, we let the model continue refining the solution. It is important to note however that the model is not privy to this verification, and hence can still stop further refinement by not generating actionable feedback in the next iteration. Under this setting, we report results for both Self-Refine and MAF after 4 iterations. Even under this modified setting, we see that our method can outperform Self-Refine because we generate diverse feedback and thus can improve more solutions in the GSM-IC dataset."
        },
        {
            "heading": "4.5 Ablation",
            "text": "In this section, we perform two ablation studies. Firstly, we analyze the impact of each feedback module, identifying those with the most significant effect on performance. Secondly, we compare the\noutcomes of our two proposed refinement strategies: Lazy vs Eager Refinement. The ablation studies are conducted on the GSM-IC dataset. Due to resource constraints, we conduct the following ablation studies on a smaller random subset of size 100."
        },
        {
            "heading": "Contribution of different Feedback Modules",
            "text": "We test the contribution of each feedback module by removing that module and calculating the accuracy under standard and Oracle verifier settings. While the standard setting highlights the potential negative impacts of some of the modules, the oracle verifier setting highlights the absolute contribution of each module. This highlights an important finding that using the error categories is paramount to gaining performance. Results for this ablation study can be found in Table 3.\nWe also calculate the accuracy of our method when using the three best-performing modules as the only source of feedback. We found that this does recover the performance of our method. Even though Variable Naming and Missing Step modules do not affect MAFs performance by a huge margin, it still makes our method more robust to a possible distribution shift. Moreover, the Variable Naming module\u2019s main contribution is not increasing the performance, but rather to increase the readability of the code and not confuse users with unclear names.\nNote that we did not remove the Programming Syntax checker module as we use Program of Thoughts which requires a Python interpreter.\nLazy vs Eager Refinement To illustrate the complementary nature of our two proposed refinement strategies, we evaluate the performance of our iterative refinement framework using all feedback modules in either Lazy or Eager mode. The results are presented in Table 4, which showcases the performance of our framework under different refinement settings. In this table, \u2019MAF\u2019 corresponds to the results obtained by combining both Lazy and Eager Refinement strategies, as defined in Table 1, while \u2019Only Lazy/Eager-Refine\u2019 displays the results of our framework when utilizing only one type of refinement strategy.\nThe results clearly demonstrate that our framework, which leverages a combination of eager and lazy refinement, consistently matches or outperforms using either strategy in isolation, across both the standard and oracle verifier settings.\nPractical considerations also favor the use of a hybrid approach incorporating both eager and lazy feedback. Relying solely on lazy feedback can lead to a situation where multiple feedback categories are condensed into a single prompt. Despite our feedback summarization technique, this can result in the iteration prompt exceeding the context window limit of many widely available models. Conversely, exclusively employing eager feedback may result in rewriting the solution for each feedback module, leading to high token usage. While using all modules in Eager-Refine mode can closely approach the performance of \u2019MAF\u2019 (as shown in Table 4), it is not scalable when dealing with a large\nnumber of feedback modules."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we present Multi-Aspect Feedback (MAF), a novel iterative refinement framework that decouples the feedback modules and takes advantage of error-specific tools to generate feedback. We demonstrate the performance of our framework on a set of diverse reasoning tasks and compare it with other iterative refinement baselines.\nContrary to previous works, we found that OverRefinement can be a problem in iterative refinement frameworks since models are not certain if their own answer is correct. Our work also draws focus on the necessity to devise better feedback methods and call for augmenting Language Models with them. We hope this work will inspire further research in this area and to this end, we make all our code, data, and prompts available."
        },
        {
            "heading": "Limitations",
            "text": "The main limitation of our approach is that the base models need to have sufficient in-context learning abilities to process the feedback and refine the solution. Even with in-context learning abilities, these models are not perfect and thus can still make mistakes while refining the solution even when correct feedback is given.\nAll the experiments conducted in this work use large powerful models provided by OpenAI. We find that open-source LMs such as Vicuna, and Alpaca can generate decent initial solutions but are not capable of refining their own solutions. Thus, we leave the investigation of improving opensource models to future work.\nAnother limitation inherent in our approach is the reliance on a fixed set of Feedback Modules. Our method necessitates the pre-selection of feedback modules before execution, which in turn demands human intervention to determine the appropriate feedback categories for each new dataset or domain. Future research could explore novel methods that can dynamically and autonomously determine the most suitable feedback modules for specific problems in real-time."
        },
        {
            "heading": "Ethics Statement",
            "text": "The experiments in this work were performed with models that are not open-sourced and are everchanging. Moreover, these models are expensive to use, and thus research using these models requires\nan enormous amount of funding. The existing literature lacks details about the datasets that are being used to train these huge models or the filtering mechanism that is being used to clean the polluted corpora.\nFurthermore, there is always a possibility for bad actors to use our method to generate more toxic or harmful text. Our approach does not guard against this."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Science Foundation award #2048122. The views expressed are those of the authors and do not reflect the official policy or position of the US government. Additionally, we thank our reviewers for their detailed and useful feedback."
        },
        {
            "heading": "A Error Categories",
            "text": "In this section we define all the error categories that are implemented in this work. Table 5 shows the error categories introduced by Golovneva et al. (2022)."
        },
        {
            "heading": "A.1 Programming Syntax Feedback",
            "text": "Programming syntax feedback module is implemented as Python Interpreter. This module aims to fix any syntax errors in the generated code. This particular module benefits from a EagerRefinement approach.\nA.2 Variable Naming Feedback\nGood variable names in a code can improve the readability of the code and potentially can improve model\u2019s own understanding of the program in further iterations. Variable Naming Feedback is another module which benefits from an eager-refine approach."
        },
        {
            "heading": "A.3 Redundancy Feedback",
            "text": "Redundant information is any information included in the reasoning that doesn\u2019t help answer the question. This additional information may distract the model from correctly answering and should thus be removed."
        },
        {
            "heading": "A.4 Commonsense Feedback",
            "text": "Commonsense reasoning errors are errors about any relation or knowledge that is should be known from general world such as \"all ducks are birds\"."
        },
        {
            "heading": "A.5 Missing Steps",
            "text": "Missing steps errors are any gaps in reasoning or missing information that prevent the reasoning chain from being correct. This also identifies the model saying that the question is unanswerable as a missing step error because that means additional reasoning steps are needed to answer the question from the passage."
        },
        {
            "heading": "A.6 Factuality Feedback",
            "text": "Factuality errors are cases where the answer reasoning states infactual information. This could be information that contradicts information given in the passage or hallucinated."
        },
        {
            "heading": "A.7 Hallucination Feedback",
            "text": "LLMs are prone to hallucination, however, it has been shown that sampling from a LLM multiple\ntimes and then selecting the majority answer can improve the results. Thus Hallucination feedback aims to fix any hallucinated facts in the initial generation. This module can be improved by using an external tool such as a Knowledge Source instead of a LLM.\nB Implementation Parameters\nTable 6 provides parameters used for Self-Refine and Table 7 provides parameters used for MAF."
        },
        {
            "heading": "C Few-Shot Prompt Examples",
            "text": "We add samples for all the prompts used in our work. Complete prompts can be found in our source code. Note that for all feedback prompt examples, there is at least one example with no errors, in which case the feedback will state that there are no errors and the reasoning is correct. This helps decrease the likelihood that the model identifies an error in a solution that is actually correct."
        },
        {
            "heading": "Dataset Base LM Feedback Refiner",
            "text": ""
        },
        {
            "heading": "Error Type Definition",
            "text": ""
        },
        {
            "heading": "Okay, here is the rewrite:",
            "text": ""
        },
        {
            "heading": "Okay! Here is the rewrite:",
            "text": ""
        },
        {
            "heading": "79,667 households minus 60,387 families equals 19,280. Thus, there are 19,280 more housing units than families. final_answer: 19,280",
            "text": ""
        },
        {
            "heading": "1.52% Race (United States Census), 0.06% Race (United States Census), 0.69% from Race (United States Census), and 1.47% from two or more races. 1.91% of the population were Race (United States Census) or Race (United States Census) of any race. 22.5% were of German people, 13.1% Irish people, 9.8% Italian people, 9.2%",
            "text": ""
        },
        {
            "heading": "86.77% Race (United States Census), 9.27% Race (United States Census), 0.23% Race (United States Census),",
            "text": ""
        }
    ],
    "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
    "year": 2023
}