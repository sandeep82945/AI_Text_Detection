{
    "abstractText": "Recently, aspect-based sentiment analysis (ABSA) models have yielded promising results. However, they are susceptible to learning spurious correlations between certain words of the input text and output labels while modeling the sentiment feature of the aspect. This spurious correlation will potentially undermine the performance of ABSA models. One direct solution for this problem is to make the model see and learn an explanation of sentiment expression rather than certain words. Motivated by this, we exploit explanations for the sentiment polarity of each aspect from large language models (LLMs) to reduce spurious correlations in ABSA. First, we formulate a prompt template that wraps the sentence, an aspect, and the sentiment label. This template is utilized to prompt LLMs to generate an appropriate explanation that states the sentiment cause. Then, we propose two straightforward yet effective methods to leverage the explanation for preventing the learning of spurious correlations. We conducted extensive comparative experiments on five datasets by integrating them with some representative ABSA models. Results show that our methods can achieve performance gains and enhance the performance and generalization ability of ABSA models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qianlong Wang"
        },
        {
            "affiliations": [],
            "name": "Keyang Ding"
        },
        {
            "affiliations": [],
            "name": "Bin Liang"
        },
        {
            "affiliations": [],
            "name": "Min Yang"
        },
        {
            "affiliations": [],
            "name": "Ruifeng Xu"
        }
    ],
    "id": "SP:2efaa53d703eeec3ad10ebb711f1cc6c45e432d9",
    "references": [
        {
            "authors": [
                "Ning Bian",
                "Xianpei Han",
                "Le Sun",
                "Hongyu Lin",
                "Yaojie Lu",
                "Ben He."
            ],
            "title": "Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models",
            "venue": "arXiv preprint arXiv:2303.16421.",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Feifan Fan",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Multi-grained attention network for aspect-level sentiment classification",
            "venue": "EMNLP, pages 3433\u20133442.",
            "year": 2018
        },
        {
            "authors": [
                "Shuqin Gu",
                "Lipeng Zhang",
                "Yuexian Hou",
                "Yin Song."
            ],
            "title": "A position-aware bidirectional attention network for aspect-level sentiment analysis",
            "venue": "COLING, pages 774\u2013784.",
            "year": 2018
        },
        {
            "authors": [
                "Ruidan He",
                "Wee Sun Lee",
                "Hwee Tou Ng",
                "Daniel Dahlmeier."
            ],
            "title": "Effective attention modeling for aspect-level sentiment classification",
            "venue": "COLING, pages 1121\u20131131.",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Qingnan Jiang",
                "Lei Chen",
                "Ruifeng Xu",
                "Xiang Ao",
                "Min Yang."
            ],
            "title": "A challenge dataset and effective models for aspect-based sentiment analysis",
            "venue": "EMNLP, pages 6280\u20136285.",
            "year": 2019
        },
        {
            "authors": [
                "Lishuang Li",
                "Yang Liu",
                "AnQiao Zhou."
            ],
            "title": "Hierarchical attention-based position-aware network for aspect-level sentiment analysis",
            "venue": "COLING, pages 181\u2013189.",
            "year": 2018
        },
        {
            "authors": [
                "Ruifan Li",
                "Hao Chen",
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy."
            ],
            "title": "Dual graph convolutional networks for aspect-based sentiment analysis",
            "venue": "ACL, pages 6319\u20136329.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lorraine Li",
                "Adhiguna Kuncoro",
                "Jordan Hoffmann",
                "Cyprien de Masson d\u2019Autume",
                "Phil Blunsom",
                "Aida Nematzadeh"
            ],
            "title": "A systematic investigation of commonsense knowledge in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Xin Li",
                "Lidong Bing",
                "Wai Lam",
                "Bei Shi."
            ],
            "title": "Transformation networks for target-oriented sentiment classification",
            "venue": "ACL, pages 946\u2013956.",
            "year": 2018
        },
        {
            "authors": [
                "Xin Li",
                "Lidong Bing",
                "Piji Li",
                "Wai Lam",
                "Zhimou Yang."
            ],
            "title": "Aspect term extraction with history attention and selective transformation",
            "venue": "IJCAI, pages 4194\u20134200.",
            "year": 2018
        },
        {
            "authors": [
                "Xinlong Li",
                "Xingyu Fu",
                "Guangluan Xu",
                "Yang Yang",
                "Jiuniu Wang",
                "Li Jin",
                "Qing Liu",
                "Tianyuan Xiang."
            ],
            "title": "Enhancing bert representation with contextaware embedding for aspect-based sentiment analysis",
            "venue": "IEEE Access, 8:46868\u201346876.",
            "year": 2020
        },
        {
            "authors": [
                "Bin Liang",
                "Hang Su",
                "Lin Gui",
                "Erik Cambria",
                "Ruifeng Xu."
            ],
            "title": "Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks",
            "venue": "Knowledge-Based Systems, 235:107643.",
            "year": 2022
        },
        {
            "authors": [
                "Nelson F Liu",
                "Matt Gardner",
                "Yonatan Belinkov",
                "Matthew E Peters",
                "Noah A Smith."
            ],
            "title": "Linguistic knowledge and transferability of contextual representations",
            "venue": "NAACL, pages 1073\u20131094.",
            "year": 2019
        },
        {
            "authors": [
                "Dehong Ma",
                "Sujian Li",
                "Xiaodong Zhang",
                "Houfeng Wang."
            ],
            "title": "Interactive attention networks for aspect-level sentiment classification",
            "venue": "IJCAI, pages 4068\u20134074.",
            "year": 2017
        },
        {
            "authors": [
                "Yukun Ma",
                "Haiyun Peng",
                "Tahir Khan",
                "Erik Cambria",
                "Amir Hussain."
            ],
            "title": "Sentic lstm: A hybrid network for targeted aspect-based sentiment analysis",
            "venue": "Cognitive Computation, 10:639\u2013650.",
            "year": 2018
        },
        {
            "authors": [
                "Ambreen Nazir",
                "Yuan Rao",
                "Lianwei Wu",
                "Ling Sun."
            ],
            "title": "Iaf-lg: An interactive attention fusion network with local and global perspective for aspect-based sentiment analysis",
            "venue": "IEEE Transactions on Affective Computing, 13(4):1730\u20131742.",
            "year": 2022
        },
        {
            "authors": [
                "Sapna Negi",
                "Paul Buitelaar."
            ],
            "title": "Insight galway: Syntactic and lexical features for aspect based sentiment analysis",
            "venue": "SemEval, pages 346\u2013350.",
            "year": 2014
        },
        {
            "authors": [
                "Minh Hieu Phan",
                "Philip O Ogunbona."
            ],
            "title": "Modelling context and syntactical features for aspectbased sentiment analysis",
            "venue": "ACL, pages 3211\u20133220.",
            "year": 2020
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitrios Galanis",
                "Harris Papageorgiou",
                "Ion Androutsopoulos"
            ],
            "title": "Semeval-2016 task 5: Aspect based sentiment analysis",
            "venue": "In SemEval,",
            "year": 2016
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitrios Galanis",
                "Harris Papageorgiou",
                "Suresh Manandhar",
                "Ion Androutsopoulos."
            ],
            "title": "Semeval-2015 task 12: Aspect based sentiment analysis",
            "venue": "SemEval, pages 486\u2013495.",
            "year": 2015
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitrios Galanis",
                "John Pavlopoulos",
                "Harris Papageorgiou",
                "Ion Androutsopoulos",
                "Suresh Manandhar."
            ],
            "title": "Semeval-2014 task 4: Aspect based sentiment analysis",
            "venue": "SemEval, pages 27\u201335.",
            "year": 2014
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Parsa Ghaffari",
                "John G Breslin."
            ],
            "title": "A hierarchical model of reviews for aspectbased sentiment analysis",
            "venue": "EMNLP, pages 999\u2013 1005.",
            "year": 2016
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Aditi Raghunathan",
                "Pang Wei Koh",
                "Percy Liang."
            ],
            "title": "An investigation of why overparameterization exacerbates spurious correlations",
            "venue": "ICML, pages 8346\u20138356. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Improving neural machine translation models with monolingual data",
            "venue": "ACL, pages 86\u201396.",
            "year": 2016
        },
        {
            "authors": [
                "Youwei Song",
                "Jiahai Wang",
                "Tao Jiang",
                "Zhiyue Liu",
                "Yanghui Rao."
            ],
            "title": "Attentional encoder network for targeted sentiment classification",
            "venue": "arXiv preprint arXiv:1902.09314.",
            "year": 2019
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "The journal of machine learning research, 15(1):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Kai Sun",
                "Richong Zhang",
                "Samuel Mensah",
                "Yongyi Mao",
                "Xudong Liu."
            ],
            "title": "Aspect-level sentiment analysis via convolution over dependency tree",
            "venue": "EMNLP, pages 5679\u20135688.",
            "year": 2019
        },
        {
            "authors": [
                "Xingwei Tan",
                "Yi Cai",
                "Changxi Zhu."
            ],
            "title": "Recognizing conflict opinions in aspect-level sentiment classification with dual attention networks",
            "venue": "EMNLP, pages 3426\u20133431.",
            "year": 2019
        },
        {
            "authors": [
                "Duyu Tang",
                "Bing Qin",
                "Xiaocheng Feng",
                "Ting Liu."
            ],
            "title": "Effective lstms for target-dependent sentiment classification",
            "venue": "COLING, pages 3298\u20133307.",
            "year": 2016
        },
        {
            "authors": [
                "Duyu Tang",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Aspect level sentiment classification with deep memory network",
            "venue": "EMNLP, pages 214\u2013224.",
            "year": 2016
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola."
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "NeurIPS, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Yuanhe Tian",
                "Guimin Chen",
                "Yan Song."
            ],
            "title": "Aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble",
            "venue": "NAACL, pages 2910\u20132922.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Bing Wang",
                "Liang Ding",
                "Qihuang Zhong",
                "Ximing Li",
                "Dacheng Tao."
            ],
            "title": "A contrastive crosschannel data augmentation framework for aspectbased sentiment analysis",
            "venue": "COLING, pages 6691\u2013 6704.",
            "year": 2022
        },
        {
            "authors": [
                "Kai Wang",
                "Weizhou Shen",
                "Yunyi Yang",
                "Xiaojun Quan",
                "Rui Wang."
            ],
            "title": "Relational graph attention network for aspect-based sentiment analysis",
            "venue": "ACL, pages 3229\u20133238.",
            "year": 2020
        },
        {
            "authors": [
                "Tianlu Wang",
                "Rohit Sridhar",
                "Diyi Yang",
                "Xuezhi Wang."
            ],
            "title": "Identifying and mitigating spurious correlations for improving robustness in nlp models",
            "venue": "Findings of NAACL, pages 1719\u20131729.",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Wang",
                "Aron Culotta."
            ],
            "title": "Identifying spurious correlations for robust text classification",
            "venue": "Findings of EMNLP, pages 3431\u20133440.",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Kai Zou."
            ],
            "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
            "venue": "EMNLP, pages 6382\u20136388.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoyu Xing",
                "Zhijing Jin",
                "Di Jin",
                "Bingning Wang",
                "Qi Zhang",
                "Xuan-Jing Huang."
            ],
            "title": "Tasty burgers, soggy fries: Probing aspect robustness in aspectbased sentiment analysis",
            "venue": "EMNLP, pages 3594\u2013 3605.",
            "year": 2020
        },
        {
            "authors": [
                "Hu Xu",
                "Bing Liu",
                "Lei Shu",
                "S Yu Philip."
            ],
            "title": "Bert post-training for review reading comprehension and aspect-based sentiment analysis",
            "venue": "NAACL, pages 2324\u20132335.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Xue",
                "Tao Li."
            ],
            "title": "Aspect based sentiment analysis with gated convolutional networks",
            "venue": "ACL, pages 2514\u20132523.",
            "year": 2018
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "ACL, pages 2416\u2013 2429.",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Zhang",
                "Zili Zhou",
                "Yanna Wang."
            ],
            "title": "Ssegcn: Syntactic and semantic enhanced graph convolutional network for aspect-based sentiment analysis",
            "venue": "NAACL, pages 4916\u20134925.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Aspect-based sentiment analysis (ABSA) aims to identify the sentiment polarity (e.g., positive, neutral, and negative) of a specified aspect in a review (Pontiki et al., 2014). For example, given a review \"great food but the service was dreadful!\" and two aspects \"food\" and \"service\", this task needs to infer their sentiment polarities \"positive\" and \"negative\", respectively.\nTraditional ABSA methods primarily rely on machine learning techniques, which incorporate some handcrafted features to enhance performance,\n\u2217 Corresponding author.\nsuch as linguistic features (Negi and Buitelaar, 2014). However, feature engineering could be a time-consuming process, requiring significant effort and expertise. To solve this dilemma, deep learning solutions are utilized to address ABSA due to their powerful contextual feature modeling capability. From conventional neural networks (Ruder et al., 2016; Xue and Li, 2018; Ma et al., 2018) to attention mechanisms (Tang et al., 2016a; Li et al., 2018a; Gu et al., 2018; Fan et al., 2018), these solutions focus on modeling the dependency relationship between an aspect and its corresponding opinion expressions. With the emergence of finetuning paradigm, the attention mechanism armed with pre-trained language models (PLMs) (Devlin et al., 2019; Song et al., 2019; Wang et al., 2020; Tian et al., 2021; Nazir et al., 2022; Zhang et al., 2022) further strengthens the connection between the aspect and its context.\nDespite their satisfactory results, most neural network methods may indulge in learning statistically spurious correlations while modeling the sentiment feature of aspect on the context. Here, spurious correlation (Wang and Culotta, 2020; Wang et al., 2022b) refers to the dependence of the model on\ncertain words of the input text without a deeper understanding of the contextual semantics, which has a know-it-when-you-see-it character. Taking the example in Figure 1 for illustration, the opinion word \"fast\" expresses different sentiment polarities of the aspect terms in distinct contexts. Here, 92% of aspect sentiment is \"positive\" in the training samples when counting the proportion of aspect sentiment polarity that co-occurs with \"fast\". Due to this unbalanced distribution, in the training phase, the neural models assume that there is a strong correlation between \"fast\" and \"positive\", especially for short texts. Consequently, when faced with a testing sample containing a derivative \"faster\", the trained models will predict the incorrect sentiment label \"positive\" based on this spurious correlation learned superficially before. Thus, most neural models may encounter difficulty in navigating spurious correlations because of a shallow understanding. Besides, they lack the capacity to self-correct, resulting in undermining their effectiveness and generalization.\nOne straightforward solution to alleviate the spurious correlation problem is to make models attend to an explanation of sentiment expression rather than certain words in the context. Here, explanation refers to the reasons for the sentiment polarity of aspect term obtained by deeply understanding the contextual semantics. However, for each training sample, it is a tricky problem to derive the sentiment explanation given the aspect and its sentiment. Recently, large language models (LLMs) (Brown et al., 2020) have achieved remarkable success in a wide range of NLP capabilities, including generation and contextual understanding. In addition, they are knowledgeable due to the substantial linguistic (Liu et al., 2019) and factual world knowledge learned. Thus, LLMs can be exploited to generate an explanation toward the sentiment of aspect through prompt-driven contextual understanding (Bian et al., 2023). Taking the second training sample in Figure 1 as an example, LLMs can yield an explanation, \"The sentiment towards \u2019Final Cut Pro\u2019 is positive because the speaker praises its efficiency and user-friendliness on the laptop, indicating satisfaction and favorable feelings about the software.\".\nInspired by this, we leverage explanations from LLMs to reduce spurious correlations in ABSA. Specifically, we first design a prompt containing an aspect term and its sentiment to induce LLMs\nto provide a relevant explanation according to the context. In this way, the output explanation can provide the reason for sentiment and may contain some external knowledge thanks to the powerful capabilities of LLMs. Then, we propose two methods to employ this explanation to improve the effectiveness and generalization of ABSA models. One is the augmentation-based method, which directly treats these explanations containing the aspect term as training samples. We mix these explanations with the original training samples to train a more robust ABSA model. This method can not only relieve the statistical bias in original samples but also learn a range of sample patterns. The other is the distillation-based method, whose basic idea is to distill the knowledge embedded in the explanation into a student ABSA model. By the distillation loss, the student ABSA model can mimic the two behaviors of the teacher, i.e., sentiment representation and output logit. In this way, the explanation can guide the learning of ABSA models and prevent them from over-focusing on spurious correlations.\nIn summary, our contributions are as follows:\n\u2022 To our knowledge, we are the first to induce LLMs to generate an explanation for the aspect\u2019s sentiment and use it to reduce spurious correlations in the ABSA task.\n\u2022 We devise two straightforward methods for utilizing this explanation, which can be integrated into most mainstream baselines.\n\u2022 We conduct extensive experiments on five benchmark datasets, showing that baselines armed with the proposed methods can achieve better performance on inference and generalization."
        },
        {
            "heading": "2 Related Work",
            "text": "Aspect-based Sentiment Analysis. ABSA aims to identify the sentiment polarity of each aspect mentioned in the text. To solve this task, various neural networks with the attention mechanism are utilized to find the semantic relation of an aspect and its context for capturing the corresponding opinion expression (Tang et al., 2016b; Ma et al., 2017; Li et al., 2018c; Fan et al., 2018; Tan et al., 2019). For instance, Fan et al. (2018) exploited a multi-grained attention mechanism to capture the word-level interaction between the aspect and its relevant context. The idea behind the attention\nmechanism is to focus on the context related to the aspect and shield the irrelevant context. To further pursue this idea, some studies (Song et al., 2019; Li et al., 2020; Yan et al., 2021; Wang et al., 2022b,a) applied pre-trained models (PLMs) such as BERT (Devlin et al., 2019) to model the semantic relationship between the given aspect and its context. The internal multi-head self-attention mechanism in PLMs is more efficient than conventional attention techniques (Vaswani et al., 2017). As a result, these studies consistently delivered better results.\nAnother research trend is to leverage syntactic knowledge from syntactic trees to handle ABSA. This syntactic knowledge helps to establish connections between the aspect and opinion words and learn syntax-aware feature representations of the aspect (He et al., 2018; Sun et al., 2019; Phan and Ogunbona, 2020; Wang et al., 2020; Tian et al., 2021; Liang et al., 2022). The core idea of these studies is to transform a constructed syntax dependency tree into a graph for posing greater attention to important words.\nAlthough these methods obtained promising results by modeling semantic relationships between aspects and contexts, they are inevitably plagued by statistical spurious correlations. Unlike them, in this paper, we aim to reduce spurious correlations with explanations from LLMs. These explanations can serve to guide ABSA models not to\nfocus on certain words in the context to prevent being trapped in the spurious correlation trap.\nLarge Language Models. With the advent of GPT-3 (Brown et al., 2020), LLMs break into the limelight and draw enormous attention. They typically feature a vast array of model parameters and undergo training on immensely large volumes of raw data. By learning from data, they memorize and understand vast amounts of knowledge (Li et al., 2022) and learn to reason (Wei et al., 2022). Knowledge and reason are crucial for building a satisfactory NLP system that can understand and generate human-like language. Consequently, LLMs like ChatGPT can achieve substantial performance improvements in a wide range of NLP tasks, including inference and dialogue, by profoundly comprehending the contextual semantics. Inspired by this, for the sentiment polarity of each aspect, we here apply LLMs to generate an explanation to explain its causes."
        },
        {
            "heading": "3 Our Approach",
            "text": ""
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "Given an ABSA training set, each sample consists of a sentence X , an aspect a, and a sentiment label y. Here, the aspect is a sub-sequence token in the sentence. ABSA aims to learn a sentiment classifier that can precisely predict a sentiment polarity y \u2208\n{positive, negative, neural} for each aspect term according to the semantics of the sentence.1"
        },
        {
            "heading": "3.2 Overview",
            "text": "As shown in Figure 2, our framework consists of two steps. The first step is explanation generation. Here, for each training sample, we use a prompt template to encapsulate the sentence, the aspect, and its sentiment to drive the LLMs to generate an explanation to indicate the corresponding sentiment cause. The second step is explanation exploitation. Here, we propose two simple yet effective methods to exploit explanations for alleviating the spurious correlations in ABSA."
        },
        {
            "heading": "3.3 Explanation Generation",
            "text": "Spurious correlations are common in current ABSA models, particularly in cases of overparameterization or insufficient training data (Sagawa et al., 2020). The fundamental reason is that these models might learn statistical correlations between superficial textual cues and sentiment labels rather than achieving a profound comprehension of contextual semantics. Consequently, this problem will hurt the performance and generality of the ABSA classifier.\nIn this work, we try to reduce spurious correlations in ABSA using explanation. To achieve this, we expect an explanation to have two functions: (i) motivating ABSA models to infer the aspect\u2019s sentiment by understanding the context rather than some surface words. (ii) providing additional knowledge as background information for better contextual understanding, especially for short texts; Recently, LLMs such as ChatGPT have exhibited incredible contextual understanding and knowledge inference on a wide range of NLP (Wei et al., 2022). It inspires us to leverage LLMs to generate an explanation for the aspect\u2019s sentiment in each training sample, which has not been explored in the literature. To this end, we design a prompt template to trigger the understanding and inference ability of LLMs, which wraps the sentence X , an aspect a, and its sentiment y:\nIn the following sentence X, explain why the sentiment expressed by aspect term a is y. Limit to forty words. Copying\n1For sentences with multiple aspects, we treat other nontargeted aspects as normal context tokens when focusing on the target aspect. In other words, a sentence will be processed multiple times, which is equal to the number of aspects it contains.\nadjectives from the original sentence is not allowed.\nWe can see that this prompt consists of three components: task description, training sample, and output limitation. They describe the task precisely and form a good output guide, which helps to enhance the generation performance. As shown in the example in Figure 2, LLM is tasked with generating a friendly explanation X\u0302 for the aspect sentiment. This explanation not only explains why the sentiment occurs based on contextual semantics (i.e., \"user-friendliness\") but also includes some background knowledge (i.e., \"software\")."
        },
        {
            "heading": "3.4 Explanation Exploitation",
            "text": "The explanation generated by the LLM provides us with a comprehensive view of the original text from the perspective of contextual semantics and background knowledge. Furthermore, the explanation does not have high-frequency adjectives (e.g., fast) due to the limitation in the prompt, which further provides a sufficient condition to mitigate statistical spurious correlations. Thus, we can use them to aid the learning of ABSA models and improve the performance of the model. Here, we present two straightforward and model-agnostic methods to achieve this.\nAugmentation-based Method. In a sense, the explanation can be considered as a paraphrase of the original sentence, which has the same semantic meaning and label but a different description. This different description not only facilitates the alleviation of statistical bias in the original sentences but also diversifies the expression of the same sentiment. Thus, mixing the original training data {(Xi, ai, yi)}Ni=1 with their explanations {(X\u0302i, ai, yi)}Ni=1 can allow for training a more robust ABSA classifier:\nLcls = \u2212 1\n2N 2N\u2211 i=1 CE(y, P (X \u2032i, a)) (1)\nwhere P (X \u2032, a) is the predictive probability distribution of sentiment; X \u2032 can be either X or X\u0302; CE denotes the cross-entropy loss function.\nThe explanation is more effective than conventional data augmentation methods (Wei and Zou, 2019) as it interprets the contextual semantics and contains some knowledge.\nDistillation-based Method. Direct mixing cannot align the original sentence with the corresponding explanation, which results in trouble providing customized guided learning. To this end, we use a guidance strategy to encourage ABSA models to reduce spurious correlations in fitting each sample. This strategy can be viewed as a knowledge distillation (Hinton et al., 2015), which aims to leverage the teacher to guide the student\u2019s training with the help of explanations.2 To achieve this guidance, we here make the student model mimic two behaviors of the teacher one via the following two losses:\nLdis = 1\nN N\u2211 i=1 KL(gs(X \u2032 i, a), gt(X \u2032 i, a)) (2)\nLhid = 1\nN N\u2211 i=1 MSE(hsX\u2032i , htX\u2032i ) (3)\nwhere gs(X \u2032i, a) and gt(X \u2032 i, a) (h s X\u2032i and htX\u2032i ) refer to the logits (hidden states), which come from student and teacher networks, respectively; KL denotes the Kullback-Leibler divergence loss function; MSE denotes the mean squared error loss function. By two losses, the explanation is utilized to facilitate the learning process of ABSA models and mitigate overly concentrated on shortcut features.\nTo yield better guidance, the teacher network tracks an exponential moving average (Tarvainen and Valpola, 2017) of the student network weights. At the time step t, the parameters of the teacher \u03b8 are updated as follows:\n\u03b8t = \u03bb \u00b7 \u03b8t\u22121 + (1\u2212 \u03bb) \u00b7 \u03d5t (4)\nwhere \u03d5t represents all parameters in the student network at time step t; \u03bb is a smoothing coefficient. With this moving average, the teacher\u2019s output is more reliable."
        },
        {
            "heading": "3.5 Training and Testing",
            "text": "For the augmentation-based method, we train the parameters of the ABSA model directly by optimizing Lcls (Eq. 1). For the distillation-based method, we update parameters of the student model by optimizing the sum of Lcls (Eq. 1), Ldis (Eq. 2), and Lhid (Eq. 3). In the test phase, the sentence and aspect are fed into the student network to predict the label.\n2In this work, the teacher model and the student model have the same framework."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Settings",
            "text": "Datasets. We use five benchmark datasets to evaluate the proposed methods: Lap14 and Rest14 from Pontiki et al. (2014), Rest15 from Pontiki et al. (2015), Rest16 from Pontiki et al. (2016), and MAMS from Jiang et al. (2019). All datasets only involve three sentiment labels, positive, neutral, and negative. Each sample in these datasets is annotated with aspects and their corresponding sentiment polarities. Here, we adopt the official data splits as done in the original papers. The basic statistics are shown in Table 1.\nSettings. If not otherwise specified, we use ChatGPT and the pre-trained uncased BERT-base3 as LLM and encoder in the framework4, respectively. For the classifier, the weight matrix is randomly initialized by a uniform distribution. To avoid over-fitting, we apply the dropout (Srivastava et al., 2014) with a probability of 0.1. Besides, we also replace the label words (i.e., positive, neutral, and negative) in the explanation with [MASK] token. We employ the AdamW optimizer to optimize parameters. The epoch, batch size, learning rate, and smoothing coefficient are set to 8, 24, 3e-5, and 0.95, respectively. We limit the maximum length of the token sequence to 256.\nWe run the experiments five times with random initialization and report the averaged results. The accuracy (Acc.) and macro-averaged F1 (F1) scores are used as the evaluation metric."
        },
        {
            "heading": "4.2 Baselines",
            "text": "To evaluate the effectiveness and generalization of the proposed methods, we integrate them with\n3https://github.com/google-research/bert 4The proposed framework only loads the LLM for infer-\nence without involving training.\nsome representative ABSA models and compare performance. These ABSA models could be categorized into three groups. (1) the PLMs-based models, which includes BERT (Devlin et al., 2019) and BERT-PT (Xu et al., 2019). (2) the attentionbased models, which includes TNet (Li et al., 2018b) and AEN (Song et al., 2019). (3) the graphbased models, which includes RGAT (Wang et al., 2020) and DualGCN (Li et al., 2021).\nIn addition to the ABSA models mentioned above, we also introduce two LLMs (MOSS5 and ChatGPT6) as strong competitors."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Table 2 shows the experimental results of ABSA models on five datasets. We can draw the following conclusions from this table:\n5The snapshot version and parameters of the MOSS are MOSS-moon-003-sft and 16B, respectively. Please refer to https://moss.fastnlp.top/\n6The snapshot version and parameters of the ChatGPT are text-davinci-003 and 175B, respectively. Please refer to https://openai.com/blog/chatgpt\nFirst, ABSA models equipped with our methods (i.e., + augmentation and + distillation) achieve better performance than peer competitors on both accuracy and F1. Among them, the bigger improvements in accuracy and F1 are 2.88% (BERT+distillation on the Lap14) and 3.56% (AEN+augmentation on the Rest14), respectively. These improvements show that (i) the proposed explanation can effectively mitigate spurious correlations, and (ii) our methods can be seamlessly compensated to existing ABSA models.\nSecond, the graph-based ABSA models perform better than the attention-based ones. For example, DualGCN improves performance by 3.99% in accuracy and 3.67% in F1 over TNet on the Lap14. Although the graph-based models have obtained satisfactory results, we can observe a boost of 0.28\u223c2.86% in accuracy and 0.38\u223c3.03% in F1 when integrated with our methods. It indicates that while exploiting the syntactic knowledge connecting aspects and opinion words to improve performance, they may still model shortcut features\nbecause of a shallow understanding of some words. Third, LLMs can yield impressive results using few demonstrations. Compared with PLMs, they are scaling up in depth and width. It causes them to become increasingly computationally and storageintensive, making deployment difficult. This is why we leverage explanations from LLMs to reduce the spurious correlations in ABSA rather than using them directly to solve it.\nFourth, we find that the augmentation-based method and distillation-based one could not tell who wins and who loses. Each has its own advantages and merits. For example, although the distillation-based method yields higher results than the augmentation-based method in some cases, the latter is superior with respect to efficiency. In addition, the augmentation-based method is more applicable in different ABSA models. Therefore, we will subsequently focus more on the augmentationbased method for future research."
        },
        {
            "heading": "5 Discussion",
            "text": "Percentage of Spurious Correlations in the Dataset. In this work, spurious correlation (Wang et al., 2022b) refers to the dependence of the model on certain words in the input text without a deeper understanding of the contextual semantics. A question naturally arises how much of the correlation\nactually is in used datasets? To answer this question, we conduct a simple probe experiment on the Aspect Robustness Test Set (ARTS) (Xing et al., 2020). ARTS enrich the initial test sets from Lap14 and Rest14 by employing three adversarial strategies7. Here, if a model predicts the same sentiment labels for an original sample as well as its adversarial samples (their true labels are different), we will assume that there is a spurious correlation between certain contextual words and the predicted label for this original sample. In other words, the predicted label does not change with the contextual semantics because the model only focuses on certain words. Based on this assumption, we count the percentage of original samples in the test set that contain spurious correlations.8 According to Table 4, we can see that spurious correlations do exist in the Lap14 and Rest14 datasets. Moreover, we can observe that the proposed methods reduce the percentage of original samples containing spurious correlations. This may suggest that the generated explanations can alleviate the spurious correlations problem in the ABSA task.\nComparison with Data Augmentation Baselines. This work aims to exploit the explanation from LLMs to reduce spurious correlations, which could be viewed as an augmented instance of the original sentence. To evaluate its effectiveness, we compare the proposed methods with five data augmentation\n7They are (1) reversing the original sentiment of the targeted aspect; (2) reversing the sentiment of the non-targeted aspects; and (3) generating more non-targeted aspects with opposite sentiment polarities from the targeted aspect.\n8It is worth reminding that this percentage is not the actual percentage of spurious correlations in the dataset, which is only an estimate under this assumption.\nModels Datasets L \u21d2 R R \u21d2 L\nAcc. F1 Acc. F1 BERT 77.02 63.83 73.82 68.74\n+ augmentation 78.93 66.33 76.33 72.02 + distillation 79.11 64.93 75.55 70.99\nBERT+PT 78.82 70.74 74.55 68.62 + augmentation 81.25 71.42 75.55 69.91\n+ distillation 82.68 75.37 73.20 66.53\nTable 5: The generalization results of ABSA models. L \u21d2 R (or R \u21d2 L) refer to that the model is trained on Lap14 (or Rest14) training data and then tested on Rest14 (or Lap14) test data. The best scores for each baseline are in bold.\nbaselines.9 Table 3 reports the experimental results. It can be seen that our methods perform better than all baselines, achieving the biggest improvements of 3.48% and 5.47% in accuracy and F1, respectively. This shows that the explanation from LLMs is more effective because of including not only a contextual understanding of sentiments but also external knowledge. Besides, we find that these augmentation baselines often consistently improve the performance of BERT, showing that modifying the original text may bring gains, despite the noise.\nGeneralization Analysis. We evaluate the proposed methods in the cross-domain scenario to check their effectiveness on generalizability. The experimental results are presented in Table 5. We can observe that: (1) Our methods significantly enhance the generalization ability of the peer baselines by a substantial margin. We attribute it to the explanations of aspect sentiment that can reduce the spurious correlations in ABSA. (2) Overall, the augmentation method is more effective than the distillation one. A possible reason for this is that explanations containing external knowledge are directly involved in the training, allowing the learning of better transfer features.\nEffectiveness in Low-Resource Scenario. Here, we carry out an experiment to observe the performance improvements achieved by our proposed methods in low-resource settings. To this end, we vary the percentage of training data from 10% to 100% in increments of 10% and depict results in Figure 3. We can see that: (1) Overall, being armed with our methods can improve the performance of BERT. It shows that introducing an explanation for sentiment is useful in low-resource scenarios. (2) The performance gradually improves as the per-\n9We add augmented versions directly to the existing set. Thus, the training set size will be doubled after this operation.\n58\n63\n68\n73\n78\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nBERT +augmentation +distillation\npercentage of training data\nF 1\n( %\n)\nlap14\n(a) On Lap14 dataset.\n50\n57\n64\n71\n78\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nBERT +augmentation +distillation\npercentage of training data\nF 1\n( %\n)\nrest14\n(b) On Rest14 dataset.\nFigure 3: Experiments in low-resource scenarios. We limit the percentage of training data when fine-tuning.\ncentage increases before the training size surpasses 50%, indicating that the more training data, the better the model is trained. Nevertheless, upon surpassing this point, the performance fluctuates moderately.\nLength-wise Performance Analysis. Spurious correlations are prone to occur when predicting the short text as the model tends to resort to the learned statistical bias facing a low-informative context. Here, we test length-wise performance to reveal the noticeable advantages of the proposed methods on short texts. Figure 4 provides the test results. We can see that the proposed methods significantly improve the performance of BERT, especially on short texts. It shows that the explanation provided by LLMs for the training samples motivates the ABSA model to be trained effectively, thus allowing for a better contextual understanding of short texts at testing.\nThe Appendix has more discussion and analysis, i.e., Quality of the Automatically-Generated Explanations, Effect of Different Prompt Templates on Performance, and Error Analysis."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce an effective two-step framework to mitigate spurious correlations in ABSA. First, we formulate a prompt template to induce LLMs to generate an appropriate explanation that states the sentiment cause. Subsequently, we propose two straightforward methods that utilize the generated explanation to prevent the assimilation of spurious correlations. Our comprehensive experiments on five ABSA datasets show that baselines armed with our methods outperform peers in prediction performance and generalization.\nLimitations\nIn this section, we list two limitations to understand this work more comprehensively:\n1. The prompt template designed in this work consists of three components: the task description, the training sample, and the output limitation. Generally speaking, a prompt-rich template allows LLMs to generate more helpful explanations about sentiment and richer relevant external knowledge. In this work, we did not design a prompt-rich template because this manual design process is time-consuming and cumbersome. In addition, designing a complex and information-rich prompt template is not the research focus of this work.\n2. We leverage explanations to reduce spurious correlations in the ABSA task. In this work, we generate an explanation for the sentiment label in each training sample, which subsequently participates in the model training process. Although spurious correlations are caused by statistical bias during training, not all training samples bring bias interference to the model. Therefore, the participation of all explanations in model training is an extensive operation, which somehow results in a waste of training resources. How to identify whether a training sample potentially brings spurious correlation interference can be a direction for subsequent research."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially supported by the National Natural Science Foundation of China (62006062, 62176076), Natural Science Foundation of Guangdong 2023A1515012922, Shenzhen Foundational Research Funding JCYJ20210324115614039 and JCYJ20220818102415032, Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies 2022B1212010005."
        },
        {
            "heading": "7 Appendix",
            "text": "Quality of the Automatically-Generated Explanations. Large language models may generate less accurate or even irrelevant explanations for the sentiment of the aspect. Such explanations may\nhave negative effects if they exist and are involved in training. In this work, we only randomly selected nearly fifty explanations for hand-checking. We find that the quality of the generated explanations is accurate and comprehensive (see examples in Table 6). In addition, We observe that these explanations are richly expressive (e.g., \"not reasonable or affordable\").\nEffect of Different Prompt Templates on Performance. In this work, the prompt template is designed based on two points: (1) including a task description (i.e., \"explain the sentiment expressed by aspect term\"), which is used to trigger the LLM\u2019s ability to understand the task; (2) including the output limitation (i.e., \"Limit to forty words. Copying adjectives from the original sentence is not allowed.\"), which is used as an output guide and prevents LLM from rephrasing the sentiment expression. Here, to explore the effect of different prompt templates on performance, we try other well-designed templates and perform comparison experiments. These prompt templates are described in detail below:\n\u2022 Prompt 1 (used in this paper): In the following sentence X , explain why the sentiment expressed by aspect term a is y. Limit to forty words. Copying adjectives from the original sentence is not allowed.\n\u2022 Prompt 2: In the following sentence X , explain why the sentiment expressed by aspect term a is y.\n\u2022 Prompt 3: In the following sentence X , the sentiment expressed by aspect term a is y. Based on this prompt, explain its reasoning. Limit to forty words. Copying adjectives from the original sentence is not allowed.\nHere, for simplicity, we choose BERT as the baseline model and BERT+augmentation as our method. Table 8 presents the experimental results. From this table, we can find that although the template used in this paper yields the best results, there is not much difference between the scores of the different templates (see Prompt 1 vs. Prompt 3). Moreover, we observe that imposing an output constraint in the template favors the performance (see Prompt 1 vs. Prompt 2).\nError Analysis. An error analysis can provide readers with a deeper understanding of whether our\nmethods have successfully reduced errors arising from statistical spurious correlations. Thus, we present a simple error analysis in Table 7. Taking the first sample as an example, BERT makes an incorrect prediction possibly because of focusing on the word \"friendly\" only. We suspect this is because, in the training samples which contain the word \"friendly\", 94.6% of aspect sentiments are \"positive\", i.e., statistically spurious correlation. Moreover, we find that the proposed method also makes a few wrong predictions, especially when the true label is neutral, as shown in the second sample. The potential reason may be that when the label is neutral, the language model generates explanations with slight sentiments due to its own bias, which would mislead the model."
        }
    ],
    "title": "Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models",
    "year": 2023
}