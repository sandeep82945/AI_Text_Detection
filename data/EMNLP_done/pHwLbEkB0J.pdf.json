{
    "abstractText": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zeroshot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt \u201cLet\u2019s think step by step!\u201d. Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development. In this work, we introduce cross-lingual prompting (CLP), aiming to improve zero-shot CoT reasoning across languages. Specifically, CLP consists of two main components: (1) cross-lingual alignment prompting and (2) taskspecific solver prompting. The cross-lingual alignment prompting is responsible for aligning representations across different languages, whereas the task-specific solver prompting is used to generate the final chain of thoughts and results for the reasoning task. In addition, we further introduce cross-lingual self-consistent prompting (CLSP) to ensemble different reasoning paths across languages. Our experimental evaluations on several benchmarks demonstrate that CLP and CLSP significantly outperform the existing prompting methods and achieve state-of-the-art performance. We hope this work will inspire further breakthroughs in cross-lingual CoT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Libo Qin"
        },
        {
            "affiliations": [],
            "name": "Qiguang Chen"
        },
        {
            "affiliations": [],
            "name": "Fuxuan Wei"
        },
        {
            "affiliations": [],
            "name": "Shijue Huang"
        },
        {
            "affiliations": [],
            "name": "Wanxiang Che"
        }
    ],
    "id": "SP:b3304c1185dac3dcdc587b7ee25b8ea935e12249",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "year": 2022
        },
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Uddin Ahmad",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "Rifat Shahriyar."
            ],
            "title": "CrossSum: Beyond English-centric cross-lingual summarization for 1,500+ language pairs",
            "venue": "Proceedings of the 61st Annual Meeting of",
            "year": 2023
        },
        {
            "authors": [
                "Terra Blevins",
                "Luke Zettlemoyer."
            ],
            "title": "Language contamination helps explains the cross-lingual capabilities of English pretrained models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563\u20133574, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel R. Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "Xnli: Evaluating crosslingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "PAL: program-aided language models",
            "venue": "International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,",
            "year": 2023
        },
        {
            "authors": [
                "Olga Golovneva",
                "Moya Chen",
                "Spencer Poff",
                "Martin Corredor",
                "Luke Zettlemoyer",
                "Maryam Fazel-Zarandi",
                "Asli Celikyilmaz."
            ],
            "title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
            "venue": "arXiv preprint arXiv:2212.07919.",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing",
            "venue": "arXiv preprint arXiv:2111.09543.",
            "year": 2021
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla."
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Mengkang Hu",
                "Yao Mu",
                "Xinmiao Yu",
                "Mingyu Ding",
                "Shiguang Wu",
                "Wenqi Shao",
                "Qiguang Chen",
                "Bin Wang",
                "Yu Qiao",
                "Ping Luo"
            ],
            "title": "Tree-planner: Efficient close-loop task planning with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Haoyang Huang",
                "Tianyi Tang",
                "Dongdong Zhang",
                "Wayne Xin Zhao",
                "Ting Song",
                "Yan Xia",
                "Furu Wei."
            ],
            "title": "Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal."
            ],
            "title": "Decomposed prompting: A modular approach for solving complex tasks",
            "venue": "The Eleventh International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona T. Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Malkin",
                "Tomasz Limisiewicz",
                "Gabriel Stanovsky."
            ],
            "title": "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Wenbo Pan",
                "Qiguang Chen",
                "Xiao Xu",
                "Wanxiang Che",
                "Libo Qin"
            ],
            "title": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
            "year": 2023
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Libo Qin",
                "Qiguang Chen",
                "Tianbao Xie",
                "Qixin Li",
                "JianGuang Lou",
                "Wanxiang Che",
                "Min-Yen Kan."
            ],
            "title": "Gl-clef: A global-local contrastive learning framework for cross-lingual spoken language understanding",
            "venue": "Proceedings of the 60th Annual Meeting of",
            "year": 2022
        },
        {
            "authors": [
                "Libo Qin",
                "Minheng Ni",
                "Yue Zhang",
                "Wanxiang Che."
            ],
            "title": "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "year": 2019
        },
        {
            "authors": [
                "Freda Shi",
                "Mirac Suzgun",
                "Markus Freitag",
                "Xuezhi Wang",
                "Suraj Srivats",
                "Soroush Vosoughi",
                "Hyung Won Chung",
                "Yi Tay",
                "Sebastian Ruder",
                "Denny Zhou"
            ],
            "title": "Language models are multilingual chain-of-thought reasoners",
            "venue": "arXiv preprint arXiv:2210.03057",
            "year": 2022
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Edward Berman",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Eshaan Tanwar",
                "Subhabrata Dutta",
                "Manish Borthakur",
                "Tanmoy Chakraborty."
            ],
            "title": "Multilingual llms are better cross-lingual in-context learners with alignment",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2023
        },
        {
            "authors": [
                "driguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and finetuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Cross-lingual summarization via chatgpt",
            "venue": "ArXiv, abs/2302.14229.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Duo Zheng",
                "Yunlong Liang",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Towards unifying multi-lingual and cross-lingual summarization",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim"
            ],
            "title": "2023c. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Genta Winata",
                "Alham Fikri Aji",
                "Zheng Xin Yong",
                "Thamar Solorio."
            ],
            "title": "The decades progress on codeswitching research in NLP: A systematic survey on trends and challenges",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages",
            "year": 2023
        },
        {
            "authors": [
                "Genta Indra Winata",
                "Andrea Madotto",
                "Zhaojiang Lin",
                "Rosanne Liu",
                "Jason Yosinski",
                "Pascale Fung."
            ],
            "title": "Language models are few-shot multilingual learners",
            "venue": "CoRR, abs/2109.07684.",
            "year": 2021
        },
        {
            "authors": [
                "Yinfei Yang",
                "Yuan Zhang",
                "Chris Tar",
                "Jason Baldridge."
            ],
            "title": "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik R. Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,",
            "year": 2023
        },
        {
            "authors": [
                "Ping Yu",
                "Tianlu Wang",
                "Olga Golovneva",
                "Badr AlKhamissi",
                "Siddharth Verma",
                "Zhijing Jin",
                "Gargi Ghosh",
                "Mona Diab",
                "Asli Celikyilmaz."
            ],
            "title": "ALERT: Adapt language models to reasoning tasks",
            "venue": "Proceedings of the 61st Annual Meeting of the",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "Star: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems, 35:15476\u201315488.",
            "year": 2022
        },
        {
            "authors": [
                "Qingcheng Zeng",
                "Lucas Garay",
                "Peilin Zhou",
                "Dading Chong",
                "Yining Hua",
                "Jiageng Wu",
                "Yikang Pan",
                "Han Zhou",
                "Rob Voigt",
                "Jie Yang."
            ],
            "title": "Greenplm: Cross-lingual transfer of monolingual pre-trained language models at almost no cost",
            "venue": "Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Bo Zheng",
                "Zhouyang Li",
                "Fuxuan Wei",
                "Qiguang Chen",
                "Libo Qin",
                "Wanxiang Che."
            ],
            "title": "HIT-SCIR at MMNLU-22: Consistency regularization for multilingual spoken language understanding",
            "venue": "Proceedings of the Massively Multilingual Natural Language Un-",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "The Eleventh International Conference on Learning Representations, ICLR 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Kaijie Zhu",
                "Jindong Wang",
                "Jiaheng Zhou",
                "Zichen Wang",
                "Hao Chen",
                "Yidong Wang",
                "Linyi Yang",
                "Wei Ye",
                "Neil Zhenqiang Gong",
                "Yue Zhang"
            ],
            "title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "year": 2023
        },
        {
            "authors": [
                "Zhuang Ziyu",
                "Chen Qiguang",
                "Ma Longxuan",
                "Li Mingda",
                "Han Yi",
                "Qian Yushan",
                "Bai Haopeng",
                "Zhang Weinan",
                "Ting Liu."
            ],
            "title": "Through the lens of core competency: Survey on evaluation of large language models",
            "venue": "Proceedings of the 22nd Chinese National",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have shown remarkable success across various NLP tasks (Qin et al., 2023; Hendy et al., 2023; Pan et al., 2023; Ziyu et al., 2023). Unlike the previous pre-trained language models (PLMs) (Devlin et al., 2019; He et al., 2021), LLMs are capable of achieving zeroshot learning without the need to modify the model parameters during the training and testing process,\n\u2217Equal Contribution\nwhich gains increasing attention. Specifically, zeroshot chain-of-thought (CoT) (Kojima et al., 2022) only needs to append the prompt \u201cLet\u2019s think step by step!\u201d, which can elicit strong reasoning capabilities from large language models and demonstrate promising performance on various tasks, including arithmetic reasoning, commonsense reasoning (Wei et al., 2022; Kojima et al., 2022) and even robotic planning(Ahn et al., 2022; Huang et al., 2022). Take a traditional CoT in Figure 1 (a) as an example, a trigger prompt \u201cLet\u2019s think step by step!\u201d is provided along with an English request to perform step-by-step reasoning. Eventually, LLMs produce the corresponding answer \u201c68 years\u201d.\nIn fact, there are over 200 countries and 7,000 languages worldwide. With the acceleration of globalization, there is an urgent need for generalizing the current CoT across different languages. Despite the remarkable success of zero-shot CoT, its reasoning abilities still struggle to generalize to different languages. Shi et al. (2022) introduce\nthe first multi-lingual dataset to evaluate the mathematical reasoning capabilities of language models to facilitate the research of cross-lingual CoT. Unlike traditional CoT scenarios, where the language of the request and CoT output is the same, crosslingual CoT requires the LLM to generate CoT in English for any given language by providing a trigger sentence \u201cLet\u2019s think in English step by step!\u201d, which is illustrated in Figure 1 (b). Unfortunately, little attention has been paid to zero-shot cross-lingual CoT.\nTo generalize the current CoT across languages, we propose a novel cross-lingual prompting (CLP), which aims to effectively bridge the gap across different languages. It consists of two components: (1) Cross-lingual Alignment Prompting and (2) Task-specific Solver Prompting. Specifically, the cross-lingual alignment prompting is used to align representations between different languages. In our experiments, instead of the traditional \u201cLet\u2019s think step by step\u201d, we use \u201cLet\u2019s understand the task in English step-by-step.\u201d. The inherent intuition is that as model gradually understands the task in English, it inherently captures the relationship between the source language and English. After aligning the representations between different languages, we further utilize a task-specific solve prompting to complete the final task by setting \u201cLet\u2019s resolve the task you understand above step-by-step!\u201d. Such simple yet effective CLP can greatly enhance the reasoning ability of cross-lingual scenarios. Furthermore, inspired by the self-consistency work, we propose cross-lingual self-consistent prompting (CLSP), which enables the model to ensemble different views of reasoning paths across languages.\nExperimental results reveal that CLP achieves the SOTA performance by outperforming all baselines with a gain of over 1.8%. In addition, CLSP can further enhance the performance by integrating knowledge across different languages. The main contributions of this work are concluded as follows:\n\u2022 We introduce cross-lingual prompting that contains cross-lingual alignment prompting and task-specific solver prompting, which jointly improve zero-shot CoT reasoning across languages;\n\u2022 We further propose cross-lingual selfconsistent prompting to integrate reasoning paths across different languages;\n\u2022 Extensive evaluations on several benchmarks\nreveal that both CLP and CLSP are capable of improving zero-shot cross-lingual CoT effectively and achieving SOTA performance (with over 1.8% improvement on AVG accuracy).\nWe hope this work can inspire further research on cross-lingual CoT and the code are available at Cross-Lingual-Prompting."
        },
        {
            "heading": "2 Background",
            "text": "This section describes the definition of traditional and cross-lingual chain-of-thought."
        },
        {
            "heading": "2.1 Traditional Chain-of-Thought",
            "text": "Chan-of-thought is a powerful technique to elicit the strong reasoning ability of large language models (LLM), which is capable of completing complex tasks. For the traditional chain-of-thought (CoT) generation approach, LLM is appended as a simple prompt \u201cLet\u2019s think step by step!\u201d to output the specific reasoning paths, which is denoted as:\nRequest: [Given sentence X] Let\u2019s think step by step!"
        },
        {
            "heading": "2.2 Cross-lingual Chain-of-Thought",
            "text": "While traditional CoT has achieved remarkable success, it is limited to generating CoT within a single language and lacks effective cross-lingual transferability. Therefore, cross-lingual CoT aims to enable models to handle requests in any language and generate CoT specifically in the target language (i.e., English) (Shi et al., 2022)."
        },
        {
            "heading": "3 Cross-lingual Prompting",
            "text": "To elicit the cross-lingual reasoning ability of LLM, we introduce cross-lingual prompting (CLP) as a solution. Specifically, CLP consists of two components: (1) cross-lingual alignment prompting (\u00a73.1) and (2) task-specific solver prompting (\u00a73.2)."
        },
        {
            "heading": "3.1 Step 1: Cross-lingual Alignment Prompting",
            "text": "Cross-lingual alignment is a core challenge for cross-lingual transfer. Therefore, to better capture the alignment information, we first introduce crosslingual alignment prompting (refer to Figure 2 (a)). Specifically, our approach initiates the LLM with a specific task of aligning information. The request is formulated as follows:\nPlease act as an expert in multi-lingual understanding in [Source Language Ls] .\nRequest: [Given sentence X] Let\u2019s understand the task in [Target Language Lt] step-by-step!\nGiven the sentence X , we first simulate the LLM\u2019s expertise in multi-lingual comprehension. Furthermore, we introduce a step-by-step alignment process from source language Ls to\ntarget language Lt . The intermediate semantic alignments are represented as {ai}Si=1, where S denotes the number of alignment steps. Overall, the formulation of our cross-lingual alignment prompting method can be expressed as follows:\nA = argmax p(a1, . . . , aS |X,Ls, Lt), (1)\nwhere A denotes the alignment response in step 1."
        },
        {
            "heading": "3.2 Step 2: Task-specific Solver Prompting",
            "text": "After achieving cross-lingual alignment, we further propose task-specific solver prompting to facilitate multi-step reasoning in a multi-lingual setting.\nSpecifically, given the target language Lt , and the alignment text A obtained from the previous step, we prompt the LLM to engage resolving target tast T . And LLM tries to determine the final result Ft along a multi-step reasoning path R = {ri}|R|i=1, where |R| represents the number of steps in the reasoning process, which is reg-\nulated by the LLM. Specifically, we design the task-specific solver prompting as:\nAfter understanding, you should act as an expert in [Target Task T ] in\n[Target Language Lt] . Let\u2019s resolve the task you understand above step-by-step!\nFormally, the set of potential reasoning path R is organized into the final reasoning path Rt for target language Lt, which can be determined as:\nRt = argmax R p(R|C,Lt, T ), (2)\nwhere C represents the dialog history, including the input variables X , Ls, Lt, and A.\nFurthermore, we provide an instruction to format the model\u2019s answer, which is defined as:\nFinally, you should format your answer as \u2018Answer: [num]\u2019.\nFormally, the answer extraction is determined as:\nFt = argmax p(f |Rt), (3)\nwhere Ft represents the text of the answer, generated from all potential reasoning result f ."
        },
        {
            "heading": "4 Cross-lingual Self-consistent Prompting",
            "text": "In our research, we observe that LLMs show varying patterns across different languages. Inspired\nby Wang et al. (2022), we propose a cross-lingual self-consistent prompting (CLSP) to integrate reasoning knowledge across different languages (as shown in Figure 2 (b)).\nSpecifically, for each step in the reasoning process, we require LLM to generate alignment responses in different target language Lt and employ respective reasoning steps. Finally, we retain the answers that exhibit a high level of consistency in the inferred reasoning results (f ) through a voting mechanism. These consistently inferred answers are then considered as the final result, which can be formulated as:\nF\u0302 = argmax |L|\u2211 t=1 |f |\u2211 f 1 (Ft = f) , (4)\nwhere |L| represents the count of target languages, |f | signifies the count of potential reasoning results f across all target languages, and 1 (X) denotes a 0-1 function that returns 0 when X is False and returns 1 when X is True."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Implementation Settings",
            "text": "We select three representative state-of-the-art pretrained large language models as baseline references for our study: GPT-3 (Brown et al., 2020),\nPaLM (Chowdhery et al., 2022) and GPT3.51. Following Wei et al. (2022) and Kojima et al. (2022), we evaluate the performance using accuracy score (Acc.). The top-p parameter in all processes is set to 1. We select the temperature in Cross-lingual Alignment Prompting from [0, 2] and the temperature in Task-specific Solver Prompting from [0, 1]."
        },
        {
            "heading": "5.2 Main Results",
            "text": "The main results are illustrated in Table 1. From the results, we have the following observations:\n(1) GPT-3.5 exhibits notable cross-lingual reasoning superiority. When evaluated in the all scenarios mentioned in Table 1, GPT-3.5 surpasses the few-shot results of PaLM-540B and GPT-3 by a significant margin (achieving improvements of 30.3%, 2.3% 7.7%, and 14.2% over PaLM-540B, respectively). As shown in Wang et al. (2023a), multi-lingual SFT and RLHF techniques lead to substantial improvement in cross-lingual reasoning performance.\n(2) CLP achieves state-of-the-art performance. As depicted in Table 1, CLP surpasses all previous baselines, specifically outperforming PALM540B(Translate-En) with an improvement of 16.4%. This improvement cannot be solely attributed to GPT-3.5 (CLP even achieves a 2.2%\n1https://platform.openai.com/docs/guides/chat/introduction\n75.4 78.9\n91.1\n78.677.0 81.7\n93.6\n81.4\n70\n80\n90\n100\nInformativeness Step Informativeness Chain\nMissing Step Faithfulness\nPe rf\nor m\nan ce\nGPT3.5 (CLP)GPT3.5 (Native-CoT)\nFigure 4: The analysis of Chain-of-Thought quality between GPT-3.5 (Native-CoT) and CLP.\nhigher average accuracy than Translate-En). These findings suggest that cross-lingual alignment prompting(CLP) goes beyond simple text translation and further enhances the model\u2019s inherent cross-lingual understanding capabilities.\n(3) CLSP further significantly improves performance. As illustrated in Table 1, CLSP exhibits a remarkable superiority over CLP across all languages (with 6.1% improvements on average accuracy). This observation reveals that integrating knowledge across different languages can effectively boost the reasoning performance on crosslingual CoT, verifying the effectiveness of crosslingual self-consistent prompting."
        },
        {
            "heading": "5.3 CLP Analysis",
            "text": ""
        },
        {
            "heading": "5.3.1 CLP results better reasoning quality",
            "text": "To further investigate why CLP works, we employ the framework of ROSCOE (Golovneva et al., 2022) to evaluate the quality of the reasoning paths in the model\u2019s Chain of Thought. The implementation details are shown in Appendix A.2.\nAs shown in Figure 4, we find that the reasoning paths of CLP demonstrate higher faithfulness, exhibiting better consistency with key steps during the reasoning process. Specifically, the faithfulness score increased by 1.6%, indicating that the model\nbetter understood the problem statement and ensured a clear inference chain without generating irrelevant or misused information. Furthermore, we observe 2.8% and 2.5% improvements in the Informativeness metrics for \u201cStep\u201d and \u201cChain\u201d, respectively. It suggests that the model\u2019s reasoning, after cross-lingual alignment, could provide more well-grounded inference steps. Additionally, CLP shows a 2.8% enhancement in the Miss-step metric, indicating that the model\u2019s reasoning could encompass a complete logical chain, leading to better performance."
        },
        {
            "heading": "5.3.2 Two-stage interactive prompting is better than single turn prompting",
            "text": "This section explores the effectiveness of twostage interactive prompting. Instead of using two turns cross-lingual alignment prompting and taskspecific solver prompting to separately perform alignment and task solving, we directly concatenate the cross-lingual alignment prompting and task-specific solver prompting using the newline character \"\\n\" for LLM.\nResults are illustrated in Figure 3. Compared with two-stage interactive prompting (CLP), we observe a significant average decrease of 10.4% in the single-turn prompting performance. We suppose that two-stage interactive prompts can better elicit the strong dialogue interactive ability of LLM, thereby enhancing the performance."
        },
        {
            "heading": "5.3.3 CLP is not a vanilla translation",
            "text": "As shown in Table 1, we can find that CLP even achieves a 2.2% higher average accuracy than Translate-En, which indicates that CLP is not a vanilla translation but utilizes the semantic alignment between the languages. To further understand how CLP works better than translation, we randomly choose 200 samples from different lan-\nguages for fine-grained exploration. First, we find that CLP has 7 different strategies (as shown in Table 7), which all contribute to the final performance, which demonstrates the effectiveness of CLP. Further, we find that breaking down stage 1 further can help improve. Breaking down the actions of stage 1 into 2 to 4 strategies can significantly enhance performance (by at least 6.5%). For example, By decomposing the alignment process into \u201cProblem Restatement\u201d and \u201cPreliminary Solution\u201d, better performance can be achieved, reaching 64.7% (an increase of 11.8% compared with Native-CoT)."
        },
        {
            "heading": "5.3.4 How does prompt selection affect CLP?",
            "text": "We validate the robustness of the zero-shot crosslingual chain-of-thought against the cross-lingual alignment prompts.\nTable 4 illustrates the performance of 4 different cross-lingual alignment prompts. The experimental results demonstrate that although there are some fluctuations in the AVG Acc. of alignment and reasoning based on specific prompts (with a maximum difference of over 4%), all cross-lingual alignment prompts can still improve the performance compared to the traditional CoT. This further verifies the effectiveness of CLP."
        },
        {
            "heading": "5.3.5 Generality Analysis of CLP",
            "text": "In order to further study the generality of our work, we verify the generality of CLP from two aspects:\nCLP works well on other benchmarks. We conduct experiments on other multilingual reasoning datasets, namely XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019). As shown in Figure 5, CLP can obtain better performance across a majority of languages. In comparison to En-CoT, we observed an average improvement of 3.1% on XNLI and 4.5% on PAWS-X2.\nCLP works well on other LLMs. To better understand the model generalization, we conduct the experiments on the XCOPA with smaller LLMs. Experimental results (as shown in Table 2) demonstrate that on smaller LLMs, CLP achieves at least a 6.8% improvement compared to En-CoT. Those further demonstrate the effectiveness and the wide applicability of CLP."
        },
        {
            "heading": "5.3.6 CLP can be further improved by in-context-learning",
            "text": "In recent years, in-context-learning (ICL) has achieved amazing results on LLMs. In order to further explore the performances of CLP within the ICL framework, a series of experiments were conducted. Subsequent analysis of the empirical findings has led to the following observations:\nUsing ICL in cross-lingual alignment prompts can significantly enhance reasoning performance. As depicted in Table 5, CLP exhibits a noteworthy 6.9% improvement over the zero-shot setting on MGSM. This further underscores the versatility of our approach as a plug-and-play solution, orthogonal to ICL methods, mutually reinforcing each other to augment performance.\n2Due to the cost constraint, we randomly select 200 samples per language from test set.\nUsing ICL in task-specific solver prompting can further boost reasoning performance. As depicted in Table 5, the results reveal an additional 1.1% performance enhancement when incorporating Complex-CoT (Fu et al., 2023) in task-specific solver prompting. This further solidifies the distinctiveness of our approach in contrast to other CoT optimization methods, underscoring its adaptability and its capacity to offer more extensive support to downstream CoT inference techniques.\nFor alignment, the example selection plays a pivotal role. We conducted experiments with various combinations of Few-shot strategies. As shown in Table 6, if few-shot relies on a single strategy, the model\u2019s average performance drops significantly to 63.5%, even far below the effect of zero-shot. Conversely, when a more diverse set of strategies is employed within Few-shot examples, the model\u2019s performance shows a substantial improvement, reaching 75.9%. It shows that more diverse strategy samples lead to better performance enhancement."
        },
        {
            "heading": "5.4 CLSP Analysis",
            "text": ""
        },
        {
            "heading": "5.4.1 Cross-lingual self-consistent prompting surpasses vanilla self-consistency",
            "text": "To validate the effectiveness of CLSP, we conduct experiments on vanilla self-consistency (VSC) (Wang et al., 2022) which obtains diverse CoT paths for better results. As shown in Figure 6, CLSP outperforms VSC about 4.5% on average, which verifies the effectiveness of CLSP. Further, we try to explore why CLSP works. We evaluate\nthe alignment scores between cross-lingual CoT inference paths (including CLSP and VSC) with all correct predicted results and manually annotated CoT inference paths. As illustrated in Figure 7, the variance of alignment scores generated by CLSP is significantly higher than VSC compared with the results of Yu et al. (2023). It shows that CLSP better ensembles language knowledge to enhance the final cross-lingual CoT performance. The implementation details are shown in Appendix A.3.1."
        },
        {
            "heading": "5.4.2 More languages can not bring more improvement",
            "text": "A natural question that arises is, \u201cDoes integrating a larger number of languages in self-consistent cross-lingual prompting lead to better overall performance?\u201d To answer this question, we explore the relationship between performance and the number of languages integrated. Some studies (Blevins and Zettlemoyer, 2022; Malkin et al., 2022) suggest that the LLM\u2019s performance is highly related with the proportion of pretraining data in each language. Therefore, we examine the language distribution (refer to Figure 8) in the widely used multilingual pretraining dataset, Common Crawl 2021. Based on the proportions, we incrementally integrated languages in descending and ascending order of their respective proportions. The results in Figure 9 demonstrate that in high-resource settings (>4%), performance improves as more languages are added. However, when incorporating low-\nresource languages, performance decreases with an increasing number of languages. These findings emphasize that the effectiveness of language integration is not solely determined by the number of languages integrated. Quantity of pretraining data for each language, especially in high-resource languages, play a crucial role. Balancing multiple languages considering available resources and impact is vital. This research provides valuable insights for developing multilingual models that strike a balance between incorporating diverse languages and maintaining high-performance standards."
        },
        {
            "heading": "5.4.3 Qualitative analysis",
            "text": "To further understand why CLP works intuitively, we provide a case study that compares the outputs generated by the traditional CoT approach and CLP. As depicted in Figure 10, we observe that the traditional CoT fails to comprehend all the information present in the query (missing the information about \u201cJessie is 20 years old\u201d), thereby resulting in the error inference of the final result. However, our proposed CLP overcomes this limitation by first utilizing the cross-lingual alignment prompting to ensure the model comprehensively understands the given query, which detailed aligns the source language to the target language sentence-by-sentence. Then the task-specific solver prompting is implied to solve this problem step-by-step without deviation from\nthe information in the query. This indicates that our proposed CLP can simulate the model\u2019s ability to understand the cross-lingual query clearly before attempting to solve the problem. And this capability is essential because if the misunderstood happened, the final result may also be erroneously inferred in a high probability. This observation further validates the effectiveness of CLP."
        },
        {
            "heading": "5.4.4 Extension to XCOPA",
            "text": "To further verify the effectiveness of CLSP, we conduct experiments on XCOPA (Ponti et al., 2020), a widely adopted dataset for assessing commonsense reasoning skills across 11 different languages.\nAs the results presented in Table 3, in comparison to the baselines, we observe a significant average improvement of 4.7% in CLP performance. And it even surpasses the results reasoning with translated requests by 1.8%. Furthermore, CLSP leads to an additional enhancement of 7.4% compared to CLP. These results signify that apart from excelling in mathematical reasoning, both CLP and CLSP demonstrate notable effectiveness in addressing common-sense reasoning tasks."
        },
        {
            "heading": "6 Related Work",
            "text": "Chain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022) is an effective and step-by-step\nstrategy applied to Large Language Models (LLMs) for zero-shot and few-shot reasoning. CoT prompts, which can be a single instruction or a set of CoT examples, facilitate the generation of intermediate reasoning steps. Recently, a series of studies (Zhou et al., 2022; Wang et al., 2022, 2023c; Khot et al., 2023) have proposed their respective prompting strategies, dividing the entire task into smaller subtasks and subsequently resolving, planning, and executing these subtasks. With the improvement in model capabilities, some works (Zelikman et al., 2022; Zhou et al., 2023; Hu et al., 2023; Gao et al., 2023) treat instructions as \"programs\" for further search, execution, or optimization. Building upon this, considering the feedback brought by execution, ReAct (Yao et al., 2023) and Reflexion (Shinn et al., 2023) further explore the interactive generation of inference decisions and task execution, thereby achieving greater synergy.\nCross-lingual Generalization Prior studies have demonstrated the benefits of pre-trained multilingual models in diverse downstream tasks, such as cross-lingual spoken language understanding (Qin et al., 2020, 2022; Zheng et al., 2022) and crosslingual summarization (Wang et al., 2023a,b; Bhattacharjee et al., 2023). Recently, with the emergence of Large Language Models (LLMs), nontraining-based cross-lingual learning has gained more attention (Brown et al., 2020; Ahuja et al., 2023; Winata et al., 2023; Zeng et al., 2023; Huang et al., 2023). Additionally, in the context of crosslingual alignment, the current common practice involves employing few-shot learning to guide models for better alignment (Winata et al., 2021; Shi et al., 2022; Tanwar et al., 2023; Lin et al., 2022).\nCompared to their work, we explore the zeroshot cross-lingual alignment CoT and introduce CLP to address this problem, which does not need any additional examples to be constructed. Furthermore, we explore Cross-lingual Self-consistent\nPrompting (CLSP) to enhance the performance by leveraging chained cross-lingual pathways devised by experts in various languages."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we introduced cross-lingual prompting (CLP) for cross-lingual Chain-of-Thought. Specifically, CLP consists of cross-lingual alignment prompting and task-specific solver prompting to align representations across languages and generate the final reasoning paths in cross-lingual settings. In addition, we proposed a cross-lingual selfconsistent prompting (CLSP) to effectively leverage knowledge across languages, which further boosts performance over CLP. Extensive experiments reveal that both CLP and CLSP can attain promising performance in cross-lingual CoT."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National NaturalScience Foundation of China (NSFC) via grant 62306342, 62236004 and 61976072. This work was also sponsored by CCF-Baidu Open Fund. We are grateful for resources from the High Performance Computing Center of Central South University. Libo Qin is the corresponding author.\nLimitations\nConsistent with the findings of Kojima et al. (2022); Zhu et al. (2023), our results also indicate that CLP exhibits varying performance improvements in reasoning based on different prompts. Although all of these prompts can enhance the performance, there are still significant performance disparities, with differences exceeding 4%. Therefore, enhancing the robustness of model alignment remains an urgent issue that needs to be addressed in the future."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Robust Analysis Implementation\nIn order to further verify the robustness of CLP, we conducted an analysis of the final results for various CLPs with different expressions. Specifically, we utilize GPT3.5 to generate 3 guiding prompts synonymous with \u201cLet\u2019s understand the task in English step by step!\u201d. Our instruction is as follows:\nAssuming you are a professional rewriter, you need to modify the following request into three different versions:. Let\u2019s think in [Target Language Lt] step by step!\nThe final generated prompt and corresponding results are shown in Table 4.\nA.2 Chain-of-Thought Quality Scoring Implementation\nThe ROSCOE framework (Golovneva et al., 2022) incorporates multiple chain-of-thought quality metrics, with the reasoning alignment vector \u03b1 = r-align(h \u2192 s) = {\u03b11, \u03b12, \u00b7 \u00b7 \u00b7 , \u03b1N} \u2208 [0, 1]N from the N -step hypothesis h = {hi}Ni=1 to the source input s of length T , where \u03b1i are defined as:\nr-align(hi \u2192 s) = 1 +maxTj=1 cos(hi, sj)\n2 . (5)\nA.2.1 Faithfulness The Faithfulness (F ) score is calculated based on the alignment between the hypothesis steps h and the source sentences s. It represents the average reasoning alignment score over the steps of reasoning:\nF = 1\nN N\u2211 i=1 r-align(hi \u2192 s). (6)\nThe Faithfulness score serves as a measure to assess whether the model misconstrued the problem statement or if the reasoning chain is characterized by vagueness, irrelevance, or the misuse of information.\nA.2.2 Informativeness Step Informativeness-Step (Info-Step) measures the utilization of information from the source text s in the reasoning steps h:\nInfo-Step= 1\n2T T\u2211 t=1 r-align(st\u2192h)+ 1 2 F. (7)\nInfo-Step assigns a higher score to reasoning steps that demonstrate a strong alignment with the source, thereby indicating the extent to which the generated hypothesis incorporates the information from the source. Conversely, a lower Info-Step score indicates reasoning steps that are unrelated to the source sentences or overlook the provided information in the context.\nA.2.3 Informativeness Chain Just like the Info-Step metric, the InformativenessChain (Info-Chain) metric measures the extent of concordance between the hypothesis chain and the source. The calculation is as follows:\nInfo-Chain= 1 + cos(h, s)\n2 . (8)\nTo facilitate this computation, we treat the reasoning chain and the source context as an integrated entity.\nA.2.4 Missing Step To pinpoint any significant steps that could be lacking in the hypothesis, (Golovneva et al., 2022) introduce the Missing Step (Miss-Step) metric, which examines the alignment between the reference reasoning text r = {ri}Ki=1 and the hypothesis h. Miss-Step is needed to meticulously assess each step in the reference and verify the existence of a similar step in the hypothesis. The metric is computed as:\nMiss-Step = K min i=1 (r-align(ri \u2192 h)). (9)\nA.2.5 Multi-lingual Setting Due to the limited support of the original ROSCOE (Golovneva et al., 2022) framework for monolingual English, we expanded ROSCOE to operate in a cross-lingual setting to enhance the assessment of Cross-lingual CoT\u2019s inference quality. For the backbone of sentence similarity computation in the model, we employed a multilingual variant of MP-Net3 (Reimers and Gurevych, 2019).\nA.3 Reasoning Alignment Scoring A.3.1 Metric Definition Reasoning Alignment Scoring (RAS) offers a simple method to evaluate the accuracy of the hypothesis chain by examining the extent of overlap between the hypothesis and the reference. One ap-\n3https://huggingface.co/sentencetransformers/paraphrase-multilingual-mpnet-base-v2\nproach to achieving this is by quantifying the reasoning alignment between the two, which can be calculated as:\nRAS = 1\nN N\u2211 i=1 r-align(hi\u2192r). (10)\nA.3.2 Implementation Setting Since completely incorrect reasoning can also lead to a significant decrease in RAS, we conducted the experiments in Figure 7 by excluding all samples with prediction errors and only calculating RAS on correctly predicted samples.\nIn Figure 7 (a), we selected English as the target language and generated seven CoT reasoning results by adjusting the model\u2019s output temperature. We calculated the RAS between the reasoning step outputs of each correctly predicted sample and the standard reasoning step outputs. By averaging the RAS of all samples, we obtained the comprehensive RAS for source-to-English comprehension. Similarly, in Figure 7 (b), we chose a high-resource language as the target language and obtained seven CoT reasoning results. We computed the RAS between the reasoning step outputs of each correctly predicted sample and the standard reasoning step outputs, and then averaged the RAS of all samples to obtain the comprehensive RAS for source-to-target language comprehension.\nOverall, the CLSP exhibits a stronger diversity in reasoning paths, particularly in the original language reasoning of zh, ja, and de, which shows a higher similarity to the original reasoning paths (\u2265 0.845). On the other hand, cross-lingual reason-\ning from es to sw, ja to sw, and ru to te demonstrates more unique reasoning paths (\u2264 0.805).\nA.4 Strategy Definition In our deep exploration, we find that CLP not only serves as simple translation but also has seven different strategies, which are summarized below:\n\u2022 Step-by-step Translation: The model divides the translation process into steps based on commas or periods and translates them step by step, as illustrated in Figure 10.\n\u2022 Key Information Extraction: The model first extracts key terms or critical conditions from the request for translation. This aids the model in achieving better cross-lingual alignment.\n\u2022 Preliminary Solution: This strategy indicates that CLP starts preliminary mathematical operations based on comprehension. It may even provide answers during the alignment phase. However, the model\u2019s second stage may modify this answer, so it is not the final solution.\n\u2022 Complete translation: This strategy indicates that the model directly performs machine translation of the request without sentence splitting or step-wise operations.\n\u2022 Problem restatement: This strategy indicates that the model rephrases the request. Unlike machine translation, problem restatement requires the model to infer, add its understanding, and include information inferred from the request through reasoning.\n\u2022 Step Division: This strategy encompasses two situations: (1) The model actively divides the cross-lingual alignment process into multiple steps. For example, it will divide the alignment process into \u201cStep 1: Identify the context and topic\u201d, \u201cStep 2: Translate the sentence\u201d and \u201cStep 3: Analyze the sentence structure\u201d. (2) The model actively plans the next task and divides the request into several sub-questions.\n\u2022 Code-switching: This strategy indicates that the model actively replaces certain words in the text with words from the target language.\n\u2022 Denial of Service: ChatGPT refuses to perform cross-lingual alignment and delegates alignment directly to the second stage.\nA.5 Few-shot Setting In order to verify the effect of CLP on ICL, we further designed experiments with few-shot settings for analysis. Specifically, we first selected 1,000 samples of data from MGSM test set for testing. In the alignment stage, we immediately used some examples from the dev set to construct cross-language alignment examples. The results of these examples were all generated by GPT3.5. We only keep the correct answers as examples.\nIn the problem-solving phase, we further used the example of Complex-CoT as a problem-solving example. The results in Table 5 show that the twostage ICL can better promote the performance of the model. This also illustrates the versatility of CLP and its ability to be orthogonal to other prompt optimization solutions.\nFurthermore, in order to explore the impact of different examples on CLP, we further analyze the impact that examples using different alignment strategies mentioned in Section 5.3.3 can have on downstream tasks. We manually annotate the dev set and used multiple strategies for annotation. Experiments in Table 6 show that as the diversity of strategies increases, the performance of the model gradually increases."
        }
    ],
    "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
    "year": 2023
}