{
    "abstractText": "In-context learning is a new learning paradigm where a language model observes a few examples and then directly outputs the test input\u2019s prediction. Previous works have shown that it is sensitive to the provided examples and randomly sampled examples probably cause inferior performance. In this paper, we propose finding \u201csupport examples\u201d for in-context learning: Given a training dataset, it aims to select one permutation of a few examples, which can well characterize the task for in-context learning and thus lead to superior performance. Although for traditional gradient-based training, there are extensive methods to find a coreset from the entire dataset, they struggle to identify important in-context examples, because incontext learning occurs in the language model\u2019s forward process without gradients or parameter updates and thus has a significant discrepancy with traditional training. Additionally, the strong dependency among in-context examples makes it an NP-hard combinatorial optimization problem and enumerating all permutations is infeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this challenge in two stages: First we filter the dataset to obtain informative in-context examples individually. Specifically, we propose a novel metric, InfoScore, to evaluate the example\u2019s in-context informativeness based on the language model\u2019s feedback, and further propose a progressive filtering process to filter out uninformative examples. Then we propose diversity-guided example search which iteratively refines and evaluates the selected example permutations, to find examples that fully depict the task. The experimental results show that LENS significantly outperforms a wide range of baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaonan Li"
        },
        {
            "affiliations": [],
            "name": "Xipeng Qiu"
        }
    ],
    "id": "SP:a1cfab949d40c5b9972a3456a5d20b2c2feb0013",
    "references": [
        {
            "authors": [
                "Amina Adadi."
            ],
            "title": "A survey on data-efficient algorithms in big data era",
            "venue": "J. Big Data, 8(1):1\u201354.",
            "year": 2021
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "FAccT",
            "year": 2021
        },
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,",
            "year": 2020
        },
        {
            "authors": [
                "Ting-Yun Chang",
                "Robin Jia"
            ],
            "title": "Data curation alone can stabilize in-context learning",
            "year": 2023
        },
        {
            "authors": [
                "Yanda Chen",
                "Chen Zhao",
                "Zhou Yu",
                "Kathleen R. McKeown",
                "He He."
            ],
            "title": "On the relation between sensitivity and accuracy in in-context learning",
            "venue": "CoRR, abs/2209.07661.",
            "year": 2022
        },
        {
            "authors": [
                "Yutian Chen",
                "Max Welling",
                "Alexander J. Smola."
            ],
            "title": "Super-samples from kernel herding",
            "venue": "CoRR, abs/1203.3472.",
            "year": 2012
        },
        {
            "authors": [
                "Qinyuan Cheng",
                "Xiaogui Yang",
                "Tianxiang Sun",
                "Linyang Li",
                "Xipeng Qiu."
            ],
            "title": "Improving contrastive learning of sentence embeddings from AI feedback",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 11122\u201311138, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Cody Coleman",
                "Christopher Yeh",
                "Stephen Mussmann",
                "Baharan Mirzasoleiman",
                "Peter Bailis",
                "Percy Liang",
                "Jure Leskovec",
                "Matei Zaharia."
            ],
            "title": "Selection via proxy: Efficient data selection for deep learning",
            "venue": "8th International Conference on Learning Repre-",
            "year": 2020
        },
        {
            "authors": [
                "Corinna Cortes",
                "Vladimir Vapnik."
            ],
            "title": "Supportvector networks",
            "venue": "Machine learning, 20(3):273\u2013297.",
            "year": 1995
        },
        {
            "authors": [
                "IBM ILOG Cplex."
            ],
            "title": "V12",
            "venue": "1: User\u2019s manual for cplex. International Business Machines Corporation, 46(53):157.",
            "year": 2009
        },
        {
            "authors": [
                "Rajarshi Das",
                "Manzil Zaheer",
                "Dung Thai",
                "Ameya Godbole",
                "Ethan Perez",
                "Jay Yoon Lee",
                "Lizhen Tan",
                "Lazaros Polymenakos",
                "Andrew McCallum."
            ],
            "title": "Casebased reasoning for natural language queries over knowledge bases",
            "venue": "Proceedings of the 2021 Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui"
            ],
            "title": "A survey for in-context learning",
            "year": 2022
        },
        {
            "authors": [
                "Avia Efrat",
                "Omer Levy"
            ],
            "title": "The turking test: Can language models understand instructions? CoRR, abs/2010.11982",
            "year": 2020
        },
        {
            "authors": [
                "Jiahui Gao",
                "Renjie Pi",
                "Yong Lin",
                "Hang Xu",
                "Jiacheng Ye",
                "Zhiyong Wu",
                "Xiaodan Liang",
                "Zhenguo Li",
                "Lingpeng Kong."
            ],
            "title": "Zerogen+: Self-guided highquality data generation in efficient zero-shot learning",
            "venue": "CoRR, abs/2205.12679.",
            "year": 2022
        },
        {
            "authors": [
                "Chengcheng Guo",
                "Bo Zhao",
                "Yanbing Bai."
            ],
            "title": "Deepcore: A comprehensive library for coreset selection in deep learning",
            "venue": "Database and Expert Systems Applications - 33rd International Conference, DEXA 2022, Vienna, Austria, August 22-24,",
            "year": 2022
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A. Smith",
                "Mari Ostendorf."
            ],
            "title": "Incontext learning for few-shot dialogue state tracking",
            "venue": "CoRR, abs/2203.08568.",
            "year": 2022
        },
        {
            "authors": [
                "Rishabh K. Iyer",
                "Jeff A. Bilmes."
            ],
            "title": "Submodular optimization with submodular cover and submodular knapsack constraints",
            "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Sys-",
            "year": 2013
        },
        {
            "authors": [
                "Dan Jurafsky",
                "James H. Martin."
            ],
            "title": "Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2nd Edition",
            "venue": "Prentice Hall series in artificial intelligence. Prentice Hall, Pearson",
            "year": 2009
        },
        {
            "authors": [
                "KrishnaTeja Killamsetty",
                "Durga Sivasubramanian",
                "Ganesh Ramakrishnan",
                "Abir De",
                "Rishabh K. Iyer."
            ],
            "title": "GRAD-MATCH: gradient matching based data subset selection for efficient deep model training",
            "venue": "Proceedings of the 38th International Conference",
            "year": 2021
        },
        {
            "authors": [
                "KrishnaTeja Killamsetty",
                "Durga Sivasubramanian",
                "Ganesh Ramakrishnan",
                "Rishabh K. Iyer."
            ],
            "title": "GLISTER: generalization based data subset selection for efficient and robust learning",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Sawan Kumar",
                "Partha Talukdar."
            ],
            "title": "Reordering examples helps during priming-based few-shot learning",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4507\u20134518, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Itay Levy",
                "Ben Bogin",
                "Jonathan Berant."
            ],
            "title": "Diverse demonstrations improve in-context compositional generalization",
            "venue": "CoRR, abs/2212.06800.",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Katerina Margatina",
                "Giorgos Vernikos",
                "Lo\u00efc Barrault",
                "Nikolaos Aletras."
            ],
            "title": "Active learning by acquiring contrastive examples",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event",
            "year": 2021
        },
        {
            "authors": [
                "Julian J. McAuley",
                "Jure Leskovec."
            ],
            "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
            "venue": "Seventh ACM Conference on Recommender Systems, RecSys \u201913, Hong Kong, China, October 12-16, 2013, pages 165\u2013172.",
            "year": 2013
        },
        {
            "authors": [
                "Yu Meng",
                "Martin Michalski",
                "Jiaxin Huang",
                "Yu Zhang",
                "Tarek F. Abdelzaher",
                "Jiawei Han."
            ],
            "title": "Tuning language models as training data generators for augmentation-enhanced few-shot learning",
            "venue": "CoRR, abs/2211.03044.",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Noisy channel language model prompting for few-shot text classification",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Baharan Mirzasoleiman",
                "Jeff A. Bilmes",
                "Jure Leskovec."
            ],
            "title": "Coresets for data-efficient training of machine learning models",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume",
            "year": 2020
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, 21-26 July, 2004, Barcelona,",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June",
            "year": 2005
        },
        {
            "authors": [
                "Mansheej Paul",
                "Surya Ganguli",
                "Gintare Karolina Dziugaite."
            ],
            "title": "Deep learning on a data diet: Finding important examples early in training",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Process-",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Pengzhen Ren",
                "Yun Xiao",
                "Xiaojun Chang",
                "Po-Yao Huang",
                "Zhihui Li",
                "Brij B. Gupta",
                "Xiaojiang Chen",
                "Xin Wang."
            ],
            "title": "A survey of deep active learning",
            "venue": "ACM Comput. Surv., 54(9):180:1\u2013180:40.",
            "year": 2022
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Ozan Sener",
                "Silvio Savarese."
            ],
            "title": "Active learning for convolutional neural networks: A core-set approach",
            "venue": "6th International Conference on Learning",
            "year": 2018
        },
        {
            "authors": [
                "Peng Shi",
                "Rui Zhang",
                "He Bai",
                "Jimmy Lin."
            ],
            "title": "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
            "venue": "CoRR, abs/2210.13693.",
            "year": 2022
        },
        {
            "authors": [
                "Jae-hun Shim",
                "Kyeongbo Kong",
                "Suk-Ju Kang."
            ],
            "title": "Core-set sampling for efficient neural architecture search",
            "venue": "CoRR, abs/2107.06869.",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Hongjin Su",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu."
            ],
            "title": "Selective annotation makes language models better few-shot learners",
            "venue": "CoRR, abs/2209.01975.",
            "year": 2022
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J. Gordon."
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "7th International Conference on Learning Represen-",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Ellen M. Voorhees",
                "Dawn M. Tice."
            ],
            "title": "Building a question answering test collection",
            "venue": "SIGIR 2000: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, July 24-28, 2000, Athens,",
            "year": 2000
        },
        {
            "authors": [
                "Zhiyong Wu",
                "Yaoxiang Wang",
                "Jiacheng Ye",
                "Lingpeng Kong."
            ],
            "title": "Self-adaptive in-context learning",
            "venue": "CoRR, abs/2212.10375.",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Jiahui Gao",
                "Zhiyong Wu",
                "Jiangtao Feng",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "Progen: Progressive zero-shot dataset generation via in-context feedback",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Zhiyong Wu",
                "Jiangtao Feng",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "Compositional exemplars for in-context learning",
            "venue": "CoRR, abs/2302.05698.",
            "year": 2023
        },
        {
            "authors": [
                "har",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "2022a. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Jake Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12,",
            "year": 2015
        },
        {
            "authors": [
                "Yiming Zhang",
                "Shi Feng",
                "Chenhao Tan."
            ],
            "title": "Active example selection for in-context learning",
            "venue": "CoRR, abs/2211.04486.",
            "year": 2022
        },
        {
            "authors": [
                "Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen."
            ],
            "title": "A survey of large language models",
            "venue": "CoRR, abs/2303.18223.",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Coleman"
            ],
            "title": "Least Confidence (the max probability over all labels), Entropy and Margin (max probability margin between different labels) to measure",
            "year": 2020
        },
        {
            "authors": [
                "Paul"
            ],
            "title": "2021) propose the GraNd score to select informative examples. GraNd is the gradient norm expectation of the example",
            "year": 2021
        },
        {
            "authors": [
                "Guo"
            ],
            "title": "2022), we use the gradients of the final fully-connected layer\u2019s parameters as these methods",
            "venue": "feature. B Implementation Details B.1 Baseline Details",
            "year": 2022
        },
        {
            "authors": [
                "Guo"
            ],
            "title": "2022), we use the gradients of the final fullyconnected layer\u2019s parameters as these methods\u2019 example feature. For baselines that output a weighted subset of examples, e.g., CRAIG or GradMatch",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In-Context Learning (ICL) is a new paradigm using the language model (LM) to perform many NLP tasks (Brown et al., 2020; Dong et al., 2022; Zhao et al., 2023). In ICL, by conditioning on a few train-\ning examples, LM can directly output the prediction of a given test input without parameter updates. Restricted by LM\u2019s max input length, it is typical to randomly sample a small set of examples from the entire dataset for in-context learning (Brown et al., 2020; Zhang et al., 2022a). However, in-context learning is sensitive to the provided examples and randomly sampled in-context examples show significant instability and probably cause inferior performance (Lu et al., 2022; Chang and Jia, 2023). In this paper, we propose to select a small list of examples that are informative and representative for the entire dataset as in-context examples. Inspired by the traditional machine learning method, Support Vector Machine (SVM) (Cortes and Vapnik, 1995), where a few support vectors are closest to the decision boundary and provide crucial discriminative information for SVM, we name the selected examples for ICL as support examples since they provide crucial task information for the LM and their quantity is usually limited, too.\nThere is a similar problem in traditional gradientbased deep learning like fine-tuning (Devlin et al., 2019), typically called Coreset Selection (Guo et al., 2022), which aims to select a set of representative training examples for the dataset to benefit many downstream scenarios like data-efficient learning (Adadi, 2021), active learning (Ren et al., 2022), neural architecture search (Shim et al., 2021), etc. However, it is challenging for these coreset selection methods to select important in-\ncontext examples because there is a significant discrepancy between traditional training and ICL. As shown in Figure 1, the \u201clearning\u201d paradigms of model training and ICL are highly different. Traditional training depends on back-propagation\u2019s gradients to update parameters while ICL occurs in LM\u2019s forward process without gradients and parameter updates. Existing coreset selection methods are always coupled with the training procedure, i.e., they usually depend on gradients or run with the training procedure. For example, Paul et al. (2021) select informative examples by their gradients\u2019 norm. Toneva et al. (2019) evaluate each example\u2019s importance by counting how many times it is forgotten, i.e., the example is misclassified after being correctly classified in the previous epoch. Additionally, coreset selection methods mainly depend on the example\u2019s gradients as the feature of example selection (Mirzasoleiman et al., 2020; Killamsetty et al., 2021a,b; Guo et al., 2022). However, LM performs ICL through inference, which does not rely on gradients or parameter updates. Hence, the gap between gradient-based training and ICL makes these methods struggle to effectively select informative examples for in-context learning.\nAnother challenge is the strong dependency among in-context examples. Previous work (Lu et al., 2022) shows that even the same example set with different orderings can result in drastically different performance from random-guess level to state-of-the-art. Here we also conduct an additional case study to shed light on examples\u2019 combinatorial dependency in Table 1. We see that compared with two examples\u2019 individual performance, combining them instead significantly hurts the performance. To cope with examples\u2019 dependency, a straightforward method is to enumerate all possible examples\u2019\ncombinations and verify their performance. However, it will lead to combinatorial explosion and thus is infeasible.\nTo tackle these challenges, we propose LENS, a fiLter-thEN-Search that finds support examples in two stages: in the first stage, we filter the dataset to obtain informative in-context examples individually. Specifically, we propose InfoScore to evaluate the example\u2019s in-context informativeness based on the LM\u2019s feedback, and further propose a progressive filtering process to filter out uninformative examples; In the second stage, we propose a diversity-guided example search method that iteratively refines and evaluates the selected examples to find support examples that can fully depict the task. We summarize our contributions as follows:\n\u2022 To the best of our knowledge, we are the first to define the support examples selection problem for in-context learning and introduce a novel filter-then-search method to tackle it.\n\u2022 We conduct experiments on various text classification datasets and compare our method with a wide range of baselines. Experimental results demonstrate that our method significantly outperforms baselines and previous coreset selection methods bring marginal improvements over the random baseline, which shows the necessity of ICL-specific designing for finding support examples.\n\u2022 We conduct further analyses on support examples and find that they exhibit different trends from random examples in many aspects, which can shed light on the principle of them and ICL. We provide the following key takeaways: 1. Support examples are less sensitive to the order compared with random examples (Lu et al., 2022). 2. Ground truth labels matter for support examples, while the previous study (Min et al., 2022b) show that they are not important for randomly sampled examples. 3. One LM\u2019s support examples can be well transferred to other LMs with different sizes and pre-training corpora and keep the superiority over random examples.\n\u2022 We provide comprehensive empirical results of previous coreset selection methods on ICL, which has not been explored. We release the implementation of our method and baselines to facilitate future research1.\n1https://github.com/LeeSureman/ICL_Support_Example"
        },
        {
            "heading": "2 Background:In-Context Learning",
            "text": "In this section, we introduce the definition of incontext learning. We focus on text classification\u2019s in-context learning using the causal language model (Radford et al., 2018). Given a language model G, n examples {xi, yi}ni=1 and a test input xtest, the prediction of xtest is generated as:\nargmax y\u2208Y\npG(y|x1 \u2295 y1 \u00b7 \u00b7 \u00b7xn \u2295 yn \u2295 xtest), (1)\nwhere Y is the label space and \u2295 is the concatenation operation. To deal with classification tasks, the original label is often mapped to word or words in G\u2019s vocabulary. For example, the positive/negative label in a binary sentiment classification can be mapped to \u201cgreat\u201d/\u201cterrible\u201d. For simplicity, we omit the verbalizer, special tokens and prompting templates in Eq (1).\nAs Eq.(1) shows, G receives the task\u2019s supervision only from the concatenated {xi, yi}ni=1 and directly output the prediction of xtest. Typically, n is limited by the max input length of G, so it is typical for researchers to randomly sample a small set of samples from the entire dataset D (Brown et al., 2020; Zhang et al., 2022a). However, ICL is sensitive to the provided examples and random in-context examples show significant instability and probably cause inferior performance(Lu et al., 2022; Chen et al., 2022). In this paper, we focus on selecting a small list of support examples that are informative for the task and performant for in-context learning, from the entire dataset D."
        },
        {
            "heading": "3 Method",
            "text": "The strong dependency among in-context examples makes selecting support examples essentially an NP-hard combinatorial optimization problem. Enumerating all combinations and evaluating them is infeasible due to the combinatorial explosion. In this section, we propose LENS, a fiLter-thENSearch method to find support examples: 1. we first filter the training dataset to obtain informative examples individually, 2. then we search the example permutation that fully depicts the task from them. In this paper, we instantiate the two stages as a novel example metric with progressive filtering and diversity-guided example search, we leave the development of more powerful components as future work. We introduce these two stages below."
        },
        {
            "heading": "3.1 Informative Examples Filtering",
            "text": "In the first stage, we aim to find those informative examples individually. There are extensive\nAlgorithm 1 Progressive Example Filtering Input: Training set D = {ei}ni=1, language model G, de-\nsired candidate size m, progressive factor \u03c1, initial score data size l.\nOutput: Individually informative examples D\u2032 1: D\u2032 \u2190 D 2: S \u2190 Randomly sample l examples from D. 3: while |D\u2032| > m do 4: for ei \u223c D\u2032 do 5: s(ei)\u2190 I(ei, S) 6: end for 7: if |D\u2032|/\u03c1 < m then 8: D\u2032 \u2190 the top-m of D\u2032 using {s(ei)}|D \u2032| i=1\n9: Break; 10: else 11: D\u2032 \u2190 the top 1\n\u03c1 of D\u2032 using {s(ei)}|D \u2032| i=1\n12: end if 13: S\u2032 \u2190 Randomly sample l \u2217 (\u03c1\u2212 1) examples fromD 14: S \u2190 S \u222a S\u2032 15: end while 16: return D\u2032\nmethods to measure the example\u2019s importance for gradient-based training, like the example\u2019s gradient norm (Paul et al., 2021), loss value in the early training stage or the times of being forgotten (Toneva et al., 2019), etc. However, these methods struggle to identify important in-context examples since ICL is based on LM-inference without gradients and parameter updates. Here we propose InfoScore (Informativeness Score) to measure the individual in-context informativeness of one example e = {x, y} for ICL based on LM\u2019s feedback as:\nI(e,D) = \u2211 e\u2032\u2208D c(e, e\u2032) (2) c(e, e\u2032) = pG(y \u2032|x, y, x\u2032)\u2212 pG(y\u2032|x\u2032), (3)\nwhere e\u2032 = {x\u2032, y\u2032}, and D is the training dataset. Eq (3) is the gap between the probabilities of the ground truth y\u2032 conditioned on (e, x\u2032) and (x\u2032), respectively. So it evaluates how informative e is for the LM to correctly classify x\u2032 and thus measures e\u2019s contribution for e\u2032 in ICL. Hence, I(e,D), the sum of Eq (3) over D, can evaluate the example\u2019s task-level in-context informativeness.\nHowever, computing all examples\u2019 InfoScores over the entire dataset is quadratic in |D| and thus infeasible. We further propose a progressive filtering process to filter out uninformative examples progressively, where promising examples receive more computation while low-quality examples get less computation, shown in Algorithm 1.\nWe filter out uninformative examples in a progressive manner. We first sample a small set of examples from D as initial \u201cscore set\u201d (line 2) to coarsely evaluate the InfoScore of each example\nAlgorithm 2 Diversity-Guided Search Input: Candidate examples D\u2032 = {ei}mi=1, candidates\u2019 fea-\nture {f(ei)}mi=1, a small validation set V , iteration num I, beam size B, example substitution size B\u2032\nOutput: A performant examples\u2019 permutation. 1: E = {Ei}Bi=1 \u2190 initialize B examples\u2019 permutations 2: for i in 1, 2 \u00b7 \u00b7 \u00b7 I do 3: E \u2032 \u2190 {} 4: for E in {Ej}Bj=1 do 5: for b in 1, 2 \u00b7 \u00b7 \u00b7 B\u2032 do 6: e\u2217 \u2190Randomly sample an example from E 7: e\u2217new \u2190 argmaxe\u2208D\u2032 s(e, E \u2212 e\u2217) 8: E\u2217 \u2190 Replace e\u2217 in E with e\u2217new 9: E \u2032 \u2190 E \u2032 \u222a {E\u2217} 10: end for 11: for b in 1, \u00b7 \u00b7 \u00b7 B \u2212 B\u2032 do 12: E\u2217 \u2190 Randomly shuffle E 13: E \u2032 \u2190 E \u2032 \u222a {E\u2217} 14: end for 15: E \u2190 Evaluate E \u2032 on V and get the top-B 16: end for 17: end for 18: return The top-1 of E\nand filter the entire dataset to 1/\u03c1 of its original size (line 5). At the following iteration, we proportionally expand the size of the score set to \u03c1 times by randomly sampling more examples from training set (line 13\u223c15) and use it to calculate InfoScore of the remaining promising examples. As the score set is expanded, the subsequent InfoScore can be calculated in a more fine-grained way and better filter informative examples. Meanwhile, the uninformative examples are filtered out in the previous iteration, which helps save the computational cost. We repeat this procedure until a small set of examples is left.\nThus we achieve filtering examples with high in-context informativeness in the complexity of O(N \u2217 log\u03c1N), where N is the size of training set. In experiments, we set \u03c1 to N 1 C to make it a linear complexity, where C is a constant. According to the size of dataset, \u03c1 is usually set between 2 - 3."
        },
        {
            "heading": "3.2 Diversity-Guided Example Search",
            "text": "After filtering, we get individually informative examples D\u2032. Since the in-context examples have high combinatorial dependency (see Table 1), a straightforward method is to enumerate all possible combinations and evaluate them on a validataion set. However, although we have reduced the candidate examples by filtering, it is still impossible to evaluate all combinations. For example, if there are 50 examples retained after filtering and we want to find a combination of 8 examples from them, it can lead to C850 (about 536 million) combinations, let\nalone considering the examples\u2019 orders. Hence we propose diversity-guided example search to iteratively refine the example selection from filtered examples and obtain the support examples, as shown in Algorithm 2. It starts with a set of initial example permutations. At each iteration, we use the diversity of in-context examples to guide the update of the current candidate permutations. Specifically, for each candidate permutation E = [ei] n i=1, we randomly select an example e\n\u2217 in E and update it with the example e\u2217new as:\ne\u2217new = argmaxe\u2208D\u2032 s(e, E \u2032) (4) s(e, E\u2032) = I(e, S)\u2212 \u03bb \u2211\ne\u2032\u2208E\u2032 sim(f(e), f(e\u2032)), (5)\nwhere E\u2032 = E \u2212 e\u2217, \u03bb is pre-defined hyperparameter, S = {esi}mi=1 is the final score set of the filtering stage. The subsequent term of s(e, E\u2032) in Eq (5) corresponds to the diversity between e and E\u2032, and f(\u00b7) is the example\u2019s feature vector calculated as:\nf(e) = [c(e, es1), c(e, e s 2) \u00b7 \u00b7 \u00b7 , c(e, es|S|)], (6)\nwhere f(e) describes e\u2019s contribution on S\u2019s each example esi in ICL and thus directly encodes e\u2019s in-context feature. If two examples\u2019 f(\u00b7) are similar, their effect on ICL can be redundant and we should avoid selecting both of them in one permutation. Note that I(e, S) and each c(e, esi ) in f(e) are calculated in the filtering stage and can be reused.\nWith s(e, E\u2032) and f(e), the updated candidate permutations can be informative and diverse, and help the LM correctly predict various examples, which can better help find the support examples that fully depict the task in ICL. In this paper, we propose and verify a simple yet effective example ICL feature. We leave the development of more powerful ones as future work.\nSince examples\u2019 order can significantly influence the performance (Lu et al., 2022; Kumar and Talukdar, 2021), we also update E with different orders by randomly shuffling (line 10\u223c13), which can reduce the risk of missing performant combinations of examples due to poor ordering. In order To explore the example search space more comprehensively and alleviate risk of the local-optimal example permutation, we consider the beam search (Jurafsky and Martin, 2009) here instead of greedy search. Specifically, we update each candidate example permutations by diversity-based example substitution and random shuffling for B\u2032 and B \u2212 B\u2032 times, respectively (line 5, 10). Then\nwe leverage a small validation set sampled from (D \u2212D\u2032) to evaluate them and keep the top-B permutations with best performance as next iteration\u2019s candidates. Through these, we can the mitigate issue of local-optimal example permutation, better iteratively refine and evaluate the candidate permutations with high informativeness and diversity in turn and obtain the examples that can fully depict the task.\nTo initialize example permutations E with informativeness and diversity, we formulate it as discrete optimization that maximizes \u2211 e\u2208E s(e, E \u2212 e), which can be solved by the discrete optimization solver like CPLEX (Cplex, 2009)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Dataset In this paper, we conduct experiments on eight text classification datasets across three task families, including Sentiment Classification: SST2, SST-5 (Socher et al., 2013), Amazon (McAuley and Leskovec, 2013) and MR (Pang and Lee, 2005); Subjectivity Classification: Subj (Pang and Lee, 2004); Topic Classification: TREC (Voorhees and Tice, 2000), AGNews (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015).\nMethod Comparison We mainly compare our proposed methods with the following baselines: Random: We randomly select examples from the training set; Random & Validation: We evaluate multiple sets of random examples on the validation set and select the best one. We consider Random & Validation under two settings whose computational cost is similar to our method: 1. the size of validation set is the same as ours at stage 2 (100) and the number of random example sets is the same as our searched and evaluated example permutations (640). 2. the size of validation set is larger, 1000, and the number of random example sets is 100. We also consider a wide range of Coreset Selection methods in gradient-based learning scenarios, according to the methodologies, they can be divided into multiple categories including: Geometry-Based Method: it assumes that data points that are close in the feature space have similar properties, including Herding (Chen et al., 2012) and K-Center Greedy (Sener and Savarese, 2018); Uncertainty-Based Method: it assumes examples with higher uncertainty can have a greater impact on the model and should be contained in coreset, including Least Confidence, Entropy, Margin (Coleman et al., 2020) and CAL (Mar-\ngatina et al., 2021); Error/Loss Based Method: It assumes the example that contributes more to the error or loss during training is more important and should be included in coreset, including Forgetting (Toneva et al., 2019) and GraNd (Paul et al., 2021); Gradient Matching Based Method: Since deep models are usually trained by gradient descent, it tries to find a coreset whose gradients can imitate the entire dataset\u2019s gradients, including CRAIG (Mirzasoleiman et al., 2020) and GradMatch (Killamsetty et al., 2021a); SubmodularityBased Method: Submodular functions (Iyer and Bilmes, 2013) naturally measure a subset\u2019s informativeness and diversity and can thus be powerful for coreset selection, including Facility Location and Graph Cut (Iyer and Bilmes, 2013); Bilevel Optimization Based Method: It transforms the coreset selection problem into a bilevel-optimization problem whose outer and inner objectives are subset selection and model parameter optimization, respectively: Glister (Killamsetty et al., 2021b). Due to the page limit, we introduce these methods and their implementation details in Appendix A and Appendix B.1, respectively.\nImplementation Details For the LM, we follow Min et al. (2022a) to use GPT2-L (Radford et al., 2018). We set the number of retained examples of filtering m, the weight of diversity \u03bb, the beam size B and the number of diversity search iterations as 500, 1, 8 and 10 respectively. We show the overall hyper-parameters, implementation details, analysis details and complexity analysis in Appendix B. For baselines and LENS, we run each method under 4 prompt templates over 10 random seeds (40 in total) and report the average performance with and without calibration (Zhao et al., 2021), unless otherwise specified. We show the overall templates and dataset statistics in Appendix C and D."
        },
        {
            "heading": "4.2 Main Results",
            "text": "We show the results in Table 2. We observe that our method significantly outperforms baselines on all datasets with or without calibration mechanism, which shows our method\u2019s best overall ability to find task-representative support examples across different settings and task families. Specially, our method shows better performance than the Random-Validation baseline and this directly demonstrates its non-triviality. Meanwhile, previous methods for gradient-based learning have similar performance with the Random baseline, and this\nindicates: 1. there is a non-negligible gap between ICL and these methods 2. it is necessary to design to ICL-specific method to find support examples.\nAdditionally, the Random baseline slightly underperforms the Zero-Shot, which shows that random examples are hard to fully characterize the task and the necessity of finding support examples for ICL. In experiments, we observe that RandomValidation suffers from the ICL\u2019s instability. Specifically, we find that considerable example permutations selected by the validation set do not consistently yield satisfactory results on the test set and this degrades its performance, whereas our method is more robust and less susceptible to this issue."
        },
        {
            "heading": "4.3 Analysis The Sensitivity of Support Examples to Orders",
            "text": "The recent study (Lu et al., 2022) shows that the\nordering of in-context examples for ICL has a significant influence on the performance. Specifically, to the same set of randomly sampled examples, different orders can result in near state-of-the-art and random-guess performance. In this section, we explore the effect of ordering for our support examples on SST-2, Amazon, MR and Subj. For each task, we select four sets of support examples and four sets of random examples and then evaluate their performance with eight randomly sampled orders. We show the performance distribution in Figure 2. We see that random examples with different orders show highly unstable performance where the worst drops to the random-guess level, which is consistent with the conclusion in previous work (Lu et al., 2022). In contrast, the support examples\u2019 performance is significantly more stable than random examples. Generally, most orders can still lead to approximately equivalent performance as the searched orders and few orders lead to the random-guess performance. The phenomenon is compatible with the conclusion from the recent work (Chen et al., 2022), which shows a strong negative correlation between ICL sensitivity and accuracy. Moreover, our support examples\u2019 lower sensitivity to the ordering demonstrates that they can more effectively depict and characterize the corresponding task.\nTransferablity across Different LMs In the main experiments, we get GPT2-L\u2019s support examples and evaluate them using the same LM. And\nhere we explore the transferability of these support examples across different LMs with various sizes and pre-training corpora. Specifically, we test the support examples of main experiments on GPT2-M (355M), GPT2-XL (1.5B) and GPT-Neo2.7B (Black et al., 2021). The results are shown in Table 3. We see that GPT2-L\u2019s support examples also show better performance than the Random baseline. Additionally, our support examples also demonstrate the consistent superiority on GPT-Neo2.7B which has a different pre-training corpus from GPT2-L. Since random examples\u2019 performance can not be well transferred to other LMs (Lu et al., 2022), these can show the strong transferability of our support examples and the utility of our method when more powerful LMs are proposed.\nGround Truth Matters for Support Examples Recently, Min et al. (2022b) suggests that ground truth (GT) labels are not important for ICL, which differs from traditional supervised learning. In their experiments, for randomly sampled examples, using ground truth labels or not leads to similar ICL performance. Here we explore the effect of ground truth labels for support examples. We show the\nperformance of support examples and random examples with GT or random labels in Figure 3. We see that the results on random examples are consistent with that in the previous paper (Min et al., 2022b). However, we observe a significantly different trend in support examples. Specifically, after removing GT labels, support examples\u2019 performance gets a strong degradation. We suppose it is because while random examples can not characterize the task well and thus their GT labels are not important, the GT labels of support examples contain crucial task information and input-output correspondence, so their GT labels are important for ICL\u2019s performance. Meanwhile, we find that under random labels, support examples also yield noticeable improvements over the random examples, which indicates that the inputs of support examples are also more informative for the task. The Impact of Hyper-parameters In this section, we evaluate the effect of each hyper-parameter. Specifically, we evaluate the effect of progressive ratio p, the number of stage 1\u2019s retained examples m, the weight of diversity \u03bb and the beam size B by separately tuning them and observing performance. Table 4 shows the results. When B is set to 0, i.e., we remove stage 2 and just select those examples with the highest InfoScore, the performance gets significantly degraded, and this directly demonstrates the effectiveness and necessity of stage 2. Except when B = 0, our method leads to consistent performance improvements compared with the Random baseline in general, across various hyper-parameter configurations, which indicates our method\u2019s robustness to hyper-parameters. Meanwhile, we observe two slight performance degradations when m = 100 or B = 1. For\nthe case that m = 100, we suppose that is because there are too few examples being retained after stage 1, limiting candidate examples\u2019 diversity. When B = 1, our stage 2 degrades to greedy search guided by the diversity, causing it susceptible to the local optimum issue.\nInfoScore and Progressive Filtering In this section, we evaluate the effect of InfoScore and progressive filtering in stage 1. Specifically, we randomly sample examples from the retained examples of stage 1 and test their average performance across 4 prompt templates with 10 random orders (40 in total). We compare the Random baseline, our filtering method and another filtering variant that filters uninformative examples, which retains those examples with low InfoScore at each iteration. We show the results in Table 5. We observe that just the proposed filtering method also leads to better ICL performance than randomly sampled examples, which directly shows that stage 1 is effective for filtering out the uninformative examples. Meanwhile, the performance points of Filtering (Informative), Random and Filtering (Uninformative) present a descending trend, which demonstrates that the proposed InfoScore can indicate the examples\u2019 in-context informativeness. However, compared with our entire method, Filtering (Informative) still shows a significant discrepancy. This indicates the necessity of considering in-context examples\u2019 dependency and the effectiveness of the proposed diversity-guided search."
        },
        {
            "heading": "5 Related Work",
            "text": "Since we introduce a wide range of coreset selection methods in Section 4.1, we omit them here and mainly introduce previous works about example selection for ICL. Previous works mainly consider example-level retrieval for ICL. Liu et al. (2022) leverage a semantic embedder to retrieve relevant examples for the given test input. Das et al. (2021) and Hu et al. (2022) use dense retrievers trained by task-specific targets\u2019 similarities to retrieve in-context examples for question answering and dialogue state tracking, respectively. Rubin et al. (2022); Shi et al. (2022) train the\ndemonstration retriever based on the feedback of the language model for semantic parsing. Wu et al. (2022) use Sentence-BERT (Reimers and Gurevych, 2019) to retrieve relevant examples and introduce an information-theoretic-driven criterion to rerank their permutations. Levy et al. (2022); Ye et al. (2023) further consider diversity in example retrieval. Different from these methods which aim to provide example-specific information for the test input, we focus on task-level example selection, which seeks to find examples that are representative for the task and is complementary to them. Moreover, because the large language models (Brown et al., 2020; Zhang et al., 2022a; Black et al., 2021) almost adopt purely causal Transformer (Vaswani et al., 2017) decoder architecture, we can calculate task-level in-context examples representation in advance and reuse them for different test inputs. Since these two settings\u2019 goals are orthogonal and complementary, we regard the hybrid setting and method as future work. Another line of methods is active learning (Ren et al., 2022) for ICL. It aims to select some examples from a large pool of unlabeled data and annotate them for ICL. Zhang et al. (2022b) propose to learn an active example selector by off-line reinforcement learning and use it to select examples to annotate for ICL. Su et al. (2022) propose a graph-based annotation method, vote-k, and use Sentence-BERT to retrieve relevant examples from the annotated examples for ICL. In this paper, we explore a different setting for ICL\u2019s example selection, where we select support examples from the annotated dataset since there are massive annotated datasets for various tasks and the prevailing large language model has shown impressive data annotation ability (Efrat and Levy, 2020; Gao et al., 2022; Ye et al., 2022; Meng et al., 2022; Cheng et al., 2023)."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a two-stage filter-thensearch method to find support examples for incontext learning from the annotated dataset: First we propose InfoScore to select informative examples individually with a progressive filtering process. Then we propose diversity-guided example search which iteratively refines and evaluates the selected examples, to find the example permutations that fully depict the task. The experimental results show that our method significantly outperforms extensive baselines, and further analyses show that each component contributes critically to\nthe improvements and shed light on the principles of support examples and in-context learning.\nLimitations\nThese are the limitations of this work:\n\u2022 Due to the computation resources limitation, we mainly conduct experiments on GPT2L (Radford et al., 2018) and analyze the crossLM transferability of support examples in section 4.3. We see the exploration on more LMs as future work.\n\u2022 In this paper, the proposed filter-then-search framework explores how to find support examples of in-context learning. We see exploring and analyzing more principles of in-context learning as future work.\n\u2022 Language models have exhibited various kinds of bias (Bender et al., 2021), since our filtering stage is based on its feedback, the filtered example might also exhibit these biases. We see language model debiasing as an important future research topic."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027)."
        },
        {
            "heading": "A Baselines",
            "text": "We mainly compare our proposed methods with following baselines: Random: We randomly select examples with random orderings from the training set; Random & Validation: We evaluate multiple sets of random examples on the validation set and select the best one. We consider Random & Validation under two settings whose computational cost is similar to our method: 1. the size of validation set is the same as ours at stage 2 (100) and the number of random example sets is the same as our searched and evaluated example permutations (640). 2. the size of validation set is larger, 1000, and the number of random example sets is 100. We also consider a wide range of Coreset Selection methods in traditional gradient-based learning scenarios, according to the methodologies, they can be divided into multiple categories including: Geometry-Based Method: it assumes that data points that are close in the feature space have similar properties. The Herding method (Chen et al., 2012) adds one data point each time into the coreset to greedily minimize the distance between coreset center and the original dataset center. The K-Center Greedy method (Sener and Savarese, 2018) selects examples to minimize the largest distance between each example in coreset and its closest example in set of examples that are not in coreset. Uncertainty-Based Method: it assumes examples with higher uncertainty can have a greater impact on the model and should be contained in coreset. Coleman et al. (2020) propose Least Confidence (the max probability over all labels), Entropy and Margin (max probability margin between different labels) to measure the examples\u2019 uncertainty scores and build coreset. CAL (Margatina et al., 2021) selects examples whose predictive likelihood exhibits the greatest divergence from their neighbors to build the coreset. Error/Loss Based Method: It assumes the example that contributes more to the error or loss during training is more important and should be added to coreset. Toneva et al. (2019) propose Forgetting to evaluate each example\u2019s importance by counting how many times it is forgotten, i.e., it is misclassified after begin correctly classified in previous training epochs. Paul et al. (2021) propose the GraNd score to select informative examples. GraNd is the gradient norm expectation of the example. The larger one example\u2019s GraNd is, the more important it is. Gradient Matching Based Method: Since deep models are usually trained by gradient descent, it tries to find a coreset whose gradients can imitate the entire dataset\u2019s gradients. Mirzasoleiman et al. (2020) propose CRAIG to convert the gradient matching problem to the maximization of a monotone submodular function and optimize it greedily. Killamsetty et al. (2021a) propose GradMatch based on CRAIG, which adds a regularization term to discourage assigning large weights to individual examples and improves the used greedy algorithm. Submodularity-Based Method: Submodular functions (Iyer and Bilmes, 2013) naturally measure the subset\u2019s informativeness and diversity and thus can be powerful for coreset selection. Iyer and Bilmes (2013) leverage Facility Location and Graph Cut as submodular functions to select the coreset. Bilevel Optimization Based Method: It transforms the coreset selection problem into a bilevel-optimization problem whose outer and inner objectives are subset selection and model parameter optimization, respectively. Glister (Killamsetty et al., 2021b) leverages a validation set on the outer optimization and the log-likelihood in the bilevel optimization. To reduce the gap between these methods and ICL, we use the same LM (GPT2-L) with \u201clast pooling\u201d fine-tuned on the whole dataset for 5 epochs to obtain relevant metrics, e.g., gradients or forgetting times for these methods. Following Guo et al. (2022), we use the gradients of the final fully-connected layer\u2019s parameters as these methods\u2019 example feature. B Implementation Details B.1 Baseline Details For those previous coreset selection methods, to reduce the gap between these methods and ICL, we use the same LM (GPT2-L) with \u201clast pooling\u201d fine-tuned on the whole dataset for 5 epochs to obtain relevant metrics, e.g., gradients or forgetting times for these methods. Following Guo et al. (2022), we use the gradients of the final fullyconnected layer\u2019s parameters as these methods\u2019 example feature. For baselines that output a weighted subset of examples, e.g., CRAIG or GradMatch, we just adopt its examples for simplicity since there are few methods to weighting in-context examples for ICL.\nB.2 Method Details We find each prompting template\u2019s corresponding support examples separately in our method and compared methods, i.e., we select examples for each prompting template separately. For simplicity, we calculate Eq (3) without calibration for experiments without or with calibration. For the filtering stage, we set the progressive factor and the size of initial score set according to the dataset\u2019s size. Specifically, we set the progressive factor to make the filter iterations be 4.\nWe run all experiments under the label balance setting and the total number of in-context examples for most datasets except DBPedia is set to 8. The number of some datasets\u2019 in-context examples is not 8 but close to 8 because 8 can not be divided by the number of its labels, e.g., 5 for SST-5. Since DBPedia has 14 labels and significantly longer input sequence, we run experiments on it under the label-unbalance setting and set the total number of examples to 4. In label balance setting, we 1. filter the same number of examples for each label, 2. initialize the example permutation of stage 2 with balanced labels 3. update e\u2217 with e\u2217new, whose label is the same as e\u2217.\nWe list the total number of examples in Table 6. And we set the size of initial score set to make the times of LM\u2019s forwards to be around 10K. We list the progressive factor p and the size of initial score set |S0|in Table 7. For other hyper-parameters, we conduct grid search for the number of retained examples of filtering m, the weight of diversity \u03bb, the beam size B and the iteration of diversityguided search over {500,1000}, {0.5,1,2}, {4,8,16} and {5,10,15} respectively on the SST-2 dataset. And we set them to be 500, 1, 8 and 10 respectively.\nB.3 Experimental Details In section \u201cThe Sensitivity of Support Examples to Orders\u201d, since the performance is sensitive to the prompting templates, we show the performance distribution under a specific prompting template. In other analysis experiments, for simplicity, we report the average performance under four different prompting templates without calibration, unless otherwise specified.\nB.3.1 The Complexity of Our Method Progressive Filtering in the filtering stage, we need to compute pairwise Eq 3 for N\u2217l\u2217\u03c1/\u03c1 = N\u2217 l times, where N is size of training set. Since we filter the dataset into 1/\u03c1 of its previous size until a\nsmall set of examples is left, the number of iteration is log\u03c1N). Thus the filtering stage\u2019s complexity over N is O(N \u2217 log\u03c1N). In experiments, we set \u03c1 to N 1 C to make it a linear complexity, where C is a constant. According to the size of dataset, \u03c1 is usually set between 2 - 3, shown in Table 7.\nDiversity-Guided Example Search At each iteration, we have B candidate permutations and separately update them B times. And then we evaluate these updated candidate permutations on the small validation set sampled from the remaining training set, whose size is fixed. Since updating the candidate permutations reuses the intermediate results of the filtering stage and does not involve the computation of the LLM (see Eq (4) and (5)), we omit it for complexity analysis. So the complexity of diversity-guided example search is consistant, B \u2217 B."
        },
        {
            "heading": "C Prompting Templates",
            "text": "We show the prompting verbalizers and templates in Table 8."
        },
        {
            "heading": "D Dataset Split and Statistics",
            "text": "We use the same dataset split in the previous work (Min et al., 2022a). Due to computational resource limitations, for Amazon, AGNews and DBPedia, we conduct experiments on a randomly sampled subset of it (30000 and 2000 for the training and test set), and we show the overall dataset statistics in Table 6.\nTask Family: Sentiment Classification\nTask: SST-2 Prompting Verbalizer: {great, terrible} Prompting Templates:\n\u2022 \u201c[INPUT] A [VERBALIZER] one. \u201d\n\u2022 \u201c[INPUT] It was [VERBALIZER]. \u201d\n\u2022 \u201c[INPUT] All in all [VERBALIZER]. \u201d\n\u2022 \u201c[INPUT] A [VERBALIZER] piece. \u201d\nExample: Input: I have to admit that I am baffled by jason x. It was terrible. If you answered yes, by all means enjoy the new guy. It was great. \u00b7 \u00b7 \u00b7 Never comes together as a coherent whole. It was Output: terrible.\nTask: SST-5 Prompting Verbalizer: {great, good, okay, bad, terrible} Prompting Templates: Same as SST-2\nTask: Amazon Prompting Verbalizer: {great, good, okay, bad, terrible} Prompting Templates: Same as SST-2\nTask: MR Prompting Verbalizer: {great, terrible} Prompting Templates: Same as SST-2\nTask Family: Topic Classification\nTask: TREC Prompting Verbalizer: {Description, Entity, Expression, Human, Location, Number} Prompting Templates:\n\u2022 \u201c[INPUT] Topic: [VERBALIZER]. \u201d\n\u2022 \u201c[INPUT] Subject: [VERBALIZER]. \u201d\n\u2022 \u201c[INPUT] This is about [VERBALIZER]. \u201d\n\u2022 \u201c[INPUT] It is about [VERBALIZER] piece. \u201d\nExample: Input: How do storms form ? Topic: Description. What city in Florida is Sea World in? Topic: Location. \u00b7 \u00b7 \u00b7 What university fired Angela Davis? Topic: Output: Human.\nTask: AGNews"
        }
    ],
    "title": "Finding Support Examples for In-Context Learning",
    "year": 2023
}