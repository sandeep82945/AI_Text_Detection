{
    "abstractText": "Simultaneous machine translation (SiMT) models are trained to strike a balance between latency and translation quality. However, training these models to achieve high quality while maintaining low latency often leads to a tendency for aggressive anticipation. We argue that such issue stems from the autoregressive architecture upon which most existing SiMT models are built. To address those issues, we propose non-autoregressive streaming Transformer (NAST) which comprises a unidirectional encoder and a non-autoregressive decoder with intra-chunk parallelism. We enable NAST to generate the blank token or repetitive tokens to adjust its READ/WRITE strategy flexibly, and train it to maximize the nonmonotonic latent alignment with an alignmentbased latency loss. Experiments on various SiMT benchmarks demonstrate that NAST outperforms previous strong autoregressive SiMT baselines. Source code is publicly available at https://github.com/ictnlp/NAST.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhengrui Ma"
        },
        {
            "affiliations": [],
            "name": "Shaolei Zhang"
        },
        {
            "affiliations": [],
            "name": "Shoutao Guo"
        },
        {
            "affiliations": [],
            "name": "Chenze Shao"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        },
        {
            "affiliations": [],
            "name": "Yang Feng"
        }
    ],
    "id": "SP:40220738fc5d71083194a65ee65e9bdc10d4b045",
    "references": [
        {
            "authors": [
                "Naveen Arivazhagan",
                "Colin Cherry",
                "Wolfgang Macherey",
                "Chung-Cheng Chiu",
                "Semih Yavuz",
                "Ruoming Pang",
                "Wei Li",
                "Colin Raffel."
            ],
            "title": "Monotonic infinite lookback attention for simultaneous machine translation",
            "venue": "Proceedings of the 57th Annual",
            "year": 2019
        },
        {
            "authors": [
                "Chih-Chiang Chang",
                "Shun-Po Chuang",
                "Hung-yi Lee."
            ],
            "title": "Anticipation-free training for simultaneous machine translation",
            "venue": "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 43\u201361, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Junkun Chen",
                "Renjie Zheng",
                "Atsuhito Kita",
                "Mingbo Ma",
                "Liang Huang."
            ],
            "title": "Improving simultaneous translation by incorporating pseudo-references with fewer reorderings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Masha Esipova"
            ],
            "title": "Can neural machine translation do simultaneous translation? CoRR, abs/1606.02012",
            "year": 2016
        },
        {
            "authors": [
                "Cunxiao Du",
                "Zhaopeng Tu",
                "Jing Jiang."
            ],
            "title": "Orderagnostic cross entropy for non-autoregressive machine translation",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Maha Elbayad",
                "Laurent Besacier",
                "Jakob Verbeek"
            ],
            "title": "Efficient wait-k models for simultaneous machine translation",
            "year": 2020
        },
        {
            "authors": [
                "Qingkai Fang",
                "Yan Zhou",
                "Yang Feng."
            ],
            "title": "Daspeech: Directed acyclic transformer for fast and high-quality speech-to-speech translation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Vladimir Karpukhin",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "Aligned cross entropy for non-autoregressive machine translation",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino Gomez",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Proceedings of the 23rd international conference on Machine learning,",
            "year": 2006
        },
        {
            "authors": [
                "Jiatao Gu",
                "James Bradbury",
                "Caiming Xiong",
                "Victor O.K. Li",
                "Richard Socher."
            ],
            "title": "Non-autoregressive neural machine translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Jiatao Gu",
                "Graham Neubig",
                "Kyunghyun Cho",
                "Victor O.K. Li."
            ],
            "title": "Learning to translate in real-time with neural machine translation",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume",
            "year": 2017
        },
        {
            "authors": [
                "Shangtong Gui",
                "Chenze Shao",
                "Zhengrui Ma",
                "Xishan Zhang",
                "Yunji Chen",
                "Yang Feng."
            ],
            "title": "Nonautoregressive machine translation with probabilistic context-free grammar",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2023
        },
        {
            "authors": [
                "He He",
                "Alvin Grissom II",
                "John Morgan",
                "Jordan BoydGraber",
                "Hal Daum\u00e9 III."
            ],
            "title": "Syntax-based rewriting for simultaneous machine translation",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Fei Huang",
                "Hao Zhou",
                "Yang Liu",
                "Hang Li",
                "Minlie Huang."
            ],
            "title": "Directed acyclic transformer for nonautoregressive machine translation",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, ICML 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Yoon Kim",
                "Alexander M Rush."
            ],
            "title": "Sequencelevel knowledge distillation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327.",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Jind\u0159ich Libovick\u00fd",
                "Jind\u0159ich Helcl."
            ],
            "title": "End-toend non-autoregressive neural machine translation with connectionist temporal classification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3016\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Dan Liu",
                "Mengge Du",
                "Xiaoxi Li",
                "Ya Li",
                "Enhong Chen."
            ],
            "title": "Cross attention augmented transducer networks for simultaneous translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 39\u201355, Online",
            "year": 2021
        },
        {
            "authors": [
                "Mingbo Ma",
                "Liang Huang",
                "Hao Xiong",
                "Renjie Zheng",
                "Kaibo Liu",
                "Baigong Zheng",
                "Chuanqiang Zhang",
                "Zhongjun He",
                "Hairong Liu",
                "Xing Li",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "STACL: Simultaneous translation with implicit anticipation and controllable la",
            "year": 2019
        },
        {
            "authors": [
                "Xutai Ma",
                "Juan Miguel Pino",
                "James Cross",
                "Liezl Puzon",
                "Jiatao Gu."
            ],
            "title": "Monotonic multihead attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Zhengrui Ma",
                "Chenze Shao",
                "Shangtong Gui",
                "Min Zhang",
                "Yang Feng."
            ],
            "title": "Fuzzy alignments in directed acyclic graph for non-autoregressive machine translation",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Gonzalo Mena",
                "David Belanger",
                "Scott Linderman",
                "Jasper Snoek."
            ],
            "title": "Learning latent permutations with gumbel-sinkhorn networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Yishu Miao",
                "Phil Blunsom",
                "Lucia Specia."
            ],
            "title": "A generative framework for simultaneous machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6697\u20136706, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Lihua Qian",
                "Hao Zhou",
                "Yu Bao",
                "Mingxuan Wang",
                "Lin Qiu",
                "Weinan Zhang",
                "Yong Yu",
                "Lei Li."
            ],
            "title": "Glancing transformer for non-autoregressive neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Minh-Thang Luong",
                "Peter J. Liu",
                "Ron J. Weiss",
                "Douglas Eck."
            ],
            "title": "Online and lineartime attention by enforcing monotonic alignments",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings",
            "year": 2017
        },
        {
            "authors": [
                "Yi Ren",
                "Chenxu Hu",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu."
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725,",
            "year": 2016
        },
        {
            "authors": [
                "Chenze Shao",
                "Yang Feng."
            ],
            "title": "Non-monotonic latent alignments for ctc-based non-autoregressive machine translation",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 8159\u20138173. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Chenze Shao",
                "Yang Feng",
                "Jinchao Zhang",
                "Fandong Meng",
                "Jie Zhou."
            ],
            "title": "Sequence-Level Training for Non-Autoregressive Neural Machine Translation",
            "venue": "Computational Linguistics, 47(4):891\u2013925.",
            "year": 2021
        },
        {
            "authors": [
                "Chenze Shao",
                "Zhengrui Ma",
                "Yang Feng."
            ],
            "title": "Viterbi decoding of directed acyclic transformer for non-autoregressive machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4390\u20134397, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Chenze Shao",
                "Jinchao Zhang",
                "Yang Feng",
                "Fandong Meng",
                "Jie Zhou."
            ],
            "title": "Minimizing the bagof-ngrams difference for non-autoregressive neural machine translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):198\u2013205.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Chen Xu",
                "Xiaoqian Liu",
                "Xiaowen Liu",
                "Qingxuan Sun",
                "Yuhao Zhang",
                "Murun Yang",
                "Qianqian Dong",
                "Tom Ko",
                "Mingxuan Wang",
                "Tong Xiao",
                "Anxiang Ma",
                "Jingbo Zhu."
            ],
            "title": "CTC-based non-autoregressive speech translation",
            "venue": "Proceedings of the 61st An-",
            "year": 2023
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng"
            ],
            "title": "Universal simultaneous machine translation with mixture-of-experts",
            "year": 2021
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Hidden markov transformer for simultaneous machine translation",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Baigong Zheng",
                "Kaibo Liu",
                "Renjie Zheng",
                "Mingbo Ma",
                "Hairong Liu",
                "Liang Huang."
            ],
            "title": "Simultaneous translation policies: From fixed to adaptive",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2847\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Lagging (AL",
                "Ma"
            ],
            "title": "2019), we also incorporate Consecutive Wait (CW; Gu et al., 2017), Average Proportion (AP; Cho and Esipova, 2016), and Differentiable Average Lagging (DAL",
            "venue": "Arivazhagan et al.,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Simultaneous machine translation (SiMT; Cho and Esipova, 2016; Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Zhang and Feng, 2023), also known as real-time machine translation, is commonly used in various practical scenarios such as live broadcasting, video subtitles and international conferences. SiMT models are required to start translation when the source sentence is incomplete, ensuring that listeners stay synchronized with the speaker. Nevertheless, translating partial source content poses significant challenges and increases the risk of translation errors. To this end, SiMT models are trained to strike a balance between latency and translation quality by dynamically determining when to generate tokens (i.e., WRITE action) and when to wait for additional source information (i.e., READ action).\n\u2217Corresponding author: Yang Feng\nHowever, achieving the balance between latency and translation quality is non-trivial for SiMT models. Training these models to produce high-quality translations while maintaining low latency often leads to a tendency for aggressive anticipation (Ma et al., 2019), as the models are compelled to output target tokens even before the corresponding source tokens have been observed during the training stage (Zheng et al., 2020). We argue that such an issue of anticipation stems from the autoregressive (AR) model architecture upon which most existing SiMT models are built. Regardless of the specific READ/WRITE strategy utilized, AR SiMT models are typically trained using maximum likelihood estimation (MLE) via teacher forcing. As depicted in Figure 1, their training procedure can have adverse effects on AR SiMT models in two aspects: 1) non-monotonicity problem: The reference used in training might be non-monotonically aligned with the source. However, in real-time scenarios, SiMT models are expected to generate translations that align monotonically with the source to reduce latency (He et al., 2015; Chen et al., 2021). The inherent verbatim alignment assumption during the MLE training of AR SiMT models restricts their performance; 2) source-info leakage bias: Following the practice in full-sentence translation systems, AR SiMT models deploy the teacher forcing strategy during training. However, it may inadvertently result in the leakage of source information. As illustrated in Figure 1, even if the available source content does not contain the word \"\u4e3e\u884c (hold)\", the AR decoder is still fed with the corresponding translation word \"held\" as the ground truth context in training. This discrepancy between training and inference encourages the AR SiMT model to make excessively optimistic predictions during the realtime inference, leading to the issue of hallucination (Chen et al., 2021).\nTo address the aforementioned problems in autoregressive SiMT models, we focus on developing\nSiMT models that generate target tokens in a nonautoregressive (NAR) manner (Gu et al., 2018) by removing the target-side token dependency. We argue that an NAR decoder is better suited for streaming translation tasks. Firstly, the target tokens are modeled independently in NAR models, which facilitates the development of a non-monotonic alignment algorithm between generation and reference, alleviating the non-monotonicity problem. Additionally, the conditional independence assumption of the NAR structure liberates the model from the need for teacher forcing in training, thereby eliminating the risk of source-side information leakage. These advantageous properties of the NAR structure enable SiMT models to avoid aggressive anticipation and encourage the generation of monotonic translations with fewer reorderings that align with the output of professional human interpreters.\nIn this work, we propose non-autoregressive streaming Transformer (NAST). NAST processes streaming input and performs unidirectional encoding. Translations are generated in a chunk-bychunk manner, with tokens within each chunk being generated in parallel. We enable NAST to generate blank token \u03f5 or repetitive tokens to build READ/WRITE paths adaptively, and train it to maximize non-monotonic latent alignment (Graves et al., 2006; Shao and Feng, 2022) with a further developed alignment-based latency loss. In this way, NAST effectively learns to generate translations that are properly aligned with the source in a monotonic manner, achieving high-quality translation while maintaining low latency.\nExtensive experiments on WMT15 German \u2192 English and WMT16 English \u2192 Romanian bench-\nmarks demonstrate that NAST outperforms previous strong autoregressive SiMT baselines."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Simultaneous Translation",
            "text": "Simultaneous machine translation models often adopt a prefix-to-prefix framework to start generating translation conditioned on partial source input. Given a source sentence x = {x1, ..., xm}, previous autoregressive SiMT models factorize the probability of target sentence y = {y1, ..., yn} as:\npg(y|x) = |y|\u220f t=1 p(yt|x\u2264g(t), y<t), (1)\nwhere g(t) is a monotonic non-decreasing function of t, denoting the number of observed source tokens when generating yt. A function g(t) represents a specific READ/WRITE policy of SiMT models.\nIn addition to translation quality, latency is a crucial factor in the assessment of SiMT models. The latency of a policy g(t) is commonly measured using Average Lagging (AL; Ma et al., 2019), which counts the number of tokens that the output lags behind the input:\nAL(g;x) = 1\n\u03c4g(|x|) \u03c4g(|x|)\u2211 t=1 (g(t)\u2212 t\u2212 1 r ), (2)\nwhere \u03c4g(|x|) is the cut-off function to exclude the counting of problematic tokens at the end:\n\u03c4g(|x|) = min{t|g(t) = |x|}, (3)\nand r = |y||x| represents the length ratio between the target and source sequences."
        },
        {
            "heading": "2.2 Non-autoregressive Generation",
            "text": ""
        },
        {
            "heading": "2.2.1 Parallel Decoding",
            "text": "Non-autoregressive generation (Gu et al., 2018) is originally introduced to reduce decoding latency1. It removes the autoregressive dependency and generates target tokens in a parallel way. Given a source sentence x = {x1, ..., xm}, NAR models factorize the probability of target sentence y = {y1, ..., yn} as:\np(y|x) = |y|\u220f t=1 p(yt|x). (4)"
        },
        {
            "heading": "2.2.2 Connectionist Temporal Classification",
            "text": "Unlike autoregressive models that dynamically control the length by generating the <eos> token, NAR models often utilize a length predictor to predetermine the length of the output sequence before generation. The predicted length may be imprecise and lacks adaptability for adjustment. Connectionist Temporal Classification (CTC; Graves et al., 2006) addresses this limitation by extending the output space Y with a blank token \u03f5. The generation a \u2208 Y\u2217 is referred to as the alignment. CTC defines a mapping function \u03b2(y;T ) that returns a set of all possible alignments of y of length T and a collapsing function \u03b2\u22121(a) that first collapses all consecutive repeated tokens in a and then removes all blanks to obtain the target. During training, CTC marginalizes out all alignments:\np(y|x) = \u2211\na\u2208\u03b2(y;T )\np(a|x), (5)\nwhere T is a pre-determined length and the alignment is modeled in a non-autoregressive way:\np(a|x) = T\u220f\nt=1\np(at|x). (6)"
        },
        {
            "heading": "3 Approach",
            "text": "We provide a detailed introduction to the nonautoregressive streaming Transformer (NAST) in this section."
        },
        {
            "heading": "3.1 Architecture Overview",
            "text": "NAST consists of a unidirectional encoder (Arivazhagan et al., 2019; Ma et al., 2019; Miao et al.,\n1Note that the concept of latency differs between NAR generation and SiMT. It refers to the delay in generating all target tokens once all source tokens are observed in the first case and to the level of synchronization between target-side generation and source-side observation in the latter case.\n2021) and a non-autoregressive decoder with intrachunk parallelism. The model architecture is depicted in Figure 2. When a source token xi is read in, NAST passes it to the unidirectional encoder, allowing it to attend to the previous source contexts through causal encoder self-attention:\nSelfAttn(xi,x\u2264i). (7)\nConcurrently, NAST upsamples xi \u03bb times and feeds them to construct the decoder hidden states as a chunk. Within the chunk, NAST handles \u03bb states in a fully parallel manner. To further clarify, we introduce h to represent the sequence of decoder states. Thus, the j-th hidden state in the i-th chunk can be denoted as h(i\u22121)\u03bb+j , subject to 1 \u2264 i \u2264 |x| and 1 \u2264 j \u2264 \u03bb. Those states can attend to information from all currently observed source contexts through cross-attention:\nCrossAttn(h(i\u22121)\u03bb+j ,x\u2264i), (8)\nand to information from all constructed decoder states through self-attention:\nSelfAttn(h(i\u22121)\u03bb+j ,h\u2264i\u03bb). (9)\nFollowing CTC (Graves et al., 2006), we extend the vocabulary to allow NAST generating the blank token \u03f5 or repeated tokens from decoder states to model an implicit READ action. We refer to the outputs from a states chunk h(i\u22121)\u03bb+1:i\u03bb as partial alignments a(i\u22121)\u03bb+1:i\u03bb, where NAST generates them in a non-autoregressive way:\np(a(i\u22121)\u03bb+1:i\u03bb|h(i\u22121)\u03bb+1:i\u03bb)\n= \u03bb\u220f j=1 p(a(i\u22121)\u03bb+j |h(i\u22121)\u03bb+j). (10)\nTo obtain the translation stream, we first apply the collapsing function \u03b2\u22121 to deal with the partial alignments generated from the i-th chunk:\nychunki = \u03b2\u22121(a(i\u22121)\u03bb+1:i\u03bb). (11)\nThen NAST concatenates the outputs from the current chunk to generated prefix ypre according to the following rule:{\nypre = ypre \u2295 ychunki2: , if y pre \u22121 = y chunki 1 ypre = ypre \u2295 ychunki , otherwise (12)\nwhere ypre\u22121 denotes the last token in the generated prefix. Consequently, upon receiving a token in the input stream, NAST is capable to generate 0 to \u03bb\ntokens at a time, endowing it with the ability to adjust its READ/WRITE strategy flexibly. Formally, each full alignment a \u2208 \u03b2(y;\u03bb|x|) can be considered as a concatenation of all the partial alignments, and implies a specific READ/WRITE policy to generate the reference y. Therefore, NAST jointly models the distribution of translation and READ/WRITE policy by marginalizing out latent alignments:\np(y|x) = \u2211\na\u2208\u03b2(y;\u03bb|x|)\np(a|x)\n= \u2211\na\u2208\u03b2(y;\u03bb|x|) \u220f 1\u2264i\u2264|x| 1\u2264j\u2264\u03bb p(a(i\u22121)\u03bb+j |x\u2264i). (13)"
        },
        {
            "heading": "3.2 Latency Control",
            "text": "While NAST exhibits the ability to adaptively determine an appropriate READ/WRITE policy, we want to impose some specific requirements on the trade-off between latency and translation quality. To accomplish this, we introduce an alignmentbased latency loss and a chunk wait-k strategy to effectively control the latency of NAST."
        },
        {
            "heading": "3.2.1 Alignment-based Latency Loss",
            "text": "Considering NAST models the distribution of READ/WRITE policy by capturing the distribution of latent alignments, it is desirable to measure the averaged latency of all latent alignments and further regularize it. Specifically, we are interested in the expected Average Lagging (AL; Ma et al., 2019) of NAST:\nAL(\u03b8;x) = Ea\u223cp\u03b8(a|x)[AL(g a;x)], (14)\nwhere ga is the policy induced from alignment a. Due to the exponentially large alignment space, it\nis infeasible to enumerate all possible ga to obtain AL(\u03b8;x). This limitation motivates us to delve deeper into AL(\u03b8;x) and devise an efficient estimation algorithm.\nTo simplify the estimation process of AL(\u03b8;x) while still excluding the lag counting of problematic words generated after all source read in, we deploy a new cut-off function that disregards tokens generated after all source observed, i.e., tokens from the last chunk:\n\u03c4ga(|x|) = min{t|ga(t) = |x|} \u2212 1. (15)\nThen we introduce a moment function m(i) to denote the number of observed source tokens when generating the i-th position in the alignment. Given the fixed upsampling strategy of NAST, it is clear that:\nm((i\u2212 1)\u03bb+ j) = i, 1 \u2264 j \u2264 \u03bb. (16)\nWe further define an indicator function 1(ai) to denote whether the i-th position in the alignment is reserved after collapsed by \u03b2\u22121. With its help, it is convenient to express the lagging of alignment a:\nAL(ga;x)\n= 1\n\u03c4ga(|x|) ( \u03c4ga (|x|)\u2211 t=1 g(t)\u2212 \u03c4ga (|x|)\u2211 t=1 t\u2212 1 r )\n= 1\n\u03c4ga(|x|) ( (|x|\u22121)\u03bb\u2211 i=1 m(i)1(ai)\u2212 \u03c4ga(|x|)(\u03c4ga(|x|)\u2212 1) 2r )\n\u2248 1 \u03c4ga(|x|) ( (|x|\u22121)\u03bb\u2211 i=1 m(i)1(ai)\u2212 |x|(\u03c4ga(|x|)\u2212 1) 2 ).\n(17)\nEquation 17 inspires us to estimate the expected average lagging AL(\u03b8;x) by separately calculating the expected values of the numerator and denomi-\nnator:\nAL(\u03b8;x)\n\u2248 Ea[\n\u2211(|x|\u22121)\u03bb i=1 m(i)1(ai)]\u2212 |x| 2 (Ea[\u03c4ga(|x|)]\u2212 1)\nEa[\u03c4ga(|x|)] .\n(18)\nIt relieves us from the intractable task of enumerating ga. Instead, we only need to handle two expectation terms: Ea[ \u2211(|x|\u22121)\u03bb i=1 m(i)1(ai)] and Ea[\u03c4ga(|x|)], which can be resolved efficiently:2{ Ea[\u03c4ga(|x|)] = \u2211(|x|\u22121)\u03bb i=1 p(1(ai))\nEa[ \u2211(|x|\u22121)\u03bb i=1 m(i)1(ai)] = \u2211(|x|\u22121)\u03bb i=1 m(i)p(1(ai))\n(19)\nwhere p(1(ai)) represents the probability that the i-th token in the alignment is reserved after collapsing and can be calculated simply as: p(1(ai)) = 1\u2212 p(ai = \u03f5)\u2212 \u2211\nv\u2208Y/\u03f5\np(ai = v)p(ai\u22121 = v).\n(20)\nWith the assistance of the aforementioned derivation, it is efficient to estimate the expected average lagging of NAST. By applying it along with a tunable minimum lagging threshold lmin, we can train NAST to meet specific requirements of low latency:\nLlatency = max(AL(\u03b8;x), lmin). (21)\n3.2.2 Chunk Wait-k Strategy In addition to the desiring property of shorter lagging, there may be practical scenarios where we aim to mitigate the risk of erroneous translations by increasing the latency. To this end, we propose a chunk wait-k strategy for NAST to satisfy the requirements of better translation quality.\n2We leave the detailed derivation of Equation 19 in Appendix A.\nNAST is allowed to wait for additional k source tokens before initializing the generation of the first chunk. The first chunk is fed to the decoder at the moment the (k + 1)-th source token is read in. Subsequently, NAST feeds each following chunk as each new source token is received. The partial alignment generated from each chunk is consistently lagged by k tokens compared with the corresponding source token until the source sentence is complete.\nFormally, the moment function for the chunk wait-k strategy can be formulated as:\nm((i\u2212 1)\u03bb+ j) = min{i+ k, |x|}, 1 \u2264 j \u2264 \u03bb. (22)\nAs depicted in Figure 3, decoder states can further access information from additional k observed source tokens through cross-attention:\nCrossAttn(h(i\u22121)\u03bb+j ,x\u2264min{i+k,|x|}), (23)\nwhich leads NAST to prioritize better translation quality at the expense of longer delay."
        },
        {
            "heading": "3.3 Non-monotonic Latent Alignments",
            "text": "While CTC loss (Graves et al., 2006) provides the convenience of directly applying the maximum likelihood estimation to train NAST, i.e., L = \u2212 log p(y|x), it only considers the monotonic mapping from target positions to alignment positions. However, non-monotonic alignments are crucial in simultaneous translation. SiMT models are expected to generate translations that are monotonically aligned with the source sentence to achieve low latency. Unfortunately, in the training corpus, source and reference pairs are often non-monotonically aligned due to differences in grammar structures between languages (e.g., SVO vs SOV). Neglecting the non-monotonic mapping during training compels the model to predict tokens for which the corresponding source has not been read, resulting in over-anticipation. To address these issues, we apply the bigram-based nonmonotonic latent alignment loss (Shao and Feng, 2022) to train our NAST, which maximizes the F1 score of expected bigram matching between target and alignments:\nLNMLA(\u03b8) = \u2212 2 \u00b7 \u2211 g\u2208G2 min{Cg(y), Cg(\u03b8)}\u2211 g\u2208G2(Cg(y) + Cg(\u03b8)) , (24)\nwhere Cg(y) denotes the occurrence count of bigram g = (g1, g2) in the target, Cg(\u03b8) represents the expected count of g for NAST, and G2 denotes the set of all bigrams in y."
        },
        {
            "heading": "3.4 Glancing",
            "text": "Due to its inherent conditional independence structure, NAST may encounter challenges related to the multimodality problem3 (Gu et al., 2018). To address this issue, we employ the glancing strategy (Qian et al., 2021) during training. This involves randomly replacing tokens in the decoder\u2019s input chunk with tokens from the most probable latent alignment. Formally, the glancing alignment is the one that maximizes the posterior probability:\na\u2217 = argmax a\u2208\u03b2(y;\u03bb|x|) p(a|x). (25)\nThen we randomly sample some positions in the decoder input and replace tokens in the input sequence with tokens from the glancing alignment sequence at those positions in training."
        },
        {
            "heading": "3.5 Training Strategy",
            "text": "In order to better train the NAST model to adapt to simultaneous translation tasks with different latency requirements, we propose a two-stage training strategy. In the first stage, we train NAST using the CTC loss to obtain the reference monotonicaligned translation with adaptive latency:\nLstage\u22121 = LCTC = \u2212 log p(y|x). (26)\nIn the second stage, we train NAST using the combination of the non-monotonic latent alignment loss and the alignment-based latency loss:\nLstage\u22122 = LNMLA + Llatency. (27)\nThis further enables NAST to generate translations that are aligned with the source in a monotonic manner, meeting specific latency requirements."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets We conduct experiments on the following benchmarks that are widely used in previous SiMT studies: WMT154 German \u2192 English (De\u2192En, 4.5M pairs) and WMT165 English \u2192 Romanian (En\u2192Ro, 0.6M pairs). For De\u2192En, we use newstest2013 as the validation set and newstest2015 as the test set. For En\u2192Ro, we use newsdev-2016\n3The multimodality problem arises when a source sentence has multiple possible translations, which a non-autoregressive system is unable to capture due to its inability to model the target dependency.\n4https://www.statmt.org/wmt15/ 5https://www.statmt.org/wmt16/\nas the validation set and newstest-2016 as the test set. For each dataset, we apply BPE (Sennrich et al., 2016) with 32k merge operations to learn a joint subword vocabulary shared across source and target languages.\nImplementation Details We select a chunk upsample ratio of 3 (\u03bb = 3) and adjust the chunk waiting parameter k and the threshold lmin in alignment-based latency loss to achieve varying quality-latency trade-offs.6 For the first stage of training, we set the dropout rate to 0.3, weight decay to 0.01, and apply label smoothing with a value of 0.01. We train NAST for 300k updates on De\u2192En and 100k updates on En\u2192Ro. A batch size of 64k tokens is utilized, and the learning rate warms up to 5 \u00b7 10\u22124 within 10k steps. The glancing ratio linearly anneals from 0.5 to 0.3 within 200k steps on De\u2192En and 100k steps on En\u2192Ro. In the second stage, we apply the latency loss only if the chunk wait strategy is disabled (k = 0). The dropout rate is adjusted to 0.1 for De\u2192En, while no label smoothing is applied to either task. We further train NAST for 10k updates on De\u2192En and 6k updates on En\u2192Ro. A batch size of 256k tokens is utilized to stabilize the gradients, and the learning rate warms up to 3 \u00b7 10\u22124 within 500 steps. The glancing ratio is fixed at 0.3. During both training stages, all models are optimized using Adam (Kingma and Ba, 2014) with \u03b2 = (0.9, 0.98) and \u03f5 = 10\u22128. Following the practice in previous research on nonautoregressive generation, we employ sequencelevel knowledge distillation (Kim and Rush, 2016) to reduce the target-side dependency in data.7 We adopt Transformer-base (Vaswani et al., 2017) as the offline teacher model and train NAST on the distilled data.\nBaselines We compare our system with the following strong autoregressive SiMT baselines:\nOffline AT Transformer model (Vaswani et al., 2017), which initiates translation after reading all the source tokens. We utilize a unidirectional encoder and employ greedy search decoding for fair comparison.\n6Further details regarding the settings of k and lmin can be found in Appendix B.\n7Note that the purpose of offline knowledge distillation is to reduce the dependency between target-side tokens in the data, in order to facilitate the learning of non-autoregressive models. This is different from the goal of performing monotonic knowledge distillation in the field of SiMT, which aims to obtain monotonic aligned data.\nWait-k Wait-k policy (Ma et al., 2019), which initially reads k tokens and subsequently alternates between WRITE and READ actions.\nMoE Wait-k Mixture-of-experts wait-k policy (Zhang and Feng, 2021), which involves employing multiple experts to learn multiple wait-k policies during training. MoE Wait-k is the current SOTA fixed policy.\nMMA Monotonic multi-head attention (MMA; Ma et al., 2020) employs a Bernoulli variable to predict the READ/WRITE action and is trained using monotonic attention (Raffel et al., 2017).\nHMT Hidden Markov Transformer (HMT; Zhang and Feng, 2023), which treats the moments of starting translating as hidden events and considers the target sequence as the observed events. This approach organizes them as a hidden Markov model. HMT is the current SOTA adaptive policy.\nMetrics To compare SiMT models, we evaluate the translation quality using BLEU score (Papineni et al., 2002) and measure the latency using Average Lagging (AL; Ma et al., 2019). Numerical results with more latency metrics can be found in Appendix B."
        },
        {
            "heading": "4.2 Main Results",
            "text": "We compare NAST with the existing AR SiMT methods in Figure 4. On De\u2192En, NAST outperforms all AR SiMT models significantly across all latency settings, particularly in scenarios with very low latency. With the latency in the range of [0, 1], where listeners are almost synchronized with the speaker, NAST achieves a translation quality of 27.73 BLEU, surpassing the current SOTA model HMT by nearly 6 BLEU points. Moreover, NAST\ndemonstrates superior performance compared to the offline AT system even when the AL is as low as 6.85, showcasing its competitiveness in scenarios where higher translation quality is desired. On En\u2192Ro, NAST also exhibits a substantial improvement under low latency conditions. On the other hand, NAST achieves comparable performance to other models on En\u2192Ro when the latency requirement is not stringent."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Importance of Non-monotonic Alignments",
            "text": "NAST is trained using a non-monotonic alignment loss, enabling it to generate source monotonicaligned translations akin to human interpreters. This capability empowers NAST to achieve highquality streaming translations while maintaining low latency. To validate the effectiveness of nonmonotonic alignment, we conduct further experi-\nments by studying the performance of NAST without LNMLA. We compare the translation quality (BLEU) and latency (AL) of models employing different chunk wait-k strategies. The results are reported in Table 1 and Table 2. Note that Llatency is not applied here for clear comparison.\nWe observe that incorporating LNMLA significantly enhances translation quality by up to 1.86 BLEU, while maintaining nearly unchanged latency. We also notice that the improvement is particularly substantial when the latency is low, which aligns with our motivation. Under low latency conditions, SiMT models face more severe non-monotonicity problems. The ideal simultaneous generation requires more reordering of the reference to achieve source sentence monotonic alignment, which leads to greater improvements of applying non-monotonic alignment loss."
        },
        {
            "heading": "5.2 Analysis on Hallucination Rate",
            "text": "NAST mitigates the risk of source information leakage during training, thereby minimizing the occurrence of hallucination during inference. To demonstrate this, we compare the hallucination rate (Chen et al., 2021) of hypotheses generated by NAST with that of the current SOTA model, HMT (Zhang and Feng, 2023). A hallucination is defined as a generated token that can not be aligned to any source word. The results are plotted in Figure 5.\nWe note that the hallucination rates of both models decrease as the latency increases. However, NAST exhibits a significantly lower hallucination rate compared to HMT. We attribute this to the fact that NAST avoids the bias caused by source-info leakage and enables a more general generationreference alignment, thus mitigating compelled predictions during training."
        },
        {
            "heading": "5.3 Performance across Difficulty Levels",
            "text": "To further illustrate NAST\u2019s effectiveness in handling non-monotonicity problem, we investigate its performance when confronted with samples of varying difficulty levels. It is intuitive to expect that samples with a higher number of cross alignments between the source and reference texts pose a greater challenge for real-time translation. Therefore, we evenly partition the De\u2192En test set into subsets based on the number of crosses in the alignments, categorizing them as easy, medium, and hard, in accordance with the approach by Zhang and Feng (2021). We compare our NAST with previous HMT model, and the results are presented in Figure 6.\nDespite the impressive performance of NAST, a closer examination of Figure 6 reveals that the superiority is particular on the challenging subset. Even when real-time requirements are relatively relaxed, the improvement in handling the hard subset remains noteworthy. We attribute this to the stringent demand imposed by the hard subset, requiring SiMT models to effectively manage word reorderings to handle the non-monotonicity. NAST benefits from non-monotonic alignment training and excels in addressing these challenges, thus enhancing its performance in handling those harder samples."
        },
        {
            "heading": "5.4 Concerns on Fluency",
            "text": "While the non-autoregressive nature endows NAST with the capability to tackle the non-monotonicity problem and source-info leakage bias, it also exposes NAST to the risk of potential fluency degradation due to the absence of target-side dependency. To have a better understanding of this problem, we\nevaluate the fluency of the De\u2192En test set output from NAST in comparison to previous HMT. Specifically, we employ the Perplexity value reported by an external pre-trained language model transformer_lm.wmt19.en8 to measure the fluency of the generated texts. A lower Perplexity value implies more fluent translations. The results are presented in Figure 7.\nThough NAST exhibits significantly improved translation quality, we find its non-autoregressive nature does impact fluency to some extent. However, we consider this trade-off acceptable. In practical scenarios like international conferences where SiMT models are employed, the language used by human speakers is often not perfectly fluent. In such contexts, the audience tends to prioritize the overall translation quality under low latency, rather than the fluency of generated sentences."
        },
        {
            "heading": "6 Related Work",
            "text": "SiMT Simultaneous machine translation requires a READ/WRITE policy to balance latency and translation quality, involving fixed and adaptive strategies. For the fixed policy, Ma et al. (2019) proposed wait-k, which first reads k source tokens and then alternates between READ/WRITE action. Elbayad et al. (2020) introduced an efficient training method for the wait-k policy, which randomly samples k during training. Zhang and Feng (2021) proposed a mixture-of-experts wait-k to learn a set of wait-k policies through multiple experts. For the adaptive policy, Gu et al. (2017) trained an agent to decide READ/WRITE via reinforcement learning. Arivazhagan et al. (2019) introduced MILk, which incorporates a Bernoulli variable to indicate the\n8https://github.com/facebookresearch/fairseq/ tree/main/examples/language_model\nREAD/WRITE action. Ma et al. (2020) proposed MMA to implement MILk on Transformer. Liu et al. (2021) introduced CAAT, which leverages RNN-T and employs a blank token to signify the READ action. Miao et al. (2021) proposed GSiMT to generate the READ/WRITE actions. Chang et al. (2022) proposed to train a casual CTC encoder with Gumbel-Sinkhorn network (Mena et al., 2018) to reorder the states. Zhang and Feng (2023) proposed HMT to learn when to start translating in the form of HMM, achieving the current state-of-theart SiMT performance.\nNAR Generation Non-autoregressive models generate tokens parallel to the sacrifice of target-side dependency (Gu et al., 2018). This property eliminates the need for teacher forcing, motivating researchers to explore flexible training objectives that alleviate strict position-wise alignment imposed by the naive MLE loss. Libovick\u00fd and Helcl (2018) proposed latent alignment model with CTC loss (Graves et al., 2006), and Shao and Feng (2022) further explored non-monotonic latent alignments. Shao et al. (2020, 2021) introduced sequence-level training objectives with reinforcement learning and bag-of-ngrams difference. Ghazvininejad et al. (2020) trained NAT model using the best monotonic alignment and Du et al. (2021) further extended it to order-agnostic cross-entropy loss. In addition, some researchers are focusing on strengthening the expression power to capture the token dependency. Huang et al. (2022) proposed directed acyclic graph layer and Gui et al. (2023) introduced probabilistic context-free grammar layer. Building upon that, Shao et al. (2022) proposed Viterbi decoding and Ma et al. (2023) further explored fuzzy alignment training, achieving the current state-of-the-art NAR model performance. Apart from text translation, the NAR model also demonstrated impressive performance in diverse areas such as speech-to-text translation (Xu et al., 2023), speech-to-speech translation (Fang et al., 2023) and text-to-speech synthesis (Ren et al., 2021)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose non-autoregressive streaming Transformer (NAST) to address the nonmonotonicity problem and the source-info leakage bias in existing autoregressive SiMT models. Comprehensive experiments demonstrate its effectiveness.\nLimitations\nWe have observed that the performance of NAST is less satisfactory when translating from English to Romanian (En\u2192Ro) compared to translating from German to English (De\u2192En). This can be attributed to the fact that Romanian shares the SVO (Subject-Verb-Object) grammar with English, while German follows an SOV (Subject-ObjectVerb) word order. NAST excels in handling word reordering in translating from SOV to SVO, especially there is a strict requirement for low latency. But it is relatively less effective in SVO-toSVO translation scenarios where there is typically a monotonic alignment between the source and reference."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Derivation of Equation 19",
            "text": "We present the detailed derivation of Equation 19 in this section.\nEa[\u03c4ga(|x|)] = \u2211 a p(a|x) (|x|\u22121)\u03bb\u2211 i=1 1(ai)\n= (|x|\u22121)\u03bb\u2211 i=1 \u2211 a p(a|x)1(ai)\n= (|x|\u22121)\u03bb\u2211 i=1 p(1(ai)),\n(28)\nEa[ (|x|\u22121)\u03bb\u2211\ni=1 m(i)1(ai)] = \u2211 a p(a|x) (|x|\u22121)\u03bb\u2211 i=1 m(i)1(ai)\n= (|x|\u22121)\u03bb\u2211 i=1 m(i) \u2211 a p(a|x)1(ai)\n= (|x|\u22121)\u03bb\u2211 i=1 m(i)p(1(ai)),\n(29)\nwhere p(1(ai)) denotes the probability that the i-th token in the alignment is reserved after collapsing."
        },
        {
            "heading": "B Numerical Results with More Metrics",
            "text": "In addition to Average Lagging (AL; Ma et al., 2019), we also incorporate Consecutive Wait (CW; Gu et al., 2017), Average Proportion (AP; Cho and Esipova, 2016), and Differentiable Average Lagging (DAL; Arivazhagan et al., 2019) as metrics to evaluate the latency of NAST.\nWe adjust lmin in Llatency and k in chunk wait-k strategy to achieve varying quality-latency tradeoffs. For clarity, we present the numerical results of NAST using specific hyperparameter settings in Table 3 and Table 4. Note that Llatency is applied to achieve lower latency, while the chunk wait-k strategy is employed to improve translation quality. Therefore, we apply Llatency only when k = 0."
        },
        {
            "heading": "C Case Study",
            "text": "To gain further insights into NAST\u2019s behavior, we examine the generation processes of two different cases within the De\u2192En test set. We visualize the generation by plotting the generated partial alignments and the collapsed outputs at each step.\nIn Figure 8, we illustrate a case in which NAST reorders words at the phrase-level compared to the reference. With the streaming input \"die Premierminister Indiens und Japans\", NAST produces \"the prime ministers of India and Japan\" instead of\nthe reference \"India and Japan prime ministers\". This output represents a source-monotonic-aligned phrase, thereby effectively reducing latency.\nIn Figure 9, we depict another generation case where NAST manages word reorderings at the sentence level in comparison to the reference. In order to ensure low latency, NAST adjusts the sentence structure while maintaining meaning consistency with the reference. When NAST processes the source words \"es sieht so au\", it promptly generates \"it looks as if \" and continues generating the subsequent words within this grammatical structure. This ensures listeners keep synchronized with the speaker."
        }
    ],
    "title": "Non-autoregressive Streaming Transformer for Simultaneous Translation",
    "year": 2023
}