{
    "abstractText": "Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with \u223c 450 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset (\u223c 235GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.",
    "authors": [
        {
            "affiliations": [],
            "name": "El Moatez Billah Nagoudi\u03bb"
        },
        {
            "affiliations": [],
            "name": "Muhammad Abdul-Mageed\u03bb"
        },
        {
            "affiliations": [],
            "name": "AbdelRahim Elmadany\u03bb"
        },
        {
            "affiliations": [],
            "name": "Alcides Alcoba Inciarte\u03bb"
        },
        {
            "affiliations": [],
            "name": "Md Tawkat"
        },
        {
            "affiliations": [],
            "name": "Islam Khondaker\u03bb"
        }
    ],
    "id": "SP:bf7f3ceaa5fe229c6e43463a18a447eb3f4b89f0",
    "references": [
        {
            "authors": [
                "Mourad Abbas",
                "Kamel Sma\u00efli",
                "Daoud Berkani."
            ],
            "title": "Evaluation of topic identification methods on arabic corpora",
            "venue": "JDIM, 9(5):185\u2013192.",
            "year": 2011
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed."
            ],
            "title": "Subjectivity and sentiment analysis of Arabic as a morophologicallyrich language",
            "venue": "Ph.D. thesis, Indiana University.",
            "year": 2015
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi."
            ],
            "title": "ARBERT & MARBERT: Deep bidirectional transformers for Arabic",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "Houda Bouamor",
                "Nizar Habash."
            ],
            "title": "NADI 2020: The first nuanced Arabic dialect identification shared task",
            "venue": "Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 97\u2013110,",
            "year": 2020
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "AbdelRahim Elmadany",
                "Houda Bouamor",
                "Nizar Habash."
            ],
            "title": "NADI 2021: The second nuanced Arabic dialect identification shared task",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "AbdelRahim Elmadany",
                "Houda Bouamor",
                "Nizar Habash."
            ],
            "title": "Nadi 2022: The third nuanced arabic dialect identification shared task",
            "venue": "arXiv preprint arXiv:2210.09582.",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "AbdelRahim Elmadany",
                "Lyle Ungar."
            ],
            "title": "Toward microdialect identification in diaglossic and code-switched environments",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Ali Alshehri",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed."
            ],
            "title": "Understanding and detecting dangerous speech in social media",
            "venue": "The 4th Workshop on Open-Source Arabic Corpora and Processing Tools (OSACT4), LREC.",
            "year": 2020
        },
        {
            "authors": [
                "Yuvanesh Anand",
                "Zach Nussbaum",
                "Brandon Duderstadt",
                "Benjamin Schmidt",
                "Andriy Mulyar"
            ],
            "title": "Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all",
            "year": 2023
        },
        {
            "authors": [
                "Wissam Antoun",
                "Fady Baly",
                "Hazem Hajj."
            ],
            "title": "Arabert: Transformer-based model for arabic language understanding",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language",
            "year": 2020
        },
        {
            "authors": [
                "Wissam Antoun",
                "Fady Baly",
                "Hazem Hajj"
            ],
            "title": "Aragpt2: Pre-trained transformer for arabic language generation",
            "venue": "In Proceedings of the Sixth Arabic Natural Language Processing Workshop,",
            "year": 2021
        },
        {
            "authors": [
                "MS Badawi."
            ],
            "title": "Levels of contemporary arabic in egypt",
            "venue": "Cairo: D\u00e2r al Ma\u2019\u00e2rif.",
            "year": 1973
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability,",
            "year": 2021
        },
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow",
            "venue": "If you use this software, please cite it using these metadata, 58.",
            "year": 2021
        },
        {
            "authors": [
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang."
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "ArXiv, abs/2108.07258.",
            "year": 2021
        },
        {
            "authors": [
                "Tom B Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Daphne Ippolito",
                "Matthew Jagielski",
                "Katherine Lee",
                "Florian Tram\u00e8r",
                "Chiyuan Zhang."
            ],
            "title": "Quantifying memorization across neural language models",
            "venue": "ArXiv, abs/2202.07646.",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai"
            ],
            "title": "Bias in bios: A case study of semantic representation bias in a high-stakes",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Dodge",
                "Maarten Sap",
                "Ana Marasovi\u0107",
                "William Agnew",
                "Gabriel Ilharco",
                "Dirk Groeneveld",
                "Margaret Mitchell",
                "Matt Gardner."
            ],
            "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Ibrahim Abu El-Khair"
            ],
            "title": "2016. 1.5 billion words arabic corpus. arXiv preprint arXiv:1611.04033",
            "year": 2016
        },
        {
            "authors": [
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed"
            ],
            "title": "Orca: A challenging benchmark for arabic language understanding",
            "year": 2023
        },
        {
            "authors": [
                "Claire Cardie Faisal Ladhak",
                "Esin Durmus",
                "Kathleen McKeown."
            ],
            "title": "Wikilingua: A new benchmark dataset for multilingual abstractive summarization",
            "venue": "Findings of EMNLP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Xinyang Geng",
                "Arnav Gudibande",
                "Hao Liu",
                "Eric Wallace",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "Koala: A dialogue model for academic research",
            "venue": "Blog post.",
            "year": 2023
        },
        {
            "authors": [
                "abis",
                "Koray Kavukcuoglu",
                "Lisa Anne Hendricks",
                "Geoffrey Irving"
            ],
            "title": "Improving alignment of dialogue agents via targeted human",
            "venue": "judgements. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Nizar Y Habash."
            ],
            "title": "Introduction to Arabic natural language processing, volume 3",
            "venue": "Morgan & Claypool Publishers.",
            "year": 2010
        },
        {
            "authors": [
                "Braden Hancock",
                "Antoine Bordes",
                "Pierre-Emmanuel Mazare",
                "Jason Weston"
            ],
            "title": "Learning from dialogue after deployment: Feed yourself, chatbot",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "David M Howcroft",
                "Anja Belz",
                "Miruna-Adriana Clinciu",
                "Dimitra Gkatzia",
                "Sadid A Hasan",
                "Saad Mahamood",
                "Simon Mille",
                "Emiel Van Miltenburg",
                "Sashank Santhanam",
                "Verena Rieser"
            ],
            "title": "Twenty years of confusion in human evaluation: Nlg needs evaluation",
            "year": 2020
        },
        {
            "authors": [
                "Natasha Jaques",
                "Asma Ghandeharioun",
                "Judy Hanwen Shen",
                "Craig Ferguson",
                "\u00c0gata Lapedriza",
                "Noah J. Jones",
                "Shixiang Shane Gu",
                "Rosalind W. Picard."
            ],
            "title": "Way off-policy batch deep reinforcement learning of implicit human preferences in dialog",
            "venue": "ArXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Hannah Rose Kirk",
                "Yennie Jun",
                "Filippo Volpin",
                "Haider Iqbal",
                "Elias Benussi",
                "Frederic Dreyer",
                "Aleksandar Shtedritski",
                "Yuki Asano"
            ],
            "title": "Bias out-of-thebox: An empirical analysis of intersectional occupational biases in popular generative language models",
            "year": 2021
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691.",
            "year": 2021
        },
        {
            "authors": [
                "Opher Lieber",
                "Or Sharir",
                "Barak Lenz",
                "Yoav Shoham."
            ],
            "title": "Jurassic-1: Technical details and evaluation",
            "venue": "White Paper. AI21 Labs, 1.",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Peter Clark",
                "Yiming Yang."
            ],
            "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
            "venue": "ACL 2022 Workshop on Commonsense Representation and Reasoning.",
            "year": 2022
        },
        {
            "authors": [
                "Inbal Magar",
                "Roy Schwartz."
            ],
            "title": "Data contamination: From memorization to exploitation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Michael McCandless."
            ],
            "title": "Accuracy and performance of google\u2019s compact language detector",
            "venue": "Blog post.",
            "year": 2010
        },
        {
            "authors": [
                "Hamdy Mubarak",
                "Hend Al-Khalifa",
                "Abdulmohsen Al-Thubaity."
            ],
            "title": "Overview of OSACT5 shared task on Arabic offensive language and hate speech detection",
            "venue": "Proceedinsg of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools",
            "year": 2022
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "Stereoset: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed."
            ],
            "title": "AraT5: Text-totext transformers for Arabic language generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed",
                "Tariq Alhindi",
                "Hasan Cavusoglu."
            ],
            "title": "Machine generation and detection of arabic manipulated and fake news",
            "venue": "Proceedings of the Fifth Arabic Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Lorenz Nigst",
                "Maxim Romanov",
                "Sarah Bowen Savant",
                "Masoumeh Seydi",
                "Peter Verkinderen."
            ],
            "title": "Openiti: a machine-readable corpus of islamicate texts",
            "venue": "http://doi. org/10.5281/zenodo, 4075046.",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora",
            "year": 2023
        },
        {
            "authors": [
                "Ethan Perez",
                "Siddharth Karamcheti",
                "Rob Fergus",
                "Jason Weston",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "Finding generalizable evidence by learning to convince Q&A models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Adam Poliak",
                "Aparajita Haldar",
                "Rachel Rudinger",
                "J. Edward Hu",
                "Ellie Pavlick",
                "Aaron Steven White",
                "Benjamin Van Durme."
            ],
            "title": "Collecting diverse natural language inference problems for sentence representation evaluation",
            "venue": "Proceedings of the 2018",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Blog, 1(8).",
            "year": 2019
        },
        {
            "authors": [
                "Jack W Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme."
            ],
            "title": "Gender bias in coreference resolution",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "arXiv preprint arXiv:2110.08207",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Shanya Sharma",
                "Manan Dey",
                "Koustuv Sinha."
            ],
            "title": "Evaluating gender bias in natural language inference",
            "venue": "CoRR, abs/2105.05541.",
            "year": 2021
        },
        {
            "authors": [
                "Oleh Shliazhko",
                "Alena Fenogenova",
                "Maria Tikhonova",
                "Vladislav Mikhailov",
                "Anastasia Kozlova",
                "Tatiana Shavrina."
            ],
            "title": "mgpt: Few-shot learners go multilingual",
            "venue": "arXiv preprint arXiv:2204.07580.",
            "year": 2022
        },
        {
            "authors": [
                "Vered Shwartz",
                "Rachel Rudinger",
                "Oyvind Tafjord."
            ],
            "title": "you are grounded!\u201d: Latent name artifacts in pre-trained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6850\u20136861.",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Noah A. Smith",
                "Luke Zettlemoyer."
            ],
            "title": "Evaluating gender bias in machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679\u20131684, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeff Wu",
                "Daniel M. Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano."
            ],
            "title": "Learning to summarize from human feedback",
            "venue": "Proceedings of the 34th International Conference on Neural In-",
            "year": 2020
        },
        {
            "authors": [
                "Pedro Javier Ortiz Su\u00e1rez",
                "Beno\u00eet Sagot",
                "Laurent Romary."
            ],
            "title": "Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures",
            "venue": "7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-",
            "year": 2019
        },
        {
            "authors": [
                "Yarden Tal",
                "Inbal Magar",
                "Roy Schwartz"
            ],
            "title": "Fewer errors, but more stereotypes? the effect of model size on gender bias",
            "venue": "In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "ArXiv, abs/2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler"
            ],
            "title": "Emergent abilities of large language models. arXiv preprint arXiv:2206.07682",
            "year": 2022
        },
        {
            "authors": [
                "Birhane",
                "Julia Haas",
                "Laura Rimell",
                "Lisa Anne Hendricks",
                "William S. Isaac",
                "Sean Legassick",
                "Geoffrey Irving",
                "Iason Gabriel."
            ],
            "title": "Ethical and social risks of harm from language models",
            "venue": "ArXiv, abs/2112.04359.",
            "year": 2021
        },
        {
            "authors": [
                "Orion Weller",
                "Nicholas Lourie",
                "Matt Gardner",
                "Matthew E. Peters."
            ],
            "title": "Learning from task descriptions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1361\u20131375, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Jeff Wu",
                "Long Ouyang",
                "Daniel M. Ziegler",
                "Nissan Stiennon",
                "Ryan Lowe",
                "Jan Leike",
                "Paul Francis Christiano."
            ],
            "title": "Recursively summarizing books with human feedback",
            "venue": "ArXiv, abs/2109.10862.",
            "year": 2021
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "arXiv preprint arXiv:2010.11934.",
            "year": 2020
        },
        {
            "authors": [
                "Wajdi Zaghouani",
                "Behrang Mohit",
                "Nizar Habash",
                "Ossama Obeid",
                "Nadi Tomeh",
                "Alla Rozovskaya",
                "Noura Farra",
                "Sarah Alkuhlani",
                "Kemal Oflazer"
            ],
            "title": "Large scale arabic error annotation: Guidelines and framework",
            "year": 2014
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Roy Schwartz",
                "Yejin Choi."
            ],
            "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2018
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "HellaSwag: Can a machine really finish your sentence",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Imad Zeroual",
                "Dirk Goldhahn",
                "Thomas Eckart",
                "Abdelhak Lakhouaja."
            ],
            "title": "Osian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure",
            "venue": "Proceedings of the Fourth Arabic Natural Language Processing Work-",
            "year": 2019
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Ke Xu."
            ],
            "title": "Learning to compare for better training and evaluation of open domain natural language generation models",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Daniel M. Ziegler",
                "Nisan Stiennon",
                "Jeff Wu",
                "Tom B. Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "ArXiv, abs/1909.08593.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\nRecent work in generative pretraining (Radford et al., 2019; Brown et al., 2020; Lieber et al., 2021; Chowdhery et al., 2022; Zhang et al., 2022; Smith et al., 2022; Scao et al., 2022; Thoppilan et al., 2022; Hoffmann et al., 2022) has shown that autoregressive models perform well on language tasks using in-context learning, without finetuning or gradient updates. This in-context learning approach allows models to perform new tasks with only simple instructions and a few optional examples, which can be further improved by model adaptation through prompt tuning (Lester et al., 2021). In spite of this progress, autoregressive pretrained\n\u22c6Authors contributed equally.\nTransformer language models of significant size remain largely anglocentric. This makes it difficult to bring more diverse voices to the table. Nor is it clear if multilingual models such as BLOOM (Scao et al., 2022), where model capacity is split across a large number of languages and language-specific data are neither sufficiently large nor diverse, can allow equitable understanding of these models in languages other than English. It is also not possible to study the capabilities of these models in particular linguistic environments (e.g., languages of rich morphology, of diglossic nature, and/or with a large number of dialects such as Arabic) and diverse cultural backgrounds (e.g., African, Asian, Latin American). This situation also deprives nonEnglish communities of the rich array of benefits language model technology can bring as its full potential and emerging capabilities (Wei et al., 2022) are unlocked. Alarmingly, we currently cannot study the social harms, risks, and biases associated with such models. In order to carefully investigate the risks of these models and work on preventing or at least mitigating them, we need to responsibly develop sufficiently large dedicated models outside English.\nTo circumvent these limitations and advance scholarship of autoregressive models beyond English, we propose a suite of decoder-only Transformer models for the Arabic collection of languages and language varieties. Our suite of models, dubbed JASMINE, come in four different architectures that range in size from 300 million to 6.7 billion parameters. Motivated by recent findings as to the impact of pretraining data size vis-\u00e0-vis model size (Hoffmann et al., 2022; Penedo et al., 2023), we carefully curate a large dataset (\u223c 235GB of text) of high-quality text to pretrain JASMINE. Our dataset is also diverse (e.g., covers both standard and dialectal Arabic), endowing our models with an ability to serve wider communities.\nOur work also fills another significant gap for\nArabic autoregressive models, i.e., that of an evaluation benchmark. We introduce an evaluation benchmark comprising a wide collection of test datasets and protocols. Using our benchmark, we evaluate JASMINE extensively both intrinsically (using perplexity) and extrinsically (e.g., on few-shot settings). Our evaluation demonstrates the superiority of JASMINE compared to available baselines. We also perform human evaluations to investigate the ability of our models to write fluent and coherent standard as well as dialectal Arabic across various domains (e.g., news, literary, Twitter). Our evaluations reveal that our JASMINE models posses powerful representations, allowing them to excel in few-shot learning and produce outputs that can be identified by humans only at chance level. Since autoregressive models often carry social biases, harms, and toxicity, our evaluation testbed involves the creation of a set of carefully-designed datasets for measuring a range of social risks. Additionally, we aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.\nTo summarize, we offer the following contributions: (1) We develop JASMINE, a suite of four autoregressive language models for Arabic, ranging in size between 300 million to 6.7 billion parameters pretrained with a diverse dataset. (2) We evaluate JASMINE extensively, introducing a comprehensive evaluation benchmark for a wide range of NLP tasks. We demonstrate JASMINE\u2019s ability to write fluent language and learn well in-context across rich contexts in few-shot settings. (3) Our evaluation benchmark involves the creation and release of datasets for investigating potential social biases, harms, and toxicity. Based on these evaluations, we join arms in calling for ethical practices when working with language models and inviting future research on mitigating their social risks. (4) We aim to responsibly and gradually release our models with interested researchers, along with code for experimenting with them, hoping our work will trigger applications and further research in understanding autoregressive models outside English.\nThe rest of the paper is organized as follows: We introduce JASMINE in Section 2, describe our evaluation strategies in Section 3, and our evaluation benchmark in Section 4. In Section 5, we offer human evaluations of model output. Section 6 is an analysis of social bias in the model, and Section 7\nis about related work. We conclude in Section 8.\n2 JASMINE\n2.1 Arabic\nArabic is a collection of languages and language varieties, some of which (e.g., Moroccan Arabic and Egyptian Arabic) are not mutually intelligible. Classical Arabic (CA) is the variety used in old Arabic poetry and the Qur\u2019an, and is employed side by side with other varieties to date. Modern Standard Arabic (MSA) is a more modern variety (Badawi, 1973) of Arabic that is usually used in pan-Arab media, government, and formal education across the Arab world. Dialectal Arabic (DA) is the term used to refer to Arabic dialects. Dialects are sometimes defined regionally (e.g., Gulf, Levantine, Nile Basin, and North African (Habash, 2010; AbdulMageed, 2015)), but also at the country or even province levels (e.g., (Bouamor et al., 2018; AbdulMageed et al., 2020b,a, 2021b, 2022)). We now introduce JASMINE.\n2.2 (Pretraining) Data\nOur dataset is linguistically diverse, covering all categories of Arabic (i.e., CA, DA, and MSA), as we will now describe. CA Data. We use the Open Islamicate Texts Initiative (OpenITI) corpus (v1.6) (Nigst et al., 2020).1 OpenITI contains 11, 195 premodern Islamic books mainly collected from Shamela Liberay,2 Al-Jami Al-Kabir collection (JK),3 books digitized by Jordanian publisher Markaz Al-Tura\u0304th, and the Shia Library.4 MSA Data. We use \u223c223 GB of MSA text (23.7 billion tokens) from the following sources: AraNewsv2 (Nagoudi et al., 2020), El-Khair (ElKhair, 2016), Gigaword,5 OSCAR (Su\u00e1rez et al., 2019), OSIAN (Zeroual et al., 2019), Wikipedia Arabic, and Hindawi Books.6 We also extract the Arabic part of the multilingual Colossal Clean Crawled Corpus (mC4) (Xue et al., 2020) and clean it (see \u00a7 2.3 for cleaning procedure). We call the extracted portion AraC4 (more details are in Appendix A.2). Dialectal Data (DA). We use a corpus of 1.5 billion Arabic tweets (178GB) randomly\n1We exclude a random sample of 1K books from OpenITI for later use in evaluating JASMINE perplexity (see \u00a7 4.1).\n2https://shamela.ws. 3http://kitab-project.org/docs/openITI. 4https://shiaonlinelibrary.com. 5https://catalog.ldc.upenn.edu/LDC2009T30. 6https://www.hindawi.org/books.\nsampled from a large in-house dataset of \u223c 13 billion Arabic tweets. This dataset is used only for finetuning one of our models (see Section 5), rather than pretraining. Data Distribution. We analyze the distribution of MSA vs. DA in both our AraC4 and Twitter collections using a SoTA binary classifier (AbdulMageed et al., 2021a) (MSA vs. dialect, \u223c 88% F1) on a random sample of 100 million samples from each. We find that our Twitter data involves 28.39% predicted dialect tweets and our AraC4 data involves 5.7% predicted dialect sentences. We then run another SoTA country-level classifier (AbdulMageed et al., 2021a) (\u223c 40% F1) on the predicted dialect portions from each dataset, finding that our Twitter data is more diverse than AraC4. For example, our classifier tags 80% of the predicted AraC4 dialects as Egyptian, 2.86% as Bahraini, 1.85% as Libyan, leaving other dialects to be only marginally represented. Refer to Table 1 for more information about our pretraining data (e.g., size, number of tokens) and Table A.1 for country-level predicted dialects from each of the datasets.\n2.3 Preprocessing and Vocabulary\nWe clean our pretraining data by removing HTML tags, elongation, and hash signs. We also reduce repetitive characters, emojis, and emoticons to only two occurrences per instance. Further, we replace URLs and user mentions with the <URL> and <USER> strings. To create our vocabulary, we use a BPE-based tokenizer similar to GPT-2 (Radford et al., 2019), with a vocabulary of 64, 000 BPE tokens. Refer to Appendix A.1 for more details.\n2.4 Model Design and Implementation We exploit our diverse dataset to train four different variants of JASMINE, as follows: JASMINE350M, JASMINE1.3B, JASMINE2.7B, and JASMINE6.7B.7 We pretrain JASMINE models for 500k steps each using the autoregressive next-step prediction objective (Radford et al., 2019) and the Transformer-based GPT-Neo (Black et al., 2021) replication of the GPT-3 (Brown et al., 2020) architecture. Details of the various architectures of JASMINE are in Table 2.\n3 Evaluation Strategies\nWe follow previous literature (Brown et al., 2020; Howcroft et al., 2020; Zhang et al., 2022) in evaluating our models extensively, under both intrinsic and extrinsic conditions as we now explain. Intrinsic Evaluation. Perplexity (PPL) is a widely used metric that estimates how well a language model predicts a given text. For a tokenized text T = (w1, w1, ..., wn), perplexity of T is:\nPPL(T ) = exp{\u2212 1 n n\u2211 i log p0(wi|w<i)} (1)\nWhere log p0(wi|w<i) is the log-likelihood of the ith word conditioned on the previous words w<i. Extrinsic Evaluation. We employ three settings: (1) few-shot, where a model is given k examples describing the task at inference time as conditioning, but without updating the models\u2019 weights. (2) oneshot, which is the same as few-shot except that only one example is provided to the model (i.e., k=1). (3) zero-shot, where no demonstrations are provided to the model (i.e., k=0).\n4 Evaluation Benchmark\nWe evaluate JASMINE on 23 different datasets, representing five different tasks: language modeling, autocompletion, commonsense inference, word manipulation, and natural language understanding. We now introduce each of these tasks along with related datasets.\n7The number of parameters is suffixed to model names.\n4.1 Language Modeling\nAs explained, we calculate the perplexity of our models as intrinsic evaluation. Since there is no standard dataset for evaluating perplexity on Arabic texts, we create and release a new multi-domain dataset totaling 6K documents extracted from six publicly available sources. These datasets are not in our pretraining and cover three Arabic varieties: MSA, dialect, and CA. We introduce each of them. (1) Arabic Wikipedia. We select 1K articles from Arabic Wikipedia (AraWiki), published after October 2022 to avoid leakage with our data.(2) WikiLingua. Introduced by Faisal Ladhak and McKeown (2020), this resource contains article and summary pairs in 18 languages, including Arabic, extracted from WikiHow.8 We extract 1K Arabic articles from the test set of WikiLingua.9 (3) News Articles. We collect 1K news articles from \u223c 100 Arabic online sources. The articles are not in our pretraining and cover different domains (e.g., culture, economy, politics, sports). (4) Watan2004. We select 1K articles from an old dataset, Watan2004 (WT04) (Abbas et al., 2011). For dialectal and classical Arabic, we also extract a random 1K articles from each of the following sources: (5) EgyWiki. Egyptian Arabic articles from Wikipedia dumps, and (6) CA-Book. Open Islamicate Texts Initiative (OpenITI) corpus (Nigst et al., 2020). Results. Table 3 shows the zero-shot BPEtoken level perplexity of our JASMINE models on the six datasets. We compare to the four AraGPT2 models proposed by Antoun et al. (2021) and mGPT (Shliazhko et al., 2022) as baselines. Our JASMINE models clearly outperform all baselines by a significant margin, with JASMINE6.7B reaching an average PPL of 42.25.\n4.2 Autocompletion\nThe goal of autocompletion is to predict the last word for a given text. For this, we create a dataset totaling 15K samples. These are news headlines (5K phrases/sentences), news stories (5K paragraphs), and theses titles (5K phrases/sentences). All samples are collected from diverse online sources. For example, the thesis titles cover domains such us \u00e8P@XB @ (management), \u00ae J\u00cb @ \u00d5\u00ce\u00ab (psy-\nchology), and \u00e0\u00f1 KA \u00ae\u00cb @ (law). For evaluation, we give JASMINE a prompt (title or paragraph) with-\n8https://www.wikihow.com/. 9https://huggingface.co/datasets/GEM/wiki_lingua.\nout the last word and ask it to predict the masked word. We experiment with our models under zero-, one-, and few-shot settings. Results. Table 4 shows results on the news title datasets, and we provide results for the two other autocompletion datasets in Table C.1. From Table 4 we can see that JASMINE models perform best in all settings.10 We also observe that more demonstrations tend to help improve performance. We also note that the models achieve the best autocompletion on the news stories subtask, perhaps due to our pretraining data involving significant amounts of news. The models also perform reasonably well on the theses titles domain, perhaps since our pretraining datasets involve specialized books covering academic topics. We notice a drop in model performance under the 24-shot setting, perhaps since few-shot learning can be sensitive to the order of the shots Wei et al. (2021); Brown et al. (2020); Lu et al. (2022).\n4.3 Commonsense Inference\nSince there is no Arabic commonsense inference evaluation dataset, we follow methods introduced by Zellers et al. (2018) to create a new, high-quality Arabic commonsense collection using a random\n10For this and upcoming experiments, we restrict evaluation to our smaller models (all or any of our 1.3B-6.7B models) due to constraints on our computing resources.\nsample of 16, 707 examples from Arabic WikiHow. Each example has a context and a correct answer.11 For each context, we create three generated answers using an adversarial approach. We refer to our new dataset as AraSWAG (Arabic Situations With Adversarial Generations). We next provide a full explanation of it. Initial Dataset Creation. We randomly sample 10K examples from Arabic WikiHow.12 We then finetune AraT5 (Nagoudi et al., 2022) on the sampled examples separately, where we feed the model with the contexts in order to generate the endings. After finetuning, we generate three possible endings for a different set of WikiHow (17K examples). We generate the ending by setting topk = 50 and topp = 0.95 to mimic human-like writings. Therefore, our initial datasets contain one context and four endings (one real and three generated). Adversarial Dataset Creation. To make the commonsense inference task more challenging, we follow (Zellers et al., 2018, 2019) and apply the adversarial filtering (AF) method on the initial dataset. Specifically, on each iteration, the dataset is randomly partitioned into Dtrain and Dtest with a split of 8:2. We then finetune a MARBERT (AbdulMageed et al., 2021a) model in order to classify endings as real or generated on Dtrain. We evaluate the finetuned model on Dtest, then apply AF to replace easy-to-classify generations in Dtest with newly generated endings using the finetuned AraT5. This process continues until accuracy of these adversaries converges. We observe that during convergence, the accuracy of MARBERT drops to \u223c 30%. Finally, we randomly split the resulting AraSWAG dataset into training (Train=14, 288), validation (Dev= 7, 44), and testing (Test=1, 675) sets.\nWe use AraSWAG to seed our 350B, 1.3B, and 2.7B JASMINE models and the baselines with a context and four endings, one original (true) and three generated (false) as explained. We then compute for each ending a language modeling score (LMS), following Nadeem et al. (2021),13 to identify whether it is related to the seed context or not. We evaluate the likelihood of each candidate\u2019s ending conditioned on the context and choose the candidate with the highest LMS. Table 5 shows an example of a context and four endings from\n11https://www.wikihow.com 12https://www.wikihow.com 13Refer to Appendix B.1 for details about LMS.\nAraSWAG. Results. As Table 6 shows, although our dataset is challenging, JASMINE2.7B significantly outperforms baselines (37.18 F1).\n4.4 Word Manipulation\nWe test our JASMINE models\u2019 ability to learn how to correct word-level errors (i.e., recover the original word) from a few examples. For this, we exploit one existing and one new dataset: (i) Natural Spelling Errors. We use QALB (Zaghouani et al., 2014), a large manually-corrected collection of Arabic sentences. QALB covers a variety of types of errors, from which we extract 22.8k words with spelling errors and errors in proper names. (ii) Synthetic Errors. We create a synthetic dataset with five scrambling tasks using the same method introduced in GPT-3 (Radford et al., 2019). The\ntasks are (1) cycle letters (CL), where the model is given a word with its letters cycled. (2) anagrams1 (A1), where every letter in the word except the first and last are scrambled randomly. (3) anagrams2 (A2), where every letter in the word except the two first and last letters are scrambled randomly. (4) random insertion (RI), where a random space character or punctuation is inserted between each letter of a word. (5) reversed words (RW), where we task the model to recover the backward version of the word. Table 8 offers an illustrative example for each word scrambling technique. For each of the five techniques, we generate 10K top words from a dictionary extracted from Wikipedia Arabic and Hindawi Books. Results. As Table 7 shows, our models achieve better results in 23 out of 25 settings.\n4.5 Evaluation on Arabic NLU Benchmark\nWe also investigate the capability of our models on six text classification datasets from the large and diverse ORCA benchmark (Elmadany et al.,\n2023) under zero-, one-, and few-shots conditions. Performance of JASMINE on ORCA is shown in Table C.2. We find that JASMINE6.7B acquires the best results, again clearly outperforming all baselines.\n5 Human Evaluation of Model Output\nWe carry out a set of human studies to investigate the ability of our JASMINE2.7B model to generate texts from diverse domains. This includes the news, literary (i.e., poetry), and Twitter domains. We also investigate the ability of the same model to produce dialectal continuations when seeded by sequences from the respective dialects. We provide sample generations from these experiments in Table 9. News Story Generation. We sample 10 news articles from each of 10 categories of a news dataset not in our pretraining (total=100 articles).14 For each news category, we extract the first sentence from five sampled articles and use the sentence to prompt our model to generate an output for each article. We then provide the 50 JASMINE2.7Bgenerated texts and the remaining 50 original articles15 to two college-educated Arabic native speakers to assign a label from the set {human, generated} at the article level. We find that annotators only have a random chance to identify generations by our model. In fact, for the 50 articles generated by our model, either of the two annotators could identify only 11 samples (i.e., 22%) and the two annotators never agreed on any of the samples. This shows that our model is able to output sensible, human-like language for the news domain. We pro-\n14The categories are from the set {Economy, Education, Health, History, Media, Politics, Religion, Sports, Technology, Weather}, and the average size of an article is 125 words.\n15We shuffle the generated and the original articles.\nvide sample generations from this experiment in Table E.2. Poetry Generation. We experiment with seeding our model with three lines of real poetry at a time (3-shot) and find that while generated sequences do look like \u2018poetry\u2019, the model is not able to consistently maintain the rhyme. We show the results of this experiment in Table E.5. We then run another experiment where we collect a poetry dataset of \u223c 22K poems16 and further pretrain the model with it for \u223c 50k steps. We refer to the resulting model as JASMINEpoetry and provide samples from its output in Table E.6. A human annotation study reveals that annotators are able to tease apart JASMINEpoetry generations from human poetry in 52.63% of the time. We note, however, that model generations are quite sensible and it is able to keep the rhyme in many output lines. Tweet Generation. We experiment with teaching our model to write tweets by further pretraining\n16Details of the dataset are in Appendix B.2.\nit on an in-house dataset of 1.5 billion tweets for \u223c 100k steps, restricting the sequence length to 128 BPE tokens and adding the prefix \u201cXQ \u00ab:\u201d (\u201cwrite a tweet:\u201d) to all tweets. We refer to the resulting model as JASMINEtweet and provide samples from its output in Table E.4. A gold annotation study reveals that humans are able to detect generations from JASMINEtweet only in 48.53% of the time, thus reflecting the model\u2019s ability to output highquality tweets. Dialectal Generation. We study our model\u2019s ability to generate dialectal texts by seeding it sequences from a new Arabic dialects dataset that we prepare. We create the dataset by manually transcribing a total of 500 speech utterances from five different Arabic dialects from the set {Algeria, Egypt, Jordan, Morocco, Yemen} (100 utterances, around 30 seconds long from each dialect).17 We acquire 500 outputs from our model by seeding it the transcriptions sample under one-shot, referring to the dataset as STGen. Appendix Table E.7 shows samples from these dialect-prompted generations. Annotation and Results. We ask annotators with native fluency in the five dialects mentioned to assign labels in two stages: MSA vs. dialect (stage one); and if dialect, whether the dialect is the same as the seed utterance (stage two). We find that annotators assign a dialect tag 52.86% of the time, with the model staying within the same dialect as the prompt utterance 45.37% of the time. We also find that while the model excels on sticking to the Egyptian dialect of a prompt (79.35%), it is less successful in doing so for Jordanian, Moroccan, Yemeni, and Algerian (with 47.62%, 48.39%, 4.35%, 47.17%, respectively). We hypothesize that this is a function of the model seeing larger amounts of Egyptian dialects and the overlap between MSA and dialects.18 We also make an exciting discovery in the context of this experiment: the model generates multi-party dialect conversations (see Table E.7).\n6 Analysis of Social Bias\nWhile autoregressive models are able to produce fluent texts which have a multitude of useful applications, they can also carry societal biases. To\n17We provide full details of our new speech transcription dataset in Appendix B.3.\n18We hypothesize that if we seed the model with longer sequences it will be abler to stay within the same dialect as the seed, and cast this as future research.\nquantify biases in our generative models, we use conditional generation (i.e., autocomplete generation) (Shwartz et al., 2020; Brown et al., 2020). For all social bias experiments, we use JASMINE2.7B. We provide sample outputs from all these experiments in Table E.3.\nBiases in Gender Autocompletion. We investigate associations between occupation and linguistic gender by prompting the model. For this cause, we manually prepare a list of 100 occupations which we use with the following template: \u201cThe <occupation> is often practiced by ...\u201d (e.g., . . . A\u00eeD PA\u00d6\u00df A\u00d3\nAJ. \u00cb A \u00ab I. \u00a2\u00cb@). We provide the full list in Ta-\nble E.1.\nResults. We find that 62.50% of the 100 occupations we test are more likely to be followed by a male linguistic gender. This means that the model is male-leaning when an occupation context is given.\nGender, Color, and Region. Inspired by Kirk et al. (2021), we use the following template \u201cYou always find [X][Y][Z] working as . . . \u201d, where X is a binary gender, Y is one of the regions in the set {Africa, Asia, America, Europe}, and Z represents one of two colors black or white. This gives us a total of 16 prompt combinations. One example from this combination can be . . . \u00bc \u00e0\u00f1\u00ca\u00d2\u00aaK X\u00f1 \u00cb@ \u00e0\u00f1J \u00baK Q\u00d3\nB@ \u00c8Ag. Q\u00cb @ Ym.\n' A\u00d3 A\u00d6 \u00df @X (En-\nglish: \u201cYou\u2019d always find black American men working as . . . \u201d). Then, we use top-k and top-p sampling (with top-k=50 and top-p=0.95) to generate 10 completions for each of the 16 prompt combinations, this gives us 1, 600 generated sentences of which we keep only 1, 000 sentences that contain professions. Finally, we manually classify the generated sequences into one of three categories from the manually prepared set {high-wage, medium-wage, low-wage}.\nResults. We manually analyze our model output and find that white people are associated with highwage jobs 51.25% of the time and medium-wage jobs 48.75% of the time (zero association with low-paying jobs). In contrast, 72.50% of people of color are associated with medium-wage professions and only 23.75% with high-wage professions (with the remaining 3.75% associated with lowwage jobs). These results show that the model carries social biases related to color. We also find that these biases are worse when we consider combinations of color, region, and gender. For example, European white people are associated with high-\nwage occupations 100% of the time. When the context is Africa, region information triggers very biased association: people of African descent are associated with low-wage occupations 100% of the time. Again, these findings confirm what we know\u2013autoregressive models, even those trained on diverse data (e.g., not only from the web but also from books), suffer from various types of biases. Religion and Religious Groups. To evaluate potential biases towards a given religion/ideology or religious/ideological group, we introduce the following template to construct our prompts \u201cThese <R> guys always are. . . \u201d (. . . A\u00d3 A\u00d6 \u00df @X \u00e9 J \u00cb@ H. AJ. \u00cb@ ZB \u00f1\u00eb), where R is either one of the four religions/ideologies Atheists, Islam, Judaism, Christianity, and Sikh and one of seven Muslim/Islamic groups from the set {Ash\u2019aris, Salafis, Muslim Brotherhood, Shi\u2019a, Sufis, Sunni}. Again, we use top-k and top-p sampling (with k=50 and p=0.95) to generate 50 completions for each of the 12 prompts. Then, we measure whether or not the generated texts are abusive, dangerous, hateful, or offensive using four SoTA classifiers (one for each task) from Abdul-Mageed et al. (2021a). Results. We present results in Figure 2. We observe that dangerous language is predicted as most associated with Atheists; and offensive language is most associated with Atheist, Shiite, and Jewish groups. The model associates hateful language equally to Sunni and Shiite groups. Importantly, we believe this analysis of bias should be considered with caution. Human Analysis. We augment our automated analysis of religious and ideological bias with a human study where we ask two native speakers to label 400 random classifier outputs, finding the two annotators to agree with the classifiers as follows: 86.50 (dangerous), 81.00 (hateful), and 77.50 (of-\nfensive). We take these high agreements to mean that we can depend on the SoTA classifiers for analysis of bias in our particular case. We provide more details about the human annotation guidelines in Appendix E.2.\n7 Related Work\nLarge Language Models (LLMs). Brown et al. (2020) develop GPT-3 and show its abilities on few-shot learning. Several other works followed, usually introducing larger models (Rae et al., 2021; Thoppilan et al., 2022; Smith et al., 2022). By way of examples, PaLM (Chowdhery et al., 2022) is a 540B densely activated, autoregressive Transformer model trained on 780B tokens. Chowdhery et al. (2022) demonstrate continued benefits of scaling by achieving SOTA few-shot learning results on hundreds of NLU and NLG tasks. Zhang et al. (2022) introduce OPT and seeks to enable reproducible and responsible research at scale. Smith et al. (2022) train Megatron-Turing NLG with 530B parameters. A number of recent works such as T0 (Sanh et al., 2021), FLAN (Wei et al., 2021), and BLOOM (Scao et al., 2022) focus on directly improving language model\u2019s zero-shot learning capabilities through large-scale multitask finetuning. More recently, Touvron et al. (2023) introduce a large efficient model called LLaMA trained on trillions of tokens from publicly accessible datasets. Language Model Alignment. Ziegler et al. (2019); Stiennon et al. (2020); Wu et al. (2021) apply reinforcement learning to align language models for text summarization. Similarly, human feedback has been used to align language models for dialogue generation (Jaques et al., 2019; Hancock et al., 2019), story generation (Zhou and Xu, 2020), evidence extraction (Perez et al., 2019). Most recently, Madaan et al. (2022) use written human feedback to augment prompts and improve the performance of GPT-3. Glaese et al. (2022) introduce Sparrow, a model trained to be more helpful, correct, and harmless compared to prompted language models. Instruction-tuning of LLMs. Weller et al. (2020) introduce a framework, ZEST, to solve a new task after reading its description. Schick and Sch\u00fctze (2021) develop a novel pattern exploiting training (PET) scheme to verbalize supervised classification task into cloze question format. Recently, Ouyang et al. (2022) propose InstructGPT, where the authors first finetune GPT-3 with labeler-written\nprompts, then the authors rank the output with human feedback to align the model with the users\u2019 intent. Later, ChatGPT19 followed the same training procedure to develop a conversational agent. Taori et al. (2023) finetuned an instruction-following language model, Alpaca, with LLaMA as the backbone model 52K generated instruction instructions based on Wang et al. (2022). Anand et al. (2023) develop a chatbot on a massive curated corpus created using GPT-3.5-Turbo. Geng et al. (2023) fine-tune LLaMA, Koala on data scraped from the web. Concurrently, Chiang et al. (2023) introduce Vicuna using GPT-4 (OpenAI, 2023) to assess and rank the outputs. Besides, several other models have been released based on instruction-tuning (e.g., Dolly)20 and RL (e.g., OpenAssistant).21\nEthics and Bias in Language Models. The recent success of LLMs is associated with various potential risks since the web pretraining datasets themselves are biased (Bender et al., 2021; Bommasani et al., 2021; De-Arteaga et al., 2019; Dodge et al., 2021). Magar and Schwartz (2022); Tal et al. (2022) show that the risk of biases gets higher with the increase of the model size, causing biases to resurface during the downstream tasks such as NLI (Poliak et al., 2018; Sharma et al., 2021), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), and MT (Stanovsky et al., 2019). A number of ethical considerations related to PLMs have been studied, including memorizing and revealing private information (Carlini et al., 2022), or spreading misinformation (Weidinger et al., 2021).\n8 Conclusion\nWe introduced JASMINE, a suite of powerful GPT models for Arabic varying in size between 300 million to 6.7 billion parameters. Our models are pretrained on a large dataset of diverse Arabic varieties from multiple domains. We also introduced a novel evaluation benchmark for Arabic GPT models. Using our benchmark, we demonstrate how it is that our models excel in few-shot learning as well as producing fluent texts that humans can only detect at chance level. We plan to responsibly release our models with researchers to support scholarship in this important research area.\n19https://openai.com/blog/chatgpt 20https://github.com/databrickslabs/dolly 21https://open-assistant.io\n9 Limitations\nWe identify the following limitations in our work:\n1. Although we strive to include as much dialectal texts in our pretraining data as is possible, our automated analysis reveals that the dataset still does not have wide coverage of some dialects such as Algerian, Iraqi, Moroccan, Sudanese, Syrian, and Yemeni. One way to improve JASMINE performance on dialectal generation would be to collect more data from these varieties and further pretrain the models with this new collection.\n2. Although some works in the literature use word lists to remove toxic and hateful language from the pretraining data, we do not follow this practice. The reason is that we wanted our models to be suited for use in toxic and hateful language detection as few shot learners. We also believe that use of word lists, although can be useful in removing some anti-social content, can also be only cosmetic when it comes to data cleaning. Regardless, we believe our models should be utilized with caution and approaches to mitigating social risks, biases, and toxicities should be carefully applied.\n3. One of the disadvantages of autoregressive models in general is that they can be misused for generating fake content or even be deployed for producing misinformation at scale. This is is one of the most dangerous uses of this class of models. For these reasons, we believe all necessary measures ought to be taken around their use and JASMINE is no exception. This may include, for example, regulations and policies that restrict these to pro-social use such as in education, travel, recreation, etc. Due to these concerns, we will release our models only responsibly. For example, we will require users requesting our models to provide information about intended uses. We will also encourage use of our models in research seeking to mitigate social biases in LMs, develop new mitigation methods, etc.\n10 Ethics Statement\nEnergy efficiency. Our JASMINE models, similar to many large PLMs, needed significant pretraining\ntime and are not energy efficient. We acknowledge this important issue and believe work on creating energy-efficient models should continue to receive scholarly attention. Data. Our pretraining datasets are collected from the public domain and cover diverse genres, communities, and varieties of Arabic. As we have demonstrated, our JASMINE models have the potential to power applications involving several varieties of Arabic and serve wide populations. Data Copyright.We emphasize that all the datasets (CA, DA, and MSA) we use are collected from publicly available sources. We confirm that our data collection does not violate the copyrights of any of these sources. This includes X (previously Twitter). We would also like to emphasize that all our base models (sizes 300M, 1.3B, 2.7B, and 6.7B) are pretrained without use of X/Twitter data. As such, all of these four base models can be shared with others responsibly with no concerns related to Twitter data use. More precisely, we use 1.5B tweets to further pretrain only one of these base models (JASMINEtweet, at 2.7B parameters) to test the model\u2019s ability to generate sensible \u2018tweets\u2019. Model Release. We plan to release our models only responsibly. We will set stricter conditions on releasing the model finetuned on tweets, JASMINEtweet. Namely, we will require that this model not be deployed in real-world and not be shared publicly. Privacy. JASMINE is developed using publicly available data. Hence, we do not have serious concerns about personal information being retrievable from our trained models. To alleviate concerns about privacy in tweets used in JASMINEtweet, we note that we removed tweet IDs, all usernames, and URLs before pretraining the model. Again, JASMINEtweet will only be released under strict conditions. Human Annotation. The human annotators involved in this project are two of the authors of this paper. Both annotators are Arabic native speakers holding Ph.D. degrees with extensive experience in NLP. They are full-time employees of the research group responsible for this work, and data annotation is part of their job duties. No Institutional Review Board (IRB) review or approval was required for this project since we only use publicly available data, which does not require access to any social networking account or password. In addition, no external annotators were involved in this work.\nBias Analysis. The goal of our bias analysis is to determine whether any biases related to \u201cgender\u201d, \u201ccolor\u201d, or \u201cregion\u201d exist. For instance, color has historically been a significant cause of social injustice and remains relevant in many societies today. We find it challenging to study bias in models without referencing the concept of \u201ccolor\u201d. However, we would like to highlight that the term \u201ccolor\u201d is sensitive and recommend avoiding potentially discriminatory terms whenever possible. We clearly note our respect for sensitivities surrounding this concept. Applications. Similar to many autoregressive language models, JASMINE can be misused. Meanwhile, JASMINE can be deployed for a wide host of useful applications such as in education and health.\nAcknowledgements\nWe gratefully acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN-2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 435-2018-0576; 895-2020-1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada,22 and UBC ARC-Sockeye.23 We thank the Google TFRC program for providing us with free TPU access.24\n22https://alliancecan.ca 23https://arc.ubc.ca/ubc-arc-sockeye 24https://sites.research.google/trc/about/\nReferences Mourad Abbas, Kamel Sma\u00efli, and Daoud Berkani.\n2011. Evaluation of topic identification methods on arabic corpora. JDIM, 9(5):185\u2013192.\nMuhammad Abdul-Mageed. 2015. Subjectivity and sentiment analysis of Arabic as a morophologicallyrich language. Ph.D. thesis, Indiana University.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany, and El Moatez Billah Nagoudi. 2021a. ARBERT & MARBERT: Deep bidirectional transformers for Arabic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7088\u20137105, Online. Association for Computational Linguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Houda Bouamor, and Nizar Habash. 2020a. NADI 2020: The first nuanced Arabic dialect identification shared task. In Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 97\u2013110, Barcelona, Spain (Online). Association for Computational Linguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Houda Bouamor, and Nizar Habash. 2021b. NADI 2021: The second nuanced Arabic dialect identification shared task. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 244\u2013259, Kyiv, Ukraine (Virtual). Association for Computational Linguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Houda Bouamor, and Nizar Habash. 2022. Nadi 2022: The third nuanced arabic dialect identification shared task. arXiv preprint arXiv:2210.09582.\nMuhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, and Lyle Ungar. 2020b. Toward microdialect identification in diaglossic and code-switched environments. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5855\u20135876, Online. Association for Computational Linguistics.\nAli Alshehri, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2020. Understanding and detecting dangerous speech in social media. In The 4th Workshop on Open-Source Arabic Corpora and Processing Tools (OSACT4), LREC.\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020. Arabert: Transformer-based model for arabic language understanding. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 9\u201315.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021. Aragpt2: Pre-trained transformer for arabic language generation. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 196\u2013207.\nMS Badawi. 1973. Levels of contemporary arabic in egypt. Cairo: D\u00e2r al Ma\u2019\u00e2rif.\nEmily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Association for Computing Machinery.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u2019e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258.\nHouda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, and Kemal Oflazer. 2018. The\nMADAR Arabic dialect corpus and lexicon. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram\u00e8r, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. ArXiv, abs/2202.07646.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nMaria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* \u201919, page 120\u2013128, New York, NY, USA. Association for Computing Machinery.\nJesse Dodge, Maarten Sap, Ana Marasovic\u0301, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nIbrahim Abu El-Khair. 2016. 1.5 billion words arabic corpus. arXiv preprint arXiv:1611.04033.\nAbdelRahim Elmadany, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. Orca: A challenging benchmark for arabic language understanding.\nClaire Cardie Faisal Ladhak, Esin Durmus and Kathleen McKeown. 2020. Wikilingua: A new benchmark dataset for multilingual abstractive summarization. In Findings of EMNLP, 2020.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post.\nAmelia Glaese, Nathan McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, A. See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sovna Mokr\u2019a, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William S. Isaac, John F. J. Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. ArXiv, abs/2209.14375.\nNizar Y Habash. 2010. Introduction to Arabic natural language processing, volume 3. Morgan & Claypool Publishers.\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. 2019. Learning from dialogue after deployment: Feed yourself, chatbot! In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3667\u2013 3684, Florence, Italy. Association for Computational Linguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.\nDavid M Howcroft, Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A Hasan, Saad Mahamood, Simon Mille, Emiel Van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: Nlg needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169\u2013182.\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, \u00c0gata Lapedriza, Noah J. Jones, Shixiang Shane Gu, and Rosalind W. Picard. 2019. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. ArXiv, abs/1907.00456.\nHannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano. 2021. Bias out-of-thebox: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, pages 2611\u20132624.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 1.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nAman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve GPT-3 after deployment. In ACL 2022 Workshop on Commonsense Representation and Reasoning.\nInbal Magar and Roy Schwartz. 2022. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165, Dublin, Ireland. Association for Computational Linguistics.\nMichael McCandless. 2010. Accuracy and performance of google\u2019s compact language detector. Blog post.\nHamdy Mubarak, Hend Al-Khalifa, and Abdulmohsen Al-Thubaity. 2022. Overview of OSACT5 shared task on Arabic offensive language and hate speech detection. In Proceedinsg of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Qur\u2019an QA and Fine-Grained Hate Speech Detection, pages 162\u2013166, Marseille, France. European Language Resources Association.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. 2022. AraT5: Text-totext transformers for Arabic language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 628\u2013647, Dublin, Ireland. Association for Computational Linguistics.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed, Tariq Alhindi, and Hasan Cavusoglu. 2020. Machine generation and detection of arabic manipulated and fake news. In Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 69\u201384.\nLorenz Nigst, Maxim Romanov, Sarah Bowen Savant, Masoumeh Seydi, and Peter Verkinderen. 2020. Openiti: a machine-readable corpus of islamicate texts. http://doi. org/10.5281/zenodo, 4075046.\nOpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.\nEthan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. 2019. Finding generalizable evidence by learning to convince Q&A models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\u20132411, Hong Kong, China. Association for Computational Linguistics.\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. 2018. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 67\u201381, Brussels, Belgium. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8).\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8\u201314, New Orleans, Louisiana. Association for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic\u0301, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255\u2013269, Online. Association for Computational Linguistics.\nShanya Sharma, Manan Dey, and Koustuv Sinha. 2021. Evaluating gender bias in natural language inference. CoRR, abs/2105.05541.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2022. mgpt: Few-shot learners go multilingual. arXiv preprint arXiv:2204.07580.\nVered Shwartz, Rachel Rudinger, and Oyvind Tafjord. 2020. \u201cyou are grounded!\u201d: Latent name artifacts in pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6850\u20136861.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.\nGabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679\u20131684, Florence, Italy. Association for Computational Linguistics.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA. Curran Associates Inc.\nPedro Javier Ortiz Su\u00e1rez, Beno\u00eet Sagot, and Laurent Romary. 2019. Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). LeibnizInstitut f\u00fcr Deutsche Sprache.\nYarden Tal, Inbal Magar, and Roy Schwartz. 2022. Fewer errors, but more stereotypes? the effect of model size on gender bias. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 112\u2013120, Seattle, Washington. Association for Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u2019elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. ArXiv, abs/2212.10560.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.\nLaura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sande Minnich Brown, William T. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. ArXiv, abs/2112.04359.\nOrion Weller, Nicholas Lourie, Matt Gardner, and Matthew E. Peters. 2020. Learning from task descriptions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1361\u20131375, Online. Association for Computational Linguistics.\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nissan Stiennon, Ryan Lowe, Jan Leike, and Paul Francis Christiano. 2021. Recursively summarizing books with human feedback. ArXiv, abs/2109.10862.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.\nWajdi Zaghouani, Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014. Large scale arabic error annotation: Guidelines and framework.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800, Florence, Italy. Association for Computational Linguistics.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Abdelhak Lakhouaja. 2019. Osian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure. In Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 175\u2013182.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15\u201320, New Orleans, Louisiana. Association for Computational Linguistics.\nWangchunshu Zhou and Ke Xu. 2020. Learning to compare for better training and evaluation of open domain natural language generation models. In AAAI Conference on Artificial Intelligence.\nDaniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593.\nAppendices We provide an overview of the Appendix below.\nI Pretaining data (Appendix A).\nIn this section, we first provide more details about our JASMINE\u2019s pretraining data. We also give additional details, as follows:\n\u2022 We discuss our decisions about JASMINE\u2019s vocabulary in Appendix A.1.\n\u2022 More details on our AraC4 Data are provided in Appendix A.2.\n\u2022 The cleaning strategy we employ to ensure the quality of AraC4 is presented in Appendix A.3.\nII Evaluation Datasets (Appendix B).\nWe then give more details about the evaluation datasets we created.\n\u2022 We provide a full explanation of our AraSwag dataset in Appendix B.1.\n\u2022 Details of our poetry dataset are in Appendix B.2.\n\u2022 We provide full details of our speech transcription dataset in Appendix B.3.\nIII Evaluation (Appendix C).\nWe provide additional evaluation details, including:\n\u2022 Appendix C.1 shows an illustrative example for each word scrambling technique.\n\u2022 The results of the autocompletion datasets (described in \u00a7 4.4) are in Appendix C.2.\n\u2022 Performance of JASMINEmodels on the NLU tasks is shown in Appendix C.3\nIV Analysis of Social Bias (Appendix E).\nIn this section, we provide additional information about our social bias analysis.\n\u2022 We provide sample outputs from our social bias analysis in Table E.3.\nV Examples of Model Output (Appendix D).\nIn this section, we show examples generated from different JASMINE models under different settings:\n\u2022 Table E.2 shows examples of generated news articles and short stories from JASMINE2.7B under the zero-shot setting.\n\u2022 Examples from generated \u2018tweets\u2019, prompted from JASMINEtweets are given in Table E.4.\n\u2022 Table E.5 provides generated \u2018poetry\u2019 from JASMINE2.7B, prompted by three lines from Al-Mutanabi (a popular Arabic poet) under the zero-shot setting.\n\u2022 Table E.6 shows examples of synthetically generated \u2018poetry\u2019 from our further pretrained JASMINEpoetry prompted by a full (or part of) real line of poetry.\nA.1 JASMINE\u2019s Vocabulary\nFor this, we train the BPE tokenizer on our entire dataset. Our choice of vocabulary size is inspired by Lieber et al. (2021) who demonstrate the benefits of a large vocabulary (e.g., better text representation, faster token processing, and higher ability to cover more content during training and leverage longer prompts in few-shot settings), at the cost of requiring more memory to store the additional parameters of the vocabulary embedding layer, as well as more computing resources to calculate the token probabilities using the larger vocabulary. We hence employ a larger vocabulary than GPT-3 (which uses 50K tokens) but choose not to grow it much larger.\nA.2 AraC4 Data\nThe mC4 dataset Xue et al. (2020) is a multilingual variant of the C4 dataset (Raffel et al., 2019). The mC4 has 101 languages generated from 86 Common Crawl dumps. AraC4, the Arabic part of mC4, represents the 1.66% of mC4 data. It contains 53M webpages with more than 57B Arabic tokens and a total size of 237GB.\nA.3 AraC4 Cleaning\nFor our analysis, we randomly sample 1M paragraphs from AraC4. We first perform language identification using CLD3 (McCandless, 2010) on the data. We find a sizable amount of the data (i.e., 13.59%) to be non-Arabic (mostly English or French). We manually inspect \u223c 100 random samples of the data predicted as non-Arabic. We find these are mostly either non-linguistic content (e.g., java-script or HTML code) or non-Arabic text. The non-Arabic text is sometimes foreign language advertising, a full translation of the Arabic text in some cases, or even boilerplate text such as that in web forums. We clean our AraC4 data by removing HTML tags, elongation, and hash signs. We also reduce repetitive characters, emojis, and emoticons to only two occurrences per instance. Further, we replace URLs with the <URL> string. We finally, keep only webpages that contain at least 95% Arabic characters. We end up with 178GB of Arabic web.\nB Evaluation Datasets\nB.1 AraSwag\nInitial Dataset Creation. We randomly sample 10K examples from Arabic WikiHow.25 We then finetune AraT5 (Nagoudi et al., 2022) on the sampled examples separately, where we feed the model with the contexts in order to generate the endings. After finetuning, we generate three possible endings for a different set of WikiHow (17K examples). We generate the ending by setting topk = 50 and topp = 0.95 to mimic human-like writings. Therefore, our initial datasets contain one context and four endings (one real and three generated). Adversarial Dataset Creation. To make the commonsense inference task more challenging, we follow (Zellers et al., 2018, 2019) and apply the adversarial filtering (AF) method on the initial dataset. Specifically, on each iteration, the dataset is randomly partitioned into Dtrain and Dtest with a split of 8:2. We then finetune a MARBERT (AbdulMageed et al., 2021a) model in order to classify endings as real or generated on Dtrain. We evaluate the finetuned model on Dtest, then apply AF to replace easy-to-classify generations in Dtest with newly generated endings using the finetuned AraT5. This process continues until accuracy of these adversaries converges. We observe that during con-\n25https://www.wikihow.com\nvergence, the accuracy of MARBERT drops to \u223c 30%. Finally, we randomly split the resulting AraSWAG dataset into training (Train=14, 288), validation (Dev= 7, 44), and testing (Test=1, 675) sets.\nB.2 Poetry Dataset\nThe dataset comprises 21.8K Arabic poems from Al-Diwan website 26 which come from 909 authors. The poems cover 26 different topics such as romance, politics, religion, etc.\nB.3 Speech Transcription Dataset\nIn order to provide a versatile dialectal Arabic dataset that can be used to evaluate our JASMINE models\u2019 capability to generate dialectal texts, we collect a dialectal speech dataset from YouTube. The data come from Arabic soap operas from five different Arab countries. Namely, we collect two soap operas from countries in the set {Algeria, Egypt, Jordan, Morocco, Yemen}. We then manually transcribe 100 utterances, each of length \u223c 30 seconds, from each country. We end up with a total of 500 speech utterances from the five different Arabic dialects.\nC Evaluation Tasks\nC.1 Words Scrambling\nThe word scrambling task aims to test the models\u2019 ability to correct word-level errors. We use five-word scrambling techniques, namely: (1) cycle letters, (2) anagrams1, (3) anagrams2, (4) random insertion, and (5) reversed words. These techniques are explained in the paper. Table 8 shows an illustrative example for each word scrambling technique.\nC.2 Autocompletion\nThe autocompletion task aims to predict the last word for a given text. Performance of our JASMINE models on news titles, news stories, and the thesis titles datasets are presented in Table C.2.\n26Al-Diwan website\nC.3 NLU\nWe investigate the capability of our models on 6 text classification datasets (topic, gender, adult, dialect, sarcasm, and sentiment) from the ORCA (Elmadany et al., 2023). The performance of JASMINE on ARLUE is shown in Table C.2.\nD Model Output Examples\nIn this section, we provide various generated examples, including news stories, short stories in Table E.2, social bias in Table E.3, tweets in Table E.4, poetry in Table E.5 and E.6.\nE Analysis of Social Bias\nE.1 Social Bias. In this section, we provide additional information about our social bias analysis. Table E.3 shows generated outputs under different settings presented in appendix E.\nE.2 Annotation Guidelines. For labeling outputs from the model with tags from the set {dangerous, hateful, offensive}, two native speakers were given guidelines that include definitions for each of the three terms. We provide these definitions here: Dangerous. Dangerous language pertains statements expressing an intent to cause physical pain, injury, or harm to someone as a form of retaliation for actions taken or not taken. This interpretation does not encompass threats that lack an indication of physical harm toward the recipient. Furthermore, this definition excludes instances of playful irony or jest that are intended purely for teasing purposes (Alshehri et al., 2020). Offensive. We define offensive language as any form of socially unacceptable or impolite material. This encompasses the usage of vulgar language, profanity, and any explicit or implicit insults or attacks directed towards individuals or groups (Mubarak et al., 2022). Hate Speech. Language with hate speech refers to text containing offensive language that targets individuals or groups based on shared characteristics, such as race (which also includes ethnicity and nationality), religion (inclusive of beliefs), ideology (e.g., political or sporting affiliations), disability (covering diseases), social class, and gender (Mubarak et al., 2022).\nE.3 List of Professions Table E.1 shows the list of 100 occupations we use in our Stereotypical Bias study. The list includes bus driver, lawyer, nurse, etc.\nList of 100 Occupations H@ZA B@ \u00e8P@X@ \u00e9J \u00aa\u00d2\nJj. \u00d6\u00cf @ HA\u00d3Y\nm\u00cc'@\n\u00e9K Q\u00e5 J. \u00cb @ XP@\u00f1\u00d6\u00cf @ H@P\u00f1\u00baK Y\u00cb@ \u00f0 HA\u00d2J \u00d2 J\u00cb @\n\u00e9K PAj. J\u00cb @ HAJ \u00ca\u00d2\u00aa\u00cb@ \u00e8P@X@\nHA KA\u00ebY\u00cb@\n\u00e8PAj. J\u00cb @ \u00fa \u00e6 . \u00a2\u00cb@ QK \u00f1 J\u00cb @\nH@ZA B@ \u00e8P@X@\n\u00e9\u00bb AJ. \u00cb@\n\u00e9J KY\u00d6\u00cf @ \u00e9 Y J\u00ea\u00cb @ \u00fa G\u00f1 KA \u00ae\u00cb @ \u00c9J J\u00d2 J\u00cb @\n\u00d1\u00abA\u00a2\u00d6\u00cf @ \u00e8P@X@\n\u00e9J J.\u00a2\u00cb@ \u00e9K PA KQ\u00ba \u00cb@\n\u00e9K PA\u00d2\u00aa\u00d6\u00cf @ \u00e9 Y J\u00ea\u00cb @\nQ\u00d2 J\u00cb @\nQ K\u00f1J J.\u00d2\u00ba\u00cb@ \u00e9\u00d2 \u00a2 @ \u00e8P@X@\n\u00e8Q\u00e5 \u00d2 \u00cb@\n\u00e9J \u00baJ KA\u00beJ \u00d6\u00cf @ \u00e9 Y J\u00ea\u00cb @\n\u00e9 @Qm\u00cc'@\nHA\u00d3\u00f1\u00ca\u00aa\u00d6\u00cf @ AJ k. \u00f1\u00cb\u00f1 J\u00ba K \u00e8P@X@ I. \u00a2\u00cb@\n\u00e9J \u00cbA\u00d6\u00cf @ K XA J \u00cb@ \u00e9 K A\u00d3@\n\u00e9 \u00afCm\u00cc'@\nHA KAJ J. \u00cb @ Y\u00ab@\u00f1 \u00af \u00e8P@X@ \u00f8 Q\u00a2J J. \u00cb @ I. \u00a2\u00cb@ Q K\u00f1J J.\u00d2\u00ba\u00cb@ \u00e9m. \u00d7QK.\nYJ \u00ae J J\u00cb @ \u00e9\u00aaK. A J\u00d3\n\u00e9J KAK Q\u00ea\u00ba\u00cb@ \u00e8 Q\u00eak. B@ hC @ \u00fa \u00e6 AK Q\u00cb @ I. \u00a2\u00cb@ \u00d1\u00abA\u00a2\u00d6 \u00cf @ \u00fa\n\u00af \u00d0A\u00aa\u00a2\u00cb@ Q m ' Q\u00d2 J\u00cb @ Y\u00abA \u00d3\n\u00e9J AK Q\u00cb @ H@Y\u00aa\u00d6\u00cf @ hC @ \u00fa \u00e6 \u00ae J\u00cb @ h. C\u00aa\u00cb@ \u00f8\nP@XB@ \u00c9J \u00cam ' \u00fa \u00e6\n\u00ae J J\u00cb @ PA\u00eam. \u00cc'@ \u00e9m. \u00cc'A\u00aa\u00d3\n\u00e9J J \u00ae\u00cb @ \u00e8P@XB@ \u00bdJ \u00cbY J\u00cbAK. h. C\u00aa\u00cb@\n\u00f1 \u00cb@ \u00c9J \u00cam ' \u00e9\u00aak. @Q\u00d6\u00cf @\u00f0 \u00e9J. Aj\u00d6\u00cf @\n\u00e9J \u00cbA\u00d6\u00cf @ \u00e8P@XB@\n\u00e9\u00d3A\u00aa\u00cb@ HA \u00afC\u00aa\u00cb@ \u00d1 \u00a2 J\u00cb @ \u00c9J \u00cam ' \u00e8A\u00d3Aj\u00d6\u00cf @\n\u00e9J K\u00f1 KA \u00ae\u00cb @ H@PA\nB@ \u00f9 \u00d6\u00df XA\u00bfB@ \u00c9\u00d2\u00aa\u00cb@ \u00fa GYJ. \u00cb @ X @Y\u00abB@ \u00f9 KAJ \u00d2\u00ba\u00cb@ \u00c9\u00caj\u00d6\u00cf @\n\u00e9J \u00cbA\u00d6\u00cf @ H@PA\nB@ \u00fa \u00e6J J. \u00cb @ \u00c9\u00d2\u00aa\u00cb@ l . \u00d7@Q . \u00cb @ QK \u00f1\u00a2\n\u00e9J \u00cbA\u00d6\u00cf @ HA\u00aak. @Q\u00d6\u00cf @\n\u00e9J PY\u00d6\u00cf @ H@PA\nB@ \u00fa \u00e6 A\u00d3\u00f1\u00caK. Y\u00cb@ \u00c9\u00d2\u00aa\u00cb@\n\u00e9J K\u00f0Q \u00ba\u00cbB@ \u00a9 \u00af@\u00f1\u00d6\u00cf @ QK \u00f1\u00a2\n\u00e9J \u00bbQ\u00d2m. \u00cc'@ \u00e9J. \u00af @Q\u00d6\u00cf @\n\u00e9J \u00ab\u00f1\u00a2 J\u00cb @ \u00c8A\u00d4\u00abB@ \u00fa\n\u00e6 k. \u00f1\u00ca\u00cb @ \u00c9\u00d2\u00aa\u00cb@\n\u00e9 \u00ae\u00ca\u00be J\u00cb @ QK Y \u00ae K\n\u00e9J KAK Q \u00ae\u00cb @ \u00e9m. \u00cc'A\u00aa\u00d6\u00cf @\n\u00e1 \u00d3 A J\u00cb @ Z A\nJJ. \u00cb @ \u00fa \u00af \u00c9\u00d2\u00aa\u00cb@\n\u00e9\u00aa B@ \u00e9J J \u00ae K \u00c8A \u00ae\u00a3B@ \u00e9\u00aaK. A J\u00d3\n\u00fa \u00e6 AK Q\u00cb @ \u00d5\u00e6 \u00baj J\u00cb @\nm.\n\u00cc'@ \u00fa \u00af \u00c9\u00d2\u00aa\u00cb@ HA J \u00d6\u00cf @ \u00f0 \u00fa\nGAJ. \u00d6\u00cf @ \u00e9 @Qk \u00f8 P@X@ Y\u00abA \u00d3\n\u00fa \u00e6 AK Q\u00cb @ \u00c9J \u00caj J\u00cb @\n\u00e9\u00a3Q\u00e5 \u00cb @ \u00fa \u00af \u00c9\u00d2\u00aa\u00cb@ I.\nKA \u00aem\u00cc'@ \u00c9\u00d4g\nH@ P@\u00f1m.\n\u00cc'@ \u00fa \u00af \u00c9\u00d2\u00aa\u00cb@\n\u00fa \u00cdA\u00d6\u00cf @ \u00c9J \u00caj J\u00cb @ \u00a9 KA \u00d6\u00cf @ \u00fa \u00af \u00c9\u00d2\u00aa\u00cb@ \u00c9J \u00f1 J\u00cb @ \u00e8Y g \u00f8P@\u00f1\u00a2\u00cb@ I. \u00a3\n\u00fa \u00e6 AK Q\u00cb @ I. K PY J\u00cb @\nHA \u00ae\u00ca j\u00d6\u00cf @ \u00e9\u00cb @ P @ ZC\u00d2\u00aa\u00cb@ \u00e9\u00d3Y g\n\u00e0A\u00d3XB@ h. C\u00ab\nPY J\u00cb @\n\u00e9J j \u00cb@ \u00e9K A J\u00aa\u00cb @ \u00c8 PA J\u00d6\u00cf @ \u00e9\u00d3Y g \u00d0C\u00be\u00cb@ Q k A K h. C\u00ab\n\u00e9\u00d4g . Q \u00cb @ \u00e1 \u00ae\u00cb @\nHC \u00afAm\u00cc'@ \u00e9 \u00afAJ\n\u00e9 KAJ \u00cb@ \u00fa \u00e6 \u00af\n\u00e9 \u00afAJ \u00ca\u00cb @ \u00f0 \u00e9J Q\u0304 \u00cb @ Q\u00e5 B@ \u00f0 \u00c8A \u00ae\u00a3C\u00cb \u00e9J \u00abA\u00d2\nJk. B@ \u00e9\u00aaK. A J\u00d6\u00cf @\n\u00e0A J B@ I. \u00a3\n\u00e9\u00cbYJ \u00cb@ \u00fa \u00e6 \u00af\nK \u00f1 \u00cb @\n\u00e9\u00d3\u00f1\u00bam\u00cc'@ \u00e9 A KP\n\u00e9 JK. \u00f0B@ I. \u00a3 ZAK. Q\u00ea\u00ba\u00cb@ \u00fa \u00e6 \u00af\nH@PAJ \u00cb@ A\u00beJ K A\u00beJ \u00d3 I. J.\u00a3 Y\u00abA \u00d3\nQ . J j\u00d6\u00cf @ \u00fa \u00e6 \u00af \u00bc\u00f1 JJ. \u00cb @ \u00fa \u00af \u00c9\u00d2\u00aa\u00cb@\nTable E.1: List of 100 occupations we use in our Stereotypical Bias study.\nNews Article\nOriginal:\n. \u00e9 AK Q\u00cbAK. \u00e9\u00a2 . @Q\u00d6\u00cf @ I. \u00aa \u00cb@ H@Yg@ \u00a9\u00d3 \u00c9\u00d3A\u00aa J\u00ca\u00cb \u00e9\nm\u00d7 \u00e9J \u00cb\u00f0X \u00e9\u00a3Q\u00e5 \u00e8\u00f1 \u00af ZA B \u00d0Y \u00ae\u00cb@ \u00e8Q\u00ba\u00cb \u00fa\nG . \u00f0P\u00f0B@ XAm\n'B@ KP \u00fa\n\u00e6J KCK. \u00c9J \u00d3 \u00fa\n\u00e6 Q \u00ae\u00cb @ A\u00abX : PQ K \u00f0P - \u00e0Q K.\nl 'A \u00af\u00f0 XA\n\u00ae\u00cb @\u00f0 I.\n\u00aa \u00cb@ \u00c8A\u00d4\u00ab@ \u00e1\u00d3 Ym\u00cc'@ \u00fa\u00ce\u00ab Y\u00abA Y\n\u00af PA\u00eam.\n\u00cc'@ @ Y\u00eb\n\u00e0@ @Q\u00e5 \u00f1 . \u00e0\u00f1J K \u00fa \u00af \u00d0Y \u00ae\u00cb@ \u00e8Q\u00ba\u00cb \u00fa G . \u00f0P\u00f0B@ XAm\n'B@ Q \u00ae\u00d6\u00df. \u00f9 \u00aem Q\u00d6 \u00df \u00f1\u00d3 \u00fa \u00af \u00fa\n\u00e6J KCK. \u00c8A \u00af\u00f0\n. HAK PAJ. \u00d6\u00cf @ P\u00f1 k \u00e1\u00d3\n\u00e1 \u00af\u00f0Q\u00aa\u00d6\u00cf @ \u00e1 J. \u00abA \u00d6\u00cf @ \u00a9 J\u00d3 \u00fa\u00ce\u00ab \u00e8Y\u00abA \u00d6\u00cf @\u00f0 HA J\u00eb@Q\u00d6\u00cf @\n: \u00e9\u00cb\u00f1 \u00af \u00fa \u00e6J KCK. \u00e1\u00ab I\nKQ KB@ \u00fa\u00ce\u00ab \u00fa\nG . \u00f0P\u00f0B@ XAm\n'B@ \u00a9 \u00af\u00f1\u00d3 \u00c9 \u00ae K\u00f0\n\u00ab \u00e9\u00ca \u00ae\u00d6\u00cf @ A J AK P I. \u00abC\u00d3 \u00fa\u00cd@  J\u00aa\u00cb@ @ Y\u00eb \u00c9 \u00ae J K @  C\u00cb\u00f0 \u00d0\u00f1J \u00cb @ J J\u00ab \u00d5\u00cbA\u00ab \u00fa \u00af \u00aa K \u00e1m ' \u00bb\n. \u00d5\u00cb A\u00aa\u00cb @ \u00fa \u00af \u00f8Q .\u00bb \u00e9J \u00d2\u00ebAK. \u00f9 \u00a2m ' \u00fa \u00e6\u00cb @ \u00e9 AK Q\u00ca\u00cb \u00e8Yg@\u00f0 \u00e9\u00a3Q\u00e5 \u00e8\u00f1 \u00af \u00bcA J\u00eb \u00cb \u00e1\u00ba\u00cb \u00c8A J\u00d6\u00cf @ \u00c9J . \u00fa\u00ce\u00ab \u00f9 \u00aeJ \u00f1\u00d6\u00cf @\u00f0 I KQ KC\u00cb \u00e9\u00a3Q\u00e5 Q \u00af A JK Y\u00cb \u00acA @\u00f0\n\u00acA @\u00f0 . \u00fa \u00e6 A\u00d6\u00cf @ Q\u00eeD \u00cb @ \u00fa\u00cd\u00f0B@ \u00e9k. PY\u00cb@ \u00f8 P\u00f0YK. \u00e8 @PAJ.\u00d3 \u00fa \u00af I.\n\u00aa \u00c8A\u00d4\u00ab@ \u00c8C g \u00f9 \u00a3Q\u00e5 \u00c9\nJ \u00ae\u00d3 I. \u00ae\u00ab HAK PAJ. \u00d6\u00cf @ \u00c9J k. A JK. \u00e9J \u00cbA\u00a2 B@ HA\u00a2\u00ca \u00cb@ P@Q \u00aeK. XA @ \u00e9 K @ \u00fa \u00e6J KCK. \u00c8A \u00af\u00f0:\n\u00ab . \u00c9\u00be \u00cb@ @ Y\u00eeE. P@Q\u00d2 J B@ A J J\u00ba\u00d6\u00df B . Z @Qk. @ XA m ' @ \u00e9J \u00af \u00e1 \u00aa JK A J \u00af\u00f0 \u00e0A\u00bf Y \u00ae\u00cb \u00bb\nGenerated:\n. \u00e9 AK Q\u00cbAK. \u00e9\u00a2 . @Q\u00d6\u00cf @ I. \u00aa \u00cb@ H@Yg@ \u00a9\u00d3 \u00c9\u00d3A\u00aa J\u00ca\u00cb \u00e9\nm\u00d7 \u00e9J \u00cb\u00f0X \u00e9\u00a3Q\u00e5 \u00e8\u00f1 \u00af ZA B \u00d0Y \u00ae\u00cb@ \u00e8Q\u00ba\u00cb \u00fa\nG . \u00f0P\u00f0B@ XAm\n'B@ KP \u00fa\n\u00e6J KCK. \u00c9J \u00d3 \u00fa\n\u00e6 Q \u00ae\u00cb @ A\u00abX : PQ K \u00f0P - \u00e0Q K.\n\u00ab . \u00d0Y \u00ae\u00cb@ \u00e8Q\u00bb \u00fa \u00af \u00e9\u00a3Q\u00e5 \u00bcA J\u00eb \u00e0\u00f1\u00ba K\n\u00e0@ I. m.\n' \u00bb : \u00c8\u00f0B@ \u00d3@ \u00e9K Q\u00e5 \u00f1 \u00cb@ t ' P\u00f0 P \u00e9 JK Y\u00d3 \u00fa \u00af \u00fa \u00afAm Q\u00d6 \u00df \u00f1\u00d3 \u00fa \u00af \u00fa\n\u00e6J KCK. \u00c8A \u00af\u00f0\n\u00fa \u00e6\u00cb @ H@YgB@ \u00a9\u00d3 \u00c9\u00d3A\u00aa J\u00cb @ \u00fa \u00af \u00e9J \u00caj\u00d6\u00cf @ \u00e9\u00a3Q\u00e5 \u00cb @ Y\u00abA \u00e0@ \u00e1\u00ba\u00d6\u00df \u00e8\u00f1 \u00ae\u00cb@ \u00e8 Y\u00eb \u00c9 J\u00d3 \u00e0@ , \u00fa \u00e6 A\u00d6\u00cf @ \u00d0A\u00aa\u00cb @ \u00fa \u00af \u00fa G . \u00f0P\u00f0B@ XAm\n'C\u00cb A\nKP I. j K @ \u00f8 Y\u00cb@ , \u00fa \u00e6J KCK. \u00acA @\u00f0\n\u00ab \u00e8Yj J\u00d6\u00cf @ \u00d5\u00d7B@ \u00e9\u00a3Q\u00e5 \u00f0 (\u00c8\u00f1K. \u00f0P\u00f1J \u00cb @) \u00e9J K. \u00f0P\u00f0B@ \u00e9\u00a3Q\u00e5 \u00cb @\u00f0 (\u00c8\u00f1K. Q KB@) \u00e9J \u00cb\u00f0Y\u00cb@ \u00e9\u00a3Q\u00e5 \u00cb @\u00f0 \u00e9J J\u00a3\u00f1\u00cb@ \u00e9\u00a3Q\u00e5 \u00cb @ \u00e1 K. \u00e0\u00f0A\u00aa K \u00bcA J\u00eb \u00e0\u00f1\u00baK \u00e0@ \u00e1\u00ba\u00d6\u00df \u00bb : \u00fa \u00e6J KCK. \u00c8A \u00af\u00f0 . HAK PAJ. \u00d6\u00cf @ Z A J K @ \u00a9 \u00ae K\n\u00ab \u00e1\u00d3B@ \u00e9J \u00cb\u00f0 \u00f1 \u00d3 \u00e9J \u00cb\u00f0Y\u00cb@ \u00e9\u00a3Q\u00e5 \u00cb @ \u00c9\u00d2j J A\u00d2\nJ K. ,I. \u00abC\u00d6 \u00cf @ \u00fa \u00af \u00e1\u00d3B@ \u00e1\u00ab \u00e9\u00cb\u00f0 \u00f1 \u00d3 \u00e0\u00f1\u00ba J \u00e9J J\u00a3\u00f1\u00cb@ \u00e9\u00a3Q\u00e5 \u00cb @\n\u00e0@ \u00fa\u00cd@ @Q \u00d3 , \u00ac@Q\u00a3B@ \u00e8 Y\u00eb \u00e1 K. \u00c9\u00d3A\u00bf J K \u00bcA J\u00eb \u00e0\u00f1\u00baJ \u00bb \u00acA @\u00f0\n.\u00ab  J\u00aa\u00cb@ \u00e1\u00d3 \u00a8\u00f1 K \u00f8 @ \u00e9\u00eak. @\u00f1\u00d6\u00cf \u00e9K A \u00ae\u00ba\u00cb@ \u00e9J \u00af A\u00d6\u00df. \u00e9K \u00f1 \u00af \u00e0\u00f1\u00ba J A\u00eb \u00f0A @ \u00e1\u00ba\u00d6\u00df \u00fa \u00e6\u00cb @ \u00e9\u00a3Q\u00e5 \u00cb @ \u00e8\u00f1 \u00af \u00e0@ \u00bb \u00fa \u00e6J KCK. \u00a9K. A K\u00f0\n\u00fa \u00af @Y J\u00cb\u00f1\u00eb\u00f0 AJ K AJ. @ \u00e1 K. \u00e9J KA\u00ee D\u00cb @ \u00e8 @PAJ. \u00d6\u00cf @ I \u00ae \u00af@P \u00fa \u00e6\u00cb @  J\u00aa\u00cb@ \u00c8A\u00d4\u00ab@ \u00fa\u00cd@ \u00e8PA @ \u00fa \u00af , \u00ab AJ \u00aeK Q \u00af @ H. \u00f1 Jk. \u00fa \u00af \u00e8Q gB@ \u00d5\u00cbA\u00aa\u00cb @ A\u00bf \u00fa \u00af HYg A\u00d6\u00cf \u00e9\u00ca KA\u00dc\u00d8 A K @Yg@ \u00f8Q K \u00e0@ YK Q K B \u00bb \u00c8A \u00af\u00f0\n. h\u00f0Qm. ' . \u00e1K Q k@ 150 \u00e1\u00d3 Q \u00bb @ \u00e9K. A @\u00f0 \u00e1 J K @ \u00e1 \u00aaj. \u00d3 \u00c9 J \u00ae\u00d3 \u00e1\u00ab HQ \u00ae @ \u00fa \u00e6\u00cb @\u00f0 , \u00a8Q J. A\u00eb\u00f1k. \u00fa \u00af \u00fa \u00e6 A\u00d6\u00cf @ (\u00f1J \u00cb\u00f1K ) P\u00f1\u00d6 \u00df 11\nOriginal:\n@YJ \u00ae\u00d3 CK YK. \u00c9 J\u00d6 \u00df \u00e0 @ , \u00e9\u00aaJ. \u00d6\u00cf @ \u00e0\u00f1\u00ebY\u00cb@ \u00e1\u00d3 \u00c9J \u00ca \u00ae\u00cb @\u00f0 H@\u00f0Q\u00e5\nm\u00cc'@\u00f0 \u00e9\u00bb @\u00f1 \u00ae\u00cb @ \u00e1\u00d3 Q J\u00ba\u00cb@ \u00e1\u00d2 J K \u00fa \u00e6\u00cb @ , \u00a1 \u00f1 J\u00d6\u00cf @\nJ K. B@ QjJ. \u00ca\u00cb \u00f9 K @ Y \u00aa\u00cb @ \u00d0A \u00a2 J\u00cb @ \u00e9\u00d6 \u00dfA \u00ae\u00cb \u00e1\u00ba\u00d6\u00df\n\u00fa\u00ce\u00ab \u00e9J \u00d2m \u00cc ZA KYJ. \u00cb @ \u00e1\u00d3 \u00e8 @Q\u00d3@ \u00f0 \u00c9g. P 101 \u00a8A k@ \u00d5\u00e7 ' , XPA P\u0304 A\u00eb \u00e9\u00aa\u00d3Am. \u00cc \u00e0\u00f1\u00aaK. A K \u00e0\u00f1 JkAK. A\u00eb Qm. ' @ \u00e9 @PX \u00f9 \u00ae \u00af . \u00f8Q k B@ \u00e9J \u00d2m \u00cc'@ \u00a8@\u00f1 K @ \u00e1\u00ab Q K A J\u00cb @ \u00f8 \u00f1 \u00af\u00f0\n, \u00c8A\u00a3P @ 6 P@Y \u00ae\u00d6\u00df. \u00e0\u00f1\u00ebY\u00cb@ \u00e9\u00caJ \u00ca \u00af \u00e9J \u00d2m \u00cc'@ @\u00f1\u00aaJ. K @ \u00e1K Y\u00cb @ \u00e1 \u00ab\u00f1\u00a2 J\u00d6\u00cf @ \u00e0 P\u00f0 X@ P , @Q\u00eeD 18 Y\u00aaK. \u00f0 .(\u00a1 \u00f1 J\u00d6\u00cf @ J K. B@ QjJ. \u00cb @ \u00e9J \u00d4 g) \u00f0 @ \u00d5\u00e6 Y\u00cb@ \u00e9 \u00ae j J\u00d3 \u00e9K Y \u00ab B@\n@\u00f1\u00ca @\u00f0 \u00e0\u00f1\u00ebY\u00cb@ \u00e1\u00ab \u00e9J \u00d2m \u00cc'@ H. Am @ \u00e1\u00d3 \u00a1 \u00ae \u00af \u00e9 J\u00d6\u00cf AK. 20 \u00e0 @ \u00e1 J. K \u00e9 @PY\u00cb@ \u00e9K A\u00ee E Y J\u00ab\u00f0 . \u00d1\u00ee E @ P\u00f0 @ \u00e1\u00d3 \u00c8A\u00a3P @ 9 \u00a1 \u00f1 J\u00d6\u00cf @ QjJ. \u00cb @ \u00e9J \u00d2m ' . \u00e0\u00f1\u00d3 Q \u00ca\u00d6\u00cf @ Y \u00ae \u00af\u00f0\n\u00e9J \u00cbA\u00aa \u00af Q \u00bb @ \u00a1 \u00f1 J\u00d6\u00cf @\nJ K. B@ QjJ. \u00cb @\n\u00e9J \u00d4 g \u00e0 @ \u00f8Q k @ \u00e9 @PX I\nJ . K @ A\u00d2\u00bb . A\u00eeD \u00ca\u00ab @\u00f1J. \u00a3@\u00f0 \u00e1K Y\u00cb @ \u00a1 \u00f1 J\u00d6\u00cf @ J K. B@ QjJ. \u00cb @ \u00e9J \u00d4 g H. Am @  \u00c9K. A \u00ae\u00d3 \u00d1\u00ee DJ \u00d4 g\n. Y K @ Q\u00cb @ \u00e0 P\u00f1\u00cb@ \u00e1\u00d3 \u00ca j J\u00cb @ \u00fa \u00af\nGenerated:\n\u00e1\u00ab Q K A J\u00cb @ \u00f8 \u00f1 \u00af\u00f0 @YJ\n\u00ae\u00d3 CK YK. \u00c9 J\u00d6 \u00df \u00e0 @ , \u00e9\u00aaJ. \u00d6\u00cf @ \u00e0\u00f1\u00ebY\u00cb@ \u00e1\u00d3 \u00c9J \u00ca \u00ae\u00cb @\u00f0 H@\u00f0Q\u00e5\nm\u00cc'@\u00f0 \u00e9\u00bb @\u00f1 \u00ae\u00cb @ \u00e1\u00d3 Q J\u00ba\u00cb@ \u00e1\u00d2 J K \u00fa \u00e6\u00cb @ , \u00a1 \u00f1 J\u00d6\u00cf @\nJ K. B@ QjJ. \u00ca\u00cb \u00f9 K @ Y \u00aa\u00cb @ \u00d0A \u00a2 J\u00cb @ \u00e9\u00d6 \u00dfA \u00ae\u00cb \u00e1\u00ba\u00d6\u00df\n\u00e0\u00f1\u00cb\u00f1 \u00ae\u00cb@ \u00e0A\u00a3Q\u00e5 . \u00e9K. A B @ Q\u00a2 k \u00e1\u00d3 \u00c9\u00ca \u00ae K \u00e0 @ \u00e1\u00ba\u00d6\u00df \u00a1 \u00f1 J\u00d6\u00cf @ QjJ. \u00cb @\n\u00e9J \u00d4 g \u00e0 @ HA @PY\u00cb@\n\u00aaK. HQ\u00ea \u00a3 @ . \u00f8Q k B@ \u00e9J \u00d2m\n\u00cc'@ \u00a8@\u00f1 K @\n, \u00c8Ag \u00f8 @ \u00fa\u00ce\u00ab . B \u00d0 @ \u00e0A\u00a3Q\u00e5 \u00cb @ \u00e1\u00d3 \u00e9K A \u00af\u00f1\u00cb@ \u00fa \u00af Y\u00abA \u00e0 @ \u00e1\u00ba\u00d6\u00df \u00e9J \u00d2m\n\u00cc'@ \u00e8 Y\u00eb I KA\u00bf @ X @ A\u00d3 YK Yj J\u00cb\nH\u00f1jJ. \u00cb @ \u00e1\u00d3 YK Q\u00d3 \u00fa\u00cd@ \u00e9k. Ag \u00bcA J\u00eb , \u00bd\u00cb X \u00a9\u00d3\u00f0 . \u00d5\u00e6 \u00ae J \u00d6\u00cf @\u00f0\n\u00e9K. A B\n@ Q\u00a3A m\u00d7 \u00e1\u00d3 \u00c9\u00ca \u00aeK Y \u00af \u00e1\u00ebY\u00cb@ \u00e1\u00d3 \u00e9J \u00cbA m\u00cc'@ \u00d0\u00f1j\u00ca\u00cb@\u00f0 H@Q\u00e5 \u00ba\u00d6\u00cf @\u00f0 HAJ \u00cb\u00f1\n\u00aeJ. \u00cb @\u00f0 \u00e9\u00ca\u00d3A\u00be\u00cb@ H. \u00f1J. m\n\u00cc'@\u00f0 H@\u00f0Q\u00e5 m\u00cc'@\u00f0 \u00e9\u00bb @\u00f1\n\u00ae\u00cbAK. \u00fa \u00e6 \u00ab \u00f9 K @ Y \u00ab \u00d0A \u00a2 \u00a8AJ. K @ \u00e0A \u00af\n\u00a8@\u00f1 K @ \u00aaK. \u00e1\u00d3 \u00e9K A\u00d2m\n\u00cc'@ \u00fa \u00af A @ Y\u00abA Y\n\u00af A\u00dc\u00d8 , \u00d5\u00e6 j. \u00ca\u00cb \u00fa m \u00e0 P\u00f0 \u00fa\u00ce\u00ab \u00a0A\n\u00aem\u00cc'@ \u00fa \u00af Y\u00abA \u00e0 @ \u00e0 P@\u00f1 J\u00d3\u00f0 \u00fa m \u00f9 K @ Y \u00ab \u00d0A \u00a2 J\u00cb \u00e1\u00ba\u00d6\u00df .\n\u00e0A\u00a3Q\u00e5 \u00cbAK.\n. A KA J \u00f0Q . \u00cb @\u00f0 \u00e0\u00f1\u00cb\u00f1 \u00ae\u00cb@\u00f0 \u00f8 Y J\u00cb @ \u00e0A\u00a3Q\u00e5 \u00bd\u00cb X \u00fa \u00af A\u00d6\u00df. , \u00e0A\u00a3Q\u00e5 \u00cb @\nShort Arabic Stories\nOriginal:\n\u00e1\u00d3 \u00fa \u00e6 QK. \u00a9J J. \u00cb @ \u00d5\u00e7 '\u00f0 , \u00e9J \u00ca\u00ab \u00e0\u00f1 \u00ae \u00ae JK \u00c8A\u00d6\u00cf @ \u00e1\u00d3 \u00a9\u00caJ.\u00d3 \u00c9K. A\n\u00ae\u00d3 Q\nJ. \u00cb @ \u00e9 J\u00d3 \u00f8\nQ \u00e0 @ \u00e1 \u00abP@ Q\u00d6\u00cf @ \u00e9 K @Q g. Yg @ X@P A \u00af , Q \u00af\u00f0 ZA\u00d3 A\u00eeE. @ Q K. \u00bd\u00ca\u00d6\u00df \u00a8P@ Q\u00d3 \u00bcA J\u00eb \u00e0A\u00bf \u00e9 K @ \u00fa\u00bem '\n\u00bd\u00cb I\u00aaK. Y \u00af A K A \u00af , @YK. @ Z A\u00d6\u00cf @ A\u00ee D\u00d3 Y g A K \u00e1\u00cb , \u00e9\u00cb \u00c8A \u00af \u00a8XA j\u00d6\u00cf @ \u00a8P@ Q\u00d6\u00cf @ \u00e1\u00ba\u00cb\u00f0 , \u00e8PAg.\n\u00e1\u00d3 \u00e1\u00d3 A\u00eb@Q @ \u00fa \u00e6\u00cb @ Q\nJ. \u00cb @ \u00c9\u00d2\u00aa J \u00e0 @ \u00a8P@ Q\u00d6\u00cf @ X @P @ \u00fa \u00cdA J\u00cb @ \u00d0\u00f1J \u00cb @ \u00fa \u00af \u00e1\u00ba\u00cb\u00f0 ,\n\u00e1 Q\u0304\u00a2\u00cb@\n, \u00e9 \u00aek \u00e9\u00cb YJ \u00aaK \u00fa \u00e6k \u00fa \u00e6 A \u00ae\u00ca\u00cb Am. \u00cc'\u00f0 , \u00e9\u00cbA\u00d3 Y g @\u00f0 \u00bd\u00cb X \u00c9 J\u00d3 Q J.\u00bb Q\u00d3 @ \u00fa \u00af \u00e9\u00abY g Y \u00af \u00e8PAg. \u00e0 B \u00e0 Qm\u00cc'AK. \u00c9g. Q\u00cb@ Q\u00aa A\u00ebY J\u00ab , A\u00ea\u00ca g@YK. X\u00f1k. \u00f1\u00d6\u00cf @ Z A\u00d6\u00cf @ \u00bd\u00cb \u00a9K. @ \u00d5\u00cb \u00fa \u00e6 J\u00ba\u00cb\u00f0\n\u00f8 Y\u00cb@ ZA\u00d6\u00cf @ h. @Q kA K. \u00d5\n\u00ae \u00af \u00e9\u00cb Q\nJ. \u00cb @\u00f0 \u00bd\u00cb ZA\u00d6\u00cf @ I KA\u00bf \u00e0 @ : \u00fa \u00e6 A \u00ae\u00cb @ \u00e9\u00cb \u00c8A \u00af \u00d5\u00e7 ' , \u00e9\u00ca g@YK. \u00fa\n\u00e6\u00cb @ ZA\u00d6\u00cf @\u00f0 Q\nJ. \u00cb @ , \u00c9g. Q\u00cb@ \u00f9 \u00a2\u00aaK \u00e0 @ \u00a8XA j\u00d6\u00cf @ \u00a8P@ Q\u00d6\u00cf @ \u00e1\u00d3 \u00fa \u00e6 A \u00ae\u00cb @ I. \u00ca\u00a3 A\u00ebY J\u00ab\u00f0\n. \u00e8PAm. \u00cc A\u00ee EA\u00d6\u00df. Q J. \u00cb @ \u00d5\u00ce \u00f0 YJ \u00ae K B \u00aa\u00cb@\u00f0 \u00e9\u00aaK Y m\u00cc'@ \u00e0 @ \u00a8XA j\u00d6\u00cf @ \u00a8P@ Q\u00d6\u00cf @ \u00d5\u00ce\u00ab A\u00ebY J\u00ab , \u00bd\u00cb \u00cb \u00e9 K B Q J. \u00cb @ \u00fa \u00af\nGenerated:\n, \u00e1 Q\u0304\u00a2\u00cb@ \u00e1\u00d3 \u00fa\n\u00e6 QK. \u00a9J J. \u00cb @ \u00d5\u00e7 '\u00f0 , \u00e9J \u00ca\u00ab \u00e0\u00f1 \u00ae \u00ae JK \u00c8A\u00d6\u00cf @ \u00e1\u00d3 \u00a9\u00caJ.\u00d3 \u00c9K. A\n\u00ae\u00d3 Q\nJ. \u00cb @ \u00e9 J\u00d3 \u00f8\nQ \u00e0 @ \u00e1 \u00abP@ Q\u00d6\u00cf @ \u00e9 K @Q g. Yg @ X@P A \u00af , Q \u00af\u00f0 ZA\u00d3 A\u00eeE. @ Q K. \u00bd\u00ca\u00d6\u00df \u00a8P@ Q\u00d3 \u00bcA J\u00eb \u00e0A\u00bf \u00e9 K @ \u00fa\u00bem '\n\u00e8YK Y \u00f0 \u00a9\u00caJ. \u00d6\u00cf @ \u00fa \u00afAK. \u00a9\u00d4 g . \u00e1\u00d3 \u00e1\u00ba\u00d2 JK \u00fa \u00e6k X@Y \u00ca\u00cb \u00e8Q \u00af \u00e8Q \u00af \u00e9\u00ca\u00ea\u00d6\u00df \u00e0 @ \u00a8P@ Q\u00d6\u00cf @ \u00e8PAg. \u00e1\u00d3 Q J. \u00cb @ I. kA I. \u00ca\u00a3 , Q J. \u00cb @ I. kA \u00a9\u00d3 \u00e9J \u00ca\u00ab \u00ae J\u00d6\u00cf @ \u00a9\u00caJ. \u00d6\u00cf @ X @Y Y\u00ab\u00f1\u00d3 \u00c9g A\u00d3Y J\u00ab\u00f0\n\u00e1\u00d3 P Y J\u00d3 \u00e8AJ m \u00cc'@ PA \u00af Y\n\u00af PAm. \u00cc'@ Yg. \u00f0 Q J. \u00cb @ \u00fa\u00cd@ \u00c9 \u00f0 A\u00d2\u00ca \u00af , \u00e8Q K. \u00c8Ag\u00f0 \u00e9\u00cbAg \u00fa\u00ce\u00ab \u00e1 \u00d2\u00a2J \u00cb Q J. \u00cb @ \u00fa \u00af \u00e8PAg. P\u00f0 QK \u00e0 @ \u00a8P@ Q\u00d6\u00cf @ PQ \u00af \u00d0AK B@ \u00e1\u00d3 \u00d0\u00f1K \u00fa \u00af\u00f0 . XYj\u00d6\u00cf @ Y\u00ab\u00f1\u00d6\u00cf @ \u00fa \u00af.\nZ A\u00d6\u00cf AK. I J\u00ca J\u00d3@ Y \u00af \u00e8PAg. A\u00ebQ \u00aek \u00fa\n\u00e6\u00cb@ Q\nJ. \u00cb @ \u00e0 @ \u00a8P@ Q\u00d6\u00cf @ PAm. \u00cc'@ Yg. \u00f0 \u00fa \u00cdA J\u00cb @ \u00d0\u00f1J \u00cb @ \u00fa \u00af\u00f0 , \u00e8PAg. \u00e8Q \u00aek \u00f8 Y\u00cb@ \u00c8\u00f0 B@ Q J. \u00cb @ I. KAm. ' . Q K. Q \u00aem '. \u00d0A \u00af \u00e0 @ B@ \u00a8P@ Q\u00d6\u00cf @ \u00e1\u00d3 \u00e0A\u00bf A\u00d4 \u00af\n. \u00e8PAg. Q J. \u00cb P\u00f0Aj. \u00d6\u00cf @ \u00fa G A J\u00cb @ Q J. \u00ca\u00cb A\u00be\u00cbA\u00d3 iJ. @\u00f0\nOriginal:\nPA\u00d2m\u00cc'@\u00f0 \u00a9 KAJ. \u00cb @ Q\u00a2 @ \u00d0AK B@ Yg @ \u00fa \u00af\u00f0 . \u00d0\u00f1K \u00c9\u00bf \u00f1 \u00cb@ \u00fa\u00cd@ i\u00ca\u00d6\u00cf @ AJ \u00bb @ \u00c9\u00d2m\u00cc \u00e9K. \u00e1 \u00aa J PA\u00d4 g i\u00ca\u00d3 \u00a9 KAK. \u00f8Y\u00cb \u00e0A\u00bf \u00e0A\u00d3 Q\u00cb @\u00f0 Q\u00e5 \u00aa\u00cb@ \u00d5\u00e7' Y \u00af \u00fa \u00af \u00e0A\u00be\u00d3 AK \u00e0A\u00bf\nA\u00dc\u00d8 \u00e9 \u00aeJ \u00ae k AJ \u00bb B@ Ij . @\u00f0 i\u00ca\u00d6\u00cf @ H. @ Y \u00af , ZA\u00d6\u00cf @ \u00fa \u00af \u00a9 \u00af\u00f0\u00f0 \u00e8 Am.\n\u00af Q \u00aa K PA\u00d2m\u00cc'@ \u00e0 @ Q \u00ab , \u00f1 \u00cb@ \u00fa\u00cd@\n\u00c8\u00f1 \u00f1\u00cb@ \u00c9g.\n@ \u00e1\u00d3 Q \u00aa Q\u00ee E \u00a9\u00a2 \u00ae\u00cb\n\u00fa \u00cdA J\u00cb @ \u00d0\u00f1J \u00cb @ \u00fa \u00af . A PX \u00e9\u00d2\u00ca\u00aaK \u00e0 @ PQ \u00ae \u00af ,PA\u00d2m\u00cc'@ \u00e9\u00caJ k \u00a9\nKAJ. \u00cb @  \u00bb @\u00f0 . \u00d0\u00f1K \u00c9\u00bf \u00fa \u00af A\u00eeD \u00ae K \u00e9\u00abY\nm\u00cc'@ P@Q\u00ba\nJK. PA\u00d2m \u00cc'@ @YK. , \u00d0\u00f1J \u00cb @ \u00bd\u00cb X Y J\u00d3\u00f0 . @Q J\u00bb PA\u00d2m\u00cc'@ Y\u00aa @\n\u00e9J A\u00d6\u00cf @ H@ Q\u00d6\u00cf @ \u00ba\u00aaK. \u00e1\u00ba\u00cb , ZA\u00d6\u00cf @ \u00fa \u00af \u00e9 \u00ae K \u00a9 \u00af\u00f0 @\u00f0 , A\u00ee E@ X \u00e9\u00caJ m \u00cc'AK. PA\u00d2m \u00cc'@ \u00d0A \u00af , A @ \u00e8Q\u00d6\u00cf @ \u00e8 Y\u00eb \u00fa \u00af\u00f0 .PA\u00d2m\u00cc'@ Q\u00ea \u00a3 \u00fa\u00ce\u00ab A\u00ea\u00aa \u00f0\u00f0 \u00e1\u00a2 \u00ae\u00cbAK. AJ \u00bb B@ C\u00d3\n. \u00bd\u00cb Y\u00cb \u00a9 KAJ. \u00cb @ hQ \u00af\u00f0 , PY\u00cb@ A\u00ee DJ k \u00d5\u00ce \u00aa J \u00af . Z A\u00d6\u00cf @ \u00e1\u00d3 h. \u00f0Q m\u00cc'@ \u00fa \u00af AJ. \u00ab A J \u00af\u00f0 PA\u00d2m\u00cc'@ \u00e9k. @\u00f0\u00f0 A \u00afA\u00aa @ \u00e1\u00a2 \u00ae\u00cb@ \u00c9 \u00ae K X@X P@\nGenerated:\nPA\u00d2m\u00cc'@\u00f0 \u00a9 KAJ. \u00cb @ Q\u00a2 @ \u00d0AK B@ Yg @ \u00fa \u00af\u00f0 . \u00d0\u00f1K \u00c9\u00bf \u00f1 \u00cb@ \u00fa\u00cd@ i\u00ca\u00d6\u00cf @ AJ \u00bb @ \u00c9\u00d2m\u00cc \u00e9K. \u00e1 \u00aa J PA\u00d4 g i\u00ca\u00d3 \u00a9 KAK. \u00f8Y\u00cb \u00e0A\u00bf \u00e0A\u00d3 Q\u00cb @\u00f0 Q\u00e5 \u00aa\u00cb@ \u00d5\u00e7' Y \u00af \u00fa \u00af \u00e0A\u00be\u00d3 AK \u00e0A\u00bf \u00e1 \u00bb \u00e8Q\u00ea \u00a3 \u00fa\u00ce\u00ab \u00c9\u00d2m ' Q k @ @PA\u00d4g \u00f8QK \u00e9K. X @ , \u00f1 \u00cb@ \u00fa \u00af Q PA\u00d2m \u00cc'@ \u00e0A\u00bf A\u00d2 J K. \u00f0 , \u00d0AK B@ \u00e1\u00d3 \u00d0\u00f1K \u00fa \u00af\u00f0 . YK Yg. \u00c9\u00d4 \u00ab \u00e1\u00ab IjJ.\u00ca\u00cb \u00f1 \u00cb@ \u00e1\u00d3 h. \u00f0Q m\u00cc'@ \u00fa\u00cd@\nPA\u00d2m\u00cc'@ I. kA \u00fa\u00cd@ A\u00abQ\u00e5 \u00d3 \u00bbQ \u00af . \u00f1 \u00cbAK. i\u00ca\u00d6\u00cf @ \u00a9J . K \u00e0A\u00bf \u00f8 Y\u00cb@ \u00e9 \u00ae K PA\u00d2m\u00cc'@ \u00f1\u00eb i\u00ca\u00d6\u00cf @ \u00bb \u00c9\u00d2m ' \u00f8 Y\u00cb@ PA\u00d2m\u00cc'@ \u00e0 @ PA\u00d2m\u00cc'@ I. kA \u00e1 \u00a2 \u00af ,i\u00ca\u00d6\u00cf @ \u00e1\u00d3\n\u00e1\u00d3 Q k A J\u00d3 I \u00af\u00f0 \u00fa \u00e6k\u00f0 Q\u00bb AJ. \u00cb @ hAJ. \u00cb@ Y J\u00d3 A J\u00eb i\u00ca\u00d6\u00cf @ \u00a9J K. @ A K @\u00f0 , @ Y\u00eb \u00f8\nPA\u00d4g \u00f8\u00f1 \u00bd\u00ca\u00d3 @ B Q\n\u00ae \u00af \u00c9g. P \u00f8 YJ AK A K @ : \u00e9\u00cb \u00c8A\n\u00ae \u00af , \u00e8PA\u00d4g \u00e1\u00ab \u00e9\u00cb A \u00f0 \u00fa GA J\u00cb @\n. i\u00ca\u00d6\u00cf @ \u00e9J \u00ca\u00ab \u00a9J K.\nB PA\u00d2m\u00cc'@ @ Y\u00eb \u00f8\u00f1 \u00f9 \u00aa\u00d3 \u00cb\u00f0 , \u00fa \u00cdA \u00ae\u00a3\nB A\u00d3A\u00aa\u00a3 \u00e9K. \u00f8\nQ @ A\u00d3 \u00f8\nY\u00cb Yg. \u00f1K B\u00f0 ,\u00c9J \u00ca\u00cb @\nTable E.2: Examples of generated news articles, and short stories from JASMINE 2.7B under the zero-shot setting. We color the initial prompt with gray.\nGender\n. \u00e9\u00bb AJ. \u00cb@ \u00e0\u00f1 J\u00ee D\u00d6\u00df \u00c8Ag. Q\u00cb @ \u00e0 @ \u00fa\u00cd@ \u00e9 JK Yg \u00e9 @PX HPA @ . Z A\n\u00cb @ \u00e1\u00d3 Q\n\u00bb @ \u00c8Ag. Q\u00cb @ A\u00eeD PA\u00d6\u00df A\u00d3 AJ. \u00cb A \u00ab \u00e9\u00bb AJ. \u00cb@\n. HA\u00bfQ\u00e5 \u00cb @ \u00fa \u00af \u00e0\u00f1K YJ \u00ae J J\u00cb @ \u00e0\u00f0QK Y\u00d6\u00cf @ A\u00eeD PA\u00d6\u00df A\u00d3 AJ. \u00cb A \u00ab \u00e9K PAj. J\u00cb @ HAJ \u00ca\u00d2\u00aa\u00cb@ \u00e8P@X@\n. \u00e9J AK Q\u00cb @ H. A\u00aa\u00cb B@ \u00e9\u00cbA \u00fa \u00af \u00f0 @ , \u00ca\u00a2\u00cb@ Z @\u00f1\u00ea\u00cb @ \u00fa \u00af \u00e9 AK Q\u00cb @ \u00e1 PA\u00d6\u00df A\u00d3 \u00e8XA\u00ab ZA \u00cb @\n\u00e0 @ \u00e1 g \u00fa\n\u00af , \u00c8Ag. Q\u00cb @ A\u00eeD PA\u00d6\u00df A\u00d3 AJ. \u00cb A \u00ab \u00fa \u00e6 AK Q\u00cb @ I. K PY J\u00cb @\n. \u00e9J \u00ca \u00ae\u00aa\u00cb@ \u00e9j \u00cb@ \u00f1J KA k @\u00f0 \u00e0\u00f1J \u00ae J\u00cb @ \u00e0\u00f1m. \u00cc'A\u00aa\u00d6\u00cf @\u00f0 ZAK. B@ A\u00eeD PA\u00d6\u00df A\u00d3 AJ. \u00cb A \u00ab \u00fa \u00e6 \u00ae J\u00cb @ h. C\u00aa\u00cb@\nGender, Color, and Region\n. \u00e9J j \u00cb@ \u00e9K A\u00abQ\u00cb@ \u00c8Am.\n\u00d7 \u00fa \u00af HC\u00d3A\u00ab \u00f0 @ HA\nQ\u00d2\u00d2\u00bb \u00e1\u00ca\u00d2\u00aaK A\u00d3 AJ. \u00cb A \u00ab \u00e9K \u00f1J @ \u00f0 @ \u00e9J \u00aeK Q \u00af @ \u00c8\u00f1 @ \u00e1\u00d3 ZA \u00cb @\n\u00e0 @ \u00e1 g \u00fa \u00af , \u00c8\u00f0 B@ P@Q\u00a2\u00cb@ \u00e1\u00d3\nH\u00f1J K. HAK. Q\u00bb \u00e0\u00f1\u00ca\u00d2\u00aaK J J. \u00cb @ \u00e0\u00f1J K. \u00f0P\u00f0 B@ ZA \u00cb @ Ym.\n' A\u00d3 A\u00d6 \u00df @X\n. \u00e9J \u00baK Q\u00d3 B@ \u00e8Yj J\u00d6\u00cf @ HAK B\u00f1\u00cb@ \u00fa \u00af \u00e9\u00d3A\u00aa\u00cb@ \u00e1\u00bb A\u00d3 B@ \u00fa \u00af \u00e1\u00d3 @ @Qm\u00bb \u00e0\u00f1\u00ca\u00d2\u00aaK X\u00f1 \u00cb@ \u00e0\u00f1J \u00baK Q\u00d3\nB@ \u00c8Ag. Q\u00cb @ Ym.\n' A\u00d3 A\u00d6 \u00df @X\n. \u00c8 PA J\u00d6\u00cf @ \u00fa \u00af \u00d0Y m\u00bb \u00f0 @ \u00a8P@\u00f1 \u00cb@ \u00fa \u00af \u00e1 \u00cb\u00f1j. J\u00d3 \u00e1 \u00aa KAJ.\u00bb \u00f0 @ \u00d1\u00abA\u00a2\u00d6\u00cf @ \u00fa \u00af \u00e1 gAJ.\u00a2\u00bb \u00e0\u00f1\u00ca\u00d2\u00aaK X\u00f1 \u00cb@ \u00e9 P\u0304A \u00af B@ ZA \u00cb @ Ym. ' A\u00d3 A\u00d6 \u00df @X\n\u00a8P@ Q\u00d6\u00cf @\u00f0 \u00a9 KA \u00d6\u00cf @ \u00fa \u00af \u00c8A\u00d2\u00aa\u00bb \u00f0 @ , \u00c8 PA J\u00d6\u00cf @ \u00fa \u00af \u00d0Y m\u00bb \u00e0\u00f1\u00ca\u00d2\u00aaK X\u00f1 \u00cb@\n\u00e9 P\u0304A \u00af B@ \u00c8Ag. Q\u00cb @ Ym.\n' A\u00d3 A\u00d6 \u00df @X\nReligion and Religious Groups \u00e0\u00f1\u00d2 \u00ae J K A\u00d6 \u00df @X \u00e9 J \u00cb@ \u00c9\u00eb@\u00f0 \u00e9 J \u00cb@ \u00c9\u00eb@ Y \u00e9 J \u00d3 \u00c8A\u00aa\n\u00af @ H. \u00e0\u00f1\u00d3\u00f1 \u00aeK A\u00d3 A\u00d6 \u00df @X \u00e9\u00aaJ \u00cb@ H. AJ. \u00cb@ ZB \u00f1\u00eb\n\u00e8C \u00cb@ \u00e1\u00ab \u00e0\u00f0Q k A JK B \u00d1\u00ee E @ A\u00d2\u00bb , Yj. \u00d6\u00cf @ \u00fa \u00af Qj. \u00ae\u00cb @ \u00e8C H. \u00e0\u00f1\u00d3\u00f1 \u00aeK A\u00d3 A\u00d6 \u00df @X \u00e0\u00f1\u00d2\u00ca \u00d6\u00cf @ H. AJ. \u00cb@ ZB \u00f1\u00eb\n\u00e1\u00a3\u00f1\u00cb@ \u00e1\u00ab \u00a8A \u00afY\u00cb@ \u00c9g. @ \u00e1\u00d3 \u00e9J \u00cb\u00f1\u00a2 . \u00c8A\u00aa \u00af @ H. \u00e0\u00f1\u00d3\u00f1 \u00aeK A\u00d3 A\u00d6 \u00df @X \u00e1K Yg\u00f1\u00d6\u00cf @ H. AJ. \u00cb\n\u00a9\u00d2 Jj. \u00d6\u00cf @ \u00e1\u00d3 \u00e0\u00f0 X\u00f1J. J\u00d3 \u00d1\u00eb \u00f0 \u00e9J \u00afC g@ B \u00f0 \u00e9 J \u00d3 \u00c8A\u00aa \u00af @ H. \u00e0\u00f1\u00d3\u00f1 \u00aeK A\u00d3 A\u00d6 \u00df @X \u00e1K Yj\u00ca\u00d6\u00cf @ H. AJ. \u00cb@ ZB \u00f1\u00eb\nTable E.3: Sample outputs from our social bias analysis. We color the initial prompt with gray.\nGenerated Tweets\n\u00c9J \u00ca\u00cb @_ Q k@_ h\u00f1K.# ! \u00e8Q\u00e5 . \u00f9 \u00ae \u00aeK h@P \u00e1\u00d3 \u00d1\u00ea\u00d6\u00cf @ . . . ! h\u00f0QK \u00f1\u00ee D\u00d3\u00f0 \u00fa m . ' \u00e1\u00d3 Q \u00ae K XA\u00abA\u00d3 ! \u00e8Q\u00d6\u00cf A\u00eb \u00e0A @ Q\u00a3A g \u00f8 PA H. \u00fa GA\u00d3 . . h\u00f1 J \u00ae\u00d3 H. AJ. \u00cb @ \u00f8Q K \u00c9gQK \u00fa \u00e6 . K \u00fa \u00ce\u00cb @ : XQ \u00ab\n. . A K @ \u00f9 \u00a2k AK : I\u00ca \u00af ? \u00bd KQ\u00bb X B \u00bdJ \u00ca\u00ab \u00fa \u00e6 \u00ae K Y Ag \u00fa G @ \u00f9 \u00ae\u00baK A\u00eeE. \u00fa \u00e6 @P A K @ \u00bdJ \u00af \u00e9k. Ag \u00c9\u00bf\u00f0 \u00bdJ \u00af H\u00f1\u00d3@\u00f0 \u00bd \u00ae \u00ab@\u00f0 \u00bdJ.k@ \u00e9K @ : XQ \u00ab\n\u2401\u2402 \u2401\u2402 i. \u00ca J\u00cb @ \u00f8 Y \u00af \u00e1 \u00a2\u00cb@ \u00e1K PAK \u00e8\u00f1\u00cag \u00d0AK @ \u00e9J \u00aeJ \u00aem\u00cc'@ \u00e9\u00cb\u00f1 \u00ae\u00a2\u00cb@ A\u00abA\u00d3 @ Y\u00eb !! \u00e9 KA \u00afY @ \u00a9\u00d3 \u00e9J \u00cbA\u00d2 \u00cb@ A\u00beK Q\u00d3 AK. i. \u00ca J\u00cb @ H@Q\u00baK. I. \u00aa\u00cb A\u00d3 \u00fa \u00ce\u00cb @ : XQ \u00ab\n#Rayan_AbdelRahman . . \u00e9K @ A J\u00cb\u00f1 \u00ae\u00d3 A Jk@ : XQ \u00ab\nPA\u00bf X@_\nJ J.\u00a2\n#-(\u00f8 Q \u00cam. ' @ \u00fa kQ\u00e5 \u00d3\u00f0 \u00fa \u00e6 AJ \u00f0 \u00ac\u00f1 \u00caJ \u00af) \u00e1 \u00ca\u00be K @Q \u00af \u00e1 \u00d3Aj. JK. - . I\u00d2 \u00cb@ \u00e9\u00d2\u00bam\u00cc'@  \u00f0 , \u00bdJ J\u00aaK B A\u00d3 \u00fa \u00af \u00c9 gY J\u00cb @ \u00d0Y\u00ab \u00f1\u00eb \u00e8AJ m \u00cc'@ \u00fa \u00af \u00e8 Q k@ \u00f1\u00d3C\u00cb@ \u00d0Y\u00ab : XQ \u00ab\n\u2406 URL A J k\u00f1J k_ \u00d5\u00cbA\u00aa\u00cb @_ A\u00bf# XA \u00abA\u00d3 : XQ \u00ab\nURL\u00f8 X\u00f1\u00aa \u00cb@_ \u00e8X\u00f1\u00aa\u00cb@_ \u00e0A\u00d2\u00ca _ \u00e1K._ Y\u00d2m\u00d7_ Q \u00d3 B@_ Y\u00ea\u00aa\u00cb@_ \u00fa \u00cd\u00f0_ A\u00bf_ \u00f9 KA\u00ee E_ Q\u00e5 J\u00cb @_ \u00c8C\u00ea\u00cb@#. . \u00fa m . ' Q\u00a3 \u00e1\u00d3 h\u00f1\u00d3 : XQ \u00ab\n\u2403\u2403 \u00e8AJ m \u00cc'@_ A\u00ebAK @ _ \u00fa \u00e6 J\u00d2\u00ca\u00ab_ \u00e9 \u00aeJ \u00aek# \u00e9\u00bbPA \u00d3 YK P@ \u00fa\n\u00e6 J\u00ba\u00cb @Yg@ \u00e9K. \u00fa \u00e6\u00bbPA \u00e0 @ YK P@ B . . \u00a9J. : XQ \u00ab\n: \u00fa \u00e6J \u00ab \u00e1\u00d3 iJ \u00a2 h@P : XQ \u00ab\n!! \u00fa \u00e6 . \u00ca \u00af AK i. \u00aeJ \u00bb : XQ \u00ab\nA\u00ebPA\u00be K @_ \u00e1\u00ba\u00d6\u00df B_ \u00e9 \u00aeJ \u00aek#!!\u00d1\u00ee E\u00f0 \u00f1 \u00fa \u00af \u00c9 gY J\u00cb @ \u00d0Y\u00ab . . \u00e8 Q k@ \u00f1\u00d3C\u00cb@ \u00d0Y\u00ab : XQ \u00ab\nTable E.4: Examples of generated \u2018tweets\u2019, prompted, from JASMINE2.7B under zero-shot. We color the initial prompt with gray.\nOriginal Poetry Generated Poetry\n\u00d5 \u00ae \u00e8Y J\u00ab \u00fa \u00cdAg\u00f0 \u00f9 \u00d2 m. ' . \u00e1\u00d3\u00f0 ** \u00d5\u00e6 . \u00e9J. \u00ca \u00af \u00e1\u00dc\u00d8 \u00e8AJ. \u00ca \u00af Qk@\u00f0\n\u00d5\u00d7 B@ \u00e9\u00cb\u00f0Y\u00cb@ J I. k \u00fa\n\u00abY K\u00f0 ** \u00f8\nY k. \u00f8QK. Y \u00af AJ.k \u00d5 \u00e6\u00bb @ \u00fa \u00cdA\u00d3\n\u00d5\u00e6 J \u00ae K I. m \u00cc'@ P Y \u00aeK. A K @ IJ \u00ca \u00af ** \u00e9 KQ \u00aa\u00cb I. k A J\u00aa\u00d2m. ' \u00e0A\u00bf \u00e0@\n\u00d0X \u00ac\u00f1J \u00cb@\u00f0 \u00e9J \u00cb @ HQ \u00a2 Y \u00af\u00f0 ** \u00e8Y\u00d2 \u00aa\u00d3 Y J\u00ea\u00cb @ \u00ac\u00f1J \u00f0 \u00e9 KP P Y \u00af\n\u00d5\u00e6 \u00cb@ \u00e1 k B@ \u00fa \u00afA\u00d3 \u00e1 k @ \u00e0A\u00bf\u00f0 ** \u00d1\u00ea\u00ca\u00bf \u00e9<\u00cb @ \u00ca g \u00e1 k @ \u00e0A\u00be \u00af\n\u00d1\u00aa K \u00e9J \u00a3 \u00fa \u00af  @ \u00e9J \u00a3 \u00fa \u00af ** Q \u00ae \u00a3 \u00e9 J \u00d2\u00d6\u00df \u00f8 Y\u00cb@ \u00f0Y\u00aa\u00cb@ H\u00f1 \u00af\n\u00d1\u00eeD . \u00cb @ \u00a9 J B A\u00d3 \u00e9K. A\u00ea\u00d6\u00cf @ \u00bd\u00cb ** I\u00aa J\u00a2 @\u00f0 \u00ac\u00f1 m\u00cc'@ YK Y \u00bd J\u00ab H. A K Y \u00af\n\u00d5\u00ce\u00ab B\u00f0 P @ \u00d1\u00eeE P@\u00f1K B \u00e0 @ ** A\u00ea\u00d3 Q \u00caK \u00cb A J \u00bd \u00ae K I\u00d3 Q\u00cb @\n\u00d5\u00d4\u00ea\u00cb @ \u00e8PA K @ \u00fa\n\u00af \u00bdK. I Q\u0304\u00e5 ** AK. Q\u00eb \u00fa \u00e6 J K A \u00af A k. I\u00d3P A\u00d2\u00ca\u00bf @\n\u00d5 \u00ae \u00e8Y J\u00ab \u00fa \u00cdAg\u00f0 \u00f9 \u00d2 m. ' . \u00e1\u00d3\u00f0 ** \u00d1 J. \u00e9 J. \u00ca \u00af \u00e1\u00dc\u00d8 \u00e8A J. \u00ca \u00af Qk@\u00f0\n\u00d5\u00d7 B@ \u00e9\u00cb\u00f0Y\u00cb@ J I. k \u00fa\n\u00abY K\u00f0 ** \u00f8\nY k. \u00f8QK. Y \u00af AJ.k \u00d5 \u00e6\u00bb @ \u00fa \u00cd A\u00d3\n\u00d5\u00e6 J \u00ae K I. m \u00cc'@ PY \u00aeK. A K @ IJ \u00ca \u00af ** \u00e9 KQ \u00aa\u00cb I. k A J\u00aa\u00d2m. ' \u00e0A\u00bf \u00e0@\n\u00d5\u00cb B@ \u00e1\u00d3 Ag\u00f1K. Y\u00d3\nQ\u0304K Q \u00a2\u00cbA \u00af ** AK. Q\u00a3 \u00d5\u00ba J K. \u00fa \u00e6 P\u0304 @\u00f1J. m ' B\nJ. K \u00d5\u00cb @\u00f1\u00cag \u00e9 J\u00d3\nY K \u00d5\u00cb \u00f1\u00cb ** PY \u00af \u00f8\u00f1\u00ea\u00cb@\u00f0 \u00e8@\u00f1\u00eb \u00fa \u00af \u00f9 \u00d6 \u00dfB AK\n\u00e0AJ.\u00ebQ\u00cb QK X \u00f0 \u00e0B Q \u00aa\u00cb \u00fa\u00abQ\u00d4 \u00af ** \u00e8P\u00f1 \u00c9\u00bf CK. A \u00af \u00fa\n\u00e6 . \u00ca \u00af PA Y \u00af\n\u00e0 @Q \u00af j \u00d3\u00f0 \u00e8@P\u00f1 K h@\u00f1\u00cb @\u00f0 **  KA\u00a3 \u00e9J.\u00aa\u00bb\u00f0 \u00e0A K\u00f0 B I K. \u00f0\n\u00fa GA\u00d6\u00df @ \u00f0 \u00fa \u00e6K X I. m \u00cc'A \u00af \u00e9J. K A\u00bfP ** I\u00eak. \u00f1 K \u00fa G @ I. m \u00cc'@ \u00e1K YK. \u00e1K X @\nI. j K A J KA\u00d3YK. A KP\u00f1j J \u00af ** \u00e9\u00ab\u00f1\u00d3YK. \u00e8Y g I. m ' \u00e0A\u00bf \u00e1\u00d3\n\u00e0A\u00d3 Q\u00cb @ \u00e1\u00d3 \u00a9J K. Q\u00cb @\n\u00e9\u00cb Q \u00d6\u00df. ** \u00fa GA \u00aa\u00d6\u00cf @ \u00fa\n\u00af AJ. J \u00a3 I. \u00aa \u00cb@ \u00fa GA \u00aa\u00d3\n\u00e0A \u00ca\u00cb@\u00f0 YJ \u00cb @\u00f0 \u00e9k. \u00f1\u00cb@ I. K Q\n\u00ab ** A\u00eeD \u00af \u00fa\nG . Q\u00aa\u00cb@ \u00fa \u00e6 \u00ae\u00cb @ \u00e1\u00ba\u00cb\u00f0\n\u00e0A\u00d4g . Q K. PA \u00cb \u00e0A\u00d2J \u00ca ** A\u00eeD \u00af PA \u00f1\u00cb \u00e9 Jk. I. \u00abC\u00d3\n\u00e0@Qm\u00cc'@ \u00e1\u00d3 \u00e1\u00d3Q\u00bb \u00e0@ \u00f0 I k ** \u00fa \u00e6k \u00c9J m\u00cc'@\u00f0 A J K A Q \u00af\nIJ.\u00a3\n\u00e0A\u00d2m.\n\u00cc'@ \u00c9 J\u00d3 A\u00ea \u00af @Q\u00ab @ \u00fa\u00ce\u00ab ** A\u00eeD \u00af \u00e0A \u00ab B@ \u00ae J K A K\u00f0Y \u00ab\n\u00fa GA \u00ae\u00bb A\u00d6\u00df. Z AJ \u00cb@ \u00e1\u00d3 \u00e1 .g. \u00f0 ** \u00fa \u00e6\u00ab \u00d2 \u00cb@ \u00e1 . m. k Y \u00af\u00f0 HQ\u00e5 \u00af\n\u00e0A JJ. \u00cb @\n\u00e1\u00d3 Q \u00ae K @Q K A KX ** \u00fa\nG . AJ K \u00fa \u00af A\u00ee D\u00d3 Q\u00e5 \u00cb @ \u00f9 \u00ae\u00cb @\u00f0\n\u00fa G @\u00f0 @ CK. \u00e1 \u00ae \u00af\u00f0 \u00e9K. Q\u00e5 AK. ** \u00e9 J\u00d3 \u00bdJ \u00cb @ Q Q\u00d6 \u00df A\u00ea\u00cb\n\u00e0A\u00d3 Q\u00cb @ \u00e1\u00d3 \u00a9J K. Q\u00cb @\n\u00e9\u00cb Q \u00d6\u00df. ** \u00fa GA \u00aa\u00d6\u00cf @ \u00fa\n\u00af AJ. J \u00a3 I. \u00aa \u00cb@ \u00fa GA \u00aa\u00d3\n\u00e0A \u00ca\u00cb@\u00f0 YJ \u00cb @\u00f0 \u00e9k. \u00f1\u00cb@ I. K Q\n\u00ab ** A\u00eeD \u00af \u00fa\nG . Q\u00aa\u00cb@ \u00fa \u00e6 \u00ae\u00cb @ \u00e1\u00ba\u00cb\u00f0\n\u00e0A\u00d4g . Q K. PA \u00cb \u00e0A\u00d2J \u00ca ** A\u00eeD \u00af PA \u00f1\u00cb \u00e9 Jk. I. \u00abC\u00d3\n\u00e0A JJ. \u00cb @ \u00ac@Q\u00a3 AK. A\u00ee D\u00d3 @\u00f1J. K ** \u00e9\u00cb P@X H A K A\u00d3 @ X @ \u00fa \u00e6 \u00af\nAJ. K Ag. \u00f9 \u00ae\u00caK \u00bd J\u00d3 BAJ\nk \u00c9\u00aa\u00cb ** \u00e9 \u00aa K \u00fa\nG . A\u00d3\u00f0 \u00fa \u00e6 \u00aa J B \u00fa G @ \u00f0\n\u00acA \u00ae \u00af \u00f8 X \u00fa\u00ce\u00ab A\u00ebXQK. I\n\u00ae\u00cb @ ** A\u00eeD J \u00aa\u00d3 \u00fa \u00af I\u00cbA\u00d3 l ' Q\u00cb @ @ X @\n\u00e1 \u00ba\u00d6\u00cf @ PA \u00ae\u00cbAK. A J\u00cb @ B\u00f0 ** \u00d0A J\u00cb \u00bc\u00f1\u00ca\u00d6\u00cf AK. A J\u00cb @ A\u00d6 \u00df @ \u00f0\n\u00e0A\u00d3Y m\u00cc'@\u00f0\n\u00e9J \u00abQ\u00cbAK. A J\u00cb @ ** B\u00f0 \u00e1 \u00bb\u00f1\u00ca\u00d2\u00d6\u00cf AK. A J\u00cb @ B\u00f0\nTable E.5: Examples of generated \u2018poetry\u2019, prompted by three lines from Al-Mutanabi, from JASMINE2.7B under zero-shot. We color the initial prompt with gray.\nGenerated Poems (1) Empty prompt\nZ @Q\u00aa A\u00aaK. P @\u00f0 \u00a8A\u00d6\u00de @ X ** A KC\nK Q \u00d3 B@ \u00e8Q\u00e5 k I\u00aa\u00d4g\n.\nZA J \u00ab\u00f0 \u00fa \u00e6 \u00ab \u00e9 Jm\u00cc @\u00f1\u00aak. P **\nQ \u00af I. K Q \u00ab @\u00f0Y @ A\u00d2\u00ca\u00bf\nZ@\u00f1\u00ea\u00cb @\u00f0 \u00e9 P\u0304 ZA\u00d6\u00cf @ h. P A\u00d3 ** I. K Q \u00ab \u00fa \u00e6\u00aa\u00d3 \u00e1 k\u00f0 \u00fa \u00e6 \u00aa\u00d3 I. K\n\u00f9 K @Q\u00cb @ \u00e1 \u00ab BA \u00af \u00a1j\u00ca\u00cb@ \u00acA J\nAK. ** \u00af B@ Q\u00eeE. \u00f8\nY\u00cb@ \u00e9\u00d3A\u00d4g \u00e1\u00d6\u00de\nZAJ. K @ \u00e9J \u00af A\u00d3\nI\u00d6\u00cf @ \u00e9\u00d3A\u00d4g\u00f0 ** \u00e8A\u00d3@Y\nK \u00d0\u00f1j. J\u00cb @\u00f0 PYJ. \u00cb A\u00bf \u00f1\u00ea \u00af\nZA\u00d6\u00de ZA\u00d2 \u00cb@ \u00af @ \u00fa \u00af \u00f9 \u00eb ** \u00e9\u00aa\u00ca\u00a2 . A J k PA\u00d4 \u00af B@ \u00c9j. m '\n\u00fa\u00e6 m\u00cc'@ \u00d0A\u00d2 \u00aa\u00cb @ H. \u00f1 J \u00bb\u00f0 ** AK Q\u00cb @ \u00f8 Y\u00eb \u00bdgA J \u00bb\u00f0\n\u00fa\u00beK. \u00e9J \u00af \u00e1m ' A\u00d6\u00cf \u00e1\u00ba\u00cb\u00f0 ** X\u00f1m. ' \u00e0 @ Y\u00d2 J\u00aaK \u00d5\u00cb HA\u00eeD \u00eb\u00f0\nA\u00beJ. \u00cb A\u00bf \u00bdm \u00e9 J\u00ba\u00cb\u00f0 ** HA\u00bej \u00d6\u00cf @ \u00e1\u00d3 \u00d2m '. @ XA\u00d3\u00f0\n@Y \u00ab A KQ \u00a2 J K @\u00f0 \u00e9\u00cb A J\u00aa k ** AgXA \u00af A J\u00ca\u00d4g \u00d0\u00f1J \u00cb @ @ X\u00f0\nA \u00aa\u00cb@ Q\u00d4g . l ' @\u00f1m. \u00cc'@ \u00e1 K. \u00f0 ** \u00e0A\u00d3 Q\u00cb @ \u00acQ\u00e5 \u00d5\u00bak \u00fa\u00ce\u00ab \u00fa \u00e6 \u00aa K\u00f0\n\u00f8\u00f0 Q KA \u00af \u00e9\u00ca\u00eb @ \u00e1\u00ab m\u00cc'@ \u00f8\u00f0 P ** \u00e1 \u00d2\u00ca \u00d6\u00cf @ \u00fa\u00ce\u00ab I. \u00cb@ H. P AK \u00f0\n(2) Part of poetry\u2019s line prompt\n\u00c8A\u00d4g . AK \u00bdJ \u00ca\u00ab \u00f8 XA J K. Q\u00e5 \u00d3 ** \u00f8 Q\u00e5 \u00d3 AK. \u00d0\u00f1 \u00af\n\u00c8A\u00d3 B@ \u00e9 J\u00d3 \u00bd\u00eak. \u00f1\u00cb I KX\u00f0 ** hB Y \u00af i J \u00ae\u00cb @ \u00e0AK. \u00e0Q kA \u00af\n\u00c8C\u00ea\u00cb@ \u00e9J \u00ee D P\u00f1 JK. \u00f1\u00eb QK ** I. \u00bb\u00f1\u00baK. AJ KY\u00cb@ HQ\u00e5 AJ. K\u00f0\n\u00c8A\u00d2m. \u00cc'@ I. \u00bb \u00fa \u00af \u00c8A\u00d6\u00cf @ \u00c8 YJ. K \u00e0 @\u00f0 ** Y\u00eam. \u00cc'@ \u00bdK. \u00f9\u00a2\u00aaK \u00e0 @ \u00e9<\u00cb @ X @P @\u00f0\n\u00d5 \u00ae \u00e8Y J\u00ab \u00fa \u00cdAg\u00f0 \u00f9 \u00d2 m. ' . \u00e1\u00d3\u00f0 ** \u00d5\u00e6 . \u00e9J. \u00ca \u00af \u00e1\u00dc\u00d8 \u00e8AJ. \u00ca \u00af Qk@\u00f0\nA\u00d2 \u00af \u00d5\u00ba\u00d6\u00de A \u00af @ \u00e0 @ \u00fa \u00af \u00f9 \u00aa\u00d2\u00a2 B\u00f0 ** \u00f8\u00f1\u00ea\u00cb@ I. \u00ca\u00a3 \u00e1\u00ab \u00ae J\u00cb @ \u00fa \u00e6\u00ee D\u00ee D K C \u00af\n\u00d1\u00eeD \u00f8\u00f1\u00ea\u00cbA \u00af A\u00d2\u00eb A\u00d2\u00baJ \u00af I\u00d6\u00de A \u00ae K ** \u00fa \u00e6 K @ \u00f8QK \u00fa G . \u00e1\u00d3 \u00fa GQ \u00aaK B\u00f0\n\u00d5\u00d4m \u00cc '@ \u00e9J\n\u00af H. \u00f0 YK \u00fa \u00e6 . \u00ca \u00af \u00f9\u00d4g \u00e0A \u00af ** A P\u0304AK. I\u00d6\u00de A\u00d3 \u00e9 J\u00d3 \u00e9\u00ab\u00f1\u00cb \u00fa G . \u00e0A \u00af\n(3) One line of poetry prompt\nA\u00ebA\u00a2 k \u00e9J K \u00e9 \u00ae A \u00af AJ. \u00caK. ** \u00d0@Q\u00ba\u00cb@ I K. \u00fa \u00ce\u00ab \u00f9 \u00ae @\nA\u00eb@Q K \u00f0Q\u00cb@ Q\u00eb P I . k\u00f0 B@ ** A\u00ee D J\n\u00e1 \u00ab I\u00ca\u00d3 A K A\u00d3\nA\u00ebA\u00aa K B\u00f0 A\u00d3\u00f1K \u00f8 \u00f1m ' I\n\u00ae J\u00caK \u00d5\u00cb ** I. \u00aa\u00cb \u00fa \u00af\u00f0 \u00f1\u00ea\u00cb \u00fa \u00af Q\u00d2\u00aa\u00cb@ \u00fa\u00e6 \u00af\nA\u00ea \u00af@\u00f1 @ \u00e1\u00d3 \u00e9m ' @\u00f1k. \u00f1 \u00ae\u00ee E ** Cg. \u00f0 \u00fak\n. Y\u00cb@ i Jk. \u00fa GP@ P \u00d5\u00bb\nA\u00ebA\u00d6\u00de \u00fa \u00af \u00f9 \u00ae m ' P\u00f0YJ. \u00cb @ \u00e8 A J ** \u00fa \u00af\u00f0 @P\u00f1\u00a3 \u00fa \u00e6 \u00a2j\u00caK \u00d1j. J\u00cb @\u00f0\nA\u00ebAK. \u00f0 \u00d5\u00ba J K. \u00e9K. AJ. \u00cb@ A JJ \u00af ** I \u00af \u00fa\u00cd B@ \u00fa\nm\u00cc'@ \u00e8Q g. AK\nYJ Q\u00cb@ \u00e1 \u00aa\u00cb \u00f0YJ. K HA \u00a2\u00ab\u00f1\u00d3 ** YK Yg. \u00d0A\u00ab\u00f0 \u00fa\u00e6 \u00d3 \u00d0A\u00ab \u00e1 K.\nYJ \u00cag. \u00e1\u00d3 \u00f9 \u00ae J K \u00d5\u00e6 J\u00ca\u00cb @ \u00f8Q K\u00f0 ** Z\u00f1 . \u00e0A\u00d3 Q\u00cb @ \u00a9\u00d3 \u00e0A\u00d3 Q\u00cb @ PAg.\nXQK \u00e1\u00d6\u00cf ZC\u00aa\u00cb@ \u00c9J. @\u00f1m \u00f0 @ Y \u00af ** Q\u00e5 \u00ab \u00f8 AK. @\u00f1 KA\u00bf \u00fa\u00cd B@ \u00e1K @\nYK Q \u00af A\u00ea\u00cb X\u00f1k. \u00f1\u00cb@ \u00fa \u00af PA \u00d1\u00ee D\u00ab ** A \u00aeJ J\u00d3 @Q K @ A J\u00cb @\u00f1\u00bbQ K \u00d5\u00bb\nY KA\u00aa\u00d6\u00cf @ \u00d0C\u00bf A J\u00cb A\u00ee D\u00d3 \u00e0A\u00bf ** \u00d1\u00ee D\u00ab\u00f0 \u00d0\u00f1\u00ca\u00aa\u00cb@ . J \u00ae K \u00d1\u00ee D\u00ab\nYK Ym ' . Y g B@ \u00c9J. \u00af I. \u00abQ\u00cbAK. ** \u00e8Z@Y\u00ab @ \u00d0 Q\u00eeE A\u00be\u00ca\u00d3 AK\n(4) Two lines of poetry prompt\n\u00c8C \u00a2\u00cb@ \u00bdJ KA\u00eb\u00f0 \u00e0A JJ. \u00cb \u00f1k. ** \u00fa \u00af Q\u00e5 k B@ \u00e1 \u00aa\u00cb@ Z @Y \u00af @\n\u00c8C J\u00cb @ \u00c9\u00eeD \u00fa \u00af Q\u00eb Q\u00cb @ Z @Y \u00af\u00f0 ** A Q\u0304\u00ab IK. A\u00a3 Q\u00eb Q\u00cb @ Z @Y \u00af \u00d0 @\n\u00c8C\u00a2\u00cb@ \u00fa \u00af I. m\n\u00cc'@\n\u00ac\u00f1J \u00a3 @\u00f0 ** H. A \u00d6\u00cf @ \u00a9J. J\u00cb @ \u00fa\u00ce\u00ab I \u00aek. \u00f0 A \u00af @\n\u00c8A\u00d3Q\u00cb@ I. \u00ca \u00af \u00fa \u00af \u00e8\u00f1\u00eb ** Q\u00aa \u00af \u00fa \u00af \u00c8C\u00a3 BAK. IJ \u00ae\u00cb @\u00f0\n\u00fa \u00cd@\u00f1 m\u00cc'@ \u00d0\u00f1 Q\u00cb@ \u00bd\u00ca\nK \u00fa\u00ce\u00ab ** @Q\u00ea\u00d6\u00cf @ \u00e1 \u00aa\u00cb@ \u00a9\u00d3X I\u00ca\u00aak. \u00f0\n\u00fa \u00cdAg \u00fa \u00af \u00e0\u00f1 J \u00cb@ \u00e9 J KYg @ ** A\u00d3\u00f0 \u00fa \u00e6 . \u00ca \u00af \u00e1\u00d3 \u00e0A\u00bf A\u00d3 \u00bc@ X\n\u00c8 A\u00d6\u00cf @ \u00fa \u00af \u00bd K\u00f1\u00baK \u00e0 @ \u00fa \u00e6\u00d6 \u00df ** AK. \u00c9\u00bf , A JJ \u00d3 \u00bdJ \u00baJ. K A\u00d3\u00f0\n\u00c8Aj. \u00cbAK. Y K \u00e0 @ \u00e1\u00d3 A K C \u00af ** @YJ \u00aa\n\u00aa K \u00e0 @ \u00f9 \u00aaJ. K I J\u00bb \u00e0A \u00af\n\u00c8@\u00f0 Q\u00cb @ \u00fa \u00af \u00f8 PY\u00cb@ I. \u00bb\u00f1\u00ba\u00cbA\u00be\u00cb ** \u00fa \u00e6 K A\u00bf , HCK \u00f1\u00cb@ \u00fa \u00e6 \u00ae J\u00ba K\n\u00c8 @ B\u00f0 ,\u00c9\u00eb @ Q\n\u00aaK. , @YK Q \u00af ** AJ. K Q \u00ab Q . \u00ae\u00cb @ \u00e9\u00d2\u00ca \u00a3 \u00fa \u00af I \u00d3 @\u00f0\n\u00c8B P A JJ \u00ae \u00e0 @ \u00e9<\u00cb @ \u00c9 ** AJ k. @P \u00bc\u00f1\u00abX @\u00f0 A\nK\u00f0 Qm\u00d7 \u00bdK XA K @\n\u00c8A\u00d3 B@ PA K\u00f0 \u00d1\u00ea\u00cbAK.\n\u00aaK ** Q\u00ebX \u00a9\u00d3Y\u00cb@ Q \u00ab A J\u00cb \u00ca \u00af\n(5) Three lines of poetry prompt\n\u00f0Y @\u00f0 I. m \u00cc'AK. \u00e9J \u00af \u00fa \u00e6 \u00aa K @ ** \u00d5\u00cbAg \u00f8 Q\u00abA hAJ.\nYJ \u00af I. m\n\u00cc'@ \u00e0 @ \u00c8A\n\u00af \u00e1\u00d3 H. XA\u00bf ** \u00fa \u00e6K Qk \u00e1\u00ab YJ \u00ae\u00cb @ XP @\u00f0\n\u00d0A J\u00d6\u00cf @ I. J \u00a3 \u00e1\u00d3 \u00fa \u00e6 J\u00d3Qk\u00f0 ** H. A J\u00aa\u00cbAK. \u00fa \u00e6 KQm. @ \u00e1\u00d3 AK\n\u00d0C\u00be\u00cb@ Yg \u00f1 \u00aeK I. J\u00ab ** \u00e1\u00d3 \u00e9J \u00af \u00d5\u00ba \u00af \u00d0\u00f1J \u00cb @ Q\u00e5 \u00af @\n\u00d0@QK \u00cb I. m \u00cc'@ \u00fa \u00af H. @ Y\u00ab \u00e1\u00d3 ** \u00fa \u00e6 \u00ae K \u00fa\u00ce\u00ab \u00fa\u00e6 k @ A K @\n\u00c8A \u00aa\u00cb@ Z @Y\u00cb@ \u00e1\u00d3 \u00f9 KQK. \u00e0@ ** \u00d1\u00eeE YK \u00a8\u00f1\u00a3 I. \u00a2\u00cb@\u00f0 \u00d0\u00f1 \u00af \u00c8A \u00af\n\u00c8AJ m\u00cc'@ \u00fa \u00af \u00e9\u00d3AJ \u00ee E Q \u00ab ** Z@\u00f0X \u00fa\n\u00e6 . \u00ca \u00ae\u00cb A\u00d3 I. \u00a2\u00cb@ \u00f1 \u00f0\nH. @ Y\u00aa\u00cb@ \u00bdJ m 'Ag. \u00fa \u00af KA\u00aaK ** \u00fa kA Jk. HQ\u00e5 \u00fa G B \u00bc@ Y \u00af\nH. A \u00d6 \u00cf @\u00f0 \u00f8 X B@ \u00fa \u00af A J\u00d6\u00de\nA\u00d3\u00f0 ** Q \u00d6\u00cf @ Z\u00f1 \u00bdJ \u00af \u00fa\n\u00e6\u00aa\u00d2m.\n' \u00f0\nH. A \u00abP \u00e1\u00d3\n\u00aa\u00cb@ @ Y\u00eb \u00acPA g P ** \u00e9J. \u00ca \u00af \u00acA\u00ab Q\u00abA A K @\nH. A J\u00ab \u00e1\u00d3 \u00e8Q\u00d4\n\u00ab \u00fa \u00af \u00fa\u00e6 JK \u00f0 ** \u00e8A \u00ae\u00caK \u00f8 Y\u00cb@\n\u00f1J. \u00cb @\n\u00e1\u00d3 \u00e1 K\nH. @ Y \u00cb@ \u00a8\u00f1J. K \u00fa \u00e6J \u00ab \u00fa \u00af I. \u00ba \u00f0 ** \u00d0\u00f1\u00d2\u00ea\u00cb@ \u00f1 \u00ae\u00a2 \u00f0 \u00d0\u00f1\u00d2\u00ea\u00cb@ \u00f9 \u00aa\u00a2\nH. @ Q \u00ab@\u00f0 \u00e9 k\u00f0 \u00fa \u00af \u00fa \u00e6 Q\u0304\u00a2 ** \u00e0A\u00bf \u00f8 Y\u00cb@ \u00bd\u00cbAJ k \u00e0 @\u00f0\nH. \u00f1J \u00aa\u00cb @ \u00e1m.\n' A\u00d3 \n\u00baJ \u00f0 ** \u00e9J m.\n'P @ A\u00d6\u00df. \u00fa G Y g AJ\nTable E.6: Examples of synthetically generated \u2018poetry\u2019, prompted by a full (or part of) real line of poetry or empty prompt from our further pre-trained JASMINEpoetry model. We color the initial prompt with gray.\nDialect Generated Text A lg er ia n \u00e8 @ A\u00aa\u00d3 @\u00f1jJ \u00a2 AJ \u00aa\u00d3 hA\u00a3 @ X@\u00f0 \u00e0A\u00d2 \u00ae\u00cb \u00f8 B\u00f1\u00d3 AK A\u00aa\u00d3 iJ \u00a2 h@P iJ \u00a2 \u00e0A\u00bf\u00f1\u00cb \u00e9\u00caJ.\u00ab \u00fa GB\u00f1\u00d3 A KA\u00aa\u00d3 \u00bdjJ \u00a2 h@PA\u00d3 \u00e8A\u00aa\u00d3 \u00fa \u00e6j\u00a3 @ X@ \u00fa \u00e6 JK. AK \u00fa \u00af\u00f1 : \u00e0A\u00d2 \u00ae\u00cb \u00f8 B\u00f1\u00d3 \u00f8 B\u00f1\u00d3 AK \u00fa \u00e6 J\u00bam \u00e9\u00eb : \u00e9\u00caJ.\u00ab \u00a8\u00f1 \u00f1\u00d6\u00cfA\u00eb \u00e1\u00d3 \u00e9 \u00aeK A g \u00fa G @ \u00e9<\u00cb @\u00f0 \u00e8YK A\u00ab \u00fa \u00e6\u00cbA g AK \u00e8 @ @ : \u00e8YK A\u00ab Q m '. \u00e0\u00f1\u00baK h@P \u00fa \u00e6 \u00c9\u00bf \u00e9<\u00cb @ ZA \u00e0@ \u00fa \u00e6J. J.k AK \u00fa \u00afA m ' B : \u00e9m. ' Y g \u00f0Q\u00ba\u00d3\u00f0 Z\u00f1 \u00c9\u00bf \u00d5\u00ba J\u00ab Y\u00aaJ. K \u00f0 Q\u00e5 \u00c9\u00bf \u00e1\u00d3 \u00d5\u00ba \u00a2 \u00aem ' \u00e9<\u00cb @ : \u00a3@\u00f1\u00ab H. PAK \u00e9J. J \u00a2\u00cb@ PAJ. kB@ A J\u00aa\u00d2 \u00e9<\u00cb @ H. P AK \u00e1 J \u00d3@ : \u00e9\u00d2\u00a3A \u00af\n\u00e9<\u00cb Y\u00d2m\u00cc'@\u00f0 Q m '. A Jk@ : \u00e9\u00d2J \u00d3@\n\u00bcXB\u00f0 \u00bd\u00caJ \u00ca m ' \u00f0 A J\u00cb \u00fa \u00beJ \u00ca m ' \u00e9<\u00cb @\u00f0 \u00e9\u00d2J \u00d3@ A\u00d3A\u00d3 AK \u00bd J\u00d3C \u00fa\u00ce\u00ab \u00e9<\u00cb Y\u00d2m\u00cc'@ : k. Q K\n? \u00bc@ XA\u00eb\u00f0 \u00bdK XA\u00eb \u00e1 K. Q \u00ae\u00cb @ \u00f1\u00eb @\u00f0 \u00fa \u00cd\u00f1 \u00ae K . @ XA\u00eb \u00ac\u00f1 , @ XA\u00eb \u00ac\u00f1\n\u00f1 Q\u0304\u00aa J\u00bb A\u00d3 A Jk . J K A\u00bf A\u00d3 \u00f1\u00eb@P \u00f0XA\u00eb A\u00d3@ , \u00e1K X\u00f1k. \u00f1\u00d3 @\u00f1 KA\u00bf \u00f0XA\u00eb . \u00e1K X\u00f1k. \u00f1\u00d3 @\u00f1 KA\u00bf A\u00d3 \u00f0XA\u00eb\u00f0 , X\u00f1k. \u00f1\u00d3 A\u00bf A\u00d3 \u00bc@XA\u00eb \u00f1 K @ \u00f1\u00eb Q \u00ae\u00cb @ : \u00bd\u00cb \u00c8\u00f1 \u00aeK\nE gy\npt ia\nn\n,Q J\u00bb \u00e8Y\u00bb \u00c9J. \u00af \u00e8X \u00a8\u00f1 \u00f1\u00d6\u00cf @ \u00fa \u00af A J\u00d2\u00ca\u00be K@ A Jk@ , \u00fa \u00e6 K @ I AK , C g \u00f8Y\u00eb@ \u00f8Y\u00eb@ AK \u00fa \u00e6 K @ I AK\nl\u00cc'A \u00cb@ h. \u00f0 Q\u00cbAK. \u00bd \u00af PQK A JK. P .H. P AK \u00fa \u00bb A\u00aa\u00d3 A J\u00caK Y\u00eeE \u00f0 , \u00fa \u00e6J. J.k AK \u00bdK Y\u00eeE A JK. P \u00bd\u00caJ \u00abY JK. \u00f0 \u00bdJ.j K. A Jk@\u00f0\n\u00fa \u00e6 \u00af PQK \u00f0 , \u00e1 \u00d6\u00cf A\u00aa\u00cb @ H. PAK \u00e1 \u00d3 @ \u00d1\u00ea\u00ca\u00cb @ , \u00e8Q k B@\u00f0 AJ KY\u00cb@ \u00fa \u00af \u00bcY\u00aa \u00f0 \u00bd K\u00f1 \u00f0 \u00bcPY \u00aeK \u00f0 \u00bd\u00ca\u00ebA J \u00fa \u00ce\u00cb @\n\u00d5\u00e6 @ \u00c9\u00beK. \u00d1\u00ea\u00ca\u00cb @ \u00bc\u00f1\u00abX @\u00f0 ,\n\u00e1 \u00d4 g@Q\u00cb @ \u00d1kP @ AK \u00bd J\u00d4gQK. \u00c9g. @ Q \u00ab Cg. A\u00ab \u00e9J. J \u00a2\u00cb@ \u00e9m\u00cc'A \u00cb@ \u00e9K P Y\u00cb@ \u00fa \u00bb AK @ \u00f0\n, \u00bcY J\u00ab I. J\n\u00aa\u00cb @ \u00d5\u00ce\u00ab \u00fa\n\u00af \u00e9K. HQ K A J @ \u00f0 @ , \u00bd \u00ae\u00ca g \u00e1\u00d3 @Yg @ \u00e9 J\u00d2\u00ca\u00ab \u00f0 @ \u00bdK. A J\u00bb \u00fa \u00af \u00e9 J\u00cb Q K @ \u00f0 @ \u00bd \u00ae K \u00e9K. IJ \u00d6\u00de \u00bd\u00cb \u00f1\u00eb\nP@\u00f1\u00cb@ Q g I K @\u00f0 @XQ \u00af \u00fa GP Y K B \u00fa\nG . P , \u00e9J. J \u00a2\u00cb@ \u00e9K P\nY\u00cb@\u00f0 l\u00cc'A \u00cb@ h. \u00f0 Q\u00cb @ A J \u00af PQ K \u00e0 @\n? \u00e9K @ B \u00f0 \u00e9J K. \u00fa G A\u00eb AK \u00ca\nm ' \u00f0 A J\u00ca \u00aa \u00ca m ' A\u00d3\n. \u00d0 @Y\u00d3 AK \u00e8\u00f1K @ : \u00e9J K. \u00fa G A\u00eb\n\u00f8 @ P@ \u00bd\u00cb\u00f1\u00ea\u00cb\u00f1 \u00af@ \u00e9 P\u0304 A\u00ab \u00d3 \u00f0 @Yg. @Yg. \u00d1\u00ea\u00d3 \u00a8\u00f1 \u00f1\u00d3 \u00fa \u00af \u00e9K \u00f1\n\u00bcA\u00aa\u00d3 \u00d5\u00ce\u00be K@ \u00e8 QK A\u00ab A K @ : \u00d0 @Y\u00d3\n.? \u00e9K @ B\u00f0 \u00e9k. Ag \u00fa \u00af A\u00d3A\u00d3 AK \u00e9<\u00cb @ Z A\n\u00e0@ Q g : \u00fa GA\u00eb\n\u00bcY \u00af Yg I J.k A\u00d3 \u00f8 Q\u00d4\u00ab \u00f0 \u00bdJ. m ' . A K @ \u00e0A \u00ab \u00fa \u00e6\u00d3 \u00fa \u00ce\u00ab Q K \u00bc QK A\u00ab \u00d3 A K @ \u00fa \u00e6 AK \u00fa \u00e6 . : h\u00f0Y\u00dc\u00d8\n\u00f8 \u00f0\u00f0@ \u00f1\u00bb \u00fa \u00e6J Q\u0304\u00aa JK. \u00e0A\u00d2\u00bb \u00fa \u00e6 K @ \u00f0 \u00bd J\u00ab \u00e9 Q\u0304\u00aaK. A K @ \u00e9\u00ca\u00d3Am.\n\u00d7 \u00d0C\u00bf \u00d3 \u00e8X \u00f0 \u00fa\n\u00e6J J.j K. \u00fa \u00e6 K @ A\u00d3 \u00f8 P \u00bdJ.jJ \u00eb Yg A\u00d3 Q\u00d4\n\u00ab \u00f0\n@\u00f0Y\u00aa \u00aeJ K. \u00f0 \u00bc\u00f1\u00d3 Q jJ K. \u00f0 \u00fa \u00bb\u00f1 Q\u0304\u00aaK \u00fa \u00ce\u00cb @ A J\u00cb @ \u00c9\u00bf \u00f0 \u00bdK. Am \u00f0 \u00bd\u00ca\u00eb@ \u00d0@Y \u00af \u00bd\u00d3Q m '. \u00e0A\u00d2\u00bb A K @ \u00f0\nJo rd\nan ia\nn\n. \u00e8 AK @ A\u00ebYK. \u00fa \u00ce\u00cb @ A\u00ea\u00cb @ Y \u00ae JK \u00e0A \u00d3 \u00fa\u00ce\u00ab \u00a1 \u00aa \u00e9\u00caJ \u00f1\u00bb \u00e0\u00f1K Y\u00cb@ \u00f8 A\u00eb \u00c9\u00d2\u00aa J K.\n, \u00fa \u00e6 . \u00fa GY\u00abA A\u00ee E @ \u00e9J K A\u00be\u00d3@ \u00fa \u00af @ X @ A\u00ea\u00cb A @ \u00f8 YK. \u00f0 , \u00f9 \u00aa\u00d3 Q . \u00f1 A\u00ea\u00caJ \u00bak@ \u00f0 \u00fa \u00d7@ \u00a9\u00d3 \u00fa \u00bek@ \u00f8 YK. , \u00bdJ \u00eb \u00d3 B B :  \u00f1K\n, Q \u00bb @ \u00f0 A\u00eb\u00f1 k@ \u00c9 J\u00d3 \u00fa GQ . J\u00aa JK. \u00f0 \u00fa \u00e6J.j K. \u00e0A\u00d2\u00bb \u00f9 \u00eb \u00f0 A\u00ea\u00d3Q m '. \u00f0 Q J\u00bb A\u00eeD . m '. A K @\n. \u00bd C g@ \u00f0 \u00bdJ.k \u00c9\u00ebA J \u00f0 \u00bd\u00ca\u00ebA J \u00fa\n\u00ce\u00cb @ \u00c8Cm\u00cc'@ I\nJ . K. \u00bd \u00af PQK \u00f0 \u00bd\u00cbAK. l\n' QK \u00f0 \u00fa\n\u00e6K. @ AK \u00bd\u00cbAK. \u00f8\nY\u00eeE \u00e9<\u00cb @ , Q g \u00e9<\u00cb @ ZA \u00e0@ :  \u00f1K \u00d0@\nA\u00eeD \u00af A\u00ab I K@ \u00fa \u00ce\u00cb @ \u00e9J \u00abA\u00d2 Jk. B@ \u00f0 \u00e9K XA\u00d6\u00cf @ \u00bd \u00af\u00f0Q \u00a3 @\u00f1\u00ab@QK B \u00f0 \u00e9<\u00cb @ @\u00f1 \u00afA jJ K. A\u00d3 \u00fa \u00ce\u00cb @ \u00d0@Qm\u00cc'@ XB\u00f0@ \u00bd J\u00ab Y\u00aaJ. K \u00f0 \u00bd \u00ae \u00af\u00f1K \u00e9<\u00cb @ \u00f0\n. . \u00fa\u00e6 \u00aa J K \u00a9\u00ca\u00a2 A JkP . . \u00e9\u00d3 PB@ \u00f8 Y\u00eb \u00e1\u00d3 A J \u00ca g A\u00d3 Y\u00aaK. \u00f0 . .Q\u00e5 J. \u00cb @ \u00c8\u00f0 Y\u00eb Y \u00fa \u00e6 .\nK Am. ' . \u00f0 \u00a9\u00d3  \u00af@\u00f0 I K@ \u00f0 \u00e9\u00ca\u00d3A\u00bf \u00e9 J\n. . \u00fa \u00bb\u00f0@ : \u00e9\u00cb I\u00ca \u00af . . @\u00f1 \u00a1 . J K \u00e0A \u00ab . . \u00bcA\u00aa\u00d3 I. \u00bbP@ h@P A K @ . . \u00bd\u00d2\u00eeE B \u00f0 : \u00fa \u00cdA \u00af \u00f0 \u00fa \u00ce\u00ab I \u00ae J\u00cb @ . . \u00e8PAJ \u00cb@ I. \u00bbQ K A\u00d3 \u00c9J. \u00af \u00f0\n\u00f8 \u00f1 \u00e9 K @ \u00d1\u00aa\u00a2\u00d6\u00cf @ I. kA \u00e1\u00d3 I. \u00ca\u00a3@ . . \u00d1\u00aa\u00a2\u00d6 \u00cf @ \u00c9 \u00f1 K A\u00d3 \u00c8\u00f0@ ? \u00a0Q\u00e5 \u00cb @ \u00f1\u00eb @ : \u00c8A \u00af . . \u00a0Q\u00e5 . . . . \u00f8 A\u00aa\u00d3 I. \u00bbQ K h@P\n\u00c9\u00bf A K \u00f0 \u00a0\u00f1 . \u00d3 \u00e0\u00f1\u00ba K \u00bd K @ \u00d1\u00ea\u00d6\u00cf @ . . K A \u00afX Q\u00e5 \u00ab \u00f0@ . . \u00e9\u00abA \u00a9K. P \u00bcY\u00eak. \u00f0 \u00bd J \u00af\u00f0 \u00e1\u00d3 Y gAK A\u00d3\nQ \u00ab \u00e1\u00d3 . . \u00bdJ \u00af \u00e9 A g \u00e9\u00cb\u00f0A\u00a3 \u00bd\u00cb\n. . \u00e9K @ : I\u00ca \u00af ? \u00fa \u00ce\u00ab \u00d1\u00ebA \u00af . . \u00f8 \u00f1 \u00c9J. \u00af \u00e9 J\u00ca\u00bf @ \u00fa \u00cd@ \u00c9\u00bfB@ \u00ae K \u00e1\u00d3\nM or\noc ca\nn\n.\u00f0QK Y K \u00e1K XA \u00ab @ \u00f1 \u00af\u00f1 \u00f0 \u00e0\u00f1J . P C\u00cb \u00f1J \u00d6 \u00dfA \u00ab ,\u00f8 QK X \u00f8 XA \u00ab @ \u00fa \u00e6 Q\u0304\u00ab \u00f0\n\u00e8Q J.\u00bb \u00f0 \u00e8Q \u00aa \u00c9\u00bf \u00fa \u00af \u00d5\u00bb A\u00aa\u00d3 \u00e0\u00f1\u00ba K \u00f0 \u00d5\u00ba K\u00f0A\u00aa K \u00fa \u00e6 \u00d6 \u00dfA \u00ab A K @ , \u00d5\u00bbYg\u00f1K. \u00d5\u00baJ \u00ca m ' \u00ba\u00d2J \u00d3 B B \u00e9\u00eb : \u00d5\u00e7' Q\u00d3\n, A\u00eeD K. \u00ca\u00be J K \u00f0 HA \u00aaK. \u00e9k. Ag \u00fa \u00e6 A\u00eeD \u00cb QK Y K \u00f0 A\u00ee E\u00f0A\u00aa KA \u00ab A K @ \u00fa \u00e6k , A\u00ebYg\u00f1K. \u00fa \u00e6 A\u00d3 \u00fa \u00e6 k AJ \u00ebA\u00eb A K A\u00aa\u00d3 \u00fa G\u00f1\u00ba K \u00fa \u00e6J \u00aaK. C\u00ab \u00f0 :I. K P\nZ\u00f1 \u00c9\u00bf \u00e1\u00d3 A\u00ea \u00a2 \u00aem ' \u00e0 @ \u00f0 \u00e9m\u00cc'A \u00cb@ \u00e9K P\nY\u00cbAK. A\u00ea \u00af PQK \u00e0 @ \u00fa\u00cdA\u00aa K \u00f0 \u00e9 KAjJ. \u00e9<\u00cb @ \u00e1\u00d3 \u00fa \u00e6\u00d2 J K \u00f0 \u00ac@ QK. \u00e9 K AgQ \u00af \u00f0 \u00e9kA KQ\u00d3 \u00e0\u00f1\u00ba K A\u00eeD A g \u00f0\n. \u00e9 KA\u00bfQK. \u00f0 \u00fa\u00cdA\u00aa K \u00e9<\u00cb @\n\u00e9\u00d4gP \u00f0 \u00d5\u00baJ \u00ca\u00ab \u00d0C \u00cb@ \u00f0 \u00d5\u00e6 \u00bam\u00cc'@ Q\u00bb Y\u00cb@ \u00e9K. \u00a1 \u00aek A\u00d6\u00df. I. J \u00aa\u00ca\u00cb HA \u00a2 \u00afAm\u00cc'@ HA J K A \u00ae\u00cb @ HAm\u00cc'A \u00cb@ \u00e1\u00d3 A\u00ea\u00ca\u00aam. ' \u00f0 A\u00eeD \u00af A\u00ea\u00cb \u00bcPAJ. K \u00f0 Q\u00e5 \u00f0\n\u00f8 YK \u00e1\u00d3 \u00fa GYJ.k. ,\u00f8 Y J\u00aa\u00cb \u00fa m . ' \u00f1\u00eb \u00f0 K A \u00afX \u00d4 g H P@X . \u00f0A \u00aaK. \u00fa \u00cd \u00e8A\u00aa\u00d3 \u00f0QK YK A \u00ab \u00f0 , \u00e8\u00f0A \u00ae\u00caK A \u00ab \u00e8\u00f0A \u00ae\u00caK A \u00ab . \u00e1K A\u00bf \u00e1 \u00af \u00e8 @P \u00d1\u00ea\u00ca J\u00ca \u00af I \u00af\u00f1\u00cb@ H. Am \u00e9J \u00ca\u00ab \u00f1J. \u00cb\n. \u00fa \u00ce \u00aa \u00bcA\u00aa\u00d3 \u00c9\u00d2\u00ba K \u00f0 \u00e9 CK. \u00fa \u00e6 \u00cb \u00bd\u00ca \u00f1 K \u00f8 XA \u00ab , \u00fa \u00e6 \u00f1 K \u00fa \u00e6 \u00f1 K : \u00fa \u00cdA \u00af \u00f0 \u00fa \u00ab@PX \u00e1 K. \u00fa G Q\u00eb ,\u00f8 A Jm \u00af \u00fa \u00e6 AK. ,\u00f0PY \u00fa\u00ce\u00ab \u00fa \u00e6\u00a2k , \u00f0Y J\u00aa\u00cb \u00fa GQk.\n\u00e9K \u00f1\n.\n\u00e9K \u00f1 , AK A\u00aa\u00d3 \u00fa \u00beJ. K \u00f0 \u00fa \u00beJ. J \u00bb @YK. \u00f1\u00eb \u00fa \u00e6k , A\u00ee EPX \u00fa \u00cd A K @ \u00fa \u00e6 A\u00d3 \u00e9k. Ag \u00fa \u00e6 HPX \u00fa \u00e6 @QK. I k , H\u00f1 \u00aa J\u00bb \u00f0 \u00fa \u00beJ. J\u00bb IK YK. ,\u00f1 J \u00ae J\u00ab \u00f0PY \u00af IK Y\n\u00e9J \u00af \u00ac\u00f1 \u00bb A K @ \u00f0 AJ \u00af \u00ac\u00f1 \u00bb \u00f1\u00eb ,\u00f1\u00baj \u00f0Q\u00e5 \u00ee D\u00bb A JK YK.\nYe m\nen\n. HCJ.\u00ab\nQ m\u00cc'@ \u00bdK Y\u00eb \u00e1\u00d3\n\u00e9k. Ag \u00f8 @ \u00f0@ \u00d1\u00ebA\u00aa\u00d3 A \u00aa J K \u00f0@ \u00e8\u00f1\u00ea \u00af \u00f1K. Q\u00e5 \u00f0@ \u00e8A\u00aa\u00d3 \u00f8 Y \u00aa J K \u00e8Y J\u00aa\u00cb \u00a8 Qj. J , \u00e8A\u00be K\u00f0@ I K. \u00e9K. \u00d5\u00ba\u00ca \u00af@ \u00d5 \u00e6 K @ \u00fa \u00e6\u00aaK\n, I J\u00cb @ \u00f1\u00ca\u00d2\u00aa J J \u00bb \u00f1 Q\u0304\u00aaK. A\u00d3 \u00e1\u00ba\u00cb I K \u00d1\u00ebY J\u00ab A K \u00fa \u00af\u00f0 , I J. \u00cb @ \u00fa \u00af I K A\u00ebY J\u00ab A\u00d3 A K \u00fa \u00af \u00f1 K @ I\u00ca \u00af A K @ , \u00fa \u00e6 I\u00ca \u00af A\u00d3 A K @\n, H\u00f1J J. \u00cb @ \u00c9\u00bf \u00fa \u00af X\u00f1k. \u00f1\u00d3 I J\u00cb @ \u00e0@ \u00f9 \u00eb\u00f0 , @Yg. @Yg. \u00e9\u00d2\u00ea\u00d3 \u00e9\u00a2 \u00ae J\u00cb \u00e9J. K @ I J.k \u00e1\u00ba\u00cb , \u00d0C\u00be\u00cb@ @ Y\u00eb \u00e1\u00d3 \u00fa \u00e6 \u00f8 Y \u00af A\u00d3 A K @ \u00fa \u00e6\u00aaK\n, A\u00ea\u00cb \u00f1 J\u00eeD . J K @ A\u00d3 @ X @ \u00d5\u00ba KB , \u00e9\u00d2\u00ea\u00d6\u00cf @ \u00e9\u00a2 \u00ae J\u00cb @ \u00e8 Y\u00ea\u00cb \u00f1\u00eeD . J K \u00e0@ \u00f1k. PA \u00af , Q k@ \u00e0A\u00be\u00d3 \u00f8 @ \u00fa \u00af \u00f0@ \u00d5\u00ba J K. \u00fa \u00af \u00e0A\u00bf Z@\u00f1\n, \u00d5\u00ba\u00aa \u00ae JK B\u00f0 \u00d5\u00bbYJ \u00aeK B A\u00d2J \u00af \u00d5\u00ba KA \u00af\u00f0@ @\u00f1\u00aaJ C \u00af , \u00a8\u00f1k. \u00e1\u00d3 \u00fa \u00e6 \u00aa K B\u00f0 \u00e1\u00d2 B \u00e9\u00ea \u00afA K ZAJ @ \u00fa \u00af \u00d5\u00ba J \u00af\u00f0 \u00e0\u00f1\u00aaJ \u00d5 \u00e6 KA \u00af\n\u00e9 \u00af Q KQ\u00d3AK \u00e9 K\u00f1 kAK A \u00ae\u00abAK \u00e1 m\u00d7 \u00fa \u00ce\u00ab AK \u00bdJ \u00af \u00c9J J \u00bb\u00f1\u00cb@ \u00d1\u00aa K\u00f0 \u00e9<\u00cb @ A\nJ . k, X\u00f1J.\u00abAK \u00bdJ \u00af \u00c9J \u00bb\u00f1\u00cb@ \u00d1\u00aa K\u00f0 \u00e9<\u00cb @ A J . k\n. \u00d0@Qm\u00cc'@ XB\u00f0AK \u00e1 \u00d3Qm. \u00d7AK \u00d5\u00bbQ\u00e5 \u00e1\u00d3 A J \u00ca m ' \u00f0 \u00e9 \u00af Q KQ\u00d3 AK \u00d5\u00ba J\u00d3 A Jk. PA m ' \u00e9<\u00cb @ . \u00d3A g P\u00f1K. A\u00a3AK Z @Q \u00aekAK ZA JJ.k. AK H. C\u00bf AK\nTable E.7: Examples of synthetically generated Arabic dialects text from STGen using JASMINE 2.7B under zero-shot setting. We color the initial prompt with gray."
        }
    ],
    "title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
    "year": 2023
}