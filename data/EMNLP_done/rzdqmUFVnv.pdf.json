{
    "abstractText": "Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the source of important words, which is crucial to edit the incomplete utterance, and introduce words from irrelevant utterances. We propose a novel and effective multi-task information interaction framework including context selection, edit matrix construction, and relevance merging to capture the multi-granularity of semantic information. Benefiting from fetching the relevant utterance and figuring out the important words, our approach outperforms existing state-of-the-art models on two benchmark datasets Restoration200K and CANAND in this field.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haowei Du"
        },
        {
            "affiliations": [],
            "name": "Dingyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Chen Li"
        },
        {
            "affiliations": [],
            "name": "Yang Li"
        },
        {
            "affiliations": [],
            "name": "Dongyan Zhao"
        }
    ],
    "id": "SP:34413166c3978bd377dfbe7d7eb43082afe9f65d",
    "references": [
        {
            "authors": [
                "Eunsol Choi",
                "He He",
                "Mohit Iyyer",
                "Mark Yatskar",
                "Wentau Yih",
                "Yejin Choi",
                "Percy Liang",
                "Luke Zettlemoyer."
            ],
            "title": "Quac: Question answering in context",
            "venue": "arXiv preprint arXiv:1808.07036.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Ahmed Elgohary",
                "Denis Peskov",
                "Jordan BoydGraber."
            ],
            "title": "Can you unpack that? learning to rewrite questions-in-context",
            "venue": "Can You Unpack That? Learning to Rewrite Questions-in-Context.",
            "year": 2019
        },
        {
            "authors": [
                "Jie Hao",
                "Linfeng Song",
                "Liwei Wang",
                "Kun Xu",
                "Zhaopeng Tu",
                "Dong Yu."
            ],
            "title": "Rast: Domain-robust dialogue rewriting as sequence tagging",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4913\u20134924.",
            "year": 2021
        },
        {
            "authors": [
                "Mengzuo Huang",
                "Feng Li",
                "Wuhe Zou",
                "Weidong Zhang"
            ],
            "title": "Sarg: A novel semi autoregressive generator for multi-turn incomplete utterance restoration",
            "year": 2021
        },
        {
            "authors": [
                "Shumpei Inoue",
                "Tsungwei Liu",
                "Nguyen Hong Son",
                "Minh-Tien Nguyen."
            ],
            "title": "Enhance incomplete utterance restoration by joint learning token extraction and text generation",
            "venue": "arXiv preprint arXiv:2204.03958.",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Jin",
                "Linfeng Song",
                "Lifeng Jin",
                "Dong Yu",
                "Daniel Gildea."
            ],
            "title": "Hierarchical context tagging for utterance rewriting",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10849\u201310857.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Qian Liu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Bin Zhou",
                "Dongmei Zhang."
            ],
            "title": "Incomplete utterance rewriting as semantic segmentation",
            "venue": "arXiv preprint arXiv:2009.13166.",
            "year": 2020
        },
        {
            "authors": [
                "Ozan Oktay",
                "Jo Schlemper",
                "Loic Le Folgoc",
                "Matthew Lee",
                "Mattias Heinrich",
                "Kazunari Misawa",
                "Kensaku Mori",
                "Steven McDonagh",
                "Nils Y Hammerla",
                "Bernhard Kainz"
            ],
            "title": "Attention u-net: Learning where to look for the pancreas",
            "year": 2018
        },
        {
            "authors": [
                "Zhufeng Pan",
                "Kun Bai",
                "Yan Wang",
                "Lianqiang Zhou",
                "Xiaojiang Liu."
            ],
            "title": "Improving open-domain dialogue systems via multi-turn incomplete utterance restoration",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Siva Reddy",
                "Danqi Chen",
                "Christopher D Manning."
            ],
            "title": "Coqa: A conversational question answering challenge",
            "venue": "Transactions of the Association for Computational Linguistics, 7:249\u2013266.",
            "year": 2019
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox."
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany,",
            "year": 2015
        },
        {
            "authors": [
                "Shuzheng Si",
                "Shuang Zeng",
                "Baobao Chang."
            ],
            "title": "Mining clues from incomplete utterance: A queryenhanced network for incomplete utterance rewriting",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Hui Su",
                "Xiaoyu Shen",
                "Rongzhi Zhang",
                "Fei Sun",
                "Pengwei Hu",
                "Cheng Niu",
                "Jie Zhou."
            ],
            "title": "Improving multi-turn dialogue modelling with utterance rewriter",
            "venue": "arXiv preprint arXiv:1906.07004.",
            "year": 2019
        },
        {
            "authors": [
                "Kai Sun",
                "Dian Yu",
                "Jianshu Chen",
                "Dong Yu",
                "Yejin Choi",
                "Claire Cardie."
            ],
            "title": "Dream: A challenge data set and models for dialogue-based reading comprehension",
            "venue": "Transactions of the Association for Computational Linguistics, 7:217\u2013231.",
            "year": 2019
        },
        {
            "authors": [
                "Yong Zhang",
                "Zhitao Li",
                "Jianzong Wang",
                "Ning Cheng",
                "Jing Xiao."
            ],
            "title": "Self-attention for incomplete utterance rewriting",
            "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8047\u20138051. IEEE.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently increasing attention has been paid to multi-turn dialogue modeling (Choi et al., 2018; Reddy et al., 2019; Sun et al., 2019) and the major challenge in this field is that speakers tend to use incomplete utterances for brevity, such as referring back to (i.e., co-reference) or omitting (i.e., ellipsis) entities or concepts that appear in dialogue history. Su et al. (2019) shows that ellipsis and co-reference can exist in more than 70% of dialogue utterances. To handle this, the task of Incomplete Utterance Rewriting (IUR) (Pan et al., 2019; Elgohary et al., 2019) is proposed to rewrite an incomplete utterance into an utterance which is semantically equivalent but self-contained to be understood without context.\nTo maintain the similarity of semantic structure between the incomplete utterance and rewritten utterance, recent approaches formulate it as a word edit task (Liu et al., 2020; Zhang et al., 2022) and predict the edit types by capturing the semantic relations between words. However, the sentence-level semantic relations between contextual utterances and incomplete utterance are ne-\n*These authors contributed equally to this work. \u2020Corresponding Author.\nglected. Unaware of which sentences contain the words needed to rewrite the incomplete utterance (important words)(Inoue et al., 2022), these models introduce incorrect words from irrelevant sentences into the rewritten utterance.\nWe take an example in table 1. The incomplete utterance has the phenomenon of co-reference (\u201che\u201d) and ellipsis (\u201canything else\u201d). Because the baseline model RUN (Liu et al., 2020) does not fetch the correct source sentence (u5) and the important words (\u201cphilosophy\u201d), it introduces irrelevant words (\u201cGlaser and Henry\u201d) into rewriting the utterance.\nTo identify the source sentences of important words and incorporate the sentence-level relations among contextual utterances and incomplete utterance, we propose our multi-granularity information capturing framework for IUR. Firstly, we classify the sentences in contexts into relevant or irrelevant\nutterances and match the rewritten utterance with the relevant contexts. Then we capture the tokenlevel semantic relations to construct the token edit matrix. The predicted relevances among sentences in contexts and incomplete utterance, which encodes the sentence-level relations, are utilized to mask the edit matrix. The rewritten utterance is derived by referring to the token matrix. We conduct experiments on two benchmark datasets in IUR and outperform the prior state-of-the-art by about 1.0 score in Restoration-200K dataset and derive competitive performance in CANARD dataset across different metrics including BLEU, ROUGE and F-score.\nOur contributions can be summarized as: 1. We are the first to incorporate sentence-level semantic relations between the utterances in contexts and the incomplete utterance to enhance the ability to seize the source sentence and figure out the important words. 2. We propose the multi-task information interaction framework to capture the multi-granularity of semantic information. Our approach outperforms existing methods on the benchmark dataset of this field, becoming the new state-of-the-art."
        },
        {
            "heading": "2 Related Work",
            "text": "There are two main streams of approaches to tackle the task of IUR: generation-based (Huang et al., 2021; Inoue et al., 2022) and edit-based (Liu et al., 2020; Si et al., 2022). Generation-based models solve this task as a seq2seq problem. Su et al. (2019) utilize pointer network to respectively predict the prob of tokens in rewritten utterance from contexts and incomplete utterance. Hao et al. (2021) formulate the task as sequence tagging to reduce the search space. Huang et al. (2021) combine a source sequence tagger with an LSTM-based decoder to maintain the grammatical correctness. Generation models lack to capture the trait of IUR, where the main semantic structure of a rewritten utterance is usually similar to the original incomplete utterance.\nEdit-based models focus on predicting wordlevel or span-level edit type between contextual utterances and the incomplete utterance. Liu et al. (2020) formulate this task as semantic segmentation and propose U-Net (Ronneberger et al., 2015; Oktay et al., 2018) to encode the word-level semantic relations. To incorporate the rich information in self-attention weights of pretrained language model\n(PTM) (Devlin et al., 2018), Zhang et al. (2022) directly take the self-attention weight matrix as the word2word edit matrix. Though these models produce competitive performance, the sentence-level semantic relations are neglected and the models tend to introduce incorrect words from irrelevant contextual utterances."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "By figure 1, our approach contains four components: context selection, edit matrix construction, relevance merging and intention check."
        },
        {
            "heading": "3.2 Context Selection",
            "text": "In this part, we capture the semantic relations between contextual utterances and incomplete utterance. Following RUN, We utilize BERT to encode the contextual representation of contexts and incomplete utterance. The input to PTM is the sequence of contexts concatenated by incomplete utterance, and the \u201c[SEP]\u201d token is applied to separate different sentences. The utterance representation is derived by pooling the hidden states of words it contains: [c1, c2, \u00b7 \u00b7 \u00b7 , cn,u] = Pooling(BERT[w1,w1, \u00b7 \u00b7 \u00b7 ,wN]), where ci denotes the representation of i-th utterance in contexts, u denotes the representation of incomplete utterance and wi denotes the i-th word token in the input sequence. We apply a MLP classifier to predict the relevance between each utterance in contexts and incomplete utterance:\nLsel = CrossEntropy({ri}, {Ri}) (1) ri = MLP([ci;u]) i = 1, 2, \u00b7 \u00b7 \u00b7 , n (2)\nwhere ri and Ri denote the predicted relevance and label of i-th utterance in contexts respectively.\nConsidering the golden label is not provided, we retrieve the utterance that contains the words used to rewrite the utterance as the label of relevant utterance. To enhance the compatibility of selected contextual utterance with the rewritten utterance, we sample negative utterances from contexts of other cases in the batch and predict the match score:\nLmat = CrossEntropy({mi}, {Mi}) (3) mi = MLP([ci + u; r]) i \u2208 CP \u222a CN (4)\nMi = { 1 i \u2208 CP 0 i \u2208 CN\n(5)\nwhere mi and Mi denote the predicted and labeled matching score of i-th contextual utterance, CP and CN denote the golden contextual utterance that includes important words and the sampled negative utterance."
        },
        {
            "heading": "3.3 Edit Matrix Construction",
            "text": "Following (Liu et al., 2020), we predict token2token edit type (Insert, Replace, None) based on word representations from PTM with U-Net and build the word edit matrix. The entry at row i and column j in the matrix denotes the edit type between i-th token in contexts and j-th token in\nincomplete utterance. It can be formulated as:\neij = MLP(U(wi,wj)) (6)\nwhere eij \u2208 R3 denotes the predicted probability of 3 edit types between i-th token in contexts and jth token in incomplete utterance, 1 \u2264 i \u2264 NC , 1 \u2264 j \u2264 NU , U denotes the U-Net architecture, NC and NU denotes the context length and incomplete utterance length."
        },
        {
            "heading": "3.4 Relevance Merging",
            "text": "If some utterance in contexts is classified as relevant by our Context Selection module, it is more possible for the words in this utterance to be adopted into editing the incomplete utterance. In this part, the relevant confidence of the utterance is equally added to the predicted prob of edit type Insert and Replace for all its constitutive words with the words in incomplete utterance:\nLedit = CrossEntropy({e\u0302ij}, {Eij}) (7) e\u0302Insertij = e Insert ij + \u03b1 \u2217 ri (8) e\u0302Replaceij = e Replace ij + (1\u2212 \u03b1) \u2217 ri (9)\nwhere Eij denotes the golden edit type between i-th token in contexts and j-th token in incomplete utterance and \u03b1 denotes parameters to tune. The relevance merging process can be seen as utilizing the relevance predicted to \u201csoftly mask\u201d the edit matrix. Even if one sentence is not selected as relevant contexts, the probabilities of edit types Insert and Replace are not strictly set to zero."
        },
        {
            "heading": "3.5 Intention Check",
            "text": "To maintain the intention consistency between the incomplete utterance and the rewritten utterance, we project the representation of incomplete utterance and rewritten utterance into intention space\nand close the distance.\nLint = KL(MLP([C;u]),MLP(r)) (10)\nC = Pooling({ci, i \u2208 CP }) (11)\nwhere C denotes the pooling representations of utterances in contexts and KL denotes the KLdivergence function."
        },
        {
            "heading": "3.6 Training and Rewriting",
            "text": "The final training loss function is computed by the weighted sum of edit loss, selection loss, matching loss and intention loss:\nLfinal = Ledit + \u03b11Lsel + \u03b12Lmat + \u03b13Lint\nwhere \u03b1i, i = 1, 2, 3 denote parameters to tune. The rewritten utterance is manipulated based on the predicted edit matrix [e\u0302ij ]\n1\u2264j\u2264NU 1\u2264i\u2264NC . That is, if\ne\u0302ij = Insert, we insert the i-th token in contexts before the j-th token in incomplete utterance, which is similar for Replace type."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets: We do experiments on two benchmark IUR datasets from different languages: Restoration200K (Chinese, (Pan et al., 2019)) and CANARD (English,(Elgohary et al., 2019)). Metrics: Following Liu et al. (2020), we utilize BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and F-score (Pan et al., 2019) as evaluation metrics. Baselines: we compare our approach with recent competitive approaches as follows: RUN (Liu et al., 2020) formulates IUR as a semantic segmentation task and obtains a much faster inference speed than generating from scratch, which can be seen as our backbone; SARG (Huang et al., 2021) proposes a semi auto-regressive generator with the high efficiency and flexibility; RAU (Zhang et al., 2022)\ndirectly extracts the co-reference and omission relationship from the self-attention weight matrix of the transformer; RAST (Hao et al., 2021) proposes a novel sequence-tagging based model to reduce the search space; QUEEN (Si et al., 2022) designs an explicit query template to bring guided semantic structural knowledge; HCT (Jin et al., 2022) constructs a hierarchical context tagger that mitigates the multiple spans issue by predicting slotted rules."
        },
        {
            "heading": "4.2 Results",
            "text": "By table 2 and 3, compared with our backbone RUN, our model improves about 1.0 ROUGE and BLEU score, and 2.5 F-score in Restoration-200K dataset, as well as 2.0 ROUGE and BLEU score in CANARD dataset. It demonstrates the efficiency of our multi-task framework to fetch the source of important words and avoid irrelevant words. Specially, we outperform QUEEN by 0.7 BLEU1, 0.6 BLEU2, 0.7 ROUGE1 and 0.3 ROUGE2, and beat RAU by about 1.0 F-score on Restoration-200K dataset, becoming the new state-of-the-art. Moreover, our model shows competitive performance on CANARD dataset, which beats QUEEN by 0.5 BLEU1 score."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "To explore different modules of our approach, we conduct the ablation study. Compared with the \u201csoft mask\u201d of relevance merging in section 3.4, we design an ablation with \u201chard mask\u201d merging: if a sentence in contexts is classified as irrelevant in section 3.2, the probabilities of Insert and Rewrite between its words and the words in the incomplete utterance are set to 0. In table 4, context selection, relevance merging and intention checking show progressive improvement across different metrics. Compared with hard mask of relevance merging, the soft mask method is overall better. The sentence classified as irrelevant may still contain im-\nportant words, so merging its relevance in a soft way is necessary. Context marching module helps the approach to capture the important words from contexts."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we argue that capturing the source of important words can diminish to introduce irrelevant words for IUR. We propose a novel and effective multi-task framework including context selection, edit matrix construction, and relevance merging to capture the multi-granularity of semantic information and fetch the relevant utterance. we do experiments on two benchmark datasets in IUR and show competitive performance.\nLimitations\nWe propose a novel and effective multi-task framework to capture the multi-granularity of semantic information and fetch the relevant utterance. With the population of large language model (LLM), the multi-task finetuning framework may bring more computation cost. We will explore the combination of our approach with LLM in the future."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key Research and Development Program of China (No.2021YFC3340304)."
        },
        {
            "heading": "A Example Appendix",
            "text": "This is a section in the appendix."
        }
    ],
    "title": "Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting",
    "year": 2023
}