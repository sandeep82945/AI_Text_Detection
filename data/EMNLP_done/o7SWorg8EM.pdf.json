{
    "abstractText": "Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the paper\u2019s text in addition to the table. Our dataset, Scientific Table Entity Linking (S2abEL), focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperforms a state-of-the-art generic table EL method. The best baselines fall below human performance, and our analysis highlights avenues for improvement. Code and the dataset is available at: https://github.com/ allenai/S2abEL/tree/main.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuze Lou"
        },
        {
            "affiliations": [],
            "name": "Bailey Kuehl"
        },
        {
            "affiliations": [],
            "name": "Erin Bransom"
        },
        {
            "affiliations": [],
            "name": "Sergey Feldman"
        },
        {
            "affiliations": [],
            "name": "Aakanksha Naik"
        },
        {
            "affiliations": [],
            "name": "Doug Downey"
        }
    ],
    "id": "SP:3de11870caad2984b5fd3ad383c6fedf49a0db8e",
    "references": [
        {
            "authors": [
                "S\u00f6ren Auer",
                "Christian Bizer",
                "Georgi Kobilarov",
                "Jens Lehmann",
                "Richard Cyganiak",
                "Zachary Ives."
            ],
            "title": "Dbpedia: A nucleus for a web of open data",
            "venue": "Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Se-",
            "year": 2007
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Chandra Sekhar Bhagavatula",
                "Thanapon Noraset",
                "Doug Downey."
            ],
            "title": "Tabel: Entity linking in web tables",
            "venue": "The Semantic Web-ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part I,",
            "year": 2015
        },
        {
            "authors": [
                "Roi Blanco",
                "Giuseppe Ottaviano",
                "Edgar Meij."
            ],
            "title": "Fast and space-efficient entity linking for queries",
            "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM \u201915, page 179\u2013188, New York, NY, USA. Association for",
            "year": 2015
        },
        {
            "authors": [
                "Arman Cohan",
                "Waleed Ammar",
                "Madeleine van Zuylen",
                "Field Cady."
            ],
            "title": "Structural scaffolds for citation intent classification in scientific publications",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Israel Cohen",
                "Yiteng Huang",
                "Jingdong Chen",
                "Jacob Benesty",
                "Jacob Benesty",
                "Jingdong Chen",
                "Yiteng Huang",
                "Israel Cohen."
            ],
            "title": "Pearson correlation coefficient",
            "venue": "Noise reduction in speech processing, pages 1\u20134.",
            "year": 2009
        },
        {
            "authors": [
                "Silviu Cucerzan."
            ],
            "title": "Large-scale named entity disambiguation based on Wikipedia data",
            "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages",
            "year": 2007
        },
        {
            "authors": [
                "Fabio Petroni"
            ],
            "title": "Autoregressive entity retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Cong Yu"
            ],
            "title": "Turl: table understanding through",
            "year": 2020
        },
        {
            "authors": [
                "huri",
                "Jens Lehmann"
            ],
            "title": "Earl: joint entity and",
            "year": 2018
        },
        {
            "authors": [
                "Weld",
                "Eric Horvitz"
            ],
            "title": "2022. A computational",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder."
            ],
            "title": "Universal language model fine-tuning for text classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328\u2013339, Melbourne, Australia.",
            "year": 2018
        },
        {
            "authors": [
                "Hiroshi Iida",
                "Dung Thai",
                "Varun Manjunatha",
                "Mohit Iyyer."
            ],
            "title": "TABBIE: Pretrained representations of tabular data",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Sarthak Jain",
                "Madeleine van Zuylen",
                "Hannaneh Hajishirzi",
                "Iz Beltagy."
            ],
            "title": "SciREX: A challenge dataset for document-level information extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7506\u2013",
            "year": 2020
        },
        {
            "authors": [
                "David Jurgens",
                "Srijan Kumar",
                "Raine Hoover",
                "Dan McFarland",
                "Dan Jurafsky."
            ],
            "title": "Measuring the evolution of a scientific field through citation frames",
            "venue": "Transactions of the Association for Computational Linguistics, 6:391\u2013406.",
            "year": 2018
        },
        {
            "authors": [
                "Marcin Kardas",
                "Piotr Czapla",
                "Pontus Stenetorp",
                "Sebastian Ruder",
                "Sebastian Riedel",
                "Ross Taylor",
                "Robert Stojnic."
            ],
            "title": "AxCell: Automatic extraction of results from machine learning papers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "gele Zamarron",
                "Madeleine van Zuylen",
                "Daniel S. Weld"
            ],
            "title": "The semantic scholar open data",
            "venue": "platform. ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Belinda Z. Li",
                "Sewon Min",
                "Srinivasan Iyer",
                "Yashar Mehdad",
                "Wen-tau Yih."
            ],
            "title": "Efficient one-pass end-to-end entity linking for questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Lajanugen Logeswaran",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova",
                "Jacob Devlin",
                "Honglak Lee."
            ],
            "title": "Zero-shot entity linking by reading entity descriptions",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Mary L McHugh."
            ],
            "title": "Interrater reliability: the kappa statistic",
            "venue": "Biochemia medica, 22(3):276\u2013282.",
            "year": 2012
        },
        {
            "authors": [
                "Varish Mulwad",
                "Tim Finin",
                "Vijay S Kumar",
                "Jenny Weisenberg Williams",
                "Sharad Dixit",
                "Anupam Joshi"
            ],
            "title": "A practical entity linking system for tables in scientific literature",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Eric Michael Smith",
                "Y-Lan Boureau",
                "Jason Weston."
            ],
            "title": "Recipes for building an open-domain chatbot",
            "venue": "Proceedings of the 16th Conference of",
            "year": 2021
        },
        {
            "authors": [
                "Pedro Ruas",
                "Francisco M. Couto."
            ],
            "title": "Nilinker: Attention-based approach to nil entity linking",
            "venue": "Journal of Biomedical Informatics, 132:104137.",
            "year": 2022
        },
        {
            "authors": [
                "Sunil Sahu",
                "Ashish Anand",
                "Krishnadev Oruganty",
                "Mahanandeeshwar Gattu."
            ],
            "title": "Relation extraction from clinical texts using domain invariant convolutional neural network",
            "venue": "Proceedings of the 15th",
            "year": 2016
        },
        {
            "authors": [
                "Kurt Shuster",
                "Mojtaba Komeili",
                "Leonard Adolphs",
                "Stephen Roller",
                "Arthur Szlam",
                "Jason Weston"
            ],
            "title": "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
            "year": 2022
        },
        {
            "authors": [
                "Hongyin Tang",
                "Xingwu Sun",
                "Beihong Jin",
                "Fuzheng Zhang."
            ],
            "title": "A bidirectional multi-paragraph reading model for zero-shot entity linking",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13889\u201313897.",
            "year": 2021
        },
        {
            "authors": [
                "Nan Tang",
                "Ju Fan",
                "Fangyi Li",
                "Jianhong Tu",
                "Xiaoyong Du",
                "Guoliang Li",
                "Sam Madden",
                "Mourad Ouzzani."
            ],
            "title": "Rpt: relational pre-trained transformer is almost all you need towards democratizing data preparation",
            "venue": "arXiv preprint arXiv:2012.02469.",
            "year": 2020
        },
        {
            "authors": [
                "Nan Tang",
                "Ju Fan",
                "Fangyi Li",
                "Jianhong Tu",
                "Xiaoyong Du",
                "Guoliang Li",
                "Sam Madden",
                "Mourad Ouzzani."
            ],
            "title": "Rpt: Relational pre-trained transformer is almost all you need towards democratizing data preparation",
            "venue": "Proc. VLDB Endow., 14(8):1254\u20131261.",
            "year": 2021
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107",
                "Markus Kr\u00f6tzsch."
            ],
            "title": "Wikidata: A free collaborative knowledgebase",
            "venue": "Commun. ACM, 57(10):78\u201385.",
            "year": 2014
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Koki Washio",
                "Hiroyuki Shindo",
                "Yuji Matsumoto."
            ],
            "title": "Global entity disambiguation with pretrained contextualized embeddings of words and entities",
            "venue": "arXiv preprint arXiv:1909.00426.",
            "year": 2019
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Koki Washio",
                "Hiroyuki Shindo",
                "Yuji Matsumoto."
            ],
            "title": "Global entity disambiguation with BERT",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Wenhao Yu",
                "Zongze Li",
                "Qingkai Zeng",
                "Meng Jiang."
            ],
            "title": "Tablepedia: Automating pdf table reading in an experimental evidence exploration and analytic system",
            "venue": "The World Wide Web Conference, WWW \u201919, page 3615\u20133619, New York, NY, USA. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Wenhao Yu",
                "Wei Peng",
                "Yu Shu",
                "Qingkai Zeng",
                "Meng Jiang."
            ],
            "title": "Experimental evidence extraction system in data science with hybrid table features and ensemble learning",
            "venue": "Proceedings of The Web Conference 2020, WWW \u201920, page 951\u2013961, New York,",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Zhang",
                "Edgar Meij",
                "Krisztian Balog",
                "Ridho Reinanda."
            ],
            "title": "Novel entity discovery from web tables",
            "venue": "Proceedings of The Web Conference 2020, WWW \u201920, page 1298\u20131308, New York, NY, USA. Association for Computing Machinery.",
            "year": 2020
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Entity Linking (EL) is a longstanding problem in natural language processing and information extraction. The goal of the task is to link textual mentions to their corresponding entities in a knowledge base (KB) (Cucerzan, 2007), and it serves as a building block for various knowledge-intensive applications, including search engines (Blanco et al., 2015), question-answering systems (Dubey et al., 2018), and more. However, existing EL methods and datasets primarily focus on linking mentions\n\u2217Work done during an internship at AI2.\nfrom free-form natural language (Gu et al., 2021; De Cao et al., 2021; Li et al., 2020; Yamada et al., 2022). Some consider tabular data, but focus on tables from the general domain (Deng et al., 2020; Tang et al., 2021b; Iida et al., 2021; Yu et al., 2019). Despite significant research in EL, there is a lack of datasets and methods for EL in scientific tables. Linking entities in scientific tables holds promise for accelerating science in multiple ways: from augmented reading applications that help users understand the meaning of table cells without diving into the document (Head et al., 2021) to automated knowledge base construction that unifies disparate tables, enabling complex question answering or hypothesis generation (Hope et al., 2022).\nEL in science is challenging because the set of scientific entities is vast and always growing, and existing knowledge bases are highly incomplete. A traditional \"closed world\" assumption often made in EL systems, whereby all mentions have corresponding entities in the target KB, is not realistic in scientific domains. It is important to detect which mentions are entities not yet in the reference KB, referred to as outKB mentions. Even for human annotators, accurately identifying whether a rarely-seen surface form actually refers to a rarely-mentioned long-tail inKB entity or an outKB entity requires domain expertise and a significant effort to investigate the document and the target KB. A further challenge is that entity mentions in scientific tables are often abbreviated and opaque, and require examining other context in the caption and paper text for disambiguation. An example is shown in Figure 1.\nIn this paper, we make three main contributions. First, we introduce S2abEL, a high-quality humanannotated dataset for EL in machine learning results tables. The dataset is sufficiently large for training and evaluating models on table EL and relevant sub-tasks, including 52,257 annotations of appropriate types for table cells (e.g. method, dataset),\n9,565 annotations of attributed source papers and candidate entities for mentions, and 8,429 annotations for entity disambiguation including outKB mentions. To the best of our knowledge, this is the first dataset for table EL in the scientific domain. Second, we propose a model that serves as a strong baseline for each of the sub-tasks, as well as end-to-end table EL. We conduct a comprehensive comparison between our approach and existing approaches, where applicable, for each sub-task. Our method significantly outperforms TURL (Deng et al., 2020), a state-of-the-art method closest to the table EL task, but only designed for generaldomain tables. We also provide a detailed error analysis that emphasizes the need for improved methods to address the unique challenges of EL from scientific tables with outKB mentions."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Entity Linking",
            "text": "In recent years, various approaches have been proposed for entity linking from free-form text, leveraging large language models (Gu et al., 2021; De Cao et al., 2021; Li et al., 2020; Yamada et al., 2019). Researchers have also attempted to extend EL to structured Web tables, but they solely rely on table contents and do not have rich surrounding text (Deng et al., 2020; Zhang et al., 2020; Bhagavatula et al., 2015; Mulwad et al., 2023; Tang et al., 2020; Iida et al., 2021). Most of these works focus\non general-purpose KBs such as Wikidata (Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014) and DBPedia (Auer et al., 2007) and typically test their approaches with the assumption that the target KB is complete with respect to the mentions being linked (e.g., De Cao et al., 2021; Deng et al., 2020; Hoffart et al., 2011; Tang et al., 2021a; Yamada et al., 2019).\nThere is a lack of high-quality datasets for table EL in the scientific domain with abundant outKB mentions. Recent work by Ruas and Couto (2022) provides a dataset that artificially mimics an incomplete KB for biomedical text by removing actual referent entities but linking concepts to the direct ancestor of the referent entities. In contrast, our work provides human-annotated labels of realistic missing entities for scientific tables, without relying on the target KB to contain ancestor relations. Our dataset offers two distinct advantages: first, it provides context from documents in addition to original table mentions, and second, it explicitly identifies mentions referring to outKB entities."
        },
        {
            "heading": "2.2 Scientific IE",
            "text": "The field of scientific information extraction (IE) aims to extract structured information from scientific documents. Various extraction tasks have been studied in this area, such as detecting and classifying semantic relations (Jain et al., 2020; Sahu et al., 2016), concept extraction (Fu et al., 2020), automatic leaderboard construction (Kardas et al., 2020; Hou et al., 2019), and citation analysis (Jur-\ngens et al., 2018; Cohan et al., 2019). Among these, Kardas et al., 2020; Hou et al., 2019; Yu et al., 2019, 2020 are the closest to ours. Given a set of papers, they begin by manually extracting a taxonomy of tasks, datasets, and metric names from those papers. Whereas our data set maps each entity to an existing canonical external KB (PwC), they target a taxonomy manually built from the particular papers and surface forms they extract from. Notably, this taxonomy emphasizes lexical representations, with entities such as \"AP\" and \"Average Precision\" treated as distinct entities in the taxonomy despite being identical metrics in reality, due to different surface forms appearing in the papers. Such incomplete and ambiguous entity identification makes it difficult for users to interpret the results and limits the practical applicability of the extracted information. In contrast, we propose a dataset and baselines for the end-to-end table EL task, beginning with a table in the context of a paper and ending with each cell linked to entities in the canonicalized ontology of the target KB (or classified as outKB)."
        },
        {
            "heading": "3 Entity Linking in Scientific Tables",
            "text": "Our entity linking task takes as input a reference KB (the Papers with Code1 taxonomy in our experiments), a table in a scientific paper, and the table\u2019s surrounding context. The goal is to output an entity from the KB for each table cell (or \"outKB\" if none). We decompose the task into several subtasks, discussed below. We then present S2abEL, the dataset we construct for scientific table EL."
        },
        {
            "heading": "3.1 Task Definition",
            "text": "Cell Type Classification (CTC) is the task of identifying types of entities contained in a table cell, based on the document in which the cell appears. This step is helpful to focus the later linking task on the correct type of entities from the target KB, and also excludes non-entity cells (e.g. those containing numeric values used to report experimental results) from later processing. Such exclusion removes a substantial fraction of table cells (74% in our dataset), reducing the computational cost.\nOne approach to CTC is to view it as a multilabel classification task since a cell may contain multiple entities of different types. However, our initial investigation found that only mentions of datasets and metrics co-appear to a notable degree\n1https://paperswithcode.com/\n(e.g., \"QNLI (acc)\" indicates the accuracy of some method evaluated on the Question-answering NLI dataset (Wang et al., 2018)). Therefore, we introduce a separate class for these instances, reducing CTC to a single-label classification task with four positive classes: method, dataset, metric, and dataset&metric.\nAttributed Source Matching (ASM) is the task of identifying attributed source(s) for a table cell within the context of the document. The attributed source(s) for a concept in a document p is the reference paper mentioned in p to which the authors of p attribute the concept. ASM is a crucial step in distinguishing similar surface forms and finding the correct referent entities. For example, in Figure 1, ASM can help clarify which entities \"BlenderBot 1\" and \"R2C2 BlenderBot\" refer to, as the first mention is attributed to Roller et al., 2021 while the second mention is attributed to Shuster et al., 2022. Identifying these attributions helps a system uniquely identify these two entities despite their very similar surface forms and the fact that their contexts in the document often overlap. In this work, we consider the documents listed in the reference section and the document itself as potential sources for attribution. The inclusion of the document itself is necessary since concepts may be introduced in the current document for the first time.\nCandidate Entity Retrieval (CER) is the process of identifying a small set of entities from the target KB that are most likely to be the referent entity for a table cell within the context of the document. The purpose of this step is to exclude unlikely candidates and pass only a limited number of candidates to the next step, to reduce computational cost.\nEntity Disambiguation (ED) with outKB Identification is the final stage. The objective is to determine the referent entity (or report outKB if none), given a table cell and its candidate entity set. The identification of outKB mentions significantly increases the complexity of the EL task, as it requires the method to differentiate between e.g. an unusual surface form of an inKB entity versus an outKB mention. However, distinguishing outKB mentions is a critical step in rapidly evolving domains like science, where existing KBs are highly incomplete."
        },
        {
            "heading": "3.2 Dataset Construction",
            "text": "Obtaining high-quality annotations for S2abEL is non-trivial. Identifying attributed sources and gold entities requires a global understanding of the text and tables in the full document. However, asking annotators to read every paper fully is prohibitively expensive. Presenting the full list of entities in the target KB to link from is also not feasible, while showing annotators short auto-populated candidate entity sets may introduce bias and miss gold entities. We address these challenges by designing a special-purpose annotation interface and pipeline, as detailed below.\nIn the construction process, we used two inhouse annotators with backgrounds in data analytics and data science, both having extensive experience in reading and annotating scientific papers. In addition, one author of the paper (author A) led and initial training phase with the annotators, and another author of the paper (author B) was responsible for evaluating the inter-annotator agreement (IAA) at the end of the annotation process.\nBootstrapping existing resources \u2014 We began constructing our dataset by populating it with tables and cell type annotations from SegmentedTables2 (Kardas et al., 2020), a dataset where table cells have been extracted from papers and stored in an array format. Each cell is annotated according to whether it is a paper, metric, and so on; and each paper is classified into one of eleven categories (e.g., NLI and Image Generation). To gather data for the ASM task, we fine-tuned a T5-small (Raffel et al., 2022) model to extract the last name of the first author, year, and title for each paper that appears in the reference section of any papers in our dataset from the raw reference strings. We then used the extracted information to search for matching papers in Semantic Scholar (Kinney et al., 2023), to obtain their abstracts. Since the search APIs do not always return the matching paper at the top of the results, we manually verified the output for each query.\nTarget KB \u2014 Papers with Code (PwC)34 is a free and open knowledge base in the scientific domain with a total of 304,611 papers, 6,550 datasets, and 1,942 methods entities as of this writing. PwC\n2https://github.com/paperswithcode/ axcell/releases\n3Our corpus is based on Papers with Code 2022/07 dump. 4https://github.com/paperswithcode/\npaperswithcode-data\nincludes basic relations between entities, such as relevant entities for a paper (denoted as PaperRelatesTo-Entity in the rest of the paper), the introducing paper for an entity, etc. Its data is collected from previously curated results and collaboratively edited by the community. While the KB has good precision, its coverage is not exhaustive \u2014 in our experiments, 42.8% of our entity mentions are outKB, and many papers have empty PaperRelatesTo-Entity relations.\nHuman Annotation \u2014 We developed a web interface using the Flask5 library for the annotation process. It provides annotators with a link to the original paper, an indexed reference section, and annotation guidelines. The detailed annotation interface with instructions can be found at Appendix C.\nFor the CTC sub-task, we asked annotators to make necessary modifications to correct errors in SegmentedTables and accommodate the extra dataset&metric class. During this phase, 15% of the original labels were changed. For the ASM subtask, annotators were asked to read relevant document sections for each cell and identify attributed sources, if any. This step can require a global understanding of the document, but candidate lists are relatively small since reference sections usually contain just tens of papers. For the EL sub-task, the web interface populates each cell with entity candidates that are 1) returned from PwC with the cell content as the search string, and/or 2) associated with the identified attributed paper(s) for this cell via the Paper-RelatesTo-Entity relation in PwC. Automatic candidate population is designed to be preliminary to prevent annotators from believing that gold entities should always come from the candidate set. Annotators were also asked to search against PwC using different surface forms of the cell content (e.g., full name, part of the cell content) before concluding that a cell refers to an outKB entity.\nTo ensure consistency and high quality, we conducted a training phase led by author A, where the two annotators were given four papers at a time to perform all annotation tasks. We then calculated the IAA between author A and each annotator for the four papers using Cohen\u2019s Kappa (McHugh, 2012), followed by disagreement discussion and guideline refinement. This process was repeated until the IAA score achieves \"substantial agreement\" as per (McHugh, 2012). Afterward, the remain-\n5flask.palletsprojects.com\ning set of papers was given to the annotators for annotation."
        },
        {
            "heading": "3.3 Dataset and Annotation Statistics",
            "text": "Dataset Statistics \u2014 Table 1 provides a summary of the statistics for S2abEL. ASM and EL annotations are only available for cells labeled positively in CTC. Metrics only are not linked to entities due to the lack of a controlled metric ontology in PwC. It is worth noting that S2abEL contains 3,610 outKB mentions versus 4,819 inKB mentions, presenting a significantly different challenge from prior datasets that mostly handle inKB mentions. More details are in Appendix A. Post-hoc IAA Evaluation \u2014 We conducted a posthoc evaluation to verify the quality of annotations, where author B, who is a researcher with a Ph.D. in Computer Science, independently annotated five random tables. The Cohen\u2019s Kappa scores show a substantial level of agreement (McHugh, 2012) between author B and the annotations (100% for CTC, 85.5% for ASM, and 60.6% for EL). These results demonstrate the quality and reliability of the annotations in S2abEL. A more detailed analysis on why EL agreement is relatively low can be found at Appendix D."
        },
        {
            "heading": "4 Method",
            "text": "In this section, we describe our approach for representing table cells, papers, and KB entities, as well as our model design for performing each of the sub-tasks defined in Section 3.1.\nCell Representation \u2014 For each table cell in a document, we collect information from both document text and the surrounding table. Top-ranked sentences were retrieved using BM25 (Robertson and Zaragoza, 2009) as context sentences, which often include explanations and descriptions of the table cell. The surrounding table captures the row and column context of the cell, which can offer valuable hints, such as the fact that mentions in the\nsame row and column usually refer to the same type of entities. More details about cell representation features are in Table 9. Paper Representation \u2014 For each referenced paper, we extract its index in the reference section, the last name of the first author, year, title, and abstract. Index, author name, and year are helpful for identifying inline citations (which frequently take the form of the index in brackets or the author and year in parens). Additionally, the title and abstract provide a summary of a paper which may contain information on new concepts it proposes. KB Entity Representation \u2014 To represent each entity in the target KB, we use its abbreviation, full name, and description from the KB, if available. The abbreviation and full name of an entity are crucial for capturing exact mentions in the text, while the description provides additional context for the entity (Logeswaran et al., 2019). Cell Type Classification \u2014 We concatenate features of cell representation (separated by special tokens) and input the resulting sequence to the pretrained language model SciBERT (Beltagy et al., 2019). For each token in the input sequence, we add its word embedding vector with an additional trainable embedding vector from a separate embedding layer to differentiate whether a token is in the cell, from context sentences, etc. (Subsequent mentions of SciBERT in this paper refer to this modified version). We pass the average of the output token embeddings at the last layer to a linear output layer and optimize for Cross Entropy loss. However, because the majority of cells in scientific tables pertain to experimental statistics, the distribution of cell types is highly imbalanced (as shown in Appendix A). To address this issue, we oversample the minority class data by randomly shuffling the context text extracted from the paper for those cells at a sentence level.\nAttributed Source Matching \u2014 To enable contextualization between cell context and a potential source, we combine the representations of each table cell and potential attributed source in the document as the input to a SciBERT followed by a linear output layer. We optimize for the Binary Cross Entropy loss, where all non-attributed sources in the document are used as negative examples for a cell. The model output measures the likelihood that a source should be attributed to given a table cell.\nCandidate Entity Retrieval \u2014 We design a method that combines candidates retrieved by two\nstrategies: (i) dense retrieval (DR) (Karpukhin et al., 2020) that leverages embeddings to represent latent semantics of table cells and entities, and (ii) attributed source retrieval (ASR) which uses the attributed source information to retrieve candidate entities.\nFor DR, we fine-tune a bi-encoder architecture (Reimers and Gurevych, 2019) with two separate SciBERTs to optimize a triplet objective function. The model is only trained on cells whose gold referent entity exists in the KB. Top-ranked most similar entities based on the BM25F algorithm (Robertson and Zaragoza, 2009)6 in Elasticsearch are used as negative examples. For each table cell ti, the top-k nearest entities Oidr in the embedding space with ranks are returned as candidates.\nFor ASR, we use the trained ASM model to obtain a list of papers ranked by their probabilities of being the attributed source estimated by the model. The candidate entity sequence Oiasr is constructed by fetching entities associated with each potentially attributed paper in ranked order using the PaperRelatesTo-Entity relations in PwC. Only entities of the same cell type as identified in CTC are retained. Note that including entities associated with lowerranked papers mitigates the errors propagated from the ASM model and the problem of imperfect entity and relation coverage that is common in real-world KBs.\nWe finally interleave Oidr and Oiasr until we reach a pre-defined entity set size K.\nEntity Disambiguation with outKB Identification \u2014 Given a table cell and its entity candidates, we fine-tune a cross-encoder architecture (Reimers and Gurevych, 2019) with a SciBERT that takes as input the fused cell representation and entity representation, followed by a linear output layer. We optimize for BCE loss using the same negative examples used in CER training. The trained model is used to estimate the probability that a table cell matches an entity. If the top-ranked entity for a cell has a matching likelihood lower than 0.5, then the cell is considered to be outKB."
        },
        {
            "heading": "5 Evaluations",
            "text": "As no existing baselines exist for the end-to-end table EL task with outKB mention identification, we compare our methods against appropriate recent\n6We chose to use BM25F instead of BM25 because it can take into account entity data with several fields, including name, full name, and description.\nwork by evaluating their performance on sub-tasks of our dataset (Section 5.1). Additionally, we report the performance of the end-to-end system to provide baseline results for future work (Section 5.2). Finally, to understand the connection and impact of each sub-task on the final EL performance, we conducted a component-wise ablation study (Section 5.3). This study provides valuable insights into the difficulties and bottlenecks in model performance.\nThe experiments are designed to evaluate the performance of methods in a cross-domain setting (following the setup in Kardas et al., 2020), where training, validation, and test data come from different disjoint topics. This ensures that the methods are not overfitting to the particular characteristics of a topic and can generalize well to unseen data from different topics."
        },
        {
            "heading": "5.1 Evaluating Sub-tasks",
            "text": ""
        },
        {
            "heading": "5.1.1 Cell Type Classification",
            "text": "We compare our method against AxCell\u2019s cell type classification component (Kardas et al., 2020), which uses a ULMFiT architecture (Howard and Ruder, 2018) with LSTM layers pre-trained on arXiv papers. It takes as input the contents of table cells with a set of hand-crafted features to provide the context of cells in the paper. We use their publicly available implementation7 with a slight modification to the output layer to suit our 5-class classification.\nTable 2 shows that our method outperforms AxCell somewhat in terms of F1 scores. Although we do not claim our method on this particular sub-task is substantially better, we provide baseline results using state-of-the-art transformer models."
        },
        {
            "heading": "5.1.2 Candidate Entity Retrieval",
            "text": "Since the goal of CER is to generate a small list of potential entities for a table cell, we evaluate the performance of the CER method using recall@K.\nFigure 2 shows the results of evaluating dense retrieval (DR), attributed source retrieval (ASR), and a combination of both methods, with different candidate size limits K. We observe that seeding the candidate set with entities associated with attributed papers significantly outperforms DR, while interleaving candidates from ASR and DR produces the most promising results. These results\n7https://github.com/paperswithcode/ axcell\ndemonstrate the effectiveness of utilizing information on attributed sources to generate high-quality candidates. It is worth noting that when K is sufficiently large, ASR considers all sources as attributed sources for a given cell, thus returning entities that are associated with any source. However, if the gold entity is not related to any cited source in the paper, it will still be missing from the candidate set. Increasing K further will not recover this missing entity, as indicated by the saturation observed in Figure 2. Error Analysis \u2014 We examined the outputs of ASR and identified two main challenges. First, we observed that in 22.8% of the error cases when K = 100, authors did not cite papers for referred concepts. These cases typically involve well-known entities such as LSTM (Hochreiter and Schmidhuber, 1997). In the remaining error cases, the authors did cite papers; however, the gold entity was not retrieved due to incomplete Paper-RelatesTo-Entity relations in the target KB\nor because the authors cited the wrong paper. We additionally investigated the error cases from DR and found that a considerable fraction was caused by the use of generic words to refer to a specific entity. For instance, the validation set of a specific dataset entity was referred to as \"val\" in the table, the method proposed in the paper was referred to as \"ours\", and a subset of a dataset that represents data belonging to one of the classification categories was referred to as \"window\". Resolving the ambiguity of such references requires the model to have an understanding of the unique meaning of those words in the context.\nWhen using the combined candidate sets, missing gold entities were only observed when both DR and ASR failed, leading to superior performance compared to using either method alone."
        },
        {
            "heading": "5.1.3 Entity Disambiguation with inKB Mentions",
            "text": "The state-of-the-art method closest to our table EL task is TURL (Deng et al., 2020), designed for general-domain tables with inKB cells. It is a structure-aware Transformer encoder pre-trained on the general-purpose WikiTables corpus (Bhagavatula et al., 2015), which produces contextualized embeddings for table cells, rows, and columns that are suitable for a range of downstream applications, including table EL. We used TURL\u2019s public code8 and fine-tuned it on the inKB cells of our dataset and compared it with our method using the same entity candidate set of size 50.\nTable 3 shows that our model achieves a substantial improvement in accuracy over TURL on nine out of ten paper folds. The examples in Table 6 (appendix) demonstrate that our model is more effective at recognizing the referent entity when the cell mention is ambiguous and looks similar to other en-\n8https://github.com/sunlab-osu/TURL\ntities in the KB. This is because TURL as a generic table embedding method focuses on just cell content and position while our approach combines cell content with the full document. Our analysis further reveals that TURL made incorrect predictions for all cells whose mentions were shorter than four characters (likely an abbreviation or a pointer to a reference paper). Meanwhile, our method correctly linked 39% of these cells."
        },
        {
            "heading": "5.2 End-to-end Evaluation",
            "text": "We now evaluate the end-to-end performance of our approach on the EL task with outKB identification. In addition to re-ranking candidate en-\ntities, the method needs to determine when cell mentions refer to entities that do not exist in the target KB. We report F1 scores for outKB entities as the prediction is binary (precision and recall are reported in Appendix Table 8). For inKB mentions, we report the hit rate at top-1. Additionally, we evaluate overall performance using accuracy.9 For each topic of papers, we report the ratio of outKB mentions to inKB mentions. The top block of Table 4 shows the end-to-end EL performance of our method. Our analysis shows a positive Pearson correlation (Cohen et al., 2009) of 0.87 between O/I ratio and overall accuracy, indicating our method tends to higher accuracy on datasets with more outKB mentions. Figure 3 shows the performance at various inKB thresholds.\nError Analysis We sampled 100 examples of incorrect predictions for both outKB and inKB mentions and analyzed their causes of errors in Table 7 (Appendix E). Our analysis reveals that a majority of incorrect inKB predictions are due to the use of generic words. For outKB mentions, the model tends to get confused when they are similar to existing entities in the target KB."
        },
        {
            "heading": "5.3 Component-wise Ablation Study",
            "text": "To investigate how much of the error in our end-toend three-step system was due to errors introduced in the first two stages (specifically, wrong cell type\n9A cell is considered a correct prediction if it is an outKB mention and predicted as such, or if it is an inKB mention and predicted as inKB with the gold entity being ranked at top 1.\nclassifications from CTC or missing correct candidates from CER), we tried measuring system performance with these errors removed. Specifically, we tried replacing the CTC output with the gold cell labels, or adding the gold entity to the output CER candidate set, or both.\nThe results in the bottom block of Table 4 show that there is no significant difference in performance with gold inputs. This could be because CTC and CER are easier tasks compared to ED, and if the model fails those tasks, it is likely to still struggle to identify the correct referent entity, even if that is present in the candidate set or the correct cell type is given."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we present S2abEL, a high-quality human-annotated dataset for Entity Linking in machine learning results tables, which is, to the best of our knowledge, the first dataset for table EL in the scientific domain. We propose a model that serves as a strong baseline for the end-to-end table EL task with identification of outKB mentions, as well as for each of the sub-tasks in the dataset. We show that extracting context from paper text gives a significant improvement compared to methods that use only tables. Identifying attributed source papers for a concept achieves higher recall@k compared with dense retrieval for candidate entity generation. However, the best baselines still fall far below human performance, showing potential for future improvement.\nLimitations\nIn this section, we discuss some limitations of our work. First, our dataset only includes tables from English-language papers in the machine learning domain, linked to the Papers with Code KB, which limits its generalizability to other domains, languages, and KBs. Second, we acknowledge that the creation of S2abEL required significant manual effort from domain experts, making it a resourceintensive process that may not be easily scalable. Third, our approach of using attributed papers to aid in identifying referent entities relies on the target KB containing relations that associate relevant papers and entities together. Fourth, we do not compare against large GPT-series models and leave this as future work. Finally, while our experiments set one initial baseline for model performance on our task, substantially more exploration of differ-\nent methods may improve performance on our task substantially."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported in part by NSF Grant 2033558."
        },
        {
            "heading": "A Detailed Dataset Statistics",
            "text": "S2abEL consists of 11 folds, each corresponding to a topic. Table 5 provides detailed statistics on the number of papers, tables, and cells for each sub-task and topic. The class distribution for CTC is as follows: other (74%), dataset (8%), method (14%), metric (3%), and dataset&metric (0.4%). For ASM, 1,532 (16.6%) cells have missing attributed paper, 1,095 (11.9%) cells attribute to the paper itself, 6,598 (71.5%) cells attribute to an entry in the reference section of the paper. For EL, 3,610 (42.8%) cells refer to outKB entities and (57.2%) cells refer to inKB entities."
        },
        {
            "heading": "B Training Details",
            "text": "We trained all our models for two epochs with a batch size of 32, using the AdamW optimizer (Loshchilov and Hutter, 2019) with linear decay warm-up. The initial learning rate was 2e-5 and the warm-up ratio was 10%. All models were trained using a single 48Gb NVIDIA A6000 GPU. For the triplet loss function in DR, we used Euclidean as the distance function with a margin of 1. For the Candidate Entity Retrieval and Entity Disambiguation tasks, we used negative examples of size 50 at training time. Additionally for the ED task, we set candidate set size limitation as 50 when making predictions."
        },
        {
            "heading": "C Annotation Interface and Guidelines",
            "text": "Our annotation interface with annotation guidelines is at https://github.com/allenai/ s2abel/blob/main/common_utils/ Annotation%20Interface.pdf. Note that there might be cells that contain a subentity mentions consisting of an entity mention and a non-entity mention string, e.g., \"Bert-large\", \"Bert\nwith 6 layers frozen\". For these cells, we asked the annotators to focus on the primary entity and our current model considers these mentions as mentions of the main entity. Thus those two mentions are labeled as method, and linked to https: paperswithcode.com/method/bert. We also specifically asked the annotators to mark cells that contain mentions of more than one primary entity or are confusing to understand, which are excluded from the dataset. We leave the tasks of linking subentities explicitly and cells to multiple entities for future work."
        },
        {
            "heading": "D Annotation Error Analysis",
            "text": "Our investigation showed that the main cause of disagreement in the EL phase was that there were cells whose matching entities were confusing to the annotators, due to their insufficient background in the specific academic area and/or the paper not clearly indicating which entity it was and/or whether should be considered a variant of an existing entity or a different entity entirely."
        },
        {
            "heading": "E Error Case Study",
            "text": "Table 6 presents examples where TURL made incorrect EL predictions while our approach made correct predictions. Table 7 summarizes the main causes of incorrect predictions made by our approach for both inKB and outKB mentions."
        }
    ],
    "title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
    "year": 2023
}