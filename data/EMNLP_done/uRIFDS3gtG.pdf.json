{
    "abstractText": "End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent\u2019s insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent\u2019s hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent\u2019s low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Cheng-Fu Yang"
        },
        {
            "affiliations": [],
            "name": "Yen-Chun Chen"
        },
        {
            "affiliations": [],
            "name": "Jianwei Yang"
        },
        {
            "affiliations": [],
            "name": "Xiyang Dai"
        },
        {
            "affiliations": [],
            "name": "Lu Yuan"
        },
        {
            "affiliations": [],
            "name": "Yu-Chiang Frank Wang"
        },
        {
            "affiliations": [],
            "name": "Kai-Wei Chang"
        },
        {
            "affiliations": [],
            "name": "MoveAhead MoveAhead"
        }
    ],
    "id": "SP:c58c92024006d5ba4df84cc8f7168c2c67d42212",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "year": 2022
        },
        {
            "authors": [
                "Anurag Ajay",
                "Aviral Kumar",
                "Pulkit Agrawal",
                "Sergey Levine",
                "Ofir Nachum."
            ],
            "title": "Opal: Offline primitive discovery for accelerating offline reinforcement learning",
            "venue": "ICLR. 6",
            "year": 2021
        },
        {
            "authors": [
                "Peter Anderson",
                "Angel Chang",
                "Devendra Singh Chaplot",
                "Alexey Dosovitskiy",
                "Saurabh Gupta",
                "Vladlen Koltun",
                "Jana Kosecka",
                "Jitendra Malik",
                "Roozbeh Mottaghi",
                "Manolis Savva"
            ],
            "title": "On evaluation of embodied navigation agents",
            "year": 2018
        },
        {
            "authors": [
                "Peter Anderson",
                "Qi Wu",
                "Damien Teney",
                "Jake Bruce",
                "Mark Johnson",
                "Niko S\u00fcnderhauf",
                "Ian Reid",
                "Stephen Gould",
                "Anton Van Den Hengel"
            ],
            "title": "2018b. Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environ",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein",
                "Sergey Levine."
            ],
            "title": "Learning with latent language",
            "venue": "NAACL-HLT. 6",
            "year": 2018
        },
        {
            "authors": [
                "Valts Blukis",
                "Chris Paxton",
                "Dieter Fox",
                "Animesh Garg",
                "Yoav Artzi."
            ],
            "title": "A persistent spatial semantic representation for high-level natural language instruction execution",
            "venue": "CoRL. 5",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Brohan",
                "Noah Brown",
                "Justice Carbajal",
                "Yevgen Chebotar",
                "Joseph Dabis",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog",
                "Jasmine Hsu"
            ],
            "title": "Rt-1: Robotics transformer for real-world control at scale",
            "year": 2022
        },
        {
            "authors": [
                "Shizhe Chen",
                "Pierre-Louis Guhur",
                "Cordelia Schmid",
                "Ivan Laptev."
            ],
            "title": "History aware multimodal transformer for vision-and-language navigation",
            "venue": "NeurIPS. 5",
            "year": 2021
        },
        {
            "authors": [
                "Abhishek Das",
                "Samyak Datta",
                "Georgia Gkioxari",
                "Stefan Lee",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Embodied question answering",
            "venue": "CVPR. 5",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT. 6, 12",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Soumith Udatha",
                "Russ R Salakhutdinov",
                "Sergey Levine."
            ],
            "title": "Imitating past successes can be very suboptimal",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Fried",
                "Ronghang Hu",
                "Volkan Cirik",
                "Anna Rohrbach",
                "Jacob Andreas",
                "Louis-Philippe Morency",
                "Taylor Berg-Kirkpatrick",
                "Kate Saenko",
                "Dan Klein",
                "Trevor Darrell"
            ],
            "title": "Speaker-follower models for vision-and-language navigation",
            "venue": "In NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Xiaofeng Gao",
                "Qiaozi Gao",
                "Ran Gong",
                "Kaixiang Lin",
                "Govind Thattai",
                "Gaurav S Sukhatme."
            ],
            "title": "Dialfred: Dialogue-enabled agents for embodied instruction following",
            "venue": "IEEE Robotics and Automation Letters, 7(4):10049\u201310056. 5",
            "year": 2022
        },
        {
            "authors": [
                "Ross Girshick."
            ],
            "title": "Fast r-cnn",
            "venue": "ICCV. 6, 12",
            "year": 2015
        },
        {
            "authors": [
                "Peter D Gr\u00fcnwald."
            ],
            "title": "The minimum description length principle",
            "venue": "MIT press. 2, 4",
            "year": 2007
        },
        {
            "authors": [
                "Yuki Inoue",
                "Hiroki Ohashi."
            ],
            "title": "Prompter: Utilizing large language model prompting for a data efficient embodied instruction following",
            "venue": "arXiv preprint arXiv:2211.03267. 5",
            "year": 2022
        },
        {
            "authors": [
                "Vihan Jain",
                "Gabriel Magalhaes",
                "Alexander Ku",
                "Ashish Vaswani",
                "Eugene Ie",
                "Jason Baldridge."
            ],
            "title": "Stay on the path: Instruction fidelity in vision-andlanguage navigation",
            "venue": "ACL. 5, 8",
            "year": 2019
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole."
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "ICLR. 5",
            "year": 2017
        },
        {
            "authors": [
                "Yiding Jiang",
                "Evan Liu",
                "Benjamin Eysenbach",
                "J Zico Kolter",
                "Chelsea Finn."
            ],
            "title": "Learning options via compression",
            "venue": "NeurIPS. 2, 5",
            "year": 2022
        },
        {
            "authors": [
                "Liyiming Ke",
                "Xiujun Li",
                "Yonatan Bisk",
                "Ari Holtzman",
                "Zhe Gan",
                "Jingjing Liu",
                "Jianfeng Gao",
                "Yejin Choi",
                "Siddhartha Srinivasa."
            ],
            "title": "Tactical rewind: Selfcorrection via backtracking in vision-and-language navigation",
            "venue": "CVPR. 5, 9",
            "year": 2019
        },
        {
            "authors": [
                "Taesup Kim",
                "Sungjin Ahn",
                "Yoshua Bengio."
            ],
            "title": "Variational temporal abstraction",
            "venue": "NeurIPS. 5",
            "year": 2019
        },
        {
            "authors": [
                "Eric Kolve",
                "Roozbeh Mottaghi",
                "Winson Han",
                "Eli VanderBilt",
                "Luca Weihs",
                "Alvaro Herrasti",
                "Matt Deitke",
                "Kiana Ehsani",
                "Daniel Gordon",
                "Yuke Zhu"
            ],
            "title": "Ai2-thor: An interactive 3d environment for visual ai",
            "venue": "arXiv preprint arXiv:1712.05474",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Krantz",
                "Erik Wijmans",
                "Arjun Majumdar",
                "Dhruv Batra",
                "Stefan Lee."
            ],
            "title": "Beyond the nav-graph: Vision-and-language navigation in continuous environments",
            "venue": "ECCV. 5",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Ku",
                "Peter Anderson",
                "Roma Patel",
                "Eugene Ie",
                "Jason Baldridge."
            ],
            "title": "Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding",
            "venue": "EMNLP. 5",
            "year": 2020
        },
        {
            "authors": [
                "Chengshu Li",
                "Ruohan Zhang",
                "Josiah Wong",
                "Cem Gokmen",
                "Sanjana Srivastava",
                "Roberto Mart\u00edn-Mart\u00edn",
                "Chen Wang",
                "Gabrael Levine",
                "Michael Lingelbach",
                "Jiankai Sun"
            ],
            "title": "Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities",
            "year": 2023
        },
        {
            "authors": [
                "Xiujun Li",
                "Chunyuan Li",
                "Qiaolin Xia",
                "Yonatan Bisk",
                "Asli Celikyilmaz",
                "Jianfeng Gao",
                "Noah Smith",
                "Yejin Choi."
            ],
            "title": "Robust navigation with language pretraining and stochastic sampling",
            "venue": "EMNLP. 5",
            "year": 2019
        },
        {
            "authors": [
                "Xiwen Liang",
                "Fengda Zhu",
                "Yi Zhu",
                "Bingqian Lin",
                "Bing Wang",
                "Xiaodan Liang."
            ],
            "title": "Contrastive instruction-trajectory learning for vision-language navigation",
            "venue": "AAAI. 5",
            "year": 2022
        },
        {
            "authors": [
                "So Yeon Min",
                "Devendra Singh Chaplot",
                "Pradeep Ravikumar",
                "Yonatan Bisk",
                "Ruslan Salakhutdinov."
            ],
            "title": "Film: Following instructions in language with modular methods",
            "venue": "ICLR. 5, 9",
            "year": 2022
        },
        {
            "authors": [
                "Dipendra Misra",
                "Andrew Bennett",
                "Valts Blukis",
                "Eyvind Niklasson",
                "Max Shatkhin",
                "Yoav Artzi."
            ],
            "title": "Mapping instructions to actions in 3d environments with visual goal prediction",
            "venue": "EMNLP. 5",
            "year": 2018
        },
        {
            "authors": [
                "Van-Quang Nguyen",
                "Masanori Suganuma",
                "Takayuki Okatani."
            ],
            "title": "Look wide and interpret twice: Improving performance on interactive instructionfollowing tasks",
            "venue": "IJCAI. 5",
            "year": 2021
        },
        {
            "authors": [
                "Aishwarya Padmakumar",
                "Jesse Thomason",
                "Ayush Shrivastava",
                "Patrick Lange",
                "Anjali Narayan-Chen",
                "Spandana Gella",
                "Robinson Piramuthu",
                "Gokhan Tur",
                "Dilek Hakkani-Tur."
            ],
            "title": "Teach: Task-driven embodied agents that chat",
            "venue": "AAAI. 5",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Pashevich",
                "Cordelia Schmid",
                "Chen Sun."
            ],
            "title": "Episodic transformer for vision-and-language navigation",
            "venue": "ICCV. 1, 3, 5, 6, 12, 14",
            "year": 2021
        },
        {
            "authors": [
                "Tomislav Pejsa",
                "Julian Kantor",
                "Hrvoje Benko",
                "Eyal Ofek",
                "Andrew Wilson."
            ],
            "title": "Room2room: Enabling life-size telepresence in a projected augmented reality environment",
            "venue": "Proceedings of the 19th ACM conference on computer-supported cooperative work",
            "year": 2016
        },
        {
            "authors": [
                "Mihir Prabhudesai",
                "Hsiao-Yu Fish Tung",
                "Syed Ashar Javed",
                "Maximilian Sieb",
                "Adam W Harley",
                "Katerina Fragkiadaki."
            ],
            "title": "Embodied language grounding with 3d visual feature representations",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "In ICLR",
            "year": 2016
        },
        {
            "authors": [
                "Manolis Savva",
                "Abhishek Kadian",
                "Oleksandr Maksymets",
                "Yili Zhao",
                "Erik Wijmans",
                "Bhavana Jain",
                "Julian Straub",
                "Jia Liu",
                "Vladlen Koltun",
                "Jitendra Malik"
            ],
            "title": "Habitat: A platform for embodied ai research",
            "venue": "In ICCV",
            "year": 2019
        },
        {
            "authors": [
                "Raphael Schumann",
                "Stefan Riezler."
            ],
            "title": "Analyzing generalization of vision and language navigation to unseen outdoor areas",
            "venue": "ACL. 5",
            "year": 2022
        },
        {
            "authors": [
                "Pratyusha Sharma",
                "Antonio Torralba",
                "Jacob Andreas."
            ],
            "title": "Skill induction and planning with latent language",
            "venue": "ACL. 2, 6",
            "year": 2021
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Jesse Thomason",
                "Daniel Gordon",
                "Yonatan Bisk",
                "Winson Han",
                "Roozbeh Mottaghi",
                "Luke Zettlemoyer",
                "Dieter Fox."
            ],
            "title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
            "venue": "CVPR. 2, 5, 6, 14",
            "year": 2020
        },
        {
            "authors": [
                "Kunal Pratap Singh",
                "Suvaansh Bhambri",
                "Byeonghwi Kim",
                "Roozbeh Mottaghi",
                "Jonghyun Choi."
            ],
            "title": "Factorizing perception and policy for interactive instruction following",
            "venue": "ICCV. 6, 14",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro Suglia",
                "Qiaozi Gao",
                "Jesse Thomason",
                "Govind Thattai",
                "Gaurav Sukhatme."
            ],
            "title": "Embodied bert: A transformer model for embodied, language-guided visual task completion",
            "venue": "arXiv preprint arXiv:2108.04927. 1, 3, 5, 6, 14",
            "year": 2021
        },
        {
            "authors": [
                "Hao Tan",
                "Licheng Yu",
                "Mohit Bansal."
            ],
            "title": "Learning to navigate unseen environments: Back translation with environmental dropout",
            "venue": "NAACL-HLT.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Tanneberg",
                "Kai Ploeger",
                "Elmar Rueckert",
                "Jan Peters"
            ],
            "title": "Skid raw: Skill discovery from raw trajectories",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS. 1",
            "year": 2017
        },
        {
            "authors": [
                "Yichi Zhang",
                "Joyce Chai."
            ],
            "title": "Hierarchical task learning from language instructions with unified transformers and self-monitoring",
            "venue": "Findings of ACL. 1, 3, 5, 9",
            "year": 2021
        },
        {
            "authors": [
                "Wang Zhu",
                "Hexiang Hu",
                "Jiacheng Chen",
                "Zhiwei Deng",
                "Vihan Jain",
                "Eugene Ie",
                "Fei Sha."
            ],
            "title": "Babywalk: Going farther in vision-and-language navigation by taking baby steps",
            "venue": "ACL. 5",
            "year": 2020
        },
        {
            "authors": [
                "Yi Zhu",
                "Yue Weng",
                "Fengda Zhu",
                "Xiaodan Liang",
                "Qixiang Ye",
                "Yutong Lu",
                "Jianbin Jiao."
            ],
            "title": "Selfmotivated communication agent for real-world visiondialog navigation",
            "venue": "ICCV. 5",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Embodied Instruction Following (EIF) necessitates an embodied AI agent to interpret and follow natural language instructions, executing multiple subtasks to achieve a final goal. Agents are instructed to sequentially navigate to locations while localizing and interacting with objects in a fine-grained\n*work partially done as a research intern at Microsoft 1The code is available at: github.com/joeyy5588/LACMA."
        },
        {
            "heading": "Camera Observation:",
            "text": "manner. In a typical EIF simulator, the agent\u2019s sole perception of the environment is through its egocentric view from a visual camera. To complete all the sub-tasks in this challenging setting, variants of Transformer (Vaswani et al., 2017) have been employed to collectively encode the long sequence of multi-modal inputs, which include language instructions, camera observations, and past actions. Subsequently, the models are trained end-to-end to imitate the ground-truth action sequences, i.e., expert trajectories, from the dataset.2\nSignificant progress has been made in this field (Pashevich et al., 2021; Suglia et al., 2021; Zhang and Chai, 2021). Nevertheless, our obser-\n2We use action sequences and trajectories interchangeably.\nvations suggest that existing approaches might not learn to follow instructions effectively. Specifically, our analysis shows that existing models can achieve a high success rate even without providing any language instructions when the environment in the test time is the same as in training. However, performance drops significantly when they are deployed into an unseen environment even when instructions are provided. This implies that the models learn to memorize visual observations for predicting action sequences rather than learning to follow the instructions. We hypothesize that this overfitting of the visual observations is the root cause of the significant performance drop in unseen environments. Motivated by this observation, we raise a research question: Can we build an EIF agent that reliably follows instructions step-by-step?\nTo address the above question, we aim to improve the alignment between the language instruction and the internal state representation of an EIF agent. Sharma et al. (2021) suggest leveraging language as intermediate representations of trajectories. Jiang et al. (2022) demonstrate that identifying patterns within trajectories aids models in adapting to unseen environments. Inspired by their observations, we conjecture two critical directions: 1) language may be used as a pivot, and 2) common language patterns across trajectories could be leveraged. We propose Language-Aligning Contrastive Learning with Meta-Actions (LACMA), a method aimed at enhancing Embodied Instruction Following. Specifically, we explicitly align the agent\u2019s hidden states, which are employed in predicting the next action, with the corresponding sub-task instruction via contrastive learning. Through the proposed contrastive training, hidden states are more effectively aligned with the language instruction.\nNevertheless, a significant semantic gap persists between high-level language instructions, e.g., \u201ctake a step left and then walk to the fireplace\u201d and the agent\u2019s low-level action space, e.g., MoveForward, RotateRight, etc. To further narrow this gap, we introduce the concept of metaactions (MA), a set of action patterns each representing higher-level semantics, and can be sequentially composed to execute any sub-task. For clarity, we will henceforth refer to the original agent\u2019s actions as low-level actions (LA) for the remainder of the paper. The elevated semantics of meta-actions may serve as a more robust learning signal, preventing the model from resorting to shortcuts based on\nits visual observations. This concept draws inspiration from recent studies that improve EIF agents with human-defined reusable skills (Brohan et al., 2022; Ahn et al., 2022). An illustrative example is shown in Figure 1.\nMore specifically, our agent first predicts metaactions, and then predicts low-level actions conditioning on the MAs. However, an LA sequence may be parsed into multiple valid MA sequences. To determine the optimal MA sequences, we parse the trajectories following the minimum description length principle (MDL; Gr\u00fcnwald, 2007). The MDL states that the shortest description of the data yields the best model. In our case, the optimal parse of a trajectory corresponds to the shortest MA sequence in length. Instead of an exhaustive search, optimal MAs can be generated via dynamic programming.\nTo evaluate the effectiveness of LACMA, we conduct experiments on the ALFRED dataset (Shridhar et al., 2020). Even with a modest set of metaactions consisting of merely 10 classes, our agent significantly outperforms in navigating unseen environments, improving the task success rate by 4.7% and 4.5% on the unseen validation and testing environments, respectively, while remaining competitive in the seen environments. Additional analysis reveals the complementary nature of the contrastive objective and meta-actions: Learning from metaactions effectively reduces the semantic gap between low-leval actions and language instructions, while the contrastive objective enforces alignment to the instructions, preventing the memorization of meta-action sequences in seen environments.\nOur contributions can be summarized as follows:\n\u2022 We propose a contrastive objective to align the agent\u2019s state representations with the task\u2019s natural language instructions.\n\u2022 We introduce the concept of meta-actions to bridge the semantic gap between natural language instructions and low-level actions. We also present a dynamic programming algorithm to efficiently parse trajectories into metaactions.\n\u2022 By integrating the proposed language-aligned meta-actions and state representations, we enhance the EIF agents\u2019 ability to faithfully follow instructions."
        },
        {
            "heading": "2 Method",
            "text": "In this section, we first define settings and notations of the embodied instruction following (EIF) tasks in Section 2.1. Then, in Section 2.2, we introduce the language-induced contrastive objective used to extract commonalities from instructions. Finally, in Section 2.3, we will explain how we generate the labels for meta-actions and how they are leveraged to bridge the gap between instructions and the corresponding action sequences."
        },
        {
            "heading": "2.1 Settings and Notations",
            "text": "Given a natural language task goal G which consists of N sub-goals, each corresponding to a subgoal instruction S = s1:N . The agent is trained to predict a sequence of executable low-level actions a1:T to accomplish the task. During training time, the ground-truth trajectories of the task are represented by the tuple (w1:L, v1:T , a1:T ), where T denotes the length of the trajectories. w1:L represents the concatenation of the task description G and all the subgoal instructions S1:N , with each instruction appended by a special token [SEP]. L stands for the total number of tokens in the concatenated sentence. v1:T denotes the camera observations of the agent over T steps, with each camera frame vt being an RGB image with a spatial size of W \u00d7H , denoted as vt \u2208 RW\u00d7H\u00d73. The action sequences a1:T denote the ground-truth actions. At each timestep t, the navigation agent, parameterized by \u03b8, is trained to optimize the output distribution P\u03b8(at|w1:L, v1:t, at\u22121). An overview of the framework is illustrated in Figure 2."
        },
        {
            "heading": "2.2 Contrastive State-Instruction Alignment",
            "text": "In Section 1, we put forth the hypothesis that tasks with similar objectives or navigation goals exhibit shared language patterns. Extracting such commonalities can effectively facilitate the alignment between language instructions and action sequences, further enhancing the generalizability of acquired skills. This alignment is further reinforced through the utilization of a contrastive objective during training. In this subsection, we first describe the process of obtaining the model\u2019s state representation, which encapsulates the relevant information necessary for the agent to make decisions and take actions. We then explain how we associate such representations with linguistic features to construct both positive and negative pairs for contrastive learning.\nState and Instruction Representations Following prior studies (Pashevich et al., 2021; Suglia et al., 2021; Zhang and Chai, 2021), we use a Transformer encoder to process all input information, which includes the input instructions, camera observations and previously executed actions (w1:L, v1:t, at\u22121). As shown in Figure 3, our model generates the state representation zvt at each timestep t, which captures the current state of the agent and the environment. To extract representations for each sub-goal, we take the output features of the [SEP] tokens appended after each instruction, resulting in N features zw1:N . Please refer to the appendix for more details."
        },
        {
            "heading": "Constructing Positive and Negative Pairs As",
            "text": "illustrated in Fig. 3, our contrastive loss function compares a specific positive pair, consisting of a state representation zvt and the feature of its corresponding subgoal instruction zwpos(t), with a collection of negative pairs. pos(t) is the index of the instruction features corresponding to state t. This mapping ensures that each frame at timestamp is correctly aligned with its corresponding language instructions. The negative pairs include other subgoal instructions from the same task (intra-task negatives) as well as instructions from different tasks (inter-task negatives). We denote these instructions as zw1:N\\pos(t). The contrastive objective takes the following form:\nLCL = \u2212 T\u2211 t=1 log exp(\u27e8zvt ,zwpos(t)\u27e9/\u03c4)\u2211N n=1 exp(\u27e8zvt ,zwn \u27e9/\u03c4) , (1)\nwhere \u27e8\u00b7, \u00b7\u27e9 denotes the inner product and \u03c4 is the temperature parameter. By contrasting the positive pair with these negative pairs, our contrastive loss LCL encourages the model to better distinguish and align state representations with the language instructions, which allows our model to transfer the learned knowledge from seen environments to unseen environments more effectively."
        },
        {
            "heading": "2.3 Learning with Meta-Actions (MA)",
            "text": "To bridge the semantic gap between natural language instructions and navigation skills, we propose the concept of meta-action (MA), representing higher-level combinations of low-level actions (LAs), as depicted in Fig. 1. In this subsection, we first introduce how we determine the optimal meta-action sequence given a low-level action trajectory and a set of pre-defined meta-actions. Next, we detail our training paradigm, which involves both generated MAs and the ground-truth LAs.\nOptimal Meta-Actions We draw an analogy between the minimum description length principle (MDL; Gr\u00fcnwald, 2007) and finding the optimal MA sequences for a given LA trajectory. Both approaches share a common goal: finding the most concise representation of the data. The MDL principle suggests that the best model is the one with the shortest description of the data. Similarly, we aim to find MA sequences that are compact yet lossless representations of LA trajectories. Therefore, we define the optimal meta-action sequence as the one that uses the minimal number of MAs to represent the given low-level action trajectory.\nMA Identification via Dynamic Programming We formulate the process of finding the minimal number of MAs to represent a given LA trajectory as a dynamic programming (DP) problem. The high level idea is to iteratively solve the subproblem of the optimal MA sequence up to each LA step. To achieve this, we first convert the LA sequence into a sequence of letters and then string match the regular expression form of the given MA set. Table 1 showcases some of the conversions. For instance, the LA sequence \u201cMoveAhead, MoveAhead, MoveAhead\u201d is written as \u201cm, m, m\u201d, and the MA \u201cMove Forward\u201d is represented as \u201cm{1,}\u201d (m appears one or more times consecutively). Next, we sequentially solve the subproblem for each time step, finding the optimal MA sequence to represent the LA trajectory up until the current time step. Further details of the algorithm, pseudo code, pre-defined meta-actions, regular expressions, and example low-level action sequences are provided in the appendix A.3 and A.2. By formulating the meta-action identification as a DP problem, we can efficiently extract the optimal meta-action sequence m1:M with a length of M to represent any low-level action sequence.\nTraining Strategies As shown in Fig. 4, we adopt the pretrain-finetune paradigm to train our model. Specifically, in the initial pre-training stage, we optimize the model using DP-labeled metaaction sequences together with Eqn. (1), the contrastive objective. The objective function of optimizing MA is the standard classification loss: LM = CrossEntropy( \u02c6m1:M ,m1:M ), thus the pretraining loss can be written as Lp = LCL + LM . In the fine-tuning stage, we utilize ground-truth LA sequences as supervision. Similarly, the fine-tuning loss can be written as: Lf = LCL + LA, where LA = CrossEntropy( \u02c6a1:T , a1:T ) denotes the loss function of LA prediction. The use of LCL in both stage enforces our model to align the learned navigation skills with the language instructions, preventing it from associating specific visual patterns or objects with certain actions.\nHowever, conditioning LA prediction on DPlabeled MAs might cause exposure bias (Ranzato et al., 2016). This can result in diminished performance in testing, where MAs are predicted rather than being explicitly labeled. To mitigate this traintest mismatch, we employ Gumbel-softmax (Jang et al., 2017), allowing our model to condition on predicted MAs for LA prediction during training."
        },
        {
            "heading": "3 Related Works",
            "text": "Embodied Instruction Following (EIF) Various benchmarks (Anderson et al., 2018b; Pejsa\net al., 2016; Misra et al., 2018; Ku et al., 2020; Krantz et al., 2020; Das et al., 2018; Prabhudesai et al., 2020; Padmakumar et al., 2022; Gao et al., 2022) and environments (Ramakrishnan et al., 2021; Kolve et al., 2017; Li et al., 2023; Savva et al., 2019) have been proposed to study embodied intelligent agents. Among them, vision-and-language navigation (VLN) is the most comparable task to our setting. Various models have demonstrated impressive performance on the task of VLN (Ke et al., 2019; Chen et al., 2021; Jain et al., 2019; Tan et al., 2019; Zhu et al., 2020; Li et al., 2019; Zhu et al., 2021; Schumann and Riezler, 2022). In addition, Liang et al. (2022) contrasted data within the same modality to improve robustness on variations of instructions and visual scenes. We specifically focus on the ALFRED dataset (Shridhar et al., 2020) as it not only involves longer episodes of navigation but also requires models to understand complex instructions, perform fine-grained grounding, and interact with objects.\nNeural EIF Agents In recent years, two lines of works have been developed to tackle embodied instruction following tasks: modular and end-toend methods. Modular methods (Min et al., 2022; Blukis et al., 2022; Inoue and Ohashi, 2022) employ multiple modules trained with specific subtasks and direct supervision to decompose the EIF tasks. While our work focuses on aligning the state representations with language instructions for improved generalization, modular methods do not produce state representations for task planning. Therefore, we focus specifically on end-toend methods (Shridhar et al., 2020; Suglia et al., 2021; Zhang and Chai, 2021; Pashevich et al., 2021; Nguyen et al., 2021) to address these limitations. These methods generally utilize a single neural network to directly predict low-level actions from input observations. However, these methods generally suffer from limited interpretability and generalization (Eysenbach et al., 2022). On the other hand, LACMA aligns the decision making process with language instructions, simultaneously enhancing interpretability and generalization.\nSkill Learning Learning skills from demonstrations has been an active research area in the field of machine learning and robotics. Several approaches have been proposed to acquire skills, including the use of latent variable models to partition the experience into different skills (Kim et al., 2019; Jiang\net al., 2022; Ajay et al., 2021; Tanneberg et al., 2021). Other works focus on learning skills from language supervision (Ahn et al., 2022; Pashevich et al., 2021; Andreas et al., 2018; Sharma et al., 2021; Fried et al., 2018). However, there remains a challenge in bridging the gap between the learned latent skills and natural language. To close the gap, we introduce the concept of meta-actions, which are higher-level actions that captures the semantic meaning of actions in relation to instructions."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Dataset The ALFRED dataset (Shridhar et al., 2020) comprises demonstrations where an agent completes household tasks based on goals specified in natural language. ALFRED consists of 21,023 train, 1,641 validation (820 seen / 821 unseen), and 3,062 test (1,533 seen / 1,529 unseen) episodes.\nEvaluation Metrics Following Shridhar et al. (2020), we report the task success rate (SR) and the goal condition success rate (GC). SR measures the percentage of tasks where the agent successfully accomplishes all the subgoals, while GC is the ratio of subgoals fulfilled at the end of the task. For example, the task \u201cput a hot potato slice on the counter\u201d consists of four goal-conditions: slicing the potato, heating the potato slice, placing it on the counter, and ensuring it is both heated and on the counter. A task is considered success only if all the goal-conditions are successful.\nImplementation Details Our method was built upon Episodic Transformer (E.T.; Pashevich et al., 2021). Specifically, BERT (Devlin et al., 2019) is used to extract features from the language instructions. For visual observations, we pre-train a ResNet-50 Faster R-CNN (Girshick, 2015) on\nthe ALFRED dataset and then use the ResNet backbone to extract image features. These inputs from different modalities are then fused by a multimodal Transformer encoder. More training details can be found in the appendix A.1.\nBaselines As discussed in Sec. 3, LACMA focuses on aligning state representations with language instructions. For fair comparisons, we specifically choose end-to-end methods that do not incorporate an explicit planner, including SEQ2SEQ (Shridhar et al., 2020), MOCA (Singh et al., 2021), EmBERT (Suglia et al., 2021), and Episodic Transformer (E.T.; Pashevich et al., 2021). Note that the original E.T. was trained using additional trajectories from the unseen environments, which violates our assumption. Therefore, we reproduce the model using only the data from the original training set."
        },
        {
            "heading": "4.2 Quantitative Results",
            "text": "The results on ALFRED are shown in Table 2. We can see that LACMA performed favorably against the best end-to-end models across different metrics. LACMA substantially improves the task success rates (SR) and goal condition success rates (GC), especially in the unseen environments. On the validation split, our method outperforms the baseline (E.T.) by 4.7% in SR, and on the test split by 4.5%. This verifies our design in aligning the learned skills with language instructions and using meta-actions to bridge the semantic gap between instructions and low-level actions. Moreover, in the seen environments, LACMA not only exhibits improvements compared to the baseline, but is also comparable to EmBERT. Note that EmBERT considers a 360-degree view, while our method only perceives a narrower 90-degree front view."
        },
        {
            "heading": "4.3 Ablation Studies",
            "text": "Following the same evaluation procedures in Sec. 4.2, we discuss each individual contribution of the contrastive loss LCL and the use of metaactions. We present the results in Table 3. The findings demonstrate the mutual benefit of these design choices, leading to the best performance.\nContrastive Objective Regarding the contrastive objective (LCL), we observe a slight improvement in model performance when using it alone, as shown in the second row of the table. The results confirm our motivation that aligning actions to instructions can enhance the agent\u2019s generalizability in the unseen environments. Furthermore, the results in the third row demonstrate that without LCL, model would become overly reliant on the metaactions as they are highly correlated to the action sequences. Model may learn a degenerate solution which rely solely on the meta-action for predicting actions, ignoring other relevant information.\nMeta-Actions In Table 3 we show that the use of meta-actions can further improved the performance with proper regularization from LCL. We hypothesize that this is because the proposed meta actions encapsulate higher-level semantics that exhibit an intuitive alignment with the instructions. The notable improvements observed in the unseen domain further validate our hypothesis that this aligning nature facilitates a better comprehension of language within our model. As a result, our model demonstrates more effective action prediction when it comes to generalizing across diverse environments. The observed performance degradation when using meta actions alone can be at-\ntributed to a phenomenon akin to what we elaborated upon in Section 4.4. In the absence of the contrastive objective, the model tends to overly depend on visual cues to predict meta actions. Importantly, as the action prediction process is conditioned on meta actions, any inaccuracies originating from the over-dependence on visuals can propagate through the system, resulting in an undesired reduction in performance."
        },
        {
            "heading": "4.4 Instruction-Sensitive EIF Agents",
            "text": "To confirm our hypothesis that current models lack sensitivity to changes in instructions, we performed experiments where models were given only the task goal description G while excluding all sub-goal instructions S1:N . The results are presented in Table 4. In addition, we evaluate how well our models alters its output in response to instruction perturbations and report the results in Table 5. The combing results suggest that the proposed LACMA is more sensitive to language input. This reinforces our aim of aligning instructions with actions, thereby mitigating model\u2019s over-reliance on visual input and enhancing the trained agents\u2019 generalization."
        },
        {
            "heading": "4.5 Language Aligned State Representations",
            "text": "In order to assess the alignment between the learned state representations and language instructions, we conducted a probing experiment using\na retrieval task. The purpose of this experiment was to evaluate the model\u2019s capability to retrieve the appropriate sub-goal instructions based on its state representations. To accomplish this, we followed the process outlined in Section 2.2, which extracts the state representations zvt and pairing them with the corresponding instruction representations zwpos(t). Subsequently, we trained a single fully-connected network to retrieve the paired instruction, with a training duration of 20 epochs and a batch size of 128.\nDuring the testing phase, we progressively increased the difficulty of the retrieval tasks by varying the number of instructions to retrieve from: 100, 1,000, and 5,000. The obtained results are presented in Table 6. Notably, our method achieved superior performance across both seen and unseen environments compared to the baseline approach. Specifically, at the retrieval from 5000 instructions, our model surpasses E.T.\u2019s recall at 1 by 20.9% and 18.2% on seen and unseen split, respectively. For the retrieval tasks involving 100 and 1,000 instructions, our method consistently outperforms E.T., demonstrating its effectiveness in aligning state representations with language instructions.\nIn addition, we also provide a holistic evaluation of instruction fidelity using the metrics proposed in Jain et al. (2019). Specifically, we calculate how well does the predict trajectories cover the groundtruth path, and report path coverage (PC), length scores (LS), and Coverage weighted by Length Score (CLS) in Table 7. From the provided table\none can see that our approach consistently outperforms E.T. across all categories. This suggests that LACMA excels in following instructions and demonstrates a stronger grasp of language nuances compared to E.T.\nThese results highlight the capability of our approach to accurately retrieve the associated instructions based on the learned state representations. By introducing the contrastive objective, our model demonstrates significant improvements in the retrieval task, showcasing its ability to effectively incorporate language instructions into the state representation."
        },
        {
            "heading": "4.6 Qualitative Results",
            "text": "We visualize the learned meta-actions and the retrieved instructions in Fig. 5. LACMA demonstrates high interpretability since we can use the state representation to retrieve the currently executing subgoal. It is worth noting that while our model may produce a meta-action sequence that differs from the DP-annotated one, the generated sequence remains valid and demonstrates a higher alignment with the retrieved instruction. This behavior stems from our training approach, where the model is not directly supervised with labeled meta-actions. Instead, we optimize the meta-action predictors through a joint optimization of the contrastive objective LCL and the action loss LA. Consequently, our model learns meta-actions that facilitate correct action generation while also aligning with the provided language instructions. Due to page limit, we visualize more trajectories in appendix A.6."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose LACMA, a novel approach that addresses the semantic gap between high-level language instructions and low-level action space in Embodied Instruction Following. Our key contributions include the introduction of contrastive learn-\ning to align the agent\u2019s hidden states with instructions and the incorporation of meta-actions, which capture higher-level semantics from the action sequence. Through these innovations, we achieve a significant 4.5% absolute gain in success rate on unseen environments. Our results demonstrate the effectiveness of LACMA in improving alignment between instructions and actions, paving the way for more robust embodied agents."
        },
        {
            "heading": "Limitations",
            "text": "Despite the effectiveness of meta-actions and the contrastive objective in our approach, there are several limitations to consider. One key limitation is the use of a ResNet-50 encoder to extract a single feature for each frame. By pooling the entire image into a single vector, there is a potential loss of fine-grained information. To address this limitation, incorporating object-aware or object-centric features could enhance the model\u2019s performance. By considering the specific objects present in the environment, the model may gain a more nuanced understanding of the scene and improve its ability to generate accurate and contextually relevant actions. Another limitation is that our model does not employ any error escaping technique like backtracking (Zhang and Chai, 2021; Ke et al., 2019) or replanning (Min et al., 2022). These techniques have shown promise in improving the model\u2019s ability to recover from errors and navigate challenging environments. By incorporating an error recovery mechanism, our model could potentially enhance\nits performance and robustness in situations where navigation plans fail or lead to incorrect actions."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our research work does not raise any significant ethical concerns. In terms of dataset characteristics, we provide detailed descriptions to ensure readers understand the target speaker populations for which our technology is expected to work effectively. The claims made in our paper align with the experimental results, providing a realistic understanding of the generalization capabilities. We thoroughly evaluate the dataset\u2019s quality and describe the steps taken to ensure its reliability."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers, Po-Nien Kung, TeLin Wu, Zi-Yi Dou and other members of UCLANLP+ group for their helpful comments. This work was partially supported by Amazon AWS credits, ONR grant N00014-23-1-2780, and a DARPA ANSR program FA8750-23-2-0004. The views and conclusions are those of the authors and should not reflect the official policy or position of DARPA or the U.S. Government."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Implementation Details Model Architecture We use Episodic Transformer (E.T.; Pashevich et al., 2021) as our backbone. We first extract input from different modalities using modality-specific encoders, followed by a multi-modal Transformer encoder to fuse and reason over the multi-modal input. Specifically, we use a BERT-base (Devlin et al., 2019) encoder to extract features from the language instructions. For visual observations, we pre-train a ResNet-50 Faster R-CNN (Girshick, 2015) on the ALFRED dataset and use the ResNet backbone to extract image features. Note that we do not update the visual backbone during our training, instead, we use 2 convolution 1 by 1 layers, followed by a fullyconnected (FC) layer, to project the features from ResNet into an embedding of the size 768. To handle actions, we train a lookup table that maps a discrete action to a 768-dimensional embedding. The multi-modal encoder comprises 2 transformer encoder layers, each with 12 self-attention heads, and a hidden size of 768, will take the aforementioned features from each modality, and produce the final state representations. We then use two separate FC layers for action and meta-action prediction. Following Pashevich et al. (2021), we use three different kinds of masking strategies for input from different modalities.\nMasking Strategy Specifically, the language input can only attend to ourselves, it has no access to the image and action input. The visual input can attend to all text features, but we use causal masks\nto prevent them from seeing the future frames and actions. In a similar spirit, we apply the same masking strategy to the action input.\nTraining parameters We train our model for 20 epochs in both pre-training and fine-tuning phases. The learning rate for both phases starts at 0 and linearly warms up to 1 \u00d7 10\u22124 for the first 1000 steps, and drops to 1\u00d7 10\u22125 after 10 epochs. The effective batch size is 32, and we utilize 4 NVIDIA 1080Ti GPUs for training."
        },
        {
            "heading": "A.2 Full List of Meta-Actions",
            "text": "We provide the full list of meta-actions in Table 8. We use letters to represent low-level actions, and we present the translate rule in Table 9. The average length of the original low-level action trajectories is around 50. While the average length of the meta-action sequences after translation is around 10, which effectively reduce the complexity of solution space. The average branching factor of low-level actions is 1250 \u2248 1053 (50 average steps for 12 low-level actions), while for meta-action it is 1010."
        },
        {
            "heading": "A.3 Dynamic Programming for Meta-Action",
            "text": "Identification\nHere we present the details of using dynamicprogramming to identify the optimal meta-action sequences. To perform DP, we begin by determining the valid interval for each meta-action. Algorithm 1 outlines the pseudo-code for this process. We initialize a table to store the intervals associ-\nAlgorithm 1 Finding Valid Interval For Meta-Actions 1: procedure CREATEMETAACTIONTABLE 2: A\u2190 low-level action sequences 3: M \u2190 set of possible meta-actions 4: MATable\u2190 table of size (|M |, |A|, |A|) initialized with 0 5: for i\u2190 1 to |M | do 6: interval\u2190 re.finditer(M [i], A) 7: for j \u2190 1 to |interval| do 8: start, end\u2190 interval[j] 9: if M [i] == moveahead then\n10: MATable[i][start][start : end]\u2190 1 11: else 12: MATable[i][start][end]\u2190 1 13: end if 14: end for 15: end for 16: return MATable 17: end procedure\nAlgorithm 2 Dynamic Programming for Meta-Action Identification 1: procedure IDENTIFYMETAACTIONS 2: A\u2190 low-level action sequences 3: M \u2190 set of possible meta-actions 4: DP \u2190 array of size |A| initialized with\u221e 5: DP [0]\u2190 0 6: MATable\u2190 CREATEMETAACTIONTABLE(A,M ) 7: MetaActions\u2190 array of size |A| initialized with \u22121 8: for i\u2190 1 to |A| do 9: for j \u2190 1 to |M | do\n10: for k \u2190 1 to |M | do 11: if MATable[i][j][k] == 1 then 12: if DP [i] + 1 \u2264 DP [j + 1] then 13: DP [j + 1]\u2190 DP [i] + 1 14: MetaActions[j + 1]\u2190 MetaActions[i].copy() 15: MetaActions[j + 1].append(k) 16: end if 17: end if 18: end for 19: end for 20: end for 21: return MetaActions[\u22121][1 :] 22: end procedure\nated with each meta-action. Using regular expressions, we identify all matching intervals for the meta-actions. If MATable[i][j][k] is equal to 1, it indicates that the i-th meta-action is valid from the j-th action to the k-th action.\nOnce we have the MATable, we can perform DP to find the optimal meta-action sequences, as de-\ntailed in Algorithm 2. We initialize a dynamic programming table with the length of the transformed action sequence. Each cell in the table represents the optimal meta-action sequence up to that point. We iterate through the table, starting from the first cell, and update each cell by considering all possible meta-actions that match the corresponding sub-\nstring of the transformed action sequence. Among these meta-actions, we select the one that will lead to the minimal use of meta-actions so far, and update the current cell with this optimal meta-action sequence, along with the number of meta-actions used. We gradually fill the dynamic programming table until we reach the end of the sequence. Finally, the DP algorithm traces back through the table to retrieve the optimal meta-action sequence."
        },
        {
            "heading": "A.4 Path-Length Weighted (PLW) Scores on ALFRED",
            "text": "Path-length weighted (PLW) scores for vision-andlanguage navigation are proposed in Anderson et al. (2018a). The path-weighted score sp is defined as:\nsp = s\u00d7 L\nmax(L, L\u0302) (2)\nwhere L denotes the path length of the groundtruth trajectories, and L\u0302 represents the length of the predicted paths.\nFrom Table 10 we can observe consistent performance trends as reported in the Table 2, where our model substantially improves the task performance in the unseen environments in terms of success rate (SR) and goal condition success rate (GC)."
        },
        {
            "heading": "A.5 Preliminary Investigation on Backtracking",
            "text": "Since our proposed contrastive learning and meta actions are orthogonal to backtracking, we believe that incorporating backtracking would further improve the performance of our LACMA. From Table 11 we can see that LACMA can be extended to incorporate backtracking and further improve the success rate. Specifically, we first use E.T. to predict a sequence of subgoals and input them into LACMA. If the interaction subgoal fails, we revert to the preceding navigation subgoal. We believe further study on more sophisticated BT methods can be interesting future works."
        },
        {
            "heading": "A.6 More Qualitative Results of the learned Meta-Actions",
            "text": "We provide more results in Fig. 6. The visualization further illustrates the learned meta-actions and their alignment with the retrieved instructions. Despite potential variations from the annotated meta-action sequences, the generated meta-action sequences remain valid and demonstrate a strong correspondence to the language instructions. These supplementary visualizations provide a comprehensive view of the effectiveness and robustness of our approach."
        },
        {
            "heading": "Turn Left Move",
            "text": "Low-level Action:\nLabeled MA (By DP)\nPredicted MA\nRotateRight MoveAhead MoveAhead RotateLeft MoveAhead\nStep Right\nRetrieved Instruction\nTurn right and walk to the counter with the coffee maker on the left\nStep Right"
        }
    ],
    "title": "LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following",
    "year": 2023
}