{
    "abstractText": "Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model\u2019s functional capacity, and provide recommendations for more multifaceted evaluation protocols. \u201cTrust arises from knowledge of origin as well as from knowledge of functional capacity.\u201d Trustworthiness Working Definition David G. Hays, 1979",
    "authors": [
        {
            "affiliations": [],
            "name": "Robert Litschko"
        },
        {
            "affiliations": [],
            "name": "Max M\u00fcller-Eberstein"
        },
        {
            "affiliations": [],
            "name": "Rob van der Goot"
        },
        {
            "affiliations": [],
            "name": "Leon Weber"
        },
        {
            "affiliations": [],
            "name": "Barbara Plank"
        }
    ],
    "id": "SP:06e412dc9c1407386c4b79279361873d8e91d7a8",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Einat Kermany",
                "Yonatan Belinkov",
                "Ofer Lavi",
                "Yoav Goldberg."
            ],
            "title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Afra Amini",
                "Massimiliano Ciaramita."
            ],
            "title": "Probing in context: Toward building robust classifiers via probing large language models",
            "venue": "arXiv preprint arXiv:2305.14171.",
            "year": 2023
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Ponti",
                "Anna Korhonen",
                "Ivan Vuli\u0107."
            ],
            "title": "Composable sparse fine-tuning for crosslingual transfer",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1778\u20131796,",
            "year": 2022
        },
        {
            "authors": [
                "Pepa Atanasova",
                "Oana-Maria Camburu",
                "Christina Lioma",
                "Thomas Lukasiewicz",
                "Jakob Grue Simonsen",
                "Isabelle Augenstein."
            ],
            "title": "Faithfulness tests for natural language explanations",
            "venue": "Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Caglar Aytekin."
            ],
            "title": "Neural networks are decision trees",
            "venue": "arXiv preprint arXiv:2210.05189.",
            "year": 2022
        },
        {
            "authors": [
                "Tita Alissa Bach",
                "Amna Khan",
                "Harry Hallock",
                "Gabriela Beltr\u00e3o",
                "Sonia Sousa."
            ],
            "title": "A systematic literature review of user trust in ai-enabled systems: An hci perspective",
            "venue": "International Journal of Human\u2013 Computer Interaction, pages 1\u201316.",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Baum",
                "Maximilian A. K\u00f6hl",
                "Eva Schmidt."
            ],
            "title": "Two challenges for CI trustworthiness and how to address them",
            "venue": "Proceedings of the 1st Workshop on Explainable Computational Intelligence (XCI 2017), Dundee, United Kingdom. Association",
            "year": 2017
        },
        {
            "authors": [
                "Samuel Bowman."
            ],
            "title": "The dangers of underclaiming: Reasons for caution when reporting how NLP systems fail",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7484\u20137499, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Sabrina Chiesurin",
                "Dimitris Dimakopoulos",
                "Arash Eshghi",
                "Ioannis Papaioannou",
                "Verena Rieser",
                "Ioannis Konstas"
            ],
            "title": "The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational",
            "year": 2023
        },
        {
            "authors": [
                "Rochelle Choenni",
                "Dan Garrette",
                "Ekaterina Shutova."
            ],
            "title": "How do languages influence each other? studying cross-lingual data sharing during llm fine-tuning",
            "venue": "arXiv preprint arXiv:2305.13286.",
            "year": 2023
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Y. Zhao",
                "Yanping Huang",
                "Andrew M. Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Ian Soboroff"
            ],
            "title": "hmc: A spectrum of human\u2013machine-collaborative relevance judgment frameworks. Frontiers of Information Access Experimentation for Research and Education, page",
            "year": 2023
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "German Kruszewski",
                "Guillaume Lample",
                "Lo\u00efc Barrault",
                "Marco Baroni."
            ],
            "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
            "venue": "Proceedings of the 56th Annual Meeting of the As-",
            "year": 2018
        },
        {
            "authors": [
                "Maxime De Bruyn",
                "Ehsan Lotfi",
                "Jeska Buhmann",
                "Walter Daelemans."
            ],
            "title": "20Q: Overlap-free world knowledge benchmark for language models",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages",
            "year": 2022
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Editing factual knowledge in language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491\u2013 6506, Online and Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer."
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "CoRR, abs/2305.14314.",
            "year": 2023
        },
        {
            "authors": [
                "Nouha Dziri",
                "Sivan Milton",
                "Mo Yu",
                "Osmar Zaiane",
                "Siva Reddy"
            ],
            "title": "On the origin of hallucinations in conversational models: Is it the datasets or the models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Eisenstein."
            ],
            "title": "Informativeness and invariance: Two perspectives on spurious correlations in natural language",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Yejin Choi",
                "Swabha Swayamdipta."
            ],
            "title": "Understanding dataset difficulty with V-usable information",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine",
            "year": 2022
        },
        {
            "authors": [
                "Robert M French."
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in cognitive sciences, 3(4):128\u2013135.",
            "year": 1999
        },
        {
            "authors": [
                "Nicholas Frosst",
                "Geoffrey Hinton."
            ],
            "title": "Distilling a neural network into a soft decision tree",
            "venue": "arXiv preprint arXiv:1711.09784.",
            "year": 2017
        },
        {
            "authors": [
                "Michael Gira",
                "Ruisu Zhang",
                "Kangwook Lee."
            ],
            "title": "Debiasing pre-trained language models via efficient fine-tuning",
            "venue": "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 59\u201369, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Tobias Glasmachers."
            ],
            "title": "Limits of end-to-end learning",
            "venue": "Asian conference on machine learning, pages 17\u201332. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Ana Valeria Gonz\u00e1lez",
                "Gagan Bansal",
                "Angela Fan",
                "Yashar Mehdad",
                "Robin Jia",
                "Srinivasan Iyer."
            ],
            "title": "Do explanations help users detect errors in opendomain QA? an evaluation of spoken vs",
            "venue": "visual explanations. In Findings of the Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "David G. Hays."
            ],
            "title": "Applications",
            "venue": "17th Annual Meeting of the Association for Computational Linguistics, pages 89\u201389, La Jolla, California, USA. Association for Computational Linguistics.",
            "year": 1979
        },
        {
            "authors": [
                "Michael A Hedderich",
                "Jonas Fischer",
                "Dietrich Klakow",
                "Jilles Vreeken."
            ],
            "title": "Label-descriptive patterns and their application to characterizing classification errors",
            "venue": "International Conference on Machine Learning, pages 8691\u20138707. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco Tulio Ribeiro",
                "Mitchell Wortsman",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi."
            ],
            "title": "Editing models with task arithmetic",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Anne Lauscher",
                "Federico Bianchi",
                "Samuel R. Bowman",
                "Dirk Hovy."
            ],
            "title": "SocioProbe: What, when, and where language models learn about sociodemographics",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Dongfang Li",
                "Baotian Hu",
                "Qingcai Chen."
            ],
            "title": "Calibration meets explanation: A simple and effective approach for model confidence estimates",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2775\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lorraine Li",
                "Adhiguna Kuncoro",
                "Jordan Hoffmann",
                "Cyprien de Masson d\u2019Autume",
                "Phil Blunsom",
                "Aida Nematzadeh"
            ],
            "title": "2022b. A systematic investigation of commonsense knowledge in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Xingxuan Li",
                "Ruochen Zhao",
                "Yew Ken Chia",
                "Bosheng Ding",
                "Lidong Bing",
                "Shafiq Joty",
                "Soujanya Poria."
            ],
            "title": "Chain of knowledge: A framework for grounding large language models with structured knowledge bases",
            "venue": "arXiv preprint arXiv:2305.13269.",
            "year": 2023
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models. arXiv preprint arXiv:2211.09110",
            "year": 2022
        },
        {
            "authors": [
                "Alex Mallen",
                "Akari Asai",
                "Victor Zhong",
                "Rajarshi Das",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
            "venue": "Proceedings of the 61st Annual Meeting of",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Manning",
                "Mihai Surdeanu",
                "John Bauer",
                "Jenny Finkel",
                "Steven Bethard",
                "David McClosky."
            ],
            "title": "The Stanford CoreNLP natural language processing toolkit",
            "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen."
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "Psychology of learning and motivation, volume 24, pages 109\u2013165. Elsevier.",
            "year": 1989
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u00ec",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz"
            ],
            "title": "Augmented language models: a survey",
            "venue": "arXiv preprint arXiv:2302.07842",
            "year": 2023
        },
        {
            "authors": [
                "Alessio Miaschi",
                "Dominique Brunato",
                "Felice Dell\u2019Orletta",
                "Giulia Venturi"
            ],
            "title": "Linguistic profiling of a neural language model",
            "venue": "In Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Alessio Miaschi",
                "Dominique Brunato",
                "Felice Dell\u2019Orletta",
                "Giulia Venturi"
            ],
            "title": "What makes my model perplexed? a linguistic investigation on neural language models perplexity",
            "year": 2021
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Wen-tau Yih",
                "Geoffrey Zweig."
            ],
            "title": "Linguistic regularities in continuous space word representations",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2013
        },
        {
            "authors": [
                "R OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv, pages 2303\u201308774.",
            "year": 2023
        },
        {
            "authors": [
                "der",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott Lundberg",
                "Sameer Singh",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro."
            ],
            "title": "Art: Automatic multistep reasoning and tool-use for large language models",
            "venue": "arXiv preprint arXiv:2303.09014.",
            "year": 2023
        },
        {
            "authors": [
                "Denis Peskoff",
                "Brandon Stewart."
            ],
            "title": "Credible without credit: Domain experts assess generative language models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 427\u2013438,",
            "year": 2023
        },
        {
            "authors": [
                "Aleksandra Piktus",
                "Christopher Akiki",
                "Paulo Villegas",
                "Hugo Lauren\u00e7on",
                "G\u00e9rard Dupont",
                "Alexandra Sasha Luccioni",
                "Yacine Jernite",
                "Anna Rogers."
            ],
            "title": "The roots search tool: Data transparency for llms",
            "venue": "arXiv preprint arXiv:2302.14035.",
            "year": 2023
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Alessandro Sordoni",
                "Yoshua Bengio",
                "Siva Reddy."
            ],
            "title": "Combining parameterefficient modules for task-level generalisation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Karl Popper."
            ],
            "title": "Karl Popper: Logik der Forschung",
            "venue": "Mohr Siebeck, T\u00fcbingen, Germany.",
            "year": 1934
        },
        {
            "authors": [
                "Garima Pruthi",
                "Frederick Liu",
                "Satyen Kale",
                "Mukund Sundararajan."
            ],
            "title": "Estimating training data influence by tracing gradient descent",
            "venue": "Advances in Neural Information Processing Systems, 33:19920\u201319930.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Deborah Raji",
                "Emily Denton",
                "Emily M. Bender",
                "Alex Hanna",
                "Amandalynne Paullada."
            ],
            "title": "Ai and the everything in the whole wide world benchmark",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, vol-",
            "year": 2021
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Pedro Rodriguez",
                "Joe Barrow",
                "Alexander Miserlis Hoyle",
                "John P. Lalor",
                "Robin Jia",
                "Jordan BoydGraber"
            ],
            "title": "Evaluation examples are not equally informative: How should that change NLP leaderboards",
            "venue": "In Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Anna Rogers",
                "Niranjan Balasubramanian",
                "Leon Derczynski",
                "Jesse Dodge",
                "Alexander Koller",
                "Sasha Luccioni",
                "Maarten Sap",
                "Roy Schwartz",
                "Noah A Smith",
                "Emma Strubell"
            ],
            "title": "Closed ai models make bad baselines",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Ruffinelli",
                "Samuel Broscheit",
                "Rainer Gemulla."
            ],
            "title": "You can teach an old dog new tricks! on training knowledge graph embeddings",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault F\u00e9vry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2022
        },
        {
            "authors": [
                "Gabriele Sarti",
                "Dominique Brunato",
                "Felice Dell\u2019Orletta"
            ],
            "title": "That looks hard: Characterizing linguistic complexity in humans and language models",
            "venue": "In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Klamm",
                "Colin Leong",
                "Daniel van Strien",
                "David Ifeoluwa Adelani"
            ],
            "title": "BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "David Schlangen."
            ],
            "title": "Targeting the benchmark: On methodology in current natural language processing research",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Schramowski",
                "Wolfgang Stammer",
                "Stefano Teso",
                "Anna Brugger",
                "Franziska Herbert",
                "Xiaoting Shao",
                "Hans-Georg Luigs",
                "Anne-Katrin Mahlein",
                "Kristian Kersting"
            ],
            "title": "Making deep neural networks right for the right scientific reasons by interacting",
            "year": 2020
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580.",
            "year": 2023
        },
        {
            "authors": [
                "Ilia Shumailov",
                "Zakhar Shumaylov",
                "Yiren Zhao",
                "Yarin Gal",
                "Nicolas Papernot",
                "Ross Anderson."
            ],
            "title": "The curse of recursion: Training on generated data makes models forget",
            "venue": "arXiv preprint arxiv:2305.17493.",
            "year": 2023
        },
        {
            "authors": [
                "Alison Smith-Renner",
                "Ron Fan",
                "Melissa Birchfield",
                "Tongshuang Wu",
                "Jordan Boyd-Graber",
                "Daniel S Weld",
                "Leah Findlater."
            ],
            "title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A. Smith",
                "Yejin Choi"
            ],
            "title": "Dataset cartography: Mapping",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto."
            ],
            "title": "Alpaca: A strong, replicable instruction-following model",
            "venue": "Stanford Center for Research on Foundation Models.",
            "year": 2023
        },
        {
            "authors": [
                "Simone Tedeschi",
                "Johan Bos",
                "Thierry Declerck",
                "Jan Haji\u010d",
                "Daniel Hershcovich",
                "Eduard Hovy",
                "Alexander Koller",
                "Simon Krek",
                "Steven Schockaert",
                "Rico Sennrich",
                "Ekaterina Shutova",
                "Roberto Navigli"
            ],
            "title": "What\u2019s the meaning of superhuman",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "2023b. Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Veniamin Veselovsky",
                "Manoel Horta Ribeiro",
                "Robert West."
            ],
            "title": "Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks",
            "venue": "arXiv preprint arXiv:2306.07899.",
            "year": 2023
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Goran Glava\u0161",
                "Fangyu Liu",
                "Nigel Collier",
                "Edoardo Maria Ponti",
                "Anna Korhonen."
            ],
            "title": "Probing cross-lingual lexical knowledge from multilingual sentence encoders",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the As-",
            "year": 2023
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Robert Litschko",
                "Goran Glava\u0161",
                "Anna Korhonen."
            ],
            "title": "Probing pretrained language models for lexical semantics",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Kaiyue Wen",
                "Zhengyan Zhang",
                "Lei Hou",
                "Zhiyuan Liu",
                "Juanzi Li."
            ],
            "title": "Finding skill neurons in pre-trained transformer-based language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Hamish Ivison",
                "Pradeep Dasigi",
                "Jack Hessel",
                "Tushar Khot",
                "Khyathi Raghavi Chandu",
                "David Wadden",
                "Kelsey MacMillan",
                "Noah A. Smith",
                "Iz Beltagy",
                "Hannaneh Hajishirzi"
            ],
            "title": "2023b. How far can camels go? exploring the state of instruction",
            "year": 2023
        },
        {
            "authors": [
                "Shailaja Keyur Sampat",
                "Siddhartha Mishra",
                "Sujan Reddy A",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen."
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022c. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Orion Weller",
                "Marc Marone",
                "Nathaniel Weir",
                "Dawn Lawrie",
                "Daniel Khashabi",
                "Benjamin Van Durme"
            ],
            "title": " according to...\" prompting language models improves quoting from pre-training data",
            "venue": "arXiv preprint arXiv:2305.13252",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "\u201cTrust arises from knowledge of origin as well as from knowledge of functional capacity.\u201d\nTrustworthiness - Working Definition David G. Hays, 1979"
        },
        {
            "heading": "1 Introduction",
            "text": "Understanding natural language requires a multitude of cognitive capabilities which act holistically to form meaning. Modeling this ability computationally is extremely difficult, thereby necessitating a compartmentalization of the problem into isolated tasks which are solvable with available methods and resources (Schlangen, 2021). Undoubtedly\n\u2217 Equal contribution.\nas of late 2022, we are witnessing a paradigm shift: Powerful LLMs, in the form of instruction-tuned, prompt-based generative models such as ChatGPT and GPT-4 (Wei et al., 2022a; Touvron et al., 2023b; Taori et al., 2023; OpenAI, 2023; Bubeck et al., 2023, inter alia), have found widespread adoption reaching far beyond the NLP community. Part of this success story is the casting of heterogeneous NLP tasks into sequence-to-sequence tasks (Raffel et al., 2020; Sanh et al., 2022; Wang et al., 2022b); which in turn enables extreme multi-task learning, and cross-task transfer learning.\nThis is in stark contrast to the traditional compartmentalized NLP paradigm (visualized in Figure 1), wherein a human-motivated language task with an input expression and an output expectation is clearly formalized into a dataset with machinereadable inputs and outputs. Both feature design and model development are highly task-specific\u2014 often manually curated. Paired with evaluation protocols for comparing model predictions with\nhuman expectations via formalized metrics or qualitative judgement, this general methodology has been widely adopted and trusted.1 However, with contemporary LLMs this compartmentalization is breaking down\u2014having severe impacts on all stages of the cycle. Therefore, a persistent and critical question regains importance: How can trust be established between the human and the model?\nAs early as 44 years ago, Hays (1979) offers an attempt and provides a definition of trustworthiness (cf. quote). Today, the topic of trustworthiness is an ongoing discussion deserving special attention (Baum et al., 2017; Eisenstein, 2022; Clarke et al., 2023). We argue that to establish trust, it is time to rethink how we deal with tasks and their evaluation. Why now? It is getting increasingly hard to predict a priori when we can expect models trained on web-scale data to work well. Were we to live in a hypothetical world with full knowledge of origin and functional capacity, then each task instance could be routed to the right model(s) to not only tap into the LLMs\u2019 full potential, but to also enable trust in their predictions. Today, the absence of this knowledge is directly linked to our lack of trust in deploying models in real-world scenarios.\nIn this position paper, we synthesize contemporary work distributed throughout different subfields of NLP and ML into a conceptual framework for trust, guided by Hays (1979)\u2019s definition and centered around knowledge facets as a guiding principle for all aspects of the model development and evaluation cycle. We outline high-level desiderata (\u00a72), and suggest directions on how to gain trust, by providing starting points of facets (\u00a73) aimed to stipulate uptake and discussion. In \u00a74 we discuss how trustworthiness relates to user trust."
        },
        {
            "heading": "2 Desiderata for Trustworthy LLMs",
            "text": "LLMs today pose a conundrum: They are seemingly universally applicable, having high functional capacity, however, the larger the model, the less we appear to know about the origins of its capabilities. How did we get here, which aspects contribute to trustworthiness, and what did we lose on the way? In the following, we aim to provide a brief history of central trust desiderata (D1-4), discussing how our knowledge of functional capacity and its origins has changed over time.\n1While not without deficiencies, evaluation protocols were arguably more heterogeneous and established than today w.r.t. quantitative/qualitative evaluation, human judgements etc.\nD1. Knowledge about Model Input. In the beginnings of NLP, researchers followed strict, taskspecific formalizations and had precise control over which \u201cingredients\u201d2 go into model training and inference (i.e., manual feature engineering). Neural models have caused a shift towards learning representations, improving performance at the cost of interpretability. While analogy tasks (Mikolov et al., 2013) have enabled analyses of how each word-level representation is grounded, contemporary representations have moved to the subword level, and are shared across words and different languages, obscuring our knowledge of the origin of their contents, and requiring more complex lexical semantic probing (Vulic\u0301 et al., 2020, 2023). This is amplified in today\u2019s instruction-based paradigm in which tasks are no longer formalized by NLP researchers and expert annotators but are formulated as natural language expressions by practitioners and end users (Ouyang et al., 2022). The cognitive process of formalizing raw model inputs into ML features has been incrementally outsourced from the human to the representation learning algorithm, during which we lose knowledge over functional capacity.\nD2. Knowledge about Model Behaviour. In the old compartmentalized view of NLP, higher-level tasks are typically broken down into pipelines of subtasks (Manning et al., 2014), where inspecting intermediate outputs improves our knowledge about model behaviour. Recently however, LLMs are usually trained on complex tasks in an end-toend fashion (Glasmachers, 2017), which makes it more difficult to expose intermediate outputs and analyze error propagation. Over time we have gained powerful black-box models, but have lost the ability to interpret intermediate states and decision boundaries, thus increasing uncertainty and complexity. Because as of today, we cannot build models that always provide factually correct, up-todate information, we cannot trust to employ these models at a large scale, in real-world scenarios, where reliability and transparency are key. In this regard, pressing questions are e.g., how hallucination and memorization behaviour can be explained (Dziri et al., 2022; Mallen et al., 2023), how models behave when trained on many languages (Conneau et al., 2020; Choenni et al., 2023), what internal features are overwritten when trained on differ-\n2We refer to ingredients as explicit inputs and LLM\u2019s parametric knowledge (De Cao et al., 2021; Mallen et al., 2023).\nent tasks sequentially (catastrophic forgetting; e.g., McCloskey and Cohen, 1989; French, 1999), how to improve models\u2019 ability to know when they do not know (model uncertainty; e.g., Li et al., 2022a), or how do LLMs utilize skills and knowledge distributed in their model parameters.\nD3. Knowledge of Evaluation Protocols. The emergence of LLMs has raised the question of how to evaluate general-purpose models. Many recent efforts have followed the traditional NLP evaluation paradigm and summarized LLM performance into evaluation metrics across existing benchmark datasets (Sanh et al., 2022; Wang et al., 2022b; Scao et al., 2022; Wei et al., 2022a; Touvron et al., 2023a). This estimates LLM performance for tasks covered by the benchmark dataset and thus establishes trust when applying the model to the same task. However, the situation is different when LLMs are used to solve tasks outside of the benchmark, which is often the case for real-world usage of LLMs (Ouyang et al., 2022). Then, the expected performance becomes unclear and benchmark results become insufficient to establish trust. One proposal to solve this issue is to evaluate on a wide variety of task-agnostic user inputs and report an aggregate metric (Ouyang et al., 2022; Chung et al., 2022; Wang et al., 2023b; Dettmers et al., 2023). This approach has the potential to cover a wider range of use cases, however, it relies mostly on manual preference annotations from human labelers or larger LLMs which is costly and has no accepted protocol yet.\nD4. Knowledge of Data Origin. So far, we discussed trust desiderata from the viewpoint of knowledge of functional capacity. Next to this, a model\u2019s behaviour is also largely influenced by its training data. Knowledge about data provenance helps us make informed decisions about whether a given LLM is a good match for the intended use case. Therefore, open access to data must be prioritized. In compartmentalized NLP, models are trained and evaluated on well-known, manually curated, task-specific datasets. Today\u2019s models are instead trained on task-heterogeneous corpora at web scale, typically of unknown provenance. For novel tasks, this means we do not know how well relevant facets (e.g., language, domain) are represented in the training data. For existing tasks, it is unclear if the model has seen test instances in their large training corpora (i.e., test data leakage; Piktus et al., 2023), blurring the lines between traditional\ntrain-dev-test splits and overestimating the capabilities of LLMs. To compound matters further, models are not only trained on natural, but also on generated data, and unknown data provenance is also becoming an issue as annotators start to use LLMs (Veselovsky et al., 2023). LLMs trained on data generated by other LLMs can lead to a \u201ccurse of recursion\u201d where (im-)probable events are over/underestimated (Shumailov et al., 2023).\n3 What Can We Do to Gain Trust Now and in Future?\nIn a world where generative LLMs seemingly dominate every benchmark and are claimed to have reached human-level performance on many tasks,3 we advocate that now is the time to treat trust as a first-class citizen and place it at the center of model development and evaluation. To operationalize the concept of trust, we denote with knowledge facets (henceforth, facets) all factors that improve our knowledge of functional capacity and knowledge of origin. Facets can be local (instance) or global (datasets, tasks). They refer to 1) descriptive knowledge such as meta-data or data/task provenance, and 2) inferred knowledge; for example which skills are exploited. We next propose concrete suggestions on how facets can help us gain trust in LLMs based on the desiderata in \u00a72.\nExplain Skills Required versus Skills Employed. It is instructive to think of prompt-based generative LLMs as instance-level problem solvers and, as such, we need to understand a-priori the necessary skills for solving instances (local facets) as well as knowing what skills are actually employed during inference. Most prior work aims to improve our understanding of tasks and the skills acquired to solve them by studying models trained specifically for each task, and can be broadly classified into: (i) linguistically motivated approaches and (ii) model-driven approaches (D1). Linguistic approaches formalize skills as cognitive abilities, which are studied, e.g., through probing tasks (Adi et al., 2017; Conneau et al., 2018; Amini and Ciaramita, 2023), checklists (Ribeiro et al., 2020) and linguistic profiling (Miaschi et al., 2020, 2021; Sarti et al., 2021). Model-driven approaches attribute regions in the model parameter space to skills (Ansell et al., 2022; Wang et al., 2022a; Ponti\n3For example, GPT-4 reportedly passed the bar exam and placed top at GRE exams, see https://openai.com/ research/gpt-4.\net al., 2023; Ilharco et al., 2023). The former can be seen as describing global facets (i.e., the overall functional capacity of black-box models), while the latter identifies local facets (i.e., skill regions in model parameters). To establish trust, we need to know what skills are required to solve instances, which is different from which skills are exercised by a model at inference time, as described next.\nBesides knowlege about skills needed to solve a task, it is important to gain knowledge about what skills are actually being applied by an LLM. This is linked to explainability and transparency, corresponding to (i) understanding the knowledge4 that goes into the inference process (D1), and (ii) the inference process itself in terms of applied skills (D2), e.g., examinations of LLMs\u2019 \u201cthought processes\u201d. Regarding (i), existing work includes attributing training instances to model predictions (Pruthi et al., 2020; Weller et al., 2023) and explaining predictions through the lens of white-box models (Frosst and Hinton, 2017; Aytekin, 2022; Hedderich et al., 2022). They are, however, often grounded in downstream task data and thus do not provide insights connected to the knowledge memorized by LLMs during pre-training (global facets). Regarding (ii), existing approaches include guiding the generation process through intermediate steps (Wei et al., 2022c; Wang et al., 2023a; Li et al., 2023) and pausing the generation process to call external tools (Schick et al., 2023; Shen et al., 2023; Paranjape et al., 2023; Mialon et al., 2023). Their shortcoming is that they operate on the input level, and similarly do not capture cases where pre-existing, model-internal knowledge is applied. Furthermore, prior work has shown that LLMs follow the path of least resistance. That is, neural networks are prone to predict the right thing for the wrong reasons (McCoy et al., 2019; Schramowski et al., 2020), which can be caused by spurious correlations (Eisenstein, 2022).5 On the path to gaining trust, we advocate for LLMs that are able to attribute their output to internal knowledge and the skills used to combine that knowledge. Alternatively, LLMs could be accompanied by white-box explanation models that (are at least a proxy) for explaining the inference process. Facilitate Representative and Comparable Qualitative Analysis. Today, the standard target for\n4Including acquired knowledge such as common sense and world knowledge (Li et al., 2022b; De Bruyn et al., 2022).\n5 \u201cThe sentiment of a movie should be invariant to the identity of the actors in the movie\u201d (Eisenstein, 2022)\nNLP papers proposing a new model is to beat previous models on a certain quantitative benchmark. We argue that if datasets and metrics are well-designed and well-grounded in skills/capabilities, they can be used as an indicator of progress.6 On the other hand, findings from negative results might be obscured without faceted quantitative analysis: even when obtaining lower scores on a benchmark, sub-parts of an NLP problem may be better solved compared to the baseline, but go unnoticed (D3). We therefore cannot trust reported SOTA results as long as the facets that explain how well sub-problems are solved remain hidden. Complementary to holistic quantitative explanations, as proposed by HELM (Liang et al., 2022), we call for a holistic qualitative evaluation where benchmarks come with standardized qualitative evaluation protocols, which facilitates comparable qualitative meta-analysis. This proposal is inspired by the manually-curated GLUE diagnostics annotations (Wang et al., 2018), which describe examples by their linguistic phenomena.7 Recycling existing tasks and augmenting them with diagnostic samples to study LLMs provides a very actionable direction for applying existing compartmentalization in a more targeted trustworthy way. Diagnostics samples should ideally represent the full spectrum of cognitive abilities required to solve a task. Designing these samples is however a complex task. We hypothesize that the set of required skills varies between tasks and should ideally be curated by expert annotators.\nBe Explicit about Data Provenance. In ML, it is considered good practice to use stratified data splits to avoid overestimation of performance on dev/test splits based on contamination. Traditionally, this stratification was done based on, e.g., source, time, author, language (cross-lingual), or domain (cross-domain). Recent advances have hinted at LLMs\u2019 ability to solve new tasks, and even to obtain new, i.e., emergent abilities (Wei et al., 2022b). These are in fact similar cross-X settings, where X is no longer a property at the level of dataset sampling, but of the broader task setup. We call for always employing a cross-X setup (D4); whether it is based on data sampling, tasks, or capabilities\u2014 urging practitioners to make this choice explicit. Transparency about data provenance and test data\n6Note that baseline comparisons can still be obscured by unfair comparisons (Ruffinelli et al., 2020).\n7https://gluebenchmark.com/diagnostics/\nleakage improve our trust in reported results. In practice, these data provenance facets are also valuable for identifying inferred knowledge such as estimated dataset/instance difficulty (Swayamdipta et al., 2020; Rodriguez et al., 2021; Ethayarajh et al., 2022), especially when used in conjunction with the aforementioned diagnostic facets.\nData provenance is also important when drawing conclusions from benchmark results (D3). Tedeschi et al. (2023) question the notion of superhuman performance and claims of tasks being solved (i.e., overclaiming model capabilities), and criticize how benchmark comparisons \u201cdo not incentivize a deeper understanding of the systems\u2019 performance\u201d. The authors discuss how external factors can cause variation in human-level performance (incl. annotation quality) and lead to unfair comparisons. Similarly, underclaiming LLMs\u2019 capabilities also obfuscates our knowledge of their functional capacity (Bowman, 2022). Additionally, in a recent study domain experts find the accuracy of LLMs to be mixed (Peskoff and Stewart, 2023). It is therefore important to be explicit about the limitations of benchmarks (Raji et al., 2021) and faithful in communicating model capabilities. At the same time, it is an ongoing discussion whether reviewers should require (i.e, disincentivize the absence of) closed-source baseline models such as ChatGPT and GPT-4, which do not meet our trust desiderata (Rogers et al., 2023). Closed-source models that sit behind APIs typically evolve over time and have unknown data provenance, thus lacking both knowledge of origin (D4), and the consistency of its functional capacity. Consequently, they make untrustworthy baselines and should not be used as an isolated measure of progress."
        },
        {
            "heading": "4 Trustworthiness and User Trust",
            "text": "So far we have discussed different avenues for improving our knowledge about LLM\u2019s functional capacity and origin, paving the way for establishing trustworthiness. From a user perspective it is essential to not only understand knowledge facets but also how they empirically impact user trust in a collaborative environment. This is especially important in high-risk scenarios such as in the medical and legal domain. One could argue, if LLMs such as ChatGPT are already widely adopted, do we already trust LLMs (too much)? To better understand user trust we need interdisciplinary research and user experience studies on human-AI collaboration.\nSpecifically, we need to know what users do with the model output across multiple interactions (e.g., verify, fact check, revise, accept). For example, Gonz\u00e1lez et al. (2021) investigate the connection between explanations (D2) and user trust in the context of question answering systems. In their study users are presented with explanations in different modalities and either accept (trust) or reject (don\u2019t trust) candidate answers. Similarly, Smith-Renner et al. (2020) discuss how generated explanations can promote over-reliance or undermine user trust. A closely related question is how the faithfulness of explanations affect user trust (Atanasova et al., 2023; Chiesurin et al., 2023). For a comprehensive overview on user trust we refer to the recent survey by Bach et al. (2022).\nWhile such controlled studies using human feedback are cost and time intensive, the minimum viable alternative for establishing trust may simply be the publication of a model\u2019s input-output history. In contrast to standalone metrics and cherry-picked qualitative examples, access to prior predictions enables post-hoc knowledge of model behaviour (D2), even without direct access to the model. This democratizes the ability to verify functional capacity and helps end users seeking to understand how well a model works for their task.\nIn summary, evaluating user trust is an integral part of trustworthiness and goes hand in hand with careful qualitative analyses and faceted quantitative evaluation. Towards this goal, we believe LLM development needs to be more human-centric."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this position paper, we emphasize that the democratization of LLMs calls for the need to rethink tasks and model evaluation, placing trustworthiness at its center. We adopt a working definition of trustworthiness and establish desiderata required to improve our knowledge of LLMs (\u00a72), followed by suggestions on how trust can be gained by outlining directions guided by what we call knowledge facets (\u00a73). Finally, we draw a connection between trustworthiness as knowledge facets and user trust as means to evaluate their impact on human-AI collaboration (\u00a74).\nLimitations\nTo limit the scope of this work, we did not discuss the topics of social and demographic biases (Gira et al., 2022), discrimination of minority groups\n(Lauscher et al., 2022) and hate speech as factors influencing our trust in LLMs. Within our proposed desiderata, this facet would fall under \u2018Knowledge of Data Origin\u2019 (\u00a72), in terms of understanding where model-internal knowledge and the associated biases originate from (D4).\nOur proposed multi-faceted evaluation protocols rely strongly on human input\u2014either via qualitative judgements and/or linguistically annotated diagnostic benchmarks (\u00a73). We acknowledge that such analyses require more time and resources compared to evaluation using contemporary, automatic metrics, and may slow down the overall research cycle. While we believe that slower, yet more deliberate analyses are almost exclusively beneficial to establishing trust, our minimum effort alternative of publishing all model predictions can also be used to build user trust (\u00a74). This simple step closely mirrors the scientific method, where hypotheses must be falsifiable by anyone (Popper, 1934). Identifying even a single incorrect prediction for a similar task in a model\u2019s prediction history, can already tell us plenty about the model\u2019s trustworthiness."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers for their insightful comments. This research is supported by the Independent Research Fund Denmark (DFF) Sapere Aude grant 9063-00077B and ERC Consolidator Grant DIALECT 101043235."
        }
    ],
    "title": "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation",
    "year": 2023
}