{
    "abstractText": "While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at link.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yimu Wang"
        },
        {
            "affiliations": [],
            "name": "Peng Shi"
        }
    ],
    "id": "SP:c9afcca93f2750ba784d93575c413b65163e9a04",
    "references": [
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman."
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October",
            "year": 2021
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shuqiang Cao",
                "Bairui Wang",
                "Wei Zhang",
                "Lin Ma."
            ],
            "title": "Visual consensus modeling for video-text retrieval",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "David Chen",
                "William Dolan."
            ],
            "title": "Collecting highly parallel data for paraphrase evaluation",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190\u2013200, Portland, Ore-",
            "year": 2011
        },
        {
            "authors": [
                "Dongsheng Chen",
                "Chaofan Tao",
                "Lu Hou",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "LiteVL: Efficient video-language learning with enhanced spatialtemporal modeling",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Yizhen Chen",
                "Jie Wang",
                "Lijian Lin",
                "Zhongang Qi",
                "Jin Ma",
                "Ying Shan."
            ],
            "title": "Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval",
            "venue": "AAAI Conference on Artificial Intelligence. arXiv. ArXiv:2301.12644 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Xing Cheng",
                "Hezheng Lin",
                "Xiangyu Wu",
                "Fan Yang",
                "Dong Shen."
            ],
            "title": "Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss",
            "venue": "CoRR, abs/2109.04290.",
            "year": 2021
        },
        {
            "authors": [
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Marius Leordeanu",
                "Hailin Jin",
                "Andrew Zisserman",
                "Samuel Albanie",
                "Yang Liu."
            ],
            "title": "Teachtext: Crossmodal generalized distillation for text-video retrieval",
            "venue": "2021 IEEE/CVF International Conference on Com-",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jianfeng Dong",
                "Xirong Li",
                "Chaoxi Xu",
                "Shouling Ji",
                "Yuan He",
                "Gang Yang",
                "Xun Wang."
            ],
            "title": "Dual encoding for zero-example video retrieval",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby."
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "9th International Conference",
            "year": 2021
        },
        {
            "authors": [
                "Maksim Dzabraev",
                "Maksim Kalashnikov",
                "Stepan Komkov",
                "Aleksandr Petiushko."
            ],
            "title": "MDMMT: multidomain multimodal transformer for video retrieval",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops",
            "year": 2021
        },
        {
            "authors": [
                "Bernard Ghanem Fabian Caba Heilbron",
                "Victor Escorcia",
                "Juan Carlos Niebles."
            ],
            "title": "Activitynet: A large-scale video benchmark for human activity understanding",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2015
        },
        {
            "authors": [
                "Xiang Fang",
                "Daizong Liu",
                "Pan Zhou",
                "Yuchong Hu."
            ],
            "title": "Multi-modal cross-domain alignment network for video moment retrieval",
            "venue": "IEEE Transactions on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Fang",
                "Daizong Liu",
                "Pan Zhou",
                "Guoshun Nan."
            ],
            "title": "You can ground earlier than see: An effective and efficient pipeline for temporal sentence grounding in compressed videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Fang",
                "Daizong Liu",
                "Pan Zhou",
                "Zichuan Xu",
                "Ruixuan Li."
            ],
            "title": "Hierarchical local-global transformer for temporal sentence grounding",
            "venue": "IEEE Transactions on Multimedia.",
            "year": 2023
        },
        {
            "authors": [
                "Valentin Gabeur",
                "Chen Sun",
                "Karteek Alahari",
                "Cordelia Schmid."
            ],
            "title": "Multi-modal transformer for video retrieval",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, volume 12349 of",
            "year": 2020
        },
        {
            "authors": [
                "Zhe Gan",
                "Yen-Chun Chen",
                "Linjie Li",
                "Chen Zhu",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Large-scale adversarial training for vision-and-language representation learning",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural",
            "year": 2020
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Zijian Gao",
                "Jingyu Liu",
                "Sheng Chen",
                "Dedan Chang",
                "Hao Zhang",
                "Jinwei Yuan."
            ],
            "title": "CLIP2TV: an empirical study on transformer-based methods for video-text retrieval",
            "venue": "CoRR, abs/2111.05610.",
            "year": 2021
        },
        {
            "authors": [
                "F. Gianfelici."
            ],
            "title": "Nearest-neighbor methods in learning and vision",
            "venue": "IEEE Transactions on Neural Networks, 19(2):377\u2013377.",
            "year": 2008
        },
        {
            "authors": [
                "Satya Krishna Gorti",
                "No\u00ebl Vouitsis",
                "Junwei Ma",
                "Keyvan Golestan",
                "Maksims Volkovs",
                "Animesh Garg",
                "Guangwei Yu."
            ],
            "title": "X-pool: Cross-modal languagevideo attention for text-video retrieval",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2022
        },
        {
            "authors": [
                "Siteng Huang",
                "Biao Gong",
                "Yulin Pan",
                "Jianwen Jiang",
                "Yiliang Lv",
                "Yuyuan Li",
                "Donglin Wang."
            ],
            "title": "VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2023
        },
        {
            "authors": [
                "Xiangru Jian",
                "Yimu Wang"
            ],
            "title": "Invgc: Robust cross-modal retrieval by inverse graph convolution",
            "year": 2023
        },
        {
            "authors": [
                "Weike Jin",
                "Zhou Zhao",
                "Pengcheng Zhang",
                "Jieming Zhu",
                "Xiuqiang He",
                "Yueting Zhuang."
            ],
            "title": "Hierarchical cross-modal graph consistency learning for video-text retrieval",
            "venue": "SIGIR \u201921: The 44th International ACM SIGIR Conference on Research and",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Wonhark Park",
                "Dongkwan Lee",
                "Nojun Kwak",
                "Yujin Wang",
                "Yimu Wang",
                "Tiancheng Gu",
                "Xingchang Lv",
                "Mingmao Sun"
            ],
            "title": "Nice: Cvpr 2023 challenge on zero-shot image captioning",
            "year": 2023
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L. Berg",
                "Mohit Bansal",
                "Jingjing Liu."
            ],
            "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,",
            "year": 2021
        },
        {
            "authors": [
                "Linjie Li",
                "Yen-Chun Chen",
                "Yu Cheng",
                "Zhe Gan",
                "Licheng Yu",
                "Jingjing Liu."
            ],
            "title": "HERO: Hierarchical encoder for Video+Language omni-representation pre-training",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Oscar: Object-semantics aligned pretraining for vision-language tasks",
            "venue": "Computer Vi-",
            "year": 2020
        },
        {
            "authors": [
                "Weixin Liang",
                "Yuhui Zhang",
                "Yongchan Kwon",
                "Serena Yeung",
                "James Zou."
            ],
            "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
            "venue": "Advances in neural information processing systems.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Liu",
                "SouYoung Jin",
                "Cheng-I Lai",
                "Andrew Rouditchenko",
                "Aude Oliva",
                "James Glass."
            ],
            "title": "Cross-modal discrete representation learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Yuqi Liu",
                "Pengfei Xiong",
                "Luhui Xu",
                "Shengming Cao",
                "Qin Jin."
            ],
            "title": "Ts2-net: Token shift and selection transformer for text-video retrieval",
            "venue": "Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "SGDR: stochastic gradient descent with warm restarts",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Ming Zhong",
                "Yang Chen",
                "Wen Lei",
                "Nan Duan",
                "Tianrui Li."
            ],
            "title": "Clip4clip: An empirical study of CLIP for end to end video clip retrieval and captioning",
            "venue": "Neurocomputing, 508:293\u2013 304.",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Ma",
                "Guohai Xu",
                "Xiaoshuai Sun",
                "Ming Yan",
                "Ji Zhang",
                "Rongrong Ji."
            ],
            "title": "X-CLIP: end-toend multi-grained contrastive learning for video-text retrieval",
            "venue": "MM \u201922: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, Octo-",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Ma",
                "Guohai Xu",
                "Xiaoshuai Sun",
                "Ming Yan",
                "Ji Zhang",
                "Rongrong Ji."
            ],
            "title": "X-CLIP: End-toend multi-grained contrastive learning for video-text retrieval",
            "venue": "ACM international conference on multimedia, MM \u201922, pages 638\u2013647, New York, NY,",
            "year": 2022
        },
        {
            "authors": [
                "Antoine Miech",
                "Jean-Baptiste Alayrac",
                "Lucas Smaira",
                "Ivan Laptev",
                "Josef Sivic",
                "Andrew Zisserman."
            ],
            "title": "End-to-end learning of visual representations from uncurated instructional videos",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic."
            ],
            "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "2019 IEEE/CVF International Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Shumpei Miyawaki",
                "Taku Hasegawa",
                "Kyosuke Nishida",
                "Takuma Kato",
                "Jun Suzuki."
            ],
            "title": "Scene-text aware image and text retrieval with dual-encoder",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student",
            "year": 2022
        },
        {
            "authors": [
                "Jae Sung Park",
                "Sheng Shen",
                "Ali Farhadi",
                "Trevor Darrell",
                "Yejin Choi",
                "Anna Rohrbach."
            ],
            "title": "Exposing the limits of video-text models through contrast sets",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Yookoon Park",
                "Mahmoud Azab",
                "Seungwhan Moon",
                "Bo Xiong",
                "Florian Metze",
                "Gourab Kundu",
                "Kirmani Ahmed."
            ],
            "title": "Normalized contrastive learning for text-video retrieval",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Milos Radovanovic",
                "Alexandros Nanopoulos",
                "Mirjana Ivanovic."
            ],
            "title": "Hubs in space: Popular nearest neighbors in high-dimensional data",
            "venue": "J. Mach. Learn. Res., 11:2487\u20132531.",
            "year": 2010
        },
        {
            "authors": [
                "Nina Shvetsova",
                "Brian Chen",
                "Andrew Rouditchenko",
                "Samuel Thomas",
                "Brian Kingsbury",
                "Rogerio Feris",
                "David Harwath",
                "James Glass",
                "Hilde Kuehne."
            ],
            "title": "Everything at Once \u2013 Multi-modal Fusion Transformer for Video Retrieval",
            "venue": "2022 IEEE/CVF",
            "year": 2022
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Ronghang Hu",
                "Vedanuj Goswami",
                "Guillaume Couairon",
                "Wojciech Galuba",
                "Marcus Rohrbach",
                "Douwe Kiela."
            ],
            "title": "FLAVA: A foundational language and vision alignment model",
            "venue": "IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Alex Jinpeng Wang",
                "Yixiao Ge",
                "Guanyu Cai",
                "Rui Yan",
                "Xudong Lin",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou."
            ],
            "title": "Object-aware Videolanguage Pre-training for Retrieval",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Haoran Wang",
                "Di Xu",
                "Dongliang He",
                "Fu Li",
                "Zhong Ji",
                "Jungong Han",
                "Errui Ding."
            ],
            "title": "Boosting video-text retrieval with explicit high-level semantics",
            "venue": "MM \u201922: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohan Wang",
                "Linchao Zhu",
                "Zhedong Zheng",
                "Mingliang Xu",
                "Yi Yang."
            ],
            "title": "Align and tell: Boosting text-video retrieval with local alignment and fine-grained supervision",
            "venue": "IEEE Transactions on Multimedia, pages 1\u201311.",
            "year": 2022
        },
        {
            "authors": [
                "Yimu Wang",
                "Shiyin Lu",
                "Lijun Zhang."
            ],
            "title": "Searching privately by imperceptible lying: A novel private hashing method with differential privacy",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, page 2700\u20132709.",
            "year": 2020
        },
        {
            "authors": [
                "Yimu Wang",
                "Ren-Jie Song",
                "Xiu-Shen Wei",
                "Lijun Zhang."
            ],
            "title": "An adversarial domain adaptation network for cross-domain fine-grained recognition",
            "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1217\u20131225.",
            "year": 2020
        },
        {
            "authors": [
                "Yimu Wang",
                "Xiu-Shen Wei",
                "Bo Xue",
                "Lijun Zhang."
            ],
            "title": "Piecewise hashing: A deep hashing method for large-scale fine-grained search",
            "venue": "Pattern Recognition and Computer Vision - Third Chinese Conference, PRCV 2020, Nanjing, China, October 16-18,",
            "year": 2020
        },
        {
            "authors": [
                "Yimu Wang",
                "Bo Xue",
                "Quan Cheng",
                "Yuhui Chen",
                "Lijun Zhang."
            ],
            "title": "Deep unified cross-modality hashing by pairwise data alignment",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Zijia Lin",
                "Zhongyuan Wang",
                "Jizhong Han",
                "Songlin Hu."
            ],
            "title": "RaP: Redundancy-aware video-language pre-training for text-video retrieval",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Hu Xu",
                "Gargi Ghosh",
                "Po-Yao Huang",
                "Prahal Arora",
                "Masoumeh Aminzadeh",
                "Christoph Feichtenhofer",
                "Florian Metze",
                "Luke Zettlemoyer."
            ],
            "title": "VLM: Task-agnostic video-language model pre-training for video understanding",
            "venue": "Findings of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Hu Xu",
                "Gargi Ghosh",
                "Po-Yao Huang",
                "Dmytro Okhonko",
                "Armen Aghajanyan",
                "Florian Metze",
                "Luke Zettlemoyer",
                "Christoph Feichtenhofer."
            ],
            "title": "VideoCLIP: Contrastive pre-training for zero-shot videotext understanding",
            "venue": "Proceedings of the 2021 Con-",
            "year": 2021
        },
        {
            "authors": [
                "Jun Xu",
                "Tao Mei",
                "Ting Yao",
                "Yong Rui."
            ],
            "title": "MSRVTT: A large video description dataset for bridging video and language",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages",
            "year": 2016
        },
        {
            "authors": [
                "Qiying Yu",
                "Yang Liu",
                "Yimu Wang",
                "Ke Xu",
                "Jingjing Liu."
            ],
            "title": "Multimodal federated learning via contrastive representation ensemble",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Youngjae Yu",
                "Jongseok Kim",
                "Gunhee Kim."
            ],
            "title": "A joint sequence fusion model for video question answering and retrieval",
            "venue": "Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII, volume",
            "year": 2018
        },
        {
            "authors": [
                "Youngjae Yu",
                "Hyungjin Ko",
                "Jongwook Choi",
                "Gunhee Kim."
            ],
            "title": "End-to-end concept word detection for video captioning, retrieval, and question answering",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI,",
            "year": 2017
        },
        {
            "authors": [
                "Shuai Zhao",
                "Linchao Zhu",
                "Xiaohan Wang",
                "Yi Yang."
            ],
            "title": "Centerclip: Token clustering for efficient textvideo retrieval",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201922,",
            "year": 2022
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Xiaopeng Lu",
                "Kyusong Lee."
            ],
            "title": "SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Linchao Zhu",
                "Yi Yang."
            ],
            "title": "Actbert: Learning global-local video-text representations",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8743\u20138752. Computer Vi-",
            "year": 2020
        },
        {
            "authors": [
                "Niebles"
            ],
            "title": "Youtube videos with 100,000 densely annotated descriptions. For a fair comparison, following the previous setting (Luo et al., 2022",
            "venue": "Gabeur et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Luo"
            ],
            "title": "2022a), we use a standard vision transformer (Dosovitskiy et al., 2021) with 12 layers which are initialized with the public CLIP (Radford",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As a fundamental task in visual-language understanding (Wang et al., 2020b; Xu et al., 2021b; Park et al., 2022a; Miyawaki et al., 2022; Fang et al., 2023a,b; Kim et al., 2023; Jian and Wang, 2023), video-text retrieval (VTR) (Luo et al., 2022; Gao et al., 2021b; Ma et al., 2022a; Liu et al., 2022a; Zhao et al., 2022; Gorti et al., 2022; Fang et al., 2022) has attracted interest from academia and industry. Although recent years have witnessed the rapid development of VTR with the support from powerful pretraining models (Luo et al., 2022; Gao et al., 2021b; Ma et al., 2022a; Liu et al., 2022a), improved retrieval methods (Bertasius et al., 2021; Dong et al., 2019; Jin et al., 2021), and videolanguage datasets construction (Xu et al., 2016), it remains challenging to precisely match video and language due to the raw data being in heterogeneous spaces with significant differences.\nCurrent VTR research (Luo et al., 2022; Ma et al., 2022a; Liu et al., 2022b) mainly aims to learn a joint feature space across modalities and then compares representations in this space. However, with the huge discrepancy between different modalities and the design of modality-independent encoders, it is challenging to directly compare and calculate the similarities between representations of different modalities generated from different encoders (Liang et al., 2022). To alleviate the mismatch caused by heterogeneous encoders and data formats, Liu et al. (2022a); Cao et al. (2022) proposed to align different modalities in a common space without supervision from text or video. However, because of the unsupervised design, the shared spaces are either randomly initialized or updated in an unsupervised fashion, which blocks the power of that aligned space. We argue that learning\na shared aligned space with supervision is a promising way to improve video-text retrieval. Borrowing from text retrieval (Karpukhin et al., 2020; Zhao et al., 2021; Gao et al., 2021a), we represent the aligned space and the space containing representations generated by modality-dependent encoders as sparse and dense spaces, respectively, as the aligned space typically carries specific semantics.\nIn this work, we propose a Supervised Shared Sparse Multi-grained Alignment framework for VTR, namely S3MA, in which the aligned sparse space is updated under the supervision of the videotext data at hand. Specifically, we initialize a finite number of sparse concepts by clustering a large number of basic concepts (words) to form the finegrained aligned sparse space. In return, each sparse concept is composed of several words, which improves the interpretability of our model. Then, we match the sparse text and video representations effectively by projecting the video representation generated by the video encoder to this fine-grained sparse space. The sparse sentence (text) representations can be obtained by looking up the sparse concepts. To obtain sparse video representations, we first calculate the cosine similarity between the video representations and the sparse concepts. Next, by summing up all the sparse concepts with the weight of the cosine similarity between video representation and sparse concepts, we obtain the sparse video representations. Furthermore, to better match these two sparse representations, we design two loss functions to update sparse concepts, pushing the sparse representations of text and video as close as possible in the shared sparse space. This shared sparse space design not only improves the performance on VTR, but also allows us to interpret what the models have learned. The sparse aligned space, as shown in Figure 5, enables the model to accurately capture the key concepts, resulting in improved alignment within the sparse space.\nRecently, Ma et al. (2022a) demonstrated that incorporating fine-grained video representations (such as frame or segment representations) with high-level video features can further improve retrieval performance. Inspired by their work, we further project frame representations into our designed aligned sparse space. Compared to highlevel video representations, frame representations can be mapped to more detailed concepts, which enriches the overall video representations. In this way, we have fine-grained (frame) and coarse-\ngrained (video and sentence) representations from the sparse space and the dense space, enabling us to calculate multi-space multi-grained similarity for exploring the potential of supervised sparse space.\nFinally, to evaluate the effectiveness of our proposed S3MA, we conducted experiments on three video-text benchmarks (Chen and Dolan, 2011; Fabian Caba Heilbron and Niebles, 2015; Xu et al., 2016). Benefiting from multi-grained and multispace similarity, our proposed S3MA outperforms previous methods on all the benchmarks without requiring any additional data during training.\nIn summary, our contributions are as follows1:\n\u2022 We propose the shared sparse space to alleviate the problem of mismatched representations from different modalities, which arises from the raw data being in heterogeneous spaces and the heterogeneous design of modality-dependent encoders.\n\u2022 Our proposed S3MA achieves SOTA performance on several metrics across three VTR benchmarks.\n\u2022 Detailed analysis reveals the importance of shared sparse space and multi-grained similarity. Besides, we demonstrate that the design of shared sparse space and multi-grained similarity significantly impacts retrieval performance."
        },
        {
            "heading": "2 Related Works",
            "text": "Video-Text Retrieval (VTR), which involves crossmodal alignment and abstract understanding of temporal images (videos), has been a popular and fundamental task of language-grounding problems (Wang et al., 2020a,c, 2021; Yu et al., 2023). Most existing conventional video-text retrieval frameworks (Yu et al., 2017; Dong et al., 2019; Zhu and Yang, 2020; Miech et al., 2020; Gabeur et al., 2020; Dzabraev et al., 2021; Croitoru et al., 2021) focus on learning powerful representations for video and text and extracting separated representations. Inspired by the success of self-supervised pretraining methods (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and visionlanguage pretraining (Li et al., 2020b; Gan et al., 2020; Singh et al., 2022) on large-scale unlabeled cross-modal data, recent works (Lei et al., 2021; Cheng et al., 2021; Gao et al., 2021b; Ma et al.,\n1The code is released at link.\n2022a; Park et al., 2022a; Wang et al., 2022b,c; Zhao et al., 2022; Gorti et al., 2022) have attempted to pretrain or fine-tune video-text retrieval models in an end-to-end manner. Frozen in time (Bain et al., 2021) uses end-to-end training on both imagetext and video-text pairs data by uniformly sampling video frames. CLIP4Clip (Luo et al., 2022) finetunes models and investigates three similarity calculation approaches for video-sentence contrastive learning on CLIP (Radford et al., 2021). Later, to enable unsupervised sparse learning in VTR, DiscretCodebook (Liu et al., 2022a) aligns modalities in a shared space filled with concepts, which are randomly initialized and unsupervisedly updated, while VCM (Cao et al., 2022) constructs a sparse space with unsupervisedly clustered visual concepts. At the same time, OA-Trans (Wang et al., 2022a) and TABLE (Chen et al., 2023) both employ a small number of semantic tags as the input to the text encoder to improve alignment between modalities.\nHowever, due to the unsupervised design, concepts in DiscretCodebook and VCM are either randomly initialized or updated unsupervisedly, which limits the potential of aligned sparse space. On the other hand, OA-Trans and TABLE only employ a limited number of concepts to serve as the input of the text encoder to encourage alignment. Meanwhile, these methods only perform the coarse-grained video-text similarity, lacking the fine-grained contrast between different modalities. In comparison, our proposed S3MA learn the aligned sparse space containing a large number of words in a supervised manner, under the supervision of text, and calculate frame-sentence similarity\nfor multi-space multi-grained alignment."
        },
        {
            "heading": "3 Methods",
            "text": "In this section, we introduce our proposed framework for video-text retrieval, which aligns language and video in a shared sparse space. Typically, in video-text retrieval, we have a set of examples {(vi, ti)}i\u2208[N ], where N is the number of examples that are of video and language."
        },
        {
            "heading": "3.1 General Video-Text Retrieval Paradigm",
            "text": "In this part, we present a general video-text retrieval framework widely used by previous methods (Luo et al., 2022; Liu et al., 2022a). With this paradigm, we can obtain three representations for different modalities from the dense space, i.e., frame representation rf , video representation rv, and sentence representation rs by modality-dependent encoders.\nFrame and video representations: Given a video v, several video frames are first sampled as the inputs of the frame encoder to obtain the frame features rf \u2208 Rntframe\u00d7d, where nframe is the number of frames and d is the dimension of features. As the frame representations rf are extracted through sampling, to explore the temporal correlation among different frames, we employ a temporal encoder to aggregate frame representations. With the temporal encoder and the frame representations rf , we obtain the video representations rv \u2208 R1\u00d7d.\nSentence representation: Given a sentence t, we use a text encoder to obtain the text representation rs \u2208 R1\u00d7d."
        },
        {
            "heading": "3.2 Fine-Grained Aligned Sparse Space",
            "text": "The key to the video-text retrieval task is to precisely align representations from different modalities. However, due to the heterogeneous encoder architectures and data formats of different modalities, it is difficult to align directly (Liang et al., 2022). Therefore, instead of directly enforcing the representations to be aligned, we propose aligning them in an aligned sparse constructed by nc sparse concepts C \u2208 Rnc\u00d7d. Each sparse concept c represents several basic concepts (words). Moreover, to supervise the updates of sparse concepts, we utilize the human-annotated knowledge at hand, i.e., text annotations in the paired video-text data.\nInitialization. First, we map all the words into embeddings by the embedding layer femb of the text encoder. But as the number of words is relatively large (for example, in Clip (Radford et al., 2021), the number of sub-words is approximately 30k), we cluster embeddings into nc clusters using KNN (Gianfelici, 2008) to form the sparse concepts C and represent all the words by their cluster\u2019s centers c. Consequently, each sparse concept c represents a bunch of words that are similar on the embedding space, enabling fine-grained alignment. The mapping from words to sparse concepts is denoted by hw2c \u2208 [nwords] \u2192 {0, 1}nc\u00d71. Now, nc sparse concepts have been initialized.\nObtaining the sparse sentence representation. For text, as the caption is at hand, we can directly tokenize the sentences into words and look up the corresponding sparse concepts in C. The sparse sentence representation rsc \u2208 R1\u00d7d is obtained by averaging all the representations of concepts that are fetched with the surface form of the sentence, as follows,\nrsc = sim t\u22a4C/|t| , (1)\nwhere |t| is the number of words in t and simt =\u2211 w\u2208t hw2c(w) is a vector with the length of nc. Obtaining the sparse video representation. We first calculate the cosine similarity simv \u2208 R1\u00d7nc between the video representations and sparse concepts C as simvj = cos(r\nv, cj), \u2200j \u2208 [nc], where simvj is the j-th element of sim\nv and cos(\u00b7, \u00b7) is the cosine similarity. Next, sparse video representations are obtained by weighted summing the sparse concepts as,\nrvc = sim vC/\u2225simv\u22251 . (2)\nObtaining the sparse frame representation. Similarly, the cosine similarity simf \u2208Rnframe\u00d7nc between the frame representations and sparse concepts is calculated as simfi,j = cos(r f i , cj),\u2200i \u2208 [nframe],\u2200j \u2208 [nc], where simfi,j is the (i, j)-th element of simf and rfi is the i-th row of r\nf . Next, sparse frame representations are obtained as,\nrfc = \u2211\ni\u2208[nframe]\nsimfi C/\u2225sim f i \u22251 . (3)\nFinally, we have the sparse frame, video, and sentence representations rfc \u2208 Rnframe\u00d7d, rvc \u2208 R1\u00d7d, rsc \u2208 R1\u00d7d with the frame and video sparse space similarity simf \u2208 Rnframe\u00d7nc and simv \u2208 Rnc along with the sentence sparse space similarity (supervision) simt."
        },
        {
            "heading": "3.3 Multi-Space Multi-Grained Similarity",
            "text": "In this part, we will demonstrate our method for calculating the similarities between data from two different modalities, as shown in Figure 3, including the similarities in the dense space and in shared sparse space, inspired by Ma et al. (2022a). We can now compute multi-space (sparse and dense spaces) multi-grained (fine-grained and coarse-grained) similarity for precise alignment."
        },
        {
            "heading": "3.3.1 Dense Space Similarity",
            "text": "Video-Sentence similarity Srv\u2212rs . To obtain a fine-grained similarity, we use a learnable matrix Arv\u2212rs \u2208 Rd\u00d7d to focus on the discriminative features of video and sentence representations as,\nSrv\u2212rs = r vArv\u2212rsr s\u22a4 .\nFrame-Sentence similarity Srf\u2212rs . To obtain a fine-grained similarity, we first calculate an instance-aware weight using the softmax function applied to the dot product of rsrf\u22a4, and then use a learnable matrix Arf\u2212rs \u2208 Rnframe\u00d7nframe to focus on discriminative frames. In this way, the similarity is calculated as,\nSrf\u2212rs = softmax(r srf\u22a4)Arf\u2212rsr frs\u22a4 ."
        },
        {
            "heading": "3.3.2 Sparse Space Similarity",
            "text": "Video-Sentence shared sparse space similarity Srvc\u2212rsc . Similarly, to obtain a fine-grained similarity on the shared sparse space, we use a learnable matrix Arvc\u2212rsc \u2208 R\nd\u00d7d to focus on the discriminative features of sparse video and sentence representations. Now, the similarity is calculated as,\nSrvc\u2212rsc = r v cArvc\u2212rscr s\u22a4 c .\nFrame-Sentence shared sparse space similarity S\nrfc\u2212rsc . With instance-aware weights\nsoftmax(rscr f\u22a4 c ) and a learnable matrix Arfc\u2212rsc \u2208 Rnframe\u00d7nframe , we get the similarity between the sparse frame and sentence representations as,\nS rfc\u2212rsc = softmax(rscr f\u22a4 c )Arfc\u2212rsc rfc r s\u22a4 c ."
        },
        {
            "heading": "3.3.3 Overall Similarity",
            "text": "The overall video-text similarity is defined as,\nS = Srf\u2212rs + Srv\u2212rs + Srfc\u2212rsc + Srvc\u2212rsc\n4 ."
        },
        {
            "heading": "3.4 Objective",
            "text": "The objective consists of three different losses. The first component is contrastive loss. Following Clip4Clip (Luo et al., 2022), we employ the symmetric InfoNCE loss over the similarity matrix to optimize the retrieval model as,\n\u2113sim =\u2113v2t + \u2113t2v\n=\u2212 1 N \u2211 i\u2208[N ] log exp(Si,i)\u2211 j\u2208[N ] exp(Si,j)\n\u2212 1 N \u2211 i\u2208[N ] log exp(Si,i)\u2211 j\u2208[N ] exp(Sj,i) ,\nwhere Si,j is similarity between i-th video and j-th text and N is the number of paired data.\nThe second loss we minimize is the alignment loss, which matches the sparse frame and video\nrepresentations (rfc and rvc ) with the sparse sentence representations rsc in the \u21132 distance, as,\n\u2113align = 1\nN \u2211 i\u2208[N ] (\u2225rvc \u2212 rsc\u22252\n+ \u2225\u2225\u2225\u2225\u2225 1rfcnframe \u2212 rsc \u2225\u2225\u2225\u2225\u2225 2 ) ,\nwhere 1 is the vector only containing 1. In addition, to match the frame and video representations with the corresponding sparse concepts, we minimize the sparse similarity loss as,\n\u2113sparse = 1\nN \u2211 i\u2208[N ] (\u2225\u2225simv \u2212 simt\u2225\u2225 2\n+ \u2225\u2225\u2225\u2225 1simfnframe \u2212 simt \u2225\u2225\u2225\u2225 2 ) ,\nThe overall objective is the linear combination of the above three losses as,\n\u2113 = \u2113sim + \u03b1\u2113align + \u03b2\u2113sparse ,\nwhere \u03b1 and \u03b2 are hyperparameters controlling the trade-off between three losses. We set \u03b1 = 0.02 and \u03b2 = 0.01 for all the experiments."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Baselines",
            "text": "To show the empirical efficiency of our S3MA, we train it on MSR-VTT (Xu et al., 2016), MSVD (Chen and Dolan, 2011), and ActivityNet (Fabian Caba Heilbron and Niebles, 2015). We compare with VLM (Xu et al., 2021a), HERO (Li et al., 2020a), VideoCLIP (Xu et al., 2021b), EvO (Shvetsova et al., 2022), OATrans (Wang et al., 2022a), RaP (Wu et al., 2022), LiteVL (Chen et al., 2022), NCL (Park et al., 2022b), TABLE (Chen et al., 2023), VOP (Huang et al., 2023), Clip4Clip (Luo et al., 2022), XCLIP (Ma et al., 2022a), DiscreteCodebook (Liu et al., 2022a), TS2-Net (Liu et al., 2022b), VCM (Cao et al., 2022), HiSE (Wang et al., 2022b), Align&Tell (Wang et al., 2022c), CenterCLIP (Zhao et al., 2022), and X-Pool (Gorti et al., 2022). Implementation details and evaluation protocols are deferred to the Appendix."
        },
        {
            "heading": "4.2 Quantitative Results",
            "text": "MSR-VTT. As shown in Table 1, S3MA achieves the best R@1 on the text-to-video retrieval results\nMethods Venue Text-to-Video Retrieval\nR@1\u2191 R@5\u2191 R@10\u2191 MnR\u2193\nMSVD\nusing ViT-B/32 and ViT-B/16, outperforming the second-best method by 2.1 and 0.4, respectively.\nThe performance of S3MA on the video-to-text retrieval task is also comparable with previous methods, achieving the best and second-best results on R@1 and R@5 using ViT-B/32. Moreover, we notice that only 1 previous method using ViT-B/16 outperforms S3MA with ViT-B/32 on the text-tovideo retrieval, demonstrating the effectiveness of S3MA. Compared to DiscreteCodebook (Liu et al., 2022a), which aligns modalities in an unsupervised manner, S3MA outperforms DiscreteCodebook on every metric. Meanwhile, S3MA also outperforms VCM (Cao et al., 2022), which constructs an aligned space with unsupervisedly clustered visual concepts, demonstrating the importance of supervising alignment in the sparse space. This suggests that aligning modalities with fine-grained supervision is a promising approach to improving video-to-text retrieval performance.\nMSVD and ActivityNet. The results on MSVD\nand ActicityNet are shown in Table 2. S3MA achieves the best R@1 on text-to-video retrieval on two datasets compared to the previous methods. Besides, with the shared sparse space and multi-grained alignment, S3MA also has the lowest MnR."
        },
        {
            "heading": "4.3 Ablation Studies",
            "text": "In this part, we present a series of ablation experiments on MSR-VTT to demonstrate the effectiveness of different components of S3MA. The evaluation of two proposed losses, similarity calculation, and the importance of word-level features are deferred to the Appendix."
        },
        {
            "heading": "4.3.1 Efficiency of Sparse Space",
            "text": "The choice of different initialization of sparse spaces. To choose the best initialization method for the sparse space, we conduct experiments using two different initializations, i.e., the embedding and semantic embedding spaces, as shown in Table 3. The embedding space is the one we use in S3MA,\nwhile the semantic embedding space, is initialized by outputs of the last layer in the text encoder, with input consisting of a word and two [SEP] tokens. By replacing the embedding initialization with the semantic embedding, the retrieval performance of S3MA decreases, proving the superiority of embedding space over the semantic embedding space. Size of sparse space. Another important factor to consider is the size of the sparse space. When we have unlimited data to train models, a large sparse space is ideal. However, when the data is limited, a large sparse space can lead to sparse gradients, resulting in most of the concepts not being able to be updated, while a small sparse space will restrict the retrieval ability as it becomes more challenging to distinguish between numerous data points. The results of these experiments can be found in Table 5. We see that halving and doubling the size of the sparse space slightly decreases performance. Impact of clustering. As S3MA clusters all the embeddings to initialize concept clusters, it is uncertain whether clustering will hinder the power of the shared sparse space. Clustering can be useful to extract high-level abstract concepts and reduce noise. However, it may also lead to a loss of information, which is important for fine-grained alignment. Specifically, we compare the performance of S3MA to that of a modified version, S3MA w/o clustering concepts, which directly uses over 30k basic concepts to form the shared sparse space. Quantitative results can be found in Table 4. The results show that without clustering, R@5, R@10, and MnR on text-to-video retrieval and R@10 and MnR on video-to-text retrieval are improved. On one hand, similar basic concepts can be better separated, which leads to more precise alignment. On the other hand, that may lead to\nsparse gradients, resulting in some concepts not being fully updated while others are over-updated. This might cause some concepts to be under or over-represented, which might negatively impact the performance (Radovanovic et al., 2010). Therefore, it\u2019s important to find the balance in clustering to achieve the best performance."
        },
        {
            "heading": "4.3.2 Efficiency of Multi-Grained Similarities",
            "text": "In order to fully evaluate the impact of multigrained similarities, we compare different variants of S3MA and the results are shown in Table 6. From these results, we can draw three conclusions,\n\u2022 Multi-grained similarities are crucial for retrieval. Using both coarse- and fine-grained alignments in the dense space improved R@1 from 42.8 and 41.9 to 44.0 and 43.6 on textto-video and video-to-text retrieval compared with only using coarse-grained alignment in the dense space, respectively. The same observation can be observed in the sparse space.\n\u2022 Sparse space plays a crucial role in improving the alignment of modalities. We observe that incorporating coarse-grained in the dense and sparse spaces improves R@1 for text-to-video\nretrieval from 42.8 to 43.3 compared to only performing coarse-grained similarity in the dense space, respectively.\n\u2022 Using multi-space and multi-grained similarities simultaneously achieves the best performance. R@1 on text-to-video and video-totext retrieval is significantly improved from 42.8 and 41.9 to 49.1 and 46.9, respectively."
        },
        {
            "heading": "4.3.3 Temporal Encoder and Larger Model",
            "text": "We also investigate the effect of the temporal encoder (TE, a small sequence transformer) and different base models. The results are shown in Table 7. S3MA with TE outperforms S3MA without TE, because it is able to better model the temporal relation among different frames in a video. Besides, using a larger base model, such as ViT-B/16, further improves the performance of S3MA, as a larger base model typically has better representation learning abilities benefiting this retrieval task as well. Similar conclusions can be found in previous works (Luo et al., 2022; Ma et al., 2022a)."
        },
        {
            "heading": "4.4 Qualitative Results",
            "text": "To qualitatively validate the effectiveness of S3MA and the alignment in the sparse space, we present examples of video-to-text and text-to-video retrieval on MSR-VTT in Figures 4, 6 and 7, and the alignment in sparse space in Figure 5, respectively. The retrieval results show the satisfactory performance of S3MA, benefiting from multi-space multi-grained similarity. Notably, S3MA demonstrates precise identification of the color (green), objects (bicycle), and humans (a man), indicating its proficiency in capturing intricate details. In Fig-\nure 5, we notice that, the video and frame features are perfectly aligned with the corresponding sparse concepts as exhibiting high similarities."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, to better align video and text modalities, we proposed a multi-space, multi-grained video-text retrieval framework, S3MA. Specifically, S3MA aligned different modalities in a finegrained shared sparse space, which is initialized with a finite number of concept clusters consisting of a number of basic concepts (words) and updated in a supervised fashion with the guide of text. Besides, S3MA employed frame (finegrained) and video (coarse-grained) features to encourage models to perform multi-grained similarity alignment. Finally, we conducted extensive experiments on three representative video-text retrieval benchmarks, showing the superiority of S3MA.\nLimitations\nIn the future, it would be promising to seek more fine-grained alignment, such as instance (object)level or word-level alignment, for aligning different modalities. Moreover, our experiment focused solely on the application of sparse retrieval in videotext retrieval. It would be great to see whether sparse retrieval can help other cross-modal retrieval tasks, e.g., audio-text, image-text, audio-video, and audio-image retrieval. Additionally, incorporating more detailed information such as the relationship between different objects and frames would be beneficial for the video-text retrieval problem.\nRegarding the sparse space, we notice that some sparse concepts are retrieved a lot during the training procedure which might lead to the emergence of hubness (Radovanovic et al., 2010). Investigating improved clustering methods to mitigate hubness would be an interesting direction for future research. That might be due to the KNN clustering strategy and in the future and introducing better clustering strategies might be able to reduce the hubness issue, such as weighted KNN, semanticbased KNN, or part-of-speech tagging-based KNN."
        },
        {
            "heading": "A Experiments",
            "text": "A.1 Datasets Details MSR-VTT (Xu et al., 2016) contains 10,000 videos with length varying from 10 to 32 seconds, each paired with about 20 human-labeled captions. Following the evaluation protocol from previous works Yu et al. (2018); Miech et al. (2019), we use the training-9k / test 1k-A splits for training and testing respectively.\nMSVD (Chen and Dolan, 2011) contains 1,970 videos with a split of 1200, 100, and 670 as the train, validation, and test set, respectively. The duration of videos varies from 1 to 62 seconds. Each video is paired with 40 English captions.\nActivityNet (Fabian Caba Heilbron and Niebles, 2015) is consisted of 20,000 Youtube videos with 100,000 densely annotated descriptions. For a fair comparison, following the previous setting (Luo et al., 2022; Gabeur et al., 2020), we concatenate all captions together as a paragraph to perform a video-paragraph retrieval task by concatenating all the descriptions of a video. Performances are reported on the \u201cval1\u201d split of the ActivityNet.\nA.2 Implementation Details and Evaluation Protocols\nFollowing Luo et al. (2022); Ma et al. (2022a), we use a standard vision transformer (Dosovitskiy et al., 2021) with 12 layers which are initialized with the public CLIP (Radford et al., 2021) checkpoints. We directly use the text encoder of CLIP as our text encoder which is also initialized with the public CLIP checkpoints.\nWe set the query, key, and value projection dimension size as 512 to match CLIP\u2019s output dimension and we initialize our logit scaling parameter \u03bb with the value from the pre-trained CLIP model. All models are optimized for 5 epochs on MSR-VTT and MSVD, and for ActivityNet, the models are trained for 20 epochs. We use AdamW (Loshchilov and Hutter, 2019) with a weight decay of 0.2 and decay the learning rate using a cosine schedule (Loshchilov and Hutter, 2017), following the method used in CLIP (Radford et al., 2021). For all experiments, we uniformly sample 12 frames from every video, resizing each frame to 224x224 as per previous works (Luo et al., 2022; Ma et al., 2022a). we set ncodes = 1024 following DiscreteCodebook (Liu et al., 2022a). To evaluate the retrieval performance of our proposed model, we use recall at Rank K\n(R@K, higher is better), median rank (MdR, lower is better), and mean rank (MnR, lower is better) as retrieval metrics, which are widely used in previous retrieval works (Radford et al., 2021; Luo et al., 2022; Ma et al., 2022a).\nA.3 Ablation Studies Evaluating the calculation of similarity between video and frame representations and cluster concepts in S3MA. In S3MA, we use cosine similarity to calculate simf and simv. Another way of calculating simf and simv might be using multilabel classification. To compare the effect of multilabel classification and cosine similarity, we conduct experiments using two multi-layer perceptrons (MLPs) with two layers and the ReLU activation to predict the similarity between video and frame representations and cluster concepts. Two MLPs are also trainable. Quantitative results are shown in Table 8. Our quantitative results, shown in Table 8, indicate that the use of MLPs decreases R@1 on text-to-video and video-to-text retrieval. This suggests that cosine similarity is more suitable for VTR. Evaluating the importance of supervised alignment in S3MA. In S3MA, the aligned sentence representation rsc is obtained from the text as in Eq. (1). This process aligns the sentence representation based on the instruction of the text. By doing so, the aligned sentence representation rsc can serve as the supervision (an anchor) for aligning video and frame features, providing a reference point for the alignment of different modalities. To investigate the importance of placing an anchor rsc for better alignment, we compare it to obtaining aligned sentence representation through the similarity between concept clusters C and sentence feature rt. This alternative approach allows us to evaluate the effectiveness of using an anchor for alignment and to understand how it improves the performance of the model. To investigate the alternative approach of obtaining aligned sentence representation without an anchor, we calculate the sentence sparse space similarity simt \u2208 R1\u00d7nc by calculating the cosine similarity between sentence representations and concepts as simtj = cos(r\ns, Cj), where simtj is the j-th element of simt, Cj is the j-th row of C, and cos is the cosine similarity. The aligned sentence representation rt without the instruction of text is obtained by matrix multiplication as follows:\nrt = simtC/\u2225simt\u22251, (4)\nwhere simt is the similarity between sentence representations and concepts. The results of this comparison can be found in Table 9. The experimental results show that with the \u201canchor\u201d, S3MA can better align different modalities as R@1, R@5, and R@10 on text-to-video retrieval and R@1 on video-\nto-text retrieval have greatly improved, indicating that the supervised (anchor-based) alignment is crucial for better performance of the model.\nEffect of losses and hyperparameter sensitivity. To further demonstrate the effectiveness of the\ntwo proposed losses designed for aligning different modalities in the shared sparse space, we conduct experiments to compare the performance of these losses. The quantitative results of these experiments are shown in Table 10. The results indicate that adding both losses simultaneously achieves the best performance on the MSR-VTT dataset. When using only one loss, the performance on text-tovideo retrieval is comparable to the method without using both losses on text-to-video retrieval, but outperforms the method without the two losses on video-to-text retrieval. Specifically, when using two losses, R@1 on text-to-video retrieval and video-to-text retrieval is improved by 1.1 and 1.5, respectively. Additionally, all the other metrics, such as R@5 and R@10, are also improved, demon-\nstrating the power of the two proposed losses in aligning different modalities in the shared sparse space. To gain a better understanding of the sensitivity of S3MA with respect to the two hyperparameters, \u03b1 and \u03b2, we conduct a series of experiments with different settings of \u03b1 and \u03b2 as shown in Table 10. The results of these experiments demonstrate that, even with varying settings of \u03b1 and \u03b2, the video-text retrieval performance remains consistent, indicating that the model is robust and not highly sensitive to these hyperparameters. This suggests that S3MA is able to achieve good performance across a wide range of settings for these hyperparameters, making it easy to adjust and optimize for specific use cases. Additionally, this also suggests that S3MA is not overly dependent on precise values of these hyperparameters, and is instead able to leverage the more important underlying features and patterns in the data.\nAre word-level features necessary? To investigate the necessity of word-level features, we introduce word-level dense and sparse representations, along with word-frame and word-video similarities, into the dense and sparse spaces. The results are presented in Table 12. Notably, we observe a decrease in performance when incorporating wordlevel contrast in both dense and sparse spaces, indicating possible feature redundancy. Moreover, our approach, which incorporates word-level contrast, can be viewed as an extension of X-CLIP (Ma et al., 2022b) with the shared sparse space. We notice that contrasting representations in the aligned sparse space enhances the retrieval performance of X-CLIP.\nA.4 Aligning Examples\nTo show the effectiveness of S3MA, we illustrate some examples of video-to-text and text-to-video retrieval examples in Figures 4, 6 and 7. We notice that S3MA is able to align some important concepts between video and text for precise retrieval. For example, in the bottom-left video-to-text result (Figure 6), the biggest difference between the top 5 retrieved texts is \u201cfootball\u201d. By precisely capturing \u201cfootball\u201d in the video, S3MA is able to give higher logits to the sentences that contain \u201cfootball\u201d. Additionally, in the last (bottom-right) text to video result (Figure 7), we notice that, by understanding \u201cman\u201d and \u201cdiscuss\u201d, S3MA is able to distinguish the top 3 retrieved videos and select the one in which a man appears. This empirically shows that\nS3MA performs well in visual and textual content understanding, benefiting from multi-space and multi-grained similarity.\nMoreover, we visualize the activation of sparse concepts by videos in Figure 8. We notice that, some hub sparse concepts are frequently retrieved while some are not retrieved a lot, which might be due to the KNN clustering. Moreover, we notice that the difference between activations from videos are separable."
        }
    ],
    "title": "Video-Text Retrieval by Supervised Sparse Multi-Grained Learning",
    "year": 2023
}