{
    "abstractText": "Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Promptbased Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics, we design a novel self-supervised learning objective based on mutual information maximization to derive enhanced representations of logical semantics for IDRR. Experimental results on PDTB 2.0 and CoNLL16 datasets demonstrate that our method achieves outstanding and consistent performance against the current state-of-the-art models.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenxu Wang"
        },
        {
            "affiliations": [],
            "name": "Ping Jian"
        },
        {
            "affiliations": [],
            "name": "Mu Huang"
        }
    ],
    "id": "SP:880fe572f7aa616a5646d43e5cf820a5c109bbc0",
    "references": [
        {
            "authors": [
                "Alexander A Alemi",
                "Ian Fischer",
                "Joshua V Dillon",
                "Kevin Murphy."
            ],
            "title": "Deep variational information bottleneck",
            "venue": "arXiv preprint arXiv:1612.00410.",
            "year": 2016
        },
        {
            "authors": [
                "Anthony J Bell",
                "Terrence J Sejnowski"
            ],
            "title": "An information-maximization approach to blind",
            "year": 1995
        },
        {
            "authors": [
                "Chunkit Chan",
                "Jiayang Cheng",
                "Weiqi Wang",
                "Yuxin Jiang",
                "Tianqing Fang",
                "Xin Liu",
                "Yangqiu Song."
            ],
            "title": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
            "venue": "arXiv preprint arXiv:2304.14827.",
            "year": 2023
        },
        {
            "authors": [
                "Chunkit Chan",
                "Xin Liu",
                "Jiayang Cheng",
                "Zihan Li",
                "Yangqiu Song",
                "Ginny Y Wong",
                "Simon See."
            ],
            "title": "Discoprompt: Path prediction prompt tuning for implicit discourse relation recognition",
            "venue": "arXiv preprint arXiv:2305.03973.",
            "year": 2023
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "Proceedings of NAACL-HLT, pages 615\u2013621.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Shengding Hu",
                "Weilin Zhao",
                "Yulin Chen",
                "Zhiyuan Liu",
                "Haitao Zheng",
                "Maosong Sun."
            ],
            "title": "Openprompt: An open-source framework for promptlearning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyi Fu",
                "Wangchunshu Zhou",
                "Jingjing Xu",
                "Hao Zhou",
                "Lei Li."
            ],
            "title": "Contextual representation learning beyond masked language modeling",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "R Devon Hjelm",
                "Alex Fedorov",
                "Samuel LavoieMarchildon",
                "Karan Grewal",
                "Phil Bachman",
                "Adam Trischler",
                "Yoshua Bengio"
            ],
            "title": "Learning deep representations by mutual information estimation and",
            "year": 2019
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Petteri Pajunen."
            ],
            "title": "Nonlinear independent component analysis: Existence and uniqueness results",
            "venue": "Neural networks, 12(3):429\u2013439.",
            "year": 1999
        },
        {
            "authors": [
                "Yangfeng Ji",
                "Jacob Eisenstein."
            ],
            "title": "One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations",
            "venue": "Transactions of the Association for Computational Linguistics, 3:329\u2013 344.",
            "year": 2015
        },
        {
            "authors": [
                "Feng Jiang",
                "Yaxin Fan",
                "Xiaomin Chu",
                "Peifeng Li",
                "Qiaoming Zhu."
            ],
            "title": "Not just classification: Recognizing implicit discourse relation on joint modeling of classification and generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Yuxin Jiang",
                "Linhan Zhang",
                "Wei Wang."
            ],
            "title": "Global and local hierarchy-aware contrastive framework for implicit discourse relation recognition",
            "venue": "arXiv preprint arXiv:2211.13873.",
            "year": 2022
        },
        {
            "authors": [
                "Yudai Kishimoto",
                "Yugo Murawaki",
                "Sadao Kurohashi."
            ],
            "title": "Adapting BERT to implicit discourse relation classification with a focus on discourse connectives",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1152\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Lingpeng Kong",
                "Cyprien de Masson d\u2019Autume",
                "Wang Ling",
                "Lei Yu",
                "Zihang Dai",
                "Dani Yogatama"
            ],
            "title": "A mutual information maximization perspective of language representation learning",
            "venue": "arXiv preprint arXiv:1910.08350",
            "year": 2019
        },
        {
            "authors": [
                "Man Lan",
                "Jianxiang Wang",
                "Yuanbin Wu",
                "Zheng-Yu Niu",
                "Haifeng Wang."
            ],
            "title": "Multi-task attention-based neural networks for implicit discourse relationship representation and identification",
            "venue": "Proceedings of the 2017 conference on empirical methods in natural",
            "year": 2017
        },
        {
            "authors": [
                "Li Liang",
                "Zheng Zhao",
                "Bonnie Webber."
            ],
            "title": "Extending implicit discourse relation recognition to the PDTB-3",
            "venue": "Proceedings of the First Workshop on Computational Approaches to Discourse, pages 135\u2013 147, Online. Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Ziheng Lin",
                "Min-Yen Kan",
                "Hwee Tou Ng."
            ],
            "title": "Recognizing implicit discourse relations in the penn discourse treebank",
            "venue": "Proceedings of the 2009 conference on empirical methods in natural language processing, pages 343\u2013351.",
            "year": 2009
        },
        {
            "authors": [
                "Ralph Linsker."
            ],
            "title": "Self-organization in a perceptual network",
            "venue": "Computer, 21(03):105\u2013117.",
            "year": 1988
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "arXiv e-prints, pages arXiv\u20132107.",
            "year": 2021
        },
        {
            "authors": [
                "Xin Liu",
                "Jiefu Ou",
                "Yangqiu Song",
                "Xin Jiang."
            ],
            "title": "On the importance of word and sentence representation learning in implicit discourse relation classification",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Yang Liu",
                "Sujian Li."
            ],
            "title": "Recognizing implicit discourse relations via repeated reading: Neural networks with multi-level attention",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1224\u20131233, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Wanqiu Long",
                "Bonnie Webber."
            ],
            "title": "Facilitating contrastive learning of discourse relational senses by exploiting the hierarchy of sense relations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10704\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Courtney Napoles",
                "Matthew R Gormley",
                "Benjamin Van Durme."
            ],
            "title": "Annotated gigaword",
            "venue": "Proceedings of the joint workshop on automatic knowledge base construction and web-scale knowledge extraction (AKBC-WEKEX), pages 95\u2013100.",
            "year": 2012
        },
        {
            "authors": [
                "Joonsuk Park",
                "Claire Cardie."
            ],
            "title": "Improving implicit discourse relation recognition through feature set optimization",
            "venue": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 108\u2013112.",
            "year": 2012
        },
        {
            "authors": [
                "Rashmi Prasad",
                "Nikhil Dinesh",
                "Alan Lee",
                "Eleni Miltsakaki",
                "Livio Robaldo",
                "Aravind K Joshi",
                "Bonnie L Webber"
            ],
            "title": "The penn discourse treebank",
            "year": 2008
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Attapol Rutherford",
                "Nianwen Xue."
            ],
            "title": "Improving the inference of implicit discourse relations via classifying explicit discourse connectives",
            "venue": "HLT-NAACL, pages 799\u2013808.",
            "year": 2015
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Shi",
                "Vera Demberg."
            ],
            "title": "Learning to explicitate connectives with Seq2Seq network for implicit discourse relation classification",
            "venue": "Proceedings of the 13th International Conference on Computational Semantics - Long Papers, pages 188\u2013199, Gothen-",
            "year": 2019
        },
        {
            "authors": [
                "Jialong Tang",
                "Hongyu Lin",
                "Meng Liao",
                "Yaojie Lu",
                "Xianpei Han",
                "Le Sun",
                "Weijian Xie",
                "Jin Xu."
            ],
            "title": "From discourse to narrative: Knowledge projection for event relation extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Siddharth Varia",
                "Christopher Hidey",
                "Tuhin Chakrabarty."
            ],
            "title": "Discourse relation prediction: Revisiting word pairs with convolutional networks",
            "venue": "Proceedings of the 20th annual SIGdial meeting on discourse and dialogue, pages 442\u2013452.",
            "year": 2019
        },
        {
            "authors": [
                "Siddharth Varia",
                "Christopher Hidey",
                "Tuhin Chakrabarty."
            ],
            "title": "Discourse relation prediction: Revisiting word pairs with convolutional networks",
            "venue": "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 442\u2013452, Stock-",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Bonnie Webber",
                "Rashmi Prasad",
                "Alan Lee",
                "Aravind Joshi"
            ],
            "title": "The penn discourse treebank 3.0 annotation manual",
            "year": 2019
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Changxing Wu",
                "Liuwen Cao",
                "Yubin Ge",
                "Yang Liu",
                "Min Zhang",
                "Jinsong Su"
            ],
            "title": "A label dependence",
            "year": 2022
        },
        {
            "authors": [
                "Nianwen Xue",
                "Hwee Tou Ng",
                "Sameer Pradhan",
                "Attapol Rutherford",
                "Bonnie Webber",
                "Chuan Wang",
                "Hongmin Wang."
            ],
            "title": "CoNLL 2016 shared task on multilingual shallow discourse parsing",
            "venue": "Proceedings of the CoNLL-16 shared task, pages 1\u201319, Berlin, Ger-",
            "year": 2016
        },
        {
            "authors": [
                "Biao Zhang",
                "Jinsong Su",
                "Deyi Xiong",
                "Yaojie Lu",
                "Hong Duan",
                "Junfeng Yao."
            ],
            "title": "Shallow convolutional neural network for implicit discourse relation recognition",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Yan Zhang",
                "Ruidan He",
                "Zuozhu Liu",
                "Kwan Hui Lim",
                "Lidong Bing."
            ],
            "title": "An unsupervised sentence embedding method by mutual information maximization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhou",
                "Man Lan",
                "Yuanbin Wu",
                "Yuefeng Chen",
                "Meirong Ma."
            ],
            "title": "Prompt-based connective prediction method for fine-grained implicit discourse relation recognition",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Implicit discourse relation recognition (IDRR) focuses on identifying semantic relations between two arguments (sentences or clauses), with the absence of explicit connectives (e.g., however, be-\n\u2217Corresponding author. 1Our code will be released at https://github.com/\nlalalamdbf/PLSE_IDRR.\ncause). As a fundamental and crucial task in discourse parsing, IDRR has benefited a multitude of natural language processing (NLP) downstream tasks, such as question answering (Rutherford and Xue, 2015), text summarization (Cohan et al., 2018) and event relation extraction (Tang et al., 2021). Explicit discourse relation recognition (EDRR) has already incontrovertibly demonstrated the remarkable effectiveness by utilizing explicit connectives in discerning discourse relation type (Varia et al., 2019a). However, implicit discourse relation recognition remains a challenging and demanding task for researchers due to the absence of connectives.\nEarly works based on traditional machine learning are dedicated to the extraction of intricate syntax features (Lin et al., 2009; Park and Cardie, 2012). With the rapid development of deep learning, pre-trained language models (PLMs), such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), have remarkably improved the performance of IDRR. Recently, there has been a growing inclination among various work to exploit the multi-level hierarchical information (an instance introduced in Appendix A.1) derived from annotated\nsenses (Wu et al., 2022; Jiang et al., 2022; Long and Webber, 2022; Chan et al., 2023b). Nevertheless, such usage exhibits an excessive reliance on the annotated data, thereby restricting the potential of IDRR for further expansion to some degree. Fortunately, there exists a plethora of unannotated utterances containing explicit connectives that can be effectively utilized to enhance discourse relation representations.\nAs depicted in Figure 1, even in the absence of annotated sense, the explicit data can proficiently convey a Contingency.Cause.Result relation, elucidating a strong correlation between the connective and the discourse relation. In expectation of knowledge transfer from explicit discourse relations to implicit discourse relations, Kishimoto et al. (2020) has undertaken preliminary investigations into leveraging explicit data and connective prediction. This work incorporated an auxiliary pre-training task named explicit connective prediction, which constructed an explicit connective classifier to output connectives according to the representation of the [CLS]. Nonetheless, this method falls short in terms of intuitive connective prediction through the [CLS] and fails to fully harness the valuable knowledge gleaned from the masked language model (MLM) task. Besides, Zhou et al. (2022) proposed a prompt-based connective prediction (PCP) method, but their approach relies on Prefix-Prompt, which is not inherently intuitive and introduces additional complexity of inference.\nIn this paper, we propose a novel Prompt-based Logical Semantics Enhancement (PLSE) method for implicit discourse relation recognition. Specifically, we manually design a Cloze-Prompt template for explicit connective prediction (pre-training) and implicit connective prediction (prompt-tuning), which achieves a seamless and intuitive integration between the pre-training and the downstream task. On the other hand, previous work (Fu et al., 2022) has revealed that the contextualized representations learned by MLM lack the ability to capture global semantics. In order to facilitate connective prediction, it is important to have a comprehensive understanding of the text\u2019s global logical semantics. Inspired by recent unsupervised representation learning with mutual information (Hjelm et al., 2019; Zhang et al., 2020), we propose a novel selfsupervised learning objective that maximizes the mutual information (MI) between the connectiverelated representation and the global logic-related\nsemantic representation (detailed in Section 3.2). This learning procedure fosters the connectiverelated representations to effectively capture global logical semantic information, thereby leading to enhanced representations of logical semantics.\nOur work focuses on identifying implicit intersentential relations, thus evaluating our model on PDTB 2.0 (Prasad et al., 2008) and CoNLL16 (Xue et al., 2016) datasets. Since the latest PDTB 3.0 (Webber et al., 2019) introduces numerous intrasentential relations, there is a striking difference in the distribution between inter-sentential and intrasentential relations reported by (Liang et al., 2020). Given this issue, we don\u2019t conduct additional experiments on PDTB 3.0.\nThe main contributions of this paper are summarized as follows:\n\u2022 We propose a prompt-based logical semantics enhancement method for IDRR, which sufficiently exploits unannotated utterances with connectives to learn better discourse relation representations.\n\u2022 Our proposed connective prediction based on Cloze-Prompt seamlessly injects knowledge related to discourse relation into PLMs and bridges the gap between the pre-training and the downstream task.\n\u2022 Our method, aiming at capturing global logical semantics information through MI maximization, effectively results in enhanced discourse relation representations."
        },
        {
            "heading": "2 Related Work",
            "text": "Prompt-based Learning Along with the booming development of large-scale PLMs like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020) and GPT3 (Brown et al., 2020), prompt-based learning has become a new paradigm in the filed of natural language processing (Liu et al., 2021). In contrast to the fine-tuning paradigm, prompt-based methods reformulate the downstream tasks to match the pre-training objective. For example, Schick and Sch\u00fctze (2021) converted a diverse set of classification problems into cloze tasks by constructing appropriate prompts and verbalizers that associate specific filled words with predicted categories.\nRepresentation Learning with MI Methods based on mutual information have a long history in unsupervised representation learning, such as\nthe infomax principle (Linsker, 1988; Bell and Sejnowski, 1995) and ICA algorithms (Hyv\u00e4rinen and Pajunen, 1999). Alemi et al. (2016) was the first to integrate MI-related optimization into deep learning models. From then on, numerous studies have demonstrated the efficacy of MI-maximization for unsupervised representation learning (Hjelm et al., 2019; Zhang et al., 2020; Kong et al., 2019). Inspired by these superior works, we explore how to derive enhanced discourse relation representations by incorporating global logical semantic information through MI-maximization.\nImplicit Discourse Relation Recognition For this task, many prior works based on deep neural network have proposed various approaches to exploiting better semantic representation of arguments, such as shallow CNN (Zhang et al., 2015) and LSTM with multi-level attention (Liu and Li, 2016). Recently, PLMs have substantially improved the performance of IDRR by leveraging their robust contextualized representations (Shi and Demberg, 2019; Liu et al., 2020). Furthermore, Kishimoto et al. (2020) proposed an auxiliary pretraining task through unannotated explicit data and connective prediction. Zhou et al. (2022) proposed a connective prediction (PCP) method based on Prefix-Prompt. Regrettably, their approaches fail to\ncapitalize on the potential of data and PLMs. On the other hand, the multi-level hierarchical information has been explored by (Wu et al., 2022; Jiang et al., 2022; Long and Webber, 2022; Chan et al., 2023b). However, the efficacy of these methods remains significantly constrained by the availability of annotated data. Therefore, our work is dedicated to the proficient utilization of unannotated explicit data via prompt-based connective prediction and MI-maximization."
        },
        {
            "heading": "3 Methodology",
            "text": "Figure 2 shows the overall architecture of our framework.2 In the following sections, we will elaborate on our methodologies including a ClozePrompt template based on connective prediction, a robust pre-training approach to incorporating explicit data, and an effective prompt-tuning method by leveraging implicit data."
        },
        {
            "heading": "3.1 Prompt Templatize",
            "text": "As Figure 2 illustrates, the input argument pair x = (Arg1, Arg2) is reformulated into a Cloze-\n2In the pre-training phase, Figure 2 does not depict the Masked Language Mask (MLM) task for universal words, yet it still remains an integral part of our framework to enhance the universal semantics of discourse relations for PLMs.\nPrompt template by concatenating two arguments and inserting some PLM-specific tokens such as [MASK], [CLS] and [SEP]. The [MASK] token is mainly used to predict the connective between two arguments, while the [CLS] and [SEP] tokens are inserted to indicate the beginning and ending of the input. In addition, the [SEP] tokens also serve the purpose of delineating the boundaries between two arguments."
        },
        {
            "heading": "3.2 Pre-training",
            "text": "In this section, we will provide a comprehensive introduction to the pre-training data and tasks, including Connectives Mask (CM), Masked Language Model (MLM), Global Logical Semantics Learning (GLSL). Pre-training Data Acquiring implicit discourse relational data poses an immensely arduous undertaking. Fortunately, explicit discourse relation data is considerably more accessible owing to the presence of connectives. Therefore, we approximately collected 0.56 million explicit argument pairs from an unannotated corpus renowned as Gigaword (Napoles et al., 2012) by leveraging its vast collection of connectives. The collected data is divided into training and validation sets in a 9:1 ratio.\nConnectives Mask Based on the strong correlation observed between explicit connectives and discourse relations, we propose the \"Connectives Mask\" task as a pivotal approach to incorporating discourse logical information into PLMs. This task not only involves learning the intricate representations of connectives but also serves as a vital bridge between the realms of the pre-training and the downstream task, that facilitates seamless knowledge transfer. For the mask strategy, we replace the connective with (1) the [MASK] token 90 % of the time (2) the unchanged token 10% of the time. The cross-entropy loss for Connectives Mask (CM) is as follows:\nLCM = 1\nN N\u2211 i=1 \u2212logP (ci |T (xi)) (1)\nwhere N denotes the number of training examples, T embodies a prompt template designed to transform the input argument pair x, c represents the connective token between the argument pair, and P estimates the probability of the answer word c.\nMasked Language Model In order to enhance\nthe universal semantics of discourse relations for PLMs, we still retain the Masked Language Model (MLM) task for universal words. The cross-entropy loss for MLM is as follows:\nLMLM = 1\nN N\u2211 i=1 mean j\u2208masked \u2212logP (ui,j |T (xi)) (2)\nwhere N and T remain consistent with Equation (1), u denotes the universal token within the argument pair, and P approximates the probability of the answer word u.\nGlobal Logical Semantics Learning To address the local dependencies of predicted connectives representations and capture global semantic information with logical coherence, we propose a novel method based on MI maximization learning. Give an argument pair x = (Arg1, Arg2) and the Cloze-Prompt template T (\u00b7), we feed T (x) through a Transformer (Vaswani et al., 2017) encoder to acquire contextualized token representations H . Specifically, we design a Multi-Head Cross-Attention (MHCA) module to extract the global logic-related semantic representation. As illustrated in the part (a) of Figure 2, we separate H into Harg1, Hcmask and Harg2, denoting as the contextualized representations of arg1, connective mask and arg2, respectively. The global logicrelated representation HGlogic is calculated through MHCA module as follows:\nQ = Hcmask K = V = Harg1 \u2295Harg2 (3) HGlogic = MultiHead(Q,K, V ) (4)\nThe learning objective is to maximize the mutual information between the connective-related representation Hcmask and the global logic-related representation HGlogic . Due to the notorious intractability of MI estimation in continuous and high-dimensional settings, we typically resort to maximizing lower bound estimators of MI. In our method, we adopt a Jensen-Shannon MI estimator suggested by (Hjelm et al., 2019; Zhang et al., 2020):\nI\u0302JSD\u03c9 (HGlogic ;Hcmask) := EP[\u2212sp(\u2212T\u03c9(HGlogic , Hcmask)] \u2212EP\u00d7P\u0303[sp(T\u03c9(H \u2032 Glogic , Hcmask)] (5)\nwhere T\u03c9 is a discrimination function parameterized by a neural network with learnable parameters \u03c9. It takes a pair of the global logic-related\nrepresentation HGlogic and the connective-related representation Hcmask as input and generates the corresponding score to estimate I\u0302JSD\u03c9 . H \u2032Glogic is a negative example randomly sampled from distribution P\u0303 = P, and sp(z) = log(1+ez) is the softplus function. The end-goal training objective is maximizing the MI between HGlogic and Hcmask , and the loss is formulated as follows:\nLGLSL = \u2212I\u0302JSD\u03c9 (HGlogic ;Hcmask) (6)\nThrough maximizing I\u0302JSD\u03c9 , The connectiverelated representation Hcmask is strongly encouraged to maintain a high level of MI with its global logic-related representation. This will foster Hcmask to incorporate comprehensive logical semantic information, leading to enhanced discourse relation representations.\nThe overall training goal is the combination of Connectives Mask loss, MaskedLM loss, and Global Logical Semantics Learning loss:\nL = LCM + LMLM + LGLSL (7)"
        },
        {
            "heading": "3.3 Prompt-tuning",
            "text": "As depicted in the part (b) of Figure 2, the input argument pair x = (Arg1, Arg2) and the ClozePrompt template T (\u00b7) exhibit a remarkable consistency with the pre-training phase. This noteworthy alignment indicates that the prompt-tuning through connective prediction fosters the model to fully leverage the profound reservoir of discourse relation knowledge acquired during the pre-training phase. In order to streamline the downstream task and avoid introducing additional trainable parameters, the MHCA module isn\u2019t used in the prompttuning phase. Besides, we construct a verbalizer mapping connectives to implicit discourse relation labels.\nVerbalizer Construction A discrete answer space, which is a subset of the PLM vocabulary, is defined for IDRR. Given that the implicit discourse relational data has already been annotated with appropriate connectives in PDTB 2.0 (Prasad et al., 2008) and CoNLL16 (Xue et al., 2016) datasets, we manually select a set of high-frequency and low-ambiguity connectives as the answer words of the corresponding discourse relations. Therefore, we construct the verbalizer mapping connectives to implicit discourse relation labels for PDTB 2.0 in Table 1, and the verbalizer for CoNLL16 is shown in Appendix A.3.\nThe cross-entropy loss for the prompt-tuning is as follows:\nL = 1 N N\u2211 i=1 \u2212logP (li = V (ci) |T (xi)) (8)\nwhere N and T remain consistent with Equation (1), V denotes the verbalizer mapping the connective c to implicit discourse relation label l, and P estimates the probability of the gold sense label l."
        },
        {
            "heading": "4 Experiment Settings",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "The Penn Discourse Treebank 2.0 (PDTB 2.0) PDTB 2.0 is a large scale corpus containing 2,312 Wall Street Journal (WSJ) articles (Prasad et al., 2008), that is annotated with information relevant to discourse relation through a lexically-grounded approach. This corpus includes three levels of senses (i.e., classes, types, and sub-types). We follow the predecessors (Ji and Eisenstein, 2015) to split sections 2-20, 0-1, and 21-22 as training, validation, and test sets respectively. We evaluate our framework on the four top-level implicit discourse classes and the 11 major second-level implicit discourse types by following prior works (Liu et al., 2019; Jiang et al., 2022; Long and Webber, 2022). The detailed statistics of the top-level and second-level senses are shown in Appendix A.2.\nThe CoNLL-2016 Shared Task (CoNLL16) The CoNLL 2016 shared task (Xue et al., 2016) provides more abundant annotation for shadow discourse parsing. This task consists of two test sets,\nthe PDTB section 23 (CoNLL-Test) and Wikinews texts (CoNLL-Blind), both annotated following the PDTB annotation guidelines. On the other hand, CoNLL16 merges several labels to annotate new senses. For instance, Contingency.Pragmatic cause is merged into Contingency.Cause.Reason to remove the former type with very few samples. There is a comprehensive list of 14 sense classes to be classified, detailed cross-level senses as shown in Appendix A.3.3"
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "In our experiments, we utilize the pre-trained RoBERTa-base (Liu et al., 2019) as our Transformers encoder and employ AdamW (Loshchilov and Hutter, 2017) as the optimizer. The maximum input sequence length is set to 256. All experiments are implemented in PyTorch framework by HuggingFace transformers4 (Wolf et al., 2020) on one 48GB NVIDIA A6000 GPU. Next, we will provide details of the pre-training and prompt-tuning settings, respectively.\nPre-training Details During the pre-training phase, we additionally add a MHCA module and a discriminator T\u03c9 to facilitate MI maximization learning. The MHCA module is a multi-head attention network, where the number of heads is 6. The discriminator T\u03c9 consists of three feedforward layers with ReLU activation, and the detailed architecture is depicted in Appendix A.4. We pre-train the model for two epochs, and other hyper-parameter settings include a batch size of 64, a warmup ratio of 0.1, and a learning rate of 1e-5.\nPrompt-tuning Details The implementation of our code refers to OpenPrompt (Ding et al., 2022). We use Macro-F1 score and Accuracy as evaluation metric. The training is conducted with 10 epochs, which selects the model that yields the best performance on the validation set. For top-level senses on PDTB 2.0, we employ a batch size of 64 and a learning rate of 1e-5. For second-level senses on PDTB 2.0 and cross-level senses on CoNLL16, we conducted a meticulous hyper-parameter search due to the limited number of certain senses. The bath size and learning rate are set to 16 and 1e-6, respectively.\n3Some instances in PDTB 2.0 and CoNLL16 datasets are annotated with multiple senses. Following previous works, we treat them as separate examples during training. At test time, a prediction is considered correct when it aligns with one of the ground-truth labels.\n4https://github.com/huggingface/transformers"
        },
        {
            "heading": "4.3 Baselines",
            "text": "To validate the effectiveness of our method, we compare it with the most advanced baselines currently available. Here we mainly introduce the Baselines that have emerged within the past two years.\n\u2022 CG-T5 (Jiang et al., 2021): a joint model that recognizes the relation label and generates the target sentence containing the meaning of relations simultaneously.\n\u2022 LDSGM (Wu et al., 2022): a label dependenceaware sequence generation model, which exploits the multi-level dependence between hierarchically structured labels.\n\u2022 PCP (Zhou et al., 2022): a connective prediction method by applying a Prefix-Prompt template.\n\u2022 GOLF (Jiang et al., 2022): a contrastive learning framework by incorporating the information from global and local hierarchies.\n\u2022 DiscoPrompt (Chan et al., 2023b): a path prediction method that leverages the hierarchical structural information and prior knowledge of connectives.\n\u2022 ContrastiveIDRR (Long and Webber, 2022): a contrastive learning method by incorporating the sense hierarchy and utilizing it to select the negative examples.\n\u2022 ChatGPT (Chan et al., 2023a): a method based on ChatGPT through the utilization of an incontext learning prompt template."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "Table 2 shows the primary experimental results of our method and other baselines on PDTB 2.0 and CoNLL16 datasets, in terms of Macro-F1 score and Accuracy. Classification performance on PDTB 2.0 in terms of F1 score for the four top-level classes and 11 second-level types is shown in Table 3 and Table 4. Based on these results, we derive the following conclusions:\nFirstly, our PLSE method has achieved new state-of-the-art performance on PDTB 2.0 and CoNLL16 datasets. Specifically, our method exhibits a notable advancement of 1.8% top-level F1, 3.35% top-level accuracy, 0.45% second-level F1,\nand 2.31% second-level accuracy over the current state-of-the-art ContrastiveIDRR model (Long and Webber, 2022) on PDTB 2.0. Moreover, with regard to CoNLL16, our method also outperforms the current state-of-the-art DiscoPrompt model (Chan et al., 2023b) by a margin of 8.70% F1 on CoNLLTest and 0.47% accuracy on CoNLL-Blind. For the performance of top-level classes on PDTB 2.0, our method significantly improves the performance of senses, particularly in Comp, Cont and Exp. It surpasses the previous best results by 3.22%, 2.59% and 2.09% F1, respectively, demonstrating a substantial efficiency of our method. For the performance of second-level types on PDTB 2.0, we also compare our model with the current state-of-the-art models. As illustrated in Table 4, our results show a noteworthy enhancement in F1 performance across five categories of second-level senses. Meanwhile, our model still remain exceptional average performance in most second-level senses.\nSecondly, a series of recent works for IDRR (Wu et al., 2022; Jiang et al., 2022; Chan et al., 2023b; Long and Webber, 2022) frequently utilize the hierarchical structure information from the annotated senses, leading to good results. Nevertheless, it\u2019s crucial to note that the performance and robust-\nness of these models are excessively reliant on the small-scale annotated data, that may pose a stumbling block to future research explorations. In the NLP community, it has become a prevalent trend to leverage large-scale unlabeled data and design superior self-supervised learning methods. Therefore, our work focuses on sufficiently exploiting unannotated data containing explicit connectives through prompt-based connective prediction. At the same time, to tackle the local dependencies of predicted connectives representations, we design a MI-maximization strategy, which aids in capturing the global logical semantics information pertinent to the predicted connectives. Overall results demonstrate the great potential of our method.\nFinally, we compare our method with ChatGPT (Chan et al., 2023a), a recent large language model, on the IDRR task. The performance of ChatGPT significantly trails our PLSE model by approxi-\nmately 35% F1 and 30% accuracy on PDTB 2.0, which highlights ChatGPT\u2019s poor capacity to comprehend the logical semantics in discourses. Therefore, further research on IDRR remains crucial and imperative for the NLP community."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "To better investigate the efficacy of individual modules in our framework, we design numerous ablations on Global Logical Semantics Learning, Connectives Mask and MaskedLM tasks. Table 5 indicates that eliminating any of the three tasks would hurt the performance of our model on both top-level and second-level senses. It is worth noting that removing Global Logical Semantics Learning significantly hurts the performance. Furthermore, to assess the proficiency of Global Logical Semantics Learning, we compare it with an auxiliary classification task leveraging the [CLS] representation or the average representation of a sequence, which aims to introduce global information. The performance\u2019s improvement through MTLmean highlights the effectiveness of incorporating global semantics. Surprisingly, MTLcls detrimentally impacts the model\u2019s performance. The reason could be that the [CLS] representation encompasses a vast amount of information irrelevant to global logical semantics, thereby interfering with connective prediction. Our method considerably outperforms MTLmean, which demonstrates that our MI maximization strategy by integrating global logical semantics is indeed beneficial for connective prediction. Besides, eliminating Connectives Mask or MaskedLM also diminishes the performance of our model, illustrating that our method can effectively utilize large-scale\nunannotated data to learn better discourse relation representations by injecting discourse logical semantics and universal semantics into PLMs."
        },
        {
            "heading": "5.3 Data Scale Analysis",
            "text": "To investigate the impact of unannotated data scale on the performance of our model, we conduct supplementary extension experiments. As show in Figure 3, the performance swiftly accelerates prior to reaching a data scale of 500, 000, yet beyond this threshold, the pace of improvement markedly decelerates. The primary reason is the difference in semantic distributions of explicit and implicit data. For example, the connectives in explicit data play an essential role in determining the discourse relations. Conversely, the implicit data does not contain any connectives, while the discourse relations of sentences remain unaffected by implicit connectives in annotations. Therefore, considering both performance and training cost, we select unannotated explicit data at a scale of 500, 000."
        },
        {
            "heading": "5.4 Few-Shot Learning",
            "text": "To evaluate the robustness of our model in few-shot learning, we conduct additional experiments using 10%, 20%, and 50% of training data, respectively. The results of few-shot learning are summarized in Figure 4. The orange line illustrates the outcomes achieved by our PLSE method, while the blue line depicts the results obtained without pretraining. The results of the comparison demonstrate that our method significantly outperforms the baseline across all metrics. In particular, our method, when trained on only 50% of training data, achieves performance comparable to the baseline trained on full training data, which further underscores the efficacy of our approach."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a novel prompt-based logical semantics enhancement method for IDRR. Our approach sufficiently exploits unannotated explicit data by seamlessly incorporating discourse relation knowledge based on Cloze-Prompt connective prediction and comprehensively learning global logical semantics through MI maximization. Compared with the current state-of-the-art methods, our model transcends the limitations of overusing annotated multi-level hierarchical information, which achieves new state-of-the-art performance on PDTB 2.0 and CoNLL2016 datasets.\nLimitations\nDespite achieving outstanding performance, there are still some limitations of our work, which could be summarized into the following two aspects. The first is the different semantic distributions between explicit and implicit data, as illustrated in Section 5.3, that significantly hampers further improvements of our model\u2019s performance. A promising future research would involve the exploration of innovative approaches to bridging the gap between explicit and implicit data, or the development of strategies to filter explicit data exhibiting similar distribution patterns to implicit data. Additionally, our connective prediction method is limited by its reliance on single tokens, thereby transforming twotoken connectives into one (e.g., for example \u2192 example). Moreover, it discards three-token connectives entirely (e.g., as soon as). As a result, our approach doesn\u2019t fully exploit the pre-trained models and explicit data to some extent. In the\nfuture, we will explore new methods for predicting multi-token connectives.\nEthics Statement\nThis study focuses on model evaluation and technical improvements in fundamental research. Therefore, we have refrained from implementing any additional aggressive filtering techniques on the text data we utilize, beyond those already applied to the original datasets obtained from their respective sources. The text data employed in our research might possess elements of offensiveness, toxicity, fairness, or bias, which we haven\u2019t specifically addressed since they fall outside the primary scope of this work. Furthermore, we do not see any other potential risks."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank the organizers of EMNLP2023 and the reviewers for their helpful suggestions. This work is partly supported by the grants from the National Natural Science Foundation of China (No. 62172044) and the Open Project Program of the National Defense Key Laboratory of Electronic Information Equipment System Research (No. 614201001032203)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Multi-level Sense Hierarchy\nThe hierarchies of both PDTB 2.0 and CoNLL16 consist of three levels. According to the PDTB annotation guidelines, all senses are organized into a three-layer hierarchical structure. Figure 5 shows the multi-level sense hierarchy of an IDRR instance in the PDTB 2.0 corpus.\nA.2 Data Statistics\nA.3 Verbalizer for CoNLL16\nA.4 Mutual Information Discriminator We concatenate the global logic-related representation HGlogic and the connective-related representation Hcmask as input. Then we feed this to the discriminator T\u03c9, which produces scores to maximize the MI estimator in Equation (5). The architecture of T\u03c9 is shown in Table 9."
        }
    ],
    "title": "Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition",
    "year": 2023
}