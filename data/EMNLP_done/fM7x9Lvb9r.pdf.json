{
    "abstractText": "Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling the MBEL problem with a discriminative classifier, we formulate it as a sequence-tosequence generation task, which better exploits the shared dependencies between source mentions and target entities. Moreover, Con2GEN matches against UMLS concepts in as many languages and types as possible, hence facilitating cross-information disambiguation. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art techniques on the XL-BEL and the Mantra GSC datasets spanning 12 typologically diverse languages.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tiantian Zhu"
        },
        {
            "affiliations": [],
            "name": "Yang Qin"
        },
        {
            "affiliations": [],
            "name": "Qingcai Chen"
        },
        {
            "affiliations": [],
            "name": "Xin Mu"
        },
        {
            "affiliations": [],
            "name": "Changlong Yu"
        },
        {
            "affiliations": [],
            "name": "Yang Xiang"
        }
    ],
    "id": "SP:a820777b5faf06cf573970d8491d2bb25ee30311",
    "references": [
        {
            "authors": [
                "Chenxin An",
                "Jiangtao Feng",
                "Kai Lv",
                "Lingpeng Kong",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Cont: Contrastive neural text generation",
            "venue": "arXiv preprint arXiv:2205.14690.",
            "year": 2022
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
            "year": 2015
        },
        {
            "authors": [
                "Yonatan Bitton",
                "Raphael Cohen",
                "Tamar Schifter",
                "Eitan Bachmat",
                "Michael Elhadad",
                "No\u00e9mie Elhadad."
            ],
            "title": "Cross-lingual unified medical language system entity linking in online health communities",
            "venue": "J. Am. Medical Informatics Assoc., 27(10):1585\u20131592.",
            "year": 2020
        },
        {
            "authors": [
                "Olivier Bodenreider."
            ],
            "title": "The unified medical language system (UMLS): integrating biomedical terminology",
            "venue": "Nucleic Acids Res., 32(Database-Issue):267\u2013 270.",
            "year": 2004
        },
        {
            "authors": [
                "Nicola De Cao",
                "Ledell Wu",
                "Kashyap Popat",
                "Mikel Artetxe",
                "Naman Goyal",
                "Mikhail Plekhanov",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Multilingual autoregressive entity linking",
            "venue": "Trans. Assoc. Comput. Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Junying Chen",
                "Dongfang Li",
                "Qingcai Chen",
                "Wenxiu Zhou",
                "Xin Liu."
            ],
            "title": "Diaformer: Automatic diagnosis via symptoms sequence generation",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on In-",
            "year": 2022
        },
        {
            "authors": [
                "Thomas H. Cormen",
                "Charles E. Leiserson",
                "Ronald L. Rivest",
                "Clifford Stein."
            ],
            "title": "Introduction to Algorithms, 3rd Edition",
            "venue": "MIT Press.",
            "year": 2009
        },
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Autoregressive entity retrieval",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ulrich Germann",
                "Michael Jahr",
                "Kevin Knight",
                "Daniel Marcu",
                "Kenji Yamada."
            ],
            "title": "Fast decoding and optimal decoding for machine translation",
            "venue": "Association for Computational Linguistic, 39th Annual Meeting and 10th Conference of the European Chap-",
            "year": 2001
        },
        {
            "authors": [
                "Jan A. Kors",
                "Simon Clematide",
                "Saber A. Akhondi",
                "Erik M. van Mulligen",
                "Dietrich RebholzSchuhmann."
            ],
            "title": "A multilingual gold-standard corpus for biomedical concept recognition: the mantra GSC",
            "venue": "J. Am. Medical Informatics Assoc., 22(5):948\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Martin Josifoski Sebastian Riedel Luke Zettlemoyer Ledell Wu",
                "Fabio Petroni."
            ],
            "title": "Zero-shot entity linking with dense entity retrieval",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ehsan Shareghi",
                "Zaiqiao Meng",
                "Marco Basaldella",
                "Nigel Collier."
            ],
            "title": "Self-alignment pretraining for biomedical entity representations",
            "venue": "Proc. of NAACL, pages 4228\u20134238.",
            "year": 2021
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ivan Vulic",
                "Anna Korhonen",
                "Nigel Collier"
            ],
            "title": "Learning domain-specialised representations for cross-lingual biomedical entity linking",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:726\u2013742.",
            "year": 2020
        },
        {
            "authors": [
                "Yaojie Lu",
                "Hongyu Lin",
                "Jin Xu",
                "Xianpei Han",
                "Jialong Tang",
                "Annan Li",
                "Le Sun",
                "Meng Liao",
                "Shaoyi Chen."
            ],
            "title": "Text2event: Controllable sequence-tostructure generation for end-to-end event extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Zaiqiao Meng",
                "Fangyu Liu",
                "Ehsan Shareghi",
                "Yixuan Su",
                "Charlotte Collins",
                "Nigel Collier."
            ],
            "title": "Rewirethen-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Sunil Mohan",
                "Donghui Li."
            ],
            "title": "Medmentions: A large biomedical corpus annotated with UMLS concepts",
            "venue": "Proc. of AKBC.",
            "year": 2019
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Shruti Rijhwani",
                "Jiateng Xie",
                "Graham Neubig",
                "Jaime G. Carbonell."
            ],
            "title": "Zero-shot neural transfer for cross-lingual entity linking",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of",
            "year": 2019
        },
        {
            "authors": [
                "Roland Roller",
                "Madeleine Kittner",
                "Dirk Weissenborn",
                "Ulf Leser."
            ],
            "title": "Cross-lingual candidate search for biomedical concept normalization",
            "venue": "CoRR, abs/1805.01646.",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Mon-",
            "year": 2014
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "CoRR, abs/1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Ledell Wu",
                "Fabio Petroni",
                "Martin Josifoski",
                "Sebastian Riedel",
                "Luke Zettlemoyer."
            ],
            "title": "Scalable zeroshot entity linking with dense entity retrieval",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Hongyi Yuan",
                "Zheng Yuan",
                "Sheng Yu."
            ],
            "title": "Generative biomedical entity linking via knowledge baseguided pre-training and synonyms-aware fine-tuning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Zheng Yuan",
                "Zhengyun Zhao",
                "Haixia Sun",
                "Jiao Li",
                "Fei Wang",
                "Sheng Yu."
            ],
            "title": "CODER: knowledge-infused cross-lingual medical term embedding for term normalization",
            "venue": "J. Biomed. Informatics, 126:103983.",
            "year": 2022
        },
        {
            "authors": [
                "Tiantian Zhu",
                "Yang Qin",
                "Qingcai Chen",
                "Baotian Hu",
                "Yang Xiang."
            ],
            "title": "Enhancing entity representations with prompt learning for biomedical entity linking",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multilingual biomedical entity linking (MBEL) is an essential yet challenging task for domainspecific natural language understanding, which refers to the process of mapping mentions in a source language to standard entities in a multilingual knowledge base (KB). Successes in building MBEL systems can assist various medical research, such as biomedical knowledge probing (Meng et al., 2022) and clinical decision support (Chen et al., 2022), which need to determine the distinct meanings of biomedical terms, regardless of language.\nAlthough promising breakthroughs have been achieved in monolingual biomedical entity linking\n*Corresponding authors.\n(typically English) (Bitton et al., 2020; Liu et al., 2021a; Zhu et al., 2022), these approaches cannot be effectively applied to other languages due to the huge discrepancies between multilingual and monolingual versions of the biomedical entity linking (BEL) task. Firstly, resources for non-English BEL are scarce, hindering the development of multilingual research. For example, although the Unified Medical Language System (UMLS) (Bodenreider, 2004) is often regarded as a large multilingual KB in the biomedical domain, more than 70% of concept synonyms are in English, while the minority of concept names are in other languages. Secondly, the ambiguity problem in MBEL is more severe compared to monolingual BEL, since named entities with the same surface form in different languages can express distinct semantics, which makes the task of MBEL more challenging.\nA dominant paradigm of most current MBEL\nsolutions is to provide a single representation for each biomedical term by fine-tuning multilingual pre-trained models, such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020), on phraselevel synonyms extracted from the UMLS (Liu et al., 2021b; Yuan et al., 2022b). However, these methods still suffer from two major drawbacks, namely indirect and uninformative. Concretely, the indirect problem refers to the candidate entities are ranked with a dot-product between dense vectors between the mention and the entity, which fails to capture the interactions between them. The uninformative problem refers to the fact that current work solves the MBEL task via shallow matching based on surface forms, which ignores the deep underlying information of entities, such as language and context.\nConsider the example in Figure 1, where Stevia is a German mention of the concept C1018893 in UMLS. In this case, although the top 1 candidate entity retrieved by SAPBERTmulti (Liu et al., 2021b) has the same surface form as the mention, it is not the target entity. Specifically, the entity Stevia is an English synonym for the concept C0075246, of type Organic Chemical or Pharmacologic Substance, which is an ambiguous instance across languages and types. However, if the MBEL model understands the context of mentions and utilizes the language and type information of entities, the target concept may have a greater chance of being retrieved, which will make the entity disambiguation across languages more precise.\nMotivated by the above observations, in this paper, we propose a prompt-based controllable contrastive generation (Con2GEN) algorithm to disambiguate biomedical entities across diverse languages. To address the indirect challenge, we exploit a sequence-to-sequence architecture to effectively cross-encode the mention and the entity to capture more latent interactions. Generative models inherently have a better ability to align input and output spaces than discriminative models, since discriminative models need to iteratively fit atomic labels to learn aligned features (Cao et al., 2022). In detail, Con2GEN summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template, mainly aiming to solve the uninformative challenge and reduce ambiguity. Specifically, the predefined template is as close as possible to natural language and\nmatches against UMLS concepts in as many languages and types as possible, which allows the transfer of the latent knowledge of multilingual pre-trained models about the task. In this way, the decoder of Con2GEN can be guided to generate in a limited space by filling the fixed slots in the predefined template. Moreover, we utilize contrastive learning to alleviate the exposure bias problem in generation (An et al., 2022) \u2014 an autoregressive model trained only using the ground truths exhibits inferior generalization performance. Finally, we evaluate the performance of our model using the XL-BEL (Liu et al., 2021b) and the Mantra GSC (Kors et al., 2015) datasets spanning 12 typologically diverse languages, and against several stateof-the-art (SOTA) baselines. From the extensive experimental results, the proposed Con2GEN strategy is demonstrated to be effective in multilingual disambiguation based on multidimensional information of bio-entities. The code is available at https://github.com/TiantianZhu110/Con2GEN.\nTo sum up, the contributions of this paper are as follows: (1) For the first time, we propose the Con2GEN, a novel controllable contrastive generative method, to address the ambiguity challenge of the MBEL task; (2) We design a constrained decoding algorithm consisting of a predefined template and a contrastive learning mechanism for entity multidimensional information injection during inference; (3) Extensive experiments show that our model achieves promising performance improvements on three public MBEL datasets spanning 12 diverse languages."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Multilingual Biomedical Entity Linking",
            "text": "Multilingual biomedical entity linking (MBEL) task is an extended version of monolingual biomedical entity linking (Zhu et al., 2022; Liu et al., 2021a; Wu et al., 2020) and cross-lingual biomedical entity linking (XBEL) (Rijhwani et al., 2019; Bitton et al., 2020), which focuses on mapping an input mention from biomedical text in a specific language to its associated entity in a curated multilingual KB. In monolingual biomedical entity linking, mentions always match the KB language, and entities in other languages are discarded. However, the monolingual formulation of biomedical entity linking has the problem that the KB might miss several entries for languages with limited coverage of entity descriptors. The XBEL formulation\nalleviates this problem to some extent and considers mentions expressed in different languages and linked to a monolingual KB (usually English) (Roller et al., 2018; Bitton et al., 2020). Although both the MBEL and XBEL formulations exploit inter-language links to identify entities in other languages, XBEL requires the target KB to be monolingual, which might still miss several entries in the KB.\nIn this work, we focus on MBEL, which is more general since it can link to entities that might not be necessarily represented in the target monolingual KB but in any of the available languages. In particular, we use UMLS (Bodenreider, 2004) as our multilingual biomedical KB, as it is a rich conceptual ontology differentiated by Concept Unique IDs (CUIs) and relations between them across 25 languages. Moreover, we maintain as much language and type information as possible, hence learning the connections between the source language and entity names in different languages."
        },
        {
            "heading": "2.2 Sequence to Sequence Text Generation",
            "text": "The sequence-to-sequence models (Sutskever et al., 2014) learn to generate an output sequence of arbitrary length from an input sequence, and have demonstrated its flexibility and effectiveness in a wide range of tasks compared to classificationbased methods. Early generative models used recurrent neural networks (RNNs) and attention mechanism (Bahdanau et al., 2015) to capture long-term dependencies between the source and the target sequences. Later, the Transformer (Vaswani et al., 2017) was proposed, which is solely based on attention mechanisms, dispensing with recurrence and convolutions, and showed significant performance improvement. Recently, Transformer based pre-trained language models (De Cao et al., 2021; Lewis et al., 2020; Raffel et al., 2020; Liu et al., 2020) provide a new trend to utilize massive text corpora and have become the baseline for most text generation tasks due to their impressive performance.\nClose to this work, mGENRE (Cao et al., 2022) also employed neural generation models for multilingual entity linking, mapping mentions to the Wikipedia titles in multiple languages. However, mGENRE focuses on multilingual entity linking in the general domain and cannot be simply transferred to the biomedical domain. Furthermore, mGENRE applies a sequence-to-structure decod-\ning algorithm whose output sequences contain nonsemantic indicators such as \u201c>>\u201d. However, such non-semantic indicators express little semantic information and have a gap with natural language, which may mislead the decoding process. Unlike the decoding algorithm of mGENRE, Con2GEN generates the associated entities by filling the fixed slots in the predefined natural language template, which allows Con2GEN to handle various entity types and alleviate ambiguity in biomedical tasks.\nFurthermore, GenBioEL (Yuan et al., 2022a) is the first to explore generative EL in the biomedical domain. However, the differences between our work and GenBioEL are as follows: (1) Different decoding objectives: GenBioEL only generates the entity name at the decoding phase, ignoring the multidimensional information of bio-entities and increasing the difficulty of disambiguation. Instead, we proposed a constrained decoding algorithm consisting of a predefined template for multidimensional information injection during inference. (2) Different training manners: GenBioEL pre-trained the model to improve the generalization ability. Instead, we utilized multi-task learning to jointly optimize the contrastive loss and the task loss and meanwhile to strengthen the proposed Con2GEN model.\nIn this paper, we aim to facilitate MBEL with a sequence-to-sequence generation method to capture the dependencies between mentions and entities. Specifically, we propose a novel prompt-based controllable contrastive decoding algorithm, which can guide the generation process using a predefined natural language template and contrastive learning. In this way, the multidimensional knowledge of biomedical entities can be injected and exploited during inference."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Problem Statement",
            "text": "Given a biomedical mention m in free text x of language l, and a multilingual KB (e.g., UMLS) of n entities E = {e1, e2, . . . , en}, the MBEL system will output the target entity et from E. In this work, we assume that each mention has its own golden CUI and each CUI has at least one synonym, regardless of language. Note that the type and language information of the bio-entities is required, which differs from the general domain setting. In the general domain, we assume that each entity descriptor contains a name that concisely describes the entity,\ne.g., the Wikipedia title (with language identifier) can be used to uniquely identify an entity descriptor, while in the biomedical domain entity names with languages are still ambiguous combinations. Note that mapping the output to CUI is a non-trivial task. However, by adding multidimensional information, ambiguity has been greatly reduced. In the experiment, if the combination of entity name, type, and language cannot uniquely identify a CUI, we will randomly select one CUI from them. More details are provided in Appendix A.1."
        },
        {
            "heading": "3.2 Model Architecture",
            "text": "In this work, we frame the MBEL task as a sequence-to-sequence generation problem and the overall architecture is illustrated in Figure 2. Given the token sequence x = x1, . . . ,m, . . . , x|x| of the mention along with contextual information, Con2GEN directly generates the most relevant entity et according to a specific template T (\u00b7) via an encoder-decoder architecture. Concretely, Con2GEN first encodes the input x, and then uses a prompt-based controllable contrastive decoding algorithm to generate the corresponding entity in natural language: a sequence of tokens y. In the following sections, we introduce in detail the neural conditional text generation network and the promptbased controllable contrastive decoding algorithm proposed in this work."
        },
        {
            "heading": "3.3 Neural Conditional Text Generation Network",
            "text": "A neural conditional text generation model (Sutskever et al., 2014) M = (f, g) generates the target sequence y with respect to the source sequence x, where f and g denotes the encoder and decoder, respectively. To this end, Con2GEN first encodes the input biomedical sequence x into the hidden vector representations He = he1, . . . , h e |x| via a multi-layer transformer encoder f :\nHe = f(x1, . . . , x|x|) (1)\nDuring decoding, g utilizes the sequential hidden vectors He to predict the target entity token-bytoken. Specifically, at step i, the token yi and the decoder state hdi are computed as follows:\nyi, h d i = g([H e;hd1, . . . , h d i\u22121]; yi\u22121) (2)\nTherefore, the entire conditional probability p(y|x) of generating the output sequence y given the input sequence x can be progressively calculated as\nfollows:\nP\u03b8(y|x) = |y|\u220f i p\u0398(yi|y<i, x) (3)\nwhere y<i = y1. . . yi\u22121, y is the identifier of entity, and \u0398 denotes the parameters of the model M. Moreover, since the input and the output tokens of our model are both natural language words, we adopt the multilingual pre-trained language model mBART (Liu et al., 2020) as our base model, which will take advantage of the general multilingual text generation knowledge."
        },
        {
            "heading": "3.4 Prompt-based Controllable Decoding",
            "text": ""
        },
        {
            "heading": "3.4.1 Template",
            "text": "By designing an appropriate template, we encourage Con2GEN to better utilize the multidimensional information of entities, therefore, to solve the uninformative problem. The desired prompt not only provides information but also defines the output format. Specifically, we create a natural language template T (\u00b7), in which we list all meaningful elements of an entity in the format of placeholders: T =[Ent] of type [Ent Type] in [Ent Lang]. Among them, [Ent] denotes the entity name. [Ent Type] is the semantic type of the entity and we use an empty string for entities without type information in UMLS. [Ent Lang] denotes the target language."
        },
        {
            "heading": "3.4.2 Controllable Decoding",
            "text": "Naturally, the most straightforward solution for decoding is the greedy algorithm (Germann et al., 2001), which selects the token with the highest probability p at each step i. However, the greedy decoding algorithm may cause invalid entities that are not in the vocabulary of the UMLS. In addition, the greedy decoding algorithm is uncontrollable since it cannot output sequences in the template format we designed, further ignoring the useful knowledge of entities.\nTaking the above needs into consideration, we employ a controllable decoding algorithm based on the trie tree (Cormen et al., 2009; Lu et al., 2021) to only output the valid token from a constrained vocabulary set at each decoding step i. During controllable decoding, the multidimensional information of entities is injected into the predefined template as a prompt for the decoder. Specifically, at each step i, the trie-based decoding algorithm will dynamically traverse the children nodes from\nthe current state. For ease of understanding, we give an example of a complete decoding process by executing a trie tree search in Appendix A.2.\nSince the vocabulary set of UMLS E is very large (over 10M deduplicated entities) and the cost of ranking each entity in E is prohibitive, we further restrict the generation space at inference time. First, we employ the beam search approximate strategy (Sutskever et al., 2014) to search for the top-b entities in E instead of scoring all entities, which greatly improves inference efficiency. Moreover, modern entity linking systems (Ledell Wu, 2020; Zhu et al., 2022) tend to apply candidate retrieval first and then apply a finer-grained reranking instead of scoring all entities at once in order to save computational cost. Inspired by this, we also leverage the SAPBERTmulti (Liu et al., 2021b) to generate top-k candidates with similar surface form for each input mention and further constrain the decoding generation space."
        },
        {
            "heading": "3.5 Training",
            "text": ""
        },
        {
            "heading": "3.5.1 Contrastive Learning",
            "text": "Typically, generation systems are trained with teacher forcing with the ground truth labels in the training phase and are exposed to incorrectly generated tokens during the inference phase, thus encountering the exposure bias problem (An et al., 2022). Although the traditional neural generation method no longer needs negative sampling, the trained model will lose the ability to discriminate\nhard negative samples during inference. Recently, contrastive learning provides a new idea for alleviating the exposure bias problem of generation models by additionally constructing positive and negative pairs during training (An et al., 2022). Contrastive learning optimizes model parameters by learning an efficient representation space for the data, in which positive pairs are pulled together and negative pairs are pushed apart.\nIn this paper, we expect that by contrasting hard\nAlgorithm 1 Contrastive learning training strategy Input: Initialize the generation model M as: \u0398 \u2032 = \u0398, one\ntraining instance as d = (x, y), where x is a input sequence, y is an output sequence\n1: for epoch e = 1 to E do 2: for batch b = 1 to B do 3: Batch data Db = {db1, db2, ..., db|Db|} 4: for dbj \u2208 Db do 5: Randomly select two positive outputs ypj , y p1 j from the synonym set of the same CUI 6: Randomly select three negative outputs yn1j ,\nyn2j , y n3 j from the union of hard negatives and\nsamples with type information replaced 7: Compute representations hpj = \u0398(xj , y p j ),\nhp1j = \u0398(xj , y p1 j ), h n1 j = \u0398(xj , y n1 j ), h n2 j = \u0398(xj , y n2 j ), h n3 j = \u0398(xj , y n3 j )\n8: Compute contrastive loss LCL with hpj , h p1 j ,\nhn1j , h n2 j , h n3 j\n9: end for 10: Compute average batch loss and update the parameter \u0398 11: end for 12: end for\nnegatives, the proposed Con2GEN model can be sensitive to the entity semantics and further alleviate the exposure bias problem. Towards this, we first create the positive and negative pairs for each training sample. Specifically, for a specific training sample (x, y), we keep the input x constant and vary the output y. Then, we filter out the synonyms in different languages with the same CUI as y to build the positive set. Moreover, the negative instances come from two sources: 1) we replace the type information in the template of y in order to create the semantically confusing negative instances; 2) we regard the incorrect candidates retrieved by SAPBERTmulti as hard negative instances.\nIn the training phase, for each training sample (x, y), we randomly select two positive samples (x, yp), (x, yp1) and three negative samples (x, yn1), (x, yn2), (x, yn3) from the additionally constructed positive and negative sets, respectively. Then we adopt the average outputs of M as the instance representation for modeling the contrastive objective. Inspired by InfoNCE (van den Oord et al., 2018), we define an objective LCL in the contrastive manner:\nLCL = \u2212log esim(h\np,hp1 )\ne sim(hp,hp1 )+ \u22113 i=1 esim(h p,hni )\n(4)\nwhere hp = \u0398(x, yp), hp1 = \u0398(x, yp1), hn1 = \u0398(x, yn1), hn2 = \u0398(x, yn2), hn3 = \u0398(x, yn3), \u0398 denotes the parameters of the model M. The details of the contrastive learning training process are described in Algorithm 1."
        },
        {
            "heading": "3.5.2 Training Objective",
            "text": "This section describes the training objective of the proposed Con2GEN model. Specifically, given a training instance (x, y) pair, where x is the source sequence, and y is the target sequence, we first compute the following negative log-likelihood (NLL) loss:\nLNLL = \u2212 |y|\u2211 i=1 log p\u03b8(yi|y<i, x) (5)\nwhere \u03b8 is the parameters of model M. Simultaneously, for each instance, we compute the contrastive loss LCL as described in the previous section. Hence, the final loss function L is the sum of the two, where L = LNLL + LCL. By combining the NLL and contrastive loss in the training phase, the Con2GEN model can be jointly optimized in a multi-task learning manner, which will improve the generalization ability of the model. Moreover,\nwe explored different ways to combine the NLL and contrastive loss. We utilized a hyper-parameter \u201clabel weight\u201d to balance the NLL and contrastive loss, and did a grid search from 0 to 1 with a step size of 0.1. And we found when the label weight equals 0.5, the performance is best on the validation set."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We used three datasets to evaluate the proposed Con2GEN model. Descriptions of the datasets and their statistics are provided in Table 1. XL-BEL (Liu et al., 2021b) contains sentences from 10 language dumps of Wikipedia, with mentions linked to the 2020AA full version of UMLS concepts. The size of the concept set in UMLS is 4, 281, 184, and the size of all surface form/synonym set is 12, 671, 936. And the concepts in UMLS belong to 127 semantic types. For each language, 1, 000 instances are randomly selected for the final test sets, where each evaluation instance consists of a triplet (sentence,mention,CUI). EMEA and Patent are two subsets of the Multilingual Annotation of Named Entities and Terminological Resource Acquisition gold-standard corpora (the Mantra GSC) (Kors et al., 2015), which belong to different text types: European Medicines Agency (EMEA) and Patent. The Mantra GSC contains French, German, Spanish, and Dutch corpora that include medical terms mapped to standard terms in SNOMED-CT, MeSH, and MedDRA of the 2020AA full version of UMLS, including 801, 038 CUIs.\nMoreover, due to the lack of training data for the MBEL task, we utilize the Medmentions English biomedical entity linking dataset (Mohan and Li, 2019) to construct the multilingual contextual training data. In order to solve the problem that the input of training data is all in English, we used the weakly supervised data augmentation method to increase the multilingual characteristics of the training data. Specifically, we swap the English mention of input with the multilingual entity of output. Since the Medmentions dataset only contains English instances, we use this simple method to generate multilingual input. We preserve as many multilingual entities as possible, which plays a role in smoothing multilingual data. We didn\u2019t use any translator in this process, which will not introduce any extra noise. The final constructed training and validation sets contain 3, 136, 568 and 412, 905 in-\nstances, respectively."
        },
        {
            "heading": "4.2 Evaluation Metrics and Baselines",
            "text": "For evaluation, we report Accuracy for evaluating the performance of the proposed model against the baseline models following the previous work (Cao et al., 2022). Appendix A.3 gives detailed descriptions of baselines."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "We used Adam optimizer with a learning rate of 1e \u2212 6, \u03b21 = 0.9, \u03b22 = 0.999, eps= 1e \u2212 8, and with warm up for 500 steps for model training. The dropout rate and attention dropout are set to 0.1 and 0.1. We clip the gradient norm within 0.1. The label smoothing of NLL loss is 0.1 and weight decay is 0.01. We used max 1, 024 tokens and variable batch size(\u2248 15). The training epoch of the model are set to 3. At test time, we use Constrained Beam Search with 7 beams. We implemented our model using the fairseq library (Ott et al., 2019). Note that all parameters are selected based on the best performance on the development set."
        },
        {
            "heading": "4.4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.4.1 Main Results",
            "text": "To evaluate our approach, we compared the proposed Con2GEN model with several SOTA models on the XL-BEL and the Mantra GSC EMEA/Patent datasets and listed the performance of each model in Table 2. As shown in Table 2, our model outperformed all other models, which improved 1.3 Accuracy points on average over the baselines on XL-BEL, EMEA and Patent test sets. We note that the discriminative PLMs (mBERT, XLM-R) underperformed the generative PLM (mBART), which demonstrates the advantage of generative models in solving the indirect problem of MBEL. Since BioPRO (Zhu et al., 2022) is a context-infused method in the monolingual BEL task, we finetuned it on the multilingual training data and reported the results in Table 2. Note that mGENRE (Cao et al., 2022) is a SOTA method in the general domain, which\nwas not compared in most biomedical studies. We compared with both the base and finetuned results of mGENRE in Table 2.\nIn addition, to analyze the contribution of each part of our model, we cumulatively removed components and listed the performance comparison of the proposed model and its variants in Table 2. The w/o type, w/o lang, and w/o type & lang methods represent the use of partial dimensional information of entities rather than multidimensional information, resulting in a drop on performance. Even so, Con2GEN\u2019s variants with partial dimensional information are still strong baselines compared with other baselines. The contrastive mechanism is removed from the w/o CL method, which drops the Accuracy point of test sets compared with Con2GEN. The contrastive learning strategy can identify the hard examples and is demonstrated to be effective on 12 language subsets from Table 2. We believe the improvements are significant since we only trained one single model which outperforms baselines on almost all languages. Although the contrastive learning strategy improved the average accuracy by only 0.2 points, it worked across 9 diverse languages, which is not trivial. Moreover, we added an ablation experiment of Con2GEN without using the controllable decoding algorithm, and the performance dropped significantly, which demonstrates the effectiveness of the proposed controllable decoding. In conclusion, the template with both language and type information is better than other templates in generation and the model that integrates contrastive learning and controllable decoding has the best performance."
        },
        {
            "heading": "4.4.2 Resolution of the Ambiguity Problem",
            "text": "In order to study the resolution of different models on the ambiguity problem, we compared the top 1 candidates prediction\u2019s performance of our proposed model with SAPBERTmulti, which is a surface form matching based MBEL model. As mentioned before, the ambiguous phenomenon is severe in the MBEL task, which exists not only in different contexts but also across languages, in-\ncreasing the challenge of the task. Table 4 lists the number of wrongly predicted top 1 candidates, the number of ambiguous examples in wrongly predicted top 1 candidates, and the proportion of ambiguous instances in the overall top 1 errors on the XL-BEL test set. It is shown that our model can greatly reduce the proportion of ambiguous examples among the total misclassified examples, which demonstrates the effectiveness of the proposed method in addressing the ambiguity challenge and understanding the deep semantics of mentions and entities."
        },
        {
            "heading": "4.4.3 Effect of Multidimensional Information Injection",
            "text": "From Table 2, we observe that mGENRE is a strong baseline even without finetuning on the multilingual biomedical dataset. To further study the potential of mGENRE on the MBEL task, we finetuned it on the training data we constructed. As can be seen from Table 3, \u201c+finetune\u201d method outper-\nforms the base mGENRE model on each XL-BEL test set, which demonstrates the effectiveness of the proposed weakly supervised data augmentation method. Moreover, we assume that the injection of multidimensional information may be useful to the mGENRE generation as well. Thus, we reported the \u201c+finetune+type\u201d results in Table 3, which improves the accuracy by 0.5 on average compared to the base model. Note that the \u201c+finetune+type\u201d variant of mGENRE still underperformed our model, which demonstrates the effectiveness of the proposed natural language template, since both mGENRE and our Con2GEN utilize controllable decoding at inference, yet their output templates differ."
        },
        {
            "heading": "4.4.4 Effect of Contrastive Learning",
            "text": "As shown in Table 2, the proposed model achieved better performance with the contrastive learning strategy compared with the \u201cw/o CL\u201d method on 12 subsets. To further study the effect of contrastive\nlearning, we had initially hoped that the contrastive learning\u2019s capability would make other neural networks work better, and we add it to the mGENRE during finetuning. In Table 3, the improvements of the mGENRE with contrastive learning can be achieved in most languages of the XL-BEL dataset, which demonstrates that our contrastive learning framework supports other sequence-to-sequence models as well."
        },
        {
            "heading": "4.4.5 Case Study",
            "text": "To illustrate the effectiveness of the proposed model, we listed the top 1 candidates retrieved by SAPBERTmulti and our Con2GEN in Figure 3, respectively. We can infer from Figure 3 that SAPBERTmulti prefers entities with similar surface form as the mentions, but may miss real matching\nentities. However, it clearly indicates that the proposed method can make up for this disadvantage and make comprehensive judgments by leveraging the contextual and language information of entities and mentions."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we focus on both the indirect and the uninformative challenges in multilingual biomedical entity linking. Towards this, we propose Con2GEN, a prompt-based controllable contrastive generation method. Our approach outperforms baselines on 3 benchmark datasets, including XLBEL, Mantra GSC EMEA, and Mantra GSC Patent. Further experiments and analysis demonstrate that the controllable contrastive decoding strategy of our model could considerably reduce ambiguity and meanwhile gain high performance.\nLimitations\nOne limitation of this work is that the generative model used in this paper requires a large amount of training data and is computationally expensive. Furthermore, due to the lack of multilingual domainspecific training data, we utilize English text and a multilingual knowledge base to construct the multilingual contextual training data. However, the constructed data may be biased for two reasons. Firstly, the number of non-English entities in the multilingual knowledge base is small, which limits the linguistic diversity of generated data. Secondly, we only replace the English mentions with non-English synonyms, and the remaining context remains in English."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported in part by the National Key Research and Development Program of China (No. 2022ZD0116002), National Natural Science Foundation of China (No. 62006061, 61872113, 62106115, 62276075, 62106114), Science and Technology Planning Project of Shenzhen (No. JCYJ20190806112210067), and Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies, China (No. 2022B1212010005)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Mapping Output to CUIs\nSAPBERTmulti only considers surface form matching and therefore cannot disambiguate the same entity name with different CUIs. However, we utilize multidimensional information, and ambiguity has\nbeen greatly reduced. Moreover, if the gold entity is one of the matching entities, SAPBERTmulti will consider it correct, but we randomly select one CUI from them for evaluation. Thus, if we adopt the evaluation method of SAPBERTmulti, the accuracy scores will improve than what we reported.\nA.2 Example of Trie Tree Search For ease of understanding, we give an example of a complete decoding process by executing a trie tree search in Figure 4. For example, there is a generation path \u201cROOT-\u70ad\u9178-of type-inorganic chemical-in-Japanese\u201d in Figure 4. Correspondingly, there must be a record like \u201cName: \u70ad\u9178, Language: Japanese, Type: inorganic chemical\u201d in UMLS. Among them, the information of name and language can be found in the UMLS file \u201cMRCONSO.RRF\u201d, and the information of type can be found in the UMLS file \u201cMRSTY.RRF\u201d. We only wrote a data processing code to integrate multidimensional information without modification.\nA.3 Descriptions of Baselines For the evaluation of the proposed model, we use the following SOTA baseline methods for comparison.\n\u2022 mBERT (Devlin et al., 2019) is a multilingual contextualized language representation encoder that is pre-trained using bidirectional transformers. Following the previous work (Liu et al., 2021b), we use the pre-trained mBERT to compute vector representations. Specifically, [CLS] of the last layer\u2019s output is used as the final representation. At inference, given a query representation, a nearest neighbor search is used to rank all candidates\u2019 representations.\n\u2022 XLM-R (Conneau et al., 2020) is a transformerbased multilingual masked language model pretrained on text in 100 languages. The use\nof XLM-R in the experiments is the same as mBERT.\n\u2022 mBART (Liu et al., 2020) is a multilingual sequence-to-sequence pre-training method that produces significant performance gains across a wide variety of tasks. We didn\u2019t finetune it on the training set.\n\u2022 BioPRO (Zhu et al., 2022) proposes a two-stage linking algorithm to enhance the monolingual entity representations based on prompt learning. Since BioPRO is a context-infused method in the monolingual BEL task, we finetuned it on the multilingual training data.\n\u2022 SAPBERTmulti (Liu et al., 2021b) proposes a multilingual extension of the self-alignment pre-training technique to improve domainspecialized representations in resource-lean languages. SAPBERTmulti restricted the target UMLS ontology to only include CUIs that appear in WikiMed (62, 531 CUIs, 399, 931 entity names), which greatly reduced the knowledge base size and task difficulty. However, we utilized the 2020AA full version of UMLS for evaluation (4, 281, 184 CUIs, 12, 671, 936 entity names), which is much harder. The results in Table 2 are produced using the officially released code of SAPBERTmulti.\n\u2022 CODER (Yuan et al., 2022b) proposes a pretraining method to use both synonyms and relations from the UMLS to direct the generation of multilingual biomedical term embeddings.\n\u2022 mGENRE (Cao et al., 2022) proposes an autoregressive formulation to the multilingual entity linking problem in the general domain. We compared with both the base and finetuned results of mGENRE in Table 2."
        }
    ],
    "title": "Controllable Contrastive Generation for Multilingual Biomedical Entity Linking",
    "year": 2023
}