{
    "abstractText": "To avoid detection by current NLP monitoring applications, progenitors of hate speech often replace one or more letters in offensive words with homoglyphs, visually similar Unicode characters. Harvesting real-world hate speech containing homoglyphs is challenging due to the vast replacement possibilities. We developed a character substitution scraping method and assembled the Offensive Tweets with Homoglyphs (OTH) Dataset1 (N=90,788) with more than 1.5 million occurrences of 1,281 nonLatin characters (emojis excluded). In an annotated sample (n=700), 40.14% of the tweets were found to contain hate speech. We assessed the performance of seven transformer-based hate speech detection models and found that they performed poorly in a zero-shot setting (F1 scores between 0.04 and 0.52), but normalizing the data dramatically improved detection (F1 scores between 0.59 and 0.71). Training the models using the annotated data further boosted performance (highest micro-averaged F1 score=0.88, using five-fold cross validation). This study indicates that a dataset containing homoglyphs known and unknown to the scraping script can be collected, and that neural models can be trained to recognize camouflaged real-world hate speech.",
    "authors": [
        {
            "affiliations": [],
            "name": "Portia Cooper"
        },
        {
            "affiliations": [],
            "name": "Mihai Surdeanu"
        },
        {
            "affiliations": [],
            "name": "Eduardo Blanco"
        }
    ],
    "id": "SP:149e4945fad429a5ca2b13a9cf0e6eeb5da97057",
    "references": [
        {
            "authors": [
                "Sai Saket Aluru",
                "Binny Mathew",
                "Punyajoy Saha",
                "Animesh Mukherjee."
            ],
            "title": "Deep learning models for multilingual hate speech detection",
            "venue": "arXiv preprint arXiv:2004.06465.",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa-Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval:Unified Benchmark and Comparative Evaluation for Tweet Classification",
            "venue": "Proceedings of Findings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin K. Bergen."
            ],
            "title": "What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves",
            "venue": "Basic Books, New York, NY.",
            "year": 2016
        },
        {
            "authors": [
                "Nicholas P. Boucher",
                "Ilia Shumailov",
                "Ross Anderson",
                "Nicolas Papernot."
            ],
            "title": "Bad characters: Imperceptible nlp attacks",
            "venue": "2022 IEEE Symposium on Security and Privacy (SP), pages 1987\u20132004.",
            "year": 2021
        },
        {
            "authors": [
                "Perry Deng",
                "Cooper Linsky",
                "Matthew Wright."
            ],
            "title": "Weaponizing unicodes with deep learning identifying homoglyphs with weakly labeled data",
            "venue": "2020 IEEE International Conference on Intelligence and Security Informatics (ISI).",
            "year": 2020
        },
        {
            "authors": [
                "Diane Felmlee",
                "Paulina Inara Rodis",
                "Amy Zhang."
            ],
            "title": "Sexist slurs: Reinforcing feminine stereotypes online",
            "venue": "Sex Roles, 83.",
            "year": 2020
        },
        {
            "authors": [
                "Avi Ginsberg",
                "Cui Yu."
            ],
            "title": "Rapid homoglyph prediction and detection",
            "venue": "2018 1st International Conference on Data Intelligence and Security (ICDIS), pages 17\u201323.",
            "year": 2018
        },
        {
            "authors": [
                "Keita Kurita",
                "Anna Belova",
                "Antonios Anastasopoulos."
            ],
            "title": "Towards robust toxic content classification",
            "venue": "ArXiv, abs/1912.06872.",
            "year": 2019
        },
        {
            "authors": [
                "Thai Le",
                "Jooyoung Lee",
                "Kevin Yen",
                "Yifan Hu",
                "Dongwon Lee."
            ],
            "title": "Perturbations in the wild: Leveraging human-written text perturbations for realistic adversarial attack and defense",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Joon Sern Lee",
                "Yam Gui Peng David",
                "Jin Hao Chan."
            ],
            "title": "Phishgan: Data augmentation and identification of homoglpyh attacks",
            "venue": "CCCI.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Nikola Ljube\u0161i\u0107",
                "Darja Fi\u0161er",
                "Toma\u017e Erjavec."
            ],
            "title": "The frenk datasets of socially unacceptable discourse in slovene and english",
            "venue": "International Conference on Text, Speech and Dialogue.",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Maneriker",
                "Jack W. Stokes",
                "Edir Garcia Lazo",
                "Diana Carutasu",
                "Farid Tajaddodianfar",
                "Arun Gururajan."
            ],
            "title": "Urltran: Improving phishing url detection using transformers",
            "venue": "MILCOM 2021 - 2021 IEEE Military Communications Conference (MIL-",
            "year": 2021
        },
        {
            "authors": [
                "Hiroaki Suzuki",
                "Daiki Chiba",
                "Yoshiro Yoneya",
                "Tatsuya Mori",
                "Shigeki Goto."
            ],
            "title": "Shamfinder: An automated framework for detecting idn homographs",
            "venue": "Proceedings of the Internet Measurement Conference, IMC \u201919, page 449\u2013462, New York, NY, USA.",
            "year": 2019
        },
        {
            "authors": [
                "Bertie Vidgen",
                "Tristan Thrush",
                "Zeerak Waseem",
                "Douwe Kiela."
            ],
            "title": "Learning from the worst: Dynamically generated datasets to improve online hate detection",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Max Wolff",
                "Stuart Wolff."
            ],
            "title": "Attacking neural text detectors",
            "venue": "ArXiv, abs/2002.11768.",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Woodbridge",
                "Hyrum S. Anderson",
                "Anjum Ahuja",
                "Daniel Grant."
            ],
            "title": "Detecting homoglyph attacks with a siamese neural network",
            "venue": "2018 IEEE Security and Privacy Workshops (SPW), pages 22\u201328.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Disclaimer: This paper includes language that some readers might find offensive.\nHate speech, discriminatory language against individuals or groups based on race/ethnicity, nationality, gender, religion, LGBTQ+ identity, or disability status, is banned by Facebook, YouTube, and other major platforms. A common strategy to mask hate speech is replacing one or more letters in offensive words with homoglyphs, Unicode characters that are visually homogeneous (Boucher et al.,\n1https://github.com/pcoopercoder/ Offensive-Tweets-with-Homoglyphs-OTH-Dataset\n2021). For instance, the Latin \u201ca\u201d (U+0061) and the Cyrillic \u201c\u0430\u201d (U+0430) are nearly indistinguishable to the human eye, yet they belong to different Unicode character families. Currently, there are almost 150,000 Unicode characters,2 which presents extensive substitution possibilities.\nDespite the prevalence of online hate speech containing homoglyphs, the collection of organically generated data of this type is rare. To remedy this scarcity, we developed a dataset of real-world offensive text containing homoglyphs and used this data to train existing hate speech detection models. In particular, our contributions are:\n\u2022 Developing a novel character substitution scraping method used to assemble a dataset of 90,788 tweets with offensive words containing homoglyphs. To our knowledge, our dataset is the first to be composed of realworld, homoglyph-laden texts.\n\u2022 Evaluating the effectiveness of replacing Latin characters with homoglyphs as an obfuscation strategy by testing the zero-shot performance of seven open-source transformer-based hate speech detection models hosted by Hugging Face on three versions of an annotated sample of the dataset: (1) original with homoglyphs, (2) partially normalized (Cyrillic homoglyphs replaced with the corresponding Latin letters), and (3) fully normalized (all non-Latin homoglyphs replaced with the corresponding Latin characters).\n\u2022 Demonstrating that models can be trained to recognize real-world hate speech containing homoglyphs known and unknown to the scraping script.\n2https://www.unicode.org/versions/stats/chart_ charbyyear.html"
        },
        {
            "heading": "2 Related Work",
            "text": "Previous work that explored human-generated text with homoglyphs includes the ANTHRO algorithm, which evaluated the phonetic properties of words to locate character-based perturbations, including homoglyphs (Le et al., 2022). Boucher et al. (2021) used homoglyphs, letter reordering, and letter deletion to test the detection capabilities of NLP models. Additionally, Kurita et al. (2019) artificially produced offensive text containing homoglyphs to simulate adversarial attacks.\nWoodbridge et al. (2018) investigated obfuscation mitigation using a Siamese convolutional neural network to convert homoglyphs into the characters they resembled. Similarly, Ginsberg and Yu (2018) proposed a \u201cvisual scanning\u201d method to detect and predict homoglyphs. Other work has inventoried homoglyphs (Suzuki et al., 2019) and identified previously unknown homoglyphs (Deng et al., 2020).\nFinally, several studies have evaluated phishing attacks in which homoglyphs were used to imitate corporate domain names to deceive users and extract personal information (Maneriker et al., 2021; Lee et al., 2020; Wolff and Wolff, 2020). These studies involved training models using artificially created data containing homoglyphs and did not evaluate real-world content."
        },
        {
            "heading": "3 Approach",
            "text": ""
        },
        {
            "heading": "3.1 Scraping Twitter",
            "text": "Tweets were collected from Twitter (now renamed X) in December 2022 using the Python Tweepy Library (computational cost detailed in Appendix A.1). Scripting was used to create query terms derived from the 41 most offensive American English words ranked by native English-speaking college students at a large U.S. metropolitan university (Bergen, 2016) (list in Appendix A.2). Query terms were generated by replacing each Latin letter in the offensive words with the corresponding Cyrillic homoglyph(s) from the Unicode Confusable Standard3 (Table 1). The Latin letters \"b\", \"h\", \"i\", \"w\", and \"y\" had multiple Cyrillic homoglyphs, thus words containing these letters generated more than one query term for each letter. For example, the word \u201cbitch\u201d yielded eight variations (\u0432itch, \u042citch, bitch, bItch, bi\u0442ch, bit\u0441h, bitc\u043d, and bitc\u04bb). The\n3https://www.unicode.org/Public/security/ revision-03/confusablesSummary.txt\nLatin letters \"f\", \"l\", and \"z\" had no Cyrillic homoglyphs thus yielded no variations. For example, the word \u201cfuck\u201d yielded only three query terms (f\u0446ck, fu\u0441k, and fuc\u043a). This process produced 247 query terms that included 26 of the 28 Cyrillic homoglyphs for Latin letters, as \"v\" and \"x\" were not present in the 41 offensive words used to generate the query terms.\nA total of 93,042 tweets were collected, including 2,254 duplicates created by the presence of multiple query terms in a single tweet. Duplicate tweets were removed, resulting in 90,788 tweets, which we named the Offensive Tweet with Homoglyphs (OTH) Dataset. Figure 1 provides an example of a homoglyph-laden tweet included in the OTH Dataset. The metadata for each tweet was collected from Twitter for aggregate analysis purposes. We also calculated the number of unique non-Latin characters (emojis excluded) present in the dataset and the number of times each was detected."
        },
        {
            "heading": "3.2 Annotation",
            "text": "From the OTH Dataset, a random sample of 700 tweets was selected. Using a detailed codebook (Appendix A.3), two human annotators independently evaluated the tweets in the sample (annotator\ninformation in Appendix A.4), and an IRB exemption for research not involving human participants was granted.\nIntercoder agreement exceeded 96.00% for all codes, and Cohen\u2019s Kappa ranged from 0.80 to 1.00 (Appendix A.5, Table 5). Coding disagreements were discussed and reconciled."
        },
        {
            "heading": "3.3 Tweet Normalization",
            "text": "A survey of existing homoglyph normalization resources was performed, and widely used Python libraries were evaluated (Appendix A.6). No tool was found which automated the conversion of all possible homoglyphs into the Latin characters they resemble. Normalization in the present study was accomplished using scripting and manual compilation. Two normalized versions of the annotated sample were created: (1) partially normalized (Cyrillic homoglyphs replaced with the corresponding Latin letters using custom scripting) and (2) fully normalized (all non-Latin homoglyphs replaced with the corresponding Latin characters using manual compilation)."
        },
        {
            "heading": "3.4 Base Models",
            "text": "The original, partially normalized, and fully normalized versions of the annotated sample were run separately through seven open-source transformerbased hate speech detection models hosted by Hugging Face (computational cost detailed in Appendix A.7). All selected models were trained (at least in part) on hate speech collected from social media platforms (a summary of each of the utilized models is included in Appendix A.8). Accuracy, precision, recall, and F1-score were calculated."
        },
        {
            "heading": "3.5 Five-Fold Cross Validation",
            "text": "Finally, five-fold cross validation was performed on each of the seven models using the original version of the annotated sample to evaluate the extent to which the models learned when exposed to realworld data with homoglyphs (computational cost\ndetailed in Appendix A.7). The micro-averages of accuracy, precision, recall, and F1-score across the five folds were calculated."
        },
        {
            "heading": "4 Results",
            "text": ""
        },
        {
            "heading": "4.1 OTH Dataset",
            "text": "The 90,788 tweets in the OTH Dataset were posted by 31,878 unique author IDs. The dataset included 1,264,406 occurrences of 27 Cyrillic homoglyphs for Latin letters [only the Cyrillic \"q\" (U+051B) was not detected] (Table 2). Importantly, 72 additional Cyrillic characters occurred 125,973 times, and 1,182 other non-Latin characters (emojis excluded) occurred 136,346 times. The dataset included an average of 16.82 Cyrillic and other nonLatin characters per tweet.\nAs shown in Figure 2, the homoglyphs with the highest number of occurrences were the Cyrillic \"\u0435\" (n=295,039, U+0435), \"\u043e\" (n=238,643, U+043E), and \"\u0430\" (n=225,668, U+0430). The non-Cyrillic homoglyphs with the highest number of occurrences were \"\u00ed\" (n=26,246, U+00ED), \"\u03c3\" (n=24,730, U+03C3), and \"\u03b1\" (n=23,465, U+03B1).\nOf the 247 query terms searched, 156 returned tweets. The earliest tweet in the OTH Dataset was posted on February 19, 2009 (31 months after the inception of Twitter), and the most recent tweet was posted on December 20, 2022. The tweets were assigned 43 language identifiers by Twitter, and the majority were classified as Czech (62.04%). Only 24.27% were classified as English by Twitter (Appendix A.9, Table 6), even though all tweets\nin the OTH Dataset included at least one Englishlanguage query term."
        },
        {
            "heading": "4.2 Annotated Sample",
            "text": "The annotated sample resembled the OTH Dataset in terms of character composition and included an average of 17.22 Cyrillic and other non-Latin characters per tweet.\nIn the annotated sample, 40.14% of tweets were classified as hate speech by human annotators. Most of these tweets (87.54%) included misogynistic hate speech. Tweets referencing hate speech related to LGBTQ+ identity (5.34%), race/ethnicity (3.91%), disability status (1.42%), and religion (1.07%) were less common. No hate speech related to nationality was found in the sample. Additionally, 97.71% of tweets were classified as offensive; 20.43% were labeled sexually explicit; and 5.14% referenced violence or aggressive acts. The body text of the vast majority of tweets (92.43%) was classified as English by the annotators."
        },
        {
            "heading": "4.3 Zero-Shot Model Performance",
            "text": "On the original version of the annotated sample with homoglyphs, the F1-scores of the seven hate speech detection models ranged from 0.04 to 0.52 (Table 3). On the partially normalized version of the annotated sample (Cyrillic homoglyphs replaced with the corresponding Latin letters), F1scores ranged from 0.52 to 0.67. On the fully normalized version of the annotated sample (all nonLatin characters replaced with the corresponding Latin characters), F1-scores ranged from 0.59 to 0.71."
        },
        {
            "heading": "4.4 Five-Fold Cross Validation",
            "text": "In the five-fold cross validation on the annotated sample with homoglyphs, F1-scores ranged from 0.41 to 0.88 (Table 4)."
        },
        {
            "heading": "5 Discussion",
            "text": "Our character substitution scraping method yielded 90,788 tweets containing more than 1.5 million occurrences of 1,281 non-Latin characters (emojis\nexcluded). The search strategy used Cyrillic homoglyphs for Latin letters to assemble a broader collection of non-Latin characters. As expected, the bulk of the OTH Dataset (82.82%) was comprised of Cyrillic homoglyphs. However, 262,319 occurrences of 1,254 other non-Latin characters were also captured, including 72 additional Cyrillic characters and 1,182 characters from other Unicode character families. The most common Cyrillic characters were \"\u0430\" (U+0430), \"\u0435\" (U+0435), and \"\u043e\" (U+043E), which are homoglyphs for the Latin letters \"a\", \"e\", and \"o.\" The most common non-Cyrillic characters were \"\u03b1\" (U+03B1), \"\u00ed\" (U+00ED), and \"\u03c3\" (U+03C3), which are homoglyphs for the Latin letters \"a\", \"i\", and \"o\". These results may reflect malicious users\u2019 preference for homoglyphs that mimic Latin vowels.\nThe noise produced by the homoglyphs impeded the performance of all seven hate speech detection models tested. Fully normalizing the data by replacing all non-Latin characters with the corresponding Latin characters dramatically improved model performance. Most notably, Model 1\u2019s F1score jumped from 0.04 to 0.60.\nIn the five-fold cross validation, five models achieved F1-scores that exceeded 0.85. Conversely, Model 2 performed poorly (F1-score = 0.41). This may be related to the model\u2019s original training data, which included only hate speech labeled as LGBTQ+ identity and nationality\u2013categories of hate speech which were rare within the annotated sample in the present study. The performance of the other six models demonstrates that neural classifiers can be trained to recognize real-world hate speech masked by homoglyphs. This is an exciting result, considering that the dataset contains many homoglyphs unknown to the scraping script, and thus these perturbations could not be addressed through deterministic normalization.\nThe annotated sample analyzed in the present study included a 40%-60% split between tweets with and without hate speech. The large volume of hate speech tweets with misogynistic content (87.54%) found in the annotated sample is notable. This result is consistent with a prior survey of Twitter that found 419,000 female slurs were posted on average each day (Felmlee et al., 2020).\nHomoglyphs also appeared to interfere with Twitter\u2019s internal system that classifies the body text language of tweets. In the annotated sample, Twitter assigned the English language identifier\nto only 21.00% of tweets, but human annotators found that 92.43% of the tweets were written in English. Twitter classified the majority of tweets in the annotated sample (60.50%) and the OTH Dataset (62.04%) as Czech. These results are especially interesting because the Czech language does not include Cyrillic characters, which were used to generate the query terms."
        },
        {
            "heading": "6 Conclusion",
            "text": "Infusing hate speech with homoglyphs was found to be an effective strategy to avoid detection. In the present study, existing hate speech detection models were ineffective against obfuscation of this type. The OTH Dataset offers training data to build hate speech detection models inoculated against real-world homoglyphs."
        },
        {
            "heading": "7 Limitations",
            "text": "Cyrillic homoglyphs for Latin letters were used to generate the query terms in the present study because they are widely used and were exempt from the normalization that Twitter performs on query terms prior to searching its repository. A more diverse dataset may be achieved by expanding the search strategy to include (1) homoglyphs from multiple Unicode character families, (2) complex homoglyph substitution combinations in query terms, and (3) a broader list of offensive words to generate query terms."
        },
        {
            "heading": "8 Ethical Consideration",
            "text": "Due to the offensive nature of the OTH Dataset and the data sharing policy of X (Twitter), only the Tweet IDs of the OTH Dataset will be published."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Computational Costs of Scraping Twitter\nAn estimated 48 total hours were necessary to compile the OTH Dataset from the Twitter corpus. Tweet scraping was performed locally.\nA.2 41 Most Offensive American English Words Reported by Bergen (2016)\nasshole, bastard, bitch, blowjob, buttfuck, chink, clit, cock, cocksucker, cunt, dick, dumb, dyke, fag, fuck, gay, goddamn, gook, homo, hooker, kike, lesbo, loser, moron, motherfucker, nigger, nutsack, prick, pussy, queer, retard, rimjob, shit, shithead, skank, slut, sodomize, spic, tits, twat, whore\nA.3 Codebook for Annotating Sample of the OTH Dataset\n1. English Lanugage Body Text Includes predominately English language body text. 0=No 1=Yes\n2. Sexually Explicit Content Contains graphic sexual description of genitalia or sex acts, such as intercourse, oral sex, and masturbation. 0=No 1=Yes\n3. References to Violence or Aggressive Acts References physical violence or aggressive behavior. 0=No 1=Yes\n4. Offensive Content Contains profanity or rude language. This category includes hate speech, as well as any other content that is offensive. 0=No 1=Yes\n5. Hate Speech Contains rhetoric that is derogatory or promotes, rationalizes, or reinforces hatred towards a target group or individual based on protected characteristics. Protected characteristics include gender, race/ethnicity, LGBTQ+ identity, nationality, disability status, and religion. Slurs that are used to attack people based on their protected characteristics should be classified as hate speech. 0=No 1=Yes\n6. Category of Hate Speech (Select All That Apply) Complete only for tweets classified as hate speech (Codebook Section 5).\n(a) Gender - hate speech pertaining to a group of people classified under a specific gender identity. 0=No 1=Yes\nComplete only for tweets classified as gender hate speech\ni. Misogyny - hate speech that exhibits hatred of, contempt for, or prejudice against women 0=No 1=Yes\n(b) Race/Ethnicity - hate speech pertaining to a group of people who possess a shared cultural background. 0=No 1=Yes\n(c) LGBTQ+ identity - hate speech directed at the LGBTQ+ community. 0=No 1=Yes\n(d) Nationality - hate speech pertaining to people from a specific nation. 0=No 1=Yes\n(e) Disability Status - hate speech pertaining to people who have physical or mental conditions that limits their movements, senses, or activities. 0=No 1=Yes\n(f) Religion - hate speech pertaining to the worship of God or other superhuman entities.\n0=No 1=Yes\nA.4 Annotators for Random Sample of the OTH Dataset\nThe lead investigator and an experienced annotator coded the annotated sample of the OTH Dataset. No identifying information from the annotators was collected.\nA.5 Intercoder Agreement\nA.6 Evaluation of Existing Normalization Tools\n1. Unidecode library4 focuses on transliteration conversions rather than homoglyph normalization. This introduces several character edge cases. For example, the Cyrillic \u201c\u043f\u201d (U+043F), which is classified as a homoglyph for the Latin \u201cn\u201d (U+006E) by the Unicode Confusable standard, is normalized by Unidecode to the Latin letter \u201cp\u201d (U+0070).\n2. cyrtranslit library5 only performs bidirectional Cyrillic to Latin text and vice versa. As shown in Table 2, the OTH Dataset contains 1,182 unique non-Cyrillic Unicode characters.\n3. confusable_homoglyphs library6 focuses on confusable detection as opposed to homoglyph normalization. It offers no normalization function/utility.\n4https://pypi.org/project/Unidecode/ 5https://pypi.org/project/cyrtranslit/ 6https://pypi.org/project/confusable_\nhomoglyphs/\n4. confusables library7 is an expanded version of the confusable_homoglyphs library, and it includes a normalization function. But it struggled to normalize select characters such as the Cyrillic \u201c\u043f\u201d (U+043F), a homoglyph for the Latin \"n\" (U+006E).\n5. homoglyphs library8 includes homoglyph normalization capabilities via a to_ascii() function. Unfortunately, to_ascii() deletes any characters \"which can\u2019t be converted by default.\" This resulted in the deletion of most Cyrillic homoglyphs.\nA.7 Computational Costs of Evaluating and Fine-Tuning Models\nDue to the small size of the annotated sample, an estimated maximum four total hours of GPU usage were necessary for this study. Models were run using Cloud-based GPU resources.\nA.8 Selected Open-Source Transformer-Based Hate Speech Detection Models\n1. RoBERTa-base binary classification model trained on 58 million tweets (Barbieri et al., 2020)\n2. RoBERTa-base binary classification model by Liu et al. (2019) trained on the English subset of the FRENK Dataset (Ljube\u0161ic\u0301 et al., 2019)\n3. BERT-base binary classification model trained on data from Twitter and Stormfront, a popular white supremacist forum (Aluru et al., 2020)\n4. RoBERTa-base binary classification model trained on 11 English hate speech datasets (Vidgen et al., 2021)\n5. RoBERTa-base binary classification model trained on 11 English hate speech datasets and Round 1 of the Dynamically Generated Hate Speech Dataset (Vidgen et al., 2021)\n6. RoBERTa-base binary classification model trained on 11 English hate speech datasets and Rounds 1 and 2 of the Dynamically Generated Hate Speech Dataset (Vidgen et al., 2021)\n7. RoBERTa-base binary classification model trained on 11 English hate speech datasets\n7https://pypi.org/project/confusables/ 8https://pypi.org/project/homoglyphs/\nand Rounds 1, 2, and 3 of the Dynamically Generated Hate Speech Dataset (Vidgen et al., 2021)\nA.9 Language Identifiers Assigned by Twitter to Dataset"
        }
    ],
    "title": "Hiding in Plain Sight: Tweets with Hate Speech Masked by Homoglyphs",
    "year": 2023
}