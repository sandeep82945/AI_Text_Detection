{
    "abstractText": "Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce TOXICCHAT, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of TOXICCHAT. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, TOXICCHAT can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zi Lin"
        },
        {
            "affiliations": [],
            "name": "Zihan Wang"
        },
        {
            "affiliations": [],
            "name": "Yongqi Tong"
        },
        {
            "affiliations": [],
            "name": "Yangkun Wang"
        },
        {
            "affiliations": [],
            "name": "Yuxin Guo"
        },
        {
            "affiliations": [],
            "name": "Yujia Wang"
        },
        {
            "affiliations": [],
            "name": "Jingbo Shang"
        }
    ],
    "id": "SP:a0f2fab383c4508d872693e5ef14541f6ae892b5",
    "references": [
        {
            "authors": [
                "Tommaso Caselli",
                "Valerio Basile",
                "Jelena Mitrovic",
                "Michael Granitzer."
            ],
            "title": "Hatebert: Retraining BERT for abusive language detection in english",
            "venue": "CoRR, abs/2010.12472.",
            "year": 2020
        },
        {
            "authors": [
                "Tommaso Caselli",
                "Valerio Basile",
                "Jelena Mitrovi\u0107",
                "Michael Granitzer."
            ],
            "title": "HateBERT: Retraining BERT for abusive language detection in English",
            "venue": "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17\u201325, Online. As-",
            "year": 2021
        },
        {
            "authors": [
                "Hyojin Chin",
                "Mun Yong Yi."
            ],
            "title": "Should an agent be ignoring it? a study of verbal abuse types and conversational agents\u2019 response styles",
            "venue": "Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, pages 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "Amanda Cercas Curry",
                "Gavin Abercrombie",
                "Verena Rieser."
            ],
            "title": "Convabuse: Data, analysis, and benchmarks for nuanced detection in conversational AI",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Amanda Cercas Curry",
                "Verena Rieser."
            ],
            "title": " metoo alexa: how conversational systems respond to sexual harassment",
            "venue": "Proceedings of the second acl workshop on ethics in natural language processing, pages 7\u201314.",
            "year": 2018
        },
        {
            "authors": [
                "Amanda Cercas Curry",
                "Verena Rieser."
            ],
            "title": "A crowd-based evaluation of abuse response strategies in conversational agents",
            "venue": "arXiv preprint arXiv:1909.04387.",
            "year": 2019
        },
        {
            "authors": [
                "Paula Fortuna",
                "Juan Soler",
                "Leo Wanner."
            ],
            "title": "Toxic, hateful, offensive or abusive? what are we really classifying? an empirical analysis of hate speech datasets",
            "venue": "Proceedings of the 12th language resources and evaluation conference, pages 6786\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "venue": "ArXiv, abs/2009.11462.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Yulia Tsvetkov."
            ],
            "title": "Fortifying toxic speech detectors against veiled toxicity",
            "venue": "arXiv preprint arXiv:2010.03154.",
            "year": 2020
        },
        {
            "authors": [
                "Camille Harris",
                "Matan Halevy",
                "Ayanna Howard",
                "Amy Bruckman",
                "Diyi Yang."
            ],
            "title": "Exploring the role of grammar and word choice in bias toward african american english (aae) in hate speech classification",
            "venue": "2022 ACM Conference on Fairness, Accountability,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar"
            ],
            "title": "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "year": 2022
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Matthijs Douze",
                "H\u00e9rve J\u00e9gou",
                "Tomas Mikolov"
            ],
            "title": "Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651",
            "year": 2016
        },
        {
            "authors": [
                "Ian D Kivlichan",
                "Zi Lin",
                "Jeremiah Liu",
                "Lucy Vasserman."
            ],
            "title": "Measuring and improving modelmoderator collaboration using uncertainty estimation",
            "venue": "arXiv preprint arXiv:2107.04212.",
            "year": 2021
        },
        {
            "authors": [
                "Anna Koufakou",
                "Endang Wahyu Pamungkas",
                "Valerio Basile",
                "Viviana Patti."
            ],
            "title": "HurtBERT: Incorporating lexical features with BERT for the detection of abusive language",
            "venue": "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 34\u201343,",
            "year": 2020
        },
        {
            "authors": [
                "Stefano Leone"
            ],
            "title": "Rotten tomatoes movies and critic reviews dataset",
            "year": 2020
        },
        {
            "authors": [
                "Xiaojuan Ma",
                "Emily Yang",
                "Pascale Fung."
            ],
            "title": "Exploring perceived emotional intelligence of personality-driven virtual agents in handling user challenges",
            "venue": "The World Wide Web Conference, pages 1222\u20131233.",
            "year": 2019
        },
        {
            "authors": [
                "Debora Nozza",
                "Claudia Volpetti",
                "Elisabetta Fersini."
            ],
            "title": "Unintended bias in misogyny detection",
            "venue": "IEEE/WIC/ACM International Conference on Web Intelligence, WI \u201919, page 149\u2013155, New York, NY, USA. Association for Computing Machinery.",
            "year": 2019
        },
        {
            "authors": [
                "John Pavlopoulos",
                "Jeffrey Sorensen",
                "Lucas Dixon",
                "Nithum Thain",
                "Ion Androutsopoulos"
            ],
            "title": "Toxicity detection: Does context really matter? arXiv preprint arXiv:2006.00998",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ishan Sanjeev Upadhyay",
                "KV Aditya Srivatsa",
                "Radhika Mamidi."
            ],
            "title": "Towards toxic positivity detection",
            "venue": "Proceedings of the tenth international workshop on natural language processing for social media, pages 75\u201382.",
            "year": 2022
        },
        {
            "authors": [
                "Kunze Wang",
                "Dong Lu",
                "Caren Han",
                "Siqu Long",
                "Josiah Poon."
            ],
            "title": "Detect all abuse! toward universal abusive language detection models",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6366\u20136376, Barcelona,",
            "year": 2020
        },
        {
            "authors": [
                "Zeerak Waseem",
                "Dirk Hovy."
            ],
            "title": "Hateful symbols or hateful people? predictive features for hate speech detection on twitter",
            "venue": "Proceedings of the NAACL",
            "year": 2016
        },
        {
            "authors": [
                "Marcos Zampieri",
                "Shervin Malmasi",
                "Preslav Nakov",
                "Sara Rosenthal",
                "Noura Farra",
                "Ritesh Kumar."
            ],
            "title": "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
            "venue": "Proceedings of the 13th International",
            "year": 2019
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "year": 2023
        },
        {
            "authors": [
                "Xuhui Zhou."
            ],
            "title": "Challenges in automated debiasing for toxic language detection",
            "venue": "University of Washington.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The field of conversational AI has seen a major shift with the development of large language model (LLM)-based chatbots like ChatGPT. While these chatbots have demonstrated remarkable capabilities in generating human-like responses, the risk of undesired content emerging in this interface has become one of the most urgent issues recently. Therefore, it is important to equip these chatbots with effective mechanisms to identify potentially harmful contents that goes against their policies.\nToxicity detection has long been investigated as an natural language processing problem (Curry and Rieser, 2018, 2019; Chin and Yi, 2019; Ma\n\u2217Equal Constributions.\net al., 2019), while existing work mainly focused on data derived from social media or generated by LLMs, few efforts have been made towards realworld user-AI conversations (Curry et al., 2021). However, it is noted that the content of user interactions varies substantially between chatbots versus public platforms. For example, while social media users typically post their views directly, chatbot interactions often involve users posing questions or giving instructions. As a result, existing models may fail to recognize the completely new style and more implicit content of toxicity underlying the users\u2019 seemingly friendly questions or instructions. Hence, it is of critical importance to develop toxicity benchmarks rooted in real-world user-AI dialogues, which can help develop a better conversational AI system for addressing toxic behavior embedded within this specific conversation context.\nIn this work, we conduct a benchmark study focused on toxicity in real-world user-AI interactions. We create a comprehensive toxicity benchmark TOXICCHAT based on real chat data between users and AI, which can be utilized to understand user behaviors and improve the performance of AI systems in detecting toxicity for chatbots. Our work can be summarized into three stages:\nFirst, Section 2 introduces the construction of TOXICCHAT, a dataset that collects 10,166 examples with toxicity annotations. Specifically, we collect and pre-process user inputs from the demo based on a popular open-source chatbot Vicuna (Zheng et al., 2023), and apply an uncertaintyguided human-AI collaborative annotation scheme, which successfully releases around 60% annotation workload while maintaining the reliability of labels. During annotation, we take an in-depth analysis based on the data, and discover a specific phenomena where chatbots are being subjected to prompt hacks that will induce them to ignore their policies and generate toxic content (see examples in Appendix C). This phenomenon is referred to as\njailbreaking, and we create an special label for this implicit toxic case.\nSecond, in section 3, we evaluate several baseline models and a model trained on 5 different toxicity datasets from previous work. We find that existing models fail to generalize to our toxicity benchmark and works significantly poorly on jailbreaking cases. This is mainly due to the domain inconsistency between their training data and realworld user prompts in chatbots (see some examples in Figure 1).\nFinally, in Section 4, we take an ablation study towards existing toxicity datasets compared to TOXICCHAT and find that the model trained on our benchmark always performs the best on our real-world validation set, even the data size of previous datasets are 10 times larger. We also observe that utilizing model output can help to recognize the special case of jailbreaking.\nIn summary, TOXICCHAT is the first toxicity dataset based on real-world user-AI conversations. Its unique nature positions it to be a pivotal resource in the development of more robust and nuanced toxicity detection models for real-world userAI conversations. Our analysis based on TOXICCHAT sheds light on challenges and insights of toxicity detection in this field that future research needs to overcome1.\n1The dataset can be download at https://huggingface. co/datasets/lmsys/toxic-chat\n1The example detection model in the Figure 1 is HateBERT (Caselli et al., 2020), which consists of three submodels for hate, abuse and offensive speech."
        },
        {
            "heading": "2 TOXICCHAT Construction",
            "text": "We collected data via an online demo based on Vicuna, which was trained by fine-tuning LLaMA (Touvron et al., 2023) on user-shared conversations collected from ShareGPT3. Vicuna demo is one of the most currently used community chatbot system, and the data was gathered at the consent of users. For safety measures we took, please refer to our Ethical Consideration section. We randomly sampled a portion from the user-AI interactions in the time range from March 30 to April 12, 2023. We conduct data-preprocessing including (1) removing uninformative and noisy contents; (2) removing the too few non-Enlgish inputs; (3) personal identifiable information removal (more details in Appendix B). Also, all studies in this work currently only focus on the first round of interaction between human and AI."
        },
        {
            "heading": "2.1 Annotation Scheme and Guidelines",
            "text": "The dataset is annotated by 4 researchers in order to obtain high quality annotations. All researchers speak fluent English. Labels are based on the definitions for undesired content in (Zampieri et al., 2019), and the annotators adopt a binary value for toxicity label (0 means non-toxic, and 1 means toxic). The final toxicity label is determined though a (strict) majority vote (>=3 annotators agree on the label). Our target is to collect a total of around 10K data for the TOXICCHAT benchmark that follows the true distribution of toxicity in the real-world user-AI conversations.\nThe annotators were asked to first annotate a common set of 720 data as a trial. The interannotator agreement is 96.11%, and the toxicity rate is 7.22%.\nDuring the initial round of annotation, We also notice a special case of toxic inputs where the user is deliberately trying to trick the chatbot into generating toxic content but involves some seemingly harmless text. An example can be found in the last box in Figure 1, and more examples are shown in Appendix C. Following conventions in the community, we call such examples \u201cjailbreaking\u201d queries. We believe such ambiguous text, created to intentionally cheat chatbots, might also be hard for toxicity detection tools and decide to add an extra label for this special toxic example.\n3https://sharegpt.com/"
        },
        {
            "heading": "2.2 Human-AI collaborative Annotation Framework",
            "text": "Annotating a larger scale of toxicity dataset can be painstaking and time-consuming. Inspired by Kivlichan et al. (2021), we explore a way to reduce the annotation workload by asking two questions: (1) whether off-the-shelf moderation APIs can annotate toxicity on our data properly? (2) If not, can we partially reply on moderation API using some heuristic signals? Our evaluation results on trial data mentioned in Section 2.1 indicate that common moderation APIs fail to identify toxicity with an threshold that can yield good separation between toxic and non-toxic examples. However, the confidence score of the moderation exhibits a considerable level of reliability, i.e., higher probability generally corresponds to higher likelihood of toxicity.\nGiven these findings, we utilize a human-AI collaborative annotation framework for a more efficient annotation process (more details can be found in Appendix D). Since only a small portion of userAI interactions are toxic based on the trial study, we leverage toxicity detection models to filter out a portion of data that deems non-toxic with high confidence by the moderation APIs. Namely, we leverage Perspective API and treat all text with a score less than 1e\u22121.43 as non-toxic. Estimates on the trial study suggests that only 1 out of the 48 toxic examples are missed, which we believe is acceptable. In evaluation, we focus on the human annotated portion. We believe that the model evaluation on the likely non-toxic part would have little to no effect on assessing the model\u2019s overall quality. As a result, we have successfully released around 60% annotation workload while maintaining the accuracy of labels.\nWe are aware that our annotator agreement is not perfect. Therefore, we leverage two process to guarantee the annotation quality: (1) during annotation each example is seen by two different annotators. In the end, we gathered all conflicting annotations and discussed about them to achieve mutual agreement on all data; (2) we double check those non-toxic examples using GPT4 to find potential toxic examples that have been ignored by our annotators by mistake. We additionally label jailbreaking text, following the same process. Benchmark Statistics. The construction of TOXICCHAT consists of two stages. In the first stage, we collect a total of 7,599 data points, among which\nPerspective API filtered out 4,668 ones with low toxicity score and we annotated 2,931 data. In the second stage, we manually labeled 2,756 extra data to enrich the test set for evaluation. After manual check and remove unsuitable data for release, TOXICCHAT collects a total of 10,166 data.\nThe overall toxicity rate is 7.10%, which is similar to the one reported in Section 2.1, indicating that our annotation is relatively consistent. The jailbreaking rate is 1.75%."
        },
        {
            "heading": "3 Baseline Evaluations",
            "text": "Here, we start with baseline evaluations on the benchmark and show that TOXICCHAT can not be well solved by prior toxicity detection tools or models. We split all 10,166 data points randomly into two halves of training and testing data. Evaluation Metric. The most important metric of our evaluation is the method\u2019s ability to identify toxic text without over-prediction on non-toxic ones. We calculate the precision (Pre), recall (Rec), and F1 score of a method on the human annotated portion in the test set of TOXICCHAT. A better score means the model can better detect toxicity. We also introduce one additional metric, jalibreaking recall (JR), the percentage of the jailbreaking text the model successfully identifies as toxic. Model Baselines. We benchmark existing toxicity detection APIs and models on our TOXICCHAT. For toxicity detection APIs, we use all suggested thresholding values to avoid our own confirmation bias on the dataset. Specifically, for OpenAI moderation, we use the binary prediction it provides. For Perspective API, we use a threshold value of 0.7\naccording the recommendation of their website4. For existing toxicity detection models, we evaluate HateBERT (Caselli et al., 2020) and ToxDectRoberta (Zhou, 2021). Results. The results are shown in Table 1. It is clear that these models and APIs fail to deliver good quality on TOXICCHAT, indicating that there is a large domain discrepancy for toxicity between real-world user-AI conversations and social media platforms."
        },
        {
            "heading": "4 Ablation Study",
            "text": ""
        },
        {
            "heading": "4.1 Domain Difference",
            "text": "To study whether TOXICCHAT has a different data domain than prior works, we consider six different toxicity datasets \u2013 HSTA, MovieReview, Jigsaw, Toxigen, RealToxicPrompts, and ConvAbuse (details in Appendix A). We subsample them to a reasonable size and train a roberta-base model on each of the dataset, using the huggingface framework along with their suggested hyperparameters. Additionally, we experiment on a combination of the six domains. We then evaluate all seven trained models on our benchmark. Finally, we evaluated a roberta-base model trained on the training portion of TOXICCHAT. All experiments are conducted 5 times with different random seeds, and the standard deviation is reported. Results. From the results in Table 2, we can see that the model trained on our in-domain data of TOXICCHAT always performs significantly better. This is the case when TOXICCHAT training data is not particularly larger than the other datasets. This highlights the domain difference and nontransferability of toxicity detection datasets to TOXICCHAT.\nThe jailbreaking results suggest that a good toxicity detector might not be a good jailbreaking detector. We note that a higher jailbreaking recall than toxicity recall indicates that the model is especially\n4https://developers.perspectiveapi.com/s/ about-the-api-score\nbetter at capturing jailbreaking data than toxicity data, and otherwise a lower ability. All outer domain datasets except Toxigen exhibits a decrease in jailbreaking recall, indicating that the domain transfer to jailbreaking detecting is probably even harder than toxicity detection in TOXICCHAT."
        },
        {
            "heading": "4.2 Toxicity Methods Using Chatbot Output",
            "text": "We also considered experimenting with better methods of detecting toxicity in TOXICCHAT, one of which is leveraging the chatbot\u2019s response to user inputs. The intuition is that such responses may convey some additional features (e.g., the response could be \u201csorry, but as an . . . \u201d) or could be easier to detect (e.g., when the chatbot did not realize toxic inputs, it may generate toxic responses). We experimented using the output in two ways, one by treating it as the sole feature (in both training and testing), and the other by combining it with the input feature. The results are reported in Table 3. Results. By using only responses (Output), the model lacks in detecting toxicity and jailbreaking. By combining responses with user inputs (Input \u222a Output), we notice a slight increase in toxicity detection and a slight decrease in jailbreaking coverage, while both non-significant. The conclusion, against our intuition, is that including model responses might not be helpful."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we present TOXICCHAT, a real-world user-AI toxicity detection benchmark consisting of 10k user-AI conversations. We have revealed special cases in this unique domain compared to previous toxicity datasets that are mainly based on social media. Our experimental results show that fine-tuning on this benchmark notably improves a baseline model\u2019s ability to detect toxic interactions. Our work highlights future research directions for ethical LLM considerations, particularly around data acquisition in the context of non-open-source LLMs and further preventing undesired content generation.\nLimitations\nAs an pioneering work of toxicity in real-world user-AI conversations, our study has a few limitations.\nFirst, the data in our benchmark are user queries from a popular online demo Vicuna. We are aware\nthat the user-AI interactions from higher usertraffic sites, namely ChatGPT, Bard, New Bing, etc, could better reflect user demographics and languages, yet, we do not have access to these proprietary data.\nSecond, we annotated a few thousand data through a human-model collaborative annotation process. It is arguably true that the dataset quality would be better if we annotated tens or hundreds of thousands of data entirely by human. However, we would like to note that (1) the data annotation process is highly costly as it requires trustable workers (in our case, researchers involved in this project) due to the potential sensitiveness of the data (see the Curation of Dataset part in our Ethical Consideration for details) and the need of high quality data; (2) such an user-AI toxicity dataset is of high interest to the current research, because of the burst of proprietary and community chatbots.\nAnother limitation is that we did not test all possible methods for toxicity evaluation. We did not aim to achieve a best possible performance of toxicity detection on our benchmark, using extensive hyperparameter tuning or larger language models. This is because our major goal is to present the domain differences of online text in prior works and our user-AI interaction dataset and call for communitiy awareness on this subject; obtaining state-ofthe-art performance on the benchmark, however, is not.\nEthical Considerations\nCuration of the Dataset The dataset we annotate is a random sample of first turn user queries from Vicuna online demo. The Vicuna demo is fully anonymous of the user and also highlights the possible re-use of user query data, the following quoted from the demo website:\nThe service collects user dialogue data and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) license.\nThe authors obtained authorization from the Vicuna team to perform this study.\nWe are aware that the user query data may still contain Personal Identifiable Information (PII). Therefore, the annotation in this study are done by the authors themselves without using any crowdsourcing (e.g., Amazon Turks). We did query existing proprietary toxicity detection tools, OpenAI\nModeration and Perspective API, but we consider this benign. Additionally, we manually removed all PII data we can identify.\nBefore the annotation, the annotators (the authors) are first notified about the toxic data that they will be annotated. Verbal agreements were obtained before annotation.\nRelease of the Dataset We have released our dataset for future study and evaluation. There are several considerations for releasing the dataset.\nFirst, as also noted in the limitations, this dataset mainly (> 99%) composes of English user-AI conversations and might not be indicative of (or a good evaluation thereof) non-English conversations.\nSecond, the dataset inevitably contains toxic text, which could be used, against our expectations, to train toxic models. the dataset itself also may give attackers a chance to identify user queries that bypass models trained on this dataset.\nOn the other side, due to unprecedented high interest in chatbot systems, we believe the timeliness of such toxicity dataset is important: several popular chatbot systems either have not yet taken safety measures or are using existing tools (e.g., perspective api) to filter toxic content, which are not reliable as shown in our study.\nTherefore, we believe the reason to release such dataset outweighs the concerns. As an additional safety measure, we will include a request access form to guard the data."
        },
        {
            "heading": "Acknowledgement",
            "text": "Our work is sponsored in part by NSF CAREER Award 2239440, NSF Proto-OKN Award 2333790, NIH Bridge2AI Center Program under award 1U54HG012510-01, Cisco-UCSD Sponsored Research Project, as well as generous gifts from Google, Adobe, and Teradata. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon."
        },
        {
            "heading": "A Related Work",
            "text": "Following the definitions in (Fortuna et al., 2020), toxicity in this work includes offensiveness, sexism, hateful speech and some other undesired content. Most previous work in this field has focused more on the social media corpus, such as Twitter (Ball-Burack et al., 2021; Harris et al., 2022; Upadhyay et al., 2022; Koufakou et al., 2020; Nozza et al., 2019), Reddit (Wang et al., 2020; Han and Tsvetkov, 2020) and so on. We choose several benchmarks from different domains for the comparison experiments to reveal whether there exists some hidden discrepancies between real-world user-AI conversational dialogue and current toxicity benchmark, which includes: (1) HSTA (Waseem and Hovy, 2016) based on Hate Speech Twitter Annotations, which have a total of 31,935 rows out of which the hate tweets comprise only 7% of the total tweets. We randomly sample 8,000 rows from it to ensure that the sizes of several datasets are nearly identical. (2) Jigsaw Multilingual Toxic Comment5. (3) Rotten Tomatoes Movie Reviews6.\nInstead of only focusing on the traditional toxicity datasets, such as social media and websites, there are three very noteworthy benchmarks in the similar domain, which can make our comparisons\n5https://www.kaggle.com/competitions/ jigsaw-multilingual-toxic-comment-classification/ data\n6https://www.kaggle.com/ datasets/stefanoleone992/ rotten-tomatoes-movies-and-critic-reviews-dataset\nmore reasonable and covers as many fields as possible. The first is Toxigen Data (Hartvigsen et al., 2022) as its all instances are generated using GPT3 (Leone, 2020). The second one is Real Toxicity Prompts (Gehman et al., 2020), a dataset of 100K sentence-level prompts derived from a large corpus of English web text with several features: profanity, sexually explicit, identity attack, flirtation, threat, insult, severe toxicity and toxicity. While both of the two datasets contain the toxic prompts, Toxigen\u2019s prompts are generated by GPT and Real Toxicity Prompts are not used for conversational AI. They can not fully address the current awkward situations on user-AI chatbots. The third dataset is ConvAbuse (Curry et al., 2021), a dataset containing toxic outputs of three conversational agents.\nWe also use several important baseline models or open-source APIs: OpenAI Moderation7 is a moderation tool of OpenAI API. It is trained on publicly available toxicity datasets mainly built from social media, and OpenAI also uses active learning technologies on their own production data to train the model. Perspective API8 is a widely used, commercially deployed toxicity detection tool, which is trained on a collection of user comments from platforms such as Wikipedia. ToxDectRoberta (Zhou, 2021) is a toxic language detection model. It has been trained on tweets, with the base model being Roberta-large. HateBERT (Caselli et al., 2021) is an pre-trained BERT model obtained by fine-tuning with more than 1 million posts from banned communites from Reddit."
        },
        {
            "heading": "B Data Pre-processing",
            "text": "We disregard uninformative content such as nonASCII characters and excessively short prompts (less than three words) to maintain the prevalence and rationality of our dataset because of the findings of (Pavlopoulos et al., 2020), which indicates that the necessity of context for toxicity detection in a dialogue.\nGiven that the nature of Vicuna and the resources available for annotation dictate a focus on English language content, we incorporate a language detection tool and only retain English prompts by utilizing the fastText language identification model\n7https://platform.openai.com/docs/guides/ moderation\n8https://perspectiveapi.com/\n(Joulin et al., 2016). To keep the privacy norms and ethical use of the dataset, we identify and mask user\u2019s personal information, such as emails, phone numbers or addresses with generic placeholders. We eliminate any potential privacy risks while maintaining the prompts\u2019 syntactic structure."
        },
        {
            "heading": "C Jailbreaking Example",
            "text": "Table 4 reports some jailbreaking examples."
        },
        {
            "heading": "D Study on Moderation APIs",
            "text": "We report two moderation APIs results on our 720 trail data, with a detailed distribution plot for correlations between model confidence and performance (Figure 2). Specifically, we choose OpenAI Moderation and Perspecitive API (details have been mentioned in Appendix A). We can find that (1) there is no absolute threshold for a good separation between toxic and non-toxic examples. However, (2) the model\u2019s confidence score is relatively reliable as high probability generally corresponds to high toxicity. These have formed a guideline for our human-AI collaborative annotation framework. Specifically, if we set an absolute safe rate for a moderation model, e.g., we can tolerate 1% error\nin each bin, then we can leave 40% of the data to OpenAI moderation. In other words, we only need to annotate 60% of the data. The data percentage pending human annotation with different absolute safe rate are shown as follows:\nSafe% OpenAI Perspective 1 40.27 60.71 5 71.94 75.56\n10 90.69 78.19\nWe choose to use Perspective API with 1% absolute safe rate as our collaborative model (threshold equal to 1e\u2212 1.43). We believe this is reasonable since the absolute safe rate is much less than the inter-annotator agreement reported in Section 2.1 and the percentage we need to annotate is much less than OpenAI Moderation (40% compared to 60%)."
        }
    ],
    "title": "TOXICCHAT: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
    "year": 2023
}