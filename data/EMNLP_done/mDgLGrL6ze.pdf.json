{
    "abstractText": "We introduce ECHO (Event Causality Inference via Human-Centric Reasoning), a diagnostic dataset of event causality inference grounded in visio-linguistic social scenarios. ECHO employs real-world human-centric deductive information built on a television crime drama. ECHO requires the Theory-of-Mind (ToM) ability to understand and reason about social interactions based on multimodal information. Using ECHO, we propose a unified Chain-of-Thought (CoT) framework to assess the reasoning capability of current AI systems. Our ToM-enhanced CoT pipeline accommodates various large foundation models in both zero-shot and few-shot visio-linguistic reasoning. We use this framework to scrutinize recent large foundation models such as InstructGPT and MiniGPT-4 on three diagnostic humancentric tasks. Further analysis demonstrates ECHO as a challenging dataset to expose imperfections and inconsistencies in reasoning. Our data and code are publicly available at https://github.com/YuxiXie/ECHo.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxi Xie"
        },
        {
            "affiliations": [],
            "name": "Guanzhen Li"
        },
        {
            "affiliations": [],
            "name": "Min-Yen Kan"
        }
    ],
    "id": "SP:8ebda560d5025d7e9ea35edf9918b980f1aac040",
    "references": [
        {
            "authors": [
                "Marianne Monteiro",
                "Jacob L. Menick",
                "Sebastian Borgeaud",
                "Andy Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Kar\u00e9n Simonyan"
            ],
            "title": "Flamingo: a visual language model",
            "year": 2022
        },
        {
            "authors": [
                "Karl Albrecht."
            ],
            "title": "Social intelligence: The new science of success",
            "venue": "John Wiley & Sons.",
            "year": 2006
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: visual question answering",
            "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile,",
            "year": 2015
        },
        {
            "authors": [
                "Ian Apperly."
            ],
            "title": "Mindreaders: the cognitive basis of\" theory of mind",
            "venue": "Psychology Press.",
            "year": 2010
        },
        {
            "authors": [
                "Ian A Apperly",
                "Stephen A Butterfill"
            ],
            "title": "Do humans have two systems to track beliefs and belieflike states",
            "venue": "Psychological review,",
            "year": 2009
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "FAccT",
            "year": 2021
        },
        {
            "authors": [
                "Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott M. Lundberg",
                "Harsha Nori",
                "Hamid Palangi",
                "Marco T\u00falio Ribeiro",
                "Yi Zhang."
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "venue": "CoRR, abs/2303.12712.",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "CoRR, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Y. Zhao",
                "Yanping Huang",
                "Andrew M. Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Alan S Cowen",
                "Dacher Keltner."
            ],
            "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
            "venue": "Proceedings of the national academy of sciences, 114(38):E7900\u2013E7909.",
            "year": 2017
        },
        {
            "authors": [
                "Ernest Davis."
            ],
            "title": "Benchmarks for automated commonsense reasoning: A survey",
            "venue": "CoRR, abs/2302.04752.",
            "year": 2023
        },
        {
            "authors": [
                "Daniel C Dennett."
            ],
            "title": "Beliefs about beliefs [p&w, sr&b",
            "venue": "Behavioral and Brain sciences, 1(4):568\u2013570.",
            "year": 1978
        },
        {
            "authors": [
                "Vanhoucke",
                "Karol Hausman",
                "Marc Toussaint",
                "Klaus Greff",
                "Andy Zeng",
                "Igor Mordatch",
                "Pete Florence."
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "CoRR, abs/2303.03378.",
            "year": 2023
        },
        {
            "authors": [
                "Lee Freese",
                "Peter J Burke."
            ],
            "title": "Persons, identities, and social interaction",
            "venue": "Advances in group processes, 11:1\u201324.",
            "year": 1994
        },
        {
            "authors": [
                "Lea Frermann",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Whodunnit? crime drama as a case for natural language understanding",
            "venue": "Trans. Assoc. Comput. Linguistics, 6:1\u201315.",
            "year": 2018
        },
        {
            "authors": [
                "Jiyang Gao",
                "Chen Sun",
                "Zhenheng Yang",
                "Ram Nevatia."
            ],
            "title": "TALL: temporal activity localization via language query",
            "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 5277\u20135285. IEEE Computer",
            "year": 2017
        },
        {
            "authors": [
                "Ahmad Ghazal",
                "Todor Ivanov",
                "Pekka Kostamaa",
                "Alain Crolotte",
                "Ryan Voong",
                "Mohammed Al-Kateb",
                "Waleed Ghazal",
                "Roberto V. Zicari."
            ],
            "title": "Bigbench V2: the new and improved bigbench",
            "venue": "33rd IEEE International Conference on Data Engineering,",
            "year": 2017
        },
        {
            "authors": [
                "Ben Goertzel."
            ],
            "title": "Artificial general intelligence: Concept, state of the art, and future prospects",
            "venue": "J. Artif. Gen. Intell., 5(1):1\u201348.",
            "year": 2014
        },
        {
            "authors": [
                "Riitta Hari",
                "Miiamaaria V Kujala."
            ],
            "title": "Brain basis of human social interaction: from concepts to brain imaging",
            "venue": "Physiological reviews, 89(2):453\u2013479.",
            "year": 2009
        },
        {
            "authors": [
                "Yushi Hu",
                "Hang Hua",
                "Zhengyuan Yang",
                "Weijia Shi",
                "Noah A. Smith",
                "Jiebo Luo."
            ],
            "title": "Promptcap: Prompt-guided task-aware image captioning",
            "venue": "CoRR, abs/2211.09699.",
            "year": 2022
        },
        {
            "authors": [
                "Furu Wei."
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "CoRR, abs/2302.14045.",
            "year": 2023
        },
        {
            "authors": [
                "Drew A. Hudson",
                "Christopher D. Manning."
            ],
            "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,",
            "year": 2019
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc V. Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "Proceedings of the 38th In-",
            "year": 2021
        },
        {
            "authors": [
                "Zhijing Jin",
                "Sydney Levine",
                "Fernando Gonzalez Adauto",
                "Ojasv Kamal",
                "Maarten Sap",
                "Mrinmaya Sachan",
                "Rada Mihalcea",
                "Josh Tenenbaum",
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "When to make exceptions: Exploring language models as accounts of human moral judgment",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens van der Maaten",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross B. Girshick."
            ],
            "title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "2017 IEEE Conference on Computer Vi-",
            "year": 2017
        },
        {
            "authors": [
                "Emre Kiciman",
                "Robert Ness",
                "Amit Sharma",
                "Chenhao Tan."
            ],
            "title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "venue": "CoRR, abs/2305.00050.",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Michal Kosinski."
            ],
            "title": "Theory of mind may have spontaneously emerged in large language models",
            "venue": "CoRR, abs/2302.02083.",
            "year": 2023
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Kenji Hata",
                "Frederic Ren",
                "Li Fei-Fei",
                "Juan Carlos Niebles."
            ],
            "title": "Dense-captioning events in videos",
            "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 706\u2013715. IEEE Computer",
            "year": 2017
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "CoRR, abs/2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP: bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Balti-",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "KaiWei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao."
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "venue": "CoRR, abs/2304.09842.",
            "year": 2023
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Faithful chain-ofthought reasoning",
            "venue": "CoRR, abs/2301.13379.",
            "year": 2023
        },
        {
            "authors": [
                "Gary Marcus."
            ],
            "title": "How come gpt can seem so brilliant one minute and so breathtakingly dumb the next? Substack newsletter",
            "venue": "The Road to AI We Can Trust.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel R Miller"
            ],
            "title": "The study of social relationships: Situation, identity, and social interaction",
            "year": 1962
        },
        {
            "authors": [
                "Shima Rahimi Moghaddam",
                "Christopher J. Honey."
            ],
            "title": "Boosting theory-of-mind performance in large language models via prompting",
            "venue": "CoRR, abs/2304.11490.",
            "year": 2023
        },
        {
            "authors": [
                "Cory S Myers",
                "Lawrence R Rabiner."
            ],
            "title": "A comparative study of several dynamic time-warping algorithms for connected-word recognition",
            "venue": "Bell System Technical Journal, 60(7):1389\u20131409.",
            "year": 1981
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "der",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Jae Sung Park",
                "Chandra Bhagavatula",
                "Roozbeh Mottaghi",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "Visualcomet: Reasoning about the dynamic context of a still image",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28,",
            "year": 2020
        },
        {
            "authors": [
                "David Premack",
                "Guy Woodruff"
            ],
            "title": "Does the chimpanzee have a theory of mind? Behavioral and brain",
            "year": 1978
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Daniel Fried",
                "Yejin Choi."
            ],
            "title": "Neural theory-of-mind? on the limits of social intelligence in large lms",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Guangyao Shen",
                "Xin Wang",
                "Xuguang Duan",
                "Hongzhi Li",
                "Wenwu Zhu."
            ],
            "title": "Memor: A dataset for multimodal emotion reasoning in videos",
            "venue": "MM \u201920: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16,",
            "year": 2020
        },
        {
            "authors": [
                "Damien Sileo",
                "Antoine Lernould."
            ],
            "title": "Mindgames: Targeting theory of mind in large language models with dynamic epistemic modal logic",
            "venue": "CoRR, abs/2305.03353.",
            "year": 2023
        },
        {
            "authors": [
                "D\u00eddac Sur\u00eds",
                "Sachit Menon",
                "Carl Vondrick."
            ],
            "title": "Vipergpt: Visual inference via python execution for reasoning",
            "venue": "CoRR, abs/2303.08128.",
            "year": 2023
        },
        {
            "authors": [
                "Maria Tsimpoukelli",
                "Jacob Menick",
                "Serkan Cabi",
                "S.M. Ali Eslami",
                "Oriol Vinyals",
                "Felix Hill."
            ],
            "title": "Multimodal few-shot learning with frozen language models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural",
            "year": 2021
        },
        {
            "authors": [
                "Tomer D. Ullman."
            ],
            "title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "venue": "CoRR, abs/2302.08399.",
            "year": 2023
        },
        {
            "authors": [
                "Ronald E Walker",
                "Jeanne M Foley."
            ],
            "title": "Social intelligence: Its history and measurement",
            "venue": "Psychological reports, 33(3):839\u2013864.",
            "year": 1973
        },
        {
            "authors": [
                "Zhenhailong Wang",
                "Manling Li",
                "Ruochen Xu",
                "Luowei Zhou",
                "Jie Lei",
                "Xudong Lin",
                "Shuohang Wang",
                "Ziyi Yang",
                "Chenguang Zhu",
                "Derek Hoiem",
                "Shih-Fu Chang",
                "Mohit Bansal",
                "Heng Ji"
            ],
            "title": "Language models with image descriptors are strong few-shot",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Wikipedia contributors."
            ],
            "title": "Csi: Crime scene investigation \u2014 Wikipedia, the free encyclopedia",
            "venue": "https://en.wikipedia.org/w/index. php?title=CSI:_Crime_Scene_Investigation& oldid=1152659091. [Online; accessed 21-May-",
            "year": 2023
        },
        {
            "authors": [
                "Chenfei Wu",
                "Shengming Yin",
                "Weizhen Qi",
                "Xiaodong Wang",
                "Zecheng Tang",
                "Nan Duan."
            ],
            "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
            "venue": "CoRR, abs/2303.04671.",
            "year": 2023
        },
        {
            "authors": [
                "Yuxi Xie",
                "Kenji Kawaguchi",
                "Yiran Zhao",
                "Xu Zhao",
                "MinYen Kan",
                "Junxian He",
                "Qizhe Xie."
            ],
            "title": "Decomposition enhances reasoning via self-evaluation guided decoding",
            "venue": "CoRR, abs/2305.00633.",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An empirical study of GPT-3 for few-shot knowledgebased VQA",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Con-",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Linjie Li",
                "Jianfeng Wang",
                "Kevin Lin",
                "Ehsan Azarnasab",
                "Faisal Ahmed",
                "Zicheng Liu",
                "Ce Liu",
                "Michael Zeng",
                "Lijuan Wang."
            ],
            "title": "MMREACT: prompting chatgpt for multimodal reasoning and action",
            "venue": "CoRR, abs/2303.11381.",
            "year": 2023
        },
        {
            "authors": [
                "Quanzeng You",
                "Hailin Jin",
                "Zhaowen Wang",
                "Chen Fang",
                "Jiebo Luo."
            ],
            "title": "Image captioning with semantic attention",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 4651\u20134659.",
            "year": 2016
        },
        {
            "authors": [
                "Luyao Yuan",
                "Zipeng Fu",
                "Jingyue Shen",
                "Lu Xu",
                "Junhong Shen",
                "Song-Chun Zhu."
            ],
            "title": "Emergence of pragmatics from referential game between theory of mind agents",
            "venue": "CoRR, abs/2001.07752.",
            "year": 2020
        },
        {
            "authors": [
                "Amir Zadeh",
                "Michael Chan",
                "Paul Pu Liang",
                "Edmund Tong",
                "Louis-Philippe Morency."
            ],
            "title": "Socialiq: A question answering benchmark for artificial social intelligence",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "MERLOT: multimodal neural script knowledge models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neu-",
            "year": 2021
        },
        {
            "authors": [
                "Andy Zeng",
                "Adrian Wong",
                "Stefan Welker",
                "Krzysztof Choromanski",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael S. Ryoo",
                "Vikas Sindhwani",
                "Johnny Lee",
                "Vincent Vanhoucke",
                "Pete Florence"
            ],
            "title": "Socratic models: Composing zero-shot multimodal reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola."
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "CoRR, abs/2302.00923.",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric P. Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "year": 2023
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Ruixiang Cui",
                "Yiduo Guo",
                "Yaobo Liang",
                "Shuai Lu",
                "Yanlin Wang",
                "Amin Saied",
                "Weizhu Chen",
                "Nan Duan."
            ],
            "title": "Agieval: A human-centric benchmark for evaluating foundation models",
            "venue": "CoRR, abs/2304.06364.",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "CoRR, abs/2205.10625.",
            "year": 2022
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Kilichbek Haydarov",
                "Xiaoqian Shen",
                "Wenxuan Zhang",
                "Mohamed Elhoseiny."
            ],
            "title": "Chatgpt asks, BLIP-2 answers: Automatic questioning towards enriched visual descriptions",
            "venue": "CoRR, abs/2303.06594.",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny."
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "CoRR, abs/2304.10592.",
            "year": 2023
        },
        {
            "authors": [
                "Yaochen Zhu",
                "Xiangqing Shen",
                "Rui Xia."
            ],
            "title": "Personality-aware human-centric multimodal reasoning: A new task",
            "venue": "CoRR, abs/2304.02313.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Social intelligence refers to the ability to understand, navigate, and respond effectively in social scenarios. As a widely investigated concept in psychology and social sciences, it has become a prominent facet of artificial intelligence (Walker and Foley, 1973; Kihlstrom and Cantor, 2000; Albrecht, 2006; Zadeh et al., 2019). In the development of social intelligence, humans gain the crucial cognitive capacity of understanding and reasoning about the mental states (i.e., beliefs, desires, intentions, emotions, and thoughts) of individuals. This pivotal form of competence is commonly denoted as Theory-of-Mind (ToM) (Premack and Woodruff, 1978; Apperly and Butterfill, 2009; Apperly, 2010). As a fundamental ability of social commonsense reasoning (Davis, 2023), ToM is important for machine reasoning to achieve artificial general intelli-\ngence (Goertzel, 2014; Zhong et al., 2023), especially towards better social intelligence.\nRecently, large language (Chowdhery et al., 2022; Chung et al., 2022; Touvron et al., 2023; OpenAI, 2023) and multimodal (Radford et al., 2021; Alayrac et al., 2022; Li et al., 2022; Huang et al., 2023) models have exhibited remarkable reasoning capabilities. However, current large foundation models still fall short of adapting to personalized scenarios for specific users (Sap et al., 2022; Bubeck et al., 2023). Hence, there has been an increasing focus on human-centric reasoning as a means of enhancing artificial social intelligence for its integration in human daily life (Bard et al., 2020; Yuan et al., 2020; Moghaddam and Honey, 2023). To this end, ToM is one of the central challenges to accelerating communication and ensuring safety in human\u2013computer interaction, requiring complex reasoning on how human beliefs and intents may vary across different scenarios (Yuan et al., 2020; Sap et al., 2022; Jin et al.,\n2022; Sileo and Lernould, 2023). Specifically, current AI systems struggle with handling interleaved multi-modalities and faithful surmising for better consistency and interpretability of reasoning (Lyu et al., 2023; Bubeck et al., 2023).\nTo enhance reasoning consistency and faithfulness, recent works on large language models (LLMs) propose to break down a problem into intermediate inferences (Wei et al., 2022; Zhou et al., 2022; Xie et al., 2023). The impressive empirical success of this Chain-of-Thought (CoT) scheme on both textual and multimodal tasks (Wei et al., 2022; Zhang et al., 2023; Driess et al., 2023) demonstrates a promising paradigm to integrate ToM inference as an intermediate step in humancentric reasoning. In this work, we introduce\nECHO (Event Causality Inference via HumanCentric Reasoning), a visio-linguistic dataset with ToM inferences for reasoning in social scenarios. With a focus on human factors, ECHO seeks to diagnose the social intelligence of current large language and multimodal models. We focus on event causality reasoning that remains challenging for recent LLMs (Kiciman et al., 2023).\nWe envision ECHO as a challenging diagnostic benchmark on human-centric reasoning. Each ECHO instance is grounded in a plot from the crime drama CSI: Crime Scene Investigation, enabling approximation of the real-world social interactions pertaining to the discordance in human beliefs. As shown in Figure 2, our core annotation process begins with ascertaining the identity of a specified character. Next, we discern their mental states via emotion interpretation. Leveraging this human-centric understanding, annotators then infer the cause or effect of a plot event for causality reasoning. To foster visio-linguistic social intelligence, we enhance ToM by guiding annotators to make causal inferences that take into account the mental states (e.g., intentions, emotions, and thoughts) of characters and pinpoint related frames as visual evidence. As such, ECHO is integrated with a unified framework to assess human-centric reasoning in the social context. As detailed in Section 4, we propose a series of diagnostic tasks to evaluate capabilities to identify roles, reason about emotions, and infer event causality.\nTo conclude, we introduce ECHO, a challenging visio-linguistic corpus of human-centric reasoning in social scenarios. We propose a unified framework to evaluate existing large foundation models\nin zero-shot and few-shot ToM-enhanced CoT reasoning. Our further analysis demonstrates how to use our diagnostic tasks to assess multimodal understanding of human factors, revealing the deficiency of current AI systems in maintaining logical correctness and consistency throughout reasoning."
        },
        {
            "heading": "2 Related Work",
            "text": "ECHO takes a further step towards social intelligence on human-centric inference in visiolinguistic scenarios, probing the ToM capacity of large foundation models via CoT reasoning.\nVisio-Linguistic Reasoning. Datasets and tasks in visio-linguistic reasoning span widely from descriptive information extraction (Antol et al., 2015; You et al., 2016; Gao et al., 2017), physical relation inference (Johnson et al., 2017; Hudson and Manning, 2019), to complex and deep reasoning on the event and human factors (Krishna et al., 2017; Zellers et al., 2019; Park et al., 2020). ECHO follows this trend to enhance the reasoning depth towards human-specific facets. Unlike recent works (Shen et al., 2020; ?; Zhu et al., 2023c) of human-centric reasoning, ECHO is integrated with rigorous ToM annotations, supporting the final inferences via CoT reasoning for better consistency. Specifically, there are long-standing arguments on the development and assessment of ToM in both human psychology and machine intelligence (Premack and Woodruff, 1978; Apperly, 2010; Kosinski, 2023; Ullman, 2023). Measurement of ToM is usually based on false belief tasks (Dennett, 1978), assessing the ability to distinguish one\u2019s own (true) belief and others\u2019 (false) belief, given the information and experience asymmetry among different individuals. Constructed on crime drama, ECHO contains an abundance of such cases of false belief to probe ToM ability.\nLarge Multimodal and Language Models. Previous research in this area mainly complies with the paradigm of pre-training and fine-tuning to construct and train large-scale multimodal models to handle interleaved visual-and-linguistic information (Radford et al., 2021; Jia et al., 2021; Zellers et al., 2021; Alayrac et al., 2022; Li et al., 2022; Huang et al., 2023). Recently, there is the emergence of offline methods which leverage the capacities of large foundation models to conduct direct few-shot or zero-shot inference (Wu et al., 2023; Yang et al., 2023; Lu et al., 2023; Zhu et al.,\n2023a). Similar with Zhu et al. (2023a), we propose a framework to enhance visual understanding via LLM prompting (Ouyang et al., 2022; OpenAI, 2023) and facilitate LLM reasoning with augmented multimodal information (Li et al., 2023)."
        },
        {
            "heading": "3 The ECHo Corpus",
            "text": "ECHO contains 2k inference instances collected via our ToM-enhanced CoT scheme. To facilitate the approximation of authentic social interactions, we ground ECHO in CSI: Crime Scene Investigation, an American procedural forensics crime series in English (Wikipedia contributors, 2023). With visual evidence and scenes in frames and utterances and narrations in screenplays, CSI represents a rich multimodal source, spanning widely factual, relational, and inferential data. As shown in Table 2, drama plots bring abundant instances of belief discrepancy and unexpected content for ToM reasoning. This enables our focus on humancentric information distillation and interpretation in ECHO\u2019s construction for ToM-enhanced inference."
        },
        {
            "heading": "3.1 Construction Pipeline",
            "text": "We pair official CSI clips with their screenplays, crawled from a publicly available website hosting TV show transcripts1 (Frermann et al., 2018). We then launch annotation in 3 rounds with 30 annota-\n1https://transcripts.foreverdreaming.org/\ntors working over 5 weeks after training sessions2.\nData Source Crawling and Preprocessing. We acquire the official CSI videos with associated screenplays of 177 episodes from the first 8 seasons. We construct ECHO via further annotation and task formulation on a subset of CSI containing 15 episodes, each of which usually features one or two cases that are independent from the preceding plots. We develop heuristic rules3 to automatically denoise, split, and categorize the scripts into plot events for subsequent task formulation.\nRound One: Plot Segmentation. The crawled screenplays are distributed in discrete plots. We refine this segmentation with data cleaning to collect segments of feasible length and substantive contents. Different from previous works of automatic vision\u2013language alignment (Myers and Rabiner, 1981; Frermann et al., 2018), we manually synchronize the screenplays with the time-stamped video clips to pinpoint the main characters for humancentric reasoning. We obtain 1, 542 plots grounded in different scenes in this round, with an average of 3 identified characters in each segment.\nRound Two: Inference Annotation. Each annotation instance features one specified event for causality inference. We sequentially operationalize\n2Appendix A details annotator training. 3Details at https://github.com/YuxiXie/ECHo.\n# Clip 100 FB (44%) 25 19 17 UC (36%) C/E cause (56%) effect (44%) FB objective (52%) subjective (48%) UC physical (69%) social (31%)\nTable 2: ToM attribute distribution on a 100- clip subset. FB and UC represent false-belief and unexpected-content, respectively.\nannotation in 3 stages as follows: 1. Characteristics Identification. Given one key character, annotators identify the character\u2019s role in the plot. We encourage them to consider social attributes such as age range, occupation, and relations (with others) to describe the role. Character roles are not static and can vary in different plots, depending on the nuances of their appearances, behaviors, and interactions in the specific context.\n2. Keyframe-Grounded Emotion Interpretation. We take a further step in human-centric reasoning to interpret mental states. Annotators choose from 13 primary emotions4 categorized by adapting the 27 emotions from Cowen and Keltner (2017) to the crime drama. We also accept free-form input for emotions when no existing options apply. Annotators then extract associated frames that feature related emotions. We take the frames as visual representations of human factors. Specifically, when multiple emotions are identified, annotators are also instructed to select more frames. We do\n4Including anger; boredom; calmness; disgust; doubt; entrancement; fear; interest; joy; sadness; shame; surprise; sympathy. We provide detailed definitions in Appendix C.\nnot strictly enforce one-to-one matching between frames and emotions, since one frame can feature several emotions, and some emotions may be more accurately captured by considering the reactions of other characters. To ensure the completeness and informativeness of selected keyframes, we implement follow-up validation next in Round Three.\n3. Event Causality Inference. Following the visio-linguistic human-centric inferences from the previous stages, we ask annotators to further infer the cause or effect of a specified event. We encourage them to consider the annotated roles and emotions to enhance reasoning consistency. Here we determine the events to annotate through two steps: 1) randomly select utterances or narrations that mention the main character(s) and occur in the middle of the plot to facilitate effective causality reasoning, and 2) filter out the automatically selected events that are insufficiently meaningful according to annotators\u2019 assessment.\nWe assign 2 to 4 annotators for each instance5 for quality control. In this round, we collect a total of\n5For some instances, we additionally assign more annotators when the inter-agreement is low.\n5, 746 annotated instances. We use the ToM inferences to formulate diagnostic tasks in Section 4.\nRound Three: Inference Validation. To qualify the annotated instances, we evaluate each data point via both automatic and manual checking. The realtime automatic checking alerts annotators if their inputs fail to meet certain informativeness criteria, such as text length and word-level overlap with the existing context. Our authoring team then carry out the manual validation. We particularly focus on instances with lower inter-rater agreement in emotion identification or anomalies in the timestamp distribution of selected frames. We specifically assess the plausibility, relevance, and completeness of annotations. For example, we reject instances where the event causality inference is weakly associated with either the plot or the annotated roles and emotions. Out of the 5, 746 annotations collected from Round Two, we finally retained 4, 280 annotations, as shown in Table 1."
        },
        {
            "heading": "3.2 Dataset Exploration",
            "text": "Table 1 and Figure 3e detail the summative statistics of collected data for the three tasks. The comparatively small scale of ECHO enables efficient assessment in the few-shot paradigm. With 4k ToM annotations on 2k inference instances, ECHO approximates authentic human-centric reasoning across visio-linguistic social scenarios. While our ECHO represents a subset of social interactions, we view it as an initial and specific step to explore ToM understanding of social intelligence. As outlined in Table 2, crime drama is rich in ToM-related cases, including false-belief and unexpected-content.\nWe further demonstrate the topic and scenario coverage of ECHO in Figure 3, by visualizing the keyword and verb\u2013noun frequencies in both input screenplays and human annotations. Alone other lines, Figure 3f shows how the correlation between roles and emotions vary in ECHO, reflecting a closer alignment with the real-world social characteristics compared against other datasets. We analyze the potential bias in our data in Section 6.5."
        },
        {
            "heading": "4 Diagnose Human-Centric Reasoning in",
            "text": "Visio-Linguistic Inference using ECHO\nECHO centers on rigorous human-centric information that supports ToM-enhanced CoT reasoning in visio-linguistic scenarios. We detail the formulation of our three sequential tasks to diagnose the human-centric reasoning ability next.\nNotation. Each ECHO instance consists of a sequence of visual frames V = [f1, f2, \u00b7 \u00b7 \u00b7 , fN ] = f1:N , a textual screenplay T . The frames are manually selected following the role and emotion identification in annotation. We gather frames featuring different characters together to represent the visual content of each clip. We designate each utterance or narration in the text to be an event Ei for the causality inference, as T = [E1, E2, \u00b7 \u00b7 \u00b7 , EM ]. There is a key character C to focus on in each instance.\nTask One: Role Identification (cf. Annotation Round Two, Stage 1). The psychoanalysis of a person\u2019s role in social interactions indicates their identity, helping to infer their intentions, actions, and relations with others (Miller, 1962; Freese and Burke, 1994). Therefore, we test the ability of role identification to probe the fundamental humancentric understanding in ToM reasoning. Given frame(s) fi of the key character C and the corresponding screenplay T , we prompt the model to generate the role r of C. The role can be defined by age, occupation, or relations with others, as these attributes can exhibit a strong correlation with the human mental states for ToM reasoning.\nTask Two: Emotion Interpretation (cf. Annotation Round Two, Stage 2). Emotions convey clues of mental states beyond verbal messages (Hari and Kujala, 2009). They bridge fundamental understanding (e.g., role identification) and further inference (e.g., intent prediction) in human-centric reasoning. We thus propose emotion interpretation as our second diagnostic task. We formulate this task as multi-choice question answering and test the alignment of model and human predictions on 13 candidate emotions, adapted from the taxonomy of Cowen and Keltner (2017) to the crime data.\nTask Three: Event Causality Inference (cf. Annotation Round Two, Stage 3). Despite the practical success of large foundation models on a wide range of reasoning tasks, there is a debate as to whether they genuinely execute causal reasoning or just reproduce memorized patterns (Bender et al., 2021; Marcus, 2022). Furthermore, these models still produce imperfections such as erroneous logic and human-factor understanding (Ghazal et al., 2017; Bubeck et al., 2023; Zhong et al., 2023; Kiciman et al., 2023). Hence, we formulate a subsequent task as event causality inference to assess the causality reasoning capacity among sociallygrounded events. We also utilize the ToM infer-\nences from the preliminary tasks as the CoT intermediate step to test the reasoning consistency among intermediate ToM inferences and the final predictions. Specifically, with the frames V and associated screenplay T , we ask models to infer the cause or effect of a given event Ei in the context."
        },
        {
            "heading": "5 ToM-Enhanced CoT Reasoning",
            "text": "Given the inputs of visual frames V and textual screenplay T , our objective is to make humancentric inference I . We follow Wei et al. (2022) to break down the process into intermediate steps R and thus accommodate the three tasks in a unified framework to assess large foundation models.\nAs illustrated in Figure 4, our framework follows the Vision + LLM paradigm (Huang et al., 2023; Zhang et al., 2023; Yang et al., 2023) to facilitate multimodal understanding using the reasoning ability inherently grown in language models.\nLLM-Enhanced Multimodal Understanding. Enlightened by the advanced capability of LLMs in complex reasoning (Brown et al., 2020; Kojima et al., 2022; Chowdhery et al., 2022), there is an emergent line of research to leverage LLMs to prompt and guide information extraction in visual understanding (Sur\u00eds et al., 2023; Wu et al., 2023; Yang et al., 2023; Zhu et al., 2023a). To diagnose the ability of human-centric reasoning of current large foundation models, we follow this paradigm to enhance multimodal information extraction with LLM reasoning. Specifically, we incorporate the LLM guidance as information-seeking questions to prompt multimodal understanding via visual\nquestion answering. We simplify the framework of Zhu et al. (2023a) by directly generating one task-specific question instead of augmenting iterative questions with accumulated contextual information. Figure 4 demonstrates an example of using the LLM-generated question to enhance multimodal understanding for human-factor extraction.\nVision-Augmented LLM Reasoning. Reciprocally, the multimodal model can facilitate LLM reasoning by augmenting information grounded in the vision. To this end, the visual information should be projected into representations that LLMs can understand, such as discrete text words (Hu et al., 2022; Wang et al., 2022; Zeng et al., 2022; Yang et al., 2022) and continuous features adapted into the textual space (Tsimpoukelli et al., 2021; Alayrac et al., 2022; Driess et al., 2023; Huang et al., 2023; Li et al., 2023). In our framework, we follow the former line of work to supplement the multimodal model generated textual descriptions into the LLM for vision augmentation. Specifically, the visual information covers knowledge of various granularities, extracted by general captioning and task-specific question-prompted answering. The task-specific questions here are generated by the LLM to guide reasoning via ToM inference."
        },
        {
            "heading": "6 Experiments",
            "text": "We assess the social intelligence of existing large foundation models using the unified framework in Section 5 on our diagnostic tasks in Section 4. We ablate components in our framework on different backboned models and evaluate their effect on both\nP RF1 anger\nboredom calmness\ndisgust doubt entrancement fear\ninterest joy sadness shame\nsurprise sympathy\nBLIP2 P RF1 MiniGPT4 P RF1 InstructGPT\n0\n20\n40\n60\n80\n100\nscore\nFigure 5: Class-wise performance comparison of three models on emotion interpretation.\nModels VL FS CoT ToM BLeU-2 Rouge-L BERT-F1\nMiniGPT-4\n\u2717 \u2717 \u2717 \u2717 2.99 11.89 51.24 \u2713 \u2717 \u2717 \u2717 3.30\u21910.31 11.99\u21910.10 51.62\u21910.38 \u2713 \u2717 \u2713 \u2717 3.31\u21910.32 11.64\u21930.25 51.21\u21930.03 \u2713 \u2717 \u2713 \u2713 3.55\u21910.56 12.05\u21910.16 51.98\u21910.74\nInstructGPT \u2713 \u2717 \u2717 \u2717 7.44 18.41 59.33 \u2713 \u2713 \u2717 \u2717 9.78\u21912.34 21.30\u21912.89 62.29\u21912.96 \u2713 \u2713 \u2713 \u2717 10.35\u21912.91 22.02\u21913.61 62.80\u21913.47 \u2713 \u2713 \u2713 \u2713\u2717 10.63\u21913.19 22.20\u21913.79 63.18\u21913.85 \u2713 \u2713 \u2713 \u2713 11.28\u21913.84 22.97\u21914.56 63.65\u21914.32\nHuman (Inter-Annotator Agreement) 15.70 23.82 64.87\nTable 4: Result Comparison on Event Causality Inference. ToM is the humancentric information. \u2713\u2717and \u2713represent model and human predictions, respectively.\nautomatic and human evaluation metrics."
        },
        {
            "heading": "6.1 Setup",
            "text": "Backbones and Prompt Construction. We use BLIP-2 (Li et al., 2023) and MiniGPT-4 (Zhu et al., 2023b), the recent public and reproducible multimodal models for visio-linguistic understanding. We evaluate InstructGPT (Ouyang et al., 2022) as the LLM backend considering the reproducibility of model performance, as stronger closed-source LLMs such as ChatGPT and GPT-4 (OpenAI, 2023) will be updated periodically. Details of prompt design can be found in Appendix E.\nEvaluation Metrics. For generation tasks, we employ conventional metrics BLeU-2 (Papineni et al., 2002), Rouge-L (Lin, 2004), and BERTScore (deberta-xlarge-mnli) (Zhang et al., 2020). Considering the limitation of the automatic metrics capped at the reference quality (Zhu et al., 2023a), we conduct qualitative analysis to compare model and human predictions. For emotion interpretation as a multilabel classification, we adopt the macro precision, recall, and F1 scores as metrics. To fur-\nther validate whether the automatic metrics based on our annotated reference answers align with the actual quality of model predictions, we conduct additional human (on a subset \u2013 238 instances \u2013 10% of the whole set) and GPT-4 evaluation (on the whole set) on event causality inference results."
        },
        {
            "heading": "6.2 Results",
            "text": "We compare different models in zero-shot and few-shot settings. In event causality inference, we compare the impacts of CoT in different formats (indicated by \u201cToM\u201d), including model-generated general intermediate steps (\u2717), model-generated ToM (\u2713\u2717), and human-annotated ToM (\u2713).\nRole Identification. We see a huge gap between model and human generations in zero-shot prompting, while few-shot brings significant performance gain, especially on InstructGPT. This demonstrates the stronger ability of LLMs for in-context learning compared with MiniGPT-4. However, we observe a trend of performance drop when enhancing reasoning via CoT. This drop may be caused by the uncertainty due to task difficulty, leading to error\naccumulation in reasoning.\nEmotion Interpretation. Likewise, InstructGPT shows a stronger in-context learning ability. Interestingly, MiniGPT-4 exhibits substantially poor performance on this task compared with the other models. We further diagnose this via the labelwise scores in Figure 5. The poor recall scores of MiniGPT-4 might be one of the reasons for the failure, as it tends to conduct single-label classification, neglecting the instruction in most cases. On the other hand, when sufficient information is provided, i.e., with multimodal inputs and CoT reasoning, there is a stable increase in the F1 score. This disparity in how CoT works for role and emotion predictions may be attributed to the different degrees of uncertainty in reasoning for the two tasks. For example, models can resort to an expedient strategy to interpret emotions directly based on human facial expressions.\nEvent Causality Inference. We evaluate the effect of ToM-enhanced CoT reasoning on both multimodal and language models. For MiniGPT-4, basic CoT reasoning without a specified format or content of ToM cannot guarantee an improvement in performance. This is in accordance with our observation on role recognition that higher uncertainty may cause a performance drop. However, as demonstrated by the ToM-enhanced CoT reasoning, our proposed human-centric tasks can benefit the final inference by incorporating ToM information about roles and emotions. Furthermore, despite the incompleteness of labeled human predictions (as shown in Table 1), human ToM still exhibits a more significant effect in the reasoning process."
        },
        {
            "heading": "6.3 Ablation Study",
            "text": "We conduct further analysis to probe the impact of different modalities and ToM-CoT reasoning.\nVision vs. Language Models. In the diagnostic tasks, we observe large differences in the generations between multimodal and language models. As the performance drops remarkably when in-\ncorporating textual information into multimodal models such as MiniGPT-4, we see that these multimodal models still struggle to handle long input contexts. This demonstrates the importance of LLM incorporation for multimodal understanding to enhance and guide information extraction and deduction for further reasoning.\nToM-enhanced CoT Reasoning. The performance gain from CoT reasoning in Table 4 varies when incorporating intermediate inferences from different sources, where human-annotated ToM (partially labeled) still outperforms the others. This demonstrates the deficiency of current large foundation models in eliciting and utilizing the ToM inferences for better reasoning."
        },
        {
            "heading": "6.4 Qualitative Evaluation",
            "text": "We compare the InstructGPT-based model with the CoT and/or ToM mechanisms in reasoning against the vanilla version that only uses few-shot prompting in human evaluation. We choose the criteria including plausibility, relevance, completeness, and overall quality and let the evaluators judge which one is better. Table 5 shows the human evaluated win/lose rate on 10% (238 instances) of the whole dataset on event causality inference. To obtain a more complete understanding, we conduct GPT-4 evaluation on the whole set. Specifically, we adapt the scoring framework from Zheng et al. (2023), with our annotated inferences as the reference answers, where scores range from 1 to 10.\nWe observe the same trend of model performance on human judgment and GPT-4 scoring in terms of the overall quality of predictions. For example, the CoT framework with human-annotated ToM achieves the highest GPT-4 score and win rate at 6.36 and 59%, respectively. However, we see different trends in the relevance and completeness ratings, where the model-generated ToM-enhanced CoT achieves the best completeness but worst relevance scores. This may come from the LLM abilities of information extraction and understanding which, however, also brings hallucination.\nshown in green , pink , and blue , respectively. Imperfections are highlighted in yellow ."
        },
        {
            "heading": "6.5 Discussion",
            "text": "We discuss our main findings in qualitative analysis by answering the following questions:"
        },
        {
            "heading": "Q1. Can models maintain reasoning robustness when input information varies in format?",
            "text": "As shown in Figure 6, LLMs present significantly higher adaptiveness to elicit different input information for reasoning. For example, InstructGPT directly synthesizes the character emotional traits such as \u201ccuriosity\u201d in the human-annotated ToMenhanced inference, while MiniGPT-4 is still at copy-and-paste level in text generation and tends to focus more on descriptive information in vision."
        },
        {
            "heading": "Q2. Can models maintain consistency and faithfulness throughout ToM-CoT reasoning?",
            "text": "MiniGPT-4 shows fact-level consistency in inference via reiterating or rephrasing selected spans. However, it struggles in reasoning about implicit or intermediate information. On the other hand, despite the advancement of LLMs, they may produce problematic hallucination, i.e., imperfect predicted ToM such as \u201cconfusion\u201d can lead to wrong final inference that may totally contradict the fact."
        },
        {
            "heading": "Q3. What potential bias exists in ECHO that",
            "text": "can lead to erroneous model predictions? One crucial problem we find in the MiniGPT-4 outputs is that it tends to randomly check one emotion option when it is not confident in the selection. This indicates a high uncertainty in the multimodal model in the mental state interpretation of humans. Possible reasons can come from both the model\nand data sides. Specifically, information from still frames can cause ambiguity without clip details, as shown by the erroneous \u201cconfusion\u201d in Figure 6. On the other hand, we acknowledge that our dataset ECHO represents a subset of social interactions, but we view it as an initial and specific step to explore ToM understanding of social intelligence."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce a visio-linguistic dataset ECHO to probe human-centric social intelligence. With our ToM-enhanced CoT framework, we diagnose the reasoning ability of large foundation models. Experiment results and further analysis demonstrate the deficiency of current AI systems and potential bias in ECHO for efficient, correct, and consistent reasoning. We foresee follow-up work on both model and data facets to develop faithful reasoning across a broader range of social scenarios.\nLimitations\nDataset Scale and Generalizability. As shown in Table 1 and Section 6.5, there can be potential bias in ECHO since we only label half of the featured characters to reason about their ToM. This imbalance of human belief considerations can lead to bias in final inferences as models may only focus on the thoughts and intentions of some of the characters. Furthermore, the reliance on crime scene content may restrict its applicability to a specific genre related to crime instead of daily life scenarios. While our ToM annotations (e.g., emotions)\ncan represent human\u2019s mental states in daily life, future work may further explore whether and how ToM inferences in different distributions can assist human-centric reasoning in more general scenarios.\nDataset Construction. We may lack a detailed analysis of the visual representations to demonstrate how this information complements the textual inputs. In event causality inference, we adopted automatic event determination to ease the efforts of human annotation, which cannot prioritize salience within the plot. This means that certain events, potentially more interesting or pertinent for causal reasoning, may go unselected.\nIn future work, we will further refine ECHO to validate and extend visio-linguistic ToM inferences to improve the coverage and balance of event topics, reasoning types, and source of inference evidence.\nEthics Statement\nWe have received approval from the Institutional Review Board (IRB)6 for our data annotation. We design the training tutorial and experimental sessions as guided and reviewed by the IRB to maintain minimal risks to participants. The review process took two months to complete.\nSince ECHO contains criminal data with violent content, it may enable malevolent imitation actors or harm to specific groups of people. To avoid this misuse potential of ECHO, we will impose strict rules for access requirements and frequently track the follow-up works to constrain its usage within research-only goals. In the future, we will also make regular updates on ECHO to further extend and balance the ToM attributes to alleviate potential bias and ambiguity in datapoints for better generalizability of our diagnostic tasks."
        },
        {
            "heading": "Acknowledgements",
            "text": "The computational work for this paper was partially performed on resources of the National Supercomputing Centre (NSCC), Singapore7. We would like to thank our annotators for their time and efforts in annotating and validating ECHO instances and for our pilot subjects for their insightful feedback for our annotation user interface refinement.\n6https://www.nus.edu.sg/research/irb. 7https://www.nscc.sg/"
        },
        {
            "heading": "A Annotator Recruitment and Training",
            "text": "We recruited prospective annotators considering both quality and diversity. First, all selected annotators possess a minimum of undergraduate education and demonstrate familiarity or expertise with the TV series CSI to ensure the annotation quality. Second, to mitigate potential biases, our recruitment aimed for a balanced distribution across various factors, including gender, academic disciplines, and nationalities.\nWe piloted our preliminary annotation pipeline for refinement. After subsequent refinement, prospective annotators are first asked to watch a prepared instruction video8 and complete a pre-annotation quiz to demonstrate their understanding of tasks. Our team manually checked applicants\u2019 responses for quality, admitting qualified subjects as participants. We show one example quiz question as follows.\nGeneral Qn. Select the important elements to focus on for each sample. \u25a1 a short video clip with the provided start and end times \u25a1 the whole video clip of the episode \u25a1 transcripts of the clip \u25a1 align characters in the clip with their names in the transcripts \u25a1 the key character to focus on for all the annotation questions \u25a1 different characters to focus on for different annotation questions Q2-Related Qn. Select the options which can be the input to Q2. \u25a1 occupation (for a living), e.g. singer, police, officer \u25a1 role (role in the event), e.g., driver, customer, suspect of the crime \u25a1 role (relation with others), e.g., the woman who stares at the others \u25a1 appearance description, e.g., the girl in a white shirt Q3-Related Qn. What kind of emotions should be selected, and what is \u201cOthers\u201d for? \u25a1 some emotions appearing via the key character\u2019s facial expressions/actions \u25a1 all emotions appearing via the key character\u2019s facial expressions/actions \u25a1 possible emotions of the key characters reflected by the others they interact with \u25a1 \"Others\": to add emotions that aren\u2019t included in the options Q4-Related Qn1. What kind of & How many frames should be selected? \u25a1 all the frames indicating the change of (emotional, motional) states of the key characters \u25a1 the frames should preferrably be evenly distributed on the clip \u25a1 there is no lower bound of the the selected number 8https://vlcsr.comp.nus.edu.sg/static/video/VL_event_causality_annotation_instruction.mp4\n\u25a1 there is no upper bound of the the selected number Q4-Related Qn2. Requirements for the frames to be checked. \u25a1 contain the facial expressions / recognizable actions of the key character \u25a1 contain only external information (others reflection) which may indicate the state of the key character Q5-Related Qn. Select the requirements for the input text. \u25a1 Keep a low overlap-rate with the transcripts (< 30%) \u25a1 Start with the text provided in front of the text box \u25a1 Focus on the key character \u25a1 Can consider emotion change of the character as labeled in Q3 \u25a1 Try to elaborate on the intrinsic logic among events which aren\u2019t directly described in the\nclip/transcripts"
        },
        {
            "heading": "B Annotation Interface",
            "text": "We provide a publicly available webpage9 to demonstrate the example annotations on an instance in Round Two. Below is an example of the detailed annotation questions.\n9https://yuxixie.github.io/_pages/CSI_example.html"
        },
        {
            "heading": "C Emotion Categorization",
            "text": "We adapt the original 27 emotions from Cowen and Keltner (2017) to crime plots, merging them into 13 emotions more suited for the domain: \u2022 Anger: wrath, outrage, fury, violence, irritability, hostility, resentment; \u2022 Boredom: blahs, doldrums, ennui, weariness, listlessness, restlessness; \u2022 Calmness: blahs, doldrums, ennui, weariness, listlessness, restlessness; \u2022 Disgust: contempt, scorn, disdain, aversion, distaste, revulsion; \u2022 Doubt: uncertainty, confusion, distrust; \u2022 Entrancement: brooding, reverie, contemplation, daydreaming, cogitation, detachment; \u2022 Fear: anxiety, dread, fright, panic, nervousness; \u2022 Interest: trust, kindness, affection, devotion, acceptance, love, anticipation, friendliness; \u2022 Joy: enjoyment, bliss, happiness, relief, delight, pride, thrill, ecstasy; \u2022 Sadness: grief, sorrow, gloom, despair, melancholy, loneliness, depression; \u2022 Shame: regret, guilt, embarrassment, remorse; \u2022 Surprise: astound, shock, astonishment, wonder; \u2022 Sympathy: commiseration, compassion, feeling."
        },
        {
            "heading": "D Frequently Asked Questions",
            "text": "D.1 The fact that multiple different causes of an event can exist complicates the evaluation of the CoT approach for event causality. How is evaluation impacted when multiple causes of an event may exist?\nWe dealt with the one-to-many problem on two aspects of data collection. First, we provide multiple reference inferences (annotations) for each event, as shown in Table 1. Second, we narrow the scope of potential causes by employing Theory-of-Mind (ToM) as the intermediate reasoning step. This serves as a directional constraint to reduce the search space of potential CoT chains.\nConsidering that our annotated reference answers may not capture the full spectrum of possible causes, we conduct both human and GPT-4 evaluations to provide qualitative analysis on the model predictions. The human evaluation results are consistent with the automatic metrics assessed in Table 4. Furthermore, the performance gain brought by ToM-constrained CoT shows the importance of ToM in our human-centric reasoning task.\nD.2 How many instances are rejected and revised during inference validation? What are the major error types and feedback types encountered during annotation and verification?\nOut of the 5, 746 annotations collected from Round Two, we finally retained 4, 280 annotations, as indicated in Table 3. Among these, 107 were revised and accepted following re-annotation. In validation, we check the annotation quality considering three criteria: plausibility, relevance, and completeness. Specifically, we mainly reject instances if they exhibit weak connections between the annotated inferences and the plot contents or the corresponding character roles and emotions.\nFeedback from annotators suggests two primary causes for annotation errors: 1) the imperfect event selection (where we reject the entire instance), and 2) insufficient incorporation of ToM in reasoning (where we reject outright or send it back for re-annotation).\nD.3 Why use instructGPT instead of stronger LLMs like ChatGPT and GPT-4?\nWe use InstructGPT considering the reproducibility of model performance, as stronger closed-source LLMs like ChatGPT and GPT-4 will be updated periodically. For reference, we conduct an additional experiment using ChatGPT (gpt-3.5-turbo-0613) in Table 6 on the task of event causality inference.\nE Implementation Details\nWe detail our experiment setup and prompt construction in this section.\nE.1 Setup For multimodal models, we use BLIP-210 (blip2_t5) (Li et al., 2023) and MiniGPT-411 (prerained_minigpt4_7b) (Zhu et al., 2023b), the recent public and reproducible multimodal models for visio-linguistic understanding. Specifically, we use pretrain_flant5xl for BLIP-2 due to the computation limit. For LLM backend, we use InstructGPT (text-davinci-003) (Ouyang et al., 2022).\nIn the automatic evaluation of text generation tasks, we measure the similarity between model outputs and human annotations, compared with the inter-agreement among annotators using the same metrics. For the multilabel classification task, the macro precision, recall, and F1 scores measure both the model\u2013human and inter-annotator agreement. The inter-annotator agreement considers the instance-wide similarity between one annotator and the others.\nE.2 Pipeline Construction Denote the multimodal model and the language model as M and L respectively. The process for ToMenhanced CoT reasoning for event causality inference is as follows (Steps 2 and 3 can be merged for simplicity, with further post-processing to extract the final answers):\nStep 1: Visual Information Extraction using M. M generates visual descriptions based on the input video clips. The prompt to M determines the type and focus of the visual descriptions generated. \u2022 input: clip frames \u2022 output: visual descriptions\nStep 2: Role and Emotion Identification using L. We augment the text prompt for L with the visual information obtained previously, eliciting roles and emotions associated with given characters. \u2022 input: visual descriptions + textual context (screenplay) \u2022 output: roles and emotions of the specified characters\nStep 3: ToM-Enhanced CoT Reasoning using L. Using the predicted roles and emotions as intermediate reasoning products, we construct the CoT prompt for L, allowing it to perform ToM-enhanced reasoning about event causality. \u2022 input: visual descriptions + textual context (screenplay) + roles and emotions \u2022 output: event causality inference\nE.3 Prompt Design For multimodal prompting, we adopt the form of visual question answering where specific instructions will be provided for human-centric information extraction and inference. For LLM prompting, we accommodate information comprising screenplays, textual descriptions of visual frames in multi-granularities, and specific instructions to stimulate reasoning.\n10LAVIS: https://github.com/salesforce/LAVIS 11MiniGPT-4: https://minigpt-4.github.io/"
        }
    ],
    "title": "ECHO: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric ReasOning",
    "year": 2023
}