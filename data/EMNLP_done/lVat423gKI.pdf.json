{
    "abstractText": "Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and lowresource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models. This paper investigates the feasibility of explicitly transferring relatively rich factual knowledge from English to non-English languages. To accomplish this, we propose two parameter-free Language Representation Projection modules (LRP2). The first module converts non-English representations into English-like equivalents, while the second module reverts English-like representations back into representations of the corresponding non-English language. Experimental results on the mLAMA dataset demonstrate that LRP2 significantly improves factual knowledge retrieval accuracy and facilitates knowledge transferability across diverse nonEnglish languages. We further investigate the working mechanism of LRP2 from the perspectives of representation space and cross-lingual knowledge neuron.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shaoyang Xu"
        },
        {
            "affiliations": [],
            "name": "Junzhuo Li"
        },
        {
            "affiliations": [],
            "name": "Deyi Xiong"
        }
    ],
    "id": "SP:157073e439c04d37159acc9163e9ee1c4d45229e",
    "references": [
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Nan Yang",
                "Saksham Singhal",
                "Wenhui Wang",
                "Xia Song",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Ming Zhou."
            ],
            "title": "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel R. Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: evaluating crosslingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
            "year": 2018
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei."
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Antonios Anastasopoulos",
                "Jun Araki",
                "Haibo Ding",
                "Graham Neubig."
            ],
            "title": "X-FACTR: multilingual factual knowledge retrieval from pretrained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Jun Araki",
                "Graham Neubig."
            ],
            "title": "How can we know what language models know",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:423\u2013438.",
            "year": 2020
        },
        {
            "authors": [
                "Nora Kassner",
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Multilingual LAMA: investigating knowledge in multilingual pretrained language models",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk."
            ],
            "title": "MLQA: evaluating cross-lingual extractive question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL",
            "year": 2020
        },
        {
            "authors": [
                "Jindrich Libovick\u00fd",
                "Rudolf Rosa",
                "Alexander Fraser."
            ],
            "title": "On the language neutrality of pre-trained multilingual representations",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume",
            "year": 2020
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona T. Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual knowledge in GPT",
            "venue": "CoRR, abs/2202.05262.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D. Manning."
            ],
            "title": "Fast model editing at scale",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick S.H. Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Klamm",
                "Colin Leong",
                "Daniel van Strien",
                "David Ifeoluwa Adelani"
            ],
            "title": "BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100",
            "year": 2022
        },
        {
            "authors": [
                "Anton Sinitsin",
                "Vsevolod Plokhotnyuk",
                "Dmitry V. Pyrkin",
                "Sergei Popov",
                "Artem Babenko."
            ],
            "title": "Editable neural networks",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Xianze Wu",
                "Zaixiang Zheng",
                "Hao Zhou",
                "Yong Yu."
            ],
            "title": "Laft: Cross-lingual transfer for text generation by language-agnostic finetuning",
            "venue": "Proceedings of the 15th International Conference on Natural Language Generation, pages 260\u2013266.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Xu",
                "Yutai Hou",
                "Wanxiang Che."
            ],
            "title": "Language anisotropic cross-lingual model editing",
            "venue": "CoRR, abs/2205.12677.",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Huiyun Yang",
                "Huadong Chen",
                "Hao Zhou",
                "Lei Li."
            ],
            "title": "Enhancing cross-lingual transfer by manifold mixup",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Da Yin",
                "Hritik Bansal",
                "Masoud Monajatipoor",
                "Liunian Harold Li",
                "Kai-Wei Chang."
            ],
            "title": "Geomlama: Geo-diverse commonsense probing on multilingual pre-trained language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Zhao",
                "Steffen Eger",
                "Johannes Bjerva",
                "Isabelle Augenstein."
            ],
            "title": "Inducing language-agnostic multilingual representations",
            "venue": "Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, *SEM 2021, Online, Au-",
            "year": 2021
        },
        {
            "authors": [
                "Bo Zheng",
                "Li Dong",
                "Shaohan Huang",
                "Wenhui Wang",
                "Zewen Chi",
                "Saksham Singhal",
                "Wanxiang Che",
                "Ting Liu",
                "Xia Song",
                "Furu Wei."
            ],
            "title": "Consistency regularization for cross-lingual fine-tuning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Joey Tianyi Zhou",
                "Hao Zhang",
                "Di Jin",
                "Hongyuan Zhu",
                "Meng Fang",
                "Rick Siow Mong Goh",
                "Kenneth Kwok."
            ],
            "title": "Dual adversarial neural transfer for lowresource named entity recognition",
            "venue": "Proceedings of the 57th Conference of the Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "mLAMA (Kassner"
            ],
            "title": "2021) is a multilingual factual knowledge probing dataset containing 53 languages and 44 factual relations, and the TREx part contains 41 of them. To obtain language vectors, we used OPUS-100",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Previous studies demonstrate that a language model is a knowledge base that can recall factual knowledge without additional fine-tuning (Petroni et al., 2019; Jiang et al., 2020b). This task of factual knowledge probing, aiming to examine what factual knowledge language models capture during the pre-training phase, can be extended to multiple languages in multilingual pretrained language models, e.g., mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), mT5 (Xue et al., 2021), XGLM (Lin et al., 2022) and BLOOM (Scao et al., 2022). Although multilingual pretrained models serve as repositories of multilingual factual knowledge, a factual knowledge gap exists\n\u2217 Corresponding author.\nbetween English and other languages in terms of the amount of factual knowledge captured for each language (Kassner et al., 2021; Jiang et al., 2020a).\nMany works on cross-lingual transfer (Conneau et al., 2020; Chi et al., 2021; Wu et al., 2022; Yang et al., 2022) validate the effectiveness of cross-lingual alignment of representation spaces in facilitating cross-lingual knowledge transfer. These studies primarily evaluate their methods on specific downstream tasks, including natural language inference (Conneau et al., 2018), sentence retrieval (Artetxe and Schwenk, 2019), question answering (Lewis et al., 2020) and text generation (Wu et al., 2022), etc.\nDifferent from such studies, we focus on the task of factual knowledge probing in multilingual pretrained language models and attempt to answer a question in this paper: Can cross-lingual alignment of representation spaces enable factual knowledge transfer across languages? In particular, we explore the feasibility of transferring factual knowledge from English to non-English languages.\nTo answer this question, we propose LRP2, which incorporates two parameter-free Language Representation Projection modules into multilingual pretrained models: a language-independent representation projection module that projects representations of non-English languages into Englishlike representations and a language-specific representation projection module that maps the Englishlike representations back to representations of individual non-English languages. These two modules, as depicted in Figure 1, locate at different layers of Transformer.\nExperiments on mLAMA (Kassner et al., 2021) suggest that LRP2 improves factual knowledge retrieval accuracy and facilitates knowledge transfer across diverse languages. We further conduct indepth analysis to investigate the varying degrees of representation alignment required by different non-English languages, as well as the transferabil-\nity of different types of factual knowledge. Delving into the working mechanism of LRP2, we identify cross-lingual knowledge neurons in multilingual pretrained language models.\nOur contributions are summarized as follows.\n\u2022 We propose a parameter-free framework LRP2 that enhances factual knowledge retrieval accuracy and cross-lingual factual knowledge transfer.\n\u2022 We reveal that LRP2 poses an impact on the alignment of representation spaces and enhances the overlap of knowledge neurons across languages.\n\u2022 We discover that cross-lingual knowledge neurons exist in multilingual language models."
        },
        {
            "heading": "2 Multilingual Factual Knowledge Probing",
            "text": "In the multilingual factual knowledge probing task, multilingual pretrained language models take language-specific fill-in-the-blank queries as input, such as \"The capital of England is [MASK]\" in English, or the corresponding Chinese question \"\u82f1 \u56fd\u7684\u9996\u90fd\u662f[MASK]\". As a knowledge base, the probed pretrained language model initially encodes the input query, then retrieves its parameterized memory and ultimately predicts an answer with a probability distribution over the vocabulary.\nThe success of factual knowledge transfer across languages relies on a language-independent representation space for different languages to trigger similar memories within the probed multilingual pretrained model and language-specific representations to allow the model to predict tokens in the corresponding language."
        },
        {
            "heading": "3 LRP2",
            "text": "The primary objective of LRP2 is to bridge the gap of factual knowledge probing between English and non-English languages by aligning their representation spaces.\nLibovick\u00fd et al. (2020) demonstrate that it is possible to induce language-neutral representations for a given language, by subtracting its corresponding language vector. The proposed LRP2 draws inspiration from this work and initiates its process by computing a set of language vectors Vl for each language l. Specifically, for language l, we feed a set of its sentences into the multilingual\nChange:1\npretrained language model to be probed. From the i-th layer of the model, we gather sentence-level vectors through mean-pooling over the representations of all tokens in the corresponding sentence. We then further average these sentence vectors, obtaining vil \u2208 Rn, where n is the hidden dimension of the model. In this way, we collect a set of vectors Vl = [v1l ,v2l , ...,vLl ], where L denotes the number of layers of the model. These language vectors serve as the basis for language representation projection within the proposed LRP2 framework.\nAs illustrated in Figure 1, LRP2 incorporates two language representation projection modules into the probed multilingual pretrained language model, which are referred to as the Language-Independent Representation Projection (LIRP) module and the Language-Specific Representation Projection (LSRP) module, respectively. These two modules\nare inserted into the model as two additional layers. Representations of a non-English language with limited information are projected to the English representation space by LIRP, which enables the non-English language to access relatively rich memory encoded in the parameters of the model, in the form of English-like representations. The accessed memory is then projected back to the non-English language by LSRP so that answers in the corresponding non-English language can be yielded.\nSpecifically, given an input query in a nonEnglish language l, the LIRP first projects the contextual representations from the i-th layer of the model into English-like representations, which can be formulated as follows:\nh\u0302il = h i l \u2212 vil + vien (1 \u2264 i < L) (1)\nwhere hil represent the i-th layer hidden states of the input query in language l. vil and v i en denote the language vectors of the i-th layer for non-English language l and English respectively. By performing this projection, the representations of non-English language l are mapped into the English space and subsequently fed to the succeeding layers.\nAs mentioned in Section 2, in the multilingual factual knowledge probing task, it is essential for the multilingual pretrained language model to yield answers in the corresponding language. To recover the language-specific information of the input language, we insert the LSRP into the j-th layer of the model. The back-projection to the input language is formulated as:\nh\u0302jl = h j l \u2212 v j en + v j l (i < j \u2264 L) (2)\nwhere hjl represent the j-th layer hidden states of the input query in language l. hjl are English-like representations because of the first projection. They are transformed back into the language l\u2019s representation space, resulting in h\u0302jl . These languagespecific representations are further fed to the succeeding layers of the model."
        },
        {
            "heading": "4 Experiments",
            "text": "We conducted extensive experiments to examine the effectiveness of the proposed LRP2 framework in factual knowledge transfer across languages."
        },
        {
            "heading": "4.1 Settings",
            "text": "We utilized the TREx portion of mLAMA (Kassner et al., 2021) for our experiments. Further information regarding mLAMA and the dataset employed to acquire language vectors can be found in Appendix A.1. We calculated factual knowledge retrieval accuracy as well as English-centric cross-lingual transferability for each language. The details on these evaluation metrics can be found in Appendix A.2. The experiments were based on two multilingual pretrained language models, mBERT1 and BLOOM2 (the version with 559 million parameters). The details of probing them can be found in Appendix A.3. Note that the i-layer for inserting LIRP and the j-layer for inserting LSRP are two hyperparameters, the details on the setting of them can be found in Appendix A.4.\n1https://huggingface.co/bert-base-multilingual-cased 2https://huggingface.co/bigscience/bloom-560m\n4:zh\u2192en projection\n10:en\u2192zh projection\n(a) Representation Spaces\nzh-en\u4e4b\u95f4\u76f8\u540c\u5173\u7cfb\u795e\u7ecf\u5143\uff0c\u5728\u4e0d\u540c\u5c42\u7684\u91cd\u53e0\u7387\uff08mbert/pseudo-\n\u591a\u8bed\u63a2\u6d4b\u7ed3\u679c\n4:zh->en translation 10:en->zh\ntranslation\n4:zh\u2192 projection 10:en\u2192\nprojection\n(b) Knowledge Neurons\nFigure 2: Distance between representation spaces and overlap rate of knowledge neurons in different layers. The languages considered here are Chinese and English. \u2019mBERT (LRP2)\u2019 represents the results with the insertion of the LIRP module into the 4-th layer and the LSRP module into the 10-th layer of mBERT, which yields the best transferability result of Chinese.\nModel Same Different Avg mBERT 17.9% 11.5% 11.7% mBERT (LRP2) 18.5% 11.9% 12.1%\nTable 2: Overlap rate of knowledge neurons for factual relations in Chinese and English."
        },
        {
            "heading": "4.2 Results",
            "text": "Table 1 presents the experimental results on mLAMA, it shows that LRP2 achieves significant improvements in terms of both factual knowledge retrieval accuracy and cross-lingual transferability across various non-English languages over the baseline. The results indicate that cross-lingual alignment of representation spaces indeed facilitates the transfer of rich factual knowledge from English to non-English. More specifically, for both mBERT and BLOOM, LRP2 demonstrates better performance in certain non-Indo-European languages as well as medium- and high-resource languages.\nAdditional experimental results on XFACTR (Jiang et al., 2020a) are provided in Appendix B.1. To provide further insights, we present the performance changes for different languages as the number of layers between LIRP and LSRP varies in Appendix B.2. The specific effects of LRP2 on different non-English languages are provided in Appendix B.3. In addition, we observe that the transferability of knowledge shows variations across different types of factual relations, as evidenced in Appendix B.4."
        },
        {
            "heading": "5 Working Mechanism of LRP2",
            "text": "In this section, we study the working mechanism of LRP2 from the perspectives of representation space and knowledge neuron."
        },
        {
            "heading": "5.1 LRP2 Affects the Alignment of Representation Spaces across Languages",
            "text": "We utilized Chinese-English parallel queries in the mLAMA dataset to collect sentence representations and further calculated the layer-wise cosine similarity of these two languages\u2019 sentence representations, as the distance between the representation spaces of these two languages. We conducted a comparative analysis of the distance with and without the utilization of LRP2.\nFigure 2a presents the distance between the representation spaces of Chinese and English. It clearly shows the distinct functions of LIRP and LSRP. Specifically, the LIRP module first brings Chinese sentences closer to the representation space of English, thereby facilitating cross-lingual knowledge transfer, while the LSRP module increases the distance between Chinese sentences and the representation space of English, inducing language-specific outputs in Chinese."
        },
        {
            "heading": "5.2 LRP2 Enhances the Overlap of Knowledge Neurons across Languages",
            "text": "Dai et al. (2022) discover that knowledge neurons expressing specific factual knowledge exist in pretrained Transformers. Building upon their work, we identify knowledge neurons in multilingual pretrained Transformers and employ them to elucidate the working mechanism of LRP2. The details on how we identify knowledge neurons in multilingual pretrained language models are provided in Appendix C.\nTable 2 showcases the overlap rate of knowledge neurons for factual relations in Chinese and English. Notably, we have two interesting findings. First, the overlap rate of knowledge neurons associated with the same relations is considerably higher compared to that with different relations, suggesting the existence of language-independent knowledge neurons within mBERT. Second, LRP2 increases the overlap rate of knowledge neurons between Chinese and English. This improvement indicates that LRP2 facilitates the alignment of English and non-English representation spaces and enhances the activation of knowledge neurons in non-English languages, making them more similar to those in English. In this way, non-English languages acquire factual knowledge transferred from English. Additionally, Figure 2b visualizes the overlap rate of knowledge neurons across different layers. Notably, the layers between LIRP and\nLSRP exhibit a prominent increase in the overlap rate of knowledge neurons between Chinese and English."
        },
        {
            "heading": "6 Related Work",
            "text": "Factual Knowledge Probing Previous works (Petroni et al., 2019; Jiang et al., 2020b) have shown that a language model is a knowledge base. Subsequent works (Kassner et al., 2021; Jiang et al., 2020a) extend monolingual factual knowledge probing to multiple languages. Notably, Jiang et al. (2020a) improve multilingual factual knowledge probing in a code-switching style. Significantly different from this, we suggest that it is essential to allow multilingual pretrained language models to yield language-specific answers.\nModel Editing A variety of approaches have been proposed to edit knowledge in monolingual language models (Sinitsin et al., 2020; Cao et al., 2021; Mitchell et al., 2022; Meng et al., 2022; Dai et al., 2022). Recently, Xu et al. (2022) define a cross-lingual model editing task, where knowledge updates in one language need to occur in other languages as well. In this paper, we focus on factual knowledge that already exists in multilingual language models and enhance the transferability of them, rather than trying to update a model with new knowledge.\nCross-lingual Knowledge Transfer Crosslingual transfer learning approaches are usually categorized into instance transfer (Zheng et al., 2021; Yang et al., 2022), parameter transfer (Chen et al., 2019; Zhou et al., 2019), and feature transfer (Libovick\u00fd et al., 2020; Zhao et al., 2021). Most of these works explore cross-lingual knowledge transfer on specific downstream tasks, while we focus on factual knowledge captured by language models and explore the possibility of cross-lingual factual knowledge transfer."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have presented a simple yet effective method to transfer factual knowledge from English to nonEnglish languages in multilingual pretrained language models. We empirically confirm that crosslingual alignment of representation spaces enables factual knowledge transfer across languages in multilingual pretrained language models. Further analysis on knowledge neurons shows that the align-\nment of English and non-English representation spaces brought by LRP2 can help non-English languages to stimulate knowledge neurons similar to English, thereby acquiring knowledge transferred from English."
        },
        {
            "heading": "Limitations",
            "text": "While LRP2 significantly improves factual knowledge retrieval accuracy and facilitates knowledge transferability across diverse non-English languages, it is noteworthy that the LIRP and LSRP modules in LRP2 are inserted into multilingual pretrained language models as two additional layers. Thus, the effectiveness of LRP2 heavily relies on the inherent capabilities of multilingual pretrained language models.\nThrough extensive experiments conducted on the proposed LRP2 framework, we have demonstrated that cross-lingual alignment of representation spaces enables factual knowledge transfer across different languages. Although this finding is applicable to multilingual pretrained language models of varying architectures, our experiments are limited to two relatively small models due to the limited compute resource available to us. We plan to investigate LRP2 on larger language models when more compute resource is available."
        },
        {
            "heading": "Acknowledgements",
            "text": "The present research was supported by Zhejiang Lab (No. 2022KH0AB01). We would like to thank the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Experiment Details",
            "text": ""
        },
        {
            "heading": "A.1 Datasets",
            "text": "mLAMA (Kassner et al., 2021) is a multilingual factual knowledge probing dataset containing 53 languages and 44 factual relations, and the TREx part contains 41 of them. To obtain language vectors, we used OPUS-100 (Zhang et al., 2020) to collect 10,000 filtered sentences for most of the 53 languages, and for languages not included in OPUS-100, such as ceb, we obtained data from the OPUS.3"
        },
        {
            "heading": "A.2 Evaluation Metrics",
            "text": "We calculated factual knowledge retrieval accuracy for each language l as Accl =\n|Rl| |Dl| \u2217 100, where\nRl represents the set of correctly predicted knowledge for language l and Dl represents the entire probing data for language l. Additionally, we calculated English-centric cross-lingual transferability as Transl =\n|Rl\u2229Ren| |Rl\u222aRen| \u2217 100. Here, the denom-\ninator |Rl \u222a Ren| corresponds to the amount of knowledge stored in the probed model, whether in non-English language l or in English form, while the numerator |Rl \u2229Ren| represents the amount of the stored knowledge both in the form of language l and English, indicating the amount of transferable knowledge."
        },
        {
            "heading": "A.3 Probing mBERT and BLOOM",
            "text": "Following mLAMA (Kassner et al., 2021), we adopted a typed querying approach for probing. This entails considering all candidate objects of a relation as the candidate pool. For each query associated with a specific relation, we determined the ranking of the correct answer within its candidate pool. The prediction is considered correct if the correct answer is ranked at the top position.\nProbing mBERT When probing mBERT, the input query follows the format like \"The capital of England is [MASK]\", the model\u2019s probability predictions for the [MASK] tokens are used to compute the ranking. The number of [MASK] tokens depends on the length of the tokenized object to be predicted. In cases of multiple [MASK] tokens, we calculated the average log probability of these tokens. We utilized the complete candidate pools for probing mBERT (with an average number of candidates per relation of approximately 90).\n3https://opus.nlpl.eu\nProbing BLOOM We notice that the objects to be predicted can appear in the middle of the corresponding query templates in the mLAMA dataset. However, due to the pre-training task of causal language modeling, autoregressive models like BLOOM are more adept at answering factual knowledge questions by predicting the next token in a given query. To address the mismatch between the form of query templates in mLAMA and the generative nature of BLOOM, we employed a compromise approach inspired by Yin et al. (2022). Specifically, when probing the autoregressive BLOOM, we filled each query with objects from its candidate pool to construct complete sentences. We then calculated the model\u2019s generation probabilities for these sentences, which serve as the prediction probabilities for different objects. Due to limitation of compute resource, we restricted the size of the candidate pools to 10 when probing BLOOM."
        },
        {
            "heading": "A.4 Hyperparameters",
            "text": "The i-layer for inserting LIRP and the j-layer for inserting LSRP are two hyperparameters. We systematically evaluate different combinations of them for each language and report the best results. This exploration allows us to investigate the potential for cross-lingual factual knowledge transfer facilitated by the alignment of representation spaces."
        },
        {
            "heading": "B Additional Results",
            "text": ""
        },
        {
            "heading": "B.1 Experiments on X-FACTR",
            "text": "Yet another dataset used to probe multilingual factual knowledge is X-FACTR (Jiang et al., 2020a). In contrast to mLAMA, this dataset contains fewer languages and slightly more factual relations (23 and 46, respectively). We supplemented experiments on 6 languages of X-FACTR, using mBERT as the baseline model. The results are listed in Table 3, which shows that LRP2 can also achieve improvements on the X-FACTR dataset.\n\u4e0d\u540c\u5c42\u7684\u7ed3\u679c \u8bed\u2f94"
        },
        {
            "heading": "B.2 Different Languages Necessitate Varying Optimal Layer Settings",
            "text": "Figure 3 presents the change of cross-lingual transferability for five languages as the number of layers between LIRP and LSRP varies. Notably, we observe that different languages exhibit distinct requirements for representation space alignment to achieve optimal transferability. In addition, we notice that the performance of certain languages is very sensitive to the choice of model layers for the insertion of LIRP and LSRP modules. For certain numbers of layers between LIRP and LSRP for some languages, such as 9 for language eu in Figure 3a, none of the particular insertion settings (the layers where LIRP and LSRP are inserted into are 1/10, 2/11, 3/12, respectively) lead to efficient knowledge transfer. We hypothesize that such sensitivity may stem from the relatively fragile nature of the representation space learned by mBERT for these languages. Consequently, the representations of these languages could easily lose semantic information and become meaningless after language representation projections, leading to a complete failure of knowledge transfer.\nIn addition, Table 4 and Table 5 show mBERT\u2019s and BLOOM\u2019s optimal layer configurations for all languages respectively, further underscoring the substantial disparity in the optimal layer settings among various languages."
        },
        {
            "heading": "B.3 The Impact of LRP2 Differs across Non-English Languages",
            "text": "Figure 4 and Figure 5 illustrate the specific effects of LRP2 on different non-English languages for mBERT and BLOOM respectively. It is note-\nworthy that LRP2 is effective for languages that are not covered by the training data of BLOOM. This can be attributed to BLOOM\u2019s utilization of a byte-level BPE algorithm for subword tokenization (Scao et al., 2022), ensuring that unknown tokens are never yielded. In this way, unknown languages can be effectively represented to a certain extent, enabling the transfer of factual knowledge between them and other languages."
        },
        {
            "heading": "B.4 The Transferability Varies across Factual Relations",
            "text": "We assess the transferability change of each factual relation in every language and consider a factual relation to be transferable from English to a non-English language if its transferability improves under any configurations of the LIRP and LSRP modules. Figure 6 illustrates the transferable percentages across all factual relations for mBERT. We observe that 37 out of 41 relations exhibit transferability from English to over 80% non-English languages. Notably, the relations P17, P1412, and P138, representing Place (e.g., Germany, Ireland) and Language (e.g., Italian, Spanish) demonstrate consistent transferability across all languages. However, some factual relations display lower transferability, e.g., P413, P264, P140, and P108, which represent Athlete Position (e.g., midfielder, pitcher), Organization (e.g., Decca, Motown), Religion (e.g., Buddhism, Islam) and Organization (e.g., Apple, Microsoft), respectively.\nIn addition, Figure 7 reveals a similar trend in the transferability of factual relations between BLOOM and mBERT. Specifically, the factual relations P264, P413 and P449 exhibit lower transferability, while relations representing Place or Language, such as P937, P530, P407, P37, and so on, demonstrate higher transferability in BLOOM.\nC Identifying Knowledge Neurons in Multilingual Pretrained Models\nWe identify knowledge neurons in multilingual pretrained models using Knowledge Attribution proposed by Dai et al. (2022). We first identify the knowledge neurons of all prompts in a relation. Specifically, for each prompt, we calculate the knowledge attribution scores of neurons and take top-20 neurons as its knowledge neurons. Further, for each factual relation, we take the top-20 neurons with the highest number of occurrences in its all prompts as knowledge neurons\nof it. For a language, we identify knowledge neurons of all its factual relations in mLAMA, such as KN P101, KN P17, etc. Unlike Dai et al. (2022), we perform score ranking at each layer of the model, i.e., for a factual relation, we obtain its knowledge neurons in all layers, e.g., KN P101 = {KN 1P101,KN 2P101, ...,KNLP101}, where L is the number of layers of pretrained language models. Specifically, we identified knowledge neurons for both Chinese and English in mBERT. For Chinese, we additionally detected knowledge neurons under the configuration that yields the best transferability result of Chinese.\nMBERT\nLanguage\nPlace Organization\nOccupation Religion\nSpecialization\nMusic Genre Musical Instrument Undefined Athlete position\nFigure 7: Transferable percentages of all factual relations in mLAMA. Results are based on BLOOM."
        }
    ],
    "title": "Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?",
    "year": 2023
}