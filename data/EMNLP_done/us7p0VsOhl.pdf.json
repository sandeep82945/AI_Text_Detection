{
    "abstractText": "Graph reasoning contributes to the integration of discretely-distributed attentive information (clues) for Multi-party Dialogue Reading Comprehension (MDRC). This is attributed primarily to multi-hop reasoning over global conversational structures. However, existing approaches barely apply questions for anti-noise graph reasoning. More seriously, the local semantic structures in utterances are neglected, although they are beneficial for bridging across semantically-related clues. In this paper, we propose a question-aware global-to-local graph reasoning approach. It expands the canonical Interlocutor-Utterance graph by introducing a question node, enabling comprehensive global graph reasoning. More importantly, it constructs a semantic-role graph for each utterance, and accordingly performs local graph reasoning conditioned on the semantic relations. We design a two-stage encoder network to implement the progressive reasoning from the global graph to local. The experiments on the benchmark datasets Molweni and FriendsQA show that our approach yields significant improvements, compared to BERT and ELECTRA baselines. It achieves 73.6% and 77.2% F1-scores on Molweni and FriendsQA, respectively, outperforming state-of-the-art methods that employ different pretrained language models as backbones.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yanling Li"
        },
        {
            "affiliations": [],
            "name": "Bowei Zou"
        },
        {
            "affiliations": [],
            "name": "Yifan Fan"
        },
        {
            "affiliations": [],
            "name": "Xibo Li"
        },
        {
            "affiliations": [],
            "name": "Ai Ti Aw"
        },
        {
            "affiliations": [],
            "name": "Yu Hong"
        }
    ],
    "id": "SP:0d87a747e7fb7d83ebff441e435943fd0cacafd4",
    "references": [
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning."
            ],
            "title": "ELECTRA: Pretraining text encoders as discriminators rather than generators",
            "venue": "Proceedings of the ICLR 2020, pages 1\u201318.",
            "year": 2020
        },
        {
            "authors": [
                "Hossein Gholamalinezhad",
                "Hossein Khosravi."
            ],
            "title": "Pooling methods in deep neural networks, a review",
            "venue": "arXiv preprint arXiv:2009.07485.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 17th NAACL, volume 1, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "Proceedings of the ICLR 2017, pages 1\u201314.",
            "year": 2017
        },
        {
            "authors": [
                "Daniil Larionov",
                "Artem Shelmanov",
                "Elena Chistova",
                "Ivan Smirnov."
            ],
            "title": "Semantic role labeling with pretrained language models for known and unknown predicates",
            "venue": "Proceedings of the RANLP 2019, pages 619\u2013628.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Luheng He",
                "Luke Zettlemoyer."
            ],
            "title": "Higher-order coreference resolution with coarse-tofine inference",
            "venue": "Proceedings of the 16th NAACL, volume 2, pages 687\u2013692.",
            "year": 2018
        },
        {
            "authors": [
                "Changmao Li",
                "Jinho D Choi."
            ],
            "title": "Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering",
            "venue": "Proceedings of the 58th ACL, pages 5709\u20135714.",
            "year": 2020
        },
        {
            "authors": [
                "Jiaqi Li",
                "Ming Liu",
                "Min-Yen Kan",
                "Zihao Zheng",
                "Zekun Wang",
                "Wenqiang Lei",
                "Ting Liu",
                "Bing Qin."
            ],
            "title": "Molweni: A challenge multiparty dialogues-based machine reading comprehension dataset with discourse structure",
            "venue": "Proceedings of the 28th COL-",
            "year": 2020
        },
        {
            "authors": [
                "Jiaqi Li",
                "Ming Liu",
                "Zihao Zheng",
                "Heng Zhang",
                "Bing Qin",
                "Min-Yen Kan",
                "Ting Liu."
            ],
            "title": "DADgraph: A discourse-aware dialogue graph neural network for multiparty dialogue machine reading comprehension",
            "venue": "Proceedings of the IJCNN 2021, pages 1\u20138.",
            "year": 2021
        },
        {
            "authors": [
                "Yanling Li",
                "Bowei Zou",
                "Yifan Fan",
                "Mengxing Dong",
                "Yu Hong."
            ],
            "title": "Coreference-aware double-channel attention network for multi-party dialogue reading comprehension",
            "venue": "Proceedings of the IJCNN 2023, pages 1\u20138.",
            "year": 2023
        },
        {
            "authors": [
                "Yiyang Li",
                "Hai Zhao."
            ],
            "title": "Self-and pseudo-selfsupervised prediction of speaker and key-utterance for multi-party dialogue reading comprehension",
            "venue": "Findings of the EMNLP 2021, pages 2053\u20132063.",
            "year": 2021
        },
        {
            "authors": [
                "Yiyang Li",
                "Hai Zhao",
                "Zhuosheng Zhang."
            ],
            "title": "Back to the future: Bidirectional information decoupling network for multi-turn dialogue modeling",
            "venue": "Proceedings of the EMNLP 2022, pages 2761\u20132774.",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Nancy Chen."
            ],
            "title": "Improving multi-party dialogue discourse parsing via domain integration",
            "venue": "Proceedings of the 2nd CODI, pages 122\u2013127.",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Proceedings of the ICLR 2018, pages 1\u201319.",
            "year": 2018
        },
        {
            "authors": [
                "Ryan Lowe",
                "Nissan Pow",
                "Iulian Vlad Serban",
                "Joelle Pineau."
            ],
            "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
            "venue": "Proceedings of the 16th SIGDIAL, pages 285\u2013294.",
            "year": 2015
        },
        {
            "authors": [
                "Xinbei Ma",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Enhanced speaker-aware multi-party multiturn dialogue comprehension",
            "venue": "arXiv preprint arXiv:2109.04066.",
            "year": 2021
        },
        {
            "authors": [
                "Xinbei Ma",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Structural characterization for dialogue disentanglement",
            "venue": "Proceedings of the 60th ACL, volume 1, pages 285\u2013297.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew L Maas",
                "Awni Y Hannun",
                "Andrew Y Ng"
            ],
            "title": "Rectifier nonlinearities improve neural network acoustic models",
            "venue": "In Proc. icml,",
            "year": 2013
        },
        {
            "authors": [
                "Cicero dos Santos",
                "Ming Tan",
                "Bing Xiang",
                "Bowen Zhou."
            ],
            "title": "Attentive pooling networks",
            "venue": "arXiv preprint arXiv:1602.03609.",
            "year": 2016
        },
        {
            "authors": [
                "Peng Shi",
                "Jimmy Lin."
            ],
            "title": "Simple bert models for relation extraction and semantic role labeling",
            "venue": "arXiv preprint arXiv:1904.05255.",
            "year": 2019
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "Proceedings of the ICLR 2018, pages 1\u201312.",
            "year": 2018
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Meire Fortunato",
                "Navdeep Jaitly."
            ],
            "title": "Pointer networks",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Zhengzhe Yang",
                "Jinho D Choi."
            ],
            "title": "FriendsQA: Open-domain question answering on tv show transcripts",
            "venue": "Proceedings of the 20th SIGDIAL, pages 188\u2013197.",
            "year": 2019
        },
        {
            "authors": [
                "Nan Yu",
                "Guohong Fu",
                "Min Zhang."
            ],
            "title": "Speakeraware discourse parsing on multi-party dialogues",
            "venue": "Proceedings of the 29th COLING, pages 5372\u20135382.",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zheng",
                "Parisa Kordjamshidi."
            ],
            "title": "Srlgrn: Semantic role labeling graph reasoning network",
            "venue": "Proceedings of the EMNLP 2020, pages 8881\u20138891.",
            "year": 2020
        },
        {
            "authors": [
                "Jie Zhu",
                "Junhui Li",
                "Muhua Zhu",
                "Longhua Qian",
                "Min Zhang",
                "Guodong Zhou."
            ],
            "title": "Modeling graph structure in transformer for better amr-to-text generation",
            "venue": "Proceedings of the EMNLP-IJCNLP 2019, pages 5459\u20135468.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Graph reasoning contributes to the integration of discretely-distributed attentive information (clues) for Multi-party Dialogue Reading Comprehension (MDRC). This is attributed primarily to multi-hop reasoning over global conversational structures. However, existing approaches barely apply questions for anti-noise graph reasoning. More seriously, the local semantic structures in utterances are neglected, although they are beneficial for bridging across semantically-related clues. In this paper, we propose a question-aware global-to-local graph reasoning approach. It expands the canonical Interlocutor-Utterance graph by introducing a question node, enabling comprehensive global graph reasoning. More importantly, it constructs a semantic-role graph for each utterance, and accordingly performs local graph reasoning conditioned on the semantic relations. We design a two-stage encoder network to implement the progressive reasoning from the global graph to local. The experiments on the benchmark datasets Molweni and FriendsQA show that our approach yields significant improvements, compared to BERT and ELECTRA baselines. It achieves 73.6% and 77.2% F1-scores on Molweni and FriendsQA, respectively, outperforming state-of-the-art methods that employ different pretrained language models as backbones."
        },
        {
            "heading": "1 Introduction",
            "text": "MDRC is a special Machine Reading Comprehension (MRC) task. It involves answering questions conditioned on the utterances of multiple interlocutors (Yang and Choi, 2019; Li et al., 2020). MDRC presents unique challenges due to two aspects:\n\u2022 MDRC relies heavily on multi-hop reasoning, where the necessary clues for reasoning are discretely distributed across utterances.\n\u2217Corresponding author.\n\u2022 Multi-hop reasoning suffers from discontinuous utterances and disordered conversations (see Figure 1-a,b).\nRecently, a variety of graph-based multi-hop reasoning (abbr., graph reasoning) approaches have been proposed to tackle MDRC (Li et al., 2021; Ma et al., 2021, 2022). Graph reasoning is generally effective for bridging across the clues hidden in the discontinuous utterances, with less interference of redundant and distracting information occurring in the disordered conversations. The effectiveness is attributed primarily to the perception of global interactive relations of interlocutor-utterance graphs.\nHowever, existing approaches encounter two bottlenecks. First, the question-disregarded graph construction methods (Li et al., 2021; Ma et al., 2021) fail to model the bi-directional interactions between\nquestion and utterances. As a result, it is prone to involving question-unrelated information during reasoning. Second, the inner token-level semantic relations in every utterance are omitted, making it difficult to perceive the exact and unabridged clues occurring in the local contexts.\nTo address the issues, we propose a Global-toLocal Graph Reasoning approach (GLGR) with Pre-trained Language Models (PLMs) (Kenton and Toutanova, 2019; Clark et al., 2020) as backbones. It encodes two heterogeneous graphs, including Question-aware Interlocutor-Utterance Graph (QIUG) and Local Semantic Role Graph (LSRG). QIUG connects the question with all utterances in the canonical Interlocutor-Utterance graph (Figure 1-c). It depicts the global interactive relations. By contrast, LSRG interconnects fine-grained nodes (tokens, phrases and entities) in an utterance in terms of their semantic roles, where semantic role labeling (Shi and Lin, 2019) is used. It signals the local semantic relations. To enable connectivity between LSRGs of different utterances, we employ coreference resolution (Lee et al., 2018) and synonym identification to identify shareable nodes (Figure 1-d).\nMethodologically, we develop a two-stage encoder network for progressive graph reasoning. It is conducted by successively encoding QIUG and LSRG, where attention modeling is used. The attentive information squeezed from QIUG and LSRG is respectively used to emphasize the global and local clues for answer prediction. Accordingly, the representation of the input is updated step-by-step during the progressive reasoning process. Residual network is used for information integration.\nWe carry out GLGR within an extractive MRC framework, where a pointer network (Vinyals et al., 2015) is used to extract answers from utterances. The experiments on Molweni (Li et al., 2020) and FriendsQA (Yang and Choi, 2019) demonstrate three contributions of this study, including:\n\u2022 The separate use of QIUG and LSRG for graph reasoning yields substantial improvements, compared to PLM-based baselines.\n\u2022 Global-to-local progressive reasoning on both graphs (i.e., GLGR) yields further improvements, allowing MDRC performance to reach 73.6% and 77.2% F1-scores, as well as 59.2% and 59.8% EM-scores.\n\u2022 GLGR is stable. It obtains general improve-\nments when different PLMs are adopted as backbones (BERT and ELECTRA).\nThe rest of the paper is organized as follows. Section 2 presents the details of GLGR. We discuss the experimental results in Section 3, and overview the related work in Section 4. We conclude the paper in Section 5."
        },
        {
            "heading": "2 Approach",
            "text": "The overall architecture of GLGR-based MDRC model is shown in Figure 2. First of all, we utilize PLM to initialize the representation of the question, interlocutors and utterances. On this basis, the firststage graph reasoning is conducted over QIUG, where Graph Attention network (GAT) (Velic\u030ckovic\u0301 et al., 2018) is used for encoding. Subsequently, we carry out the second-stage graph reasoning over LSRG, where graph transformer layers (Zhu et al., 2019) are used for encoding. Finally, we concate-\nnate the initial hidden states and their updated versions obtained by GLGR, and feed them into the pointer network for answer prediction.\nIn the following subsections, we detail computational details after presenting the task definition."
        },
        {
            "heading": "2.1 Task Definition",
            "text": "Formally, the task is defined as follows. Given a multi-party dialogue D = {U1, U2, ..., Un} with n utterances and a question Q, MDRC aims to extract the answer A of Q from D. When the question is unanswerable, A is assigned a tag \u201cImpossible\u201d. Note that each utterance Ui comprises the name of an interlocutor Si who issues the utterance, as well as the concrete content of the utterance."
        },
        {
            "heading": "2.2 Preliminary Representation",
            "text": "We follow Li and Zhao (2021)\u2019s study to encode the question Q and dialogue D using PLM, so as to obtain token-level hidden states H of all tokens in Q and D. Specifically, we concatenate the question Q and dialogue D to form the input sequence X , and feed X into PLM to compute H:\nX = {[CLS], Q,[SEP ], U1, [SEP ], ..., Un, [SEP ]} H = PLM(X, \u03b8) (1)\nwhere, [CLS] and [SEP] denote special tokens. The hidden states H \u2208 Rl\u00d7d serve as the universal representation of X , where l is the maximum length of X , and d is the hidden size. The symbol \u03b8 denotes all the trainable parameters of PLM. In our experiments, we consider three different PLMs as backbones in total, including BERTbase-uncased (BERTbase), BERT-large-uncased (BERTlarge) (Kenton and Toutanova, 2019) and ELECTRA (Clark et al., 2020).\nTo facilitate understanding, we clearly define different levels of hidden states as follows:\n\u2022 H refers to the hidden states of all tokens in X , i.e., the universal representation of X . h is the hidden state of a token x (x \u2208 X).\n\u2022 O is the hidden state of a specific node, known as node representation. Specifically, Oq, Os\nand Ou denote the representations of question node, interlocutor node and utterance node."
        },
        {
            "heading": "2.3 Global-to-Local Graph Reasoning",
            "text": "We carry out Global-to-Local Graph Reasoning (GLGR) to update the hidden states H of the input sequence X . GLGR is fulfilled by two-stage progressive encoding over two heterogeneous graphs, including QIUG and LSRG."
        },
        {
            "heading": "2.3.1 Global Graph Reasoning on QIUG",
            "text": "QIUG\u2013 QIUG is an expanded version of the canonical interlocutor-utterance graph (Li et al., 2021; Ma et al., 2021) due to the involvement of questionoriented relations, as shown in Figure 1-(c).\nSpecifically, QIUG comprises one question node, Nu utterance nodes and Ns interlocutor nodes. We connect the nodes using the following scheme:\n\u2022 Each question node is linked to all utterance nodes. Bidirectional connection is used, in the directions of \u201cquerying\u201d and \u201cqueried-by\u201d.\n\u2022 Each interlocutor node is connected to all the utterance nodes she/he issued. Bidirectional connection is used, in the directions of \u201cissuing\u201d and \u201cissued-by\u201d.\n\u2022 Utterance nodes are connected to each other in terms of Conversational Discourse Structures (CDS) (Liu and Chen, 2021; Yu et al., 2022). CDS is publicly available in Molweni (Li et al., 2020), though undisclosed in FriendsQA (Yang and Choi, 2019). Therefore, we apply Liu and Chen (2021)\u2019s open-source CDS parser to tread with FriendsQA. Bidirectional connection is used, i.e., in the directions of \u201creplying\u201d and \u201creplied-by\u201d.\nConsequently, QIUG contains six types of interactive relations T={querying, queried-by, issuing, issued-by, replying, replied-by}. Each directed edge in QIUG solely signals one type of relation.\nNode Representation\u2013 For an interlocutor node Si, we consider the tokens of her/his name and look up their hidden states in the universal representation H . We aggregate the hidden states by mean pooling (Gholamalinezhad and Khosravi, 2020). The resultant embedding Osi \u2208 R1\u00d7d is used as the node representation of Si.\nFor an utterance node Ui, we aggregate the hidden states of all tokens in Ui. Attention pooling (Santos et al., 2016) is used for aggregation. The resultant embedding Oui \u2208 R1\u00d7d is used as the node representation of Ui. We obtain the representation Oq of the question node Q in a similar way.\nMulti-hop Reasoning\u2013 Multi-hop reasoning is used to discover and package co-attentive information across nodes and along edges. Methodologically, it updates the hidden states of all tokens in a node using the attentive information O\u030a in the neighboring nodes. Formally, the hidden state of each\ntoken is updated by: h\u030a=[h;O\u030a], i.e., concatenating h \u2208 R1\u00d7d and O\u030a \u2208 R1\u00d7d.\nWe utilize L1-layer GAT (Velic\u030ckovic\u0301 et al., 2018) networks to compute O\u030a, where attention-weighted information fusion is used:\nO\u030a (l+1) i = \u2211 Oj\u2208Ei \u03b1 (l) i,jW\u030a (l)O (l) j (2)\nwhere, Ei comprises a set of neighboring nodes of the i-th node Oi. W\u030a is a trainable scalar parameter, and the superscript (l) signals the computation at the l-the layer of GAT. Besides, \u03b1i,j is a nodelevel attention score of Oi (Oi /\u2208 Ei) relative to Oj (Oj \u2208 Ei). The resultant O\u030aL1i is used to update the hidden states of all tokens in the i-th node.\nDivide-and-conquer Attention Modeling\u2013 Different interactive relations have distinctive impacts on attention modeling. For example, the \u201cqueriedby\u201d relation (i.e., an edge directed from utterance to question) most likely portends the payment of more attention to the possible answer in the utterance. By contrast, the \u201creplying\u201d and \u201creplied-by\u201d relations (i.e., edges between utterance nodes) induce the payment of more attention to the complementary clues in the utterances. In order to distinguish between the impacts, we separately compute nodelevel attention scores \u03b1 for different types of edges in QIUG. Given two nodes Oi and Oj with a t-type edge, the attention score \u03b1i,j is computed as:\n\u03b1 (l) i,j =\nexp (f([O (l) i ;O (l) j ], W\u0307 (l)))\u2211 Ok\u2208Ei,t exp (f([O (l) i ;O (l) k ], W\u0307 (l))) (3)\nwhere [; ] is the concatenation operation, while f (\u00b7) denotes the LeakyRelu (Maas et al., 2013) activation function. Ei,t is the set of all neighboring nodes that are connected to Oi with a t-type edge. W\u0307\n(l) t \u2208 R2d\u00d71 is a trainable parameter. Prediction and Training\u2013 Using multi-hop reasoning, we reproduce the universal representation: H\u030a=[H;O\u030aL1all ]. We feed H\u030a into a two-layer pointer network to predict the answer, determining the start and end positions of the answer in X . Note that the hidden state of [CLS] in H\u030a is used to predict the \u201cImpossible\u201d tag, i.e., a tag signaling unanswerable question. During training, we use the cross-entropy loss function to optimize the model."
        },
        {
            "heading": "2.3.2 Local Graph Reasoning on LSRG",
            "text": "Global graph reasoning is grounded on the global relations among question, interlocutor and utterance nodes, as well as their indecomposable node representations. It barely uses the inner token-level semantic relations in every utterance for multi-hop\nreasoning. However, such local semantic correlations actually contribute to the reasoning process, such as the predicate-time and predicate-negation relations, as well as coreferential relations. Therefore, we construct the semantic-role graph LSRG, and use it to strengthen local graph reasoning.\nLSRG\u2013 LSRG is an undirected graph which comprises semantic-role subgraphs of all utterances in D. To obtain the subgraph of an utterance, we leverage Allennlp-SRL parser (Shi and Lin, 2019) to extract the predicate-argument structures in the utterance, and regard predicates and arguments as the fine-grained nodes. Each predicate node is connected to the associated argument nodes with undirected role-specific edges (e.g., \u201cARG1-V\u201d). Both the directly-associated arguments and indirectlyassociated are considered for constructing the subgraph, as shown in Figure 1-(d).\nGiven the semantic-role subgraphs of all utterances, we form LSRG using the following scheme:\n\u2022 We combine the subgraphs containing similar fine-grained nodes. It is fulfilled by connecting the similar nodes. A pair of nodes is determined to be similar if their inner tokens have an overlap rate more than 0.5.\n\u2022 Interlocutor name is regarded as a special fine-grained node. We connect it to the finegrained nodes in the utterances she/he issued.\n\u2022 We combine subgraphs containing coreferential nodes. Lee et al. (2018)\u2019s coreference resolution toolkit1 is used.\nFine-grained Node Representation\u2013 The finegrained nodes generally contain a varied number of tokens (e.g., \u201ccan not\u201d and \u201cthe data files\u201d). To obtain identically-sized representations of them, we aggregate the hidden states of all tokens in each fine-grained node. Attention pooling (Santos et al., 2016) is used for aggregation.\nIn our experiments, there are two kinds of tokenlevel hidden states considered for fine-grained node representation and reasoning on LSRG, including the initial case h obtained by PLM, as well as the refined case h\u030a (\u030ah \u2208 H\u030a) by global graph reasoning. When h is used, we perform local graph reasoning independently, without the collaboration of global graph reasoning. It is carried out for MDRC in an ablation study. When h\u030a is used, we perform global-to-local graph reasoning. It is conducted in\n1https://demo.allennlp.org/coreference-resolution\nthe comparison test. We concentrate on h\u030a in the following discussion.\nMulti-hop Reasoning on LSRG\u2013 It updates hidden states of all tokens in each fine-grained node Oi, where the attentive information O\u030c of its neighboring nodes in LSRG is used for updating. Formally, the hidden state of each token is updated by: h\u030c=[\u030ah;O\u030c]. We use a L2-layer graph transformer (Zhu et al., 2019) to compute O\u030c \u2208 R1\u00d7d as follows:\nO\u030c (l+1) i = \u2211 Oj\u2208Ei \u03b2 (l) ij ( O (l) j W\u030c (l) o + r (l) ij W\u030c (l) r ) (4)\nwhere, Ei is a set of neighboring fine-grained nodes of the i-th node Oi in LSRG. W\u030co \u2208 Rd\u00d7d and W\u030cr \u2208 Rd\u00d7d are trainable parameters. In addition, ri,j \u2208 R1\u00d7d is the learnable embedding of the rolespecific edge between Oi and its j-th neighboring node Oj . Before training, the edges holding the same semantic-role relation are uniformly assigned a randomly-initialized embedding. Besides, \u03b2i,j is a scalar, denoting the attention score between Oi and Oj . It is computed as follows:\n\u03b2 (l) ij =\nexp ( e (l) ij ) \u2211M\ni=1 exp ( e (l) ik ) (5) e (l) ij = O (l) i W\u0307 (l) e ( O (l) j W\u0307 (l) o + r (l) ij W\u0307 (l) r )T / \u221a d (6)\nwhere, W\u0307 (l)e , W\u0307 (l) o and W\u0307 (l) r are trainable parameters of dimensions Rd\u00d7d. M denotes the number of neighboring fine-grained nodes in Ei.\nQuestion-aware Reasoning\u2013 Obviously, the LSRG-based attentive information O\u030ci is independent of the question. To fulfill question-aware reasoning, we impose the question-oriented attention upon O\u030ci. Formally, it is updated by: O\u030ci \u21d0 \u03b3i \u00b7 O\u030ci, where the attention score \u03b3i is computed as follows:\n\u03b3i = Sigmoid (O qWqOi/ \u221a d) (7)\nwhere, Oq is the representation of the question node, and Wq \u2208 Rd\u00d7d is a trainable parameter.\nPrediction and Training\u2013 To fulfill global-tolocal reasoning (GLGR), we reproduce the universal representation: H\u030c=[H\u030a;O\u030cL2all ]. It is actually equivalent to the operation of concatenating the initial representation H , QIUG-based co-attentive information O\u030aL1all and LSRG-based co-attentive information O\u030cL2all , i.e., H\u030c=[H;O\u030a L1 all ;O\u030c L2 all ].\nIn the scenario of independent local graph reasoning, the QIUG-based co-attentive information is omitted. Accordingly, the universal representation is calculated as: H\u030c=[H;O\u030cL2all ], where O\u030c L2 all is computed using h instead of h\u030a.\nWe feed H\u030c into the two-layer pointer network for predicting answers. Cross-entropy loss is used to optimize the model during training."
        },
        {
            "heading": "3 Experimentation",
            "text": ""
        },
        {
            "heading": "3.1 Datasets and Evaluation",
            "text": "We experiment on two benchmark datasets, including Molweni (Li et al., 2020) and FriendsQA (Yang and Choi, 2019). Molweni is an MDRC dataset manufactured using Ubuntu Chat Corpus (Lowe et al., 2015). The dialogues in Molweni are accompanied with ground-truth CDS, as well as either answerable or unanswerable questions. FriendsQA is another MDRC dataset whose dialogues are excerpted from the TV show Friends. It is characterized by colloquial conversations. CDS is undisclosed in FriendsQA. We use Liu and Chen (2021)\u2019s CDS parser to pretreat the dataset.\nWe follow the common practice (Li et al., 2020; Yang and Choi, 2019) to split Molweni and FriendsQA into the training, validation and test sets. The data statistics about the sets are shown in Table 1. We use F1-score and EM-score (Li and Zhao, 2021; Ma et al., 2021) as the evaluation metrics."
        },
        {
            "heading": "3.2 Backbone and Hyperparameter Settings",
            "text": "To verify the stability of GLGR, we construct three GLGR-based models using different PLMs as backbones, including BERT-base-uncased (BERTbase), BERT-large-uncased (BERTlarge) (Kenton and Toutanova, 2019) and ELECTRA (Clark et al., 2020). The hyperparameters are set as follows.\nAll the models are implemented using Transformers Library (Li and Choi, 2020). AdamW optimizer (Loshchilov and Hutter, 2018) is used for training. Towards the experiments on Molweni, we set the batch size to 16 for BERT and 12 for ELECTRA. The learning rates are set to 1.8e-5, 5.2e5 and 1e-5 for BERTbase, BERTlarge and ELECTRA, respectively. For FriendsQA, we set the batch sizes to 16, 8 and 12 for BERTbase, BERTlarge and ELECTRA, respectively. The learning rates are set to 1.8e-5 for BERTbase and 1e-5 for both BERTlarge and ELECTRA backbone. In addition,\nThe numbers of network layers of GAT and graph transformer are set in the same way: L1=L2=2."
        },
        {
            "heading": "3.3 Compared MDRC Models",
            "text": "Following Ma et al. (2021)\u2019s study, we consider the standard span-based MRC model (Kenton and Toutanova, 2019) as the baseline. We compare with a variety of state-of-the-art MDRC models:\n\u2022 DADgraph (Li et al., 2021) constructs a CDSbased dialogue graph. It enables the graph reasoning over conversational dependency features and interlocutor nodes. Graph Convolutional Network (GCN) is used for reasoning.\n\u2022 ULM-UOP (Li and Choi, 2020) fine-tunes BERT on a larger number of FriendsQA transcripts (known as Character Mining dataset (Yang and Choi, 2019)) before task-specific training for MDRC. Two self-supervised tasks are used for fine-tuning, including utteranceoriented masked language modeling and utterance order prediction. In addition, BERT is trained to predict both answers and sources (i.e., IDs of utterances containing answers).\n\u2022 SKIDB (Li and Zhao, 2021) uses a multi-task learning strategy to enhance MDRC model. Self-supervised interlocutor prediction and key-utterance prediction tasks are used within the multi-task framework.\n\u2022 ESA (Ma et al., 2021) uses GCN to encode the interlocutor graph and CDS-based utterance graph. In addition, it is equipped with\na speaker masking module, which is able to highlight co-attentive information within utterances of the same interlocutor, as well as that among different interlocutors.\n\u2022 BiDeN (Li et al., 2022) incorporates latent information of utterances using different temporal structures (e.g., future-to-current, currentto-current and past-to-current interactions)."
        },
        {
            "heading": "3.4 Main Results",
            "text": "Table 2 shows the test results of our GLGR models and the compared models, where different backbones (BERTbase, BERTlarge and ELECTRA) are considered for the general comparison purpose.\nIt can be observed that our GLGR model yields significant improvements, compared to the PLM baselines. The most significant performance gains are 4.3% F1-score and 4.3% EM-score, which are obtained on FriendsQA compared to the lite BERTbase (110M parameters). When compared to the larger BERTlarge (340M parameters) and ELECTRA (335M parameters), GLGR is able to achieve the improvements of no less than 1.7% F1-score and 2.4% EM-score. In addition, GLGR outperforms most of the state-of-the-art MDRC models. The only exception is that GLGR obtains a comparable performance relative to ESA when BERTbase is used as the backbone. By contrast, GLGR substantially outperforms ESA when BERTlarge and ELECTRA are used.\nThe test results reveal distinctive advantages of the state-of-the-art MDRC models. DADgraph is a vest-pocket model due to the involvement of a sole interlocutor-aware CDS-based graph. It offers the basic performance of graph reasoning for MDRC. ESA is grounded on multiple graphs, and it separately analyzes co-attentive information for subdivided groups of utterances. Multi-graph reasoning and coarse-to-fine attention perception allow ESA to be a competitive MDRC model. By contrast, ULM-UOP doesn\u2019t rely heavily on conversational structures. Instead, it leverages larger dataset and diverse tasks for fine-tuning, and thus enhances the ability of BERT in understanding domain-specific languages at the level of semantics. It can be observed that ULM-UOP achieves similar performance compared to ESA. SKIDB successfully leverages multi-task learning, and it applies interesting and effective self-supervised learning tasks. Similarly, it enhances PLMs in encoding domain-specific languages, which is not limited to\nBERT. It can be found that SKIDB obtains comparable performance on Molweni compared to ESA.\nOur GLGR model combines the above advantages by external conversational structure analysis and internal semantic role analysis. On this basis, GLGR integrates global co-attentive information with local for graph reasoning. It can be observed that GLGR shows superior performance, although it is trained without using additional data."
        },
        {
            "heading": "3.5 Ablation Study",
            "text": "We conduct the ablation study from two aspects. First of all, we progressively ablate global and local graph reasoning, where QIUG and LSRG are omitted accordingly. Second, we respectively ablate different classes of edges from the two graphs, i.e., disabling the structural factors during multi-hop reasoning. We refer the former to \u201cGraph ablation\u201d while the latter the \u201cRelation ablation\u201d.\nGraph ablation\u2013 The negative effects of graph ablation is shown in Table 3. It can be easily found that similar performance degradation occurs when QIUG and LSRG are independently pruned. This implies that local reasoning on LSRG is effectively equivalent to global reasoning on QIUG, to some extent. When graph reasoning is thoroughly disabled (i.e., pruning both QIUG and LSRG), the performance degrades severely.\nRelation ablation\u2013 The negative effects of relation ablation is shown in Table 4. For QIUG, we condense the graph structure by respectively disabling interlocutor, question and utterance nodes. It\ncan be found that the performance degradation rates in the three ablation scenarios are similar. This demonstrates that all the conversational structural factors are crucial for multi-hop reasoning. For LSRG, we implement relation ablation by unbinding the co-referential fine-grained nodes, omitting semantic-role relations, and removing questionaware reasoning, respectively. The way to omit semantic-role relations is accomplished by full connection. It can be observed that ablating semanticrole relations causes relatively larger performance degradation rates."
        },
        {
            "heading": "3.6 The Impact of Utterance Number",
            "text": "We follow the common practice (Li and Zhao, 2021; Li et al., 2022) to verify the impacts of utterance numbers. The FriendsQA test set is used in the experiments. It is divided into four subsets, including the subsets of dialogues containing 1) no more than 9 utterances, 2) 10\u223c19 utterances, 3) 20\u223c29 utterances and 4) no less than 30 utterances. The GLGR is re-evaluated over the subsets. We illustrate the performance in Figure 3.\nIt can be found that the performance decreases for the dialogues containing a larger number of utterances, no matter whether the baseline or GLGR is used. In other words, both the models are distracted by plenty of noises in these cases. Nevertheless, we observe that GLGR is able to stem the tide of performance degradation to some extent. Therefore, we suggest that GLGR contributes to the anti-noise multi-hop reasoning, although it fails to solve the problem completely."
        },
        {
            "heading": "3.7 Reliability of Two-stage Reasoning",
            "text": "There are two alternative versions of GLGR: one changes the reason sequence of QIUG and LSRG (local-to-global graph reasoning). The other performs the single-phase reasoning over a holistic graph that interconnects QIUG and LSRG. In this\nsubsection, we intend to compare them to the global-to-local two-stage GLGR.\nFor the single-phase reasoning version, we combine QIUG with LSRG by two steps, including 1) connecting noun phrases in the question node to the similar fine-grained nodes in utterances, and 2) connecting utterance nodes to the entities occurring in them. On this basis, we encode the resultant holistic graph using GAT, which serves as the single-phase GLGR. It is equipped with ELECTRA and the pointer network.\nThe comparison results (single-phase GLGR versus two two-stage GLGRs) are shown in Table 5, where ELECTRA is used as the backbone. It is evident that the single-stage GLGR obtains inferior performance. It is most probably because that the perception of co-attentive information among fine-grained nodes in LSRG suffers from the interference of irrelevant coarse-grained nodes in QIUG. This drawback raises a problem of combining heterogeneous graphs for multi-hop reasoning. Besides, we observe that the global-to-local reasoning method exhibits better performance compared to local-to-global graph reasoning. We attribute it to the initial local graph reasoning in the local-toglobal reasoning, which ineffectively integrates the distant and important context information while focusing on local semantic information. This leads to suboptimal multi-hop reasoning and highlights the importance of the graph reasoning order in handling complex information dependencies."
        },
        {
            "heading": "3.8 Case Study",
            "text": "GLGR is characterized as the exploration of both global and local clues for reasoning. It is implemented by highlighting co-attentive information in coarse-grained and fine-grained nodes.\nFigure 4 shows a case study for GLGR-based MDRC, where the heat maps exhibit the attention distribution on both utterance nodes and tokenlevel nodes. There are three noteworthy phenomena occurring in the heat maps. First, GLGR assigns higher attention scores to two utterance nodes, which contain the answer and closely-related clues, respectively. Second, both the answer and clues are assigned higher attention scores, compared to other token-level nodes. Finally, the answer and clues emerge from different utterance nodes.\nThis is not an isolated case, and the phenomena stand for the crucial impacts of GLGR on MDRC."
        },
        {
            "heading": "4 Related Work",
            "text": ""
        },
        {
            "heading": "4.1 Multi-party Dialogue Reading Comprehension",
            "text": "A variety of graph-based approaches have been studied for MDRC. They successfully incorporate conversational structural features into the dialogue modeling process.\nMa et al. (2021) construct a provenance-aware graph to enhance the co-attention encoding of discontinuous utterances of the same interlocutor. Li et al. (2021) and Ma et al. (2022) apply CDS to bring the mutual dependency features of utterances into the graph reasoning process, where GCN (Kipf and Welling, 2017) is used for encoding. Recently, Li et al. (2022) propose a back-and-forth comprehension strategy. It decouples the past and future dialogues, and models interactive relations in terms of conversational temporality. Li et al. (2023) add the coreference-aware attention modeling in PLM to strengthen the multi-hop reasoning ability.\nAnother research branch focuses on the study of language understanding for dialogues, where self-supervised learning is used for the general-tospecific modeling and transfer of the pretrained models. Li and Choi (2020) transfer BERT to the task-specific data, where two self-supervised tasks are used for fine-tuning, including Utterance-level Masked Language Modeling (ULM) and UtteranceOrder Prediction (UOP). During the transfer, the larger-sized dataset of FriendsQA transcripts is used. Similarly, Li and Zhao (2021) transfer PLMs to dialogues using a multi-task learning framework,\nincluding the tasks of interlocutor prediction and key utterance prediction."
        },
        {
            "heading": "4.2 Semantic Role Labeling",
            "text": "In this study, we use Semantic Role Labeling (SRL) to build LSRG for local graph reasoning. To facilitate the understanding of SRL, we present the related work as follows.\nSRL is a shallow semantic parsing task that aims to recognize the predicate-argument structure of each sentence. Recently, advances in SRL have been largely driven by the development of neural networks, especially the Pre-trained Language Models (PLMs) such as BERT (Kenton and Toutanova, 2019). Shi and Lin (2019) propose a BERT-based model that incorporates syntactic information for SRL. Larionov et al. (2019) design the first pipeline model for SRL of Russian texts.\nIt has been proven that SRL is beneficial for MRC by providing rich semantic information for answer understanding and matching. Zheng and Kordjamshidi (2020) introduce an SRL-based graph reasoning network to the task of multi-hop question answering. They demonstrate that the finegrained semantics of an SRL graph contribute to the discovery of an interpretable reasoning path for answer prediction."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose a global-to-local graph reasoning approach towards MDRC. It explores attentive clues for reasoning in both coarse-grained graph QIUG and fine-grained graph LSRG. Experimental results show that the proposed approach outperforms the PLMs baselines and state-of-the-art models. The codes are available at https://github.com/ YanLingLi-AI/GLGR.\nThe main contribution of this study is to jointly use global conversational structures and local semantic structures during encoding. However, it can be only implemented by two-stage reasoning due to the bottleneck of in-coordinate interaction between heterogeneous graphs. To overcome the issue, we will use the pretrained Heterogeneous Graph Transformers (HGT) for encoding in the future. Besides, the graph-structure based pretraining tasks will be designed for task-specific transfer learning.\nLimitations\nWhile GLGR demonstrates several strengths, it also has limitations that should be considered. First,\nGLGR relies on annotated conversation structures, co-reference, and SRL information. This dependency necessitates a complex data preprocessing process and makes the model susceptible to the quality and accuracy of the annotations. Therefore, it is important to ensure the accuracy and robustness of the annotations used in the model training and evaluation process. Second, GLGR may encounter challenges in handling longer dialogue contexts. The performance may exhibit instability when confronted with extended and more intricate conversations. Addressing this limitation requires further investigation of the stability and consistency in a real application scenario."
        },
        {
            "heading": "Acknowledgements",
            "text": "The research is supported by National Key R&D Program of China (2020YFB1313601), National Science Foundation of China (62376182, 62076174) and Institute of Infocomm Research of A*STAR (CR-2021-001)."
        }
    ],
    "title": "GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension",
    "year": 2023
}