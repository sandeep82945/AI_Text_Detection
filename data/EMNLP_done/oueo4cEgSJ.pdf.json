{
    "abstractText": "Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP1, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaochen Wang"
        },
        {
            "affiliations": [],
            "name": "Junyu Luo"
        },
        {
            "affiliations": [],
            "name": "Jiaqi Wang"
        },
        {
            "affiliations": [],
            "name": "Ziyi Yin"
        },
        {
            "affiliations": [],
            "name": "Suhan Cui"
        },
        {
            "affiliations": [],
            "name": "Yuan Zhong"
        },
        {
            "affiliations": [],
            "name": "Yaqing Wang"
        },
        {
            "affiliations": [],
            "name": "Fenglong Ma"
        }
    ],
    "id": "SP:b822dcf78df955e39de7b24fb136892ec425faad",
    "references": [
        {
            "authors": [
                "Emily Alsentzer",
                "John R Murphy",
                "Willie Boag",
                "WeiHung Weng",
                "Di Jin",
                "Tristan Naumann",
                "Matthew McDermott."
            ],
            "title": "Publicly available clinical bert embeddings",
            "venue": "arXiv preprint arXiv:1904.03323.",
            "year": 2019
        },
        {
            "authors": [
                "Chacha Chen",
                "Junjie Liang",
                "Fenglong Ma",
                "Lucas Glass",
                "Jimeng Sun",
                "Cao Xiao."
            ],
            "title": "Unite: Uncertaintybased health risk prediction leveraging multi-sourced data",
            "venue": "Proceedings of the Web Conference 2021, pages 217\u2013226.",
            "year": 2021
        },
        {
            "authors": [
                "Changyou Chen",
                "Jianyi Zhang",
                "Yi Xu",
                "Liqun Chen",
                "Jiali Duan",
                "Yiran Chen",
                "Son Tran",
                "Belinda Zeng",
                "Trishul Chilimbi"
            ],
            "title": "Why do we need large batch sizes in contrastive learning? a gradient-bias perspective",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Edward Choi",
                "Mohammad Taha Bahadori",
                "Elizabeth Searles",
                "Catherine Coffey",
                "Michael Thompson",
                "James Bost",
                "Javier Tejedor-Sojo",
                "Jimeng Sun."
            ],
            "title": "Multi-layer representation learning for medical concepts",
            "venue": "proceedings of the 22nd ACM SIGKDD in-",
            "year": 2016
        },
        {
            "authors": [
                "Edward Choi",
                "Mohammad Taha Bahadori",
                "Jimeng Sun",
                "Joshua Kulas",
                "Andy Schuetz",
                "Walter Stewart."
            ],
            "title": "Retain: An interpretable predictive model for healthcare using reverse time attention mechanism",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Edward Choi",
                "Cao Xiao",
                "Jimeng Sun",
                "Walter F Stewart"
            ],
            "title": "Mime: Multilevel medical embedding of electronic health records for predictive healthcare",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Suhan Cui",
                "Junyu Luo",
                "Muchao Ye",
                "Jiaqi Wang",
                "Ting Wang",
                "Fenglong Ma."
            ],
            "title": "Medskim: Denoised health risk prediction via skimming medical claims data",
            "venue": "2022 IEEE International Conference on Data Mining (ICDM), pages 81\u201390. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Suhan Cui",
                "Jiaqi Wang",
                "Xinning Gui",
                "Ting Wang",
                "Fenglong Ma."
            ],
            "title": "Automed: Automated medical risk predictive modeling on electronic health records",
            "venue": "2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 948\u2013953.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Paul Hager",
                "Martin J Menten",
                "Daniel Rueckert."
            ],
            "title": "Best of both worlds: Multimodal contrastive learning with tabular and imaging data",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23924\u201323935.",
            "year": 2023
        },
        {
            "authors": [
                "\u00c1lvaro S Hervella",
                "Jos\u00e9 Rouco",
                "Jorge Novo",
                "Marcos Ortega."
            ],
            "title": "Self-supervised multimodal reconstruction pre-training for retinal computeraided diagnosis",
            "venue": "Expert Systems with Applications, 185:115598.",
            "year": 2021
        },
        {
            "authors": [
                "Alvaro S Hervella",
                "Jos\u00e9 Rouco",
                "Jorge Novo",
                "Marcos Ortega."
            ],
            "title": "Multimodal image encoding pretraining for diabetic retinopathy grading",
            "venue": "Computers in Biology and Medicine, 143:105302.",
            "year": 2022
        },
        {
            "authors": [
                "\u00c1lvaro S Hervella",
                "Jos\u00e9 Rouco",
                "Jorge Novo",
                "Marcos Ortega."
            ],
            "title": "Retinal microaneurysms detection using adversarial pre-training with unlabeled multimodal images",
            "venue": "Information Fusion, 79:146\u2013161.",
            "year": 2022
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Kexin Huang",
                "Jaan Altosaar",
                "Rajesh Ranganath."
            ],
            "title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
            "venue": "arXiv preprint arXiv:1904.05342.",
            "year": 2019
        },
        {
            "authors": [
                "Qiao Jin",
                "Bhuwan Dhingra",
                "William Cohen",
                "Xinghua Lu."
            ],
            "title": "Probing biomedical embeddings from language models",
            "venue": "Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 82\u201389.",
            "year": 2019
        },
        {
            "authors": [
                "Alistair Johnson",
                "Lucas Bulgarelli",
                "Tom Pollard",
                "Steven Horng",
                "Leo Anthony Celi",
                "Roger Mark."
            ],
            "title": "Mimic-iv",
            "venue": "PhysioNet. Available online at: https://physionet. org/content/mimiciv/1.0/(accessed August 23, 2021).",
            "year": 2020
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Lu Shen",
                "Li-wei H Lehman",
                "Mengling Feng",
                "Mohammad Ghassemi",
                "Benjamin Moody",
                "Peter Szolovits",
                "Leo Anthony Celi",
                "Roger G Mark."
            ],
            "title": "Mimic-iii, a freely accessible critical care database",
            "venue": "Scientific data, 3(1):1\u20139.",
            "year": 2016
        },
        {
            "authors": [
                "Yash Khare",
                "Viraj Bagal",
                "Minesh Mathew",
                "Adithi Devi",
                "U Deva Priyakumar",
                "CV Jawahar."
            ],
            "title": "Mmbert: Multimodal bert pretraining for improved medical vqa",
            "venue": "2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1033\u20131036. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Lehman",
                "Alistair Johnson"
            ],
            "title": "Clinical-t5: Large language models built using mimic clinical text",
            "year": 2023
        },
        {
            "authors": [
                "Yikuan Li",
                "Mohammad Mamouei",
                "Gholamreza SalimiKhorshidi",
                "Shishir Rao",
                "Abdelaali Hassaine",
                "Dexter Canoy",
                "Thomas Lukasiewicz",
                "Kazem Rahimi"
            ],
            "title": "2022a. Hi-behrt: Hierarchical transformer-based model for accurate prediction of clinical events",
            "year": 2022
        },
        {
            "authors": [
                "Yikuan Li",
                "Shishir Rao",
                "Jos\u00e9 Roberto Ayala Solares",
                "Abdelaali Hassaine",
                "Rema Ramakrishnan",
                "Dexter Canoy",
                "Yajie Zhu",
                "Kazem Rahimi",
                "Gholamreza Salimi-Khorshidi."
            ],
            "title": "Behrt: transformer for electronic health records",
            "venue": "Scientific reports, 10(1):1\u201312.",
            "year": 2020
        },
        {
            "authors": [
                "Yikuan Li",
                "Ramsey M Wehbe",
                "Faraz S Ahmad",
                "Hanyin Wang",
                "Yuan Luo."
            ],
            "title": "Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences",
            "venue": "arXiv preprint arXiv:2201.11838.",
            "year": 2022
        },
        {
            "authors": [
                "Sicen Liu",
                "Xiaolong Wang",
                "Yongshuai Hou",
                "Ge Li",
                "Hui Wang",
                "Hui Xu",
                "Yang Xiang",
                "Buzhou Tang."
            ],
            "title": "Multimodal data matters: Language model pre-training over structured and unstructured electronic health records",
            "venue": "IEEE Journal of Biomedical",
            "year": 2022
        },
        {
            "authors": [
                "Junyu Luo",
                "Muchao Ye",
                "Cao Xiao",
                "Fenglong Ma."
            ],
            "title": "Hitanet: Hierarchical time-aware attention networks for risk prediction on electronic health records",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data",
            "year": 2020
        },
        {
            "authors": [
                "Renqian Luo",
                "Liai Sun",
                "Yingce Xia",
                "Tao Qin",
                "Sheng Zhang",
                "Hoifung Poon",
                "Tie-Yan Liu."
            ],
            "title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "venue": "Briefings in Bioinformatics, 23(6).",
            "year": 2022
        },
        {
            "authors": [
                "Fenglong Ma",
                "Radha Chitta",
                "Jing Zhou",
                "Quanzeng You",
                "Tong Sun",
                "Jing Gao."
            ],
            "title": "Dipole: Diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks",
            "venue": "Proceedings of the 23rd ACM SIGKDD international conference on",
            "year": 2017
        },
        {
            "authors": [
                "Fenglong Ma",
                "Muchao Ye",
                "Junyu Luo",
                "Cao Xiao",
                "Jimeng Sun."
            ],
            "title": "Advances in mining heterogeneous healthcare data",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 4050\u20134051.",
            "year": 2021
        },
        {
            "authors": [
                "Liantao Ma",
                "Junyi Gao",
                "Yasha Wang",
                "Chaohe Zhang",
                "Jiangtao Wang",
                "Wenjie Ruan",
                "Wen Tang",
                "Xin Gao",
                "Xinyu Ma."
            ],
            "title": "Adacare: Explainable clinical health status representation learning via scaleadaptive feature extraction and recalibration",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Yiwen Meng",
                "William Speier",
                "Michael K Ong",
                "Corey W Arnold."
            ],
            "title": "Bidirectional representation learning from transformers using multimodal electronic health record data to predict depression",
            "venue": "IEEE journal of biomedical and health informatics,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Peng",
                "Shankai Yan",
                "Zhiyong Lu."
            ],
            "title": "Transfer learning in biomedical natural language processing: an evaluation of bert and elmo on ten benchmarking datasets",
            "venue": "arXiv preprint arXiv:1906.05474.",
            "year": 2019
        },
        {
            "authors": [
                "Yixuan Qiu",
                "Feng Lin",
                "Weitong Chen",
                "Miao Xu."
            ],
            "title": "Pre-training in medical data: A survey",
            "venue": "Machine Intelligence Research, 20(2):147\u2013179.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Laila Rasmy",
                "Yang Xiang",
                "Ziqian Xie",
                "Cui Tao",
                "Degui Zhi."
            ],
            "title": "Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
            "venue": "NPJ digital medicine, 4(1):86.",
            "year": 2021
        },
        {
            "authors": [
                "Junyuan Shang",
                "Tengfei Ma",
                "Cao Xiao",
                "Jimeng Sun."
            ],
            "title": "Pre-training of graph augmented transformers for medication recommendation",
            "venue": "arXiv preprint arXiv:1906.00346.",
            "year": 2019
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Elman Mansimov",
                "Ruslan Salakhudinov."
            ],
            "title": "Unsupervised learning of video representations using lstms",
            "venue": "International conference on machine learning, pages 843\u2013852. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Shengpu Tang",
                "Parmida Davarmanesh",
                "Yanmeng Song",
                "Danai Koutra",
                "Michael W Sjoding",
                "Jenna Wiens."
            ],
            "title": "Democratizing ehr analyses with fiddle: a flexible data-driven preprocessing pipeline for structured clinical data",
            "venue": "Journal of the American Medical",
            "year": 2020
        },
        {
            "authors": [
                "Sindhu Tipirneni",
                "Chandan K Reddy."
            ],
            "title": "Selfsupervised transformer for sparse and irregularly sampled multivariate clinical time-series",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), 16(6):1\u201317.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Jiaqi Wang",
                "Cheng Qian",
                "Suhan Cui",
                "Lucas Glass",
                "Fenglong Ma."
            ],
            "title": "Towards federated covid-19 vaccine side effect prediction",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 437\u2013452. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Neha Warikoo",
                "Yung-Chun Chang",
                "Wen-Lian Hsu."
            ],
            "title": "Lbert: Lexically aware transformer-based bidirectional encoder representation model for learning universal bio-entity relations",
            "venue": "Bioinformatics, 37(3):404\u2013412.",
            "year": 2021
        },
        {
            "authors": [
                "Kristoffer Wickstr\u00f8m",
                "Michael Kampffmeyer",
                "Karl \u00d8yvind Mikalsen",
                "Robert Jenssen."
            ],
            "title": "Mixing up contrastive learning: Self-supervised representation learning for time series",
            "venue": "Pattern Recognition Letters, 155:54\u201361.",
            "year": 2022
        },
        {
            "authors": [
                "Cao Xiao",
                "Edward Choi",
                "Jimeng Sun."
            ],
            "title": "Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review",
            "venue": "Journal of the American Medical Informatics Association, 25(10):1419\u20131428.",
            "year": 2018
        },
        {
            "authors": [
                "Yanbo Xu",
                "Siddharth Biswal",
                "Shriprasad R Deshpande",
                "Kevin O Maher",
                "Jimeng Sun."
            ],
            "title": "Raim: Recurrent attentive and intensive model of multimodal patient monitoring data",
            "venue": "Proceedings of the 24th ACM SIGKDD international conference on Knowl-",
            "year": 2018
        },
        {
            "authors": [
                "Bo Yang",
                "Lijun Wu"
            ],
            "title": "How to leverage the multimodal EHR data for better medical prediction",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Yuan",
                "Zheng Yuan",
                "Ruyi Gan",
                "Jiaxing Zhang",
                "Yutao Xie",
                "Sheng Yu."
            ],
            "title": "Biobart: pretraining and evaluation of a biomedical generative language model",
            "venue": "arXiv preprint arXiv:2204.03905.",
            "year": 2022
        },
        {
            "authors": [
                "E Patient-level"
            ],
            "title": "Experiments Baselines regarding the patient-level task are listed below",
            "venue": "HiTANet(Luo",
            "year": 1997
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pretraining is a widely adopted technique in natural language processing (NLP). It entails training a model on a large dataset using unsupervised learning before fine-tuning it on a specific downstream task using a smaller labeled dataset. Pretrained models like BERT (Devlin et al., 2018) and GPT (Radford et al., 2018) have demonstrated remarkable success across a range of NLP tasks, contributing to significant advancements in various NLP benchmarks.\nIn the medical domain, with the increasing availability of electronic health records (EHR), researchers have attempted to pre-train domainspecific models to improve the performance of various predictive tasks further (Qiu et al., 2023). For instance, ClinicalBERT (Huang et al., 2019) and ClinicalT5 (Lehman and Johnson, 2023) are pretrained on clinical notes, and Med2Vec (Choi et al., 2016a) and MIME (Choi et al., 2018) on medical\n1Source codes are available at https://github.com/ XiaochenWang-PSU/MedHMP.\ncodes. These models pretrained on a single type of data are too specific, significantly limiting their transferability. Although some pretraining models (Li et al., 2022a, 2020; Meng et al., 2021) are proposed to use multimodal EHR data2, they ignore the heterogeneous and hierarchical characteristics of such data.\nThe EHR data, as depicted in Figure 1, exhibit a hierarchical structure. At the patient level, the EHR systems record demographic information and capture multiple admissions/visits in a timeordered manner. Each admission represents a specific hospitalization period and contains multiple stay records, International Classification of Diseases (ICD) codes for billing, drug codes, and a corresponding clinical note. Each stay record includes hourly clinical monitoring readings like heart rate, arterial blood pressure, and respiratory rate.\nIn addition to the intricate hierarchy of EHR data, the prediction tasks vary across levels. As we move from the top to the bottom levels, the prediction tasks become more time-sensitive. Patientlevel data are usually used to predict the risk of a patient suffering from potential diseases after six months or one year, i.e., the health risk prediction\n2Most of the multimodal pretraining models use medical images and other modalities, such as (Hervella et al., 2021). However, it is impossible to link EHR data and medical images in practice due to data privacy issues.\ntask. Admission-level data are employed for relatively shorter-term predictions, such as readmission within 30 days. Stay-level data are typically utilized for hourly predictions, such as forecasting acute respiratory failure (ARF) within a few hours.\nDesigning an ideal \u201cone-in-all\u201d medical pretraining model that can effectively incorporate multimodal, heterogeneous, and hierarchical EHR data as inputs, while performing self-supervised learning across different levels, is a complex undertaking. This complexity arises due to the varying data types encountered at different levels. At the stay level, the data primarily consist of time-ordered numerical clinical variables. However, at the admission level, the data not only encompass sequential numerical features from stays but also include sets of discrete ICD and drug codes, as well as unstructured clinical notes. As a result, it becomes challenging to devise appropriate pretraining tasks capable of effectively extracting knowledge from the intricate EHR data.\nIn this paper, we present a novel Hierarchical Multimodal Pretraining framework (called MEDHMP) to tackle the aforementioned challenges in the Medical domain. MEDHMP simultaneously incorporates five modalities as inputs, including patient demographics, temporal clinical features for stays, ICD codes, drug codes, and clinical notes. To effectively pretrain MEDHMP, we adopt a \u201cbottom-to-up\u201d approach and introduce level-specific self-supervised learning tasks. At the stay level, we propose reconstructing the numerical time-ordered clinical features. We devise two pretraining strategies for the admission level. The first focuses on modeling intra-modality relations by predicting a set of masked ICD and drug codes. The second involves modeling inter-modality relations through modality-level contrastive learning. To train the complete MEDHMP model, we utilize a two-stage training strategy from stay to admission levels3.\nWe utilize two publicly available medical datasets for pretraining the proposed MEDHMP and evaluate its performance on three levels of downstream tasks. These tasks include ARF, shock and mortality predictions at the stay level, readmis-\n3It is important to note that we have not incorporated patient-level pertaining in MEDHMP. This decision is based on the understanding that the relations among admissions in EHR data are not as strong as consecutive words in texts. Arbitrary modeling of such relations may impede the learning of stay and admission levels.\nsion prediction at the admission level, and health risk prediction at the patient level. Through our experiments, we validate the effectiveness of the proposed MEDHMP by comparing it with stateof-the-art baselines. The results obtained clearly indicate the valuable contribution of MEDHMP in the medical domain and highlight its superior performance enhancements in these predictive downstream tasks."
        },
        {
            "heading": "2 Methodology",
            "text": "As highlighted in Section 1, EHR data exhibit considerable complexity and heterogeneity. To tackle this issue, we introduce MEDHMP as a solution that leverages pretraining strategies across multiple modalities and different levels within the EHR hierarchy to achieve unification. In the following sections, we present the design details of the proposed MEDHMP."
        },
        {
            "heading": "2.1 Model Input",
            "text": "As shown in Figure 1, each patient data consist of multiple time-ordered hospital admissions, i.e., P = [A1,A2, \u00b7 \u00b7 \u00b7 ,AN ], where Ai (i \u2208 [1, n]) is the i-th admission, and N is the number of admissions. Note that for different patients, N may be different. Each patient also has a set of demographic features denoted as D. Each admission Ai consists of multiple time-ordered staylevel data denoted as Si, a set of ICD codes denoted as Ci, a piece of clinical notes denoted as Li, and a set of drug codes Gi, i.e., Ai = {Si, Ci,Li,Gi}. The stay-level data Si contains a sequence of hourly-recorded monitoring stays, i.e., Si = [S1i ,S2i , \u00b7 \u00b7 \u00b7 ,S Mi i ], where S j i represents the feature matrix of the j-th stay, and Mi denotes the number of stays within each admission."
        },
        {
            "heading": "2.2 Stay-level Self-supervised Pretraining",
            "text": "We conduct the self-supervised pretraining in a bottom-to-top way and start from the stay level. When pretraining the stay-level data, we only use Si and D since the diagnosis codes Ci, drug codes Gi and clinical notes Li are recorded at the end of the i-th admission. However, demographic information is highly related to a patient\u2019s clinical monitoring features in general. Due to the monitoring features being recorded with numerical values, we propose to use a reconstruction strategy as the stay-level pretraining task, as illustrated in Figure 2."
        },
        {
            "heading": "2.2.1 Stay-level Feature Encoding",
            "text": "Each stay Sji \u2208 Si consists of a set of timeordered hourly clinical features, i.e., Sji = [mji,1,m j i,2, \u00b7 \u00b7 \u00b7 ,m j i,T ], where m j i,t \u2208 Rdf is the recorded feature vector at the t-th hour, T is the number of monitoring hours, and df denotes the number of time-series clinical features. To model the temporal characteristic of Sji , we directly apply long-short term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and treat the output cell state hji as the representation of the j-th stay, i.e.,\nhji = LSTMenc([m j i,1,m j i,2, \u00b7 \u00b7 \u00b7 ,m j i,T ]), (1)\nwhere LSTMenc is the encoding LSTM network."
        },
        {
            "heading": "2.2.2 Clinical Feature Reconstruction",
            "text": "A naive approach to reconstructing the input staylevel feature Sji is simply applying an LSTM decoder as (Srivastava et al., 2015) does. However, this straightforward approach may not work for the clinical data. The reason is that the clinical feature vector mji,k \u2208 S j i is extremely sparse due to the impossibility of monitoring all the vital signs and conducting all examinations for a patient. To accurately reconstruct such a sparse matrix, we need to use the demographic information D as the guidance because some examinations are highly related to age or gender, which also makes us achieve the goal of multi-modal pretraining.\nSpecifically, we first embed the demographic information D into a dense vector representation, i.e., d = MLPd(D), where MLPd denotes the multilayer perceptron activated by the ReLU function. To fuse the demographic representation and the stay representation, we propose to use a transformer block in which self-attention is performed for modality fusion, followed by residual calculation, normalization, and a pooling operation com-\npressing the latent representation to the unified dimension size. We obtain the bimodal representation bji as follows:\nb\u0302ji = Softmax( WQh \u27e8h j i ,d\u27e9 \u00b7W K h \u27e8hji ,d\u27e9\u221a\ndr ) \u00b7WVh \u27e8hji ,d\u27e9,\nbji = MaxPooling(LayerNorm(\u27e8h j i ,d\u27e9+ b\u0302 j i )),\n(2)\nwhere \u27e8\u00b7, \u00b7\u27e9 means the operation of stacking, WQh , WKh , W V h \u2208 Rdr\u00d7dr are trainable parameters, and dr is the unified size of representation. Using the fused representation bji , MEDHMP then reconstructs the input clinical feature matrix Sji . Since the clinical features are time-series data, we take bji as the initial hidden state of the LSTM decoder LSTMdec to sequentially reconstruct the corresponding clinical feature m\u0303ji,k = LSTMdec(b j i )."
        },
        {
            "heading": "2.2.3 Stay-level Pretraining Loss",
            "text": "After obtaining the reconstructed clinical features [m\u0303ji,1, m\u0303 j i,2, \u00b7 \u00b7 \u00b7 , m\u0303 j i,T ], we then apply the mean squared error (MSE) as the pretraining loss to train the parameters in the stay-level as follows:\nLstay = 1\nN \u2217M \u2217 T N\u2211 i=1 M\u2211 j=1 T\u2211 t=1 ||mji,t \u2212 m\u0303 j i,t|| 2 2.\n(3)"
        },
        {
            "heading": "2.3 Admission-level Pretraining",
            "text": "The stay-level pretraining allows MEDHMP to acquire the sufficient capability of representing stays, laying the groundwork for the pretraining at the admission level. Next, we introduce the details of pretraining at this level."
        },
        {
            "heading": "2.3.1 Admission-level Feature Encoding",
            "text": "As introduced in Section 2.1, each admission Ai = {Si, Ci,Li,Gi}. To conduct the self-supervised pretraining, the first step is to encode each input to a latent representation.\nIn Section 2.2, we can obtain the representation of each hourly feature bji using Eq. (2). Thus, we can further have the stay-level overall representation si by aggregating all hourly representations of Si via a linear transformation as follows:\nsi = W \u22a4 s \u27e8b1i ;b2i ; \u00b7 \u00b7 \u00b7 ;bMi \u27e9+ bs, (4)\nwhere \u27e8\u00b7; \u00b7\u27e9 is the concatenation operation. Ws \u2208 Rdr\u00d7M\u2217dr and bs \u2208 Rdr are parameters.\nFor ICD codes Ci and drug codes Gi, they will be converted to binary vectors and then map them to\nlatent representations via MLP layers, which is similar to the mapping of the demographic information, as follows:\nci = MLPc(Ci),gi = MLPg(Gi). (5)\nFor the unstructured clinical notes Li, we directly use a pretrained domain-specific encoder (Lehman and Johnson, 2023) to generate its representation li.\nUsing the learned representations, we can conduct admission-level pretraining. Due to the unique characteristics of multimodal EHR data, we will focus on two kinds of pretraining tasks: mask code prediction for intra-modalities and contrastive learning for inter-modalities, as shown in Figure 3."
        },
        {
            "heading": "2.3.2 Intra-modality Mask Code Prediction",
            "text": "In the natural language processing (NLP) domain, mask language modeling (MLM) (Devlin et al., 2018) is a prevalent pretraining task encouraging the model to capture correlations between tokens. However, the EHR data within an admission Ai are significantly different from text data, where the ICD and drug codes are sets instead of sequences. Moreover, the codes are distinct. In other words, no identical codes appear in Ci and Gi. Thus, it is essential to design a new loss function to predict the masked codes.\nLet cmi \u2208 R|C| and gmi \u2208 R|G| denote the mask indicator vectors, where |C| and |G| denote the distinct number of ICD codes and drug codes, respectively. If the j-th ICD code is masked, then cmi [j] = 1; otherwise, c m i [j] = 0. Let c \u2032 i and g \u2032 i denote the embeddings learned for the remaining codes. To predict the masked codes, we need to obtain the admission representation. Toward this\nend, we first stack all the learned embeddings as follows:\nfi = \u27e8si, c\u2032i,g\u2032i, li\u27e9. (6)\nThen another transformer encoder block is used to obtain the cross-modal admission representation as follows:\na\u0302i = Softmax( WQa fi \u00b7WKa fi\u221a\ndr ) \u00b7WVa fi,\nai = MaxPooling(LayerNorm(fi + a\u0302i)), (7)\nwhere WQa , WKa , and W V a \u2208 Rdr\u00d7dr are trainable parameters. We can predict the masked codes using the learned admission representation ai using Eq. (7) as follows:\npci = Sigmoid(MLPmc(ai)), pgi = Sigmoid(MLPmg(ai)), (8)\nwhere the predicted probability vectors pci \u2208 R|C| and pgi \u2208 R|G|.\nFinally, the MSE loss serves as the objective function of the masked code prediction (MCP) task for the intra-modality modeling as follows:\nLMCP = 1\nN N\u2211 i=1 (||pci \u2212 cmi ||22\n+ ||pgi \u2212 g m i ||22),\n(9)\nwhere \u2299 is the element-wise multiplication."
        },
        {
            "heading": "2.3.3 Inter-modality Contrastive Learning",
            "text": "The intra-modality modeling aims to learn feature relations within a single modality using other modalities\u2019 information. On top of it, we also\nconsider inter-modality relations. Intuitively, the four representations {si, ci,gi, li} within Ai share similar information. If a certain modality ri \u2208 {si, ci,gi, li} is masked, the similarity between ri and the aggregated representation ai\\ri learned from the remaining ones should be still larger than that between ri and another admission\u2019s representation aj\\rj within the same batch, where j \u0338= i.\nBased on this intuition, we propose to use the noise contrastive estimation (NCE) loss as the intermodality modeling objective as follows:\nLCL = 1\n3N N\u2211 i=1 \u2211 ri\u2208{ci,gi,li} u(ri),\nu(ri) = \u2212 log esim(ri,ai\\ri)/\u03c4\u2211B\nj=1,j \u0338=i e sim(ri,aj\\rj)/\u03c4\n,\n(10)\nwhere sim(\u00b7, \u00b7) denotes the cosine similarity, B is the batch size, and \u03c4 is the temperature hyperparameter. ai\\ri is obtained using Eqs. (6) and (7) by removing the masked modality ri. Note that in our design, si is a trained representation by optimizing the stay-level objective via Eq. (3). However, the other three modality representations are learned from scratch or the pretrained initialization. To avoid overfitting si, we do not mask the stay-level representation si in Eq. (10)."
        },
        {
            "heading": "2.3.4 Admission-level Pretraining Loss",
            "text": "The final loss function in the admission-level pretraining is represented as follows:\nLadmission = LMCP + \u03bbLCL, (11)\nwhere \u03bb is a hyperparameter to balance the losses between the intra-modality mask code prediction task and the inter-modality contrastive learning."
        },
        {
            "heading": "2.4 Training of MEDHMP",
            "text": "We use a two-stage training strategy to train the proposed MEDHMP. In the first stage, we pre-train the stay-level task via Eq. (3) by convergence. In the second stage, we use the learned parameters in the first stage as initialization and then train the admission-level task via Eq. (11)."
        },
        {
            "heading": "3 Experiments",
            "text": "In this section, we first introduce the data for pretraining and downstream tasks and then exhibit experimental results (mean values of five runs)."
        },
        {
            "heading": "3.1 Data Extraction",
            "text": "We utilize two publicly available multimodal EHR datasets \u2013 MIMIC-III (Johnson et al., 2016) and MIMIC-IV (Johnson et al., 2020) \u2013 to pretrain the proposed MEDHMP. We adopt FIDDLE (Tang et al., 2020) to extract the pretraining data and use different levels\u2019 downstream tasks to evaluate the effectiveness of the proposed MEDHMP. For the stay-level evaluation, we predict whether the patient will suffer from acute respiratory failure (ARF)/shock/mortality within 48 hours by extracting data from the MIMIC-III dataset4. For the admission-level evaluation, we rely on the same pipeline for extracting data from the MIMIC-III dataset to predict the 30-day readmission rate. For the patient-level evaluation, we conduct four health risk prediction tasks by extracting the heart failure data from MIMIC-III following (Choi et al., 2016b) and the data of chronic obstructive pulmonary disease (COPD), amnesia, and heart failure from TriNetX5. The details of data extraction and statistics can be found in Appendix A. The implementation details of MEDHMP are in Appendix B."
        },
        {
            "heading": "3.2 Stay-level Evaluation",
            "text": "We conduct two experiments to validate the usefulness of the proposed MEDHMP at the stay level."
        },
        {
            "heading": "3.2.1 Stay-level Multimodal Evaluation",
            "text": "In this experiment, we take two modalities, i.e., demographics and clinical features, as the model inputs. The bimodal representation bji learned by Eq. (2) is then fed into a fully connected layer followed by the sigmoid activation function to calculate the prediction. We use the cross entropy as the loss function to finetune MEDHMP.\nWe use F-LSTM (Tang et al., 2020), FCNN (Tang et al., 2020), RAIM (Xu et al., 2018), and DCMN (Feng et al., 2019) as the baselines. The details of each baseline can be found in Appendix C. We utilize the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall curve (AUPR) as evaluation metrics.\nThe experimental results are presented in Table 1, showcasing the superior performance of MEDHMP compared to the bimodal baselines in all three stay-level tasks. This indicates the proficiency of MEDHMP in effectively utilizing both\n4https://github.com/MLD3/FIDDLE-experiments/ tree/master/mimic3_experiments\n5https://trinetx.com/\nclinical and demographic features. Remarkably, MEDHMP demonstrates a particularly strong advantage when handling tasks with smaller-sized datasets (See Table 8 for data scale). This observation suggests that MEDHMP greatly benefits from our effective pre-training procedure, enabling it to deliver impressive performance, especially in low-resource conditions.\nNote that in the previous work (Yang and Wu, 2021), except for the demographics and clinical features, clinical notes are used to make predictions on the ARF task. We also conducted such experiments on the three tasks, and the results are listed in Appendix D. The experimental results still demonstrate the effectiveness of the proposed pretraining framework."
        },
        {
            "heading": "3.2.2 Stay-level Unimodal Evaluation",
            "text": "To validate the transferability of the proposed MEDHMP, we also conduct the following experiment by initializing the encoders of baselines using the pretrained MEDHMP. In this experiment, we only take the clinical features as models\u2019 inputs. Two baselines are used: LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017). We use the pretrained LSTM encoder LSTMenc in Section 2.2.1 to replace the original linear encoders in LSTM and Transformer. Our encoder will be finetuned with the training of LSTM and Transformer.\nThe experimental results on the ARF task are shown in Figure 4. As mentioned in Section 2.4, we train the LSTM encoder LSTMenc twice. \u201cw. MEDHMPa\u201d means that the baselines use a well-trained admission-level LSTMenc. \u201cw. MEDHMPs\u201d indicates that the baselines use a stay-level trained LSTMenc. \u201cOriginal\u201d denotes the original baselines. We can observe that using partially- or well-trained encoders helps improve performance. These results also confirm the necessity of the proposed two-stage training strategy."
        },
        {
            "heading": "3.3 Admission-level Evaluation",
            "text": "We also adopt the readmission prediction task within 30 days to evaluate MEDHMP at the ad-\nmission level. In this task, the model will task all modalities as the input, including demographics, clinical features, ICD codes, drug codes, and a corresponding clinical note for admission. In this experiment, we first learn the representations, i.e., si using Eq. (4), ci and gi via Eq. (5), and li, to obtain the stacked embedding fi. We then apply Eq. (7) to obtain the admission embedding ai. Finally, a fully connected layer with the sigmoid function is used for prediction. We still use the cross-entropy loss as the optimization function.\nWe follow the existing work (Yang and Wu, 2021) and use its eight multimodal approaches as baselines, which adopt modality-specific encoders and perform modality aggregation via a gating mechanism. Different from the original model design, we perform a pooling operation on the latent representation of multiple clinical time series belonging to a specific admission, such that baselines can also take advantage of multiple stays. Details of these models can be found in Appendix D. We still use AUROC and AUPR as evaluation metrics.\nAdmission-level results are listed in Table 2, and we can observe that the proposed MEDHMP outperforms all baselines. Compared to the best baseline performance, the AUROC and AUPR scores of MEDHMP increase 7% and 29%, respectively. These results once again prove the effectiveness of the proposed pretraining model."
        },
        {
            "heading": "3.4 Patient-level Evaluation",
            "text": "Even though MEDHMP has not been pretrained on patient-level tasks, it is still capable of handling tasks at this level since its unimodal encoders acquire the ability to generate a high-quality representation of each admission, thus become feasible to be utilized to boost existing time series-targeting models. Health risk prediction, which utilizes a sequence of hospital admissions for illness forecasting, is applied as the task at the patient level.\nIn this experiment, the model will take a sequence of admission-level ICD codes as the input, which is still a unimodal evaluation. We use the following approaches as baselines: LSTM (Hochreiter and Schmidhuber, 1997), Dipole (Ma et al., 2017), RETAIN (Choi et al., 2016b), AdaCare (Ma et al., 2020), and HiTANet (Luo et al., 2020). Details of these approaches can be found in Appendix E. Following previous health risk prediction work (Chen et al., 2021; Cui et al., 2022a), we use AUPR, F1, and Cohen\u2019s Kappa as the evaluation metrics."
        },
        {
            "heading": "3.4.1 Performance Comparison",
            "text": "The experimental results are shown in Table 3, where the approach with the subscript \u201ca\u201d denotes the baseline using the pretrained MEDHMP to initialize the ICD code embedding ci via Eq. (5). We can find that introducing the pretrained unimodal encoder from MEDHMP achieves stable improvement across most of the baselines and tasks. These results demonstrate the flexibility and effectiveness of our proposed MEDHMP in diverse medical scenarios. The knowledge from our pretrained model can be easily adapted to any sub-modality setting."
        },
        {
            "heading": "3.4.2 Influence of Training Size",
            "text": "Intuitively, pretraining could lead to improved initialization performance compared to models trained from scratch, thereby enhancing its suitability in\nlow-resource settings such as zero-shot learning and few-shot learning. Inspired by these characteristics, we explore low-resource settings that simulate common real-world health-related scenarios. We replicate the experiments introduced in the previous section but vary the size of the training set from 1% to 100%.\nFigure 5 shows the experimental results using the HiTANet model. We can observe that using the pretraining initialization, HiTANeta always achieves better performance. Even with 10% training data, it can achieve comparable performance with the plain HiTANet using 100% data. This promising result confirms that the proposed pretraining framework MEDHMP is useful and meaningful for medical tasks, especially when the training data are insufficient."
        },
        {
            "heading": "3.4.3 Convergence Analysis with Pretraining",
            "text": "In this experiment, we aim to explore whether using pretraining can speed up the convergence of model training. We use the basic LSTM model as the baseline and output the testing performance at each epoch. Figure 6 shows the results. We can observe that at each epoch, the F1 score of LSTMa is higher than that of LSTM, indicating the benefit of using pretraining. Besides, LSTMa achieves the best performance at the 5-th epoch, but the F1 score of the plain LSTM still vibrates. Thus, these results clearly demonstrate that using pretraining techniques can make the model converge faster with less time and achieve better performance."
        },
        {
            "heading": "4 Ablation Study",
            "text": ""
        },
        {
            "heading": "4.1 Hierarchical Pretraining",
            "text": "For the comprehensive analysis of the effect of stayand admission-level pretraining, we perform ablation studies spanning downstream tasks at all three levels. Results of patient-level, admission-level, and stay-level tasks are listed in Table 4, 5 and 6, respectively. The subscripts \u201ca\u201d (admission) and \u201cs\u201d (stay) in these tables indicate which pretrained model is used as the initialization of MEDHMP.\nFrom the results of all three tables, we can observe that the combination of both stay- and admission-level pretraining manifests superior performance, further underlining the necessity of adopting hierarchical pretraining strategies. Besides, compared with the model without any pretraining techniques, merely using a part of the proposed pretraining strategy for initialization can improve the performance. These observations imply the correct rationale behind our design of hierarchical pretraining strategies."
        },
        {
            "heading": "4.2 Multimodal Modeling",
            "text": "To investigate how intra- and inter-modality modeling techniques benefit our admission-level pretraining, we perform an ablation study on three tasks at the stay-level to examine the effectiveness of Mask\nCode Prediction (MCP) and Contrastive Learning (CL) losses. We compare MEDHMP pretrained with all loss terms, with MCP and stay-level loss, with CL and stay-level loss, and stay-level loss only, respectively. Results presented in Table 7 clearly demonstrate the efficacy of each proposed loss term as well as the designed pretraining strategy. Besides, lacking each of them results in performance reduction, highlighting that combining intra- and inter-modality modeling is indispensable for boosting the model comprehensively."
        },
        {
            "heading": "5 Related Work",
            "text": "Predictive modeling using EHR data has attracted significant attention in recent years (Cui et al., 2022b; Ma et al., 2021; Xiao et al., 2018; Wang et al., 2022). To enhance predictive performance, pretraining techniques have been explored. In this section, we provide a concise overview of studies conducted on pretraining with both single-modal and multimodal EHR data."
        },
        {
            "heading": "5.1 Unimodal Pretraining with EHR Data",
            "text": "Several pretrained models have been proposed by utilizing single-modal EHR data. Building upon the success of Large Language Models\n(LLMs) (Devlin et al., 2018; Radford et al., 2018) in NLP, researchers have endeavored to train medical-specific language models using clinical notes (Li et al., 2022b; Lehman and Johnson, 2023; Alsentzer et al., 2019; Peng et al., 2019) and PubMed data (Luo et al., 2022; Lee et al., 2020; Yuan et al., 2022; Jin et al., 2019; Warikoo et al., 2021). However, these models primarily rely on mask language modeling techniques for pretraining, thereby overlooking the distinctive characteristics of medical data.\nGiven the time-ordered nature of admissions, medical codes can be treated as a sequence. Some pertaining models have proposed to establish representations of medical codes (Rasmy et al., 2021; Li et al., 2020; Shang et al., 2019; Choi et al., 2016a, 2018). Nevertheless, these studies still adhere to the commonly used pretraining techniques in the NLP domain. Another line of work (Tipirneni and Reddy, 2022; Wickstr\u00f8m et al., 2022) is to conduct self-supervised learning on clinical features. However, these pretrained models can only be used for the downstream tasks at the stay level, limiting their transferability in many clinical application scenarios."
        },
        {
            "heading": "5.2 Multimodal Pretraining with EHR data",
            "text": "Most of the multimodal pretraining models in the medical domain are mainly using medical images (Qiu et al., 2023) with other types of modalities, such as text (Hervella et al., 2021, 2022a,b; Khare et al., 2021) and tabular information (Hager et al., 2023). Only a few studies focus on pretraining on multimodal EHR data without leveraging medical images. The work (Li et al., 2022a, 2020) claims their success on multimodal pretraining utilizing numerical clinical features and diagnosis codes. In (Liu et al., 2022), the authors aim to model the interactions between clinical language and clinical codes. Besides, the authors in (Meng et al., 2021) use ICD codes, demographics, and topics learned from text data as the input and utilize the mask language modeling technique to pretrain the model. However, all existing pretrained work on EHR data still follows the routine of NLP pre-\ntraining but ignores the hierarchical nature of EHRs in their pretraining, resulting in the disadvantage that the pretrained models cannot tackle diverse downstream tasks at different levels."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we present a novel pretraining model called MEDHMP designed to address the hierarchical nature of multimodal electronic health record (EHR) data. Our approach involves pretraining MEDHMP at two levels: the stay level and the admission level. At the stay level, MEDHMP uses a reconstruction loss applied to the clinical features as the objective. At the admission level, we propose two losses. The first loss aims to model intra-modality relations by predicting masked medical codes. The second loss focuses on capturing inter-modality relations through modality-level contrastive learning. Through extensive multimodal evaluation on diverse downstream tasks at different levels, we demonstrate the significant effectiveness of MEDHMP. Furthermore, experimental results on unimodal evaluation highlight its applicability in low-resource clinical settings and its ability to accelerate convergence."
        },
        {
            "heading": "7 Limitations",
            "text": "Despite the advantages outlined in the preceding sections, it is important to note that MEDHMP does have its limitations. Owing to the adoption of a large batch size to enhance contrastive learning (see Appendix B for more details), it becomes computationally unfeasible to fine-tune the language model acting as the encoder for clinical notes during our admission-level pretraining. As a result, ClinicalT5 is held static to generate a fixed representation of the clinical note, which may circumscribe potential advancements. Additionally, as described in Appendix A, we only select admissions with ICD-9 diagnosis codes while excluding those with ICD-10 to prevent conflicts arising from differing coding standards. This selection process, however, implies that MEDHMP currently lacks the capacity to be applied in clinical scenarios where ICD-10 is the standard for diagnosis code."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is partially supported by the US National Science Foundation under Grants #2238275 and #2212323, and the US National Institutes of Health under Grant R01AG077016."
        },
        {
            "heading": "A Data Processing",
            "text": "We utilize two publicly available multimodal EHR datasets \u2013 MIMIC-III (Johnson et al., 2016) and MIMIC-IV (Johnson et al., 2020) \u2013 to pretrain the proposed MEDHMP. Considering that MIMIC-III uses ICD-9 codes while MIMIC-IV incorporates both ICD-9 and ICD-10 codes, we only select admissions with ICD-9 diagnosis codes to avoid potential conflicts between different coding standards. To prevent the label leakage issue during the testing stage, we remove all signals related to downstream tasks from the original data.\nWe adopt the EHR-oriented preprocessing pipeline, FIDDLE (Tang et al., 2020), for feature and label extraction at the stay level. We standardize the length of the clinical monitoring feature to T = 48 hours, which is the upper bound for clinical feature-related tasks mentioned in (Tang et al., 2020). We filter out the features with a frequency lower than 5% since extremely sparse features can significantly harm computing efficiency and be memory burdensome. After data preprocessing, each hourly clinical feature mji,t is represented as a 1,318-dimensional sparse vector, i.e., df = 1, 318. The demographics for each patient are represented as a 73-dimensional sparse vector, i.e., the length of D is 73. The number of unique ICD codes |C|\nis 7,686, and the number of unique drug codes |G| is 1,701. Finally, we get 99,000 admissions with 100,563 stays for pretraining MEDHMP.\nThe three datasets extracted from TriNetX are supervised by clinicians. We employ the extraction method described in (Choi et al., 2016b) to identify case patients. Specifically, we identify the initial diagnosis date and utilize the patient\u2019s historical data leading up to a six-month window, where the diagnosis date marks its end. This approach ensures that we prevent label leakage and successfully accomplish the objective of early prediction. Three control cases are chosen for each positive case based on matching criteria such as gender, age, race, and underlying diseases. For control patients, we use the last 50 visits in the database.\nThe statistics of data used for both pretraining and downstream tasks can be found in Table 8.\nB Implementation and Configuration\nAll models were implemented using PyTorch 2.0.0 and Python 3.9.12. Preprocessing and experiments were conducted in the Ubuntu 20.04 system with 376 GB of RAM and two NVIDIA A100 GPUs.\nEach experiment was repeated five times to eliminate randomness, and the mean of the evaluation metrics was reported in all experimental results.\nFor unimodal evaluations, we used the same set of hyperparameters, regardless of whether a pretrained encoder was used, to ensure a fair comparison. For multimodal evaluations, we either used the hyperparameters reported by the authors of the baselines or suggested in their release codes. For detailed hyperparameters not provided by these authors, we used the same hyperparameters as in our model for a fair comparison. dr was set to 256 for our pretraining and evaluation in downstream tasks. For stay-level pretraining, our model was pretrained for 200 epochs with a learning rate of 5e-4, a batch size of 128, and a weight decay of 1e-8. At the admission level, our model was pretrained for 300 epochs, with a learning rate of 2e-5 and a weight decay of 1e8. Following previous works (Chen et al., 2022, 2020), which emphasized the necessity of adopting a large batch size in contrastive learning, we set the batch size to 4096 to enhance our inter-modality modeling. \u03c4 in contrastive learning loss was set to 0.1. The hyperparameter \u03bb mentioned in Eq. (11) was set to 0.1 to balance the masked code prediction (MCP) and contrastive learning (CL) losses\nin the stay-level pretraining. The masking rate in the MCP task was set to 15%, following the design of (Devlin et al., 2018). The optimizer used throughout the pretraining stage was AdamW.\nFor downstream tasks, we selected the hyperparameters of our model using Grid Search. The batch size was chosen from the set [16, 32, 64], and the learning rate was searched in the range from 2e-5 to 5e-3. The maximum number of epochs was set to 30, and the patience for early stopping and weight decay were configured to 5 and 1e-2, respectively, to avoid overfitting. We found that the SGD optimizer performed better during the fine-tuning procedure."
        },
        {
            "heading": "C Stay-level Experiments",
            "text": "Besides unimodal baselines mentioned in Section 3.2.2, the following approaches serving as baselines in the multimodal evaluation at the stay level are listed below: (1) F-LSTM (Tang et al., 2020) is a classic Long Short-Term Memory (LSTM) model taking concatenation of clinical features and demographic information as input. (2) F-CNN (Tang et al., 2020) is a typical Convolutional Neural Network (CNN) architecture using the concatenation of clinical features and demographic information\nfor prediction. (3) Raim (Xu et al., 2018) is an attention-based model specially designed for analyzing ICU monitoring data, which uses a combination of attention mechanisms and multimodal data integration. (4) DCMN (Feng et al., 2019) combines two separate memory networks, one for processing clinical time series and one for processing static tables. Its dual-attention mechanism design allows the model to aggregate features effectively."
        },
        {
            "heading": "D Stay-level Experiments with Clinical Notes",
            "text": "All the baselines utilized in the readmission prediction task are based on the previous work (Yang and Wu, 2021). In their study, the authors investigate various combinations of unimodal encoders and employ a gating mechanism for modality aggregation. In this approach, one modality is considered the main modality, and the embeddings from the other modalities are added as auxiliary modalities. Specific details regarding the composition of these baselines, including how the unimodal encoders are combined, can be found in Table 9.\nThe experimental results are presented in Table 10. It is evident that relying solely on a single modality, such as clinical notes, is inadequate for\nachieving accurate predictions when compared to multimodal baselines. Among all the multimodal models, our proposed MEDHMP consistently outperforms the others in the majority of scenarios. These results highlight two key findings: (1) the significance of integrating multimodal information in health predictive modeling tasks and (2) the efficacy of the proposed pretraining technique."
        },
        {
            "heading": "E Patient-level Experiments",
            "text": "Baselines regarding the patient-level task are listed below. (1) LSTM(Hochreiter and Schmidhuber, 1997) is a typical backbone model appearing in time series forecasting tasks. (2) HiTANet(Luo et al., 2020) adopts the time-aware attention mechanism design that enables itself to capture the dynamic disease progression pattern. (3) Dipole(Ma et al., 2017) relies on the combination of bidirectional GRU and attention mechanism to analyze sequential visits of a patient. (4) AdaCare(Ma et al., 2020) applies the Convolutional Neural Network for feature extraction, followed by a GRU block for prediction. (5) Retain (Choi et al., 2016b) utilizes the reverse time attention mechanism to capture dependency between various visits of a patient."
        },
        {
            "heading": "F Evaluation Metrics",
            "text": "Evaluation metrics used in our experiments are listed below:\n\u2022 AUROC (Area Under the Receiver Operating Characteristic Curve) represents the likelihood that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. It provides an aggregate measure of performance across all possible classification thresholds.\n\u2022 AUPRC (Area Under the Precision-Recall Curve) measures the area beneath the Precision-Recall curve, a plot of the precision against recall for different threshold values.\n\u2022 F1 Score is the harmonic mean of precision and recall, offering a balance between the two when their values diverge.\n\u2022 Cohen\u2019s Kappa is a statistic that measures inter-rater agreement for categorical items, accounting for the possibility of the agreement occurring by chance."
        },
        {
            "heading": "G Experiments on EICU Database",
            "text": "To further validate the transferability of our proposed MEDHMP, we conduct experiments using data from additional medical databases, i.e., eICU6. Results can be found in Table 11. Our proposed MEDHMP shows superior performance consistent with experiments on the MIMIC-III database, implying its excellent capability of learning general medical features.\n6https://eicu-crd.mit.edu/"
        }
    ],
    "title": "Hierarchical Pretraining on Multimodal Electronic Health Records",
    "year": 2023
}