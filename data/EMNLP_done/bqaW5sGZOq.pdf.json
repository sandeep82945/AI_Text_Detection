{
    "abstractText": "Machine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translatetest), or translating the training set into the target languages and finetuning a multilingual model (translate-train). However, most research in the area focuses on the multilingual models rather than the MT component. We show that, by using a stronger MT system and mitigating the mismatch between training on original text and running inference on machine translated text, translate-test can do substantially better than previously assumed. The optimal approach, however, is highly task dependent, as we identify various sources of cross-lingual transfer gap that affect different tasks and approaches differently. Our work calls into question the dominance of multilingual models for cross-lingual classification, and prompts to pay more attention to MTbased baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mikel Artetxe"
        },
        {
            "affiliations": [],
            "name": "Vedanuj Goswami"
        },
        {
            "affiliations": [],
            "name": "Shruti Bhosale"
        },
        {
            "affiliations": [],
            "name": "Angela Fan"
        },
        {
            "affiliations": [],
            "name": "Luke Zettlemoyer"
        }
    ],
    "id": "SP:fee08092b857adaa471d78efae5f36a211b1e613",
    "references": [
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Translation artifacts in cross-lingual transfer learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7674\u20137684, Online. Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Carmen Banea",
                "Rada Mihalcea",
                "Janyce Wiebe",
                "Samer Hassan."
            ],
            "title": "Multilingual subjectivity analysis using machine translation",
            "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127\u2013135, Honolulu,",
            "year": 2008
        },
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Jianfeng Gao",
                "Yejin Choi"
            ],
            "title": "PIQA: Reasoning about physical commonsense in natural language",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Xian-Ling Mao",
                "Heyan Huang",
                "Furu Wei."
            ],
            "title": "XLM-E: Cross-lingual language model pre-training via ELECTRA",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6170\u20136182,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord"
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "year": 2018
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: Evaluating cross-lingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Sumanth Doddapaneni",
                "Gowtham Ramesh",
                "Mitesh M. Khapra",
                "Anoop Kunchukuttan",
                "Pratyush Kumar"
            ],
            "title": "A primer on pretrained multilingual language models",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Duh",
                "Akinori Fujino",
                "Masaaki Nagata"
            ],
            "title": "Is machine translation ripe for cross-lingual sentiment classification",
            "venue": "In Proceedings of the 49th Annual Meeting of the Association",
            "year": 2011
        },
        {
            "authors": [
                "Sergey Edunov",
                "Myle Ott",
                "Michael Auli",
                "David Grangier."
            ],
            "title": "Understanding back-translation at scale",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489\u2013500, Brussels, Belgium. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Yuwei Fang",
                "Shuohang Wang",
                "Zhe Gan",
                "Siqi Sun",
                "Jingjing Liu."
            ],
            "title": "Filter: An enhanced fusion method for cross-lingual language understanding",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12776\u201312784.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Fei",
                "Meishan Zhang",
                "Donghong Ji."
            ],
            "title": "Cross-lingual semantic role labeling with highquality translated training corpus",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7014\u20137026, On-",
            "year": 2020
        },
        {
            "authors": [
                "Blaz Fortuna",
                "John Shawe-Taylor."
            ],
            "title": "The use of machine translation tools for cross-lingual text mining",
            "venue": "Proceedings of the ICML Workshop on Learning with Multiple Views.",
            "year": 2005
        },
        {
            "authors": [
                "Iker Garc\u00eda-Ferrero",
                "Rodrigo Agerri",
                "German Rigau."
            ],
            "title": "Model and data transfer for crosslingual sequence labelling in zero-resource settings",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6403\u20136416, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Iker Garc\u00eda-Ferrero",
                "Rodrigo Agerri",
                "German Rigau"
            ],
            "title": "2022b. T-projection: High quality annotation projection for sequence labeling",
            "year": 2022
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Todor Mihaylov",
                "Dimitrina Zlatkova",
                "Yoan Dinkov",
                "Ivan Koychev",
                "Preslav Nakov."
            ],
            "title": "EXAMS: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering",
            "venue": "Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Tim Isbister",
                "Fredrik Carlsson",
                "Magnus Sahlgren"
            ],
            "title": "Should we stop training more monolingual models, and simply use machine translation instead",
            "venue": "In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),",
            "year": 2021
        },
        {
            "authors": [
                "Alankar Jain",
                "Bhargavi Paranjape",
                "Zachary C. Lipton."
            ],
            "title": "Entity projection via machine translation for cross-lingual NER",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Phillip Keung",
                "Yichao Lu",
                "Gy\u00f6rgy Szarvas",
                "Noah A. Smith"
            ],
            "title": "The multilingual Amazon",
            "year": 2020
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual language models",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Liu",
                "Genta Indra Winata",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Preserving cross-linguality of pre-trained models via continual learning",
            "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 64\u201371,",
            "year": 2021
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Cynthia Gao",
                "Vedanuj Goswami",
                "Francisco Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Jaehoon Oh",
                "Jongwoo Ko",
                "Se-Young Yun."
            ],
            "title": "Synergy with translation artifacts for training and inference in multilingual tasks",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6747\u20136754, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Siva Reddy"
            ],
            "title": "Modelling latent translations for",
            "year": 2021
        },
        {
            "authors": [
                "Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Yinfei Yang",
                "Yuan Zhang",
                "Chris Tar",
                "Jason Baldridge."
            ],
            "title": "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Tao Yu",
                "Shafiq Joty."
            ],
            "title": "Effective fine-tuning methods for cross-lingual adaptation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8492\u20138501, Online and Punta Cana, Dominican Republic. Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Roy Schwartz",
                "Yejin Choi."
            ],
            "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He."
            ],
            "title": "PAWS: Paraphrase adversaries from word scrambling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Bo Zheng",
                "Li Dong",
                "Shaohan Huang",
                "Wenhui Wang",
                "Zewen Chi",
                "Saksham Singhal",
                "Wanxiang Che",
                "Ting Liu",
                "Xia Song",
                "Furu Wei."
            ],
            "title": "Consistency regularization for cross-lingual fine-tuning",
            "venue": "Proceedings of the 59th Annual Meeting of the Associa-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent work in cross-lingual learning has pivoted around multilingual models, which are typically pretrained on unlabeled corpora in multiple languages using some form of language modeling objective (Doddapaneni et al., 2021). When finetuned on downstream data in a single language\u2014 typically English\u2014these models are able to generalize to the rest of the languages thanks to the aligned representations learned at pretraining time, an approach known as zero-shot transfer. The so called translate-train is an extension of this method that augments the downstream training data by translating it to all target languages through MT. A third approach, translate-test, uses MT to translate the test data into English, and runs inference using an English-only model.\n\u2217Work done at Meta AI\nThe conventional wisdom is that translate-train tends to bring modest improvements over zero-shot, while translate-test has been largely overlooked and is often not even considered as a baseline (Hu et al., 2020; Ruder et al., 2021). In this context, most recent research in multilingual NLP has focused on pretraining stronger multilingual models (Conneau et al., 2020; Xue et al., 2021; Chi et al., 2022), and/or designing better fine-tuning techniques to elicit cross-lingual transfer (Liu et al., 2021; Yu and Joty, 2021; Zheng et al., 2021; Fang et al., 2021). However, little attention has been paid to the MT component despite its central role in both translate-train and translate-test. Most authors use the official translations that some multilingual benchmarks come with, and it is unclear the extent to which better results could be obtained by using stronger MT systems or developing better integration techniques.\nIn this work, we revisit the use of MT for crosslingual learning through extensive experiments on 6 classification benchmarks. We find evidence that translate-test is more sensitive than translate-train to the quality of the MT engine, and show that better results can be obtained by mitigating the mismatch between training on original downstream data and running inference on machine translated\ndata. As exemplified by Figure 1, we demonstrate that, with enough care, translate-test can work substantially better than previously assumed (e.g., outperforming both zero-shot and translate-train on XNLI for the first time), calling into question the dominance of multilingual pretraining in the area.\nHowever, these trends are not universal, as we find that the optimal approach is highly task dependent. We introduce a new methodology to quantify the underlying sources of cross-lingual transfer gap that cause this discrepancy across tasks. We find that translate-test excels on complex tasks requiring commonsense or real world knowledge, as it benefits from the use of a stronger English-only model. In contrast, translate-train performs best at shallower tasks like sentiment analysis, for which the noise introduced by MT outweighs the benefit of using a better pretrained model."
        },
        {
            "heading": "2 Experimental setup",
            "text": "Pre-trained models. We use RoBERTa large (Liu et al., 2019) and XLM-R large (Conneau et al., 2020) as our primary English-only and multilingual models, respectively. XLM-R can be considered the multilingual version of RoBERTa,1 as they are both encoder-only masked language models trained with the same codebase. So as to understand the benefits from using a stronger English-only model, we also experiment with DeBERTaV3 large (He et al., 2021). All the 3 models have 304M backbone parameters, although they differ in the size of their vocabulary.\nMachine translation. We use the 3.3B NLLB model (NLLB Team et al., 2022) as our MT engine. We explore two decoding strategies as indicated in each experiment: beam search with a beam size of 4, and nucleus sampling (Holtzman et al., 2020) with top-p = 0.8. Unless otherwise indicated, we translate at the sentence level using the xx_sent_ud_sm model in spaCy for sentence segmentation.2 For variants translating at the document level, we concatenate all fields (e.g., the premise and the hypothesis in XNLI) using a special word <sep> to separate them. For experiments involving MT fine-tuning, we use a learning rate of 5e-05 and a batch size of 32k tokens with dropout disabled, and use the final checkpoint after 25k steps.\n1In fact, XLM-R stands for XLM-RoBERTa. 2 https://spacy.io/models/xx#xx_sent_ud_sm\nEvaluation. We use the following 6 datasets for evaluation: XNLI (Conneau et al., 2018), a Natural Language Inference (NLI) dataset covering 15 languages; PAWS-X (Yang et al., 2019), an adversarial paraphrase identification dataset covering 7 languages; MARC (Keung et al., 2020), a sentiment analysis dataset in 6 languages where one needs to predict the star rating of an Amazon review; XCOPA (Ponti et al., 2020), a causal commonsense reasoning dataset covering 12 languages; XStoryCloze (Lin et al., 2021), a commonsense reasoning dataset in 11 languages where one needs to predict the correct ending to a four-sentence story; and EXAMS (Hardalov et al., 2020), a dataset of high school multiple choice exam questions in 16 languages. For a fair comparison between different approaches, we exclude Quechua and Haitian Creole from XCOPA, as the former is not supported by NLLB and the latter is not supported by XLM-R. In all cases, we do 5 finetuning runs with different random seeds, and report accuracy numbers in the test set averaged across all languages and runs. Following common practice, we use MultiNLI (Williams et al., 2018) as our training data for XNLI, and PAWS (Zhang et al., 2019) as our training data for PAWS-X. For MARC, we use the English portion of the training set. XCOPA, XStoryCloze and EXAMS do not come with a sizable training set in English, so we train on the combination of the following multiple choice datasets: Social IQa (Sap et al., 2019), SWAG (Zellers et al., 2018), COPA (Roemmele et al., 2011), OpenBookQA (Mihaylov et al., 2018), ARC (Clark et al., 2018) and PIQA (Bisk et al., 2020).\nFine-tuning. We use HuggingFace\u2019s Transformers library (Wolf et al., 2019) for all of our experiments. So as to handle a variable number of candidates in multiple choice tasks (XCOPA, XStoryCloze and EXAMS), we feed each inputcandidate pair independently into the model, take its final representation from the first token, downproject into a scalar score through a linear projection, and apply the softmax function over the scores of all candidates. For the remaining tasks, we simply learn a regular classification head. In all cases, we use a batch size of 64 and truncate examples longer than 256 tokens. We train with a learning rate of 6e-6 with linear decay and 50 warmup steps, and use the final checkpoint without any model selection. When using the original English training set, we finetune for two epochs. For experiments\ninvolving some form of data augmentation, where each training example is (back-)translated into multiple instances, we finetune for a single epoch."
        },
        {
            "heading": "3 Main results",
            "text": "We next present our main results revisiting MT for cross-lingual classification, both for translate-test (\u00a73.1) and translate-train (\u00a73.2). Finally, we put everything together and reconsider the state-of-theart in the light of our findings (\u00a73.3)."
        },
        {
            "heading": "3.1 Revisiting translate-test",
            "text": "While translate-test might be regarded as a simple baseline, we argue that there are two critical aspects of it that have been overlooked in prior work. First, little attention has been paid to the MT engine itself: most existing works use the official translations from each dataset without any consideration about their quality, and the potential for improvement from using stronger MT systems is largely unknown. Second, translate-test uses original (human generated) English data to fine-tune the model, but the actual input that is fed into the model at test time is produced by MT. Prior work has shown that original and machine translated data have different properties, and this mismatch is detrimental for performance (Artetxe et al., 2020), but there is barely any work addressing this issue. In what follows, we report results on our 6 evaluation tasks using a strong MT system, and explore two different approaches to mitigate the train/test mismatch issue: adapting MT to produce translations that are more similar to the original training data (\u00a73.1.1), and adapting the training data to make it more similar to the translations from MT (\u00a73.1.2)."
        },
        {
            "heading": "3.1.1 MT adaptation",
            "text": "Table 1 reports translate-test results with RoBERTa using translations from 4 different MT systems: the official ones from each dataset (if any), the vanilla NLLB 3.3B model, and two fine-tuned variants of it. In the first variant (+dom adapt), we segment the downstream training data into sentences, backtranslate them into all target languages using beam search, and fine-tune NLLB on it as described in \u00a72. The second variant (+doc level) is similar, except that we concatenate the back-translated sentences first, and all the input fields (e.g. the premise and the hypothesis) afterwards, separated by <sep>, and fine-tune NLLB on the resulting synthetic parallel data. For this last variant we also translate at\nthe document level at test time, whereas the rest of the systems translate at the sentence level.3\nWe find that document-level fine-tuned NLLB obtains the best results across the board, obtaining consistent improvements over vanilla NLLB in all benchmarks except EXAMS. We remark that the former does not have any unfair advantage in terms of the data it sees, as it leverages the exact same downstream data that RoBERTa is finetuned on. Sentence-level fine-tuned NLLB is only able to outperform vanilla NLLB on XNLI, MARC and EXAMS, and does worse on PAWS-X and XStoryCloze. This shows that a large part of the improvement from fine-tuning NLLB can be attributed to learning to jointly translate all sentences and fields from each example.\nFinally, we also find a high variance in the quality of the official translations in the two datasets that include them. More concretely, the official translations are 3.0 points better than vanilla NLLB on XCOPA, and 3.1 points worse on XNLI. Despite overlooked to date, we argue that this factor has likely played an important role in some of the results from prior work. For instance, Ponti et al. (2020) obtain their best results on XCOPA using translate-test with RoBERTa, whereas Conneau et al. (2020) find that this approach obtains the worst results on XNLI. These seemingly contradictory findings can partly be explained by the higher quality of the official XCOPA translations, and would have been less divergent if the same MT engine was used for both datasets."
        },
        {
            "heading": "3.1.2 Training data adaptation",
            "text": "We experiment with a form of data augmentation where we translate each training example into another language and then back into English.4 The\n3We also tried translating at the document level with the rest of the systems, but this worked poorly in our preliminary experiments, as the model would often only translate the first sentence and ignore the rest.\n4In the forward (out-of-English) direction, we use beam search for multiplechoice tasks (XCOPA, XStoryCloze, EX-\nresulting data is aimed to be more similar to the input that the model will be exposed to at test time, as they are both produced by the same MT model. For each task, we use all target languages it covers as the pivot, and further combine the back-translated examples with the original data in English.5 We compare this approach to the baseline (training on the original English data), and report our results in Table 2. So as to understand how complementary training data adaptation and MT adaptation are, we also include a system combining the two, where we use the document-level fine-tuned model from \u00a73.1.1 to translate the test examples.\nWe find that roundtrip MT outperforms the baseline by a considerable margin in all tasks but EXAMS. While prior work has shown that multilingual benchmarks created through (professional) translation have artifacts that can be exploited through similar techniques (Artetxe et al., 2020), it is worth noting that we also get improvements on MARC, which was not generated through translation. Finally, we observe that combining this approach with MT adaptation is beneficial in most cases, but the improvements are small and inconsistent across tasks. This suggests that the two techniques are little complementary, which is not surprising as they both try to mitigate the same underlying issue."
        },
        {
            "heading": "3.2 Revisiting translate-train",
            "text": "As seen in \u00a73.1, the MT engine can have a big impact in translate-test. To get the full picture, we next explore using the same MT engines for translate-train, including the official translations (when available) as well as NLLB with beam search\nAMS), and nucleus sampling for the rest of the tasks. In the backward direction, we use beam search for all tasks. We made this decision based on preliminary experiments on the development set. Sampling in the forward direction produced more diverse translations, but was noisy for multiplechoice tasks, where some options are very short. The use of beam search when translating into English is consistent with the decoding method used at test time.\n5As such, if the original dataset has k examples and we have n target languages, the resulting data will consist of k \u2217 (n+ 1) examples.\nand nucleus sampling. In all cases, we translate the downstream training data into all target languages covered by each task, and fine-tune XLM-R on this combined data (including a copy of the original data in English). We also include a zero-shot system as a baseline, which fine-tunes the same XLM-R model on the original English data only. Table 3 reports our results.\nWe find that translate-train obtains substantial improvements over zero-shot in half of the tasks (XNLI, PAWS-X and XStoryCloze), but performs at par or even worse in the other half (MARC, XCOPA and EXAMS). This suggests that translatetrain might not be as generally helpful as found in prior work (Hu et al., 2020). In those tasks where MT does help, nucleus sampling outperforms beam search. This is in line with prior work in MT finding that sampling is superior to beam search for back-translation (Edunov et al., 2018) but, to the best of our knowledge, we are first to show that this also holds for cross-lingual classification.\nFinally, we find that translation quality had a considerably larger impact for translate-test than it does for translate-train. More concretely, the XNLI accuracy gap between the official translations and vanilla NLLB was 3.1 for translate-train, and is only 0.2 for translate-test when using beam search, or 1.0 when using nucleus sampling. This suggests that the use of relatively weak MT engines in prior work might have resulted in underestimating the potential of translate-test relative to other approaches."
        },
        {
            "heading": "3.3 Reconsidering the state-of-the-art",
            "text": "We have so far analyzed translate-test and translate-train individually. We next put all the pieces together, and compare different pretrained models (XLM-R, RoBERTa and DeBERTa) using zero-shot, translate-train with nucleus sampling, and two variants of translate-test: the naive approach using vanilla NLLB, and our improved approach combining document-level MT adaptation and training data adaptation. We report our results\nin Table 4. We observe that our improved variant of translate-test consistently outperforms the vanilla approach. Interestingly, the improvements are generally bigger for stronger pretrained models: an average of 2.0 points for XLM-R, 2.3 for RoBERTa, and 2.5 for DeBERTa.\nWhen comparing different approaches, we find that zero-shot obtains the worst results in average, while translate-train is only 0.1 points better than translate-test with XLM-R. However, this comparison is unfair to translate-test, as there is no point in using a multilingual pretrained model when one is translating everything into English at test time. When using RoBERTa (a comparable English-only model), translate-test outperforms the best XLMR results by 2.0 points. Using the stronger DeBERTa model further pushes the improvements to 5.3 points.\nThese results evidence that translate-test can be considerably more competitive than suggested by prior work. For instance, the seminal work from Conneau et al. (2020) reported that RoBERTa translate-test lagged behind XLM-R zero-shot and translate-train on XNLI. As illustrated in Figure 1, we show that, with enough care, it can actually obtain the best results of all, outperforming the original numbers from Conneau et al. (2020) by 8.1 points. Our approach is also 3.9 points better than Ponti et al. (2021), the previous state-of-the-art for translate-test in this task. While most work in cross-lingual classification focuses on multilingual models, this shows that competitive or even superior results can also be obtained with English-only models.\nNevertheless, we also find a considerable variance across tasks. The best results are obtained by translate-test in 4 out of 6 benchmarks\u2014in most cases by a large margin\u2014but translate-train\nis slightly better in the other 2. For instance, DeBERTa translate-test is 13.7 points better than XLM-R translate-train on XCOPA, but 1.6 points worse on MARC. This suggests that different tasks have different properties, which make different approaches more or less suitable."
        },
        {
            "heading": "4 Analyzing the variance across tasks",
            "text": "As we have just seen, the optimal cross-lingual learning approach is highly task dependent. In this section, we try to characterize the specific factors that explain this different behavior. To that end, we build on the concept of cross-lingual transfer gap, which is defined as the difference in performance between the source language that we have training data in (typically English) and the target language that we are evaluating on (Hu et al., 2020). While prior work has used this as an absolute metric to compare the cross-lingual generalization capabilities of different multilingual models, we argue that such a transfer gap can be attributed to different sources depending on the approach used, which we try to quantify empirically.\nIn what follows, we identify the specific sources of transfer gap that each approach is sensitive to (\u00a74.1), propose a methodology to estimate their impact using a monolingual dataset (\u00a74.2), and present the estimates that we obtain for various tasks and target languages (\u00a74.3)."
        },
        {
            "heading": "4.1 Sources of cross-lingual transfer gap",
            "text": "We next dissect the specific sources of cross-lingual transfer gap that each approach is sensitive to:\nTranslate-test. All the degradation comes from MT, as no multilingual model is used. We distinguish between (i) the information lost in the translation process (either caused by translation errors or superficial patterns removed by MT), and (ii) the\ndistribution shift between the original data seen during training and the machine translated data seen during evaluation (e.g., stylistic differences like translationese that existing models might struggle generalizing to, even if no information is lost).\nZero-shot. All the degradation comes from the multilingual model, as no MT is used. We distinguish between (i) source-language representation quality6 relative to a monolingual model (English-only models being typically stronger than their multilingual counterparts, but only usable with translate-test), (ii) target-language representation quality relative to the source language (the representations of the target language\u2014 typically less-resourced\u2014being worse than those of the source language), and (iii) representation misalignment between the source and the target language (even when a model has a certain capability in both languages, there can be a performance gap when generalizing from the source to the target language if the languages are not well-aligned).\nTranslate-train. The degradation comes from both MT and the multilingual model. However, while both source and target language representation quality have an impact,7 this approach is not sensitive to representation misalignment, as the model is trained and tested in the same language. Regarding MT, there is no translation and therefore no information lost at test time, so we can consider potential translation errors at training time to be further inducing a distribution shift.\nFinally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those\n6We consider that a pretrained model A has learned higher quality representations than model B if fine-tuning it in our target task results in better downstream performance. In this case, English-only models like RoBERTa are generally superior to their multilingual counterparts like XLM-R when evaluated on downstream English tasks, so we say that their source-language representation quality is higher.\n7Even if the model is trained and tested on the target language, we consider that there is still a degradation from the source-language representation quality under our framework. This is because we are measuring the degradation from using a multilingual model in the target language as opposed to a monolingual model in the source language, which is decomposed into the difference between the monolingual and multilingual model in the source language, plus the difference between the source and the target language in the multilingual model.\ncreated through translation (Artetxe et al., 2020), but can also be a result of the task having naturally different properties in different languages (e.g., question answering datasets in different languages might cover different topics that are relevant to their corresponding speaker communities). Quantifying this factor would require comparing translated and original annotations for each task, which we do not consider in our analysis given the lack of such data."
        },
        {
            "heading": "4.2 Methodology",
            "text": "Let Tsrc be some training data in the source language, Ttgt = MTsrc\u2192tgt(Tsrc) its MTgenerated translation into the target language, and Tbt = MTtgt\u2192src(Ttgt) its MT-generated translation back into the source language. Similarly, let Esrc, Etgt and Ebt be analogously defined evaluation sets. We define acc(T,E) as the accuracy of our multilingual model when training on T and evaluating on E, and accmono(T,E) as the accuracy of its equivalent monolingual model when training on T and evaluating on E. Given this, we estimate the cross-lingual gap from each of the sources discussed in \u00a74.1 as follows:\nMT information lost. We take the difference between training and testing on original data, and training and testing on back-translated data. Given that there is not a distribution shift between train and test induced by MT, the difference in performance can be solely attributed to the information lost. MT is used twice when translating into the target language and then back into the source language, so we make the assumption that each of them introduces a similar error and divide the total gap by two to estimate the impact of a single step:\n\u2206MTinfo = acc(Tsrc, Esrc) \u2212 acc(Tbt, Ebt)\n2\nMT distribution shift. We take the difference between training and testing on back-translated data, and training on original data and evaluating on back-translated data. Similar to \u2206MTinfo, we divide this difference by two, assuming that each MT step introduces the same error:\n\u2206MTdist = acc(Tbt, Ebt) \u2212 acc(Tsrc, Ebt)\n2\nSource representation quality. We take the difference between the monolingual and the multilingual model, training and testing on original data:\n\u2206repsrc = accmono(Tsrc, Esrc) \u2212 acc(Tsrc, Esrc)\nTarget representation quality. We take the difference between training and testing on original data, and training and testing on translated data, and further subtract \u2206MTinfo to account for the error from MT:\n\u2206reptgt = acc(Tsrc, Esrc)\u2212acc(Ttgt, Etgt)\u2212\u2206MTinfo\nRepresentation misalignment. We take the difference between training and testing on translated data, and training on original data and testing on translated data, and further subtract \u2206MTdist to account for the error from the distribution mismatch in the later:\n\u2206repalign = acc(Ttgt, Etgt)\u2212acc(Tsrc, Etgt)\u2212\u2206 MT dist"
        },
        {
            "heading": "4.3 Results and discussion",
            "text": "We experiment on our usual set of 6 tasks, using the exact same training data as in \u00a73 as Tsrc, and the English portion of the respective test sets as Esrc. EXAMs does not have an English test set, so we use the analogous OpenBookQA (Mihaylov et al., 2018) instead. We use XLM-R as our main model and RoBERTa as our monolingual model. We explore a diverse set of target languages: 5 high-resource languages related to\nEnglish (German, Dutch, French, Spanish, Italian), 5 high-resource unrelated languages (Turkish, Vietnamese, Japanese, Finnish, Arabic), and 5 lowresource unrelated languages (Malagasy, Oromo, Javanese, Uyghur, Odia). Figure 2 reports the estimates that we obtain,8 and we next summarize our main findings:\nDegradation from MT. We find that the impact of MT greatly varies across tasks. After manual inspection, we believe that the most relevant factors causing these differences are the format, linguistic complexity and domain of each task. For instance, degradation from MT is most prominent in OpenBookQA, a dataset of multiple choice science questions. We find two reasons for this: (i) the questions and answers often contain domainspecific terminology that is hard to translate, and (ii) the answer candidates are often unnatural or lack the necessary context to be translated in isolation. In contrast, each example in XStoryCloze consists of 5 short and simple sentences that form a story, and we find the impact of MT to be small in this dataset. As expected, MT has a larger abso-\n8Different from our previous experiments, we do not perform multiple finetuning runs with different random seeds due to the high computational cost. However, our results are averaged across languages\u2014each of them requiring separate finetuning runs.\nlute impact in distant and low-resource languages. While degradation from MT comes predominantly from information lost during the translation process, a non-negligible part can be attributed to the distribution shift, which we addressed in \u00a73.1.\nDegradation from the multilingual model. The impact of both the source and target representation quality greatly varies across tasks, and the two are highly correlated. This suggests that there are certain tasks for which representation quality (both source and target) is generally important. By definition, the degradation from the source representation quality is invariant across languages, but the degradation from the target representation quality becomes dramatically higher for unrelated and, more importantly, low-resource languages. In general, we find that tasks requiring commonsense or factual knowledge, like XStoryCloze and OpenBookQA, are heavily sensitive to source and target representation quality, whereas shallower tasks like sentiment analysis (MARC) are barely affected. Finally, we find that, in most tasks, representation misalignment has a relatively small impact compared to representation quality. This suggests that multilingual models learn well-aligned representations that allow cross-lingual transfer at fine-tuning time, but puts into question the extent to which cross-lingual transfer is happening at pre-training time, as the target language representations can be considerably worse than the source language representations.\nExplaining the variance across tasks. Our results can explain why the optimal cross-lingual learning approach is highly task dependent as observed in \u00a73.3. For instance, we find that source representation quality is the most important factor in XStoryCloze, but does not have any impact in MARC. This explains why translate-test\u2014the only approach that is not sensitive to this source of transfer gap\u2014obtains the best results on XStoryCloze, but lags behind other approaches on MARC. Similarly, we find that the impact of the MT distribution shift is highest on XNLI, which is also the tasks at which our improved translate-test approach mitigating this issue brings the largest improvements. We remark that our analysis is only using the English portion of the benchmarks. This shows that it is feasible to characterize the cross-lingual learning behavior of downstream tasks even if no multilingual data is available."
        },
        {
            "heading": "5 Related work",
            "text": "While using MT for cross-lingual classification is a long standing idea (Fortuna and Shawe-Taylor, 2005; Banea et al., 2008; Shi et al., 2010; Duh et al., 2011), there is relatively little work focusing on it in the era of language model pretraining. Ponti et al. (2021) improve translate-test by treating the translations as a latent variable, which allows them to finetune the MT model for the end task through minimum risk training and combine multiple translations at inference. Our approach is simpler, but obtains substantially better results on XNLI and PAWS-X. Artetxe et al. (2020) explore a simpler variant of our training data adaptation (\u00a73.1.2), but their focus is on translation artifacts and our numbers are considerably stronger. Oh et al. (2022) show that translate-train and translate-test are complementary and better results can be obtained by combining them. Isbister et al. (2021) report that translate-test outperforms monolingual models fine-tuned on the target language, but their work is limited to sentiment analysis in Scandinavian languages. While we focus on classification tasks, there is also a considerable body of work exploring MT for cross-lingual sequence labeling, which has the additional challenge of projecting the labels (Jain et al., 2019; Fei et al., 2020; Garc\u00edaFerrero et al., 2022a,b)"
        },
        {
            "heading": "6 Conclusions",
            "text": "Contrary to the conventional wisdom in the area, we have shown that translate-test can outperform both zero-shot and translate-train in most classification tasks. While most research in crosslingual learning pivots around multilingual models, these results evidence that using an Englishonly model through MT is a strong\u2014and often superior\u2014alternative that should not be overlooked. However, there is no one method that is optimal across the board, as not all tasks are equally sensitive to the different sources of cross-lingual transfer gap. Using a new approach to quantify such sources of transfer gap, we find evidence that complex tasks like commonsense reasoning are more sensitive to representation quality, making them more suitable for translate-test, whereas shallower tasks like sentiment analysis work better with multilingual models. In the future, we would like to extend our study to other types of NLP problems like generation and sequence labelling, and study how the different approaches work at scale.\nLimitations\nOur study is limited to classification tasks, an important yet incomplete subset of NLP problems. Translate-train and translate-test can also be applied to other types of problems like generation or sequence labeling, but require additional steps (e.g. projecting labels in the case of sequence labeling). We believe that it would be interesting to conduct similar studies on these other types of problems.\nAt the same time, most multilingual benchmarks\u2014including some used in our study\u2014have been created through translation, which prior work has shown to suffer from annotation artifacts (Artetxe et al., 2020). It should be noted that the artifacts characterized by Artetxe et al. (2020) favor translate-train and harm translate-test, so we believe that our strong results with the latter are not a result of exploiting such artifacts. However, other types of more subtle interactions might be possible (e.g. translating a text that is itself a translation might be easier than translating an original text). As such, we encourage future research to revisit the topic as better multilingual benchmarks become available.\nFinally, our experiments are limited to the traditional pretrain-finetune paradigm with encoderonly models. There is some early evidence that translate-test also outperforms current multilingual autoregressive language models (Lin et al., 2021; Shi et al., 2022), but further research is necessary to draw more definitive conclusions."
        }
    ],
    "title": "Revisiting Machine Translation for Cross-lingual Classification",
    "year": 2023
}