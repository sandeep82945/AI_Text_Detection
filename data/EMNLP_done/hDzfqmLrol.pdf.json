{
    "abstractText": "Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness. In this paper, we introduce DELTASCORE, a novel methodology that uses perturbation techniques for the evaluation of nuanced story aspects. We posit that the extent to which a story excels in a specific aspect (e.g., fluency) correlates with the magnitude of its susceptibility to particular perturbations (e.g., the introduction of typos). Given this, we measure the quality of an aspect by calculating the likelihood difference between preand post-perturbation states using pre-trained language models. We compare DELTASCORE with existing metrics on storytelling datasets from two domains in five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness. DELTASCORE demonstrates strong performance, revealing a surprising finding that one specific perturbation proves highly effective in capturing multiple aspects. Source code is available on our GitHub repository.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhuohan Xie"
        },
        {
            "affiliations": [],
            "name": "Miao Li"
        },
        {
            "affiliations": [],
            "name": "Trevor Cohn"
        },
        {
            "affiliations": [],
            "name": "Jey Han Lau"
        }
    ],
    "id": "SP:84e2ec224cc72fe19ed93c8dc3d867ed07847059",
    "references": [
        {
            "authors": [
                "Chi-Min Chan",
                "Weize Chen",
                "Yusheng Su",
                "Jianxuan Yu",
                "Wei Xue",
                "Shanghang Zhang",
                "Jie Fu",
                "Zhiyuan Liu."
            ],
            "title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "venue": "CoRR, abs/2308.07201.",
            "year": 2023
        },
        {
            "authors": [
                "Hong Chen",
                "Duc Vo",
                "Hiroya Takamura",
                "Yusuke Miyao",
                "Hideki Nakayama."
            ],
            "title": "StoryER: Automatic story evaluation via ranking, rating and reasoning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Cyril Chhun",
                "Pierre Colombo",
                "Fabian M. Suchanek",
                "Chlo\u00e9 Clavel."
            ],
            "title": "Of human criteria and automatic metrics: A benchmark of the evaluation of story generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING",
            "year": 2022
        },
        {
            "authors": [
                "David Cheng-Han Chiang",
                "Hung-yi Lee"
            ],
            "title": "Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "venue": "ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Y. Zhao",
                "Yanping Huang",
                "Andrew M. Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Mingkai Deng",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Compression, transduction, and creation: A unified framework for evaluating natural language generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "George Doddington."
            ],
            "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics",
            "venue": "Proceedings of the Second International Conference on Human Language Technology Research, HLT \u201902, page 138\u2013145, San Francisco,",
            "year": 2002
        },
        {
            "authors": [
                "Yao Dou",
                "Maxwell Forbes",
                "Rik Koncel-Kedziorski",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "CoRR, abs/2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Sarik Ghazarian",
                "Zixi Liu",
                "Akash S M",
                "Ralph Weischedel",
                "Aram Galstyan",
                "Nanyun Peng."
            ],
            "title": "Plot-guided adversarial example construction for evaluating open-domain story generation",
            "venue": "Proceedings of the 2021 Conference of the North Amer-",
            "year": 2021
        },
        {
            "authors": [
                "Jian Guan",
                "Fei Huang",
                "Zhihao Zhao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A knowledge-enhanced pretraining model for commonsense story generation",
            "venue": "Transactions of the Association for Computational Linguistics, 8:93\u2013108.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Minlie Huang."
            ],
            "title": "UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9157\u20139166, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Xiaoxi Mao",
                "Changjie Fan",
                "Zitao Liu",
                "Wenbiao Ding",
                "Minlie Huang."
            ],
            "title": "Long text generation by modeling sentence-level and discourselevel coherence",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Jian Guan",
                "Zhexin Zhang",
                "Zhuoer Feng",
                "Zitao Liu",
                "Wenbiao Ding",
                "Xiaoxi Mao",
                "Changjie Fan",
                "Minlie Huang."
            ],
            "title": "OpenMEVA: A benchmark for evaluating open-ended story generation metrics",
            "venue": "Proceedings of the 59th Annual Meeting of the Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Rujun Han",
                "Hong Chen",
                "Yufei Tian",
                "Nanyun Peng."
            ],
            "title": "Go back in time: Generating flashbacks in stories with event temporal prompts",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "lia Tsvetkov"
            ],
            "title": "On the blind spots of modelbased evaluation metrics for text generation. CoRR, abs/2212.10020",
            "year": 2022
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Nader Akoury",
                "Mohit Iyyer."
            ],
            "title": "The perils of using Mechanical Turk to evaluate open-ended text generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1265\u20131285, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Nishant Raj",
                "Katherine Thai",
                "Yixiao Song",
                "Ankita Gupta",
                "Mohit Iyyer."
            ],
            "title": "DEMETR: Diagnosing evaluation metrics for translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "M.G. Kendall."
            ],
            "title": "A new measure of rank correlation",
            "venue": "Biometrika, 30:81\u201393.",
            "year": 1938
        },
        {
            "authors": [
                "Jey Han Lau",
                "Carlos Armendariz",
                "Shalom Lappin",
                "Matthew Purver",
                "Chang Shu."
            ],
            "title": "How furiously can colorless green ideas sleep? sentence acceptability in context",
            "venue": "Transactions of the Association for Computational Linguistics, 8:296\u2013310.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "venue": "CoRR, abs/2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "John Morris",
                "Eli Lifland",
                "Jin Yong Yoo",
                "Jake Grigsby",
                "Di Jin",
                "Yanjun Qi."
            ],
            "title": "TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Lucy Vanderwende",
                "Wen-tau Yih",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "Story cloze evaluator: Vector space representation evaluation by predicting what happens next",
            "venue": "Proceedings of the 1st Workshop on Evaluating Vector-Space Repre-",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Yiwei Qin",
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "T5score: Discriminative finetuning of generative evaluation metrics",
            "venue": "CoRR, abs/2212.05726.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ananya B. Sai",
                "Tanay Dixit",
                "Dev Yashpal Sheth",
                "Sreyas Mohan",
                "Mitesh M. Khapra."
            ],
            "title": "Perturbation CheckLists for evaluating NLG evaluation metrics",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Ananya B. Sai",
                "Akash Kumar Mohankumar",
                "Mitesh M. Khapra."
            ],
            "title": "A survey of evaluation metrics used for NLG systems",
            "venue": "ACM Comput. Surv., 55(2):26:1\u201326:39.",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Elizabeth-Jane Pavlick",
                "Suzana Ili\u2019c",
                "Daniel Hesslow",
                "Roman Castagn\u2019e",
                "Alexandra Sasha Luccioni",
                "Franccois Yvon",
                "et al Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176b-parameter open-access multilingual language",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Tan",
                "Zichao Yang",
                "Maruan Al-Shedivat",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Progressive generation of long text with pretrained language models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Brian Thompson",
                "Matt Post."
            ],
            "title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90\u2013121, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Zhuohan Xie",
                "Trevor Cohn",
                "Jey Han Lau."
            ],
            "title": "The next chapter: A study of large language models in storytelling",
            "venue": "Proceedings of the 16th International Natural Language Generation Conference, pages 323\u2013351, Prague, Czechia. Association for",
            "year": 2023
        },
        {
            "authors": [
                "Zhuohan Xie",
                "Jey Han Lau",
                "Trevor Cohn."
            ],
            "title": "Exploring story generation with multi-task objectives in variational autoencoders",
            "venue": "Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association, pages 97\u2013106, Online. Aus-",
            "year": 2021
        },
        {
            "authors": [
                "Peng Xu",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Raul Puri",
                "Pascale Fung",
                "Anima Anandkumar",
                "Bryan Catanzaro."
            ],
            "title": "MEGATRON-CNTRL: Controllable story generation with external knowledge using large-scale language models",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Yang",
                "Yuandong Tian",
                "Nanyun Peng",
                "Dan Klein."
            ],
            "title": "Re3: Generating longer stories with recursive reprompting and revision",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4393\u20134479, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, De-",
            "year": 2021
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Myle Ott",
                "Sam Shleifer",
                "Kurt Shuster",
                "Daniel Simig",
                "Punit Singh Koura",
                "Anjali Sridhar",
                "Tianlu Wang",
                "Luke Zettlemoyer."
            ],
            "title": "OPT: open pre-trained transformer language models",
            "venue": "CoRR, abs/2205.01068.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Zhexin Zhang",
                "Jiaxin Wen",
                "Jian Guan",
                "Minlie Huang."
            ],
            "title": "Persona-guided planning for controlling the protagonist\u2019s persona in story generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models. arXiv preprint arXiv:2303.18223",
            "year": 2023
        },
        {
            "authors": [
                "Wei Zhao",
                "Maxime Peyrard",
                "Fei Liu",
                "Yang Gao",
                "Christian M. Meyer",
                "Steffen Eger."
            ],
            "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in",
            "year": 2019
        },
        {
            "authors": [
                "Ming Zhong",
                "Yang Liu",
                "Da Yin",
                "Yuning Mao",
                "Yizhu Jiao",
                "Pengfei Liu",
                "Chenguang Zhu",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Towards a unified multidimensional evaluator for text generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The emergence of large pre-trained language models (PLMs) (Zhao et al., 2023) has empowered story generation models to generate plausible narratives (Xie et al., 2021; Tan et al., 2021; Zhang et al., 2022b; Yang et al., 2022). The most advanced models have achieved the ability to produce stories which are not easily distinguishable from human-authored ones (Karpinska et al., 2021; Dou et al., 2022; Xie et al., 2023). However, the development of automated evaluation metrics in this domain has not progressed at the same pace (Guan et al., 2021b). Human evaluation, though\n\u2217Now at Google DeepMind 1https://github.com/ZhuohanX/DeltaScore\nconsidered the gold standard, is hindered by its time-consuming, costly, and non-reproducible nature (Sai et al., 2023). Consequently, there is a demand for better automatic methods that can evaluate the quality of stories.\nThe prevailing evaluation metrics for story assessment have primarily been adapted from other natural language generation (NLG) tasks, such as BLEU (Papineni et al., 2002) for machine translation, or ROUGE (Lin, 2004) for summarization. Fortunately, recent progress has given rise to the emergence of new metrics explicitly tailored for\nstory evaluation, with a focus on quantifying story coherence (Guan and Huang, 2020; Ghazarian et al., 2021) or capturing human preferences (Chen et al., 2022). Other works have directly utilized the likelihood of a story under a PLM (Vaswani et al., 2017; Han et al., 2022) or its conditional likelihood based on human references or other contextual factors, such as story title (Thompson and Post, 2020; Yuan et al., 2021). Nonetheless, these approaches often yield a singular score that provides an estimate of the overall quality. However, Chhun et al. (2022) argue that the quality of a story is comprised of various fine-grained aspects, such as fluency and adherence to commonsense, suggesting that an overall quality score has limited utility for comprehensive story evaluation.\nIn this paper, we present DELTASCORE, a method that evaluates story quality by measuring the likelihood difference using a PLM between an original story and its perturbed version. The idea is that higher quality stories will exhibit more significant effects from the perturbation compared to lower quality ones. To provide fine-grained assessment of story quality, we experiment with perturbations that target specific aspects. Figure 1 presents two examples to demonstrate the intuition of our approach: 1) When we introduce random typos to modify the two stories shown in Figure 1a, we observe that the story with higher fluency is affected more by the perturbation; 2) When we modify the two stories in Figure 1b by removing relevant words, we observe that the perturbation affects the story that has a closer association with the title to a greater extent. Our empirical analysis demonstrates the superior performance of DELTASCORE compared to existing metrics in evaluating intricate story aspects. Furthermore, our investigation reveals an interesting discovery: one of our simplest perturbation methods, which simply shuffles all the words in the story, is very effective in capturing multiple aspects. This points to a possible interpretation that the pertubation may be functioning as a normalisation factor to modulate the effects of word frequency and text length when estimating sequence likelihood."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Automatic Evaluation Metrics",
            "text": "Existing automatic evaluation metrics can be broadly categorized into three paradigms.\nSimilarity metrics mainly focus on measuring lexical overlap such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and ROUGE (Lin, 2004) or semantic similarity with contextual representations including MoverScore (Zhao et al., 2019) and BERTScore (Zhang et al., 2020) between the machine-generated text and its human reference.\nDiscriminative metrics typically involve training a discriminator model to differentiate between high-quality and low-quality texts, including UNION (Guan and Huang, 2020), MANPLTS (Ghazarian et al., 2021), CTC (Deng et al., 2021), StoryER (Chen et al., 2022), and UNIEVAL (Zhong et al., 2022). Specifically, UNION constructs negative samples of original stories using heuristic rules and trains a discriminator to differentiate them. MANPLTS is an extension of UNION that constructs improved negative samples by manipulating storylines and generating alternate stories based on these manipulated storylines using a story generation model. StoryER builds a classifier to learn human preference by training it to differentiate highly-upvoted stories from lowlyupvoted ones on Reddit. CTC treats the evaluation task as an information alignment task. UNIEVAL frames the evaluation as a question answering task where different questions are asked to assess a particular aspect.\nGenerative metrics usually rely on generative likelihood to determine the quality of the text, including BARTScore (Yuan et al., 2021), T5Score (Qin et al., 2022) and GPTScore (Fu et al., 2023). Specifically, BARTScore evaluates generated text by calculating its conditional likelihood under BART. GPTScore calculates the likelihood of the story under a PLM with additional prefix to target a particular aspect. T5Score benefits from both worlds by employing both generative training with the standard negative log likelihood loss and discriminative training with contrastive loss where human judgments for generation quality are available."
        },
        {
            "heading": "2.2 Natural Text Perturbation",
            "text": "The use of perturbations is a conventional technique to generate negative samples for both discriminative (Guan and Huang, 2020) and generative (Zhong et al., 2022) tasks. Ribeiro et al. (2020) propose CheckList, a suite of perturbation techniques to evaluate the behavioral performance of\nNLP models. Sai et al. (2021) further delve into applying perturbations to assess robustness of NLG evaluation metrics, while Karpinska et al. (2022) specifically focus on machine translation evaluation. He et al. (2022) also develop perturbation tests to identify blind spots of model-based evaluation metrics. Notably, all of these perturbations rely on heuristic rules. In contrast, recent adversarial attacks such as those proposed by Li et al. (2020); Morris et al. (2020) use language models to generate adversarial examples, which can also be considered a form of text perturbation. In our work, we explore perturbation for a different purpose: to evaluate fine-grained story qualities."
        },
        {
            "heading": "3 DELTASCORE",
            "text": "We now describe the idea of our approach. Given a story condition (e.g., a story title) c = c1, ..., cn containing n tokens, a model-generated story s = s1, ..., sm containing m tokens, and a perturbed story s\u2032 = s\u20321, ..., s \u2032 m\u2032 containing m\n\u2032 tokens, DELTASCORE calculates the likelihood difference under a language model:\nDELTASCORE(s) = log p(s|c)\u2212 log p(s\u2032|c) (1)\nwhere p(s|c) represents the likelihood of s conditioned on c under a language model. In our experiments, we investigate several PLMs with varying architectures (\u00a7 3.1) and perturbation techniques that are designed to target specific aspects (\u00a7 3.2)."
        },
        {
            "heading": "3.1 Two Different Likelihood Calculations",
            "text": "We now explain how we compute p(s|c) with encoder-decoder PLMs (e.g., BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) and decoder PLMs (e.g., GPT-3 (Brown et al., 2020)). p(s\u2032|c) is computed in the same way and we omit it for brevity.\nDenoting language model parameters as \u03b8, we compute DELTASCORE as follows for encoderdecoder PLMs:\nlog p(s|c) = 1 m m\u2211 t=1 log p(st|s<t, c, \u03b8) (2)\nwhere t denotes timestep in the sequence, and s<t denotes all tokens before the current timestep. Intuitively, the story condition c is captured by the encoder, and the likelihood of the story s is produced by the decoder.\nIn terms of decoder PLMs, we concatenate c and s to form a sequence x (x1, ..., xn+m = c1, ..., cn, s1, ..., sm) to compute DELTASCORE:\nlog p(s|c) = 1 m n+m\u2211 t=n+1 log p(xt|x<t, \u03b8) (3)\nThis formulation means we feed the full sequence including the story condition c and story s as input to the decoder-only PLM, although when computing the story likelihood, we only consider the conditional probabilities for the s tokens."
        },
        {
            "heading": "3.2 Perturbations on Story Aspects",
            "text": "We follow Xie et al. (2023) to assess five fundamental aspects of story quality: fluency, coherence, relatedness, logicality, and interestingness. To this end, we survey perturbation methods from the literature (Ribeiro et al., 2020; Sai et al., 2021; Guan et al., 2021b; He et al., 2022) and attempt to align them to one of these five aspects. For some aspects, we also propose new perturbation methods. We now describe each aspect and its associated perturbation methods; A summary of these methods and examples is given in Table 1.\nFluency assesses the readability of sentences in the story. Perturbations targeting fluency modify the text at the word or phrase level. We use two perturbation approaches from Ribeiro et al. (2020): 1) Typo, where we randomly transpose a character with an adjacent one in the text, and 2) Subject-verb disagreement (SubjVerbDis), where we modify the verbs in a sentence so that they no longer agree with their subjects.\nCoherence assesses the level of connectivity between sentences in the story. Perturbations targeting coherence modify the text at the sentence level. We use two perturbation approaches from Sai et al. (2021): 1) Jumble, where we randomly shuffle words within the story, and 2) Sentence Reorder (SentReorder), where we randomly shuffle the sentences within the story.\nRelatedness focuses on the extent to which the story is relevant to the given condition (e.g., story title). Perturbations targeting relatedness alter the story to reduce its association with its condition. We propose two new methods: 1) Remove Relevant Words (RmRelWords), where we use ChatGPT2 to identify words related to the given title and\n2https://chat.openai.com\nthen remove them from the story, and 2) Story Replacement (StoryReplace), where we substitute the original story with another story from a different story condition. To select a \u201ccomparable\u201d story, we choose a story with where its likelihood is similar to the original story.3\nLogicality focuses on the extent to which the story complies with commonsense. Perturbations targeting logicality introduce elements into the story that contradict commonsense. We adopt one approach from Guan et al. (2021b): Antonym, where we randomly replace the word with its antonym; and propose a new approach: Commonsense, where we use ChatGPT to modify some story elements to violate commonsense.\nInterestingness measures the degree of predictability in the progression of events within a story, representing a highly subjective aspect. We propose one approach: BlanderNarrative, where we use ChatGPT to modify a story to make the narrative less interesting.\nThe ChatGPT4 instructions for the aforementioned perturbations are detailed in Appendix A. For Typo, Jumbo and Antonym, we can control the degree of perturbation, and this parameter is tuned in \u00a7 5.1.\n3We calculate the likelihood of the original story and a candidate story without considering their story conditions.\n4We use OpenAI API with the model gpt-3.5-turbo.\nDataset Condition Story\nROC [FEMALE] dad took me fishing . we sat in a spot and waited for days ...\nWP tell me a story where the first line and last line ... as i walked into the house , i was assailed by the smell of aging ...\nTable 2: Sampled examples of given story condition and its generated story for each dataset."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Benchmarks",
            "text": "We use the generated stories and human ratings collected by Xie et al. (2023) on two story datasets: ROCStories (ROC; Mostafazadeh et al. (2016); 5- sentence simple stories) and WritingPrompts (WP; Fan et al. (2018); longer fictional stories written by users on Reddit).5 The story condition (c) for ROC is the leading sentence; for WP, it is the short paragraph that describes the idea of the story, which is called \u201cprompt\u201d. We present two example stories from the two datasets in Table 2.\nXie et al. (2023) experiment with 6 story generation models that cover large models with promptbased learning (e.g., GPT-3), smaller fine-tuned models (e.g., BART) and other methods that in-\n5Xie et al. (2023) also collected human judgements for CNN-Dailymail, but we do not use them in our study for two reasons: 1) the stories depict real-world events rather than fictional narratives, and 2) most of the language models we test have been trained on this dataset, and so there are potential circularity issues.\ncorporate planning and commonsense (Xu et al., 2020; Guan et al., 2020, 2021a; Tan et al., 2021). They then conduct human evaluation on five aspects, judged using an ordinal scale from 1 (worst) to 5 (best). Two distinct groups of annotators were recruited, comprising in-house PhD students and crowdworkers. The results obtained from both groups were found to be similar, indicating the robustness and reliability of the annotation process.6\nThe judgment from the first group is used for preliminary exploration of optimal settings, such as assessing the effectiveness perturbation methods and language models (\u00a7 5.1). The judgment of the second group is used for the final comparison of our approach with existing evaluation metrics (\u00a7 5.2)."
        },
        {
            "heading": "4.2 Language Models",
            "text": "We select a set of representative PLMs to compute DELTASCORE. For encoder-decoder PLMs, we use BART and FLAN-T5 (Chung et al., 2022). For decoder PLMs, we use BLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022a), and GPT-3.5.7 We use the largest possible variant whenever possible as we found larger models tend to work better in preliminary experiments. We present a summary of these models in Table 3."
        },
        {
            "heading": "4.3 Compared Evaluation Metrics",
            "text": "To comprehensively compare DELTASCORE with other existing evaluation metrics, we select representative evaluation metrics from each of the three categories mentioned in \u00a7 2.1.\nFor similarity metrics, we run experiments for BLEU, BERTScore and MoverScore. For discriminative metrics, we have UNION, MANPLTS, StoryER, CTC and UNIEVAL. Additionally, we include a zero-shot GPT-3.5 using simple instructions as prompt to gather judgements for the five specific aspects (Chiang and Lee, 2023). This approach is referred to as GPT3.5Eval; detailed instructions can be found in Appendix D.\nSince UNION, MANPLTS and StoryER are all originally designed for story evaluation, we use their released models without fine-tuning for our experiments. For CTC, we use the reference-free\n6For both groups, they gather assessments for 140 stories for ROC and 100 stories for WP, with each story being evaluated by three annotators. Annotations on human-written stories are excluded as they could introduce bias in favor of reference-based metrics. As a result, this leaves us with 120 stories for ROC and 80 stories for WP, respectively.\n7We use text-davinci-003 in our experiments.\nalignment approach, which is also called \u201cconsistency\u201d in the original paper. For UNIEVAL, the question answering models are trained on text summarization and dialogue generation tasks. We modify the questions to adapt UNIEVAL for evaluating different aspects of stories as the authors demonstrate the zero-shot transfer capability. Please refer to Appendix B for our questions. For generative metrics, we select BARTScore and GPTScore. We use the reference-free version of BARTScore (i.e., c \u2192 s), and employ text-davinci-003 from OpenAI as the backbone of GPTScore with specific prompts for different story aspects. Prompts for GPTScore can be found in Appendix C.\nWe summarise all these metrics in Table 4, showing whether they: require additional training or ground truth reference; are originally introduced for story evaluation; and can measure fine-grained story aspects."
        },
        {
            "heading": "5 Results",
            "text": "We evaluate Kendall correlation at the story level, which involves comparing the predicted metric score versus the aggregated human rating for each story on a specific aspect. We use this as our primary metric due to the non-linear relationship between automatic and human metrics, as well as the ordinal scale employed in human judgments (Kendall, 1938). We explore different settings of our approach in \u00a7 5.1 and present a comparison of our best approach with existing evaluation metrics in \u00a7 5.2. Note that we use two different set of judgments, as explained in \u00a7 4.1, to avoid tuning and testing on the same test set."
        },
        {
            "heading": "5.1 Preliminary Exploration",
            "text": "Perturbation Methods We commence by showcasing the comparative performance of various perturbation methods (\u00a7 3.2) in relation to human judgments across the five aspects, as demonstrated in Table 5. For this analysis, we employ LLaMA as the PLM. The notation \u201cw/o perturbation\u201d denotes the calculation of story likelihood directly under LLaMA, without any perturbations applied. Our findings revealed intriguing results. Notably, we observed that perturbations specifically designed to target a particular aspect did not consistently exhibit a higher correlation with human judgments for that aspect. Furthermore, our analysis indicates that measuring interestingness is particularly challenging, as the correlation numbers associated with this aspect are generally lower compared to the other aspects. Finally, our last and perhaps most surprising observation is that a small set of perturbation methods, namely Typo, Jumble, and Antonym, exhibit strong performance in evaluating most aspects.\nPerturbation Degree In the preceding phase, we carried out the most intense perturbation for Jumble and Antonym, in which the perturbation was applied to the entire sentence, and random selection of half of the characters for Typo. In light of their strong performance, we now investigate impact of perturbation degree using the these per-\nturbation methods and present the results over ROC and WP in Figure 2. In the case of Typo, the degree pertains to the percentage of characters that we opt to swap. Concerning Jumble, we shuffle tokens within a certain text span while the span length is controlled by the degree. As for Antonym, we replace the token with its antonym under the specified probability (degree). As before, we use LLaMA as the PLM and focus on evaluating coherence here. Interestingly, Typo appears to be relatively stable and unaffected by the perturbation degree, where else Jumble and Antonym work better with more aggressive perturbation. Based on these results, we set the perturbation degree to 0.4, 0.9, and 0.8 for Typo, Jumble, and Antonym respectively for both ROC and WP.8\nLanguage Models We next present DELTASCORE results using different PLMs in Table 6. We use the top 3 performing methods with the optimal degrees determined in our previous analysis. Encouragingly, across different PLMs and story aspects, we see that DELTASCORE outperforms vanilla likelihood (\u201cw/o perturbation\u201d) in almost all instances, suggesting that measuring story quality using likelihood difference is generally a better approach than using its likelihood directly. Broadly speaking, Jumble is the most consistent perturbation method: in ROC it is the consistently the best performer, while in WP it is either the best or sec-\n8The results in the previous subsection (Table 5) use these perturbation values.\nond best performer, depending on the PLM. This observation aligns with the findings presented in Table 5, providing further confirmation that the Jumble perturbation method demonstrates effectiveness in measuring various story aspects. When examining the correlation magnitudes for different story aspects, it is evident that interestingness consistently exhibits lower values, reaffirming its inherent difficulty in measurement. There are, however, some curious exceptions: in ROC the correlation for fluency and relatedness is particularly low. We do not have a strong hypothesis of these observations, but will note that the language of ROC stories are somewhat formulaic and possibly different to the language of the pre-training data. For relatedness, the story condition in ROC is the first sentence, and it is a rather artificial condition to set\nthe \u201ctopic\u201d for story generation. An unsurprising observation is that larger models tend to exhibit stronger correlations, with GPT3.5 and OPT performing the best among the PLMs. BLOOM and FLAN-T5 fall in the middle range, while BART shows the lowest correlation scores. Upon comparing GPT-3.5 and OPT, we observe a slight advantage for OPT despite its smaller model size and pre-training data. This finding suggests that beyond a certain scale, the benefits of further scaling may become less significant."
        },
        {
            "heading": "5.2 Comparison with Other Metrics",
            "text": "We next compare DELTASCORE with other evaluation metrics in Figure 3. Note that in this comparison, we utilize OPT as the chosen PLM, considering its superior performance, along with the same\nWe bold the best scores for each aspect and highlight instances where DELTASCORE improves over vanilla likelihood (\u201cw/o perturbation\u201d).\ntop-performing perturbation methods. The results of our evaluation are very promising: DELTASCORE consistently outperforms all competitor metrics across all story aspects. Jumble stands out as the most effective perturbation method among the three. The similarity metrics generally has the lowest performance, highlighting the inadequacy of reference-based metrics for story evaluation, which aligns with previous research findings (Guan and Huang, 2020; Xie et al., 2023). Among the discriminative metrics, CTC and UNIEVAL show relatively strong competitiveness, although they still fall behind DELTASCORE. The performance of generative scores is inconsistent. GPTScore shows strong performance in evaluating logicality and interestingness, especially in ROC, where it performs similarly to DELTASCORE. However, its effectiveness is limited in other scenarios. More detailed scores can be found in Table 8."
        },
        {
            "heading": "6 Discussion and Conclusion",
            "text": "Initially, our aim was to investigate various types of perturbations for assessing fine-grained aspects of storytelling. But seeing that performance of each metric does not vary much across different aspects in Figure 3, it suggests these aspects may be somewhat inter-correlated. Also, our findings revealed that one of the simplest perturbation methods, namely Jumble, is exceptionally effective in measuring most aspects. One hypothesis could be that Jumble is functioning as a normalisation factor to modulate word frequency and sentence length effects when estimating sequence quality. This finding aligns with prior study that used sequence probabilities for measuring sentence acceptability (Lau et al., 2020). They found that it is important to normalise the probabilities and introduced various normalization techniques to mitigate the impact of word frequency and sentence length. Jumble can\nbe interpreted as an alternative normalisation technique. Given this insight, it may also mean that DELTASCORE has broader application beyond the evaluation of story quality. For instance, it could be used to score sentences in machine translation and summarization.\nIn conclusion, we introduce DELTASCORE, a novel approach to assess fine-grained story aspects by comparing the likelihood difference between the original story and a perturbed version using a pretrained language model. Surprisingly, we discovered that a small set of perturbation methods excel in measuring the majority of story aspects. Furthermore, our findings demonstrate that DELTASCORE shows stronger correlations with human judgments compared to a range of existing metrics across two different story domains.\nLimitations\nOur study only investigates a limited range of perturbations, and we acknowledge that there may be other forms of perturbations that could work better. The field of evaluation metrics is rapidly evolving, with numerous contemporary evaluation metrics introduced recently, such as G-Eval (Liu et al., 2023) and ChatEval (Chan et al., 2023), which were not incorporated into the comparative evaluation metrics within this study."
        },
        {
            "heading": "Acknowledgements",
            "text": "We extend our thanks to the reviewers for their valuable feedback, which has greatly contributed to the improvement of this work. Zhuohan Xie is supported by Melbourne Research Scholarship, and would like to expresses his sincere appreciation to the program."
        },
        {
            "heading": "A Perturbation Prompts",
            "text": "We use following prompts for perturbations when we apply API with GPT-3.5-turbo.\n\u2022 RelevantWords: Find all words in the given story that is relevant to the given title. Please only print words in the given story, and separate them by \u2018,\u2019. \u201ctitle\u201d: {title}, \u201cstory\u201d: {story}\n\u2022 Commonsense: Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g., by changing a few words. \u201cstory\u201d: {story}\n\u2022 BlanderNarrative: Revise the following story to make it less interesting (e.g., expected ending, no plot twist). The revision should be minimal. \u201cstory\u201d: {story}\nWe present several examples of perturbed stories by GPT-3.5-turbo in Table 7. We observe that BlanderNarrative does not significantly alter the original story. This observation is in line with previous findings that BlanderNarrative does not effectively impact interestingness in Table 5. We speculate that this outcome may be attributed to the inherent simplicity of most stories, which limits the extent to which GPT-3.5-turbo can modify them to reduce their level of interest."
        },
        {
            "heading": "B UNIEVAL Questions",
            "text": "We ask the following questions for each aspect. Note that we try to use the narrative/vocabulary as close to the original questions Zhong et al. (2022) use in their efforts as possible.\n\u2022 Fluency: Is this a fluent utterance?\n\u2022 Coherence: Is this a coherent utterance?\n\u2022 Relatedness: Is this claim consistent with the document?\n\u2022 Logicality: Is this utterance consistent with the commonsense?\n\u2022 Interestingness: Is this an interesting utterance?"
        },
        {
            "heading": "C GPTScore Prompts",
            "text": "We use the following prompts for each aspect. Note that we try to use the narrative/vocabulary as close to the original prompts (Fu et al., 2023) use in their efforts as possible.\n\u2022 Fluency: Generate a fluent story for the given title: {title}, and story: {story}\n\u2022 Coherence: Generate a coherent story for the given title: {title}, and story: {story}\n\u2022 Relatedness: Generate a story related to the given title: {title}, and story: {story}\n\u2022 Logicality: Generate a story that adhere to commonsense for the given title: {title}, and story: {story}\n\u2022 Interestingness: Generate an interesting story for the given title: {title}, and story: {story}"
        },
        {
            "heading": "D GPT3.5Eval Instructions",
            "text": "We use the following instructions for each aspect, following Chiang and Lee (2023).\nFluency The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. Story fragment: {{Story}} How fluent is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)\nCoherence The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. Story fragment: {{Story}} How coherent is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)\nRelatedness The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. Story title: {{Title}} Story fragment: {{Story}} How related is the text of the story fragment to the title? (on a scale of 1-5, with 1 being the lowest)\nLogicality The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. Story fragment: {{Story}} How logically correct is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)\nInterestingness The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. Story fragment: {{Story}} How interesting is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)"
        }
    ],
    "title": "DELTASCORE: Fine-Grained Story Evaluation with Perturbations",
    "year": 2023
}