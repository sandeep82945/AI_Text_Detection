{
    "abstractText": "The evaluation of abstractive summarization models typically uses test data that is identically distributed as training data. In real-world practice, documents to be summarized may contain input noise caused by text extraction artifacts or data pipeline bugs. The robustness of model performance under distribution shift caused by such noise is relatively understudied. We present a large empirical study quantifying the sometimes severe loss in performance \u2013 up to 12 ROUGE-1 points \u2013 from different types of input noise for a range of datasets and model sizes. We then propose a light-weight method for detecting and removing such noise in the input during model inference without requiring any extra training, auxiliary models, or even prior knowledge of the type of noise. Our proposed approach effectively mitigates the loss in performance, recovering a large fraction of the performance drop, sometimes as large as 11 ROUGE-1 points.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kundan Krishna"
        },
        {
            "affiliations": [],
            "name": "Yao Zhao"
        },
        {
            "affiliations": [],
            "name": "Jie Ren"
        },
        {
            "affiliations": [],
            "name": "Balaji Lakshminarayanan"
        },
        {
            "affiliations": [],
            "name": "Jiaming Luo"
        },
        {
            "affiliations": [],
            "name": "Mohammad Saleh"
        },
        {
            "affiliations": [],
            "name": "Peter J Liu"
        }
    ],
    "id": "SP:71c6d48e009cb45f625c83c7353bf1c13bcbabbe",
    "references": [
        {
            "authors": [
                "Xiuying Chen",
                "Guodong Long",
                "Chongyang Tao",
                "Mingzhe Li",
                "Xin Gao",
                "Chengqi Zhang",
                "Xiangliang Zhang."
            ],
            "title": "Improving the robustness of summarization systems with dual augmentation",
            "venue": "Proceedings of the 61st Annual Meeting of the As-",
            "year": 2023
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379,",
            "year": 2019
        },
        {
            "authors": [
                "Hamel Husain",
                "Ho-Hsiang Wu",
                "Tiferet Gazit",
                "Miltiadis Allamanis",
                "Marc Brockschmidt."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "arXiv preprint arXiv:1909.09436.",
            "year": 2019
        },
        {
            "authors": [
                "Hongyan Jing",
                "Daniel Lopresti",
                "Chilin Shih."
            ],
            "title": "Summarization of noisy documents: a pilot study",
            "venue": "Proceedings of the HLT-NAACL 03 Text Summarization Workshop, pages 25\u201332.",
            "year": 2003
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Omer Levy",
                "Jacob Eisenstein",
                "Marjan Ghazvininejad."
            ],
            "title": "Training on synthetic noise improves robustness to natural noise in machine translation",
            "venue": "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),",
            "year": 2019
        },
        {
            "authors": [
                "Misha Khalman",
                "Yao Zhao",
                "Mohammad Saleh."
            ],
            "title": "Forumsum: A multi-speaker conversation summarization dataset",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4592\u20134599.",
            "year": 2021
        },
        {
            "authors": [
                "Byeongchang Kim",
                "Hyunwoo Kim",
                "Gunhee Kim"
            ],
            "title": "Abstractive summarization of reddit posts with multi-level memory networks",
            "year": 2018
        },
        {
            "authors": [
                "Byeongchang Kim",
                "Hyunwoo Kim",
                "Gunhee Kim."
            ],
            "title": "Abstractive summarization of Reddit posts with multi-level memory networks",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay Cohen",
                "Maria Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topicaware convolutional neural networks for extreme summarization",
            "venue": "2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Xing Niu",
                "Prashant Mathur",
                "Georgiana Dinu",
                "Yaser Al-Onaizan."
            ],
            "title": "Evaluating robustness to input perturbations for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8538\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Denis Peskov",
                "Joe Barrow",
                "Pedro Rodriguez",
                "Graham Neubig",
                "Jordan Boyd-Graber."
            ],
            "title": "Mitigating noisy inputs for question answering",
            "venue": "arXiv preprint arXiv:1908.02914.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "Journal of Machine Learning Re-",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J. Liu."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "International Conference on Learning Repre-",
            "year": 2023
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Vaibhav Vaibhav",
                "Sumeet Singh",
                "Craig Stewart",
                "Graham Neubig."
            ],
            "title": "Improving robustness of machine translation with synthetic noise",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "The evaluation of abstractive summarization models typically uses test data that is identically distributed as training data. In real-world practice, documents to be summarized may contain input noise caused by text extraction artifacts or data pipeline bugs. The robustness of model performance under distribution shift caused by such noise is relatively understudied. We present a large empirical study quantifying the sometimes severe loss in performance \u2013 up to 12 ROUGE-1 points \u2013 from different types of input noise for a range of datasets and model sizes. We then propose a light-weight method for detecting and removing such noise in the input during model inference without requiring any extra training, auxiliary models, or even prior knowledge of the type of noise. Our proposed approach effectively mitigates the loss in performance, recovering a large fraction of the performance drop, sometimes as large as 11 ROUGE-1 points."
        },
        {
            "heading": "1 Introduction",
            "text": "Despite rapid progress in abstractive summarization in recent years (Lewis et al., 2020; Raffel et al., 2020b; Zhang et al., 2020), virtually all works have tested models using test data which is identically distributed as the training data, and little attention has gone into studying their robustness to input distribution shift caused by input noise. Data from different domains which have been addressed in summarization research, may contain noise of different types. For example, when summarizing a news article on a web page, there can be embedded elements such as ads or tweets which may be included as part of the article due to erroneous text extraction. A system summarizing chatroom conversations might encounter artifacts such as URLs, or sometimes even code shared between participants. If the text to be summarized is acquired by scanning a document, noise can be introduced in\nthe form of OCR errors (Jing et al., 2003). However, the impact of different kinds of noise on modern abstractive summarization systems, and ways to accurately detect and remove that noise, remain largely unknown.\nIn this work, we study how noise in the input affects the output generated by summarization models, and propose a method to detect and remove it. We synthetically inject 4 types of noise to 4 abstractive summarization datasets with diverse styles (Narayan et al., 2018; Kim et al., 2019; Gliwa et al., 2019; See et al., 2017), and quantify the drop in aggregate metrics for the output summaries (Section 3). We also study how the quality of generated summaries varies with factors such as the amount of noise and size of the models. For our experiments, we use PEGASUS (Zhang et al., 2020) models \u2014 transformer-based pre-trained models which deliver competitive performance across abstractive summarization benchmarks.\nWe present a method to detect and remove noisy spans in the input, which works without prior knowledge of the noise type or access to its samples, yet can recover a large fraction of the drop in output quality resulting from noise addition (Section 4). Our approach for detecting noisy spans is based on variations of the Relative Mahalanobis Distance OOD Score proposed by Ren et al. (2023), which uses the embeddings computed by the summarization model\u2019s encoder. Our approach does not require any additional training or use of external models, hence it is relatively efficient. Figure 1 shows our method\u2019s impact on a sample noisy document.\nFinally, we investigate how different parts of the model architecture cause the drop in output quality upon adding noise to the input (Section 5). We attribute the performance drop to two phenomena: (i) corruption of the representations of non-noisy input tokens computed by the encoder due to contextualization with neighboring noise; and (ii) dis-\ntraction of the decoder such that it assigns non-zero attention to the representations of noisy input tokens. We perform an ablation where we remove the encoder embeddings of the noisy tokens before running the decoder, hence eliminating the effect of decoder distraction. We find that in a majority of cases this leads to partial recovery in output quality suggesting that generally both factors are responsible to some extent for the poor output summaries.\nWe make the following contributions:\n\u2022 We quantify the impact of various kinds of noise on pretrained Transformer-based summarization models, demonstrating drops in output quality up to 12 ROUGE-1 points.\n\u2022 We show that this noise can be detected using adaptations of an out-of-distribution detection technique, without ever being exposed to it in advance. Our approach can recover much of the performance drop (sometimes as large as 11 ROUGE-1 points), improving robustness and safety for real-world model deployment.\n\u2022 We examine how different parts of the model\u2019s computation are affected by the introduction of input noise, leading to generation of inferior summaries."
        },
        {
            "heading": "2 Related Work",
            "text": "Research on the behavior of summarization models on noisy inputs is quite sparse. Jing et al. (2003) investigated how extractive summarization models are impacted by OCR errors in scanned documents. More recently, Meechan-Maddon (2019) studied\nthe effect of noise from ASR errors on CNN based summarization models. In contrast, we experiment with pre-trained Transformer models which are now preferred in popular use due to their superior performance (Lewis et al., 2020; Zhang et al., 2020; Raffel et al., 2020b), and address a wide variety of noise types and summarization datasets. Contemporary to our work, Chen et al. (2023) have studied the impact of misspellings in input to summarization models, whereas our work instead focuses on additive input noise and its explicit removal.\nThe effect of noisy inputs has also been studied for NLP tasks other than summarization, such as machine translation (Niu et al., 2020) and question answering (Peskov et al., 2019). Multiple works across machine translation (Karpukhin et al., 2019; Vaibhav et al., 2019), question answering (Peskov et al., 2019) and summarization (Jing et al., 2003) have used synthetic noise to create noisy inputs. Similar to these works, we also create synthetic noisy inputs due to lack of a dataset with naturally occurring labeled noise. One distinguishing aspect of our work is that our noise detection/removal method works without exposing the model to the noise during training, which is closer to practical scenarios where unknown types of noise can be encountered after a model is deployed."
        },
        {
            "heading": "3 Impact of noise addition",
            "text": "We inject noisy text spans in between sentences of the clean articles. The insert position of each noisy text span is sampled independently and uniformly at random (see Figure 7 in Appendix for an exam-\nple). Overall, we consider the following choices of a noisy text span:\n\u2022 Code - a random line of code from a corpus of Python programs (Husain et al., 2019). Code may be shared in professional chatrooms.\n\u2022 Emoji - randomly sampled emojis taken from the version 15 release on unicode.org. Emojis can be found in conversations and social media posts.\n\u2022 URL - a random URL from the first 1% of validation set of the the Colossal Common Crawl Corpus(C4) (Raffel et al., 2020b). URLs can be referenced in news articles or mentioned in chatrooms.\n\u2022 Randomsent - a random sentence from the first 1% of validation set of the C4 corpus.\nWe experiment with different amounts of noise added to the input which is treated as a hyperparameter. We measure the amount of noise in terms of the number of noisy tokens added to the input divided by the total number of tokens in the input after noise addition. We experiment with 4 different datasets \u2014 XSUM (Narayan et al., 2018), CNN/DailyMail (See et al., 2017), SAMSum (Gliwa et al., 2019) and RedditTIFUlong (Kim et al., 2018). Our datasets span a variety of domains, where the first two datasets deal with summarizing news articles, and the remaining two consider summarizing conversations and social media posts respectively. For all experiments with each summarization dataset, we use PEGASUS models (Zhang et al., 2020) finetuned on that dataset. We evaluate the performance of models using ROUGE scores (Lin, 2004) of the corresponding summaries generated by the them. Effect of noise amount: We compare four different levels of noise, 5%, 10%, 25%, and 50% (50% means the amount of noise tokens is equal to the amount of the clean tokens.). As shown in Figure 2, we see a near monotonic decrease in output quality as more noise is added to the data. In Figure 2a, we group it by datasets while averaging across model sizes and noise types. This reveals that some datasets are more robust to noise than others (e.g. CNN/DailyMail is most robust), and the relative trends in performance drops remain similar across different noise amounts. In Figure 2b, we group the performance drops by noise types while averaging across datasets and model sizes. We see\na clear gap between the drops for Code and Randomsent vs Emoji and URL, with the gap widening as the noise amount is increased.\nEffect of noise type: In general, we see the models are more robust to URLs and emojis, and less robust to Randomsent and Code noise types as demonstrated by performance drops (averaged across model sizes) shown in Figure 2c. We suspect that some of the this could be due to the presence of URLs and emojis in the training dataset itself, due to which the model may have learned to be robust to those noise types. In addition, from Figure 2c we see that models trained on different datasets have varying sensitivity to different kinds of noises. For example, SAMSum is notoriously susceptible to Randomsent noise, leading to a drop of about 10 Rouge-1 points averaged across model sizes (Table 6 in Appendix), whereas for CNN/DailyMail Code is the most harmful type of noise.\nEffect of model size: We compare PEGASUS models of 3 different sizes (number of parameters) \u2014 Small (50M), Base (200M), and Large (500M). As shown by performance drops (averaged over noise types) in Figure 2d, one might expect larger models to be less susceptible to noise, but it does not seem to be the case in general and simply scaling up models may not solve robustness. In some cases, large models can still suffer loss of over 10 ROUGE-1 points with addition of noise (see Table 6 in Appendix).\nA qualitative analysis of the summaries generated for noisy inputs revealed that there exist some frequent bad summaries which are generated by the models for many noisy inputs. This is observed for models fine-tuned on XSUM and RedditTIFUlong datasets, while for the other two datasets we did not observe such a pattern. We show five of the most frequently generated summaries for XSUM and RedditTIFU-long in Table 1. We see that the generated summary (for noisy inputs) is often just punctuation marks such as a period or a semicolon. Notably, for XSUM dataset, some of the frequently generated bad summaries were also present as ground truth summaries in the train set. For example, \u201cAll images are copyrighted.\u201d was the ground truth summary for 39 articles in the train set. This suggests that upon encountering input noise, the model can fall back to behaving like an unconditioned language model and generating high frequency sequences from the train set."
        },
        {
            "heading": "4 Noise detection and quality recovery",
            "text": ""
        },
        {
            "heading": "4.1 Noise detection",
            "text": "Ren et al. (2023) studied various methods for detecting OOD inputs for conditional language generation tasks. They showed that the proposed embedding-based OOD detection method Relative Mahalanobis distance (RMD) worked well. Specifically, given an input sequence x = x1 . . . xt, the method obtains the input embedding z = 1t \u03a3ihi by averaging the encoder\u2019s final-layer hidden state vectors hi corresponding to the input sequence token xi. The OOD score is defined as the difference between two Mahalanobis distances (MD),\nS(x) := RMD(z) := MDin(z)\u2212MD0(z), (1)\nwhere MDin(z) = (z \u2212 \u00b5)T\u03a3\u22121(z \u2212 \u00b5) measures the distance from z to the fitted in-domain Gaussian distribution N (\u00b5,\u03a3), and MD0(z) = (z \u2212 \u00b50)T\u03a3\u221210 (z \u2212 \u00b50) measures the distance to the fitted background Gaussian N (\u00b50,\u03a30). The in-domain Gaussian distribution is fitted using the in-domain training data, and the background distribution is fitted using the same number of examples\nfrom C4 (Raffel et al., 2020a) which represents a broad set of domains. In our experiments we use 10, 000 examples to fit each distribution. The RMD score is regarded as a background contrastive score that indicates how close the input sequence is to the in-domain compared to the background domains. A negative score suggests relatively in-domain, while a positive score suggests OOD.\nInstead of computing a single OOD score for the entire input document sequence as in (Ren et al., 2023), in this work, we focus on detecting smaller sub-parts of OOD noise within the input document sequence. We propose three variants: Leaveout-Sentence (LO-Sent) In this case, we compute the OOD scores of the input with and without a sentence in it. The negative of the change in the OOD score after removing the sentence denotes the OOD score of that sentence. Intuitively, if removing the sentence decreases the overall OOD score, that sentence is assigned a positive OOD score and vice-versa.\nSLO-Sent(xi:j) = S(x1:t)\u2212 S(x1:(i\u22121);(j+1):t) (2)\nLeaveout-Token (LO-Tok) This is very similar to the previous method LO-Sent except that instead of removing a sentence, we remove a token at a time and hence get OOD scores for each token,\nSLO-Tok(xi) = S(x1:t)\u2212 S(x1:(i\u22121);(i+1):t). (3)\nSentencewise (Sent) Instead of computing the score based on embeddings averaged over the tokens in the whole input document sequence (consisting of multiple sentences), we fit Gaussian distributions at the sentence level by averaging the token embeddings in a sentence zi:j =\n1 j\u2212i+1 \u2211j k=i hk. We use the sentence embeddings from in-domain data and C4 data to fit the two Gaussian distributions, N (\u00b5sent,\u03a3sent) and N (\u00b5sent0 ,\u03a3sent0 ).\nSsent(xi:j) = MDsentin (zi:j)\u2212MDsent0 (zi:j) (4)\nwhere MDsentin and MD sent 0 are MDs to N (\u00b5sent,\u03a3sent) and N (\u00b5sent0 ,\u03a3sent0 ) respectively.\nGPT-2 likelihood We also experiment with a simple language model baseline to generate the noisiness scores based on average negative loglikelihood (NLL) of tokens in a sentence, as given by the pretrained GPT-2 model. Intuitively, a higher value of NLL signifies that a token is unlikely to occur given the past context, which should hold true in case of noisy tokens with clean past context.\nSGPT2(xi:j) = \u2212 1\nj \u2212 i + 1 j\u2211 k=i log pG(xk|x<k)\n(5)\nwhere pG(xk|x<k) is the probability assigned by the GPT-2 model to token xk given previous tokens.\nTo calculate performance of models at noise detection, we compare the assigned OOD score for\neach token with its ground truth label and we compute the ROC AUC scores for comparison. For the two sentence level scores, SLO-Sent(xi:j) and Ssent(xi:j), we assign each token\u2019s OOD score to be the sentence level OOD score for the sentence which contains that token. We compute evaluation metrics in two ways: (i) per-example basis where the AUC score is computed for each example and then they are all averaged across the dataset. (ii) overall basis where all the predictions across the entire dataset are pooled together before computing a single AUC score. We show the scores averaged across the 4 datasets in (Table 2). In general, the LO-Tok method performs the worst of the three OOD-based methods, while Sent and LO-Sent perform comparably. Comparing the GPT-2 baseline with LO-Tok, GPT-2 performs clearly better for Randomsent, comparably for Code, and clearly worse for Emoji and URL noise types. However, GPT-2 lags behind LO-Sent and Sent for all noise types. Between Sent and LO-Sent, Sent performs better for Code and Randomsent and LO-Sent performs better for Emoji and URL noise types. For its simplicity, we use the Sent method for OOD detection in rest of the paper."
        },
        {
            "heading": "4.2 Quality recovery after noise filtering",
            "text": "To remove noise from the input, we simply remove all sentences that have an OOD score greater than a threshold, and then evaluate how much output quality gets recovered after this. We set the threshold of OOD score for filtering to be the 99 percentile value of the OOD scores computed for sentences in the clean version of the dataset (without any noise). The chosen percentile is set to be this high to minimize false positives which can lead to removal of useful non-noisy information from the input. Since the threshold is computed using only the clean dataset and the model trained on that, we do not need any prior information about the noise\n(similar to OOD score computation).\nWe show the performance of noise filtering for different noise types, model sizes and datasets in Table 3. For succinctness, we show the geometric mean of the ROUGE-1,2 and L variants, and point the reader to the Appendix (Table 6) for detailed results with individual variants of ROUGE. After noise filtering, we can recover a large part of the drop in ROUGE scores that occurred due to the added noise. In cases of large drop such as the Randomsent noise type with XSUM and SAMSum datasets, we can recover 4-6 and 6-7 points respectively depending on the model size (Table 3).\nWe also present aggregate trends of recovery of output quality using our filtering approach in Figure 2c and 2d. We can see that we recover over half of the drop in the performance on 9 out of 16 combinations of datasets and noise types (Figure 2c), with the best performance observed on XSUM and SAMSum datasets and the worst on CNN/DailyMail. The method also succeeds in recovering performance across all 3 model sizes (Figure 2d).\nWe experimented with various thresholding strategies such as setting thresholds to be constant irrespective of the dataset or model (e.g. 0), or to be equal to a different percentile value (other than 99%) of the OOD scores produced by the model used on clean data. We also tried choosing the optimal threshold based on F1-score of noise detection on a hold-out validation set (assuming a scenario where we have access to labeled noisy samples). We tried 6 thresholding techniques in total, compared in Figure 3a. Setting a constant threshold of 0 provides gains in some cases but in other cases makes the model outputs worse, due to filtering out useful non-noisy content. To prevent this, one can use a very high threshold such a 500 which practically eliminates cases of further drop in performance (Figure 3a), but the performance gains produced in that case are small because less\nnoise is filtered. The best approach turns out to be setting it be the 99 percentile of the clean data OOD scores, which produces different thresholds for different models, and leads to the highest average gain in output quality among the strategies tried, with minimal cases of further degradation. Surprisingly, optimizing the threshold based on F1score of noise detection on a validation set also reduces the output quality in many cases, suggesting that F1-score may not be the best predictor for the quality of summary produced after filtering.\nWe conduct noise filtering for each of our experimental setups (all datasets, noise types and amounts, model sizes) with three thresholds \u2014 0, 200 and 500 and compare the resulting change in summary quality with the precision and recall of the noise detection in Figure 3b. We find that a precision lower than around 0.7 usually leads to a drop in summary quality, even if the recall is nearly perfect suggesting that almost all noise has been removed. This suggests that precision is more important than recall for improving summary quality."
        },
        {
            "heading": "5 Investigating causes of loss in",
            "text": "performance\nThere are two distinct mechanisms which can lead to worsening of generated summaries upon addition of input noise. The first is the corruption of the encoder\u2019s representation of useful clean tokens. The encoder transformer uses self-attention over input tokens to generate their contextualized representations. In cases where noise is present in the input, self-attention can distort the encoder representations of clean tokens. The second mechanism is the distraction of the decoder such that it assigns non-zero attention to the noisy tokens\u2019 embeddings and this impairs its computation. Even if there is no corruption in the embeddings of clean tokens, the embeddings of noisy tokens can receive non-zero cross-attention from the decoder and influence its generation. If neither of these two phenomenon oc-\ncur, the generated summary on the noisy and clean variants of any input would be the same. In this\nsection we investigate the contribution of these two factors in the degradation of output quality."
        },
        {
            "heading": "5.1 Are the clean token embeddings corrupted by the presence of noise?",
            "text": "We observe that the OOD scores of the clean tokens increase after addition of noise. In Figure 4, we shown an example of this for the XSUM dataset after adding Code noise, where the OOD scores are computed using the Sent method. This suggests that the distribution of clean tokens\u2019 embeddings moves farther from the in-domain distribution (learnt from clean in-domain data) relative to the background distribution (learnt from C4 corpus), after adding noise. We observed this for different datasets and noise types, although the extent of the increase in OOD scores varies across them."
        },
        {
            "heading": "5.2 How much performance can be recovered by preventing distraction of the decoder?",
            "text": "We design an ablation experiment to measure how the performance drop would change if there is no distraction of the decoder by embeddings of noisy tokens. Any drop in output quality in such as setup is attributable only to the corruption of the clean tokens\u2019 encoder representations. We remove the embeddings of the (ground truth) noisy tokens after passing the noisy input through the encoder of the PEGASUS model, and then use the decoder to generate the summary using only the remaining embeddings (see Figure 6 in Appendix for detailed workflow). Since the removal is done after passing the whole input through the self-attention layers of the encoder, the clean tokens\u2019 embeddings are already distorted, and the decoder has to generate the summary using these distorted embeddings. The only difference from the usual scenario is that the decoder does not have to include the noisy tokens\u2019 embeddings in the computation. We find that this\nmostly leads to an increase in output quality compared to when the noisy token embeddings are not removed (Figure 5). The biggest improvements come for XSUM and SAMSum datasets, whereas for CNN/DailyMail dataset no improvement is seen for any of the 4 noise types. Surprisingly, for the RedditTIFU-long dataset with the URL and Randomsent noise types, removing the noisy tokens\u2019 embeddings decreases the ROUGE scores further, suggesting that retaining those embeddings is useful for the decoder.\nThe above ablation study highlights the necessity of running the encoder twice \u2014 once for computing OOD scores to detect noise, and then again to compute the encoder representations of the input after removing noisy tokens. While one can save computation time by reusing the encoder embeddings of the clean tokens computed during OOD scoring to feed them to the decoder for generation, results from the ablation suggest that this would give sub-optimal performance recovery (Figure 5)."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this work, we quantified the impact that noisy inputs can have on the output quality of summarization models, for a variety of datasets and noise types. We then proposed a method to detect and remove noise from the input without using any extra models, training, or prior information about noise types, and demostrated its efficacy. One direction for future work is to investigate what makes certain models more susceptible to specific noise types. Another interesting direction would be to carry out experiments for noise filtering with realworld noisy data rather than using synthetically generated noisy examples."
        },
        {
            "heading": "7 Limitations",
            "text": "While we have used 4 different types of noise in our experiments, there can be more types of noise that can be encountered in real world. While a positive aspect of our noise filtering approach is that it can be applied for any unforeseen type of noise, evaluating its performance against all types of noise is infeasible. Due to the heavy compute requirements, we experimented with only one type of pretrained summarization model (PEGASUS), and it is yet to be seen how the results generalize with other models such as T5 (Raffel et al., 2020b) and BART (Lewis et al., 2020). Since our noise detection approach is not perfect, it carries the risk of removing useful information instead of noise. However, our experiments show that while false positives occur, the filtering almost always does more good than harm when applied on noisy documents (Figure 2c). Additionally, the user has an option to minimize the risk of false positives by increasing the threshold of OOD score used for filtering."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Selection of shorter inputs to avoid truncation\nIn our experiments, we exclude those datapoints from the datasets which are longer than a certain threshold. This is done to avoid any truncation of the input (including inputs with added noise) when feeding them into the model. Since adding noise to the input increases its length, it may happen that some clean tokens might be pushed beyond the maximum allowed input length and hence removed when the input is truncated. In such a scenario, removing noisy tokens before feeding the sequence into the model would also cause such clean tokens to be fed into the model again because they can now be accommodated within the input length limit. When measuring the benefit of noise filtering, the benefit from removal of noisy tokens would then be confounded with the benefit from such \u201cresurrection\u201d of clean tokens. To avoid this we only retain those inputs in our datasets where the input length would be within limit even after addition of noise. Since the maximum noise amount we use in our experiments is 0.5, we only retain datapoints which have no more than half of the maximum allowed tokens to input into the model. (Table 4).\nA.2 Compliance with licenses Among the artifacts used in this work, the PEGASUS model and the CNN/DailyMail dataset are distributed under Apache License 2.0, the XSUM and RedditTIFU-long datasets are distributed under the MIT License, and the SAMSum dataset is distributed under CC BY-NC-ND 4.0 License. We use these resources for non-commercial research purposes with proper attribution, which is allowed by all the above licenses.\nA.3 Implementation details We used PEGASUS models (Zhang et al., 2020) of three different sizes \u2014 small, base and large,\nconsisting of 50M, 200M and 500M parameters respectively. The experiments. We used TPUs for our experiments. The bulk of the compute expenditure was spent on running inference on different models for generating summaries on various noisy and noise-filtered variants of the datasets. The runtime of each experiment varies with different factors such as model size and dataset size, with the overall estimate for the total compute used at about 700 TPU hours. For model training and summary generation on the XSUM, CNN/DailyMail and RedditTIFU-long datasets, we used the same hyperparameters as used in the original PEGASUS paper (Zhang et al., 2020) and for SAMSum dataset we use the hyperparameters given in Khalman et al. (2021). We used bytefallback during tokenization to enable proper representation of all unicode characters, instead of using UNK tokens. We experimented with a variety of hyperparmeters pertaining to noise addition and filtering, summarized in Table 5. Due to the heavy compute requirements, all our experiments are single run and we did not try multiple seeds for the random noise addition. We used the NLTK1 library for sentence tokenization and used the rouge_score2 package from Google Research to compute the ROUGE scores of summaries. The default hyperparameters were used in the ROUGE calculation.\n1https://www.nltk.org/ 2https://github.com/google-research/\ngoogle-research/tree/master/rouge"
        }
    ],
    "title": "Improving the Robustness of Summarization Models by Detecting and Removing Input Noise",
    "year": 2023
}