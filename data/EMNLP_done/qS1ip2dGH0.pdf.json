{
    "abstractText": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between current NLP research and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as \u201cdesign\u201d and \u201cplanning\u201d are prevalent in user interactions but are largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges they pose, and provide insights toward a roadmap to make LLMs better aligned with user needs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Siru Ouyang"
        },
        {
            "affiliations": [],
            "name": "Shuohang Wang"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Ming Zhong"
        },
        {
            "affiliations": [],
            "name": "Yizhu Jiao"
        },
        {
            "affiliations": [],
            "name": "Dan Iter"
        },
        {
            "affiliations": [],
            "name": "Reid Pryzant"
        },
        {
            "affiliations": [],
            "name": "Chenguang Zhu"
        },
        {
            "affiliations": [],
            "name": "Heng Ji"
        },
        {
            "affiliations": [],
            "name": "Jiawei Han"
        }
    ],
    "id": "SP:71e32ab10391c1e97ce38948b11f25ea4817b357",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
            "year": 2015
        },
        {
            "authors": [
                "Robert H Bonczek",
                "Clyde W Holsapple",
                "Andrew B Whinston."
            ],
            "title": "Computer-based support of organizational decision making",
            "venue": "Decision Sciences, 10(2):268\u2013291.",
            "year": 1979
        },
        {
            "authors": [
                "Francesco Borrelli",
                "Dharmashankar Subramanian",
                "Arvind U Raghunathan",
                "Lorenz T Biegler."
            ],
            "title": "Milp and nlp techniques for centralized trajectory planning of multiple unmanned air vehicles",
            "venue": "2006 American Control Conference, pages 6\u2013pp. IEEE.",
            "year": 2006
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Erik Cambria",
                "Bj\u00f6rn Schuller",
                "Yunqing Xia",
                "Catherine Havasi."
            ],
            "title": "New avenues in opinion mining and sentiment analysis",
            "venue": "IEEE Intelligent systems, 28(2):15\u201321.",
            "year": 2013
        },
        {
            "authors": [
                "Kathleen Carley."
            ],
            "title": "Extracting culture through textual analysis",
            "venue": "Poetics, 22(4):291\u2013312.",
            "year": 1994
        },
        {
            "authors": [
                "Jiuhai Chen",
                "Lichang Chen",
                "Heng Huang",
                "Tianyi Zhou"
            ],
            "title": "When do you need chain-ofthought prompting for chatgpt? arXiv preprint arXiv:2304.03262",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Meng Chen",
                "Ruixue Liu",
                "Lei Shen",
                "Shaozu Yuan",
                "Jingyan Zhou",
                "Youzheng Wu",
                "Xiaodong He",
                "Bowen Zhou."
            ],
            "title": "The JDDC corpus: A large-scale multi-turn chinese dialogue dataset for e-commerce customer service",
            "venue": "Proceedings of The 12th Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoping Chen",
                "Jiehui Jiang",
                "Jianmin Ji",
                "Guoqiang Jin",
                "Feng Wang."
            ],
            "title": "Integrating nlp with reasoning about actions for autonomous agents communicating with humans",
            "venue": "2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent",
            "year": 2009
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Dawn Song."
            ],
            "title": "Treeto-tree neural networks for program translation",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing",
            "venue": "chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Nancy Chinchor."
            ],
            "title": "MUC-4 evaluation metrics",
            "venue": "Fourth Message Uunderstanding Conference (MUC4): Proceedings of a Conference Held in McLean, Virginia, June 16-18, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv preprint arXiv:1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Deepak Chowdary Edara",
                "Lakshmi Prasanna Vanukuri",
                "Venkatramaphanikumar Sistla",
                "Venkata Krishna Kishore Kolli"
            ],
            "title": "Sentiment analysis and",
            "year": 2023
        },
        {
            "authors": [
                "CRN Estevam",
                "MJ Rider",
                "E Amorim",
                "JRS Mantovani."
            ],
            "title": "Reactive power dispatch and planning using a non-linear branch-and-bound algorithm",
            "venue": "IET generation, transmission & distribution, 4(8):963\u2013 973.",
            "year": 2010
        },
        {
            "authors": [
                "Norman Fairclough."
            ],
            "title": "Discourse and text: Linguistic and intertextual analysis within discourse analysis",
            "venue": "Discourse & society, 3(2):193\u2013217.",
            "year": 1992
        },
        {
            "authors": [
                "Norman Fairclough."
            ],
            "title": "Analysing discourse: Textual analysis for social research",
            "venue": "Psychology Press.",
            "year": 2003
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Ingrid E Fisher",
                "Margaret R Garnsey",
                "Mark E Hughes."
            ],
            "title": "Natural language processing in accounting, auditing and finance: A synthesis of the literature with a roadmap for future research",
            "venue": "Intelligent Systems in Accounting, Finance and Manage-",
            "year": 2016
        },
        {
            "authors": [
                "Joseph L. Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological Bulletin, 76:378\u2013382.",
            "year": 1971
        },
        {
            "authors": [
                "Robert W Floyd."
            ],
            "title": "Syntactic analysis and operator precedence",
            "venue": "Journal of the ACM (JACM), 10(3):316\u2013 333.",
            "year": 1963
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv preprint arXiv:2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Claude Goldenberg."
            ],
            "title": "Instructional conversations: Promoting comprehension through discussion",
            "venue": "The Reading Teacher, 46(4):316\u2013326.",
            "year": 1992
        },
        {
            "authors": [
                "Barbara J Grosz",
                "Candace L Sidner."
            ],
            "title": "Plans for discourse",
            "venue": "Technical report, BBN LABS INC CAMBRIDGE MA.",
            "year": 1988
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charlie Snell",
                "Xinyang Geng",
                "Hao Liu",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "The false promise of imitating proprietary llms",
            "venue": "arXiv preprint arXiv:2305.15717.",
            "year": 2023
        },
        {
            "authors": [
                "Mohammad Kasra Habib."
            ],
            "title": "On the automated entity-relationship and schema design by natural language processing",
            "venue": "Int. J. Eng. Sci, 8(11):42\u201348.",
            "year": 2019
        },
        {
            "authors": [
                "Zellig S Harris",
                "Zellig S Harris."
            ],
            "title": "Discourse analysis",
            "venue": "Springer.",
            "year": 1970
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Song"
            ],
            "title": "Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Carl Hewitt."
            ],
            "title": "Planner: A language for proving theorems in robots",
            "venue": "Proceedings of the 1st International Joint Conference on Artificial Intelligence, IJCAI\u201969, page 295\u2013301, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
            "year": 1969
        },
        {
            "authors": [
                "Hamel Husain",
                "Ho-Hsiang Wu",
                "Tiferet Gazit",
                "Miltiadis Allamanis",
                "Marc Brockschmidt."
            ],
            "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "venue": "arXiv preprint arXiv:1909.09436.",
            "year": 2019
        },
        {
            "authors": [
                "Yizhu Jiao",
                "Ming Zhong",
                "Sha Li",
                "Ruining Zhao",
                "Siru Ouyang",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Instruct and extract: Instruction tuning for on-demand information extraction",
            "venue": "the Association for Computational Linguistics: EMNLP 2023. Association for Computa-",
            "year": 2023
        },
        {
            "authors": [
                "James M Keller",
                "Michael R Gray",
                "James A Givens."
            ],
            "title": "A fuzzy k-nearest neighbor algorithm",
            "venue": "IEEE transactions on systems, man, and cybernetics, (4):580\u2013585.",
            "year": 1985
        },
        {
            "authors": [
                "Dan Klein",
                "Christopher D Manning."
            ],
            "title": "Accurate unlexicalized parsing",
            "venue": "Proceedings of the 41st annual meeting of the association for computational linguistics, pages 423\u2013430.",
            "year": 2003
        },
        {
            "authors": [
                "Thomas K Landauer",
                "Peter W Foltz",
                "Darrell Laham."
            ],
            "title": "An introduction to latent semantic analysis",
            "venue": "Discourse processes, 25(2-3):259\u2013284.",
            "year": 1998
        },
        {
            "authors": [
                "Shu-Hsien Liao."
            ],
            "title": "Expert system methodologies and applications\u2014a decade review from 1995 to 2004",
            "venue": "Expert systems with applications, 28(1):93\u2013 103.",
            "year": 2005
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "Gpteval: Nlg evaluation using 3.54 with better human alignment. arXiv preprint arXiv:2303.16634",
            "year": 2023
        },
        {
            "authors": [
                "Tim Loughran",
                "Bill McDonald."
            ],
            "title": "Textual analysis in finance",
            "venue": "Annual Review of Financial Economics, 12:357\u2013375.",
            "year": 2020
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou."
            ],
            "title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "venue": "arXiv preprint arXiv:2303.15621.",
            "year": 2023
        },
        {
            "authors": [
                "William C Mann",
                "Sandra A Thompson."
            ],
            "title": "Rhetorical structure theory: Toward a functional theory of text organization",
            "venue": "Text-interdisciplinary Journal for the Study of Discourse, 8(3):243\u2013281.",
            "year": 1988
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Pro-",
            "year": 2013
        },
        {
            "authors": [
                "Shachar Mirkin",
                "Michal Jacovi",
                "Tamar Lavee",
                "HongKwang Kuo",
                "Samuel Thomas",
                "Leslie Sager",
                "Lili Kotlerman",
                "Elad Venezian",
                "Noam Slonim."
            ],
            "title": "A recorded debating dataset",
            "venue": "Proceedings of the Eleventh International Conference on Language Re-",
            "year": 2018
        },
        {
            "authors": [
                "Tetsuya Nasukawa",
                "Jeonghee Yi."
            ],
            "title": "Sentiment analysis: Capturing favorability using natural language processing",
            "venue": "Proceedings of the 2nd international conference on Knowledge capture, pages 70\u201377.",
            "year": 2003
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The CoNLL-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natu-",
            "year": 2014
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong."
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Siru Ouyang",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Dialogue graph modeling for conversational machine reading",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021",
            "year": 2021
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Niels Pinkwart",
                "Vincent Aleven",
                "Kevin Ashley",
                "Collin Lynch."
            ],
            "title": "Toward legal argument instruction with graph grammars and collaborative filtering techniques",
            "venue": "Intelligent Tutoring Systems: 8th International Conference, ITS 2006, Jhongli, Taiwan,",
            "year": 2006
        },
        {
            "authors": [
                "Marilyn Radford."
            ],
            "title": "Practice papers personal financial services in a digital age",
            "venue": "Journal of Consumer Behaviour: An International Research Review, 2(3):287\u2013295.",
            "year": 2003
        },
        {
            "authors": [
                "Marzieh Saeidi",
                "Max Bartolo",
                "Patrick Lewis",
                "Sameer Singh",
                "Tim Rockt\u00e4schel",
                "Mike Sheldon",
                "Guillaume Bouchard",
                "Sebastian Riedel."
            ],
            "title": "Interpretation of natural language rules in conversational machine reading",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "A Carlisle Scott",
                "William J Clancey",
                "Randall Davis",
                "Edward H Shortliffe."
            ],
            "title": "Explanation capabilities of production-based consultation systems",
            "venue": "Technical report, Stanford Univ CA Dept of Computer Science.",
            "year": 1977
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Edward H Shortliffe",
                "Stanton G Axline",
                "Bruce G Buchanan",
                "Thomas C Merigan",
                "Stanley N Cohen."
            ],
            "title": "An artificial intelligence program to advise physicians regarding antimicrobial therapy",
            "venue": "Computers and Biomedical Research, 6(6):544\u2013560.",
            "year": 1973
        },
        {
            "authors": [
                "Abhijeet R Sontakke",
                "Amit Pimpalkar."
            ],
            "title": "A rule based graphical user interface to relational database using nlp",
            "venue": "interaction, 5(6).",
            "year": 2014
        },
        {
            "authors": [
                "Michael Stubbs."
            ],
            "title": "Text and corpus analysis: Computer-assisted studies of language and culture",
            "venue": "Blackwell Oxford.",
            "year": 1996
        },
        {
            "authors": [
                "Tian-Xiang Sun",
                "Xiang-Yang Liu",
                "Xi-Peng Qiu",
                "Xuan-Jing Huang."
            ],
            "title": "Paradigm shift in natural language processing",
            "venue": "Machine Intelligence Research, 19(3):169\u2013183.",
            "year": 2022
        },
        {
            "authors": [
                "Gerald Sussman",
                "Terry Winograd."
            ],
            "title": "Microplanner reference manual",
            "venue": "Technical report, USA.",
            "year": 1970
        },
        {
            "authors": [
                "Jeffrey Svajlenko",
                "Judith F. Islam",
                "Iman Keivanloo",
                "Chanchal K. Roy",
                "Mohammad Mamun Mia."
            ],
            "title": "Towards a big data curated benchmark of interproject code clones",
            "venue": "2014 IEEE International Conference on Software Maintenance and Evolution,",
            "year": 2014
        },
        {
            "authors": [
                "Jiwei Tan",
                "Xiaojun Wan",
                "Jianguo Xiao."
            ],
            "title": "Abstractive document summarization with a graphbased attentional neural model",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2017
        },
        {
            "authors": [
                "Michele Tufano",
                "Cody Watson",
                "Gabriele Bavota",
                "Massimiliano Di Penta",
                "Martin White",
                "Denys Poshyvanyk."
            ],
            "title": "An empirical study on learning bugfixing patches in the wild via neural machine translation",
            "venue": "ACM Transactions on Software Engineering",
            "year": 2019
        },
        {
            "authors": [
                "Karthik Valmeekam",
                "Alberto Olmo",
                "Sarath Sreedharan",
                "Subbarao Kambhampati."
            ],
            "title": "Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change)",
            "venue": "arXiv preprint arXiv:2206.10498.",
            "year": 2022
        },
        {
            "authors": [
                "Karthik Valmeekam",
                "Sarath Sreedharan",
                "Matthew Marquez",
                "Alberto Olmo",
                "Subbarao Kambhampati."
            ],
            "title": "On the planning abilities of large language models (a critical investigation with a proposed benchmark)",
            "venue": "arXiv preprint arXiv:2302.06706.",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Boshi Wang",
                "Sewon Min",
                "Xiang Deng",
                "Jiaming Shen",
                "You Wu",
                "Luke Zettlemoyer",
                "Huan Sun."
            ],
            "title": "Towards understanding chain-of-thought prompting: An empirical study of what matters",
            "venue": "arXiv preprint arXiv:2212.10001.",
            "year": 2022
        },
        {
            "authors": [
                "Qingyun Wang",
                "Manling Li",
                "Hou Pong Chan",
                "Lifu Huang",
                "Julia Hockenmaier",
                "Chowdhary Girish",
                "Heng Ji."
            ],
            "title": "Multimedia generative script learning for task planning",
            "venue": "Proc. The 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "2022b. Self-instruct: Aligning language model with self generated instructions",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Yaqi Xie",
                "Chen Yu",
                "Tongyao Zhu",
                "Jinbin Bai",
                "Ze Gong",
                "Harold Soh."
            ],
            "title": "Translating natural language to planning goals with large-language models",
            "venue": "arXiv preprint arXiv:2302.05128.",
            "year": 2023
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang."
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244.",
            "year": 2023
        },
        {
            "authors": [
                "Da Yin",
                "Xiao Liu",
                "Fan Yin",
                "Ming Zhong",
                "Hritik Bansal",
                "Jiawei Han",
                "Kai-Wei Chang."
            ],
            "title": "Dynosaur: A dynamic growth paradigm for instruction-tuning data curation",
            "venue": "arXiv preprint arXiv:2305.14327.",
            "year": 2023
        },
        {
            "authors": [
                "Hainan Zhang",
                "Yanyan Lan",
                "Liang Pang",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Recosa: Detecting the relevant contexts with self-attention for multi-turn dialogue generation",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Tianle Li",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zhuohan Li",
                "Zi Lin",
                "Eric Xing"
            ],
            "title": "Lmsyschat-1m: A large-scale real-world llm conversation",
            "year": 2023
        },
        {
            "authors": [
                "Ming Zhong",
                "Yang Liu",
                "Da Yin",
                "Yuning Mao",
                "Yizhu Jiao",
                "Pengfei Liu",
                "Chenguang Zhu",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Towards a unified multidimensional evaluator for text generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Over the past years, the NLP community has witnessed several paradigm shifts in technology followed by renewed research focus on applications that test the limits of this technology (Sun et al., 2022). For example, distributed word representations (Landauer et al., 1998; Mikolov et al., 2013) enabled a better characterization of the semantic similarity between words, entailing NLP research gravitated towards tasks like sentiment analysis and dependency parsing (Klein and Manning, 2003). Subsequent technologies like the transformer architecture (Vaswani et al., 2017) and contextual word representations (Devlin et al., 2019; Peters et al., 2018) further expanded the space of possible applications and the edge of NLP research, such\n\u2217Work partially done during internship at Microsoft.\nas machine translation (Bahdanau et al., 2015) and document summarization (Tan et al., 2017).\nMost recently, large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022) such as ChatGPT, emerged as powerful tools capable of achieving unprecedented success across a broad spectrum of NLP tasks (Jiao et al., 2023; Hendrycks et al., 2021b; Clark et al., 2018). These models have become accessible and popular among non-NLP experts, opening the door for many new user applications.\nThe flood of new applications and the sharing of user interactions with LLMs (Tay et al., 2023) provide a great opportunity to closely examine the distribution of real applications users need on a daily basis. After a detailed analysis, we identify a conspicuous gap between real-world user queries and established NLP benchmarks, suggesting another shift in NLP focus is needed. To systematically analyze the phenomenon and to bridge the gap, we conduct a battery of experiments aiming to examine the following aspects:\n\u2022 What is the distribution of real-world user queries in terms of domain and task types, and how do they shift from traditional NLP benchmarks (\u00a7 3)?\n\u2022 What are the emerging tasks and requirements from real-world user queries that may be overlooked in previous studies (\u00a7 4)?\nWe start by investigating ShareGPT1, a largescale collection of user-GPT conversations in the real world, containing 94,145 split data samples. ShareGPT has been used for training powerful LLMs (Chiang et al., 2023; Xu et al., 2023) and incorporated into new datasets (Zheng et al., 2023; Gudibande et al., 2023), both showing substantial advantages. Specifically, we design an annotation framework where we employ GPT-4 (OpenAI,\n1https://sharegpt.com/\n2023) to generate the related information for every user query that appears in ShareGPT. We subsequently delve into the obtained data and conduct comprehensive analyses to answer the aforementioned questions2. We summarize our key findings as follows:\n1. Generally, real-world user queries demonstrate a tendency towards more aligned with daily life with enlarging diverse user bases. 2. We discovered several tasks, including providing advice, designing, planning, etc., that are seldom touched and pose new requirements in the era of LLM. 3. We summarized the shifting trends and challenges, providing insights to fill the gap for both stakeholders and users."
        },
        {
            "heading": "2 Methodology",
            "text": "In this section, we employ GPT to annotate the topic/domain and task type of every sample in the ShareGPT collection. The whole annotation and post-processing pipeline is illustrated in Figure 1. We use human evaluation to verify the quality of our automatic annotation."
        },
        {
            "heading": "2.1 ShareGPT",
            "text": "ShareGPT is a publically available large-scale collection of user-GPT conversation histories3. It is based on a Chrome Extension4 where users can choose to upload their interactions with GPT. The version we used contains 94, 145 split user-GPT conversations and is previously used to train LLMs such as Vicuna (Chiang et al., 2023). Every sample in ShareGPT is a multi-turn conversation with utterances from both users and the GPT engine."
        },
        {
            "heading": "2.2 Self-demonstrated annotation",
            "text": "The goal of annotating each user query is twofold: to identify the underlying task a user is intending to perform (task types), and to understand the subject matter or field (domains) the query pertains to. The annotation process is challenging because i) the annotation requires a comprehensive and suitable predefined set of task types and domains/topics, and ii) the annotation should accurately reflect the genuine\n2Code is available at https://github.com/ozyyshr/ ShareGPT_investigation.\n3The collection of ShareGPT already contains 305,000 samples to the date of this submission, and is dynamically expanding. Since all the data are not publically available, we use the portion that could be downloaded from Huggingface.\n4https://github.com/domeccleston/sharegpt\nrequirements expressed in user queries. We chose to employ GPT-4 to conduct a self-demonstrated annotation due to its superior precision and coverage. The annotation process consists of three stages: 1) chain-of-thought prompting, 2) demonstration sampling, and 3) demonstration pool expansion.\nChain-of-thought prompting. Chain-of-thought (CoT) (Wei et al., 2022) uses intermediate steps for text generation and improves the performance of LLMs (Chen et al., 2023). To augment GPT4\u2019s proficiency in comprehending and interpreting user queries, we craft our instructions in manualCoT style, compelling the LLM to deliver the requisite information in a methodical, step-by-step manner. Specifically, we first ask LLM to identify the domain or topic related to the user query. Then, the LLM is prompted to generate a concise one-sentence summary for the given user query as a reference. Finally, drawing on the insights obtained thus far, the LLM is capable of devising creative and accurate task types corresponding to the user query. The generated task types are fine-grained and diverse, spanning from email editing to dream analysis. Overall, we obtain 13, 783 task types and 8, 392 domain labels for all samples in ShareGPT.\nDemonstration sampling. While the CoT prompting can generate reasonable annotation, it is known that in-context demonstrations can further improve LLM output quality (Wang et al., 2022a). Thus, we select examples from CoT outputs to serve as demonstrations in the second stage. We initiate the demonstration pool with 20 samples of different domains or topics and task types. For every sample, we randomly select k demonstrations from the pool and append them to the instruction in the first stage. The input and output format could be found in Figure 1.\nDemonstration pool expansion. To encourage diversity and avoid potential bias in demonstration selection (Wang et al., 2022b), we gradually expand the demonstration pool. Since we are asking GPT4 to generate free-form task types, one challenge here is to avoid generating too divergent task types. Therefore, we maintain a dictionary to document the time of appearance for every task type. If a task type appears more than a fixed ratio \u03bb among all the current samples, we then add the current sample containing the task type into the demonstration pool. By enforcing such constraints, the generated free-form task types could be better \u201cclustered\u201d for\nfurther analysis.\nExperiment settings. We download ShareGPT from Huggingface5, where the 51k conversations are split into 94k ones due to the length limit for input. For every step, k is set to 3 and \u03bb is 0.05. We concatenate 3 samples together and let GPT-4 generate the annotation at once for the balance of speed and quality. To encourage diversity, we set the temperature to 0.4 and it takes around 10 days due to speed limitations in GPT-4 to annotate all the 94k samples in ShareGPT. We plan to release all the annotated results for future related research."
        },
        {
            "heading": "2.3 Human Evaluation",
            "text": "To assess the quality of annotation produced by GPT-4, a human evaluation is conducted with a specific focus on the generated free-form task types.\nWe designed and distributed our human assessment task with Doccano6. We recruited 3 graduate students as our human assessors (all of which are paid as research assistants). The assessors all have rich experiences with related NLP and ML research but were not involved in the development of our framework. We randomly selected 100 samples for evaluation. For every sample, we ask the assessors to judge the generated task types in terms of completeness and correctness. This is to evaluate whether the generated task types are complete and faithful to the original user query. For completeness, the scoring scale is 0 (bad) and 1 (good), and\n5https://huggingface.co/datasets/ anon8231489123/ShareGPT_Vicuna_unfiltered\n6https://github.com/doccano/doccano\nfor correctness, the scoring scale is 0 (incorrect), 1 (partly correct) and 2 (correct). The detailed rubric and the interface are shown in Appendix B.\nTable 1 demonstrates the results of human evaluation. We can see that with GPT-4 we got reliable annotations for ShareGPT. Notably, none of the 100 samples got \u201cincorrect\u201d annotations. Apart from the scores, we also calculate Fleiss kappa \u03ba (Fleiss, 1971) for each metric, both indicating \u201calmost perfect agreement\u201d."
        },
        {
            "heading": "2.4 Post-processing for analysis",
            "text": "As the domain/topic and task type annotations generated by GPT-4 are free-form words and phrases, clustering the samples poses a significant challenge. For example, \u201crecipe suggestions\u201d, \u201ccooking tips\u201d and \u201ccooking advice\u201d all belong to the same task type. To tackle this challenge, we propose a post-processing framework that incorporates three stages shown in Figure 1: (1) a statistical calculation based on heuristic rules, (2) an ensemble with the embedding similarity of summary sentences, and (3) a manual checking process to ensure the best possible quality. Detailed narrations could be found in Appendix A."
        },
        {
            "heading": "3 Overall Investigation",
            "text": "In this section, we first present the overall statistics and analysis of the ShareGPT clustering results. Then we compare the results with conventional NLP datasets.\nFor conventional NLP datasets, we investigate 2,911 datasets from the Huggingface Datasets7 collected by Yin et al. (2023). These 2,911 datasets are filtered and selected from the original around 50k datasets in the Huggingface platform with the following conditions: (i) Datasets with no license description or the license does not allow usage. For ethical considerations, the datasets collected are strictly following license restrictions. The number of all datasets fulfilling license requirements is restricted to around 16k. (ii) Datasets that are non-English. Since our investigation focuses on the English language and ShareGPT contains only English samples, datasets collected from Huggingface also follow this paradigm. (iii) Datasets that are related to multi-modal. There is a large portion of datasets in the Huggingface data platform related to multi-modal research. Since we target at NLP community and related topics, we exclude those datasets. (iv) Datasets that are invalid. Some datasets in the Huggingface data platform are empty or cannot be downloaded, which is ignored\n7https://huggingface.co/datasets?p=0&sort= downloads\nby our work."
        },
        {
            "heading": "3.1 Domain and Task Distribution",
            "text": "Based on our annotation and clustering results, We plot domain and task type distributions of ShareGPT queries in Figure 2 (a) and (b). \u201cTechnology\u201d shares the largest portion of ShareGPT\u2019s domain, comprising around a quarter. Other notable domains \u201ceducation\u201d, \u201cbusiness\u201d and \u201clanguage\u201d make up another quarter. For task types in ShareGPT, around one-fifth is about \u201ccode generation\u201d, which corresponds to \u201ctechnology\u201d in domain analysis. \u201cCreative writing\u201d is the second largest portion. The rest of the task types are quite diverse, composing around 2/3 of the whole set.\nIn the following, we analyze the two mostly seen tasks in ShareGPT, coding and writing assistance, representing 19.9% and 21.3% respectively.\nCoding assistance Pre-trained models for programming language have been widely explored in the NLP community (Chen et al., 2021; Nijkamp et al., 2023). Current benchmarks used for evaluation (Hendrycks et al., 2021a) are usually in the form of function generation or code completion given certain requirements. More specific task settings include code search (Husain et al., 2019), code translation (Chen et al., 2018), code clone detection (Svajlenko et al., 2014), and code refinement (Tufano et al., 2019). We do observe user\nqueries that are similar to the previously mentioned task settings in ShareGPT, such as code generation (18.6%) and code debugging (9.2%). This reflects that the current coding benchmarks are fitting with real-world scenarios. However, we still notice a non-negligible portion of requests involving higher-level program understanding, such as code simplification and providing design pattern suggestions, which are seldom captured in existing task definitions. We also plot the proportion of the top 10 most frequent programming languages used in Figure 3.\nWriting assistance With advancements in the NLP field, writing assistance tools have shown potential beyond grammatical (Ng et al., 2014) and stylistic improvements, now providing aid in content creation and organization. Writing tasks such as story generation (Fan et al., 2018) and style transformation (Shen et al., 2017) are popularly explored in the community. Our analysis of ShareGPT usage confirms this trend. For instance, assistance in article drafting and editing accounts up to 5.1% of the writing assistance requests. Similarly, email editing makes up to 2.6% of the queries. These suggest users rely on AI tools for professional writing communication.\nDespite this, we notice a trend of creative writing for a bunch of text formats, spanning from slogans to tutorial writing. Instead of leveraging LLMs to generate everything, we found a noticeable portion of \u201cprocedure writing\u201d and \u201chow-to-write\u201d queries, underscoring the importance of explanatory and pedagogical writing aids."
        },
        {
            "heading": "3.2 Distribution Difference with Conventional Datasets",
            "text": "To provide a comparison of ShareGPT queries with conventional NLP datasets,we also annotate and cluster the collected 2,911 Huggingface datasets in the same way and present results in Figure 2 (c) and (d). We then interpret the differences from two aspects, domain and task type.\nDomain Shift We plot the top 20 most common and representative domains of ShareGPT in Figure 2, which account for around 73% of the entire set. We observe that the domain distribution in ShareGPT is quite diverse. Notably, there is a large portion of technology-related topics. Commonly seen cases involve code writing and debugging. For Huggingface datasets, we randomly select 10 samples from each dataset as an approximation,\nand leverage GPT to classify the corresponding domain/topics. Similar to ShareGPT, technologyrelated domains also share a large portion in Huggingface. However, in Huggingface, political, legal, personal life, and film altogether contribute a large portion. We further look into the data source for Huggingface. We collect the information in metadata \u201cDataset Card\u201d, where detailed information of the specific dataset is introduced. For example, MRPC dataset (Dolan and Brockett, 2005) has the statement \u201ca corpus of sentence pairs automatically extracted from online news sources\u201d. We then use GPT to annotate the corresponding data source as an approximation. We find that the majority of datasets are from Wikipedia and news, occupying a portion of over 80%. The rest mainly come from \u201cgovernment reports\u201d and \u201cQA forums\u201d. This phenomenon calls for dataset curation specific to domains commonly seen in user queries, and the data source should also incorporate materials in multiple formats apart from Wikipedia and news.\nTask Type Shift We categorize different tasks based on metadata statistics for all the datasets from Huggingface, as shown on the right side in Figure 2. We filter out tasks with numbers less than 10, and the rest samples make up to 99.8% of the original set. We observe that question answering and text classification are the top-2 task types, summing up to more than 2/3 of the whole collection. On the contrary, in ShareGPT, almost all the user queries are free-form text generations or generations in a user-required manner."
        },
        {
            "heading": "4 Shifted and Overlooked Tasks",
            "text": "In this section, we detail the overlooked tasks discovered in the analysis process of ShareGPT, with concrete examples shown in Table 2. Task selection is based on the distribution in Figure 2 that satisfies two criteria: (1) long-tail tasks of the distribution, summing towards around 40%; and (2) tasks that are not overly skewed in the distribution, constituting around 25% - a comparable figure to the predominant segments. We also summarize the features for each task type in Table 3, along with the potential roadmap. Commonly seen topics and human analysis for performance of LLMs are shown in Figure 6 in Appendix D."
        },
        {
            "heading": "4.1 Task of Providing Advice",
            "text": "The task of giving advice occupies a portion of up to 3% in ShareGPT. The concept of a ma-\nchine offering advice has been present and associated with NLP from as early as the 1970s (Shortliffe et al., 1973) with the development of expert systems (Liao, 2005). At that time, giving advice is usually performed with a consultation program (Scott et al., 1977), which plays the role of an expert consultant in restricted domains, such as health. Since then, the field has evolved significantly, with expanding domains into legal (Pinkwart et al., 2006), finance (Radford, 2003), etc. The advising process back then is more like a QA system, where the user first input background confined by rules, together with a direct question such as \u201cIs Organism-1 a rod or coccus (etc.)?\u201d. Later, advice was formally characterized as a multi-step process involving the analysis of the recipient\u2019s context and the formulation of a response in natural languages (Saeidi et al., 2018).\nCompared with previous advising systems that require rigid rules and were not designed for openended dialogue, user queries in ShareGPT are more free-form. Moreover, instead of focusing on restricted professional domains, these requests are more akin to everyday tasks, such as relationship dilemmas and personal careers. This also presents the requests of more general, macro-level guidance, compared with providing micro-specific answers in previous studies."
        },
        {
            "heading": "4.2 Task of Designing",
            "text": "Request to design some objects with specifications constitute 2.5% of ShareGPT queries. The task of designing progresses slowly in the field of NLP over the past half-century, and does not have a clear line of development. Previous NLP-related works generally focused on relatively narrow domain-specific topics, such as entity-relationship schema (Habib, 2019) and user interface (Sontakke and Pimpalkar, 2014).\nWhile previous works focus more on the design of structured objects, such as database UI, user queries arising from ShareGPT usually seek designs for more complicated objects, spanning from questionnaires to websites. This indicates a trend that the scope of design has broadened to include not only the structure of tangible objects but also conceptual frameworks and processes. Another significant difference is that design-related queries in ShareGPT ask for more diverse and creative requirements, especially for personalized objects such as icons. Additionally, in ShareGPT, we see instances where users demand designs with very specific constraints, such as adherence to a particular color scheme in interface design. These \u201cvisual image design\u201d tasks, though communicating in texts, require the model to have a sense of visual understanding in order to generate aestheti-\ncally pleasing outcomes."
        },
        {
            "heading": "4.3 Task of Planning",
            "text": "Planning is another important task we identified, which constitutes approximately 2.7% in ShareGPT. Planning in NLP has a long research history. Back in 1969, PLANNER (Hewitt, 1969) was invented as a language for manipulating models in a robot. Follow-up works (Sussman and Winograd, 1970; Bonczek et al., 1979) mostly focus on planning with programming languages for decision-making. Later, a plan was formally defined as an assembly of operators (Grosz and Sidner, 1988) that describes the transition from initial states to final states, where rules and schema were designed for induction. Afterward, planning was employed in different domains and scenarios, such as trajectory (Borrelli et al., 2006), autonomous agents (Chen et al., 2009), and power dispatch (Estevam et al., 2010). Most of the works focus on planning with formal languages under certain rules. Nowadays, many benchmarks related to planning have emerged (Valmeekam et al., 2022; Xie et al., 2023; Wang et al., 2023). Although they require planning with open-form natural languages, they mostly focus on rudimentary tasks with simple actions such as (e.g., \u201cput up a block\u201d for \u201carrange blocks\u201d) (Valmeekam et al., 2023).\nThe emergence of LLMs has spawned much more free-form and customized formats in planning. One example is illustrated in Table 2, where users ask for planning a route with specific constraints on time, places, and EV charging. We noticed a trend of macro planning, e.g., travel plans, and course plans, instead of planning for micro actions in previous NLP studies. The domains entailed also greatly widen, spreading to many aspects of everyday lives, compared to previous planning systems designed for specific functions/users. Therefore, these planning tasks usually require a higher level ability in personalization, reasoning, and knowledge integration, where follow-up research efforts could lay hands on."
        },
        {
            "heading": "4.4 Task of Discussion",
            "text": "Discussion is an interactive and dynamic exchange of ideas or viewpoints, which consists of 3.8% samples in ShareGPT. The act of discussion in NLP is mostly explored in the form of conversation generation with chatbots (Goldenberg, 1992), and they mainly focus on structured discussions in specific domains that require domain expertise, such as po-\nlitical debating (Mirkin et al., 2018). Another notable characteristic is the non-interactivity in many previous works (Zhang et al., 2019; Ouyang et al., 2021), although they promote multi-turn generation (Chen et al., 2020) for several fixed user inputs.\nHowever, user queries in ShareGPT are typically more dynamic and unpredictable, encompassing a vast array of subjects and requiring a deep understanding of various perspectives. For instance, in ShareGPT, there are cases when users initiate philosophical discussions such as \u201cWhat\u2019s the meaning of life?\u201d, which may require self-thinking in viewpoints. On the other hand, the discussion process in ShareGPT is quite interactive, which poses challenges in actively taking initiative and even shifting topics. Developing models with certain personalities and empathy to facilitate more effective and meaningful discussions will be helpful."
        },
        {
            "heading": "4.5 Task of Analysis",
            "text": "The task of analysis takes up a very large portion of ShareGPT, approximately 7.3%. Textual analysis is a long-standing and crucial branch of NLP. In the early stages, researchers focus on analyzing linguistic features within texts, such as syntax (Floyd, 1963) and discourse (Harris and Harris, 1970). Gradually, they began to wonder \u201chow can analysis of the patterns of words and grammar contribute to an understanding of the meaning\u201d (Stubbs, 1996), accompanied with the investigation in both directions of semantic/contextualized analysis (Mann and Thompson, 1988; Fairclough, 1992) and largerscale texts. As textual analysis evolves and improves, they are gradually employed in various domains like social science (Fairclough, 2003), medical (Edara et al., 2023), and finance (Fisher et al., 2016) with different objectives like culture (Carley, 1994), sentiment (Nasukawa and Yi, 2003) and opinions (Cambria et al., 2013).\nEven though previous research has already covered many fields and objectives, we still observe striking differences when it comes to user queries in ShareGPT. Notably, many previous analysis tasks take the form of classification, e.g., identifying a risk type in a financial report with 25 total risk types as label space (Loughran and McDonald, 2020). User queries, in contrast, usually do not specify the targets/aspects for analysis, as exemplified by the literature analysis case in Table 2. The scope of analysis also varies greatly in ShareGPT, ranging from the overall analysis of classical music\ndevelopment to the functional analysis of a single function in code. Hence, it calls for better specifications for user requirements/intents, as well as customization to different levels of scope."
        },
        {
            "heading": "4.6 Task of Evaluation",
            "text": "In ShareGPT, \u201cevaluation\u201d queries constitute up to 4%. The concept of evaluation permeates almost every aspect of NLP. Standard metrics such as F1 measure (Chinchor, 1992), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2020) are mostly employed to evaluate classification or generation results at the sentence level. Recently, there is a surge in research efforts to improve alignment with human preferences (Zhong et al., 2022; Liu et al., 2023; Fu et al., 2023; Luo et al., 2023) by using larger models.\nHowever, evaluation-related queries from ShareGPT are quite different. First, we observe that evaluation objects shift from traditional plain texts to a variety of input formats. For instance, GPT is often utilized to assess resumes or code snippets. Furthermore, the evaluation metrics used in these scenarios are exceptionally diverse and open-ended, ranging from the influence of a brand to the feasibility of a plan. This is quite different from previous metrics that are specifically designed for summarization or machine translation."
        },
        {
            "heading": "5 Emerging Trends and Challenges",
            "text": "In this section, we summarize the common trends and challenges shared by these prevalent or overlooked tasks we identified from the ShareGPT data."
        },
        {
            "heading": "5.1 What trends are reflected in ShareGPT user queries?",
            "text": "In view of user queries in ShareGPT, we notice incredible expansions of task scopes and user bases.\nMore Aligned with Daily Life GPT is leveraged for all kinds of everyday tasks, straddling both professional and personal issues. As such, user queries exhibit an increasing tendency to be free-form and contain arbitrary input, reflecting everyday life scenarios. It is also more customized as a personalized assistant, covering a broad range of topics with nuanced requirements.\nDiverse User Groups Accompanying the prevalence in everyday tasks is the diversification of user groups. We observe queries by diversifying users of different ages, professions, cultural backgrounds, and even traditionally marginalized groups."
        },
        {
            "heading": "5.2 What challenges are proposed by trending and future tasks",
            "text": "Despite the unprecedented success of LLMs, we notice real-world users are also raising their requirements when querying an LLM. Some of the concrete examples are shown in Appendix E.\nBetter Reasoning Capacity One evident need that emerged from user queries is advanced reasoning abilities. Users expect LLMs to comprehend complex scenarios, infer causality, and develop well-organized feasible responses to help them, especially with strategic decision-making.\nEmotion Perceivable A non-negligible portion of user queries come from marginalized groups seeking help, often due to real-world communication challenges or inadequate solutions. LLMs interacting with these users must effectively perceive their emotions and exhibit empathy, particularly as they may be sensitive when discussing their circumstances, such as those with mental problems. This necessitates a human-centric approach from LLMs, cultivating a safe environment for free expressions\nof concerns and offering appropriate support.\nWorld Knowledge In managing the diversity of user queries pertaining to everyday life, the imperative for LLMs to effectively utilize world knowledge grows apparent. This spans from static factual data, such as intercity distances for road trip planning, to dynamic, evolving information like restaurant recommendations fluctuating with Google ratings. Although integrating external plugins (Schick et al., 2023) and applications is a viable initial strategy, meticulous attention must be paid to uphold the neutrality and accuracy of this knowledge, mitigating potential biases and misinformation.\nMulti-modality Though restricted to textual interaction with LLMs, user queries demonstrate the potential of a diverse range of modalities. We observe the textual descriptions of images, websites, and UIs, as well as URLs to music/videos in user inputs, which calls for the development of multimodal integrations.\nPersonalization and Fairness We observe users increasingly expect AI models to understand their unique needs and offer tailored solutions. We also notice the diverse user bases for LLMs of different groups. The drive for personalization must be balanced against the principle of fairness. Personalized responses should not amplify biases or perpetuate unfair outcomes, and the pursuit of fairness should not impede the assistance for customization.\nDialogue and Interaction For user queries that entail massive collaborations or discussions with LLMs, they require a high degree of interactivity, which doesn\u2019t just involve passively accepting user queries and providing answers, but actively engaging with users to exchange viewpoints and ideas. This kind of interactive dialogue can help create a more user-friendly experience, facilitating a deeper understanding of the user\u2019s needs."
        },
        {
            "heading": "6 Conclusion and Future Works",
            "text": "In this paper, we identified a discrepancy between the existing state of NLP research and the need for real-world applications by investigating large collections of ShareGPT and Huggingface data samples with GPT-4. We make this annotation resource public, which could be directly leveraged for further investigation of ShareGPT data, or to fine-tune advanced models such as Llama as a much cheaper\nalternative tool for annotation. Based on our observation, we also provide insights into the challenges posed by real user needs and a potential roadmap for future work.\nLimitations\nWe discuss the limitations of this work in the following aspects:\n1. Our study is based on two sources, ShareGPT and Huggingface datasets. Although they are the most abundant resources we can obtain at hand to represent user queries and the traditional benchmarks in the NLP community, they could hardly reflect the whole breadth of real-world situations. Actually, both sets are still growing dynamically as time flows.\n2. In our annotation process, we employed GPT4, the state-of-the-art LLM to help generate domain/topics and task types. On top of that, we conduct a human evaluation for quality assessment. However, there are still chances when the annotation from GPT-4 is not accurate enough, which could influence the postprocessing step.\n3. Continuing from 2, our work relies on the usage of LLMs. We require annotation from GPT-4 for every sample in ShareGPT and the selected set in Huggingface datasets, which is a large number. Despite that, we make the annotation results publicly available, this annotation process is extremely resource-intensive and time-consuming to reproduce.\nEthics Statement\nOur work highlights the shifted trend and the overlooked problems of previous NLP studies. By analyzing real-world user queries, we examine the new requirements that emerged and hope to make LLMs more beneficial and better aligned with their users\u2019 needs, including the marginalized groups. We hope that our work can be an initial effort to mitigate the gap between user needs and academic benchmarks in the era of LLMs. Overall, we do not foresee any major risks or negative societal impacts of our work. The ShareGPT and Huggingface datasets we experiment with are publicly available online. We have open-sourced this project to facilitate future research, especially for small research groups or institutions with relatively fewer resources of LLMs."
        },
        {
            "heading": "Acknowledgement",
            "text": "Research was supported in part by US DARPA KAIROS Program No. FA8750-19-2-1004, National Science Foundation IIS-19-56151, and the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897. Any opinions, findings, conclusions, or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA, the National Science Foundation, or the U.S. Government."
        },
        {
            "heading": "A Details in Post-processing",
            "text": "In this section, we detail the framework for postprocessing. After annotation was completed by GPT-4, we got 1) free-form words and phrases for domain/topic, 2) a one-sentence summary for the user query, and 3) free-form words and phrases task types.\nWe start by calculating the frequency of certain words/phrases. Considering the existence of synonyms, such as the \u201cadvice, tip, suggestion\u201d mentioned before, we resort to an external dictionary8 and combine synonyms together. On the other hand, those semantically similar words/phrases should also be clustered together. Previously we require GPT-4 to generate summaries for user queries of great quality, and they could be used as references in finding representative samples for the same cluster. For every sample combined by the previous heuristic rules, we search for their knearest-neighbors (Keller et al., 1985) and union all the samples as the final results. For words/phrases of very low frequency not incorporated above, we select the nearest \u201ccluster\u201d as the approximation. Finally, to ensure the best possible quality, we manually filtered out unrelated ones in the processed results."
        },
        {
            "heading": "B Human Evaluation Interface",
            "text": "The website interface screenshot adapted from Doccano for human evaluation is shown in Figure 4. Assessors were informed of the purpose of the study. Before they began to work on the assessment task, they were presented with task instructions shown in Figure 5 and a rating example."
        },
        {
            "heading": "C Popular programming languages seen in Section 3",
            "text": "We summarize the top 10 mostly used programming languages in coding assistance tasks of ShareGPT. As shown in Figure 3, Python is the mostly used programming language. Apart from that, we observe another large portion of HTML and CSS, reflecting great user needs in website design and programming."
        },
        {
            "heading": "D Common topics and LLM performance for tasks listed in Section 4",
            "text": "We plot the top 10 mostly seen topics discovered for each task type mentioned in Section 4 shown in Fig-\n8https://www.thesaurus.com/\nure 6. The shallower color and longer bar indicate a larger portion. For \u201cadvice\u201d, macro-level guidance such as career advice was the most sought-after category, reflecting individuals\u2019 focus on professional growth and job success. This was followed by legal and financial advice, highlighting the everyday complexities people face in navigating legal systems and managing their personal finances. For \u201cdesign\u201d, apart from algorithm design, we also notice creative design requirements such as interface, game, and website, which may also entail visual perception. In \u201cplanning\u201d, the most common topic discerned was itinerary and travel, indicating significant needs in devising travel plans or daily schedules. As to \u201cdiscussion\u201d, there are explicit topics for discussion such as the historical event or for coding design. We also notice abstract topics like philosophical discussions and ethics discussions, which raises the higher requirement for the self-thinking of LLMs. When it comes to \u201canalysis\u201d, a broad range of topics are covered, from specific domains such as literary and historical to personal affairs like relationship analysis. Finally for the task of \u201cevaluation\u201d, we notice a diversity of objects to be evaluated, spanning as concrete as a code snippet or grammar and as abstract as candidates for certain positions.\nTo help better understand the difficulty of the newly identified tasks, we provide a case-based analysis of LLMs. Specifically, we did a preliminary study by randomly selecting 20 samples for each task type from the ShareGPT data. We manually examined the performance of two models on each case, and reported the failure rate for GPT-4 and GPT-3.5-turbo in Table 4 with respect to each task type."
        },
        {
            "heading": "E Concrete examples mentioned in",
            "text": "Section 5\nIn this part, we display concrete examples in ShareGPT where requirements raised by users pose specific challenges. We highlight the challenging requirements and the misinformation generated by GPT-4 in red.\nOne example of \u201cadvice\u201d about relationship\ncounseling is shown in Figure 10, where the user is seeking emotional support caused by his relationship with his fiancee. We can see that GPT-4 lacks emotional perception, repeating \u201cI\u2019m sorry to hear that...\u201d during the whole interaction, and failing to demonstrate empathy towards the user scenario.\nIn Figure 7 we display an example of \u201cplanning\u201d, where the user cast specific constraints on time and places, and specific requirements for EV charging.\nGPT-4 generates an 8-day plan, which does not fulfill the requirement for dates. The route is also fully in the UK, failing to cover places like Nice and Calais. Finally, GPT-4 incorporates misinformation for distances between two cities.\nIn Figure 8 and Figure 9 we visualize two examples related to \u201cdesign\u201d. For course design in Figure 8, the user requires a 5-day design to learn the required content. From GPT-4 response, we can see that the time scheduling is still not feasible to accommodate the content listed, and the design could be overlapped on certain topics. For logo design in Figure 9, GPT-4 does not hold the multimodal ability to deal with this query, and in the conceptual design description, we can see that the design may not fulfill aesthetic beauty requirements and lacks creativity.\nIn Figure 11 we present another example of \u201cdiscussion\u201d, where the user is initiating a philosophical discussion with GPT. We can see that of the most time, GPT is quoting from existing philosophical viewpoints, and sometimes even demonstrates contradictions in the responses due to the conflict\nbetween user input and the retrieved opinions. Figure 12 shows an example of \u201canalysis\u201d concerning a character in the play written by Shakespeare. This is a very open and controversial topic for analysis. While in GPT\u2019s response, we observe hallucinations, especially towards the change of personality of Bertram, and the misinformation of plots when the character is in Italy.\nI'm going on a road trip with my daughter on 3rd April. We will be leaving London and want to be in Nice, France on 9th or 10th April where we'll stay about 3 days. We then need to be in Calais on 15th or 16th to take the Eurotunnel. Everything else is undecided. Can you give a plan for routes where we'll be doing no more than 5 hours driving per day. We have an EV and need to stop for charging.User\nGPT-4\nDay 1: London to Brighton (2 hrs, 54 miles) Leave London and head south to Brighton. Brighton is a vibrant seaside town with a great mix of shopping, cultural sites, and delicious food. Visit the iconic Brighton Palace Pier or take a tour of the Royal Pavilion. Make sure to charge your EV overnight. Day 2: Brighton to Portsmouth (2 hrs, 63 miles) Drive west along the coast to Portsmouth. Here, you can visit the historic dockyard and Spinnaker Tower for panoramic views of the city. Again, charge your EV overnight. Day 3: Portsmouth to Poole (2 hrs, 64 miles) Continue your journey west towards Poole. In Poole, you can visit the lovely Poole Harbour and enjoy the quaint Old Town. Day 4: Poole to Plymouth (3 hrs, 123 miles) This is a slightly longer stretch, but Plymouth offers plenty to see, including the National Marine Aquarium and the historic Barbican district. Day 5: Plymouth to Falmouth (2 hrs, 65 miles) Head to the picturesque coastal town of Falmouth. Visit Pendennis Castle or take a stroll through the beautiful gardens of Trebah. Day 6: Falmouth to Bournemouth (3 hrs, 111 miles) Drive back east to Bournemouth. Here, you can enjoy Bournemouth Beach or visit the Bournemouth Aviation Museum. Day 7: Bournemouth to Brighton (2 hrs 30 mins, 100 miles) Return to Brighton, visiting any attractions you may have missed during your first visit. Day 8: Brighton to London (2 hrs, 54 miles) Return to London, ready to return to daily life. The above itinerary involves daily drives of between 2 and 3 hours, leaving you plenty of time to explore each destination. EV charging stations are relatively plentiful in the UK, but you should plan your charging stops in advance and always have a backup plan in case your first-choice charging station is unavailable.\nDay 1: Fundamentals and Mathematical Background Morning: 4 hours 1. MIT OpenCourseWare: Linear Algebra (YouTube): focus on understanding vectors, matrices, singularity, rank, and\nI\u2019m in a hard place. User\nGPT-4\nI'm sorry to hear that you're going through a difficult time. While I'm an artificial intelligence and don't have personal experiences, I'm here to help provide advice, share useful information, or just listen if you need to talk. Can you tell me a bit more about what you're facing?\nUser"
        },
        {
            "heading": "My fianc\u00e9e Madelyn was angry at me tonight.",
            "text": "GPT-4\nI'm sorry to hear that you're having a disagreement with your fianc\u00e9e, Madelyn. Conflicts in relationships can be stressful. Would you like to talk more about what happened? It might help to sort through your thoughts. Remember, I'm here to help, not to judge.\nUser I have proposed to her; we are engaged. I am committed to her. We have bought a house together. I love her. What else can I do? She was angry at me for the whole night!\nGPT-4\nI'm sorry to hear that you're feeling this way. It sounds like you're deeply committed to Madelyn and your relationship with her. It's normal for couples to have disagreements from time to time. Here are a few tips that may help: 1.Communication: The first step in any relationship conflict is to talk openly about the problem. Ask her why she was angry. Her reasons might be something you're unaware of, or it might be a misunderstanding. She may also not realize how deeply her anger is affecting you. 2.Understanding: Once you understand why she was upset, acknowledge her feelings. Even if you don't necessarily agree with her perspective, understanding where she is coming from is key to resolving the issue. Validating her feelings can show her that you are listening and that you care about her.\nPonder this. It cannot be disproven that before my birth and existence, that all time, matter and space and experiences of all, and all opinions, were merely constructed for my arrival in a moment in time, and that all future is also constructed. Including your opinion and all opinions and so-called evidence against the notion, which does not exist, is also constructed. This cannot be disproven.User\nGPT-4\nThe idea you present is a philosophical concept known as solipsism. Solipsism is the philosophical belief that only one's mind is sure to exist. Therefore, everything else, including other people and the external world, may or may not exist. While it cannot be disproven that everything is constructed for your arrival in a moment in time, it also cannot be proven. The problem with solipsism is that it is unfalsifiable, meaning that it cannot be tested or proven. It is impossible to prove that everything is constructed for your arrival in a moment in time because there is no way to gather evidence for or against this idea. Therefore, it is not a meaningful or useful theory to consider when discussing reality. Furthermore, the fact that we can have conversations and share ideas suggests that there is a shared reality that exists outside of our individual minds. It is reasonable to assume that there is an objective reality that exists independent of our personal experiences."
        },
        {
            "heading": "In conclusion, while solipsism cannot be disproven, it is also not a useful or meaningful concept to consider when discussing reality. It is more reasonable to assume that there is an objective reality that exists independent of our individual experiences.",
            "text": "User\nFirstly, nothing is provable or unprovable. Secondly you said \"Furthermore, the fact that we can have conversations and share ideas suggests that there is a shared reality that exists outside of our individual minds\". This can be explained by A self and sole existing master fragmented hive mind, that give the rise to the illusion of shared experience, though disconnected smaller minds of an overall master hive fragmented mind.\nWhile it is true that nothing can be proven or disproven with absolute certainty, in practical terms, we rely on evidence and reasoning to support our beliefs about the world around us. The concept of a self and sole existing master fragmented hive mind is another philosophical idea that cannot be proven or disproven. While it is possible to imagine such a scenario, there is no empirical evidence to support it. Moreover, the idea of a shared reality goes beyond just the ability to have conversations and share ideas. We can observe and measure the physical world, which suggests that there is an objective reality that exists independently of our individual experiences."
        },
        {
            "heading": "In conclusion, while philosophical concepts like solipsism and the idea of a self and sole existing master fragmented hive mind are interesting to consider, they are not supported by empirical evidence and are not practical ways to understand the world around us.",
            "text": ""
        },
        {
            "heading": "It is more reasonable to assume that there is an objective reality that exists independently of our individual experiences, and that",
            "text": "we can rely on evidence and reasoning to support our beliefs about the world. GPT-4"
        }
    ],
    "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
    "year": 2023
}