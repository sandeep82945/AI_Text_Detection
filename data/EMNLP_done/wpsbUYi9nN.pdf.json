{
    "abstractText": "Precisely understanding users\u2019 contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a textbased search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user\u2019s real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search. The code is released at https://github.com/ kyriemao/LLM4CS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kelong Mao"
        },
        {
            "affiliations": [],
            "name": "Zhicheng Dou"
        },
        {
            "affiliations": [],
            "name": "Fengran Mo"
        },
        {
            "affiliations": [],
            "name": "Jiewen Hou"
        },
        {
            "affiliations": [],
            "name": "Haonan Chen"
        },
        {
            "affiliations": [],
            "name": "Hongjin Qian"
        }
    ],
    "id": "SP:61845c0d2f268f45b62c03719dca6fab92a489bf",
    "references": [
        {
            "authors": [
                "Raviteja Anantha",
                "Svitlana Vakulenko",
                "Zhucheng Tu",
                "Shayne Longpre",
                "Stephen Pulman",
                "Srinivas Chappidi."
            ],
            "title": "Open-domain question answering goes conversational via question rewriting",
            "venue": "NAACL-HLT, pages 520\u2013534. Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Luiz Henrique Bonifacio",
                "Hugo Abonizio",
                "Marzieh Fadaee",
                "Rodrigo Frassetto Nogueira."
            ],
            "title": "Inpars: Data augmentation for information retrieval using large language models",
            "venue": "CoRR, abs/2202.05144.",
            "year": 2022
        },
        {
            "authors": [
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process-",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Jie Zhao",
                "Anjie Fang",
                "Besnik Fetahu",
                "Rokhlenko Oleg",
                "Shervin Malmasi"
            ],
            "title": "Reinforced question rewriting for conversational question answering",
            "year": 2022
        },
        {
            "authors": [
                "J Shane Culpepper",
                "Fernando Diaz",
                "Mark D Smucker."
            ],
            "title": "Research frontiers in information retrieval: Report from the third strategic workshop on information retrieval in lorne (swirl 2018)",
            "venue": "ACM SIGIR Forum, volume 52, pages 34\u201390. ACM New",
            "year": 2018
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "Deeper text understanding for IR with contextual neural language modeling",
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris,",
            "year": 2019
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Arun Tejasvi Chaganty",
                "Vincent Y Zhao",
                "Aida Amini",
                "Qazi Mamunur Rashid",
                "Mike Green",
                "Kelvin Guu."
            ],
            "title": "Dialog inpainting: Turning documents into dialogs",
            "venue": "International Conference on Machine Learning, pages 4558\u20134586. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Vincent Y. Zhao",
                "Ji Ma",
                "Yi Luan",
                "Jianmo Ni",
                "Jing Lu",
                "Anton Bakalov",
                "Kelvin Guu",
                "Keith B. Hall",
                "Ming-Wei Chang."
            ],
            "title": "Promptagator: Few-shot dense retrieval from 8 examples",
            "venue": "11th International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Jeffrey Dalton",
                "Chenyan Xiong",
                "Jamie Callan."
            ],
            "title": "Trec cast 2019: The conversational assistance track overview",
            "venue": "In Proceedings of TREC.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Dalton",
                "Chenyan Xiong",
                "Jamie Callan."
            ],
            "title": "Cast 2020: The conversational assistance track overview",
            "venue": "In Proceedings of TREC.",
            "year": 2021
        },
        {
            "authors": [
                "Jeffrey Dalton",
                "Chenyan Xiong",
                "Jamie Callan."
            ],
            "title": "Trec cast 2021: The conversational assistance track overview",
            "venue": "In Proceedings of TREC.",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Xueguang Ma",
                "Jimmy Lin",
                "Jamie Callan."
            ],
            "title": "Precise zero-shot dense retrieval without relevance labels",
            "venue": "CoRR, abs/2212.10496.",
            "year": 2022
        },
        {
            "authors": [
                "Vitor Jeronymo",
                "Luiz Henrique Bonifacio",
                "Hugo Abonizio",
                "Marzieh Fadaee",
                "Roberto de Alencar Lotufo",
                "Jakub Zavrel",
                "Rodrigo Frassetto Nogueira"
            ],
            "title": "Inpars-v2: Large language models as efficient dataset generators for information",
            "year": 2023
        },
        {
            "authors": [
                "Dongfu Jiang",
                "Xiang Ren",
                "Bill Yuchen Lin."
            ],
            "title": "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
            "venue": "CoRR, abs/2306.02561.",
            "year": 2023
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over BERT",
            "venue": "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval",
            "year": 2020
        },
        {
            "authors": [
                "Sungdong Kim",
                "Gangwoo Kim"
            ],
            "title": "Saving dense retriever from shortcut dependency in conversational search",
            "year": 2022
        },
        {
            "authors": [
                "Antonios Minas Krasakis",
                "Andrew Yates",
                "Evangelos Kanoulas."
            ],
            "title": "Zero-shot query contextualization for conversational search",
            "venue": "Proceedings of the 45th International ACM SIGIR conference on research and development in Information Retrieval (SIGIR).",
            "year": 2022
        },
        {
            "authors": [
                "Sheng-Chieh Lin",
                "Jheng-Hong Yang",
                "Jimmy Lin."
            ],
            "title": "Contextualized query embeddings for conversational search",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Sheng-Chieh Lin",
                "Jheng-Hong Yang",
                "Rodrigo Nogueira",
                "Ming-Feng Tsai",
                "Chuan-Ju Wang",
                "Jimmy Lin."
            ],
            "title": "Conversational question reformulation via sequence-to-sequence architectures and pretrained language models",
            "venue": "arXiv preprint arXiv:2004.01909.",
            "year": 2020
        },
        {
            "authors": [
                "Sheng-Chieh Lin",
                "Jheng-Hong Yang",
                "Rodrigo Nogueira",
                "Ming-Feng Tsai",
                "Chuan-Ju Wang",
                "Jimmy Lin."
            ],
            "title": "Multi-stage conversational passage retrieval: An approach to fusing term importance estimation and neural query rewriting",
            "venue": "ACM Transactions on",
            "year": 2021
        },
        {
            "authors": [
                "Iain Mackie",
                "Shubham Chatterjee",
                "Jeffrey Dalton."
            ],
            "title": "Generative relevance feedback with large language models",
            "venue": "CoRR, abs/2304.13157.",
            "year": 2023
        },
        {
            "authors": [
                "Kelong Mao",
                "Zhicheng Dou",
                "Bang Liu",
                "Hongjin Qian",
                "Fengran Mo",
                "Xiangli Wu",
                "Xiaohua Cheng",
                "Zhao Cao."
            ],
            "title": "Search-oriented conversational query editing",
            "venue": "ACL (Findings), volume ACL 2023 of Findings of ACL. Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Kelong Mao",
                "Zhicheng Dou",
                "Hongjin Qian."
            ],
            "title": "Curriculum contrastive context denoising for fewshot conversational dense retrieval",
            "venue": "Proceedings of the 45th International ACM SIGIR conference on research and development in Information Retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Kelong Mao",
                "Zhicheng Dou",
                "Hongjin Qian",
                "Fengran Mo",
                "Xiaohua Cheng",
                "Zhao Cao."
            ],
            "title": "Convtrans: Transforming web search sessions for conversational dense retrieval",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Kelong Mao",
                "Hongjin Qian",
                "Fengran Mo",
                "Zhicheng Dou",
                "Bang Liu",
                "Xiaohua Cheng",
                "Zhao Cao."
            ],
            "title": "Learning denoised and interpretable session representation for conversational search",
            "venue": "Proceedings of the ACM Web Conference, pages 3193\u20133202.",
            "year": 2023
        },
        {
            "authors": [
                "Yuning Mao",
                "Pengcheng He",
                "Xiaodong Liu",
                "Yelong Shen",
                "Jianfeng Gao",
                "Jiawei Han",
                "Weizhu Chen."
            ],
            "title": "Generation-augmented retrieval for opendomain question answering",
            "venue": "ACL/IJCNLP (1), pages 4089\u20134100. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Fengran Mo",
                "Kelong Mao",
                "Yutao Zhu",
                "Yihong Wu",
                "Kaiyu Huang",
                "Jian-Yun Nie."
            ],
            "title": "ConvGQR: generative query reformulation for conversational search",
            "venue": "ACL, volume ACL 2023. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Fengran Mo",
                "Jian-Yun Nie",
                "Kaiyu Huang",
                "Kelong Mao",
                "Yutao Zhu",
                "Peng Li",
                "Yang Liu."
            ],
            "title": "Learning to relate to previous turns in conversational search",
            "venue": "29th ACM SIGKDD Conference On Knowledge Discover and Data Mining (SIGKDD).",
            "year": 2023
        },
        {
            "authors": [
                "Hongjin Qian",
                "Zhicheng Dou."
            ],
            "title": "Explicit query rewriting for conversational dense retrieval",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Tao Shen",
                "Guodong Long",
                "Xiubo Geng",
                "Chongyang Tao",
                "Tianyi Zhou",
                "Daxin Jiang."
            ],
            "title": "Large language models are strong zero-shot retriever",
            "venue": "CoRR, abs/2304.14233.",
            "year": 2023
        },
        {
            "authors": [
                "Weiwei Sun",
                "Lingyong Yan",
                "Xinyu Ma",
                "Pengjie Ren",
                "Dawei Yin",
                "Zhaochun Ren."
            ],
            "title": "Is chatgpt good at search? investigating large language models as re-ranking agent",
            "venue": "CoRR, abs/2304.09542.",
            "year": 2023
        },
        {
            "authors": [
                "Lamm",
                "Viktoriya Kuzmina",
                "Joe Fenton",
                "Aaron Cohen",
                "Rachel Bernstein",
                "Ray Kurzweil",
                "Blaise Ag\u00fcera y Arcas",
                "Claire Cui",
                "Marian Croak",
                "Ed H. Chi",
                "Quoc Le."
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "CoRR, abs/2201.08239.",
            "year": 2022
        },
        {
            "authors": [
                "Svitlana Vakulenko",
                "Shayne Longpre",
                "Zhucheng Tu",
                "Raviteja Anantha."
            ],
            "title": "Question rewriting for conversational question answering",
            "venue": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining (WSDM), pages 355\u2013363.",
            "year": 2021
        },
        {
            "authors": [
                "Svitlana Vakulenko",
                "Nikos Voskarides",
                "Zhucheng Tu",
                "Shayne Longpre."
            ],
            "title": "A comparison of question rewriting methods for conversational passage retrieval",
            "venue": "ECIR (2), volume 12657 of Lecture Notes in Computer Science, pages 418\u2013424. Springer.",
            "year": 2021
        },
        {
            "authors": [
                "Christophe Van Gysel",
                "Maarten de Rijke."
            ],
            "title": "Pytrec_eval: An extremely fast python interface to trec_eval",
            "venue": "SIGIR. ACM.",
            "year": 2018
        },
        {
            "authors": [
                "Nikos Voskarides",
                "Dan Li",
                "Pengjie Ren",
                "Evangelos Kanoulas",
                "Maarten de Rijke."
            ],
            "title": "Query resolution for conversational search with limited supervision",
            "venue": "Proceedings of the 43rd International ACM SIGIR conference on research and development in",
            "year": 2020
        },
        {
            "authors": [
                "Liang Wang",
                "Nan Yang",
                "Furu Wei."
            ],
            "title": "Query2doc: Query expansion with large language models",
            "venue": "CoRR, abs/2303.07678.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "11th International Conference on Learning Representations, ICLR 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "Advances in neural information processing systems.",
            "year": 2020
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yi Luan",
                "Hannah Rashkin",
                "David Reitter",
                "Gaurav Singh Tomar"
            ],
            "title": "Conqrr: Conversational query rewriting for retrieval with reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Lee Xiong",
                "Chenyan Xiong",
                "Ye Li",
                "Kwok-Fung Tang",
                "Jialin Liu",
                "Paul N. Bennett",
                "Junaid Ahmed",
                "Arnold Overwijk."
            ],
            "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
            "venue": "9th International Conference on Learning",
            "year": 2021
        },
        {
            "authors": [
                "Shi Yu",
                "Jiahua Liu",
                "Jingqin Yang",
                "Chenyan Xiong",
                "Paul Bennett",
                "Jianfeng Gao",
                "Zhiyuan Liu."
            ],
            "title": "Fewshot generative conversational query rewriting",
            "venue": "Proceedings of the 43rd International ACM SIGIR conference on research and development in Informa-",
            "year": 2020
        },
        {
            "authors": [
                "Shi Yu",
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Tao Feng",
                "Zhiyuan Liu."
            ],
            "title": "Few-shot conversational dense retrieval",
            "venue": "Proceedings of the 44th International ACM SIGIR conference on research and development in Information Retrieval (SIGIR).",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "11th International Con-",
            "year": 2023
        },
        {
            "authors": [
                "Yutao Zhu",
                "Huaying Yuan",
                "Shuting Wang",
                "Jiongnan Liu",
                "Wenhan Liu",
                "Chenlong Deng",
                "Zhicheng Dou",
                "Ji-Rong Wen."
            ],
            "title": "Large language models for information retrieval: A survey",
            "venue": "arXiv preprint arXiv:2308.07107.",
            "year": 2023
        },
        {
            "authors": [
                "Noah Ziems",
                "Wenhao Yu",
                "Zhihan Zhang",
                "Meng Jiang."
            ],
            "title": "Large language models are built-in autoregressive search engines",
            "venue": "ACL (Findings), volume ACL 2023 of Findings of ACL. Association for Computational Linguistics.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Conversational search has been expected to be the next generation of search paradigms (Culpepper et al., 2018). It supports search via conversation to provide users with more accurate and intuitive search results and a much more user-friendly search experience. Unlike using traditional search engines which mainly process keyword queries, users could imagine the conversational search system as a knowledgeable human expert and directly start a\n\u2217Corresponding author.\nmulti-turn conversation with it in natural languages to solve their questions. However, one of the main challenges for this beautiful vision is that the users\u2019 queries may contain some linguistic problems (e.g., omissions and coreference) and it becomes much harder to capture their real search intent under the multi-turn conversation context (Dalton et al., 2021; Mao et al., 2022a).\nTo achieve conversational search, an intuitive method known as Conversational Query Rewriting (CQR) involves using a rewriting model to transform the current query into a de-contextualized form. Subsequently, any ad-hoc search models can be seamlessly applied for retrieval purposes. Given that existing ad-hoc search models can be reused directly, CQR demonstrates substantial practical value for industries in quickly initializing their conversational search engines. Another type of method, Conversational Dense Retrieval (CDR), tries to learn a conversational dense retriever to encode the user\u2019s real search intent and passages into latent representations and performs dense retrieval. In contrast to the two-step CQR method, where the rewriter is difficult to be directly optimized towards search (Yu et al., 2021; Mao et al., 2023a), the conversational dense retriever can naturally learn from session-passage relevance signals.\nHowever, as conversational search sessions are much more diverse and long-tailed (Mao et al., 2022b; Dai et al., 2022; Mo et al., 2023a), existing CQR and CDR methods trained on limited data still show unsatisfactory performance, especially on more complex conversational search sessions. Many studies (Vakulenko et al., 2021b; Lin et al., 2021a; Qian and Dou, 2022; Krasakis et al., 2022) have demonstrated the performance advantages of using de-contextualized human rewrites on sessions which have complex response dependency. Also, as reported in the public TREC CAsT 2021 benchmark (Dalton et al., 2022), existing methods still suffer from significant degradation in their ef-\nfectiveness as conversations become longer. Recently, large language models (LLMs) have shown amazing capabilities for text generation and conversation understanding (Brown et al., 2020; Wei et al., 2022; Thoppilan et al., 2022; Zhu et al., 2023). In the field of information retrieval (IR), LLMs have also been successfully utilized to enhance relevance modeling via various techniques such as query generation (Bonifacio et al., 2022; Dai et al., 2023), query expansion (Wang et al., 2023a), document prediction (Gao et al., 2022; Mackie et al., 2023), etc. Inspired by the strong performance of LLMs in conversation and IR, we try to investigate how LLMs can be adapted to precisely grasp users\u2019 contextual search intent for conversational search.\nIn this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLM as a search intent interpreter to facilitate conversational search. Specifically, we first prompt LLM to generate both short query rewrites and longer hypothetical responses in multiple perspectives and then aggregate these generated contents into an integrated representation that robustly represents the user\u2019s real search intent. Under our framework, we propose three specific prompting methods and aggregation methods, and conduct extensive evaluations on three widely used conversational search benchmarks, including CAsT19 (Dalton et al., 2020), CAsT-20 (Dalton et al., 2021), and CAsT-21 (Dalton et al., 2022)), to comprehensively investigate the effectiveness of LLMs for conversational search.\nIn general, our framework has two main advantages. First, by leveraging the powerful contextual understanding and generation abilities of large language models, we show that additionally generating hypothetical responses to explicitly supplement more plausible search intents underlying the short rewrite can significantly improve the search performance. Second, we show that properly aggregating multiple rewrites and hypothetical responses can effectively filter out incorrect search intents and enhance the reasonable ones, leading to better search performance and robustness.\nOverall, our main contributions are:\n\u2022 We propose a prompting framework and design three tailored prompting methods to leverage large language models for conversational search, which effectively circumvents the serious data scarcity problem faced by the con-\nversational search field.\n\u2022 We show that additionally generating hypothetical responses and properly aggregating multiple generated results are crucial for improving search performance.\n\u2022 We demonstrate the exceptional effectiveness of LLMs for conversational search through both automatic and human evaluations, where the best method in our LLM4CS achieves remarkable improvements in search performance over state-of-the-art CQR and CDR baselines, surpassing even human rewrites."
        },
        {
            "heading": "2 Related Work",
            "text": "Conversational Search. Conversational search is an evolving field that involves retrieving relevant information based on multi-turn dialogues with users. To achieve conversational search, two main methods have been developed: conversational query rewriting and conversational dense retrieval. Conversational query rewriting converts the conversational search problem into an ad-hoc search problem by reformulating the search session into a standalone query rewrite. Existing methods try to select useful tokens from the conversation context (Voskarides et al., 2020; Lin et al., 2021b)\nor train a generative rewriter based on the pairs of sessions and rewrites (Lin et al., 2020; Yu et al., 2020; Vakulenko et al., 2021a). To make the rewriting process aware of the downstream retrieval process, some studies propose to adopt reinforcement learning (Wu et al., 2022; Chen et al., 2022) or enhance the learning of rewriter with ranking signals (Mao et al., 2023a; Mo et al., 2023a). On the other hand, conversational dense retrieval (Yu et al., 2021) directly encodes the whole conversational search session to perform end-to-end dense retrieval. Existing methods mainly try to improve the session representation through context denoising (Mao et al., 2022a; Krasakis et al., 2022; Mo et al., 2023b; Mao et al., 2023b), data augmentation (Lin et al., 2021a; Mao et al., 2022b; Dai et al., 2022), and hard negative mining (Kim and Kim, 2022).\nIR with LLMs. Due to the revolutionary natural language understanding and generation abilities, LLMs are attracting more and more attention from the IR community. LLMs have been leveraged to enhance the relevance modeling of retrieval through query generation (Bonifacio et al., 2022; Jeronymo et al., 2023; Dai et al., 2023), query expansion (Wang et al., 2023a), document prediction (Gao et al., 2022; Mackie et al., 2023), etc. Besides, Shen et al. (2023) proposed to first use the retriever to enhance the generation of LLM and then use the generated content to augment the original search query for better retrieval. Ziems et al. (2023) treated LLM as a built-in search engine to retrieve documents based on the generated URL. There are also some works leveraging LLM to perform re-ranking (Sun et al., 2023; Jiang et al., 2023). Different from previous studies, in this paper, we propose the LLM4CS framework that focuses on studying how LLM can be well utilized to capture the user\u2019s contextual search intent to facilitate conversational search."
        },
        {
            "heading": "3 LLM4CS: Prompting Large Language Models for Conversational Search",
            "text": "In this section, we introduce our LLM4CS framework, which leverages LLM as a text-based search intent interpreter to facilitate conversational search. Figure 1 shows an overview of LLM4CS. In the following, we first describe our task formulation of conversational search, and then we elaborate on the specific prompting methods and aggregation\nmethods integrated into the framework. Finally, we introduce the retrieval process."
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "We focus on the task of conversational passage retrieval, which is the crucial first step of conversational search that helps the model access the right evidence knowledge. Given the user query qt and the conversation context Ct = (q1, r1, ..., qt\u22121, rt\u22121) of the current turn t, where qi and ri denote the user query and the system response of the historical i-th turn, our goal is to retrieve passages that are relevant to satisfy the user\u2019s real search intent of the current turn."
        },
        {
            "heading": "3.2 Prompting Methods",
            "text": "The prompt follows the formulation of [Instruction, Demonstrations, Input], where Input is composed of the query qt and the conversation context Ct of the current turn t. Figure 4 shows a general illustration of the prompt construction.1 Specifically, we design and explore three prompting methods, including Rewriting (REW), Rewriting-Then-Response (RTR), and RewritingAnd-Response (RAR), in our LLM4CS framework."
        },
        {
            "heading": "3.2.1 Rewriting Prompt (REW)",
            "text": "In this prompting method, we directly treat LLM as a well-trained conversational query rewriter and prompt it to generate rewrites. Only the red part of Figure 4 is enabled. Although straightforward, we show in Section 4.5 that this simple prompting method has been able to achieve quite a strong search performance compared to existing baselines."
        },
        {
            "heading": "3.2.2 Rewriting-Then-Response (RTR)",
            "text": "Recently, a few studies (Mao et al., 2021; Gao et al., 2022; Yu et al., 2023; Mackie et al., 2023) have shown that generating hypothetical responses for search queries can often bring positive improvements in retrieval performance. Inspired by them, in addition to prompting LLM to generate rewrites, we continue to utilize the generated rewrites to further prompt LLM to generate hypothetical responses that may contain relevant information to answer the current question. The orange part and the blue part of Figure 4 are enabled. Specifically, we incorporate the pre-generated rewrite (i.e., the orange part) into the Input field of the prompt and\n1We put this figure in Appendix A due to the space limitation. See our open-sourced code for the full prompt of each prompting method.\nthen prompt LLM to generate informative hypothetical responses by referring to the rewrite."
        },
        {
            "heading": "3.2.3 Rewriting-And-Response (RAR)",
            "text": "Instead of generating rewrites and hypothetical responses in a two-stage manner, we can also generate them all at once with the red part and the blue part of Figure 4 being enabled. We try to explore whether this one-stage generation could lead to better consistency and accuracy between the generated rewrites and responses, compared with the two-step RTR method."
        },
        {
            "heading": "3.2.4 Incorporating Chain-of-Thought",
            "text": "Chain-of-thought (CoT) (Wei et al., 2020) induces the large language models to decompose a reasoning task into multiple intermediate steps which can unlock their stronger reasoning abilities. In this work, we also investigate whether incorporating the chain-of-thought of reasoning the user\u2019s real search intent could improve the quality of rewrite and response generation.\nSpecifically, as shown in the green part of Figure 4, we manually write the chain-of-thought for each turn of the demonstration, which reflects how humans infer the user\u2019s real search intent of the current turn based on the historical conversation context. When generating, we instruct LLM to first generate the chain-of-thought before generating rewrites (and responses). We investigate the effects of our proposed CoT tailored to the reasoning of contextual search intent in Section 4.6."
        },
        {
            "heading": "3.3 Content Aggregation",
            "text": "After prompting LLM multiple times to generate multiple rewrites and hypothetical responses, we then aggregate these generated contents into an integrated representation to represent the user\u2019s complete search intent for search. Let us consider that we have generated N query rewrites Q = (q\u03021, ..., q\u0302N ) and M hypothetical responses R = (r\u0302i1, ..., r\u0302iM ) for each rewrite q\u0302i, sorted by their generation probabilities from high to low2. Note that in RAR prompting, the rewrites and the hypothetical responses are always generated in pairs (i.e., M = 1). While in RTR prompting, one rewrite can have M hypothetical responses since they are generated in a two-stage manner. Next, we utilize a dual well-trained ad-hoc retriever3 f\n2That is, the generation probability orders are: P (q\u03021) \u2265 ... \u2265 P (q\u0302N ) and P (r\u0302i1) \u2265 ... \u2265 P (r\u0302iM ).\n3The parameters of the query encoder and the passage encoder are shared.\n(e.g, ANCE (Xiong et al., 2021)) to encode each of them into a high-dimensional intent vector and aggregate these intent vectors into one final search intent vector s. Specifically, we design and explore the following three aggregation methods, including MaxProb, Self-Consistency (SC), and Mean, in our LLM4CS framework."
        },
        {
            "heading": "3.3.1 MaxProb",
            "text": "We directly use the rewrite and the hypothetical response that have the highest generation probabilities. Therefore, compared with the other two aggregation methods that will be introduced later, MaxProb is highly efficient since it actually does not require multiple generations.\nFormally, for REW prompting:\ns = f(q\u03021). (1)\nFor the RTR and RAR prompting methods, we mix the rewrite and hypothetical response vectors:\ns = f(q\u03021) + f(r\u030211)\n2 . (2)"
        },
        {
            "heading": "3.3.2 Self-Consistency (SC)",
            "text": "The multiple generated rewrites and hypothetical responses may express different search intents but only some of them are correct. To obtain a more reasonable and consistent search intent representation, we extend the self-consistency prompting method (Wang et al., 2023b), which was initially designed for reasoning tasks with predetermined answer sets, to our contextual search intent understanding task, which lacks a fixed standard answer. To be specific, we select the intent vector that is the most similar to the cluster center of all intent vectors as the final search intent vector, since it represents the most popular search intent overall.\nFormally, for REW prompting:\nq\u0302\u2217 = 1\nN N\u2211 i=1 f(q\u0302i), (3)\ns = argmax f(q\u0302i)\nf(q\u0302i) \u22a4 \u00b7 q\u0302\u2217, (4)\nwhere q\u0302\u2217 is the cluster center vector and \u00b7 denotes the dot product that measures the similarity.\nFor RTR prompting, we first select the intent vector f(q\u0302k) and then select the intent vector f(r\u0302kz) from all hypothetical responses generated based on\nthe selected rewrite q\u0302k:\nk = argmax i\nf(q\u0302i) \u22a4 \u00b7 q\u0302\u2217, (5)\nr\u0302\u2217k = 1\nM M\u2211 j=1 f(r\u0302kj), (6)\nz = argmax j\nf(r\u0302kj) \u22a4 \u00b7 r\u0302\u2217k, (7)\ns = f(q\u0302k) + f(r\u0302kz)\n2 , (8)\nwhere k and z are the finally selected indexes of the rewrite and the response, respectively.\nThe aggregation for RAR prompting is similar to RTR prompting, but it does not need response selection since there is only one hypothetical response for each rewrite:\ns = f(q\u0302k) + f(r\u0302k1)\n2 . (9)"
        },
        {
            "heading": "3.3.3 Mean",
            "text": "We average all the rewrite vectors and the corresponding hypothetical response vectors.\nFor REW prompting:\ns = 1\nN N\u2211 i=1 f(q\u0302i). (10)\nFor the RTR and RAR prompting methods:\ns =\n\u2211N i=1[f(q\u0302i) + \u2211M j=1 f(r\u0302ij)]\nN \u2217 (1 +M) . (11)\nCompared with MaxProb and Self-Consistency, the Mean aggregation comprehensively considers more diverse search intent information from all sources. It leverages the collaborative power to enhance the popular intents, but also supplements plausible intents that are missing in a single rewrite or a hypothetical response."
        },
        {
            "heading": "3.4 Retrieval",
            "text": "All candidate passages are encoded into passage vectors using the same retriever f . At search time, we return the passages that are most similar to the final search intent vector s as the retrieval results."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Metrics",
            "text": "We carry out extensive experiments on three widely used conversational search datasets: CAsT-19 (Dalton et al., 2020), CAsT-20 (Dalton et al., 2021), and\nCAsT-21 (Dalton et al., 2022), which are curated by the human experts of TREC Conversational Assistance Track (CAsT). Each CAsT dataset has dozens of information-seeking conversations comprising hundreds of turns. CAsT-19 and CAsT-20 share the same retrieval corpora while CAsT-21 has a different one. In contrast, CAsT-20 and CAsT-21 have a more complex session structure than CAsT-19 as their questions may refer to previous responses. All three datasets provide human rewrites and passagelevel (or document-level) relevance judgments labeled by TREC experts. Table 1 summarizes the basic dataset statistics.4\nFollowing previous work (Dalton et al., 2020, 2021; Yu et al., 2021; Mao et al., 2022a), we adopt Mean Reciprocal Rank (MRR), NDCG@3, and Recall@100 as our evaluation metrics and calculate them using pytrec_eval tool (Van Gysel and de Rijke, 2018). We deem relevance scale \u2265 2 as positive for MRR on CAsT-20 and CAsT-21. For CAsT-21, we split the documents into passages and score each document based on its highestscored passage (i.e., MaxP (Dai and Callan, 2019)). We conduct the statistical significance tests using paired t-tests at p < 0.05 level."
        },
        {
            "heading": "4.2 Implementation details",
            "text": "We use the OpenAI gpt3.5-turbo-16k as our LLM. The decoding temperature is set to 0.7. We randomly select three conversations from the CAsT225 dataset for demonstration. CAsT-22 is a new conversational search dataset also proposed by TREC CAsT, but only its conversations are released6 and the relevance judgments have not been made public. Therefore, it cannot be used for evaluation and we just use it for demonstration. For REW prompting, we set N = 5. For RTR prompting, we set N = 1 and M = 5. For RAR prompting, we set N = 5, and M is naturally set to 1. Following previous studies (Yu et al., 2021; Mao\n4Only the turns that have relevance labels are counted. 5https://github.com/daltonj/treccastweb/tree/\nmaster/2022 6Until the submission deadline of EMNLP 2023.\net al., 2022a,b; Mo et al., 2023a), we adopt the ANCE (Xiong et al., 2021) checkpoint pre-trained on the MSMARCO dataset as our ad-hoc retriever f . We uniformly truncate the lengths of queries (or rewrites), passages, and hypothetical responses into 64, 256, and 256."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We compare our few-shot LLM4CS against the following six conversational search systems:\n(1) T5QR (Lin et al., 2020): A T5 (Raffel et al., 2020)-based conversational query rewriter trained with the human rewrites.\n(2) ConvDR (Yu et al., 2021): A conversational dense retriever fine-tuned from an ad-hoc retriever by mimicking the representations of human rewrites.\n(3) COTED (Mao et al., 2022a): An improved version of ConvDR (Yu et al., 2021) which incorporates a curriculum learning-based context denoising objective.\n(4) ZeCo (Krasakis et al., 2022): A variant of ColBERT (Khattab and Zaharia, 2020) that matches only the contextualized terms of the current query with passages to perform zero-shot conversational search.\n(5) CRDR (Qian and Dou, 2022): A conversational dense retrieval method where the dense retrieval part is enhanced by the distant supervision from query rewriting in a unified framework.\n(6) ConvGQR (Mo et al., 2023a): A query reformulation framework that combines query rewriting with generative query expansion.\nT5QR, CRDR, and ConvGQR are trained on the training sessions of QReCC (Anantha et al., 2021), which is a large-scale conversational question answering dataset. The performances of ConvDR and COTED are reported in the few-shot setting using 5-fold cross-validation according to their original papers. We also present the performance of using human rewrites for reference. Note that the same ANCE checkpoint is used to perform dense retrieval for all baselines except ZeCo to ensure fair comparisons."
        },
        {
            "heading": "4.4 Main Results",
            "text": "The overall performance comparisons are presented in Table 2. The reported performance of LLM4CS results from the combination of the RAR prompting method, the Mean aggregation method, and our tailored CoT, which shows to be the most effective combination. We thoroughly investigate the effects\nof using different prompting and aggregation methods in Section 4.5 and investigate the effects of the incorporation of CoT in Section 4.6.\nFrom Table 2, we observe that LLM4CS outperforms all the compared baselines in terms of search performance. Specifically, LLM4CS exhibits a relative improvement of over 18% compared to the second-best results on the more challenging CAsT20 and CAsT-21 datasets across all metrics. In particular, even compared to using human rewrites, our LLM4CS can still achieve better results on most metrics, except for the Recall@100 of CAsT19 and NDCG@3 of CAsT-21. These significant improvements, which are unprecedented in prior research, demonstrate the strong superiority of our LLM4CS over existing methods and underscore the vast potential of using large language models for conversational search."
        },
        {
            "heading": "4.5 Effects of Different Prompting Methods and Aggregation Methods",
            "text": "We present a comparison of NDCG@3 performance across various prompting and aggregation methods (excluding the incorporation of CoT) in Table 3. Our findings are as follows:\nFirst, the RAR and RTR prompting methods clearly outperform the REW prompting, demonstrating that the generated hypothetical responses can effectively supplement the short query rewrite to improve retrieval performance. However, even the simple REW prompting can also achieve quite competitive performance compared to existing baselines, particularly on the more challenging CAsT-20 and CAsT-21 datasets, where it shows significant superiority (e.g., 0.380 vs. 0.350 on CAsT20 and 0.465 vs. 0.385 on CAsT-21). These positive results further highlight the significant advantages of utilizing LLM for conversational search.\nSecond, in terms of aggregation methods, both Mean and SC consistently outperform MaxProb. These results indicate that depending solely on the top prediction of the language model may not provide sufficient reliability. Instead, utilizing the collective strength of multiple results proves to be a better choice. Additionally, we observe that the Mean aggregation method, which fuses all generated contents into the final search intent vector (Equation 11), does not consistently outperform SC (e.g., on CAsT-20), which actually only fuses one rewrite and one response (Equation 8). This suggests that taking into account more generations\nmay not always be beneficial, and a careful selection among them could be helpful to achieve improved results."
        },
        {
            "heading": "4.6 Effects of Chain-of-Thought",
            "text": "We show the ablation results of our tailored chainof-thought in Figure 2. We also provide a real\nexample to show how our CoT takes effect in Appendix B.1. From the results, we observe that:\nIncorporating our chain-of-thought into all prompting and aggregation methods generally improves search performance. This demonstrates the efficacy of our chain-of-thought in guiding the large language model towards a correct understanding of\nthe user\u2019s contextual search intent. In contrast, the improvements are particularly notable for the REW prompting method compared to the RTR and RAR prompting methods. It appears that the introduction of multiple hypothetical responses diminishes the impact of the chain-ofthought. This could be attributed to the fact that including multiple hypothetical responses significantly boosts the quality and robustness of the final search intent vector, thereby reducing the prominence of the chain-of-thought in enhancing search performance."
        },
        {
            "heading": "5 Human Evaluation",
            "text": "The retrieval performance is influenced by the adhoc retriever used, which implies that automatic search evaluation metrics may not fully reflect the model\u2019s capability to understand contextual search intent. Sometimes, two different rewrites can yield significantly different retrieval scores, even though they both accurately represent the user\u2019s real search intent. To better investigate the contextual search intent understanding ability of LLM, we perform a fine-grained human evaluation on the rewrites generated by our LLM4CS (REW + MaxProb).\nSpecifically, we manually compare each model\u2019s rewrite with the corresponding human rewrite and label it with one of the following four categories: (1) Good-H: The model\u2019s rewrite is nearly the same as the human rewrite. (2) Good-A: The expression of the model\u2019s rewrite is different from the human rewrite but it also successfully conveys the user\u2019s real search intent. (3) Bad-C: the rewrite has coreference errors. (4) Bad-O: the rewrite omits important contextual information or has other types of errors. Furthermore, we apply the same principle to label the rewrites of T5QR for comparison purposes. A few examples of such categorization are presented in Appendix B.2.\nThe results of the human evaluation are shown in Figure 3, where we observe that:\n(1) From a human perspective, 85.5%, 89.4%, and 84.8% of the rewrites of LLM4CS success-\nfully convey the user\u2019s real search intent for CAsT19, CAsT-20, and CAsT-21, respectively. In contrast, the corresponding percentages for T5QR are merely 75.1%, 62.0%, and 58.6%. Such a high rewriting accuracy of LLM4CS further demonstrates the strong ability of LLM for contextual search intent understanding.\n(2) In the case of CAsT-20 and CAsT-21, a significantly higher percentage of rewrites are labeled as Good-A, in contrast to CAsT-19, where the majority of good rewrites closely resemble the human rewrites. This can be attributed to the higher complexity of the session structure and questions in CAsT-20 and CAsT-21 compared to CAsT-19, which allows for greater freedom in expressing the same search intent.\n(3) The rewrites generated by LLM4CS exhibit coreference errors in less than 3% of the cases, whereas T5QR\u2019s rewrites contain coreference errors in approximately 10% of the cases. This observation highlights the exceptional capability of LLM in addressing coreference issues."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we present a simple yet effective prompting framework (i.e., LLM4CS) that leverages LLMs for conversational search. Our framework generates multiple query rewrites and hypothetical responses using tailored prompting methods and aggregates them to robustly represent the user\u2019s contextual search intent. Through extensive automatic and human evaluations on three CAsT datasets, we demonstrate its remarkable performance for conversational search. Our study highlights the vast potential of LLMs in conversational search and takes an important initial step in advancing this promising direction. Future research will focus on refining and extending the LLM4CS framework to explore better ways of generation to facilitate search, improving aggregation techniques, optimizing the LLM-retriever interaction, and incorporating reranking strategies.\nLimitations\nOur work shows that generating multiple rewrites and hypothetical responses and properly aggregating them can effectively improve search performance. However, this requires invoking LLM multiple times, resulting in a higher time cost for retrieval. Due to the relatively high generation latency of LLM, the resulting query latency would be intolerable for users when compared to conventional search engines. A promising approach is to design better prompts capable of obtaining all informative content in one generation, thereby significantly improving query latency. Another limitation is that, similar to the typical disadvantages of CQR methods, the generation process of LLM lacks awareness of the downstream retrieval process. Exploring the utilization of ranking signals to enhance LLM generation would be a compelling direction for future research of conversational search."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the National Natural Science Foundation of China No. 62272467, Public Computing Cloud, Renmin University of China, and Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the \u201cDouble-First Class\u201d Initiative, Renmin University of China, and the Outstanding Innovative Talents Cultivation Funded Programs 2024 of Renmin University of China. The work was partially done at Beijing Key Laboratory of Big Data Management and Analysis Methods."
        },
        {
            "heading": "A Prompt of LLM4CS",
            "text": "Figure 4 shows a general illustration of the prompt of LLM4CS. The prompt consist of three parts, which are Instruction, Demonstration, and Input. The red part is for REW prompting, the blue part is for the RTR and RAR promptings, and the orange part is for RTR prompting. The green part is for our designed chain-of-thought."
        },
        {
            "heading": "B Case Study",
            "text": "B.1 Examples of Chain-of-Thought The example in Table 4 shows how our CoT takes effect. The CoT and Our Rewrite fields are generated by LLM4CS (REW + CoT). We can find that the generated CoT effectively illustrates the rationale behind the rewriting process. Please refer to our anonymously open-sourced repository for more examples.\nB.2 Examples of Human Evaluation An example for each category is shown in Table 5. Please refer to our anonymously open-sourced repository for more examples."
        }
    ],
    "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
    "year": 2023
}