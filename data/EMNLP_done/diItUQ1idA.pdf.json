{
    "abstractText": "ive Open Information Extraction Kevin Pei, Ishan Jindal, Kevin Chen-Chuan Chang University of Illinois at Urbana-Champaign, IBM Research {kspei2, kcchang}@illinois.edu, ishan.jindal@ibm.com",
    "authors": [
        {
            "affiliations": [],
            "name": "Kevin Pei"
        },
        {
            "affiliations": [],
            "name": "Ishan Jindal"
        },
        {
            "affiliations": [],
            "name": "Kevin Chen-Chuan Chang"
        }
    ],
    "id": "SP:e7e36986d3ab6c273e326b003239f9d776bcaf1f",
    "references": [
        {
            "authors": [
                "Gabor Angeli",
                "Melvin Jose Johnson Premkumar",
                "Christopher D Manning."
            ],
            "title": "Leveraging linguistic structure for open domain information extraction",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Nguyen Bach",
                "Sameer Badaskar."
            ],
            "title": "A review of relation extraction",
            "venue": "Literature review for Language and Statistics II, 2:1\u201315.",
            "year": 2007
        },
        {
            "authors": [
                "Sangnie Bhardwaj",
                "Samarth Aggarwal",
                "Mausam Mausam."
            ],
            "title": "Carb: A crowdsourced benchmark for open ie",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Lei Cui",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Neural open information extraction",
            "venue": "arXiv preprint arXiv:1805.04270.",
            "year": 2018
        },
        {
            "authors": [
                "Ond\u0159ej Du\u0161ek",
                "Zden\u011bk Kasner."
            ],
            "title": "Evaluating semantic accuracy of data-to-text generation with natural language inference",
            "venue": "arXiv preprint arXiv:2011.10819.",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Edunov",
                "Myle Ott",
                "Michael Auli",
                "David Grangier."
            ],
            "title": "Understanding back-translation at scale",
            "venue": "arXiv preprint arXiv:1808.09381.",
            "year": 2018
        },
        {
            "authors": [
                "Oren Etzioni",
                "Michele Banko",
                "Stephen Soderland",
                "Daniel S Weld."
            ],
            "title": "Open information extraction from the web",
            "venue": "Communications of the ACM, 51(12):68\u201374.",
            "year": 2008
        },
        {
            "authors": [
                "Anthony Fader",
                "Luke Zettlemoyer",
                "Oren Etzioni."
            ],
            "title": "Paraphrase-driven learning for open question answering",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1608\u20131618.",
            "year": 2013
        },
        {
            "authors": [
                "Anthony Fader",
                "Luke Zettlemoyer",
                "Oren Etzioni."
            ],
            "title": "Open question answering over curated and extracted knowledge bases",
            "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165.",
            "year": 2014
        },
        {
            "authors": [
                "Yang Gao",
                "Nicolo Colombo",
                "Wei Wang."
            ],
            "title": "Adapting by pruning: A case study on bert",
            "venue": "arXiv preprint arXiv:2105.03343.",
            "year": 2021
        },
        {
            "authors": [
                "Jiabao Han",
                "Hongzhi Wang."
            ],
            "title": "Generative adversarial networks for open information extraction",
            "venue": "Advances in Computational Intelligence, 1(4):1\u201311.",
            "year": 2021
        },
        {
            "authors": [
                "Martin Josifoski",
                "Nicola De Cao",
                "Maxime Peyrard",
                "Robert West."
            ],
            "title": "Genie: generative information extraction",
            "venue": "arXiv preprint arXiv:2112.08340.",
            "year": 2021
        },
        {
            "authors": [
                "Keshav Kolluru",
                "Vaibhav Adlakha",
                "Samarth Aggarwal",
                "Soumen Chakrabarti"
            ],
            "title": "2020a. Openie6: Iterative grid labeling and coordination analysis for open information",
            "year": 2010
        },
        {
            "authors": [
                "Keshav Kolluru",
                "Samarth Aggarwal",
                "Vipul Rathore",
                "Soumen Chakrabarti"
            ],
            "title": "2020b. Imojie: Iterative memory-based joint open information extraction",
            "venue": "arXiv preprint arXiv:2005.08178",
            "year": 2005
        },
        {
            "authors": [
                "Keshav Kolluru",
                "Muqeeth Mohammed",
                "Shubham Mittal",
                "Soumen Chakrabarti"
            ],
            "title": "Alignmentaugmented consistent translation for multilingual open information extraction",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Hermann Kroll",
                "Jan Pirklbauer",
                "Wolf-Tilo Balke."
            ],
            "title": "A toolbox for the nearly-unsupervised construction of digital library knowledge graphs",
            "venue": "Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in.",
            "year": 2021
        },
        {
            "authors": [
                "William L\u00e9chelle",
                "Fabrizio Gotti",
                "Philippe Langlais."
            ],
            "title": "Wire57: A fine-grained benchmark for open information extraction",
            "venue": "arXiv preprint arXiv:1809.08962.",
            "year": 2018
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Keming Lu",
                "I Hsu",
                "Wenxuan Zhou",
                "Mingyu Derek Ma",
                "Muhao Chen"
            ],
            "title": "Summarization as indirect supervision for relation extraction",
            "venue": "arXiv preprint arXiv:2205.09837",
            "year": 2022
        },
        {
            "authors": [
                "Xiaolu Lu",
                "Soumajit Pramanik",
                "Rishiraj Saha Roy",
                "Abdalghani Abujabal",
                "Yafang Wang",
                "Gerhard Weikum."
            ],
            "title": "Answering complex questions by joining multi-document evidence with quasi knowledge graphs",
            "venue": "Proceedings of the 42nd Interna-",
            "year": 2019
        },
        {
            "authors": [
                "Mausam",
                "Michael Schmitz",
                "Stephen Soderland",
                "Robert Bart",
                "Oren Etzioni."
            ],
            "title": "Open language learning for information extraction",
            "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-",
            "year": 2012
        },
        {
            "authors": [
                "Nathan Ng",
                "Kyra Yee",
                "Alexei Baevski",
                "Myle Ott",
                "Michael Auli",
                "Sergey Edunov."
            ],
            "title": "Facebook fair\u2019s wmt19 news translation task submission",
            "venue": "arXiv preprint arXiv:1907.06616.",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Pei",
                "Ishan Jindal",
                "Kevin Chen-Chuan Chang",
                "Chengxiang Zhai",
                "Yunyao Li."
            ],
            "title": "When to use what: An in-depth comparative empirical analysis of openie systems for downstream applications",
            "venue": "arXiv preprint arXiv:2211.08228.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Youngbin Ro",
                "Yukyung Lee",
                "Pilsung Kang."
            ],
            "title": "Multi2oie: Multilingual open information extraction based on multi-head attention with bert",
            "venue": "arXiv preprint arXiv:2009.08128.",
            "year": 2020
        },
        {
            "authors": [
                "Hadeel Saadany",
                "Constantin Orasan."
            ],
            "title": "Bleu, meteor, bertscore: Evaluation of metrics performance in assessing critical translation errors in sentimentoriented text",
            "venue": "arXiv preprint arXiv:2109.14250.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Solawetz",
                "Stefan Larson."
            ],
            "title": "Lsoie: A large-scale dataset for supervised open information extraction",
            "venue": "arXiv preprint arXiv:2101.11177.",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Ido Dagan."
            ],
            "title": "Creating a large benchmark for open information extraction",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2300\u20132305.",
            "year": 2016
        },
        {
            "authors": [
                "Michael Vasilkovsky",
                "Anton Alekseev",
                "Valentin Malykh",
                "Ilya Shenbin",
                "Elena Tutubalina",
                "Dmitriy Salikhov",
                "Mikhail Stepnov",
                "Andrey Chertok",
                "Sergey Nikolenko."
            ],
            "title": "Detie: Multilingual open information extraction inspired by object detection",
            "venue": "Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Nan Duan",
                "Ben Kao",
                "Junwei Bao",
                "Ming Zhou."
            ],
            "title": "Answering questions with complex semantic constraints on open knowledge bases",
            "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Manage-",
            "year": 2015
        },
        {
            "authors": [
                "Junlang Zhan",
                "Hai Zhao."
            ],
            "title": "Span model for open information extraction on accurate corpus",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9523\u20139530.",
            "year": 2020
        },
        {
            "authors": [
                "Mengli Zhang",
                "Gang Zhou",
                "Wanting Yu",
                "Wenfen Liu."
            ],
            "title": "Far-ass: Fact-aware reinforced abstractive sentence summarization",
            "venue": "Information Processing & Management, 58(3):102478.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Open Information Extraction (OpenIE) is a traditional NLP task that extracts structured information from unstructured text to be used for other downstream applications.Traditionally, OpenIE focuses on extracting the surface forms of relations as they appear in the raw text, which we term extractive OpenIE. One of the main drawbacks of this approach is that implicit semantic relations (inferred relations) can not be extracted, compromising the performance of downstream applicationsIn this paper, we broaden the scope of OpenIE relations from merely the surface form of relations to include inferred relations, which we term abstractive OpenIE. This new task calls for the development of a new abstractive OpenIE training dataset and a baseline neural model that can extract those inferred relations. We also demonstrate the necessity for a new semantics-based metric for evaluating abstractive OpenIE extractions. Via a case study on Complex QA, we demonstrate the effectiveness of abstractive OpenIE. 1"
        },
        {
            "heading": "1 Introduction",
            "text": "Open Information Extraction (OpenIE) is the task of extracting relation tuples from unstructured text (Etzioni et al., 2008; Mausam et al., 2012; Angeli et al., 2015). Unlike traditional information extraction, OpenIE is open domain, intended to be easy to deploy in different domains without fine-tuning. These relations can then be used in downstream applications like summarization (Zhang et al., 2021), question-answering (Lu et al., 2019), and knowledge base population (Kroll et al., 2021). In order to support these applications, OpenIE needs to extract as many different types of relations as possible. One particular relation type of interest is \"Inferred Relations\". We define an \"Inferred\n1Code and models are available at https://github.com/ kevinpei/AbstractiveOpenIE\nRelation\" to be a relation where the predicate contains words that are not in the original sentence. For example, given the sentence \"Albert Einstein (14 March 1879 - 18 April 1955) was a Germanborn theoretical physicist\", the relation (Albert Einstein, died on, 18 April 1955) can be inferred even though \"died on\" is not in the original sentence. Extracting inferred relations increases recall, which is explicitly desired by various downstream tasks including question-answering, slot filling, event schema induction, summarization, and knowledge base population (Pei et al., 2022). Based on the number of inferred relations in the manually annotated dataset WiRe57, extracting inferred relations could increase the total number of relations extracted by 50% (L\u00e9chelle et al., 2018). Existing neural OpenIE models struggle to extract these inferred relations, with only one previous model, OpenIE6, including hand-written rules to extract only some cases of inferred relations (Kolluru et al., 2020a). Table 1 has an example of an inferred relation.\nAnother problem is that the extraction is very dependent on the sentence\u2019s syntax. For downstream\napplications using OpenIE, it is important to be able to extract either different surface forms of a relation or its canonical form. The surface form refers to how it appears within the text, while the canonical form refers to the semantic meaning. In question answering (QA), several methods repeatedly paraphrase the questions so that the surface forms of extracted relations match at least one of the question paraphrases, indicating that extracting more surface forms of relation would answer more questions (Fader et al., 2013, 2014; Yin et al., 2015). In addition, the more complex a sentence\u2019s syntax is, such as having more clauses, the more difficult it is to extract high-quality relations. An illustrative example of how being limited to extracting surface forms can be found in Table 1.\nBy design, all existing neural OpenIE models are unable to extract these abstractive relations, which could be utilized by the downstream application. Therefore, in this work, we propose an abstractive Open Information Extraction (abstractive OpenIE) task. The purpose of this task is to extract relation tuples that are far beyond the reach of any existing OpenIE tasks. We define abstractive OpenIE as a task that given an input sentence generates ordered tuples in the form of (subject, predicate, object) for all possible relations (inferred or non-inferred) within the sentence.\nAlthough not explicitly defined as such, existing neural models often treat OpenIE as a labeling problem, where tokens are labeled as being part of the subject, predicate, or object of a relation (Kolluru et al., 2020a; Vasilkovsky et al., 2022). Even in cases where OpenIE is defined as a generative problem, the generated relations don\u2019t contain words outside the vocabulary of the original sentence (Kolluru et al., 2020b) (Han and Wang, 2021). Due to the labeling problem definition, prior neural OpenIE models struggle to extract relations with predicates that don\u2019t appear in the original sentence. We refer to all preexisting neural OpenIE models as extractive OpenIE methods, because they can only generate relations by extracting tokens from the original sentence.\nOne such attempt to go beyond extractive OpenIE is the OpenIE6 model Kolluru et al. (2020a). It explicitly concatenates manually defined out-ofvocabulary tokens at the end of each sentence to allow for the extraction of specific inferred relations. However, obtaining such a list is non-trivial and can not scale to every domain. We differ from\nOpenIE6 in the sense that abstractive OpenIE models trained on abstractive OpenIE training datasets generate this inferred relation on the fly and do not require defining a list of out-of-vocabulary tokens. Therefore, in this paper, we derive abstractive OpenIE training datasets from existing information extraction datasets and train a baseline machinelearning model that extracts abstractive relations.\nFurther, we also develop an abstractive OpenIE evaluation metric to evaluate the quality of abstractive OpenIE models. Our problem warrants a new evaluation metric because all the existing OpenIE evaluation metrics are lexical and evaluated based on the token overlap between the predicted relations and the gold standard relation. These lexical metrics are undesirable for the proposed task as the relations extracted using the abstractive OpenIE model do not have to use the tokens present in the input sentence. Therefore, we propose a semantics-based metric for evaluating abstractive OpenIE models.\nIn summary, our contributions are as follows:\n\u2022 We propose an abstractive OpenIE task to expand the scope of OpenIE extractions compared to prior extractive OpenIE models.\n\u2022 We derive an abstractive OpenIE training dataset and develop an initial abstractive OpenIE model as a baseline.\n\u2022 We propose a general-purpose semanticsbased evaluation metric for evaluating any OpenIE model.\n\u2022 We perform a comprehensive comparison between abstractive and extractive OpenIE models."
        },
        {
            "heading": "2 Related Work",
            "text": "OpenIE Datasets: Given how data-hungry deep learning models are and how costly it is to manually label OpenIE datasets, most OpenIE training sets are weakly labeled using high-confidence extractions from prior OpenIE models to get \"silverstandard\" labels. For example, the CopyAttention (Cui et al., 2018), SpanOIE (Zhan and Zhao, 2020), and OIE4 (Kolluru et al., 2020b) training sets are created from high-confidence OpenIE4 extractions from Wikipedia. LSOIE (Solawetz and Larson, 2021) is instead created from examples from the QA-SRL 2.0 dataset. Because traditional OpenIE is\nextractive, there are no inferred relations in OpenIE training sets, with only hand-labeled benchmarks containing inferred relations. As a result, these training sets are not well-suited for training an abstractive OpenIE model.\nIn contrast, there are several benchmarks with inferred relations. WiRe57 (L\u00e9chelle et al., 2018) is 57 manually annotated sentences. CaRB (Bhardwaj et al., 2019) uses crowdsourcing to re-annotate the sentences in the OIE2016 benchmark, the first commonly used OpenIE benchmark (Stanovsky and Dagan, 2016). ReOIE2016 (Zhan and Zhao, 2020) is a different manual re-annotation of OIE2016 to attempt to resolve problems arising from incorrect extractions. LSOIE also has its own benchmark created using the same method as its training set. WiRe57, CaRB, and ReOIE2016 all contain inferred relations, making them useful for evaluating abstractive OpenIE. OpenIE Models: OpenIE6 is a neural OpenIE model that performs BIOES tagging for the subject, predicate, and object of each relation (Kolluru et al., 2020a). At the end of each sentence, it appends the tokens \"be\", \"of\", and \"from\" so that they can also be tagged as part of the predicate. However, this method limits inferred relation extraction to only those containing the tokens they manually specify and doesn\u2019t help with the issue of extracting only the surface form of the relation.\nIMoJIE is an OpenIE model that tries to reduce the redundancy of relations by appending extracted\nrelations to the end of each sentence (Kolluru et al., 2020b). This new sentence is then given as input so the model can identify what relations have previously been extracted at the cost of significantly reduced extraction speed. Although it uses a generative neural model, IMoJIE relies on its copy mechanism to extract relations, so its vocabulary is limited so that it only generates tokens that are within the original sentence. In addition, the focus on reducing redundancy means it is also constrained to extracting only a single surface form of each relation in each sentence.\nGen2OIE is an OpenIE model that fine-tunes a seq2seq model to generate relations (Kolluru et al., 2022). It follows a two-stage approach, where predicates are first extracted, then arguments are extracted for each predicate. Unlike previous OpenIE models, Gen2OIE can generate relations using tokens that do not appear in the original sentence.\nClosed Information Extraction (CIE) is a related task where relations within an existing KB are extracted from unstructured text. GenIE proposes a generative model to perform this task (Josifoski et al., 2021). However, CIE is an inherently more limited task than OpenIE due to its dependence on a preexisting domain. CIE models are unable to extract relations from new and emerging domains and require human effort to transfer to new domains. OpenIE Evaluation Metrics: Existing OpenIE metrics are lexical. This means that extracted relations are evaluated based on the token overlap\nbetween the predicted relations and the gold standard relations. In particular, OIE2016 is based on tuple-level matching, treating relations extraction as a binary classification problem where a gold standard relation is extracted if a predicted relation contains a majority of tokens in the gold standard relation (Stanovsky and Dagan, 2016). WiRE57 and CaRB use token-level matching, where predicted relations are evaluated based on the token overlap between the best matches between the predicted and gold standard relations (L\u00e9chelle et al., 2018) (Bhardwaj et al., 2019). Because the abstractive relations extracted using abstractive OpenIE do not have to use the original sentence\u2019s tokens, evaluating them using lexical metrics is undesirable.\nThere has been previous interest in semanticsbased metrics for evaluating abstractive summarization and machine translation. BERTScore is a popular metric that calculates the cosine similarity between the BERT contextual embeddings of each token in the document and each token in the summary. The highest total similarity score possible from the mapping of tokens in the document to tokens in the summary is then chosen as the BERTScore (Zhang et al., 2019). In theory, this metric would take into account the context of each word, which would capture the semantics of each word. However, it has been found that BERTScore may still be insufficient in cases where individual tokens like negations significantly change the meaning of the sentence, even if it is marginally better than lexical methods like BLEU, ROUGE, and METEOR (Saadany and Orasan, 2021)."
        },
        {
            "heading": "3 Abstractive OpenIE",
            "text": "Abstractive OpenIE is defined as a task that generates ordered tuples in the form of (subject, predicate, object) for all possible relations (inferred or non-inferred) within a given sentence. In this section, we will describe all the pieces required to accomplish this task."
        },
        {
            "heading": "3.1 Training Sets",
            "text": "Although there are existing OpenIE training sets, they do not fit our goals because they are purely extractive. The training set needs to contain inferred relations so that trained models can extract inferred relations. To address this problem, we use two methods to derive abstractive OpenIE training sets from OIE4, a preexisting OpenIE training set: Paraphrasing Via Back Translation\nBack translation is the translation of a text into a different language, then translation back into the original language (Edunov et al., 2018). The resulting text should retain the same semantic meaning, but may differ in the specific words or syntax used. To generate abstractive OpenIE training data, we generate back translations of the sentences but retain the gold standard relations. Because the back translated sentences use different words and syntax, the gold standard relations may no longer consist of only words from the original sentence, thus becoming inferred relations. We provide an example in Table 3.\nWhen generating paraphrases, we need to make sure that the paraphrased sentence has the same semantic meaning as the original sentence and contains the same relations. Thus, we perform a validation step where we use entailment to measure the quality of the paraphrase. During this step, we use three measures to ensure the quality of the paraphrase. We measure whether the original sentence entails the paraphrase to ensure the paraphrase doesn\u2019t contain extraneous information not in the original sentence. We measure whether the paraphrase entails the original sentence to ensure the paraphrase contains all information present in the original sentence. Finally, we measure whether the paraphrased sentence entails all of the gold standard relations to ensure that the relations are the same for the original sentence and the paraphrase. If any of these hypotheses does not have an entailment confidence above a certain threshold, then we do not use the paraphrase in the training data."
        },
        {
            "heading": "Data Augmentation Via Relation Extraction",
            "text": "Although paraphrasing can create inferred rela-\ntions in that the words used may not match the sentence exactly, the relations remain fundamentally the same. The inferred relations that the benchmarks such as WiRe57 contain are not derived from paraphrases of the sentence, so creating paraphrases as training data for them is not appropriate. Instead, we augment the data with additional inferred relations derived using relation extraction (RE). We provide an example in Table 4.\nRE also aims to extract relations from unstructured text, but instead of being completely open domain, RE is limited to extracting a specific set of relations that must be defined beforehand (Bach and Badaskar, 2007). However, those relations may take a variety of surface forms. For instance, the relation \"country_of_birth\" could take the form \"Einstein was born in Ulm\", \"Einstein (born 14 March 1879 in Ulm)\", other forms. We thus use RE models to extract additional inferred relations for abstractive OpenIE training. To ensure quality and prevent redundancy, we only keep extracted relations above a certain level of confidence and which are not entailed by or entail preexisting OpenIE gold standard relations."
        },
        {
            "heading": "3.2 Benchmarks",
            "text": "In contrast to existing OpenIE training dataset, there are several OpenIE benchmarks which contain inferred relations because they were manually annotated or used crowdsourcing for annotation. For evaluation, we use WiRe57, CaRB, Re-OIE2016, and LSOIE test sets. Each of these benchmarks contains a different proportion of inferred relations, in Table 2. In particular, the manual annotation of WiRe57 makes prior extractive OpenIE methods perform poorly compared to their performance on other OpenIE benchmarks. Unlike\nthe other benchmarks, LSOIE contains no inferred relations at all, meaning in theory extractive OpenIE methods should be able to extract all relations. Thus, we can use performance on LSOIE to directly compare abstractive OpenIE and extractive OpenIE models on the extractive OpenIE task.\nStatistics for the derived training sets and benchmarks is available in Table 2."
        },
        {
            "heading": "3.3 Abstractive Tuple Generator",
            "text": "Prior OpenIE models are not suited for the proposed task because all existing models are extractive models. As a result, we use generative models to generate relations for a given sentence. We choose to fine-tune T5, a text-to-text transformer model, to generate relations from a sentence (Raffel et al., 2020).\nInspired by Multi2OIE, we perform relation generation in two stages, a predicate and an argument stage (Ro et al., 2020). In the predicate stage, all predicates are extracted from the sentence at once. The input for this stage is the sentence, while the gold standard is the predicates of all gold standard relations separated by the special \"[pred]\" token. Although the order of relations in our output doesn\u2019t matter, we need to enforce a strict order for the model to learn. Thus, we order the predicates by their position within the sentence.\nFor the argument prediction stage, for each predicate the model predicts the arguments for the relation with that predicate. Because multiple relations may have the same predicate, we specify the predicate by including all predicates before it in the sentence. For each relation, we assume there are two arguments, which the model extracts simultaneously. The input for this stage is the sentence with the predicate concatenated to the end, separated by\na \"[pred]\" special token, while the gold standard is the arguments for the gold relation corresponding to that predicate, in Table 5."
        },
        {
            "heading": "3.4 Semantic-based Evaluation Metrics",
            "text": "CaRB is a popular metric for evaluating OpenIE models, but it requires the predicates of the prediction and gold standard to match to score a given prediction. Although it serves as a good proxy for a semantic metric in extractive OpenIE, it is significantly less useful for abstractive OpenIE where the space of all possible predicates is much larger than just the tokens in the sentence.\nTo evaluate abstractive OpenIE, we require a semantics-based metric rather than a lexical metric based on token matching. Although previous semantics-based evaluation metrics like BERTScore exist, we do not find them to be appropriate for our use case. Previous semantics-based evaluation metrics do not work well for cases where a single token can dramatically change the semantics of a statement, such as negations like \"not\" (Saadany and Orasan, 2021). Thus, we introduce a set of 3 evaluation metrics based on entailment for more accurate semantic evaluation. Each of these metrics measures semantic coherence at different granularities, and which granularity is most important will depend on the application and properties of the datasets. We demonstrate this necessity with an example in Table 6.\nWhen calculating the entailment score for a relation, we remove special characters so that it resembles a sentence. For instance, for the relation triple {Sharon; had been; in a coma}, we form the statement \"Sharon had been in a coma.\" Sentence-tuple entailment The first metric we propose is sentence-tuple entailment. For recall, we combine all the relations together and see if the combined relations entail the sentence. If the combined relations do not entail the sentence, that means the sentence contains information not in any relation and thus the extracted relations as a whole have poor recall. For precision, we take the average of the entailment score obtained when seeing if the sentence entails an individual relation for all extracted relations. If the relation is not entailed, that means it contains information not in the sentence and thus has poor precision. Combined tuple-tuple entailment The second metric we propose is combined tuple-tuple entailment. This metric is inspired by a metric proposed\nby (Du\u0161ek and Kasner, 2020). For this metric, we use the gold standard relations to evaluate the extracted tuples. The combined tuple in this case refers to the combination of all gold standard relations. For recall, we combine all the predicted relations together and see if the combined relations entail the combined gold relations. If the combined predictions do not entail the combined gold, that means the gold relations contains information not in any prediction and thus the extracted relations as a whole have poor recall. For precision, we take the average of the entailment score obtained when seeing if the combined gold entails an individual relation for all extracted relations. If the prediction is not entailed, that means it contains information not in any gold relation and thus has poor precision. Compared to the sentence-tuple entailment metric, this one excludes any extraneous information in the sentence not in the gold standard relations from evaluation. Tuple-tuple entailment The third metric we propose is tuple-tuple entailment. This metric is based on the OpenIE metric CaRB (Bhardwaj et al., 2019). For recall, for each gold standard relation we calculate the entailment for each extracted relation if the gold standard entails that prediction. Then, for each gold standard relation its recall is equal to the highest entailment score achieved by any of the predictions. The recall for the sentence is the average of the recall of its relations. Note that the highest recall for multiple gold standard relations can be achieved by the same predicted relation if the predicted relation contains all of those gold standard relations. For precision, for each gold standard relation we calculate the entailment for each extracted relation if the prediction entails that gold standard relation. Then, we find the optimal matching of gold standard relations to extracted relations that results in the highest average precision. Unlike recall, when calculating precision a predicted relation can only entail a single gold standard relation. This is because we want the number of predictions to match the number of gold relations."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "Datasets and Metrics",
            "text": "We evaluate the trained abstractive OpenIE model on four benchmarks: WiRe57, CaRB, Re-OIE2016, and LSOIE-wiki with respectively decreasing proportion of inferred relations.\nSince OIE4 trained OpenIE models showed superior F1 performance on all these benchmarks as compared to other OpenIE training sets we derive abstractive training data from this dataset. We generate four different versions of OIE4 using the methods we describe in Section 3.1. The first version is the original extractive dataset, the second version uses backtranslation for paraphrasing, the third version is augmented by relation extraction, and the fourth uses both backtranslation and relation extraction for augmentation. For backtranslation we use Facebook-FAIR\u2019s WMT\u201919 GermanEnglish and English-German models (Ng et al., 2019) and retain only those back translated sentences whose entailment confidence is above 80%. For relation extraction, we use a pretrained SuRE model, a state-of-the-art relation extraction model (Lu et al., 2022) without any additional fine-tuning and keep all relations with confidence above 80%. These confidence thresholds are hyperparameters that may be adjusted.\nWe compare performance using the preexisting CaRB metric, as well as our own introduced semantics-based metrics of tuple-tuple entailment,\ncombined tuple-tuple entailment, and sentencetuple entailment. The entailment model we use for our datasets and evaluation metrics is a BERTbased encoder model trained on MNLI, SNLI, and the Hans dataset (Gao et al., 2021)."
        },
        {
            "heading": "Models and Hyperparameters",
            "text": "We fine-tune the T5-base model for our experiments. We fine-tuned T5 for 5 epochs with an initial learning rate of 2e-5 and batch size of 12. We validate T5 on a subset of the OIE4 training set using the tuple-tuple entailment metric. We also compare our model with Multi2OIE, a state-ofthe-art neural extractive OpenIE model (Ro et al., 2020). We train Multi2OIE on the original OIE4 dataset with no paraphrasing. We use the default hyperparameters of Multi2OIE."
        },
        {
            "heading": "5 Results and Analysis",
            "text": "For this section, we focus on the sentence-tuple semantic score because it offers a holistic comparison of the extracted relations and the sentence and does not rely upon potentially incomplete or faulty gold relations. Full tables with our empirical results in-\ncluding other metrics can be found in Appendix A.\nWe first compare performance on all relations in Figure 1. In general, abstractive OpenIE leads to better performance the higher the proportion of inferred relations in the test set. This is expected because Multi2OIE can not extract inferred relations at all. When considering the full benchmarks, of the data augmentation methods we use, SuRE augmentation works the best. Training on backtranslated OIE4 degrades the performance compared to the base extractive OIE4 data. This may be because back translation reduces the amount of training data. Additionally, back translation often just replaces the gold standard predicate with a synonym instead of changing the syntax of the sentence, which does not help in the extraction of inferred relations.\nTo demonstrate the complementary nature of abstractive OpenIE to extractive OpenIE, we combine their extractions. When combining their extractions, we remove redundant relations by removing relations that are entailed by any other relations. If two relations entail each other, then we keep the longer one. A comparison of combined models can be found in Figure 2. When combining model predictions, we observe that back translation actually helps more than SuRE augmentation. This suggests that SuRE augmentation helps extractive OpenIE relations, while back translation is more useful for increasing the recall to inferred relations that could not be extracted by Multi2OIE. The more inferred relations in the benchmark, the more beneficial merging extractions are.\nWe also evaluate our abstractive OpenIE models"
        },
        {
            "heading": "OpenIE Model Documents MRR P@1 Hit@5",
            "text": "on only the inferred relations within each benchmark. To do this, we remove non-inferred relations from the gold standards. We can only measure the resulting recall of the models because the models are trained to generate both inferred and noninferred relations and the metrics we use penalize the precision when there are too many predicted relations for a given sentence, which would be the case for any sentence that had non-inferred relations. Figure 3 shows the results of these experiments. As before, the more inferred relations in the benchmark, the better suited an abstractive OpenIE model is for the task.\nUpon a manual examination of the generated relations of each model, we observe that fine-tuning T5 on SuRE-augmented data results in generated relations replacing some of its predicates with the predicates from SuRE. Table 8 demonstrates one example of a model generating a predicate that does not exist within the sentence but is a common predicate among the SuRE-augmented relations."
        },
        {
            "heading": "Case Study",
            "text": "To further test the applicability of abstractive OpenIE, we evaluate its performance on QUEST, a downstream Complex QA task that uses OpenIE in its pipeline (Lu et al., 2019). QUEST specifically desires higher recall from its OpenIE model, which can be achieved by extracting inferred relations. We show the results in Table 7. The results show that augmenting the training data improves downstream performance, indicating that including more inferred relations in the training data is helpful for this task."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce abstractive OpenIE, an alternative to what we call extractive OpenIE, the paradigm all current OpenIE models currently follow, in order to address the problems of inferred relations and surface form extraction. We find that existing OpenIE datasets and metrics are ill-suited for this task. As a result, we introduce abstractive training set, model, and metrics. We then compare our models trained on different abstractive\ntraining sets and the state-of-the-art extractive OpenIE model using preexisting OpenIE benchmarks. Overall, we find that our models achieve higher performance on inferred relations, which extractive OpenIE models have previously struggled with. We believe abstractive OpenIE has potential as a task that will greatly benefit downstream applications that use OpenIE in their pipeline."
        },
        {
            "heading": "7 Limitations",
            "text": "In this work, we used a relatively smaller T5-base model. A model with more parameters may have led to improved performance. Further, the corpora we chose are all limited to English. As a result, our results are not generalizable to any downstream task that relies on different languages."
        },
        {
            "heading": "Ethics Statement",
            "text": "We did not create any of the models, datasets, or applications covered in this paper. Any ethical issues with the preexisting OpenIE datasets we use in this paper will reflect on this work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, Futurewei Technologies HF2017060011 and 094013, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) and IBMIllinois Discovery Accelerator Institute (IIDAI), grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative. Any opinions, findings, conclusions, or recommendations expressed in this publication are\nthose of the author(s) and do not necessarily reflect the views of the funding agencies."
        },
        {
            "heading": "A Empirical Results",
            "text": "We present our empirical results in tables 9, 10, and 11."
        }
    ],
    "title": "Abstractive Open Information Extraction",
    "year": 2023
}