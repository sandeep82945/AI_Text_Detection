{
    "abstractText": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking documentlevel machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs\u2019 ability on discourse modeling. The study focuses on three aspects: 1) Effects of ContextAware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation;1 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.2",
    "authors": [
        {
            "affiliations": [],
            "name": "Longyue Wang"
        },
        {
            "affiliations": [],
            "name": "Chenyang Lyu"
        },
        {
            "affiliations": [],
            "name": "Tianbo Ji"
        },
        {
            "affiliations": [],
            "name": "Zhirui Zhang"
        },
        {
            "affiliations": [],
            "name": "Dian Yu"
        },
        {
            "affiliations": [],
            "name": "Shuming Shi"
        },
        {
            "affiliations": [],
            "name": "Zhaopeng Tu"
        }
    ],
    "id": "SP:7e48a684b4e643c77f0c3dca15750e682ae10e57",
    "references": [
        {
            "authors": [
                "Guangsheng Bao",
                "Yue Zhang",
                "Zhiyang Teng",
                "Boxing Chen",
                "Weihua Luo."
            ],
            "title": "G-transformer for document-level machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Rachel Bawden",
                "Rico Sennrich",
                "Alexandra Birch",
                "Barry Haddow."
            ],
            "title": "Evaluating discourse phenomena in neural machine translation",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Welch Bl."
            ],
            "title": "The generalization of \u2018student\u2019s\u2019 problem when several different population varlances are involved",
            "venue": "Biometrika.",
            "year": 1947
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Dallas Card",
                "Peter Henderson",
                "Urvashi Khandelwal",
                "Robin Jia",
                "Kyle Mahowald",
                "Dan Jurafsky."
            ],
            "title": "With little power comes great responsibility",
            "venue": "arXiv preprint arXiv:2010.06595.",
            "year": 2020
        },
        {
            "authors": [
                "Sheila Castilho."
            ],
            "title": "Towards document-level human mt evaluation: On the issues of annotator agreement, effort and misevaluation",
            "venue": "Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep",
            "year": 2018
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Hila Gonen",
                "Luke Zettlemoyer."
            ],
            "title": "Dictionary-based phrase-level prompting of large language models for machine translation",
            "venue": "arXiv preprint arXiv:2302.07856.",
            "year": 2023
        },
        {
            "authors": [
                "Yvette Graham",
                "Barry Haddow",
                "Philipp Koehn."
            ],
            "title": "Statistical power and translationese in machine translation evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2020
        },
        {
            "authors": [
                "Nuno M. Guerreiro",
                "Elena Voita",
                "Andr\u00e9 Martins."
            ],
            "title": "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Compu-",
            "year": 2023
        },
        {
            "authors": [
                "Junliang Guo",
                "Zhirui Zhang",
                "Linli Xu",
                "Hao-Ran Wei",
                "Boxing Chen",
                "Enhong Chen."
            ],
            "title": "Incorporating bert into parallel sequence decoding with adapters",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2020
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla."
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Guoping Huang",
                "Lemao Liu",
                "Xing Wang",
                "Longyue Wang",
                "Huayang Li",
                "Zhaopeng Tu",
                "Chengyan Huang",
                "Shuming Shi."
            ],
            "title": "Transmart: A practical interactive machine translation system",
            "venue": "arXiv preprint arXiv:2105.13072.",
            "year": 2021
        },
        {
            "authors": [
                "Yuchen Eleanor Jiang",
                "Tianyu Liu",
                "Shuming Ma",
                "Dongdong Zhang",
                "Mrinmaya Sachan",
                "Ryan Cotterell."
            ],
            "title": "Discourse centric evaluation of machine translation with a densely annotated parallel corpus",
            "venue": "arXiv preprint arXiv:2305.11142.",
            "year": 2023
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen-tse Huang",
                "Xing Wang",
                "Zhaopeng Tu."
            ],
            "title": "Is chatgpt a good translator? a preliminary study",
            "venue": "arXiv preprint arXiv:2301.08745.",
            "year": 2023
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Mohit Iyyer."
            ],
            "title": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
            "venue": "arXiv preprint arXiv:2304.03245.",
            "year": 2023
        },
        {
            "authors": [
                "Nagata",
                "Toshiaki Nakazawa",
                "Michal Nov\u00e1k",
                "Martin Popel",
                "Maja Popovi\u0107."
            ],
            "title": "Findings of the 2022 conference on machine translation (WMT22)",
            "venue": "Proceedings of the Seventh Conference on Machine Translation.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kocmi",
                "Christian Federmann."
            ],
            "title": "Large language models are state-of-the-art evaluators of translation quality",
            "venue": "arXiv preprint arXiv:2302.14520.",
            "year": 2023
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing krippendorff\u2019s alpha-reliability",
            "year": 2011
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Hongyuan Lu",
                "Haoyang Huang",
                "Dongdong Zhang",
                "Haoran Yang",
                "Wai Lam",
                "Furu Wei."
            ],
            "title": "Chainof-dictionary prompting elicits translation in large language models",
            "venue": "arXiv preprint arXiv:2305.06575.",
            "year": 2023
        },
        {
            "authors": [
                "Chenyang Lyu",
                "Jitao Xu",
                "Longyue Wang."
            ],
            "title": "New trends in machine translation using large language models: Case examples with chatgpt",
            "venue": "arXiv preprint arXiv:2305.01181.",
            "year": 2023
        },
        {
            "authors": [
                "Xinglin Lyu",
                "Junhui Li",
                "Zhengxian Gong",
                "Min Zhang."
            ],
            "title": "Encouraging lexical translation consistency for document-level neural machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Valentin Mac\u00e9",
                "Christophe Servan."
            ],
            "title": "Using whole document context in neural machine translation",
            "venue": "Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Mary L McHugh."
            ],
            "title": "Interrater reliability: the kappa statistic",
            "venue": "Biochemia medica.",
            "year": 2012
        },
        {
            "authors": [
                "Graham Neubig",
                "Zhiwei He"
            ],
            "title": "Zeno gpt machine translation report",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Gray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners. OpenAI",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research.",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C Farinha",
                "Alon Lavie."
            ],
            "title": "COMET: A neural framework for MT evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Snover",
                "Bonnie Dorr",
                "Rich Schwartz",
                "Linnea Micciulla",
                "John Makhoul."
            ],
            "title": "A study of translation edit rate with targeted human annotation",
            "venue": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical",
            "year": 2006
        },
        {
            "authors": [
                "Zewei Sun",
                "Mingxuan Wang",
                "Hao Zhou",
                "Chengqi Zhao",
                "Shujian Huang",
                "Jiajun Chen",
                "Lei Li."
            ],
            "title": "Rethinking document-level neural machine translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Katherine Thai",
                "Marzena Karpinska",
                "Kalpesh Krishna",
                "Bill Ray",
                "Moira Inghilleri",
                "John Wieting",
                "Mohit Iyyer."
            ],
            "title": "Exploring document-level literary machine translation with parallel paragraphs from world literature",
            "venue": "arXiv preprint arXiv:2210.14250.",
            "year": 2022
        },
        {
            "authors": [
                "Zhaopeng Tu",
                "Yang Liu",
                "Shuming Shi",
                "Tong Zhang."
            ],
            "title": "Learning to remember translation history with a continuous cache",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting palm for translation: Assessing strategies and performance",
            "venue": "arXiv preprint arXiv:2211.09102.",
            "year": 2022
        },
        {
            "authors": [
                "Elena Voita",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Context-aware monolingual repair for neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Elena Voita",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "When a good translation is wrong in context: Contextaware machine translation improves on deixis, ellipsis, and lexical cohesion",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Elena Voita",
                "Pavel Serdyukov",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Context-aware neural machine translation learns anaphora resolution",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
            "year": 2018
        },
        {
            "authors": [
                "Longyue Wang."
            ],
            "title": "Discourse-aware neural machine translation",
            "venue": "Ph.D. thesis, Ph. D. thesis, Dublin City University, Dublin, Ireland.",
            "year": 2019
        },
        {
            "authors": [
                "Longyue Wang",
                "Zefeng Du",
                "Donghuai Liu",
                "Cai Deng",
                "Dian Yu",
                "Haiyun Jiang",
                "Yan Wang",
                "Leyang Cui",
                "Shuming Shi",
                "Zhaopeng Tu."
            ],
            "title": "Disco-bench: A discourse-aware evaluation benchmark for language modelling",
            "venue": "arXiv preprint arXiv:2307.08074.",
            "year": 2023
        },
        {
            "authors": [
                "Longyue Wang",
                "Siyou Liu",
                "Mingzhou Xu",
                "Linfeng Song",
                "Shuming Shi",
                "Zhaopeng Tu."
            ],
            "title": "A survey on zero pronoun translation",
            "venue": "arXiv preprint arXiv:2305.10196.",
            "year": 2023
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Shuming Shi",
                "Tong Zhang",
                "Yvette Graham",
                "Qun Liu."
            ],
            "title": "Translating pro-drop languages with reconstruction models",
            "venue": "Proceedings of the 2018 AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Xing Wang",
                "Shuming Shi."
            ],
            "title": "One model to learn both: Zero pronoun prediction and translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "Exploiting cross-sentence context for neural machine translation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2017
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "Learning to jointly translate and predict dropped pronouns with a shared reconstruction mechanism",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2018
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Xiaojun Zhang",
                "Hang Li",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "A novel approach to dropped pronoun translation",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2016
        },
        {
            "authors": [
                "Longyue Wang",
                "Mingzhou Xu",
                "Derek F. Wong",
                "Hongye Liu",
                "Linfeng Song",
                "Lidia S. Chao",
                "Shuming Shi",
                "Zhaopeng Tu."
            ],
            "title": "GuoFeng: A benchmark for zero pronoun recovery and translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "RF Woolson."
            ],
            "title": "Wilcoxon signed-rank test",
            "venue": "Wiley encyclopedia of clinical trials.",
            "year": 2007
        },
        {
            "authors": [
                "Tong Xiao",
                "Jingbo Zhu",
                "Shujie Yao",
                "Hao Zhang."
            ],
            "title": "Document-level consistency verification in machine translation",
            "venue": "Proceedings of Machine Translation Summit XIII: Papers.",
            "year": 2011
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Ankur Bapna",
                "Melvin Johnson",
                "Ali Dabirmoghaddam",
                "Naveen Arivazhagan",
                "Orhan Firat."
            ],
            "title": "Multilingual document-level translation enables zero-shot transfer from sentences to documents",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Zaixiang Zheng",
                "Xiang Yue",
                "Shujian Huang",
                "Jiajun Chen",
                "Alexandra Birch."
            ],
            "title": "Toward making the most of context in neural machine translation",
            "venue": "ArXiv.",
            "year": 2020
        },
        {
            "authors": [
                "Jinhua Zhu",
                "Yingce Xia",
                "Lijun Wu",
                "Di He",
                "Tao Qin",
                "Wengang Zhou",
                "Houqiang Li",
                "Tie-Yan Liu."
            ],
            "title": "Incorporating bert into neural machine translation",
            "venue": "arXiv preprint arXiv:2002.06823.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the past several years, machine translation (MT) has seen significant advancements with the introduction of pre-trained models such as BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019), and\n\u2217Equal contribution. 1The protocol employed in this work was approved by the\nTencent Institutional Review Board (IRB). 2We release our data and annotations at https://github. com/longyuewangdcu/Document-MT-LLM.\nT5 (Raffel et al., 2020). These models have demonstrated impressive performance on MT (Zhu et al., 2020; Guo et al., 2020; Xue et al., 2021). However, most of the existing work has focused on sentencelevel translation, which can result in translations that lack coherence and context. Recent years have seen a growing interest in document-level translation, which is a crucial task that involves translating entire documents (Wang et al., 2017; Bawden et al., 2018; Wang, 2019; Zhang et al., 2022) while modelling specific discourse phenomena (Wang et al., 2016; Voita et al., 2018; Wang et al., 2018a,b, 2019; Voita et al., 2019b; Wang et al., 2023b). The most popular large language model (LLM) \u2013 ChatGPT3 shows the ability of maintaining long-term coherence and consistency in a conversation by conditioning on previous conversational turns. Ad-\n3https://chat.openai.com. All corresponding results were obtained from GPT-3.5 and GPT-4 in March 2023. The reproducibility is discussed in Section Limitation.\nditionally, the model is trained on a large dialogue dataset, which allows it to learn the patterns and conventions of human communication, further improving its ability to document-level understanding and generation (as shown in Figure 1).\nIn this paper, we are particularly interested in how LLMs such as ChatGPT perform for modeling document-level text, encompassing discourse phenomena such as entity consistency, referential expressions, and coherence. Taking document-level MT as a testbed, we conduct an empirical study from three in-depth perspectives:\n\u2022 Effects of Context-Aware Prompts: ChatGPT needs a prompt as guidance to trigger its translation ability. Thus, we enable prompts to guide ChatGPT to consider document-level contexts as long as possible. Jiao et al. (2023) has found that the candidate prompts generally work well and show minor performance differences on sentencelevel translation. In this work, we further investigate the effects of prompts on the translation quality and specific discourse phenomena.\n\u2022 Comparison of Advanced Translation Models: While ChatGPT has demonstrated remarkable abilities in long-text NLP tasks, we are specifically interested in how it performs on documentlevel translation. Consequently, we conduct a systematic comparison of commercial MT products and advanced document-level approaches, utilizing both automatic and human evaluations to assess their discourse awareness.\n\u2022 Analysis of Discourse Modelling Abilities: A more challenging question is the extent to which ChatGPT capture and utilize discourse knowledge. To answer this question, we introduce a probing method through contrastive testing and explanation. In addition, the impact of various training techniques on the ability of LLMs to model discourse has not been thoroughly investigated. We compare variant models of ChatGPT that incorporate techniques such as code pretraining, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). However, this is not a strict comparison because there are other confounding variables employed during the evolution of ChatGPT. In general, we hope to pose this open question that stimulates reflection and sparks further investigation.\nWe conduct experiments on a variety of document-level translation benchmarks, covering three language pairs (i.e. Chinese\u21d2English,\nEnglish\u21d2German and English\u21d2Russian) and seven domains (i.e. news, social, fiction, Q&A, TED, Europarl, and subtitle). We adopt a comprehensive set of evaluation methods to measure the performance of the models on documentlevel translation, including general-purpose metrics, discourse-specific metrics, and human evaluation. The main contributions are: \u2022 Our empirical study shows the superior capabil-\nities of LLMs over advanced MT systems and methods on document-level translation, indicating their potential to form a new paradigm.\n\u2022 We establish a benchmark with a probing method to thoroughly assess the document-level translation quality and the ability of learning discourse knowledge, which will be made available for future research.\n\u2022 To facilitate future research on document MT, we publicly release the instruction-based benchmark, system outputs as well as human annotations."
        },
        {
            "heading": "2 Experimental Step",
            "text": ""
        },
        {
            "heading": "2.1 Dataset",
            "text": "Table 1 shows statistics of document-level datasets used in our experiments. About Group #1, we utilized the latest datasets, mZPRT (Wang et al., 2022) and WMT2022 (Kocmi et al., 2022), for evaluation to ensure that the testing data had not been used in commercial systems (e.g. Google Translate and ChatGPT) before. As seen, this covers four domains (i.e. news, social media, web fiction, and Q&A forum) in Chinese\u21d2English. Regarding Group #2, we utilized four widelyused benchmarks to compare established documentlevel methods with GPT-like applications. This covers three domains (i.e. TED talks, news commentary, European Parliament) in Chinese\u21d2English and English\u21d2German. In Group #3, we employed an English\u21d2Russian contrastive testset (Voita et al., 2019b) that specifically targets discourse phenomena, such as deixis, ellipsis, and lexical cohesion. We use this dataset to further exploit models\u2019 capacity for modeling discourse knowledge.\nAs seen, we also report the average length of a document (|W|/|D|), which can be considered a measure of the complexity of discourse modeling. As the length of the document increases, it becomes more challenging to model accurate cohesive devices and discourse structure. From this perspective, the mZPRT Fiction and IWSLT TED datasets pose a greater challenge compared to others.\nDiscussion on Data Contamination We conducted ChatGPT in March 28\u223c31 2023 with the official notice \u201cthe training data is up to September 2021\u201d.4 Different from previous work that leaned on dated or single-type datasets to assess ChatGPT\u2019s capabilities (Jiao et al., 2023; Lu et al., 2023), we carefully chosen both lastst, public and diverse testsets to mitigate the risks associated with data contamination. Taking Probing Discourse Knowledge (Section 5.1) for example, while the contrastive testset used for the prediction task originated in 2019, the evaluation on the explanation task remains devoid of public references. Our conclusions are comprehensively made by considering both prediction and explanation results, balancing out any potential data contamination concerns. Despite precautions, there remains a risk of data contamination, given that publicly available datasets are easily incorporated into LLM training (e.g. pretraining, SFT, or RLHF). A better way is to consistently integrate and employ the latest datasets when evaluating LLMs."
        },
        {
            "heading": "2.2 Evaluation Method",
            "text": "Translation Quality We evaluate different approaches and systems using classical sentenceand document-level evaluation metrics. About sentence-level metrics, we employ the commonlyused sacreBLEU (Post, 2018) and TER (Snover et al., 2006). Additionally, we utilize COMET (Rei et al., 2020), which leverages pretrained language\n4https://platform.openai.com/docs/models/ gpt-4.\nmodels to achieve high correlations with human quality judgments. About document-level metrics, we report document-level sacreBLEU (d-BLEU) (Liu et al., 2020), which is computed by matching n-grams in the whole document. Note that all evaluations are case-sensitive. To facilitate sentencelevel evaluation of document outputs, we implemented automatic sentence splitting/alignment5 on the output and then manually corrected errors.\nDiscourse Awareness To target specific discourse phenomena, we utilized two targeted metrics, namely, CTT and AZPT, which respectively evaluate the consistency of terminology translation and accuracy of zero pronoun (ZP) translation. Regarding CTT, one repeated terminology should keep the same translation throughout the whole document (Xiao et al., 2011). We adopt a lexical translation consistency metric (Lyu et al., 2021):\nCTT =\n\u2211 t\u2208TT \u2211k i=1 \u2211k j=i+1 1(ti=tj)\nC2k\n|TT| (1)\nfor each terminology word w \u2208 TT , the C2k denotes the size of the combination of translation set (t1, . . . , tk), and function 1(ti = tj) returns 1 if ti is same as tj , otherwise 0. The metric illustrates how frequently translation pairs of w is same within a document. The higher the metric value is, the more likely w is translated consistently. Regarding ZP, it is a discourse phenomenon that appears frequently in pronoun-dropping (pro-drop)\n5https://github.com/rsennrich/Bleualign.\nlanguages such as Chinese and Japanese. Recovering ZPs in a target language (non-pro-drop) needs an understanding of the discourse structure. We used the AZPT score to measure the accuracy of ZP translation (Wang et al., 2022):\nAZPT = \u2211\nz\u2208ZPA(tz|z) |ZP|\n(2)\nwhere ZP is the list of zero pronouns in the source sentences, tz is the generated translation for the zero pronoun z, and A(tz|z) is a binary scorer to judge whether tz is the correct translation of z.\nHuman Evaluation To thoroughly validate our conclusions, we also conduct a human evaluation (see Section Limitation.). We establish two sets of evaluation criteria: 1) general quality, covering aspects such as fluency and adequacy; 2) discourseaware quality, including factors such as consistency, word choice, and anaphora. The detailed scoring criteria are listed in Appendix\u00a7 A.2. Ac-\ncordingly, each output will be assigned two distinct scores (0\u223c5). For each domain subset, we assessed 100 instances, with each instance containing outputs from 5 different systems. This amounted to an evaluation of roughly 70K words in total. The scores were assigned to each window of neighboring sentences, taking into account the context provided by the entire document. Our intent was for evaluators to consider discourse properties beyond single sentences, while also avoiding the difficult task of evaluating an entire document. We employed two professional evaluators for our study. The payment and background is detailed in Section Ethical Considerations and Appendix\u00a7 A.2, respectively. Besides, our annotators were given practice items, and the annotations reaches 0.86 Cohen\u2019s kappa scores (McHugh, 2012), demonstrating that the annotators work efficiently and consistently under this guideline."
        },
        {
            "heading": "3 Effects of Context-Aware Prompts",
            "text": ""
        },
        {
            "heading": "3.1 Motivation",
            "text": "Existing document NMT methods can be mainly classified into two categories: multisentence (Wang et al., 2017; Voita et al., 2018; Tu et al., 2018) and whole-document (Mac\u00e9 and Servan, 2019; Bao et al., 2021) approaches. ChatGPT is capable of not only handling long text in a single conversational turn but also recalling the entire context in the chat box. Accordingly, we design prompts to trigger the document-level translation ability of ChatGPT.\nThe prompt engineering is necessary to ensure ChatGPT\u2019s robust ability to interpret instructions and to model long-term dependencies. Our research confirms the neutrality and representativeness of various prompts, allowing other researchers\nto utilize them with confidence, unburdened by concerns of unintended biases."
        },
        {
            "heading": "3.2 Comparison of Different Prompts",
            "text": "We query ChatGPT itself for advice and obtain a number of candidate prompts, and then refine them into three document-level prompts as shown in Table 3. We utilize P1 to translate a document sentence by sentence, with each sentence placed in a single conversational turn and the entire document contained within one chat box. This mainly takes advantage of ChatGPT\u2019s long-term modeling ability in the chat box. P2 and P3 combine multiple continuous sentences and translate them in one conversational turn until the entire document is finished. This aims to maximize the length of document as input. The only difference is whether or not the sentential boundary tag \u201c[]\u201d is inserted into each sentence.\nWe compare the three candidate prompts on the Zh\u21d2En translation task using two testsets, WMT2022 News and mZPRT Fiction. Table 2 shows the translation quality in terms of a variety of automatic evaluation metrics. In general, ChatGPT reliably performs well with three candidate prompts, showing only minor variations in performance. This aligns with prior findings in sentencelevel translation with ChatGPT (Jiao et al., 2023). Out of the three prompts, the prompt involved multi-turn contexts without sentence boundaries (P3) achieves the best scores in most evaluation metrics, except for COMET. Regarding discourse phenomena, P3 outperforms other candidates with better consistency of terminology translation and higher accuracy of ZP translation. Upon examining the output samples, we noticed that ChatGPT may sometimes forget the sentential boundary tag\nof P2 and combine all sentences together. Takeaway: (1) Despite translating a document sentence by sentence, ChatGPT\u2019s ability to model long-term dependencies already exists within the chat box. (2) Increasing document length as a input can further enhance translation quality and discourse awareness. (3) ChatGPT tends to translate a document without adhering to strict sentential boundaries, mirroring a natural approach adopted by humans during document translation, which doesn\u2019t necessitate sentence-to-sentence translation."
        },
        {
            "heading": "4 Comparison of Translation Models",
            "text": "In this section, we compare various systems and methods for the document-level translation task. In the following experiments, we use the P3 prompt for ChatGPT and the same document-level window size for MT models as the default setting."
        },
        {
            "heading": "4.1 ChatGPT vs. Commercial Systems",
            "text": "Commercial systems are known for their high accuracy and efficiency in translation, making them a strong contender for any machine translation evaluation. By comparing with commercial systems, we can gauge ChatGPT\u2019s performance relative to the best available MT technologies. We compare GPT3.5/GPT-4 with three commercial translation products, including Google Translate,6 DeepL Translate,7 and Tencent TranSmart (Huang et al., 2021).8 We employ both automatic (d-BLEU) and human evaluation (general/discourse-aware quality) as detailed in Section 2.2.\nTable 4 shows the results. When evaluated using d-BLEU, commercial MT systems generally out-\n6https://translate.google.com. 7https://www.deepl.com. 8https://transmart.qq.com.\nperform LLM-based systems, except for the Q&A domain, which involves informal spoken language. While the difference in performance is not significant in the news domain (e.g. the gap between DeepL and GPT-4 is only 0.6 points), it is considerable in the social media and web fiction domains (i.e. the gaps are 3.3 and 1.9 points). A surprising finding is that GPT-4 and GPT-3.5 perform significantly better than MT systems in terms of human evaluation. The potential reasons may be: (1) d-BLEU only measures the similarity of the n-grams between the MT output and the reference translations. However, human takes into account additional factors such as coherence, fluency, and naturalness of the translation, which may not necessarily correlate with d-BLEU scores. (2) ChatGPT and MT systems may have different strengths and weaknesses. For example, ChatGPT may be better at modeling long-term dependencies and capturing discourse-level information, which could result in higher human evaluation. On the other hand, MT systems may perform better in terms of word-level accuracy, which is reflected in d-BLEU. Note that, our findings is distinguished from Neubig and He (2023). Focusing on long-text translation, we compare ChatGPT with MT systems, and underscore ChatGPT\u2019s enhanced capacity to model long-term dependencies in comparison to MT systems. On the other hand, Neubig and He (2023) investigate the varying performances of GPT models based on sentence length. They found that GPT models perform better on shorter sentences while worse on longer ones. Karpinska and Iyyer (2023) recently highlighted that GPT-3.5 has the capability to utilize document-level context effectively for literary translation, yet it is not free from critical errors. While the Fiction testset in our work is categorized under literary, we did not find obvious omission errors in the output. A more detailed comparison is earmarked for future exploration. Karpinska and Iyyer (2023) recently pointed that GPT-3.5 can effectively leverage document-level context for literary translation, but critical errors persist. Although the Fiction subset belongs to literary, we did not find omission errors in the output and we leave this fine-grained comparison for future work. Takeaway: (1) There is a certain degree of discrepancy discrepancy between human and automatic evaluation, which potentially provide complementary reference points when measuring the quality of document-level translations; (2) This discrepancy underlines the complexity inherent in accurately evaluating the capabilities of such systems. We further explore evaluation methods in Section 5.1."
        },
        {
            "heading": "4.2 ChatGPT vs. Document NMT Methods",
            "text": "Document NMT methods are specifically designed to handle part or entire documents, making them a relevant point of comparison for evaluating ChatGPT\u2019s ability to model long-term dependencies and discourse phenomena. We compare with five advanced document-level NMT models: \u2022 MCN (Zheng et al., 2020): A multi-channel network that integrates a hierarchical encoder and a parallel decoder, which leverages the document structure and semantics for translation.\n\u2022 G-Trans (Bao et al., 2021): A graph-based transformer that incorporates document-level discourse structure as a directed acyclic graph, enhancing the representation of the context.\n\u2022 Sent2Sent: A superior sentence-level baseline that employs a transformer architecture to translate each sentence independently and then merges the translations into a document-level output.\n\u2022 MR-Doc2Doc and MR-Doc2Sent: Sun et al. (2022) explore to resolve document translation with the end-to-end, namely document-todocument (Doc2Doc) pattern, and utilize Multiresolutional Training, which combines documents with shorter segments like sentences or paragraphs to improve translation quality (denoted as MR-Doc2Doc). Additionally, they reproduce the document-to-sentence baseline (MRDoc2Sent) that introduces extra model modules to capture contextual information.\nTo enable a fair comparison with previous work, we use four widely used document-level translation benchmarks: TED (ZH-EN and EN-DE), News (EN-DE), and Europarl (EN-DE). We adopt tokenized case-insensitive BLEU and d-BLEU as the evaluation metrics. As MR-Doc2Doc and ChatGPT generate document-level translations that are difficult to separate into individual sentences, we only report d-BLEU scores for these models.\nTable 5 lists the results. The MR-Doc2Doc with extra model pre-training achieves the best document-level performance among previous models. Thanks to the document-level LM pre-training, ChatGPT easily outperforms MR-Doc2Doc\u22c6 on TED (EN-DE) and News (EN-DE) datasets, obtaining similar performance on TED (ZH-EN) dataset. Surprisingly, ChatGPT performs poorly on the Europarl (EN-DE) dataset, even worse than Sent2Sent. We suspect this phenomenon may be caused by\nthe domain distribution bias of the training data. Moreover, we find that ChatGPT is unstable, and its translation results sometimes exhibit omissions and obvious copying behaviors. Note that, the commonly-used datasets were created between 2012 and 2017, a time frame that raises the possibility of these datasets being incorporated into the training data of newer language models. Takeaway: (1) ChatGPT has exhibited superior performance and may become a new promising paradigm for document-level NMT; (2) It is still debatable whether these benchmarks can be considered as appropriate measures for evaluating document-level translation methods. We advocate for greater transparency from model developers regarding their training datasets. Additionally, this highlights the importance of designing innovative evaluation techniques that can reliably assess model capabilities while sidestepping concerns related to data contamination."
        },
        {
            "heading": "5 Analysis of Large Language Models",
            "text": "We analyze the ability of LLMs to capture discourse knowledge from two perspectives: (1) probing the discourse knowledge encoded in LLMs, and (2) examining the impact of different training techniques on discourse modeling."
        },
        {
            "heading": "5.1 Probing Discourse Knowledge in LLM",
            "text": "In order to verify whether LLMs truly learn to utilize the context to resolve discourse inconsistencies, we adopt the contrastive test sets proposed by Voita et al. (2019b). This dataset includes deixis, lexicon consistency, ellipsis (inflection), and ellipsis (verb phrase) for evaluating discourse phenomena in English-Russian translations. Each instance has\na positive translation and a few negative ones that differ by only one specific word. The goal is to determine if a model is more likely to generate a correct translation than incorrect variations. In this experiment, we compare GPT-3.5/GPT-4 with advanced methods, such as Sent2Sent, MR-Doc2Doc, CADec (Voita et al., 2019b) and DocRepair (Voita et al., 2019a), where CADec and DocRepair introduce context-aware post-editing modules to refine the sentence-level translations. For these baselines, we adopt force decoding to generate scores for all translation candidates in each instance. If the score of the positive translation is the highest, then this instance is counted as correct. For ChatGPT, we query them with the prompt P4 in Table 6 to obtain responses and correspondent explanations for each instance. Then some heuristic rules and manual verification are used to calculate final performance.\nEvaluation on Prediction As shown in Table 7, GPT-3.5 performs worse than DocRepair (discouseenhanced method) across all discourse phenomena, with particularly significant gaps present in deixis and lexical consistency tests. These results show that it is difficult to handle deixis and lexical consistency phenomena with large-scale document-level pre-training. GPT-4 exhibits significant improvements in these areas, but it still lags behind DocRepair in deixis, lexical consistency, and ellipsis (inflection) phenomena. Takeaway: (1) GPT-3.5 demonstrates lower accuracy in contrastive prediction compared to conventional translation models, whereas GPT-4 exhibits significant improvement. (2) As there is no detailed technical report available for GPT-4, we argue that its significant improvements are likely due to the use of supervised data and RLHF. We further explore this in Section 5.2.\nEvaluation on Explanation We conduct human evaluations to assess the quality of LLM-generated explanations. This provides an additional way to explore the discourse knowledge contained within LLMs. As illustrated in Table 8, we randomly select 100 examples for each contrastive test set and request native speakers to evaluate whether the models\u2019 responses contain the correct prediction and explanation, respectively. Then the Phi coefficient (r\u03d5) is further calculated to better measure the correlation between two binary variables (i.e., prediction and explanation). We can observe that the accuracy of explanation is often not reflective of the accuracy of prediction, indicating a mismatch in utilizing discourse knowledge for prediction and explanation. In addition, GPT-3.5 is not good at explaining the reason for selecting the correct translation, while GPT-4 exhibits high performance in this aspect and brings better accuracy of prediction. Takeaway: (1) GPT-4 demonstrates a strong ability to explain discourse knowledge. (2) Despite GPT-4\u2019s superior performance in prediction and explanation, the correlation between prediction and explanation does not appear to be significantly improved compared to GPT-3.5."
        },
        {
            "heading": "5.2 Potential Impacts of Training Techniques",
            "text": "LLMs have become the foundation of natural language processing research (Brown et al., 2020), with recent advancements such as learning from source code (Chen et al., 2021) and RLHF showing promise in improving LLM performance (Ouyang et al., 2022). To investigate the potential impacts of these approaches on discourse modelling, we conduct experiments on Chinese\u21d2English Fiction and English\u21d2Russian datasets using different variants of LLMs trained with distinct techniques (detailed in \u00a7A.3). Accordingly, we use P3 and P4 prompts.\nTable 9 shows the results. When SFT with highquality demonstration examples, the translation performance can achieve 14.1 d-BLEU, which reaches to an acceptable level (InstructGPT +FeedME-1 vs. +SFT). Moreover, code pretraining can improve the document translation quality by 2 dBLEU points and the discourse knowldge probing by 4/1.5 (CodexGPT +FeedME-2 vs. InstructGPT +FeedME-1). When further adding PPO method, it outperforms all other combination strategies on translation quality, discourse awareness and discourse knowledge probing (CodexGPT +FeedME2 +PPO vs. others). This shows that RLHF strongly enhances LLM\u2019s capability of translation. Lastly, GPT-3.5 and GPT-4 excel in d-BLEU and human evaluation scores as well as probing performance, demonstrating the importance of contextual information for complex, lengthy document translation. Takeaway: (1) Methods like code pretraining, SFT and RLHF appear to enhance the performance of document translation and discourse modeling; (2) However, it is quite challenging to explore the non-open source systems due to various confounding factors introduced during their development. Therefore, we advocate for organizations like OpenAI to provide greater transparency to aid researchers in such explorations."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We provide a comprehensive evaluation of LLMs (such as GPT-3.5 and GPT-4) for document-level machine translation. Our evaluation covers three main aspects: (1) the effects of discourse-aware\nprompts, (2) comparison of advanced translation models, and (3) analysis of discourse modelling abilities. With the release of the GPT-4 model, the discourse-aware performance has been significantly improved, making it a promising paradigm for document-level translation. Despite its prowess in generative tasks, it struggles with discerning subtle distinctions for ranking.\nIn our future work, we plan to explore more document-level evaluation method (Castilho, 2021; Jiang et al., 2023; Kocmi and Federmann, 2023), latest long-text benchmarks (Wang et al., 2023a; Thai et al., 2022; Wang et al., 2023c) and other MT scenarios (Ghazvininejad et al., 2023; Guerreiro et al., 2023; Lyu et al., 2023). Furthermore, we intend to delve into a more detailed analysis and comparison in future work. For instance, we will employ appropriate significant test methods to account for multiple comparisons (e.g. nonparametric Kruskal-Wallis test, Bonferroni correction) and conduct a power analysis (Card et al., 2020; Graham et al., 2020; Vilar et al., 2022; Hendy et al., 2023). About annotation consistency, we further apply the Krippendorff\u2019s alpha coefficient and check the confidence interval (Krippendorff, 2011).\nLimitations\nWe list the main limitations of this work as follows: \u2022 Potential Inaccuracy of Conclusions. Our con-\nclusions are derived from experiments conducted on a limited set of datasets, which may not guarantee accuracy or applicability across all contexts. These limitations might inadvertently introduce\nbias or overlook certain phenomena, potentially impacting the comprehensiveness of our findings. In response to this concern, we strive to use an extensive array of the most recent datasets, spanning various language pairs and domains. This broad coverage aims to encompass a wide range of linguistic and contextual variations, thereby enhancing the generalizability of our findings.\n\u2022 Model Updates in ChatGPT and Reproducibility. When the underlying model or its parameters of ChatGPT are updated or changed, the conclusions derived from prior evaluations may no longer be valid or entirely accurate. To mitigate this issue, this paper has tried utmost to ensure the reproducibility of our findings: (1) We release all system outputs accompanied by exact timestamps and change logs. This ensures that researchers can reliably reproduce and validate our results. (2) We evaluated all systems at two distinct points: in March and August 2023. While there were minor variations in the exact performance figures between these two evaluations, our overarching conclusions and core findings remained unchanged and consistent.\n\u2022 Criteria of Human Evaluation and Refinement. The design of the criteria still has room for improvement. For example, in the \"Discourse Awareness\" category, there is only a slight difference between Scores 5 and 4, but a more significant gap between Scores 3 and 2. Given the scarcity of benchmark standards on discourse evaluation from past research, we published the detailed scores for further analysis and highlight this area as an opportunity for refinement.\nEthical Considerations\n\u2022 Annotation Process and Annotator Profile. A one-week trial annotation phase was conducted for bidding (five companies participated) on our enterprise-level annotation platform. The authors answered questions posted by the annotators of these companies and updated annotation guidelines accordingly. The Q&A history is recorded and updated in the formal annotation phase. After evaluating the trial annotations based on accuracy and consistency, we selected a professional language service company (large enterprise9) headquartered in Beijing, China. Their annotators were experts in both source and target languages,\n9It is based on the information provided by https://www. kanzhun.com/firm.\nwith a background in translation and linguistics (detailed in Table11). To understand any potential biases, annotators were inquired about their familiarity with the specific translation models under evaluation and any affiliations with AI or translation companies. We ensured that none of the annotators had conflicts of interest, and their annotations were routinely cross-checked for consistency. In terms of compensation, annotators received an hourly wage of $37.4. This rate aligns closely with the mean hourly wages observed for U.S. interpreters/translators and foreign language teachers.10\n\u2022 Annotator Consent and IRB Review. Prior to the commencement of the study, all participating annotators gave their informed consent, confirming their understanding of the study\u2019s objectives and the intended research use of their annotations. An exhaustive IRB review was undertaken and finalized before the project\u2019s onset (IRB Protocol Number: IRB-2023-00067 and Approval Date: 01/12/2023). The protocol employed in this work was approved by the Tencent Institutional Review Board. We remain steadfast in our commitment to uphold and adhere to the institution\u2019s highest ethical and professional benchmarks throughout our research endeavors.\n\u2022 Reproducibility Challenges and Mitigation Strategies. The evolving nature of closed commercial platforms indeed presents challenges to the reproducibility of research. As these platforms continue to upgrade and improve, previous versions of models may be retired or modified, which can make replication efforts problematic. To tackle this issue, we have made several additions to our paper: (1) Documentation of Specifics: We have included exact versions of models we evaluated, along with the precise date of evaluation and other pertinent details. This allows for a clear record of the conditions under which our study was conducted. (2) Release of System Outputs: We release all system outputs, which ensures that researchers can reliably reproduce and validate our results. (3) Advocacy for Archiving Historical Versions: We emphasize the importance for both the AI community and commercial organizations to maintain archives of previous model iterations. By doing this, re-\n10It is based on the information provided by https://www. bls.gov/oes/current/oes273091.htm and https://www. bls.gov/oes/current/oes251124.htm.\nsearchers can readily access and evaluate past versions, ensuring continuity in analysis even as new model versions emerge."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful to the anonymous reviewers, area chairs and ethics committee for their insightful comments and suggestions which will serve to improve the paper considerably. Their insights are invaluable, not only in helping us present our findings more cautiously, but also in educating us beyond the scope of this single paper."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Significance Testing\nMachine Translation For automatic evaluations, we used the non-parametric one-tailed Wilcoxon signed-rank test (Woolson, 2007). About results in Table 4, the significance test contrasting GPT3.5/GPT-4 with others yields a p-value of less than 0.05, indicating they do significantly boosts translation quality. For human evaluation, we employ Welch\u2019s t-test (Bl, 1947). Table 10 shows the overall significance test by combining all datasets, and the results for each domain are also consistent.\nProbing Task We performed a Welch\u2019s t-test (Bl, 1947) with unequal variances to verify the significance between GPT-3.5 and GPT-4 in Table 7 and 8. We find that the corresponding two-tailed pvalue is smaller than 0.001, which indicates the significance between them.\nA.2 Human Evaluation Guidelines\nGuidelines Table 12 presents the human evaluation criteria for document-level translation, with scores ranging from 0 to 5. A score of 5 indicates excellent overall translation quality, with no grammatical errors, accurate word choice, consistent key terms, and consistent context and tone throughout the passage. A score of 0 indicates poor overall translation quality, with more than half of the translation being mistranslated or missing, inconsistent key terms, and poor fluency and clarity. In between scores reflect varying degrees of translation quality, with factors such as fluency, accuracy, consistency of key terms, and context and tone consistency affecting the score.\nHuman Evaluators We employed two professional evaluators for our study through Tencent\u2019s designated supplier. Table 11 detailed their background related to this task.\nA.3 Training Methods in LLMs\n\u2022 GPT-3 (Brown et al., 2020): A LLM with 175B parameters pre-trained on large-scale web corpora (approximately 400B tokens) We used OpenAI API davinci.\n\u2022 InstructGPT (SFT) (Ouyang et al., 2022): A GPT-3 model trained with supervised fine-tuning on human demonstrations similar to Ouyang et al. (2022).11\n\u2022 InstructGPT (FeedME-1) (Ouyang et al., 2022): An improved version of GPT-3 with supervised\n11https://platform.openai.com/docs/ model-index-for-researchers.\nfine-tuning on human-written demonstrations and model-generated examples rated by humans with high quality.\n\u2022 InstructGPT (FeedME-2) (Ouyang et al., 2022): An improved version of Codex (Chen et al., 2021) with supervised fine-tuning on human-written demonstrations and human-rated examples with high quality.12\n\u2022 InstructGPT (PPO) (Ouyang et al., 2022): An improved version of InstructGPT (FeedME-2) with extra training of RLHF, which is trained with a reward model learned from human comparisons.13\n\u2022 ChatGPT: A further improved version of InstrucGPT that can perform tasks via dialogue with users, which is able to take contextual information in dialogue into consideration.\n12https://openai.com/blog/openai-codex. 13https://openai.com/blog/\ninstruction-following."
        }
    ],
    "title": "Document-Level Machine Translation with Large Language Models",
    "year": 2023
}