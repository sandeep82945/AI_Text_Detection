{
    "abstractText": "Current dialogue systems face diverse user requests and rapid change domains, making quickly adapt to scenarios with previous unseen slot types become a major challenge. Recently, researchers have introduced novel slot detection (NSD) to discover potential new types. However, dialogue system with NSD does not bring practical improvements due to the system still cannot handle novel slots in subsequent interactions. In this paper, we define incremental novel slot detection (INSD), which separates the dialogue system to deal with novel types as two major phrases: 1) model discovers unknown slots, 2) training model to possess the capability to handle new classes. We provide an effective model to extract novel slots with set prediction strategy and propose a query-enhanced approach to overcome catastrophic forgetting during the process of INSD. We construct two INSD datasets to evaluate our method and experimental results show that our approach exhibits superior performance. We release the data and the code at https://github.com/ cs-liangchen-work/NovelIE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chen Liang"
        },
        {
            "affiliations": [],
            "name": "Hongliang Li"
        },
        {
            "affiliations": [],
            "name": "Changhao Guan"
        },
        {
            "affiliations": [],
            "name": "Qingbin Liu"
        },
        {
            "affiliations": [],
            "name": "Jian Liu"
        },
        {
            "affiliations": [],
            "name": "Jinan Xu"
        },
        {
            "affiliations": [],
            "name": "Zhe Zhao"
        }
    ],
    "id": "SP:fcc3f9905d7974dcb71a565cf102d83ba626d1d6",
    "references": [
        {
            "authors": [
                "Ankur Bapna",
                "Gokhan Tur",
                "Dilek Hakkani-Tur",
                "Larry Heck."
            ],
            "title": "Towards zero-shot frame semantic parsing for domain scaling",
            "venue": "arXiv preprint arXiv:1707.02363.",
            "year": 2017
        },
        {
            "authors": [
                "Pengfei Cao",
                "Yubo Chen",
                "Jun Zhao",
                "Taifeng Wang."
            ],
            "title": "Incremental event detection via knowledge consolidation networks",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko."
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328,",
            "year": 2020
        },
        {
            "authors": [
                "Alice Coucke",
                "Alaa Saade",
                "Adrien Ball",
                "Th\u00e9odore Bluche",
                "Alexandre Caulier",
                "David Leroy",
                "Cl\u00e9ment Doumouro",
                "Thibault Gisselbrecht",
                "Francesco Caltagirone",
                "Thibaut Lavril"
            ],
            "title": "Snips voice platform: an embedded spoken language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Zhang Dongjie",
                "Longtao Huang."
            ],
            "title": "Multimodal knowledge learning for named entity disambiguation",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Keqing He",
                "Jinchao Zhang",
                "Yuanmeng Yan",
                "Weiran Xu",
                "Cheng Niu",
                "Jie Zhou."
            ],
            "title": "Contrastive zero-shot learning for cross-domain slot filling with adversarial attack",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Charles T Hemphill",
                "John J Godfrey",
                "George R Doddington."
            ],
            "title": "The atis spoken language systems pilot corpus",
            "venue": "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "Vojt\u011bch Hude\u010dek",
                "Ond\u0159ej Du\u0161ek",
                "Zhou Yu."
            ],
            "title": "Discovering dialogue slots with weak supervision",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Minsoo Kang",
                "Jaeyoo Park",
                "Bohyung Han."
            ],
            "title": "Class-incremental learning by knowledge distillation with adaptive feature consolidation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.",
            "year": 2022
        },
        {
            "authors": [
                "Gyuhak Kim",
                "Changnan Xiao",
                "Tatsuya Konishi",
                "Zixuan Ke",
                "Bing Liu."
            ],
            "title": "Open-world continual learning: Unifying novelty detection and continual learning",
            "venue": "arXiv preprint arXiv:2304.10038.",
            "year": 2023
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly.",
            "year": 1955
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem."
            ],
            "title": "Learning without forgetting",
            "venue": "IEEE transactions on pattern analysis and machine intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jinchao Zhang",
                "Yufeng Chen",
                "Jinan Xu",
                "Jie Zhou."
            ],
            "title": "A dependency syntactic knowledge augmented interactive architecture for end-to-end aspect-based sentiment analysis",
            "venue": "Neurocomputing.",
            "year": 2021
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jinchao Zhang",
                "Yufeng Chen",
                "Jinan Xu",
                "Jie Zhou."
            ],
            "title": "An iterative multi-knowledge transfer network for aspect-based sentiment analysis",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jinchao Zhang",
                "Jinan Xu",
                "Yufeng Chen",
                "Jie Zhou."
            ],
            "title": "A novel aspect-guided deep transition model for aspect based sentiment analysis",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Jian Liu",
                "Yubo Chen",
                "Kang Liu",
                "Wei Bi",
                "Xiaojiang Liu."
            ],
            "title": "Event extraction as machine reading comprehension",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Minqian Liu",
                "Shiyu Chang",
                "Lifu Huang."
            ],
            "title": "Incremental prompting: Episodic memory prompt for lifelong event detection",
            "venue": "arXiv preprint arXiv:2204.07275.",
            "year": 2022
        },
        {
            "authors": [
                "Zihan Liu",
                "Genta Indra Winata",
                "Peng Xu",
                "Pascale Fung."
            ],
            "title": "Coach: A coarse-to-fine approach for cross-domain slot filling",
            "venue": "arXiv preprint arXiv:2004.11727.",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Madotto",
                "Zhaojiang Lin",
                "Zhenpeng Zhou",
                "Seungwhan Moon",
                "Paul Crook",
                "Bing Liu",
                "Zhou Yu",
                "Eunjoon Cho",
                "Zhiguang Wang."
            ],
            "title": "Continual learning in task-oriented dialogue systems",
            "venue": "arXiv preprint arXiv:2012.15504.",
            "year": 2020
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen."
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "Psychology of learning and motivation.",
            "year": 1989
        },
        {
            "authors": [
                "Natawut Monaikul",
                "Giuseppe Castellucci",
                "Simone Filice",
                "Oleg Rokhlenko."
            ],
            "title": "Continual learning for named entity recognition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Libo Qin",
                "Fuxuan Wei",
                "Tianbao Xie",
                "Xiao Xu",
                "Wanxiang Che",
                "Ting Liu."
            ],
            "title": "Gl-gin: Fast and accurate non-autoregressive model for joint multiple intent detection and slot filling",
            "venue": "arXiv preprint arXiv:2106.01925.",
            "year": 2021
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Dilek Hakkani-T\u00fcr",
                "Larry Heck."
            ],
            "title": "Scalable multi-domain dialogue state tracking",
            "venue": "2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "Proceedings of the AAAI conference on artificial intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Darsh J Shah",
                "Raghav Gupta",
                "Amir A Fayazi",
                "Dilek Hakkani-Tur."
            ],
            "title": "Robust zero-shot cross-domain slot filling with example values",
            "venue": "arXiv preprint arXiv:1906.06870.",
            "year": 2019
        },
        {
            "authors": [
                "Zeqi Tan",
                "Yongliang Shen",
                "Shuai Zhang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "A sequence-to-set network for nested named entity recognition",
            "venue": "arXiv preprint arXiv:2105.08901.",
            "year": 2021
        },
        {
            "authors": [
                "Weikang Wang",
                "Jiajun Zhang",
                "Qian Li",
                "Mei-Yuh Hwang",
                "Chengqing Zong",
                "Zhifei Li."
            ],
            "title": "Incremental learning from scratch for task-oriented dialogue systems",
            "venue": "arXiv preprint arXiv:1906.04991.",
            "year": 2019
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Zequn Zhang",
                "Li Jin",
                "Zhi Guo",
                "Shuchao Li",
                "Weihong Wang",
                "Jianwei Lv."
            ],
            "title": "Heft: A history-enhanced feature transfer framework for incremental event detection",
            "venue": "KBS.",
            "year": 2022
        },
        {
            "authors": [
                "Yanan Wu",
                "Zhiyuan Zeng",
                "Keqing He",
                "Hong Xu",
                "Yuanmeng Yan",
                "Huixing Jiang",
                "Weiran Xu."
            ],
            "title": "Novel slot detection: A benchmark for discovering unknown slot types in the task-oriented dialogue system",
            "venue": "arXiv preprint arXiv:2105.14313.",
            "year": 2021
        },
        {
            "authors": [
                "Yuxia Wu",
                "Tianhao Dai",
                "Zhedong Zheng",
                "Lizi Liao."
            ],
            "title": "Actively discovering new slots for task-oriented conversation",
            "venue": "arXiv preprint arXiv:2305.04049.",
            "year": 2023
        },
        {
            "authors": [
                "Yuxia Wu",
                "Lizi Liao",
                "Xueming Qian",
                "Tat-Seng Chua."
            ],
            "title": "Semi-supervised new slot discovery with incremental clustering",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Yu Xia",
                "Quan Wang",
                "Yajuan Lyu",
                "Yong Zhu",
                "Wenhao Wu",
                "Sujian Li",
                "Dai Dai."
            ],
            "title": "Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Yan",
                "Junda Ye",
                "Zhongbao Zhang",
                "Liwen Wang."
            ],
            "title": "Aisfg: Abundant information slot filling generator",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Yu",
                "Heng Ji",
                "Prem Natarajan."
            ],
            "title": "Lifelong event detection with knowledge transfer",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Nasser Zalmout",
                "Xian Li"
            ],
            "title": "Prototyperepresentations for training data filtering in weaklysupervised information extraction",
            "year": 2022
        },
        {
            "authors": [
                "Hanlei Zhang",
                "Hua Xu",
                "Ting-En Lin",
                "Rui Lyu."
            ],
            "title": "Discovering new intents with deep aligned clustering",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaodong Zhang",
                "Houfeng Wang."
            ],
            "title": "A joint model of intent determination and slot filling for spoken language understanding",
            "venue": "IJCAI, volume 16.",
            "year": 2016
        },
        {
            "authors": [
                "Yunan Zhang",
                "Qingcai Chen."
            ],
            "title": "A neural spanbased continual named entity recognition model",
            "venue": "arXiv preprint arXiv:2302.12200.",
            "year": 2023
        },
        {
            "authors": [
                "Junhao Zheng",
                "Zhanxian Liang",
                "Haibin Chen",
                "Qianli Ma."
            ],
            "title": "Distilling causal effect from miscellaneous other-class for continual named entity recognition",
            "venue": "arXiv preprint arXiv:2210.03980.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Slot filling (SF), a crucial component of taskoriented dialogue systems, aims to identify specific spans in user utterances (Zhang and Wang, 2016; Shah et al., 2019; Qin et al., 2021; Liang et al., 2021a; Hudec\u030cek et al., 2021; Yan et al., 2022). As dialogue systems are widely used in daily life, traditional slot filling needs to be quickly adapted to scenarios with previous unseen types (Shah et al., 2019; He et al., 2020; Liu et al., 2020b), which is known as a novel slot detection (NSD) task (Wu et al., 2021, 2022).\n\u2217 Work was done under the guidance of Jian Liu et al. when Liang in Beijing Jiaotong University, Li and Guan provided some experimental results.\n\u2020Corresponding author.\nHowever, prior research in NSD only focuses on identifying new slots (Wu et al., 2021, 2022), which is too idealized and does not bring practical improvements to the dialogue system. Consider the user\u2019s request contained an unknown type of slot \u2019tooth will out\u2019 in Figure 1, the dialogue system with novel slot detection just provides a reply that is not wrong but less than satisfactory, and will deliver an inability to handle it all the time. Actually, dialogue systems need a specialized procedure to deal with discovered new slot or entity types.\nIn this paper, we focus on a more practical setting: after the model recognizes new types, the dialogue system will view them as known and can return satisfying replies to user requests concerning\nthese types in subsequent interactions, as shown at the bottom of Figure 1. We start with a collection of annotated examples with some pre-defined types. Our goal is to identify examples with novel types (i.e., types not contained in the pre-defined set), and then, the new types are seen as known, and we should train model to have the ability to identify entities of these types, therefore resulting in incremental learning (Cao et al., 2020; Monaikul et al., 2021; Yu et al., 2021).\nWe separate the dialogue system to deal with novel types as two major phrases: 1) model discovers slots with unknown classes, 2) training model to possess the capability to handle new classes. We call this unified process incremental novel slot detection (INSD). For identifying new slots, we employ the set prediction method to generate a great number of triples (start, end, type), and subdivide the non-predefined entity triple into the non-entity triple and the novel-entity triple to extract novel spans. Given that the NSD task lacks a clear objective for fitting due to unseen information of new classes at the training stage, we further provide two strategies to make the model better suit this task, which can improve robustness and encourage model to generate diverse outputs. For the INSD task, we construct two datasets based on slot filling dataset (Coucke et al., 2018; Hemphill et al., 1990). It is generally challenging to mitigate the catastrophic forgetting (McCloskey and Cohen, 1989) during the process of INSD, inspired by previous works (Cao et al., 2020; Monaikul et al., 2021; Liu et al., 2022; Xia et al., 2022), we propose a queryinduced strategy to realize explicitly knowledge learning from stored data and implicitly knowledge transferring from old model together.\nWe evaluate the proposed method on two common benchmarks and two datasets we constructed. From the results, our approach exhibits promising results on INSD and NSD (\u00a7 5). We also perform ablation studies to assess our method of discovering new classes and solving catastrophic forgetting (\u00a7 6.1). Finally, we conduct further experiments to explore the NSD model\u2019s ability to identify indomain and novel slots (\u00a7 6.2).\nTo summarize, the contributions of our work are three-fold:\n\u2022 We define incremental novel slot detection (INSD) aligned with real-world scenarios to improve the dialogue system\u2018s ability to deal with new slot types, and we provide effective\nstrategies to overcome catastrophic forgetting during incremental learning.\n\u2022 We propose a novel slot detection (NSD) model with set prediction, which demonstrates broader applicability.\n\u2022 Results show that our method delivers the best performance on INSD and NSD tasks."
        },
        {
            "heading": "2 Related Work",
            "text": "Novel Slot Detection. Prior works have extensively studied slot filling (SF) task (Zhang and Wang, 2016; Bapna et al., 2017; Rastogi et al., 2017; Shah et al., 2019; Rastogi et al., 2020; Liang et al., 2021b; Yan et al., 2022) to recognize specific entities and fill into semantic slots for user utterances. Recently, Wu et al. (2021) define novel slot detection (NSD) to extract out-of-domain or potential new entities and propose a two-step pipeline to detect them, which has attracted much attention. Subsequent works focus on the end-to-end paradigm (Liang et al., 2019) to directly obtain novel entities without intermediate steps. Wu et al. (2023) design a bi-criteria active learning framework and Wu et al. (2022) introduce semisupervised learning scheme to perform iterative clustering. In contrast to the previous method, we propose a generative method based on set prediction to produce new entity triples.\nThe set prediction is an end-to-end method that enables model to directly generate the final expected set, which was proposed in DETR (Carion et al., 2020) to solve object detection task. Researchers have introduced it to natural language processing (NLP). Tan et al. (2021) propose a sequence-to-set network to handle the complexity of nested relationships in named entity recognition task. Dongjie and Huang (2022) obtain multimodal inputs representation by DETR\u2019s decoder.\nIncremental Learning. Incremental learning (Wang et al., 2019; Madotto et al., 2020; Kim et al., 2023) has been introduced to simulate the setting of type/class-increasing in real-world scenarios, which is mainly researched in information extraction (IE) (Wei et al., 2022; Zhang and Chen, 2023). Different from assuming data arrived with definite entity types at each stage (Cao et al., 2020; Monaikul et al., 2021), we define a new task incremental novel slot detection that type is unknown and needs to discover in the new data stream.\nCatastrophic forgetting (McCloskey and Cohen, 1989) is a long-standing problem in incremental learning. Current approaches to overcome this issue can be roughly classified into two groups. The first is knowledge distillation (KD) (Li and Hoiem, 2017; Monaikul et al., 2021; Yu et al., 2021; Zheng et al., 2022; Kang et al., 2022), which transfers the knowledge from the previous model into the current model. The second is data retrospection (Cao et al., 2020; Liu et al., 2022; Xia et al., 2022), which samples old data and stores them in a memory buffer with a fixed capacity. In this paper, we propose a query-induced strategy to enhance KD and data retrospection."
        },
        {
            "heading": "3 Approach",
            "text": ""
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "Novel slot detection (NSD) is a fresh task in slot filling (SF). For a given sentence S, a NSD model aims at extracting potential new entities or slots in S (Wu et al., 2021). The slot in the NSD dataset D consists of two type sets: pre-defined type set Tp and novel type set Tn. Sentences in the training set only contain Tp, whereas sentences in the test set include both Tp and Tn.\nIn realistic scenarios, once the model has discovered new slot classes, these types should be treated as known. Therefore, we define a new task incremental novel slot detection (INSD) aligned with real-world scenarios. Following previous work in incremental learning (Cao et al., 2020; Monaikul et al., 2021), we adopt the assumption that the entity class arrived at different time points. But different from them, novel types will be identified at each stage and be seen as known in the next stage in our setting.\nFor INSD task, we construct the dataset D = {D1, D2, ..., Dk} with k stage, where D1 is annotated and others is unlabeled. Each Di in D contains a slot type set Tpi = {t1, t2, ...}, where types in Tp1 are pre-arranged in the initial stage and Tpi(i > 1) is composed of new types detected by model Mi\u22121 (Mi\u22121 is training at i \u2212 1 stage). In other words, the slot classes that are predicted as novel in i\u2212 1 stage will be viewed as in-domain types in i-th stage. At step i, we first apply the model Mi\u22121 to annotate Di with slot types Tpi, which are new discovered classes at step i \u2212 1. Then, we train Mi\u22121 on labeled Di and get Mi. Mi also has the competence to identify novel classes contained in next-stage data. The process is shown\nat the top of Figure 2."
        },
        {
            "heading": "3.2 Novel Detection with Set Predicted Network",
            "text": "Next, we describe the proposed NSD model, which is depicted at the bottom-left of Figure 2. We first present the details of the model structure, which consists of two main components: query representation and learning and bipartite matching. Finally, we show how to extract novel entities.\nQuery Representation. Our model locates novel entities by recognizing the start positions and end positions based on a group of queries. Considering the information of the novel entity is entirely invisible in NSD task, we employ a random initialization strategy for the representation matrix of queries and optimize them by the model itself during training, which is distinct from the joint encoding approach in the machine reading comprehension (MRC) model (Liu et al., 2020a). Let Qs \u2208 Rd\u00d7l and Qe \u2208 Rd\u00d7l stand for the embedding matrix of queries Q = {q1, .., ql} to predict start position and end position respectively, where l is the number of queries and d is their dimension.\nLearning and Bipartite Matching. For the given sentence S, we adopt BERT-large (Devlin et al., 2018) to encode it and get representation H . By fusing Qs, Qe and H , the model can generate triples (s.e, t), which is composed of the start position s, end position e and slot type t. Specifically, we first calculate probability distributions of each token being start index and end index, as follows:\nPs = softmax(Qs \u00b7H) (1)\nPe = softmax(Qe \u00b7H) (2)\nThen we integrate the above positional probability information into H to get span representation, which can be regarded as an attention mechanism:\nHspan = \u2211 i ((P (i)s \u00b7H(i)) + (P (i)e \u00b7H(i))) (3)\nwhere the superscript (i) means the i-th value in matrix. Finally, we predict the slot class of Hspan, which comprises pre-defined slot types and label \u2019O\u2019 (indicates that a span does not belong to indomain slot types). We compute the probability distribution of each class as follows:\nPt = T \u00b7Hspan + b \u2208 R1\u00d7(j+1) (4)\nwhere T and b are parameters in the linear layer, and j is the number of pre-defined entity type.\nGiven that a sentence may contain one or more entities, we use hungarian algorithm (Kuhn, 1955) to optimize the matching of triples between the predicted entity y = (s.e, t) and golden entity y\u0302 = (s\u0302, e\u0302, t\u0302).\nFor training model, we adopt the following loss:\nL(y, y\u0302) = \u2212 log(wsPs(s|s\u0302)+ wePe(e|e\u0302) + wtPt(t|t\u0302))\n(5)\nwhere ws, we and wt are the weights for different tokens and entity types.\nNovel Type Prediction. We modify the model\u2019s loss to predict novel entities. Specifically, the value in wb and we for the last token \u2019[SEP]\u2019 and the value in wt for the label \u2019O\u2019 are both set to a smaller value than others. After training the model on the corrected loss, there are two forms triplet for nonpre-defined entities: i) (ssep, esep, t): the span is the last token \u2019[SEP]\u2019 with any label, ii) (s, e,O): the label \u2019O\u2019 with any span. By assigning appropriate values for wb, we and wt, we can treat the second form (s, e,O) as novel entity triple.\nWe further present two strategies to make the model better suit the NSD: i) contrastive learning: we pull the entity embeddings of the same type together in a mini-batch (Zhang et al., 2021) to improve model robustness. ii) noise-induced adversarial learning: we propose adversarial training with noise injection to alleviate the overfitting\nof the model to in-domain types and promote the model to generate diverse triples. Random values are added to the query representation Qs and Qe at the training stage."
        },
        {
            "heading": "3.3 Query Enhanced Incremental Learning",
            "text": "Based on constructed NSD model, we now introduce our approach to mitigate the problem of catastrophic forgetting during the process of INSD, separated by data retrospection and knowledge distillation with query representation enhancement. We summarize the procedure in Algorithm 1.\nQuery Enhanced Data Retrospection Data retrospection method manages a constrained memory buffer B with a capacity of c to store a small number of old examples. The capacity size c is constant due to data storage costs. When new data comes, we will update the memory buffer to release allocated space for accommodating new types. Therefore, selecting appropriate examples from old data is crucial for data retrospection. Prototype representation has been successfully utilized in samples filtering (Cao et al., 2020; Zalmout and Li, 2022), which is a center vector integrating all features from given dataset. The calculation is as follows:\nHc = 1\nN N\u2211 n=1 Hn (6)\nwhere N is the number of data, Hn is the embedding vector of the first token \u2019[CLS]\u2019 in a sentence and is seen as the sentence representation.\nWe believe there are distinctions between slot classes, attributed to differences in data volume and learning difficulty. Consequently, we propose a new prototype learning with a query-induced data split strategy, rather than uniformly partitioning the space according to slot types. We first leverage query to cluster data: the sentences in which novel entities were successfully identified by query qi are grouped together as a subset. Then we compute the prototype representation of the subset and calculate the distance between the center and each sentence in subsets. The sentence with a small distance will be sampled and stored in the buffer B.\nIn our method, we treat data at each stage equally, so the memory B is divided evenly according to the number of stages. At stage i, we discard c/i data in B and utilize that memory buffer to store the examples sampled in Di.\nQuery Enhanced Knowledge Distillation Knowledge distillation addresses catastrophic forgetting by transferring the knowledge already acquired in the previous model. At i-th stage, Mi\u22121 acts as a teacher model that possesses the ability to identify old slot classes, and Mi serves as a student model. By distilling the knowledge from Mi\u22121 to Mi, Mi can largely overcome the catastrophic forgetting for old types.\nThe distillation module of previous methods generally contains encoded sentence representation and predicted probability distribution (the probability of the start position, end position, and type classification in our model). The loss is calculated as follows:\nLsent = MSE([H]i\u22121, [H]i) (7)\nLp = KL([P ]i\u22121, [[P ]i), P \u2208 {Ps, Pe, Pt} (8)\nwhere KL represents the KL divergence loss and MSE denotes the mean squared error function. [\u00b7]i\u22121 and [\u00b7]i are the value computed by model Mi\u22121 and Mi, respectively. Considering that the query plays a crucial role in the extraction of span and corresponding type, we also distill the query representation:\nLquery = MSE([q]i\u22121, [q]i) (9)\nFinally, we train the model with the following loss:\nLkd = Lsent + Lp + Lquery (10)\nAlgorithm 1 Addresses Catastrophic Forgetting Input: training set D = {D1, D2, ..., Dk} with k stage, memory buffer B with capacity c, NSD model M\nQuery Enhancement Integration We integrate the aforementioned two query enhancement strategies to realize explicitly knowledge learning from old data and implicitly knowledge transferring from previous model together. We summarize the procedure in Algorithm 1."
        },
        {
            "heading": "4 Experimental setups",
            "text": "Experimental Setting. We conduct experiments on two slot filling datasets: Snips (Coucke et al., 2018) and ATIS (Hemphill et al., 1990). Snips is annotated with 13,084 sentences and has 39 slot types. ATIS provides 4,478 training examples with 44 slot types. The statistics of the datasets are shown in Table 1.\nFor NSD task, we follow the data setup in Wu et al. (2021). We employ the remove processing strategy that randomly selects several classes as unknown and the sentences containing unknown slots are deleted from the training set. For metrics, we apply the span-level precision (P), recall (R), and f1-scores (F1).\nFor INSD task, we construct two multi-stage datasets based on Snips and ATIS. We divide the official training set into k subsets {D1, D2, ..., Dk} and assign slot types {Tp1, Tp2, ..., Tpk} for each stage. We process the Di as follows: 1) directly remove the sentences including types in Tp\u00b7 (\u00b7>i), 2) label the slot values belonging to Tp\u00b7 (\u00b7<i) with \u2019O\u2019, and the corresponding text tokens are replaced with \u2019MASK\u2019, 3) Di (i >1) is unannotated and needs NSD model to discover and label novel slots. We exploit four metrics to assess model perfor-\nmance: micro-averaged f1 and macro-averaged f1 for current known types, and micro-averaged f1 and macro-averaged f1 for all types.\nImplementations. We employ BERT (Devlin et al., 2018) as contextualized encoder to implement our model. For constructed multi-stage INSD dataset, the k is set to 5 in Snips and 3 in ATIS. The model is trained for 30 epochs with a learning rate of 1e-5 and a batch size of 8. The query number is selected in {20, 40, 60, 80} and the weight to generate novel slot span is chosen from {1e-1, 1e-3, 1e-5}. We set the capacity of memory as 100.\nBaselines. For INSD task, we compare our approach to the knowledge distillation model KL (Monaikul et al., 2021) and the data retrospection model KCN (Cao et al., 2020). For NSD task, we compare the proposed method to the pipeline model GDA (Wu et al., 2021) with two distance\nstrategies GDA-min and GDA-different."
        },
        {
            "heading": "5 Overall Results",
            "text": "In this section, we present the main results of incremental novel slot detection (INSD) framework and novel slot detection (NSD) model.\nModel Performance of INSD We visualize the results of our proposed approach and baselines during the incremental learning process in Figure 3. It can be observed that our method demonstrates the best performance compared with baseline approaches for all metrics and datasets, achieving 22.73% macro-f1 and 41.75% micro-f1 on Snips and 26.37% macro-f1 and 72.53% micro-f1 on ATIS respectively. Such results indicate that our method shows the best ability to overcome catastrophic forgetting for the INSD task. From the figure, we also see that the data retrospection method\n(KCN) brings much higher f1 improvements than the knowledge distillation approach (KL), which suggests that the explicit data-storing strategy is more effective. Our proposed approach introduces queries to enhance them and can further improve model performance.\nModel Performance of NSD Table 2 presents the performance of our method as well as baselines on two datasets. For extracting novel slots (NSD), we can see that our model outperforms previous methods by a large margin for all datasets and scenarios. For example, our method beats GDA-different by gaining improvements of 7.82%, 20.13%, and 8.52% in terms of F1-scores in different settings on Snips. For identifying in-domain slot (IND), our model also obtains comparable results compared with baselines, but it exhibits inferior performance in some settings such as 15% sampling on ATIS. By comprehensively analyzing, we can find that our proposed model demonstrates better capability in discovering novel types of slots than known slots."
        },
        {
            "heading": "6 Discussion",
            "text": "We conduct further experiments to investigate the proposed INSD framework and NSD model, separated by ablation study and the learning ability of the NSD model."
        },
        {
            "heading": "6.1 Ablation Experiment",
            "text": "Ablation Study of INSD We perform ablation experiments to investigate the proposed INSD method. The results of 3-th and 5-th stage are depicted in table 3. As can be seen, our query enhancement approach can lead to better performance, attaining 1% and 6% increases in terms of F1 separately in knowledge distillation and data storage. Results demonstrate that query representation is important in novel slot extraction and indicate that first clustering the data via queries and then calculating the prototype representation can select beneficial samples.\nAblation Study of NSD We conduct an ablation study to explore the impact of contractive learning (CL) and noise-induced adversarial learning (NL) on our NSD model. The results are presented in Table 4. As expected, CL and NL can both yield greater F1 scores than the base model. Overall, the NL method is more effective because noise can contribute to generating diverse outputs. In addi-\ntion, unlike typical NLP tasks, NSD task lacks a clear objective for fitting and is trained with slot filling (SF) task, which makes it essential to introduce noise during training stage. By employing both CL and NL, the model demonstrates a much higher improvement in F1-score, surpassing the sum of the gains obtained by each method individually. A reasonable explanation is that our proposed CL can enhance model robustness by encouraging entity embeddings of the same type to be close, which mitigates the negative impact of noise when training model.\nThe Impact of Weight Parameters and Query Number We study the effect of weight and query number on model performance. For simplicity, we set fixed entity type weight wt for the label \u2019O\u2019 and assign the same value for the last position weight ws and we. Figure 4 depicts the results. We can conclude that: 1) Our model exhibits high sensitivity to the weight and query number. 2) As the count of novel entities increases (5%\u219210%\u219215%), the\nmodel requires a greater number of queries and reduced weight values, which both encourage our model to generate a larger quantity of novel entity triplets (s, e, t). Specifically, the increasing number of queries allows the model to learn various query representations for different novel slot types, while decreasing the weight values can result in a higher count of (s, e,O) triplets (the label \u2019O\u2019 with any span) and a lower count of (ssep, esep, t) triplets (the span is the last token \u2019[SEP]\u2019 with any label)."
        },
        {
            "heading": "6.2 Learning Ability of NSD Model",
            "text": "The Ability to Discover Novel Entity We want to explore the NSD model\u2019s inclination in identifying novel slot classes, are newly discovered and pre-defined types similar or dissimilar? To answer this, we first randomly select a subset of classes as in-domain and then choose novel classes from the remains based on label semantics similarity and entity representation similarity. We conduct experiments in two settings \u2019Sim\u2019 and \u2019Disim\u2019. The results are shown in Table 5. We can observe that the previous model GDA tends to discover novel types that differ from pre-defined types, whereas our model delivers superior performance in all settings. Results reveal the broader applicability of our model.\nLearning Ability of Queries We investigate the ability of queries to identify various types of spans, containing non-entity, novel entity, and in-domain entity. The proportion of new and pre-defined entities extracted by each query is plotted in Figure 5,\nwhere the queries that only recognize the non-entity span were not drawn and we only present the result in 5% and 15% scenarios on Snips. From the figure, we can see that the count of queries for nonentity greatly outnumbers the queries for the entity, such as 7 entity queries and 73 non-entity queries in the 15% sampling setting, primarily due to the higher number of non-entity spans. Interestingly, it can be observed that each entity query possesses the capability to recognize both potential new and pre-defined entities in our model."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we introduce a new task incremental novel slot detection (INSD) and establish a benchmark with two datasets and four metrics. Moreover, we propose a query-induced strategy to alleviate the problem of catastrophic forgetting during incremental learning and provide an effective model to identify novel slots. Our proposed approach achieves state-of-the-art performance in all datasets and scenarios."
        },
        {
            "heading": "8 Limitations",
            "text": "Although the proposed method exhibits superior performance in overcoming catastrophic forgetting and identifying novel slots, the extracted entity quantity decreases as the stages advance, and several classes cannot even be recognized. However, for incremental novel slot detection task, it is crucial to discover as many unseen classes as possible. Another limitation is that our model to detect new slots shows high sensitivity to the weight and query number, which means that researchers need to spend time to carefully tune the parameters for different datasets."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "This work was supported by the Fundamental Research Funds for the Central Universities 2023JBMC058. It was also supported by the National Natural Science Foundation of China (No.62106016, 61976015, 61976016, 61876198 and 61370130), and the Open Projects Program of the State Key Laboratory of Multimodal Artificial Intelligence Systems."
        }
    ],
    "title": "Novel Slot Detection with an Incremental Setting",
    "year": 2023
}