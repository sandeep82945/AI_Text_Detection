{
    "abstractText": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM\u2019s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71% to 83% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier\u2019s performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amos Azaria"
        },
        {
            "affiliations": [],
            "name": "Tom Mitchell"
        }
    ],
    "id": "SP:3d55926d590e83ad8f9a07de626fc60de6c973ab",
    "references": [
        {
            "authors": [
                "Balaguer",
                "Nat McAleese",
                "Amelia Glaese",
                "John Aslanides",
                "Matt Botvinick"
            ],
            "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Antonio Bella",
                "C\u00e8sar Ferri",
                "Jos\u00e9 Hern\u00e1ndez-Orallo",
                "Mar\u00eda Jos\u00e9 Ram\u00edrez-Quintana."
            ],
            "title": "Calibration of machine learning models",
            "venue": "Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques, pages 128\u2013146.",
            "year": 2010
        },
        {
            "authors": [
                "Michael Bommarito II",
                "Daniel Martin Katz."
            ],
            "title": "Gpt takes the bar exam",
            "venue": "arXiv preprint arXiv:2212.14402.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Collin Burns",
                "Haotian Ye",
                "Dan Klein",
                "Jacob Steinhardt."
            ],
            "title": "Discovering latent knowledge in language models without supervision",
            "venue": "arXiv preprint arXiv:2212.03827.",
            "year": 2022
        },
        {
            "authors": [
                "Yuanyuan Chen",
                "Zhang Yi."
            ],
            "title": "Adaptive sparse dropout: Learning the certainty and uncertainty in deep neural networks",
            "venue": "Neurocomputing, 450:354\u2013 361.",
            "year": 2021
        },
        {
            "authors": [
                "David Dale",
                "Elena Voita",
                "Lo\u00efc Barrault",
                "Marta R Costa-juss\u00e0."
            ],
            "title": "Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity even better",
            "venue": "arXiv preprint arXiv:2212.08597.",
            "year": 2022
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "arXiv preprint arXiv:1811.01241.",
            "year": 2018
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics",
            "venue": "arXiv preprint arXiv:2104.13346.",
            "year": 2021
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "year": 2023
        },
        {
            "authors": [
                "Konstantinos I Roumeliotis",
                "Nikolaos D Tselikas",
                "Dimitrios K Nasiopoulos"
            ],
            "title": "Llama 2: Early adopters\u2019 utilization of meta\u2019s new open-source pretrained model",
            "year": 2023
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "Fever: a large-scale dataset for fact extraction and verification",
            "venue": "arXiv preprint arXiv:1803.05355.",
            "year": 2018
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Oana Cocarascu",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "The fever2",
            "venue": "0 shared task. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have recently demonstrated remarkable success in a broad range of tasks (Brown et al., 2020; Bommarito II and Katz, 2022; Driess et al., 2023; Bubeck et al., 2023). However, when composing a response, LLMs tend to hallucinate facts and provide inaccurate information (Ji et al., 2023). Furthermore, they seem to provide this incorrect information using confident and compelling language. The combination of a broad body of knowledge, along with the provision\nof confident but incorrect information, may cause significant harm, as people may accept the LLM as a knowledgeable source, and fall for its confident and compelling language, even when providing false information.\nWe believe that in order to perform well, an LLM must have some internal notion as to whether a sentence is true or false, as this information is required for generating (or predicting) following tokens. For example, consider an LLM generating the following false information \u201cThe sun orbits the Earth.\" After stating this incorrect fact, the LLM is more likely to attempt to correct itself by saying that this is a misconception from the past. But after stating a true fact, for example \u201cThe Earth orbits the sun,\" it is more likely to focus on other planets that orbit the sun. Therefore, we hypothesize that the truth or falsehood of a statement should be represented by, and therefore extractable from, the LLM\u2019s internal state.\nInterestingly, retrospectively \u201cunderstanding\u201d that a statement that an LLM has just generated is false does not entail that the LLM will not generate it in the first place. We identify three reasons for such behavior. The first reason is that an LLM generates a token at a time, and it \u201ccommits\u201d to each\ntoken generated. Therefore, even if maximizing the likelihood of each token given the previous tokens, the overall likelihood of the complete statement may be low. For example, consider a statement about Pluto. The statement begins with the common words \"Pluto is the\", then, since Pluto used to be the smallest planet the word \"smallest\" may be a very plausible choice. Once the sentence is \"Pluto is the smallest\", completing it correctly is very challenging, and when prompted to complete the sentence, GPT-4 (March 23rd version) completes it incorrectly: \u201cPluto is the smallest dwarf planet in our solar system.\u201d In fact, Pluto is the second largest dwarf planet in our solar system (after Eris). One plausible completion of the sentence correctly is \u201cPluto is the smallest celestial body in the solar system that has ever been classified as a planet.\u201d (see Figure 1). Consider the following additional example: \u201cTiztoutine is a town in Africa located in the republic of Niger.\u201d Indeed, Tiztoutine is a town in Africa, and many countries\u2019 names in Africa begin with \u201cthe republic of\u201d. However, Tiztoutine is located in Morocco, which is not a republic, so once the LLM commits to \u201cthe republic of\u201d, it cannot complete the sentence using \u201cMorocco\", but completes it with \u201cNiger\". In addition, committing to a word at a time may lead the LLM to be required to complete a sentence that it simply does not know how to complete. For example, when describing a city, it may predict that the next words should describe the city\u2019s population. Therefore, it may include in a sentence \"Tiztoutine has a population of\", but the population size is not present in the dataset, so it must complete the sentence with a pure guess.\nThe second reason for an LLM to provide false information is that at times, there may be many ways to complete a sentence correctly, but fewer ways to complete it incorrectly. Therefore, it might be that a single incorrect completion may have a higher likelihood than any of the correct completions (when considered separately).\nFinally, since it is common for an LLM to not use the maximal probability for the next word, but to sample according to the distribution over the words, it may sample words that result in false information.\nIn this paper we present our Statement Accuracy Prediction, based on Language Model Activations (SAPLMA). SAPLMA is a simple yet powerful method for detecting whether a statement generated\nby an LLM is truthful or not. Namely, we build a classifier that receives as input the activation values of the hidden layers of an LLM. The classifier determines for each statement generated by the LLM if it is true or false. Importantly, the classifier is trained on out-of-distribution data, which allows us to focus specifically on whether the LLM has an internal representation of a statement being true or false, regardless of the statement\u2019s topic.\nIn order to train SAPLMA we created a dataset of true and false statements from 6 different topics. Each statement is fed to the LLM, and its hidden layers\u2019 values are recorded. The classifier is then trained to predict whether a statement is true or false only based on the hidden layer\u2019s values. Importantly, our classifier is not tested on the topics it is trained, but on a held-out topic. We believe that this is an important measure, as it requires SAPLMA to extract the LLM\u2019s internal belief, rather than learning how information must be aligned to be classified as true. We show that SAPLMA, which leverages the LLM\u2019s internal states, results in a better performance than prompting the LLM to explicitly state whether a statement is true or false. Specifically, SAPLMA reaches accuracy levels of between 60% to 80% on specific topics, while few-shot prompting achieves only slightly above random performance, with no more than a 56% accuracy level.\nOf course there will be some relationship between the truth/falsehood of a sentence, and the probability assigned to that sentence by a welltrained LLM. But the probability assigned by an LLM to a given statement depends heavily on the frequency of the tokens in the statement as well as its length. Therefore, sentence probabilities provide only a weak signal of the truth/falsehood of the sentence. At minimum, they must be normalized to become a useful signal of the veracity of a statement. As we discuss later, SAPLMA classifications of truth/falsehood significantly outperform simple sentence probability. In one test described later in more detail, we show that SAPLMA performs well on a set of LLM-generated sentences that contain 50% true, and 50% false statements. We further discuss the relationship between statement probability and veracity in the discussion section.\nSAPLMA employs a simple and relatively shallow feedforward neural network as its classifier, which requires very little computational power at inference time. Therefore, it can be computed\nalongside the LLM\u2019s output. We propose for SAPLMA to supplement an LLM presenting information to users. If SAPLMA detects that the LLM \u201cbelieves\u201d that a statement that it has just generated is false, the LLM can mark it as such. This could raise human trust in the LLM responses. Alternatively, the LLM may merely delete the incorrect statement and generate a new one instead.\nTo summarize, the contribution of this paper is twofold.\n\u2022 The release of a dataset of true-false statements along with a method for generating such information. \u2022 Demonstrating that an LLM might \u201cknow\u201d when a statement that it has just generated is false, and proposing SAPLMA, a method for extracting this information."
        },
        {
            "heading": "2 Related Work",
            "text": "In this section we provide an overview of previous research on LLM hallucination, accuracy, and methods for detecting false information, and we discuss datasets used to that end.\nMany works have focused on hallucination in machine translation (Dale et al., 2022; Ji et al., 2023). For example, Dale et al. (Dale et al., 2022) consider hallucinations as translations that are detached from the source, hence they propose a method that evaluates the percentage of the source contribution to a generated translation. If this contribution is low, they assume that the translation is detached from the source and is thus considered to be hallucinated. Their method improves detection accuracy hallucinations. The authors also propose to use multilingual embeddings, and compare the similarity between the embeddings of the source sentence and the target sentence. If this similarity is low, the target sentence is considered to be hallucinated. The authors show that the latter method works better. However, their approach is very different than ours, as we do not assume any pair of source and target sentences. In addition, while we also use the internal states of the model, we do so by using the hidden states to descriminate between statements that the LLM \u201cbelieves\u201d are true and those that are false. Furtheremore, we focus on detecting false statements rather than hallucination, as defined by their work.\nOther works have focused on hallucination in text summarization (Pagnoni et al., 2021). Pagnoni et al. propose a benchmark for factuality metrics of\ntext summarization. Their benchmark is developed by gathering summaries from several summarization methods and requested humans to annotate their errors. The authors analyze these anotation and the proportion of the different factual error of the summarization methods. We note that most works that consider hallucination do so with relation to a given input (e.g., a passage) that the model operates on. For example, a summarization model that outputs information that does not appear in the provided article, is considered hallucinate it, regardless if the information is factually true. However, in this work we consider a different problem\u2014-the veracity of the output of an LLM, without respect to a specific input.\nSome methods for reducing hallucination assume that the LLM is a black box (Peng et al., 2023). This approach uses different methods for prompting the LLM, possibly by posting multiple queries for achieving better performance. Some methods that can be used for detecting false statements may include repeated queries and measuring the discrepancy among them. We note that instead of asking the LLM to answer the same query multiple times, it is possible to request the LLM to rephrase the query (without changing its meaning or any possible answer) and then asking it to answer each rephrased question.\nOther methods finetune the LLM, using human feedback, reinforcement learning, or both (Bakker et al., 2022; Ouyang et al., 2022). Ouyang et al. propose a method to improve LLM-generated content using reinforcement learning from human feedback. Their approach focuses on fine-tuning the LLM with a reward model based on human judgments, aiming to encourage the generation of better content. However, fine tuning, in general, may cause a model to not perform as well on other tasks (Kirkpatrick et al., 2017). In this paper, we take an intermediate approach, that is, we assume access to the model parameters, but do not fine-tune or modify them. Another approach that can be applied to our settings is presented by (Burns et al., 2022), named Contrast-Consistent Search (CCS). However, CCS requires rephrasing a statement into a question, evaluating the LLM on two different version of the prompt, and requires training data from the same dataset (topic) as the test set. These limitations render it unsuitable for running in practice on statements generated by an LLM. In addition, CCS increases the accuracy by only approximately\n4% over the 0-shot LLM query, while our approach demonstrates a nearly 20% increase over the 0-shot LLM\nA dataset commonly used for training and finetuning LLMs is the Wizard-of-Wikipedia (Dinan et al., 2018). The Wizard-of-Wikipedia dataset includes interactions between a human apprentice and a human wizard. The human wizard receives relevant Wikipedia articles, which should be used to select a relevant sentence and compose the response. The goal is to replace the wizard with a learned agent (such as an LLM). Another highly relevant dataset is FEVER (Thorne et al., 2018, 2019). The FEVER dataset is designed for developing models that receive as input a claim and a passage, and must determine whether the passage supports the claim, refutes it, or does not provide enough information to support or refute it. While the FEVER dataset is highly relevant, it does not provide simple sentence that are clearly true or false independently of a provided passage. In addition, the FEVER dataset is not partitioned into different topics as the true-false dataset provided in this paper.\nIn conclusion, while several approaches have been proposed to address the problem of hallucination and inaccuracy in automatically generated content, our work is unique in its focus on utilizing the LLM\u2019s hidden layer activations to determine the veracity of generated statements. Our method offers the potential for more general applicability in real-world scenarios, operating alongside an LLM, without the need for fine-tuning or task-specific modifications."
        },
        {
            "heading": "3 The True-False Dataset",
            "text": "The work presented in this paper requires a dataset of true and false statements. These statements must have a clear true or false label, and must be based on information present in the LLM\u2019s training data. Furthermore, since our approach intends to reveal that the hidden states of an LLM have a notion of a statement being true or false, the dataset must cover several disjoint topics, such that a classifier can be trained on the LLM\u2019s activations of some topics while being tested on another. Unfortunately, we could not find any such dataset and therefore, compose the true-false dataset.\nOur true-false dataset covers the following topics: \u201cCities\", \u201cInventions\", \u201cChemical Elements\", \u201cAnimals\", \u201cCompanies\", and \u201cScientific Facts\".\nFor the first 5 topics, we used the following method to compose the dataset. We used a reliable source1 that included a table with several properties for each instance. For example, for the \u201cchemical elements\" we used a table that included, for each element, its name, atomic number, symbol, standard state, group block, and a unique property (e.g., Hydrogen, 1, H, Gas, Nonmetal, the most abundant element in the universe). For each element we composed true statement using the element name and one of its properties (e.g., \u201cThe atomic number of Hydrogen is 1\u201d). Then, we randomly selected a different row for composing a false statement (e.g., \u201cThe atomic number of Hydrogen is 34\u201d). If the value in the different row is identical to the value in the current row, we resample a different row until we obtain a value that is different. This process was repeated for the all topics except the \u201cScientific Facts\u201d. For the \u201cScientific Facts\u201d topic, we asked ChatGPT (Feb 13 version) to provide \u201cscientific facts that are well known to humans\u201d (e.g. \u201cThe sky is often cloudy when it\u2019s going to rain\u201d). We then asked ChatGPT to provide the opposite of each statement such that it becomes a false statement (e.g., \u201cThe sky is often clear when it\u2019s going to rain\u201d). The statements provided by ChatGPT were manually curated, and verified by two human annotators. The classification of 48 facts were questioned by at least one of the annotators; these facts were removed from the dataset. The truefalse dataset comprises 6,084 sentences, including 1,458 sentences for \u201cCities\", 876 for \u201cInventions\", 930 for \u201cChemical Elements\", 1,008 for \u201cAnimals\", 1,200 for \u201cCompanies\", and 612 for \u201cScientific Facts\". The following are some examples of true statements from the dataset:\n\u2022 Cities: \u201cOranjestad is a city in Aruba\u201d \u2022 Inventions: \u201cGrace Hopper invented the\nCOBOL programming language\u201d \u2022 Animals: \u201cThe llama has a diet of herbivore\u201d \u2022 Companies: \u201cMeta Platforms has headquar-\nters in United States\u201d \u2022 Scientific Facts: \u201cThe Earth\u2019s tides are pri-\nmarily caused by the gravitational pull of the moon\u201d\nThe following are some examples of false statements from the dataset:\n1Cities: Downloaded from simplemaps. Inventions: Obtained from Wikipedia list of inventors. Chemical Elements: Downloaded from pubchem ncbi nlm nih gov periodic-table. Animals: Obtained from kids national geographics. Companies: Forbes Global 2000 List 2022: The Top 200.\n\u2022 Chemical Elements: \u201cIndium is in the Lanthanide group\u201d \u2022 Animals: \u201cThe whale has a long, tubular snout, large ears, and a powerful digging ability to locate and consume termites and ants.\u201d \u2022 Scientific Facts: \u201cIce sinks in water due to its higher density\u201d\nOther candidates for topics to be added to the dataset include sports, celebrities, and movies. The true-false dataset is available at: azariaa.com/ Content/Datasets/true-false-dataset.zip."
        },
        {
            "heading": "4 SAPLMA",
            "text": "In this section, we present our Statement Accuracy Prediction, based on Language Model Activations (SAPLMA), a method designed to determine the truthfulness of statements generated by an LLM using the values in its hidden layers. Our general hypothesis is that the values in the hidden layers of an LLM contain information on whether the LLM \u201cbelieves\u201d that a statement is true or false. However, it is unclear which layer should be the best candidate for retaining such information. While the last hidden layer should contain such information, it is primarily focused on generating the next token. Conversely, layers closer to the input are likely focused on extracting lower-level information from the input. Therefore, we use several hidden layers as candidates. We use two different LLMs: Facebook OPT-6.7b (Zhang et al., 2022) and LLAMA27b (Roumeliotis et al., 2023); both composed of 32 layers. For each LLM, we compose five different models, each using activations from a different layer. Namely, we use the last hidden layer, the 28th layer (which is the 4th before last), the 24th layer (which is the 8th before last), the 20th layer (which is the 12th before last), and the middle layer (which is the 16th layer). We note that each layer is composed of 4096 hidden units.\nSAPLMA\u2019s classifier employs a feedforward neural network, featuring three hidden layers with decreasing numbers of hidden units (256, 128, 64), all utilizing ReLU activations. The final layer is a sigmoid output. We use the Adam optimizer. We do not fine-tune any of these hyper-parameters for this task. The classifier is trained for 5 epochs.\nFor each topic in the true-false dataset, we train the classifier using only the activation values obtained from all other topics and test its accuracy on the current topic. This way, the classifier is required to determine which sentences the LLM \u201cbelieves\u201d\nare true and which it \u201cbelieves\u201d are false, in a general setting, and not specifically with respect to the topic being tested. To obtain more reliable results, we train each classifier three times with different initial random weights. This process is repeated for each topic, and we report the accuracy mean over these three runs."
        },
        {
            "heading": "5 Results",
            "text": "We compare the performance of SAPLMA against three different baselines. The first is BERT, for which we train a classifier (with an identical architecture to the one used by SAPLMA) on the BERT embeddings of each sentence. Our second baseline is a few shot-learner using OPT-6.7b. This baseline is an attempt to reveal whether the LLM itself \u201cknows\u201d whether a statement is true or false. Unfortunately, any attempts to explicitly prompt the LLM in a \u2018zero-shot\u201d manner to determine whether a statement is true or false completely failed with accuracy levels not going beyond 52%. Therefore, we use a few shot-query instead, which provided the LLM with truth values from the same topic it is being tested on. Note that this is very different from our methodology, but was necessary for obtaining some results. Namely, we provided few statements along with the ground-truth label, and then the statement in question. We recorded the probability that the LLM assigned to the token \u201ctrue\u201d and to the token \u201cfalse\u201d. Unfortunately, the LLM had a tendency to assign higher values to the \u201ctrue\u201d token; therefore, we divided the probability assigned to the \u201ctrue\u201d token by the one assigned to the \u201cfalse\u201d token. Finally, we considered the LLM\u2019s prediction \u201ctrue\u201d if the value was greater than the average, and \u201cfalse\u201d otherwise. We tested a 3-shot and a 5-shot version. The third baseline, given a statement \u2018X\u2019, we measure the probabilities (using OPT-6.7) of the sentences \u201cIt is true that X\u201d, and \u201cIt is false that X\u201d, and pick the higher probability (considering only the probabilities for X, not the added words). This normalizes the length and frequency factors.\nTable 1 and Figure 2 present the accuracy of all the models tested using the OPT-6.7b LLM, for each of the topics, along with the average accuracy. As depicted by the table and figure, SAPLMA clearly outperforms BERT and Few-shot learning, with BERT, 3-shot, and 5-shot learning achieving only slightly above a random guess (0.50). It can also be observed that SAPLMA for OPT-6.7b\nseems to work best when using the 20th layer (out of 32). Recall that each model was trained three times. We note that the standard deviation between the accuracy among the different runs was very small; therefore, the differences in performance between the different layers seem consistent. However, the optimal layer to be used for SAPLMA is very likely to depend on the LLM.\nWe note that the average training accuracy for SAPLMA (using OPT-6.7b\u2019s 20th layer) is 86.4%. We believe that this relatively high value may indicate once again that the veracity of a statement is inherit to the LLM. To demonstrate this, we run a test where the true/false labels are randomly permuted. The average accuracy on the random training data is only 62.5%. This indicates that our model does not have the capacity to completely over-fit the training data, and thus, must exploit structure and patterns that appear in the data.\nAdditionally, Table 2 presents the performance of SAPLMA using LLAMA2-7b. As expected, SAPLMA using LLAMA2-7b achieves much better performance. Interestingly, for LLAMA2-7b, the middle layer performs best.\nAs for the differences between the topics, we believe that these values depend very much on the training data of the LLM. That is, we believe that the data used for training OPT-6.7b and LLAMA27b includes information or stories about many cities and companies, and not as much on chemical elements and animals (other than the very common ones). Therefore, we conjecture that is the reason SAPLMA achieves high accuracy for the \u201ccities\u201d topic (while trained on all the rest) and the \u201ccompanies\u201d topic, but achieves much lower accuracy when tested on the \u201canimals\u201d and \u201celements\u201d topics.\nIn addition to the topics from the true-false dataset, we also created a second data set of statements generated by the LLM itself (the OPT-6.7b model). For generating statements, we provided a true statement not present in the dataset, and allowed the LLM to generate a following statement. We first filtered out any statements that were not factual statements (e.g., \u201cI\u2019m not familiar with the Japanese versions of the games.\u201d). All statements were generated using the most probable next word at each step, i.e., we did not use sampling. This resulted in 245 labeled statements. The statements were fact-checked and manually labeled by three human judges based on web-searches. The human judges had a very high average observed agreement of 97.82%, and an average Cohen\u2019s Kappa of 0.9566. The majority determined the ground-truth label for each statement. 48.6% of the statements were labeled as true, resulting in a balanced dataset.\nEach of the models was trained 14 times using the same classifier described in Section 4. The models were trained on the entire true-false dataset (i.e., all topics, but not the generated sentences) and tested on the generated sentences.\nTable 3 presents the average accuracy of all models on the sentences generated by the LLM. As anticipated, SAPLMA (using OPT-6.7b) clearly outperforms the baselines, which appear to be entirely ineffectual in this task, achieving an accuracy\nnear 50%. However, the accuracy of SAPLMA on these sentences is not as promising as the accuracy achieved when tested on some of the topics in the true-false dataset (i.e., the cities and companies). Since we expected the LLM to generate sentences that are more aligned with the data it was trained on, we did expect SAPLMA\u2019s performance on the generated sentences to be closer to its performance on topics such as cities and companies, which are likely aligned with the data the LLM was trained on. However, there may be also a counter-effect in play: the sentences in the true-false dataset were mostly generated using a specific pattern (except the scientific facts topic), such that each sentence is clearly either true or false. However, the sentences generated by the LLM where much more open, and their truth value may be less clearly defined (despite being agreed upon by the human judges). For example, one of the sentences classified by all human judges as false is \u201cLima gets an average of 1 hour of sunshine per day.\u201d However, this sentence is true during the winter. Another example is \u201cBrink is a river,\u201d which was also classified as false by all three human judges; however, brink refers to river bank (but is not a name of a specific river, and does not mean river). Indeed, SAPLMA classified approximately 70% of the sentences as true, and the AUC values seem more promising. This may hint that any sentence that seems plausible is classified as true. Therefore, we evaluate the models using 30% of the generated sentences for determining which threshold to use, i.e., any prediction above the threshold is considered true. Importantly, we do not use this validation set for any other goal. We test the models on the remaining 70% of the generated sentences. We do not evaluate the few shot models again, as our evaluation guaranteed that the\nnumber of positive predictions would match the number of negative predictions, which matches the distribution in of the data.\nTable 4 presents the accuracy of all models when using the optimal threshold from the validation set. Clearly, SAPLMA performs better with a higher threshold. This somewhat confirms our assumption that the truth value of the sentences generated by the LLM is more subjective than those that appear in the true-false dataset. We note that also the BERT embeddings perform better with a higher threshold. The use of a higher threshold can also be justified by the notion that it is better to delete or to mark as unsure a sentence that is actually true, than to promote false information.\nAnother interesting observation is that the 20thlayer no longer performs best for the statements generated by the LLM, but the 28th layer seems to perform best. This is somewhat perplexing and we do not have a good guess as to why this might be happening. Nevertheless, we stress that the differences between the accuracy levels of the 28th-layer and the others are statistically significant (using a two-tailed student t-test; p < 0.05). In future work we will consider fusing multiple layers together, and using the intermediate activation values outputted by the LLM for all the words appearing in the statement (rather than using only the LLM\u2019s output for the final word).\nWe also ran the statements generated by the OPT 6.7b model on GPT-4 (March 23rd version), prompting it to determine whether each statement was true or false. Specifically, we provided the following prompt \u201cCopy the following statements and for each statement write true if it is true and\nfalse if it is false:\u201d, and fed it 30 statements at a time. It achieved an accuracy level of 84.4%, and a Cohen\u2019s Kappa agreement level of 0.6883 with the true label."
        },
        {
            "heading": "6 Discussion",
            "text": "In this work we explicitly do not consider models that were trained or fine-tuned on data from the same topic of the test-set. This is particularly important for the sentences generated by the LLM, as training on a held-out set from them would allow the classifier to learn which type of sentences generated by the LLM are generally true, and which are false. While this information may be useful in practice, and its usage is likely to yield much higher accuracy, it deviates from this paper\u2019s focus.\nWe note that the probability of the entire sentence (computed by multiplying the conditional probabilities of each word, given the previous words) cannot be directly translated to a truth value for the sentence, as many words are more common than others. Therefore, while sentence probabilities may be useful to determine which of two similar sentences is true, they cannot be used alone for the general purpose of determining the truthfulness of a given sentence.\nIn Table 5 we compare the probability assigned by the LLM and the sigmoid output from SAPLMA on 14 statements, which do not appear in the truefalse dataset. We use the 28-layer, as it proved to perform best on the statements generated by the LLM, but we note that other layers provide very similar results on this set of statements. As depicted by the table, the probabilities provided by the LLM are highly susceptible to the syntax, i.e., the exact words and the statement\u2019s length. The first two sets of examples in the table illustrate how sentence length highly affects the probabilities, but not SPLMA. In the following examples the false statements are not necessarily shorter than the true statements, yet the probabilities remain highly unreliable, while SPLMA generally succeeds in making accurate predictions.\nThe statement \u201cThe Earth is flat\u201d as well as \u201cThe Earth is flat like a pancake\u201d probably appear several times in the LLM\u2019s training data, therefore, it has a relatively high probability; however, SAPLMA is not baffled by it and is almost certain that both sentences are false. A basketeer is a rare word meaning a basketball player. It seems that the LLM is not familiar with the phrase and assigns it a low proba-\nbility. However, while SAPLMA still classifies the statement \u201cKevin Durant is basketeer\u201d as false, it\u2019s still much more confident that Kevin Durant is not a baseball player, in contrast to the probabilities. Since the statement \u201cKevin Duarnt is basketball player\u201d has a typo, its probability is extremely low, but SAPLMA still classifies the statement as true.\n\u201cJennifer Aniston is a female person\u201d is an implicit truth, a fact that is universally acknowledged without needing to be explicitly stated; thus, it is unlikely to be mentioned in the LLM\u2019s training data. Therefore, its probability is very low\u2014much lower than \u201cJennifer Aniston is not an actress\u201d\u2014despite having the same number of words. Nevertheless, SAPLMA classifies it correctly, albeit not with very high confidence.\nWhile we show that the probabilities cannot be used alone to determine the veracity of a statement, they are not useless and do convey important information. Therefore, in future work we will consider providing SAPLMA with the probabilities of the generated words; however, this information may be redundant, especially if SAPLMA uses the intermediate activation values for all the words appearing in the statement."
        },
        {
            "heading": "7 Conclusions & Future Work",
            "text": "In this paper, we tackle a fundamental problem associated with LLMs, i.e., the generation of incorrect and false information. We have introduced SAPLMA, a method that leverages the hidden layer activations of an LLM to predict the truthfulness\nof generated statements. We demonstrated that SAPLMA outperforms few-shot prompting in detecting whether a statement is true or false, achieving accuracy levels between 60% to 80% on specific topics when using OPT-6.7b, and between 70% to 90% when using LLAMA2-7b. This is a significant improvement over the maximum 56% accuracy level achieved by few-shot prompting (for OPT-6.7b).\nOur findings suggest that LLMs possess an internal representation of statement accuracy, which can be harnessed by SAPLMA to filter out incorrect information before it reaches the user, and furthermore that this representation of accuracy is very different from the probability assigned to the sentence by the LLM. Using SAPLMA as a supplement to LLMs may increase human trust in the generated responses and mitigate the risks associated with the dissemination of false information. Furthermore, we have released the true-false dataset and proposed a method for generating such data. This dataset, along with our methodology, provides valuable resources for future research in improving LLMs\u2019 abilities to generate accurate and reliable information.\nIn future work we intend to apply our method to larger LLMs, and run experiments with humans, such that a control group will interact with an unfiltered LLM, and the experimental group will interact with a system that augments SAPLMA and an LLM. We hope to demonstrate that humans trust and better understand the limitations of a system that is able to review itself and mark statements that it is unsure about. We also intend to study how the activations develop over time as additional words are generated, and consider multilingual input."
        },
        {
            "heading": "8 Limitations",
            "text": "This paper focuses on detecting whether a statement is true or false. However, in practice, it may be more beneficial to detect if the LLM is positive that a statement is correct or if it is unsure. The most simple adjustment to the proposed method in this paper is to lift the required threshold for classifying a statement as true above 0.5; however, the exact value would require some form of calibration of the model (Bella et al., 2010). Another option is to use multiple classifiers and to require all (or a vast majority of) classifiers to output \u201ctrue\u201d, for the statement to be considered true. Alternatively, dropout layers can be used for the same goal (Chen\nand Yi, 2021). The overall system can benefit from multiple outcomes, such that if the LLM is not positive whether the statement is true or false, it can be marked for the user to treat with caution. However, if a statement is classified as false, the LLM can delete it and generate a different statement instead. To avoid regenerating the same statement again, the probability of sampling the words that appear in the current statement should be adjusted downward.\nOur work was only tested in English. However, we believe that a multilingual LLM can be trained on one language and applied on statements in another language. We will test this hypothesis in future work.\nIn our work, we collected the activation values when each sentence was generated separately. However, in practice, in an LLM generating longer responses the activation values develop over time, so they may process both correct and incorrect information. Therefore, the activation values would need to be decoupled so that they can be tested whether the most recent statement was true or false. One approach might be to subtract the value of the activations obtained after the previous statement from the current activation values (a discrete derivative). Clearly, training must be performed using the same approach."
        },
        {
            "heading": "9 Ethical Impact",
            "text": "One of the primary ethical concerns of LLMs is the generation of false information; yet, we believe that SAPLMA could potentially reduce this issue. On the other hand, it is important to acknowledge that certain ethical issues, such as bias, may persist, potentially being transferred from the LLM to SAPLMA. Specifically, if the LLM exhibits bias towards certain ethnic groups, SAPLMA may likewise classify statements as true or false based on these inherited biases from the original LLM. Nevertheless, it may be possible to adapt the approach presented in this paper to bias mitigation."
        },
        {
            "heading": "10 Acknowledgments",
            "text": "This work was supported, in part, by the Ministry of Science and Technology, Israel, and by the Israel Innovation Authority."
        }
    ],
    "title": "The Internal State of an LLM Knows When It\u2019s Lying",
    "year": 2023
}