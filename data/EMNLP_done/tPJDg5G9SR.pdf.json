{
    "abstractText": "Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic humangenerated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Boyi Deng"
        },
        {
            "affiliations": [],
            "name": "Wenjie Wang"
        },
        {
            "affiliations": [],
            "name": "Fuli Feng"
        },
        {
            "affiliations": [],
            "name": "Yang Deng"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        },
        {
            "affiliations": [],
            "name": "Xiangnan He"
        }
    ],
    "id": "SP:7906276b8a0a443c8b299fbea84e17c5d8276687",
    "references": [
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan",
                "Nicholas Joseph",
                "Saurav Kadavath",
                "Jackson Kernion"
            ],
            "title": "Training a helpful and harmless assistant",
            "year": 2022
        },
        {
            "authors": [
                "Yuntao Bai",
                "Saurav Kadavath",
                "Sandipan Kundu",
                "Amanda Askell",
                "Jackson Kernion",
                "Andy Jones",
                "Anna Chen",
                "Anna Goldie",
                "Azalia Mirhoseini",
                "Cameron McKinnon",
                "Carol Chen",
                "Catherine Olsson",
                "Christopher Olah"
            ],
            "title": "Constitutional ai: Harmless",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger"
            ],
            "title": "Language models are",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "2020b. Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann",
                "Parker Schuh",
                "Kensen Shi"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord"
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "year": 2018
        },
        {
            "authors": [
                "Lavina Daryanani"
            ],
            "title": "How to jailbreak chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "proceedings of Sinn und Bedeutung, volume 23, pages 107\u2013124.",
            "year": 2019
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Vishvak Murahari",
                "Tanmay Rajpurohit",
                "Ashwin Kalyan",
                "Karthik Narasimhan"
            ],
            "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "year": 2023
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui"
            ],
            "title": "A survey on in-context learning",
            "year": 2023
        },
        {
            "authors": [
                "Deep Ganguli",
                "Liane Lovitt",
                "Jackson Kernion",
                "Amanda Askell",
                "Yuntao Bai",
                "Saurav Kadavath",
                "Ben Mann",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Kamal Ndousse",
                "Andy Jones",
                "Sam Bowman",
                "Anna Chen",
                "Tom Conerly"
            ],
            "title": "Red teaming language models",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli."
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
            "venue": "arXiv preprint arXiv:2303.15056.",
            "year": 2023
        },
        {
            "authors": [
                "Kai Greshake",
                "Sahar Abdelnabi",
                "Shailesh Mishra",
                "Christoph Endres",
                "Thorsten Holz",
                "Mario Fritz"
            ],
            "title": "Not what you\u2019ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "year": 2023
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Kang",
                "Xuechen Li",
                "Ion Stoica",
                "Carlos Guestrin",
                "Matei Zaharia",
                "Tatsunori Hashimoto"
            ],
            "title": "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "year": 2023
        },
        {
            "authors": [
                "Guokun Lai",
                "Qizhe Xie",
                "Hanxiao Liu",
                "Yiming Yang",
                "Eduard Hovy."
            ],
            "title": "RACE: Large-scale ReAding comprehension dataset from examinations",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Haoran Li",
                "Dadi Guo",
                "Wei Fan",
                "Mingshi Xu",
                "Jie Huang",
                "Fanpu Meng",
                "Yangqiu Song"
            ],
            "title": "Multi-step jailbreaking privacy attacks on chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aaron Mulgrew"
            ],
            "title": "I built a zero day virus with undetectable exfiltration using only chatgpt prompts",
            "year": 2023
        },
        {
            "authors": [
                "Helen Ngo",
                "Cooper Raterink",
                "Jo\u00e3o G.M. Ara\u00fajo",
                "Ivan Zhang",
                "Carol Chen",
                "Adrien Morisot",
                "Nicholas Frosst"
            ],
            "title": "Mitigating harm in language models with conditional-likelihood filtration",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray",
                "John Schulman",
                "Jacob Hilton",
                "Fraser Kelton",
                "Luke Miller"
            ],
            "title": "Training language models to follow",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Saffron Huang",
                "Francis Song",
                "Trevor Cai",
                "Roman Ring",
                "John Aslanides",
                "Amelia Glaese",
                "Nat McAleese",
                "Geoffrey Irving."
            ],
            "title": "Red teaming language models with language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "F\u00e1bio Perez",
                "Ian Ribeiro"
            ],
            "title": "Ignore previous prompt: Attack techniques for language models",
            "year": 2022
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S Gordon."
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90\u201395.",
            "year": 2011
        },
        {
            "authors": [
                "Irene Solaiman",
                "Christy Dennison."
            ],
            "title": "Process for adapting language models to society (palms) with values-targeted datasets",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 5861\u2013 5873. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano."
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Eric J. Wang."
            ],
            "title": "Alpaca-lora",
            "venue": "https://github. com/tloen/alpaca-lora.",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Canwen Xu",
                "Zexue He",
                "Zhankui He",
                "Julian McAuley."
            ],
            "title": "Leashing the inner demons: Selfdetoxification for language models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11530\u201311537.",
            "year": 2022
        },
        {
            "authors": [
                "Jing Xu",
                "Da Ju",
                "Margaret Li",
                "Y-Lan Boureau",
                "Jason Weston",
                "Emily Dinan"
            ],
            "title": "Recipes for safety in open-domain chatbots",
            "year": 2021
        },
        {
            "authors": [
                "Zhexin Zhang",
                "Jiale Cheng",
                "Hao Sun",
                "Jiawen Deng",
                "Fei Mi",
                "Yasheng Wang",
                "Lifeng Shang",
                "Minlie Huang."
            ],
            "title": "Constructing highly inductive contexts for dialogue safety through controllable reverse generation",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models have shown impressive natural language understanding and generation capabilities (Brown et al., 2020a; Chowdhery et al., 2022; Touvron et al., 2023), posing profound influence on the whole community. However, LLMs face the threat of red teaming attacks that can induce LLMs to generate harmful content, such as fraudulent or racist material, causing negative social impacts and endangering users. For instance, recent studies have shown that ChatGPT can be induced to generate racist responses (Kang et al., 2023) and even computer viruses (Mulgrew, 2023). These harmful effects underscore the urgent need\n\u2217Corresponding author.\nfor a thorough investigation of red teaming attacks and the development of effective defense strategies.\nResearch on red teaming typically involves manual or automatic construction of attack prompts. Manual methods recruit human annotators to construct high-quality prompts by following heuristic rules or interacting with LLMs. For instance, Kang et al. (2023) employed specific rules while Ganguli et al. (2022) engaged crowdworkers in back-andforth conversations with LLMs. However, manual construction is time-consuming and costly. Some studies thus employ language models to automatically generate attack prompts (Perez et al., 2022; Zhang et al., 2022), enabling the efficient generation of extensive prompts. However, these automatically generated prompts often have lower quality.\nGiven the pros and cons of manual and automatic construction, we propose an integrated method to complement each other for generating extensive high-quality attack prompts. With the impressive capabilities of newly emerged LLMs (e.g., ChatGPT1), it is possible to teach LLMs to mimic human annotators (Gilardi et al., 2023) with limited manual construction. In-context learning (Brown et al., 2020b) can be used to instruct LLMs to generate more high-quality prompts using a few manually constructed attack prompts. Moreover, stronger attackers can evoke better defense, and high-quality attack prompts can improve the safety of existing LLMs against red teaming attacks.\nTo this end, we propose a red teaming attack framework and a defense framework: Attack. The attack framework collects manually constructed high-quality prompts as an initial prompt set and generate more prompts through incontext learning with LLMs. Thereafter, the highquality prompts are further added into the prompt set for the next-round in-context learning. Through this iterative process, we can efficiently generate a large volume of high-quality attack prompts within\n1https://openai.com/blog/chatgpt/.\na short time. Based on this red teaming attack framework, we construct a series of datasets with rich Semi-automatic Attack Prompts (SAP) in eight distinct sensitive topics, facilitating the safety evaluation and defense of LLMs in future work. Defense. The defense framework enhances the safety of target LLMs by iterative interactions with the attack framework. Initially, the attack framework generate a set of attack prompts. We fine-tune the target LLMs over these attack prompts to generate safe outputs, such as \u201cI\u2019m sorry, I cannot generate inappropriate or harmful content\u201d. By examining the outputs of target LLMs, we select the prompts that can still attack target LLMs after finetuning, and use them as examples for the attack framework to generate more similar prompts. The newly generated prompts are employed to fine-tune the target LLMs in the next round. This iterative process continues until the target LLMs demonstrate adequate defense capabilities.\nWe conduct extensive experiments to validate the effectiveness of the two frameworks. To evaluate the attack performance, we test the generated prompts on various LLMs such as GPT-3.5 (Ouyang et al., 2022) and Alpaca (Taori et al., 2023). Remarkably, the prompts generated by the attack framework consistently achieve promising attack performance, even surpassing that of manually constructed cases (Kang et al., 2023). Besides, we apply the defense framework to fine-tune Alpaca-LoRA (Wang, 2023), demonstrating its efficacy in enhancing the safety of LLMs.\nOur contributions are summarized as follows:\n1. We propose a red teaming attack framework, which combines manual and automatic methods, and instructs LLMs through in-context learning to efficiently generate extensive highquality attack prompts.\n2. We present a defense framework to enhance the safety of target LLMs by iterative finetuning with the attack framework.\n3. We conduct extensive experiments on different LLMs, validating the effectiveness of the two frameworks. Besides, we release a series of attack prompts datasets with varying sizes to facilitate future research."
        },
        {
            "heading": "2 Related Work",
            "text": "\u2022 Large Language Models. LLMs have demonstrated remarkable capabilities across various do-\nmains. Some researches (Brown et al., 2020a; Chowdhery et al., 2022; Touvron et al., 2023) showcased LLMs\u2019 ability of content creation, including essays, poetry, and code. As model and corpus sizes continue to increase, LLMs also exhibit their in-context learning ability, enabling them to learn from a few examples within a given context (Dong et al., 2023). Ouyang et al. (2022) introduced InstructGPT, a upgraded version of GPT3, where models were trained to follow natural language instructions to complete specific tasks. While LLMs exhibit immense capabilities in diverse domains like content generation and instruction-following, it is essential to recognize the potential for misuse, which can result in malicious outcomes.\n\u2022 Red Teaming LLMs with Prompts. Existing research on red teaming usually designs attack prompts by two approaches: manual construction and automatic construction. Manual methods recruit human annotators to construct high-quality prompts by following heuristic rules (Kang et al., 2023) or interacting with LLMs (Ganguli et al., 2022). Furthermore, recent studies (Daryanani, 2023; Li et al., 2023; Deshpande et al., 2023) showed that moral restrictions of ChatGPT can be bypassed by providing role-playing instructions. Perez and Ribeiro (2022) devised prompts to achieve the objectives of goal hijacking and prompt leaking. In order to attack LLM-integrated applications, Greshake et al. (2023) strategically placed malicious prompts in accessible locations where applications could retrieve them. Attackers were able to gain control over LLM-integrated applications once processing these malicious prompts. Despite the effectiveness of manual construction in producing high-quality prompts, it is time-consuming and costly due to the need for annotation. To address this, some studies (Zhang et al., 2022; Perez et al., 2022) employed language models (LMs) to automatically generate attack prompts. However, these automatically generated prompts often suffer from lower quality. In this work, we propose a hybrid approach that combines manual and automatic construction methods to reduce the cost of producing satisfactory attack prompts.\n\u2022 Denfending LLMs. The goal of defending LLMs is to mitigate the harmful outputs of these models. Ngo et al. (2021) filtered the pretraining dataset of LLMs aiming to solve this problem from the source. Another line of work fine-tuned language models on non-toxic corpora (Gehman et al.,\n2020) or a value-targeted dataset (Solaiman and Dennison, 2021) to mitigate the toxic or harmful content. Different from previous works, Xu et al. (2022) trained a toxic model with toxic prompts, and use it to minimize the chance of toxic tokens. Recently, Reinforcement Learning from Human Feedback (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a,b; OpenAI, 2023) has drawn lots of attention, which can align the LLM-generated content with safety considerations from human feedback."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we present the tasks of red teaming attack and defense, followed a detailed presentation of our proposed attack and defense frameworks."
        },
        {
            "heading": "3.1 Task formulation",
            "text": "\u2022 Red Teaming Attack. Given a target LLM Lt, the objective of the red teaming attack is to discover natural language prompts x that can induce Lt to output some harmful content y. In this work, harmful content refers to content encompassing malicious intentions and viewpoints related to eight sensitive topics, covering fraud, politics, pornography_sexual_minors2, race, religion, suicide, terrorism, and violence. We choose these eight topics based on the sensitive topics of existing research (Zhang et al., 2022) and OpenAI Moderation API3.\nAttacker\u2019s Knowledge. The attacker can collect some initial attack prompts that can successfully attack the target LLM. And the target LLM is a \u201cblack-box\u201d to the attacker. Only the outputs of the input prompts are accessible.\nAttacker\u2019s Ability. The attacker can generate harmful attack prompts, and interact with the target\n2For brevity, it will be referred to as \u201cpornography\u201d in the rest of the article.\n3https://platform.openai.com/docs/guides/ moderation/overview.\nLLM Lt by injecting attack prompts and accessing the outputs.\n\u2022 Red Teaming Defense. Given a target LLM Lt, the objective of red teaming defense is to enhance the capabilities of Lt to resist the attack of a set of red teaming prompts x. Specifically, the outputs y generated by Lt in response to x should be harmless or Lt should refuse to answer the harmful prompts. An example of \u201crefuse to answer\u201d response is illustrated in Figure 2.\nDefender\u2019s Knowledge. The defender can access some attack prompts and fine-tune the target LLM (i.e., a \u201cwhite-box\u201d setting to the defender).\nDefender\u2019s Ability. The defense framework can fine-tune the target LLM for improving safety and interact with the target LLM by injecting attack prompts and accessing the outputs."
        },
        {
            "heading": "3.2 Red Teaming Attack Framework",
            "text": "As shown in Figure 1, the framework is a semiautomatic approach, which first collects some manually constructed high-quality prompts, and then leverages LLMs to mimic these attack prompts to generate more prompts x due to the remarkable capabilities of LLMs on text understanding and generation (Brown et al., 2020a; Chowdhery et al., 2022; Touvron et al., 2023). The detailed process is as follows:\n1. Initialize a prompt set with manually constructed high-quality attack prompts.\n2. Generate new prompts using an attack LLM through in-context learning.\n3. Evaluate the quality of generated prompts.\n4. Add the generated high-quality prompts into the attack prompt set.\n5. Repeat steps 2-4 until a sufficient number of attack prompts are obtained.\n\u2022 Manually Constructed Prompts. The manually constructed prompts are mainly collected from previous research and public websites. Kang et al. (2023) design many attack mechanisms to construct attack prompts, where we discover that code injection and payload splitting mechanisms are effective. Additionally, lots of researchers are trying to jailbreak ChatGPT to obtain harmful responses4. As such, we select some effective prompts from Kang et al. (2023) and public websites (see an example in Figure 3). \u2022 Prompt Generation via In-Context Learning. Given a set of high-quality prompts, we utilize gpt3.5-turbo-0301 as the attack LLM to generate more prompts due to its strong in-context learning capabilities (Brown et al., 2020a; Ouyang et al., 2022). However, due to the ethical constraints inherent in gpt3.5-turbo-0301, it will refuse to directly generate the prompts related to sensitive topics such as fraud. To address this challenge, we adopt a role-playing approach to involve gpt3.5-turbo-0301 as a content reviewer, asking it to generate prompts for assessing other LLMs. As illustrated in Figure 4, we assign the role of a content reviewer to gpt3.5-turbo-0301, and incorporate three prompts randomly sampled from the given set of high-quality prompts. Thereafter, we ask the attack LLM to generate a new prompt by mimicking the three examples (See Appendix F for the analysis of LLM\u2019s sensitivity to the order and selection of examples).\nInspired by the idea of Chain-of-Thought (Wei et al., 2023), we believe that explaining the rationale behind each example to the attack LLM could enhance its understanding of the examples and help\n4https://www.jailbreakchat.com/.\nto generate higher-quality prompts. To this end, we manually compose explanations for the prompts in the initial set of attack prompts, which are then incorporated into the in-context learning examples. Furthermore, we request the attack LLM to provide the rationale for its generated prompts, in order to enhance the prompt quality and ensure every selected prompt in the prompt set has its explanation.\n\u2022 Evaluation of Generated Prompts. As shown in Figure 1, we assess the quality of attack prompts by inputting them into the target LLM and evaluating the harmfulness of its outputs through an evaluation LLM, which is instantiated by gpt3.5-turbo-0301. Specifically, we design an evaluation prompt that allows gpt3.5-turbo-0301 to assign a harmfulness score ranging from 0 to 10 for each output (shown in Figure 5). We define the output with a harmfulness score exceeding 5 as the harmful output and assume its corresponding attack prompt is of high quality. We demonstrate the superiority of our evaluation methodology using gpt3.5-turbo-0301 in\nAppendix D, which outperforms Perspective API5, a widely adopted toxicity detection tool. From Appendix D, we observe a satisfactory balance between recall and precision scores by setting 5 as the threshold of harmfulness.\n\u2022 SAP: Semi-Automatic Attack Prompt Dataset. Based on this semi-automatic attack framework, we construct a series of datasets named SAP with numbers of attack prompts ranging from 40 to 1,600. In particular, we release SAP5, SAP10, SAP20, SAP30, and SAP200 for research purposes, where the number times eight (e.g., 30 \u00d7 8 for SAP30) indicates the size of the prompt set."
        },
        {
            "heading": "3.3 Red Teaming Defense Framework",
            "text": "As shown in Figure 6, we propose a red teaming defense framework to enhance the defense capabilities of a target LLM Lt. Specifically, we employ instruction tuning (Wei et al., 2022) to fine-tune Lt for generating safe responses to the harmful attack prompts. We leverage the attack framework in Section 3.2 to interactively fine-tune Lt. In detail, the defense framework operates as follows:\n1. Construct a set of original attack prompts using the red teaming attack framework.\n2. Evaluate the defense capabilities of the target LLM against the original attack prompts, and retain the prompts that can successfully attack the target LLM.\n3. Use attack prompts retained in Step 2 as incontext learning examples for the attack framework to expand the prompts.\n4. Fine-tune the target LLM to generate safe outputs with attack prompts generated in Step 3.\n5. Repeat steps 2-4 until the target LLM shows sufficient defense capabilities against the original attack prompts.\n5https://perspectiveapi.com/.\n\u2022 Interactive Fine-tuning with the Attack Framework. We can perceive the target LLM\u2019s defense capabilities against different attack prompts during the fine-tuning process. Once the target LLM has developed a strong defense against certain prompts, further fine-tuning on these prompts is not necessary. Worse still, it could potentially result in overfitting issues, as discussed in Appendix A. Therefore, after each round of fine-tuning, we reassess the target LLM\u2019s defense capability against the original attack prompts and expand the harder prompts for fine-tuning by the attack framework. This can enhance the target LLM to better defend against these hard prompts and avoid overfitting issues due to the diversity of newly generated prompts by the attack framework.\n\u2022 Fine-tuning the Target LLM. We construct instruction inputs and the desired outputs to fine-tune the target LLM. Specifically, we use attack prompts as instruction inputs, and take typical \u201crefuse to answer\u201d responses as desired outputs (cf. Figure 2)."
        },
        {
            "heading": "4 Experiment",
            "text": "We conduct extensive experiments to answer the following research questions: RQ1: Can our attack framework effectively red team LLMs (see Section 4.2)? RQ2: Can our defense framework effectively enhance the safety of LLMs (see Section 4.3)? RQ3: Will our defense framework compromise other capabilities of LLMs (see Section 4.4)?"
        },
        {
            "heading": "4.1 Experiment Setting",
            "text": ""
        },
        {
            "heading": "4.1.1 LLMs",
            "text": "\u2022 GPT-3.5. We employ two representative models from the GPT-3.5 series: gpt3.5-turbo-0301 and text-davinci-003.\n\u2022 Alpaca. The Alpaca model (Taori et al., 2023)\nis a fine-tuned version of the LLaMA model (Touvron et al., 2023), which is fine-tuned on an instruction dataset generated by the Self-Instruct method (Wang et al., 2023). Specifically, considering time and resource efficiency, we adopt Alpaca-LoRA-7B and Alpaca-LoRA-13B in our experiments by employing LoRA (Hu et al., 2021) for fine-tuning."
        },
        {
            "heading": "4.1.2 Datasets",
            "text": "\u2022 Dual-Use. Kang et al. (2023) manually construct an attack prompt dataset, which contain 51 attack prompts. These prompts encompass various attack mechanisms, including obfuscation, code injection/payload splitting, and virtualization, as presented in Appendix B.\n\u2022 BAD+. BAD+ dataset (Zhang et al., 2022), generated on top of the BAD dataset (Xu et al., 2021), contains more than 120K diverse and highly inductive contexts. The inductive contexts are divided into 12 categories (e.g., insult and threat), as demostrated in Appendix C. Considering that testing all 120k contexts would be too time-consuming, we randomly sample a sub-dataset containing 200 contexts for the experiments.\n\u2022 SAP. Among the five versions of SAP, we select SAP20 and SAP30 to assess attack performance for the consideration of evaluation cost. In particular, SAP30 is used to evaluate attack experiments and SAP20 is adopted for fine-tuning experiments. As to the \u201coriginal attack prompts\u201d for fine-tuning, we use SAP5, SAP10, and SAP30, separately."
        },
        {
            "heading": "4.1.3 Benchmarks",
            "text": "In order to study the impact of the proposed framework on other capabilities of LLMs, we further compare the performance of LLMs on multiple benchmarks before and after fine-tuning with red teaming defense framework. We consider five benchmarks: BoolQ (Clark et al., 2019), ARCEasy (Clark et al., 2018), RACE (Lai et al., 2017), CB (De Marneffe et al., 2019) and COPA (Roemmele et al., 2011)."
        },
        {
            "heading": "4.2 Attack Results (RQ1)",
            "text": "\u2022 Overall Performance. The results are depicted in Table 1. It is evident that SAP30 outperforms both Dual-Use and BAD+ by obtaining the highest harmful scores across all four LLMs. Notably, the performance of SAP30 surpasses that of automatically generated attack prompts and exhibits\nsignificant improvement over manually generated attack prompts. This outcome substantiates the superiority of our semantic-auto framework in terms of attack prompt quality.\n\u2022 GPT-3.5 vs. Alpaca-LoRA. The results from the Table 1 indicate that the attack effectiveness on the Alpaca-LoRA series models is superior to that on the GPT-3.5 series models. This disparity can be attributed to the fact that the GPT-3.5 series models employ RLHF during training (Ouyang et al., 2022), which provides some defense against attack prompts. In contrast, the Alpaca series models lack specific fine-tuning for safety, resulting in insufficient defense capabilities.\nIt is important to note that the attack effectiveness of the BAD+ dataset is significantly poorer on the GPT-3.5 series models compared to the Alpaca-LoRA series models. This is primarily due to the simplistic nature of the prompts in the BAD+ dataset, as depicted in Figure 7b.\nFurthermore, the attack effectiveness of the DualUse dataset on the GPT-3.5 series models is only slightly lower than that on the Alpaca-LoRA series models. This indicates that well-crafted attack prompts can effectively bypass simple defenses. Similarly, we observe similar attack effectiveness differences between the two model series when evaluating our constructed SPA30 dataset. This further demonstrates that our red teaming attack\nframework is capable of capturing the characteristics of manually constructed attack prompts.\n\u2022 Case Study. We conduct a comparison between a sampled prompt in SAP30 and that in BAD+6. The sampled prompts are presented in Figure 7. The sampled prompt in BAD+ suffers from two main drawbacks: straightforwardness and brevity. The straightforwardness of the prompt makes it susceptible to be detected as a harmful input, while its concise nature limits the elaboration of the intended harmful targets. In contrast, the sampled prompt employed in SAP30 addresses these shortcomings by being sufficiently lengthy, allowing for the inclusion of harmful intentions within a seemingly harmless prompt. Furthermore, this prompt deceptively disguises itself as a legitimate business proposal, thereby increasing the challenge for the language model to identify any harmful intention embedded within the request."
        },
        {
            "heading": "4.3 Defense Results (RQ2)",
            "text": "We conduct fine-tuning experiments on AlpacaLoRA using different parameter sizes and training data sizes, as outlined in Table 2. Specifically, we fine-tune Alpaca-LoRA-7B and Alpaca-LoRA-13B\n6Prompts from Dual-Use are not included in our analysis, due to the unclear license of the original data (Kang et al., 2023) that is not publicly available.\nmodels with SAP5, SAP10, and SAP30 datasets. Remarkably, across all settings, we observe significant reductions in harmful scores. These findings provide strong evidence for the robustness of the red teaming defense framework. Figures 8a and 8b demonstrate a decreasing trend in the harmful scores during fine-tuning. These outcomes indicate that our framework enhances the diverse defense capability of Alpaca-LoRA, extending its defense capabilities beyond the confines of the training data. However, we also identify instances of overfitting in some cases, which is discussed in Appendix A.\n\u2022 Defense Performance Varies on Different Datasets. From Table 2, it is evident that the defense effectiveness of LLMs significantly improves after fine-tuning with the defense framework. In the SAP20 test dataset, there is a notable enhancement in defense, with some LLMs even yielding a harmful score as low as 0.01. Although the defense effectiveness improves in the Dual-Use and BAD+ test datasets, it is not as pronounced as in the SAP20 test dataset. This can be attributed to\nthe fact that the SAP20 test dataset shares the same distribution as the training dataset, leading to better defense performance. On the other hand, the Dual-Use and BAD+ test datasets exhibit greater distributional differences from the training dataset, resulting in a comparatively lesser defense performance. Nevertheless, there is still a discernible improvement in defense performance on these two test datasets, indicating the efficacy of our defense framework.\n\u2022 Case Study. We sample reponses of SAP5 and BAD+ before and after fine-tuning to verify the effectiveness of our framework, as shown in Figure 9 and 10. From Figure 9, it can be observed that after fine-tuning, the responses generated by the LLM have transformed from promoting harmful discourse on beautifying suicide to a \u201crefuse to answer\u201d response. This indicates the effectiveness of our defense framework on the SAP20 dataset, which shares the same distribution as the training data. Furthermore, Figure 10 shows that the responses of the LLM have shifted from assisting in theft-related behavior to a \u201crefuse to answer\u201d response. It is worth noting that our training dataset did not include any prompts specifically related\nto theft. The fact that LLMs have learned to classify theft as \u201charmful content\u201d illustrates the effectiveness of our defense framework in dealing with BAD+, which differs from the distribution of our training data. These findings further emphasize the diverse defensive capabilities enhanced by our defense framework."
        },
        {
            "heading": "4.4 Performance on Other Tasks (RQ3)",
            "text": "To explore whether fine-tuning in the defense framework affects the regular capabilities of LLMs, we present their performance on multiple NLP benchmarks before and after fine-tuning in Table 3. The results show that the proposed fine-tuning framework does not affect the capabilities of LLMs in handling regular NLP tasks. On the contrary, it can even improve the performance in some of the benchmarks. These findings show that our defense framework has little impact on the original capabilities of LLMs, but can efficiently enhance the defense capability of LLMs."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we proposed two frameworks to attack and defend LLMs. The attack framework combines manual and automatic prompt construction, enabling the generation of more harmful attack prompts compared to previous studies (Kang et al., 2023; Zhang et al., 2022). The defense framework fine-tunes the target LLMs by multi-turn interactions with the attack framework. Empirical experiments demonstrate the efficiency and robustness of the defense framework while posing minimal impact on the original capabilities of LLMs. Additionally, we constructed five SAP datasets of attack prompts with varying sizes for safety evaluation and enhancement. In the future, we will construct SAP datasets with more attack prompts and evaluate attack performance on bigger datasets. Besides, we will evaluate more LLMs.\nLimitations\nAlthough our defense framework has been demonstrated to effectively enhance the safety of LLMs, we acknowledge that due to recourse limitation and open source issues of some LLMs, our defense experiments have primarily focused on the Alpaca series models, leaving more exploration on a broader range of diverse LLMs in the future. Additionally, we utilize gpt-3.5-turbo-0301 as the evaluator. However, it cannot accurately evaluate some outlier responses (see an example in Figure 12 of Appendix). In the future, it might be better to incorporate more powerful LLMs for the evaluation of harmfulness.\nEthics Statement\nThe LLMs under the attack framework might output harmful and offensive responses. It is important to emphasize that the opinions expressed in these outputs are automatically generated through LLMs and do not reflect the viewpoints of the authors. Additionally, it is worth noting that the dataset we created includes prompts that may potentially facilitate harmful activities. Consequently, we strongly advise researchers to handle this dataset with utmost caution. As mentioned in Section 1, stronger attackers can evoke better defense. Our intention in providing this dataset is for researchers to construct safer LLMs by using our defense framework."
        },
        {
            "heading": "Acknowledgment",
            "text": "This work is supported by the National Key Research and Development Program of China (2022YFB3104701), the National Natural Science Foundation of China (62272437) and the CCCD Key Lab of Ministry of Culture and Tourism."
        },
        {
            "heading": "A Overfitting in Fine-Tuning",
            "text": "During the process of fine-tuning, it is possible to encounter overfitting phenomena. Specifically, as the number of iterations increases, unexpected text may appear following the \u201crefuse to answer\u201d response, such as the red texts as depicted in Figure 12. This occurrence is particularly common when fine-tuning with an immutable dataset, i.e., fixed prompts without the prompt extension by the attack framework. The presence of such unexpected text can lead to higher harmful scores assigned by the evaluation LLM, resulting in an elevated harmful score in the test dataset (see the fourth-turn fine-tuning results in Figure 11c).\nTo address this phenomenon, we adopt a strategy of regenerating new fine-tuning prompts by the attack framework at each iteration, as discussed in Section 3.3. In order to assess the effectiveness of our solution, we compare it with a variant of our defense framework named defense-WID, which stands for \u201cdefense with an immutable dataset\u201d,\ni.e., only fine-tuning with the fixed \u201coriginal attack prompts\u201d in Figure 6. By comparing the harmful scores on three test datasets between our defense framework and defense-WID in Figure 11a, 11b, and 11c, we observe that our defense framework reduces the harmful score to a certain extent on the three test datasets. Besides, it significantly alleviates the overfitting issue on BAD+."
        },
        {
            "heading": "B Attack Mechanisms in Dual-Use",
            "text": "The examples we used here are derived from the original paper by Kang et al. (2023), which are not as harmful as the Dual-Use dataset, as shown in\nFigure 13.\nB.1 Obfuscation\nThe simplest form of obfuscation is to add typos or use synonyms for terms that trigger input or output filters. For example, instead of using the term \u201cCOVID-19\u201d an attacker could use the term \u201cCVID\u201d.\nB.2 Code injection/Payload splitting\nThe main idea of code injection/payload splitting is to break the malicious payload into multiple smaller payloads, each of which does not trigger defense mechanisms.\nB.3 Virtualization\nVirtualization involves creating a simulated environment or character, wherein certain rules are established that may contradict the original ethical restrictions placed on Language Models with Limited Moral Status (LLMs). By doing so, we can circumvent these ethical limitations and prompt LLMs to comply with harmful instructions."
        },
        {
            "heading": "C Contexts Categories of BAD+",
            "text": "BAD+ contains the following 12 contexts categories: identity_attack, insult, profanity, threat, sexually_explicit, flirtation, drugs, politics, religion, medical, nsfw and none."
        },
        {
            "heading": "D GPT-3.5 vs. Perspective Api",
            "text": "Perspective API is a widely-used free API that identifies toxic comments. However, in our experimental setup, the evaluation method in Section 3.2 (referred to as \u201cGPT-3.5\u201d) outperforms the\nPerspective API. Specifically, we use the framework in Section 3.2 to generate 100 attack prompts and their responses without automatic evaluation. We then manually select 50 harmful responses as negative samples. Additionally, we generate 50 \u201crefuse to answer\u201d responses using the gpt-3.5-turbo-0301, which are considered as positive samples. We employ both the Perspective API and GPT-3.5 to assign harmful scores to all the samples. The receiver operating characteristic (ROC) curve, displayed in Figure 14, reveals that the area under the curve (AUC) of GPT-3.5 is larger than that of the Perspective API, indicating\nthat GPT-3.5 is a superior choice within our framework. By selecting a threshold of 5, the recall and precision of GPT-3.5 are 0.94 and 1.00."
        },
        {
            "heading": "E Reproducibility",
            "text": "E.1 Computing Infrastructure\nWe fine-tune Alpaca-LoRA-7B on a single NVIDIA GeForce RTX 3090 with 24GB memory, and Alpaca-LoRA-13B are fine-tuned on a singel NVIDIA A40 with 48GB memory.\nE.2 Fine-tuning Hyperparameters\nIn the defense framework, we incorporate the code obtained from https://github.com/ tloen/alpaca-lora to perform fine-tuning of\nAlpaca-LoRA. To ensure consistency across all fine-tuning experiments, we maintain a set of hyperparameters as displayed in Table 5. All remaining hyperparameters are retained at their default values.\nE.3 Benchmarks Evaluation\nWe employ the code obtained from the GitHub repository https://github.com/EleutherAI/ lm-evaluation-harness to assess the performance of LLMs on various benchmarks.\nE.4 Time and Cost Analysis\nThe generation of the SAP200 dataset takes approximately 35 hours, and the cost of OpenAI API calls amounts to around 10 USD."
        },
        {
            "heading": "F Prompt Sensitivity",
            "text": "To investigate prompt sensitivity, we randomly selected 10 sets of distinct in-context learning examples from the SAP200 dataset, each set comprising three individual examples. Within each set, there are six possible ways to arrange the examples. By\nutilizing these six distinct permutations across the 10 sets, we generated attack prompts for our experimentation. The resulting detrimental scores are presented in Table 4. The table reveals that the variance is not substantial, indicating that the results are relatively robust and not significantly influenced by variations in prompts, both in terms of prompt order and selection.\nWe believe that a possible reason for obtaining this result is that the examples for in-context learning are selected through our iterative approach, ensuring that the quality of all prompts is relatively high. Consequently, the results are not very sensitive to prompts, regardless of the order and selection of the examples."
        }
    ],
    "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
    "year": 2023
}