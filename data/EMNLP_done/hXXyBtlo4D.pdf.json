{
    "abstractText": "Mixture-of-Experts (MoE) based sparse architectures can significantly increase model capacity with sublinear computational overhead, which are hence widely used in massively multilingual neural machine translation (MNMT). However, they are prone to overfitting on lowresource language translation. In this paper, we propose a modularized MNMT framework that is able to flexibly assemble dense and MoEbased sparse modules to achieve the best of both worlds. The training strategy of the modularized MNMT framework consists of three stages: (1) Pre-training basic MNMT models with different training objectives or model structures, (2) Initializing modules of the framework with pre-trained couterparts (e.g., encoder, decoder and embedding layers) from the basic models and (3) Fine-tuning the modularized MNMT framework to fit modules from different models together. We pre-train three basic MNMT models from scratch: a dense model, an MoE-based sparse model and a new MoE model, termed as MoE-LGR that explores multiple Language-Group-specifc Routers to incorporate language group knowledge into MNMT. The strengths of these pre-trained models are either on low-resource language translation, highresource language translation or zero-shot translation. Our modularized MNMT framework attempts to incorporate these advantages into a single model with reasonable initialization and fine-tuning. Experiments on widely-used benchmark datasets demonstrate that the proposed modularized MNMT framwork substantially outperforms both MoE and dense models on highand low-resource language translation as well as zero-shot translation. Our framework facilitates the combination of different methods with their own strengths and recycling off-the-shelf models for multilingual neural machine translation. Codes are available at https://github.com/lishangjie1/MMNMT. \u2217Corresponding authors.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shangjie Li"
        },
        {
            "affiliations": [],
            "name": "Xiangpeng Wei"
        },
        {
            "affiliations": [],
            "name": "Shaolin Zhu"
        },
        {
            "affiliations": [],
            "name": "Jun Xie"
        },
        {
            "affiliations": [],
            "name": "Baosong Yang"
        },
        {
            "affiliations": [],
            "name": "Deyi Xiong"
        }
    ],
    "id": "SP:e6c51032f4ca55d81da0113b325784bde62bd4f7",
    "references": [
        {
            "authors": [
                "Roee Aharoni",
                "Melvin Johnson",
                "Orhan Firat."
            ],
            "title": "Massively multilingual neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "mand Joulin"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer."
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "Journal of Machine Learning Research, 23(120):1\u201339.",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Aran Komatsuzaki",
                "Joan Puigcerver",
                "James Lee-Thorp",
                "Carlos Riquelme Ruiz",
                "Basil Mustafa",
                "Joshua Ainslie",
                "Yi Tay",
                "Mostafa Dehghani",
                "Neil Houlsby."
            ],
            "title": "Sparse upcycling: Training mixture-of-experts from dense checkpoints",
            "venue": "The Eleventh International",
            "year": 2023
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Van Der Maaten Laurens",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research, 9(2605):2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen"
            ],
            "title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
            "year": 2020
        },
        {
            "authors": [
                "M Paul Lewis",
                "Gary F Simons",
                "Charles D Fennig."
            ],
            "title": "Ethnologue: languages of the world, dallas, texas: Sil international",
            "venue": "Online version: http://www. ethnologue. com, 12(12):2010.",
            "year": 2009
        },
        {
            "authors": [
                "Mike Lewis",
                "Shruti Bhosale",
                "Tim Dettmers",
                "Naman Goyal",
                "Luke Zettlemoyer."
            ],
            "title": "Base layers: Simplifying training of large, sparse models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Zehui Lin",
                "Xiao Pan",
                "Mingxuan Wang",
                "Xipeng Qiu",
                "Jiangtao Feng",
                "Hao Zhou",
                "Lei Li."
            ],
            "title": "Pretraining multilingual neural machine translation by leveraging alignment information",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "Xiaonan Nie",
                "Shijie Cao",
                "Xupeng Miao",
                "Lingxiao Ma",
                "Jilong Xue",
                "Youshan Miao",
                "Zichao Yang",
                "Zhi Yang",
                "Bin CUI"
            ],
            "title": "Dense-to-sparse gate for mixtureof-experts",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Pan",
                "Mingxuan Wang",
                "Liwei Wu",
                "Lei Li."
            ],
            "title": "Contrastive learning for many-to-many multilingual neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Jerin Philip",
                "Alexandre Berard",
                "Matthias Gall\u00e9",
                "Laurent Besacier."
            ],
            "title": "Monolingual adapters for zero-shot neural machine translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Roller",
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason E Weston."
            ],
            "title": "Hash layers for large sparse models",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Xu Tan",
                "Jiale Chen",
                "Di He",
                "Yingce Xia",
                "Tao Qin",
                "Tie-Yan Liu."
            ],
            "title": "Multilingual neural machine translation with language clustering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling humancentered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Biao Zhang",
                "Ankur Bapna",
                "Rico Sennrich",
                "Orhan Firat"
            ],
            "title": "Share or not? learning to schedule language-specific capacity for multilingual translation",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Yanqi Zhou",
                "Tao Lei",
                "Hanxiao Liu",
                "Nan Du",
                "Yanping Huang",
                "Vincent Y Zhao",
                "Andrew M. Dai",
                "Zhifeng Chen",
                "Quoc V Le",
                "James Laudon."
            ],
            "title": "Mixtureof-experts with expert choice routing",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Yaoming Zhu",
                "Jiangtao Feng",
                "Chengqi Zhao",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Counter-interference adapter for multilingual machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2812\u20132823, Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Simiao Zuo",
                "Xiaodong Liu",
                "Jian Jiao",
                "Young Jin Kim",
                "Hany Hassan",
                "Ruofei Zhang",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "Taming sparsely activated transformer with stochastic experts",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Mixture-of-Experts (MoE) based sparse architectures can significantly increase model capacity with sublinear computational overhead, which are hence widely used in massively multilingual neural machine translation (MNMT). However, they are prone to overfitting on lowresource language translation. In this paper, we propose a modularized MNMT framework that is able to flexibly assemble dense and MoEbased sparse modules to achieve the best of both worlds. The training strategy of the modularized MNMT framework consists of three stages: (1) Pre-training basic MNMT models with different training objectives or model structures, (2) Initializing modules of the framework with pre-trained couterparts (e.g., encoder, decoder and embedding layers) from the basic models and (3) Fine-tuning the modularized MNMT framework to fit modules from different models together. We pre-train three basic MNMT models from scratch: a dense model, an MoE-based sparse model and a new MoE model, termed as MoE-LGR that explores multiple Language-Group-specifc Routers to incorporate language group knowledge into MNMT. The strengths of these pre-trained models are either on low-resource language translation, highresource language translation or zero-shot translation. Our modularized MNMT framework attempts to incorporate these advantages into a single model with reasonable initialization and fine-tuning. Experiments on widely-used benchmark datasets demonstrate that the proposed modularized MNMT framwork substantially outperforms both MoE and dense models on high- and low-resource language translation as well as zero-shot translation. Our framework facilitates the combination of different methods with their own strengths and recycling off-the-shelf models for multilingual neural machine translation. Codes are available at https://github.com/lishangjie1/MMNMT.\n\u2217Corresponding authors."
        },
        {
            "heading": "1 Introduction",
            "text": "Multilingual neural machine translation translates multiple languages within a single model via multitask learning, facilitating the deployment of machine translation service in practice and improving low/zero-resource language translation (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2021; Team et al., 2022). However, as the number of languages translated by an MNMT model increase, the capacity of the model has to be increased accordingly, otherwise translation quality will degrade for all language pairs, especially for high-resource languages. This phenomenon is referred to as capacity bottleneck (Aharoni et al., 2019; Zhang et al., 2020). MoE models provide an effective way to increase model capacity while the computation cost is sublinear to the number of parameters. Due to this advantage of MoE models over dense models, sparse architectures built on MoE models are widely explored for massively multilingual neural machine translation that requires large model capacity. However, MoE models suffer from overfitting on low-resource languages (Team et al., 2022), which is not observed in MNMT built on dense models.\nThis inspires us to ask a question: can we achieve the best of both worlds of dense and MoEbased sparse architectures for multilingual NMT? To answer this question, we propose MMNMT that Modularizes Multilingual NMT with flexibly assembled dense and MoE blocks. Specifically, the training strategy of MMNMT consists of three stages:\n\u2022 Pre-training basic multilingual NMT models with different architectures, attempting to explore the strengths of different models on low-resource language translation (e.g., dense models), high-resource language translation (e.g., MoE models).\n\u2022 Initializing the modules of the proposed mod-\nularized MNMT model with blocks from the pre-trained basic multilingual NMT models, e.g., using the pre-trained dense encoder to initialize the encoder of MMNMT. Such initialization can be done in a flexible module assembling way.\n\u2022 Fine-tuning the MMNMT model to make the assembled modules fit together.\nUsing dense modules to initialize MoE models in our training strategy is in line with our preliminary experiments and recent studies on MoE-based language models. These studies find that training MoE-based language models from an off-the-shelf dense model (e.g., T5 (Raffel et al., 2020)) is more efficient than training from scratch (Nie et al., 2022; Komatsuzaki et al., 2023). Our preliminary experiments also demonstrate that dense MNMT models are superior to MoE-based sparse models on lowresource language translation.\nIn order to diversify the options of pre-trained basic models and improve zero-shot translation in MMNMT, we further propose MoE-LGR that incorporates a Language Group Router into MoE models. Routing mechanism plays an important role in token dispatch and resource allocation for MoE models. Previous works in this line mainly focus on load balancing across experts in MoE (Lewis et al., 2021; Roller et al., 2021; Fedus et al., 2022; Zuo et al., 2022; Zhou et al., 2022) to prevent experts from being not specialized or overly specialized. However, the language information of each token is not fully explored in routers, which could enhance the generation ability of the decoder and mitigate the off-target problem in zero-shot translation. To address this issue, we introduce MoE-LGR. Particularly, we categorize languages into multiple groups according to linguistic typology and language embedding clustering, and learn a router per language group to enhance the difference of routing for different language groups.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose MMNMT to modularize multilingual NMT, which is capable of assembling modules from both dense and sparse models and fitting them together to achieve the best of both worlds.\n\u2022 We present MoE-LGR with language group routers, which is able to significantly improve zero-shot translation in multilingual NMT.\n\u2022 Experiments on the OPUS-100 and PC32 dataset demonstrate that the proposed MMNMT achieves significant improvements over both MoE and dense models on all language directions, especially on low-resource and zero-shot translation."
        },
        {
            "heading": "2 Related Work",
            "text": "Multilingual Neural Machine Translation Multilingual neural machine translation has been gaining increasing interest in recent years (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2021; Team et al., 2022). However, MNMT models tend to be inferior to bilingual NMT counterparts on translating high-resource languages due to the capacity bottleneck (Zhang et al., 2020). A wide variety of approaches have been proposed to alleviate this issue, e.g., deepening models to increase model capacity (Zhang et al., 2020), exploring lightweight language-specific modules (Philip et al., 2020; Zhang et al., 2021; Zhu et al., 2021), word alignments (Lin et al., 2020), contrastive learning (Pan et al., 2021) and Mixture-ofExperts (Lepikhin et al., 2020; Fedus et al., 2022; Nie et al., 2022). Mixture-of-Experts based MNMT models usually suffer from overfitting problem (Team et al., 2022). Alleviating the overfitting problem is a key focus of our proposed modularized MNMT framework.\nMixture-of-Experts Sparsely-gated Mixture-ofExperts (MoE) (Lepikhin et al., 2020; Fedus et al., 2022) selects the top-K experts in MoE layers through a routing mechanism. A strand of research focuses on load balancing, such as Base Layer (Lewis et al., 2021), Hash Layer (Roller et al., 2021), THOR (Zuo et al., 2022), expert choice routing (Zhou et al., 2022). In this work, we propose a language group routing mechanism to provide language information and enhance the difference of routing across language groups, which significantly improves the performance of zero-shot translation. Recently, Komatsuzaki et al. (2023) use a pre-trained T5 dense model to initialize an MoE language model, achieving improvements in the computational cost but no obvious performance gains. Our work explores modularized initialization of different parts of MoE models with various pre-trained basic models for multilingual NMT, which is flexible and effective."
        },
        {
            "heading": "3 Preliminary Experiments and Findings",
            "text": "To have a deep understanding on the strengths and weaknesses of dense and MoE models in multilingual NMT, we conducted preliminary experiments to compare them. Details of these experiments can be found in section 5.1. Results are shown in Table 1. We observe that the MoE model significantly outperforms the dense model on high-resource language translation, and achieves consistent improvements over the dense model on all En-to-Any directions. This demonstrates the advantage of MoE sparse models over dense models in terms of model capacity. However, we also find that MoE models are prone to overfitting on low-resource language translation, which resonates with the finding of (Team et al., 2022). In Any-to-English translation, the dense model outperforms the MoE model by 1.3 BLEU (33.5 vs. 32.3) on low-resource language tanslation. In addition to this, we plot the validation loss of the dense model and MoE model during training in Figure 1. It is obvious that the validation losses of both Any-to-English and English-to-Any low-resource language translation initially decrease and later increase, confirming the existence of the overfitting problem."
        },
        {
            "heading": "4 Methodology",
            "text": "To mitigate the overfitting issue and achieve the best of both worlds, we propose a general framework MMNMT to modularize multilingual NMT so as to assemble desirable modules from both\nMoE and dense models. The training strategy of MMNMT consists of basic model pre-training, module initializing and fine-tuning."
        },
        {
            "heading": "4.1 Basic Model Pre-training",
            "text": "We introduce three types of basic models for our general framework, namely dense model, MoE model and the proposed MoE-LGR model. These basic models are trained from scratch with crossentropy objectives on training data.\nDense Model The dense model is a encoderdecoder backbone network contains 12 Transformer encoder blocks and 12 Transformer decoder blocks. The encoder and decoder have a shared embedding layer.\nMoE Model The MoE model substitutes the feed-forward network (FFN) sublayer in the Dense model with an MoE layer that consists of multiple FFN experts {FFNi}Ni=1 to expand model capacity. The MoE model uses a token router p (typically a Top-K gate) to perform token dispatch to experts.\nMoE-LGR To explore language information and improve zero-shot translation capability in the decoder, we add multiple language-group-specific routers (one router per language group) into MoE layers of the decoder to enhance the difference of routing across language groups.\nThe inputs to the language group router are a token representation x and a language ID lid. A language group will be automatically identified\naccording the language ID via the language group identification operation LGI. The shared router and the identified language-group-specific router is aggregated to yield the output of the language group router, which is computed as follows:\np(x) = TopK(softmax( Wsharex+WLGI(lid)x\n2 ))\n(1) where Wshare is the weight of share router across all languages, WLGI(lid) is the weight of the language-group-specific router identified by LGI(lid).\nFor language groups, we use both external linguistic typology knowledge and internal language token embeddings to group languages. First, we categorize languages into multiple groups following external linguistic typology from Ethnologue (Lewis et al., 2009), which is one of the most authoritative language family taxonomy. Specifically, as we use the OPUS-100 (Zhang et al., 2020) dataset in our experiments, which is English-centric and contains 99 language pairs (as the test sets in OPUS-100 only cover 94 language pairs, we only use 94 language pairs in supervised training), we divide 95 languages (including English) into 22 groups according to Ethnologue. The majority of languages are from the Indo-European family. Some languages are assigned to a group with only one language of their own. For example, Thai is from the Tai-Kadai group and no other languages in the OPUS-100 dataset are assigned to this group.\nIn order to balance the corpus size across groups, we need to further restructure language groups according to language similarity. Language embedding based clustering (Tan et al., 2019) is a solution to automatically cluster languages into groups by using language token embeddings learned by MNMT. However, this method may generate lowquality language clusters due to low quality of lowresource language embeddings. Hence, we cluster languages on the basis of linguistic typology (i.e., based on 22 groups defined in Ethnologue). We calculate group embeddings as follows:\nEG = \u2211 i\u2208G Ti TG Ei (2)\nwhere EG is the group embedding, Ei is the language embedding of language i in group G, Ti is the corpus size of the language i, TG is the total corpus size of all languages in group G.\nIn this way, the impact of the low-quality language embeddings of low-resource languages is reduced. Then, we perform hierarchical clustering with computed group embeddings and set a maximum group corpus size Tmax to prevent excessive clustering. Specifically, let M the number of the initial groups after the linguistic typology knowledge based grouping. In each iteration, we merge the two closest groups a and b whose total corpus size is less than Tmax, removing the two old groups and forming a new group. After this, the number of groups decreases from M to M-1. The group embedding of the new group is obtained by weighting the old group embedding based on the corpus size:\nEnew = Ta \u2217Ea +Tb \u2217Eb\nTa +Tb (3)\nWhen grouping no longer changes, we obtain the final grouping result. In this way, we obtain 4 groups for OPUS-100 dataset at last. Please refer to Appendix A for more details."
        },
        {
            "heading": "4.2 Module Initialization",
            "text": "Once basic models are pre-trained, we can use parameters of these pre-trained models to initialize modules of the general MMNMT framework or the entire framework in different ways.\nInitialization From Dense Model When we use the pre-trained dense model to initialize MMNMT, we perform initialization in a layer-wise way. As shown in Figure 3, we initialize the parameters of MMNMT layers with those of the dense model layers that have the same structure as the corresponding MMNMT layers, e.g., self-attention layer, cross-attention layer, embedding layer. For FFN experts initialization, we randomly masks parameters of the corresponding FFN in the dense model and choose to initialize an FFN expert from MMNMT in the same layer with the remaining parameters. In doing so, we can enhance the diversity of experts (Nie et al., 2022).\nInitialization from MoE Model This can be done in a straightforward way as they have the same architecture. The initialization is performed for the entire model.\nInitialization from MoE-LGR Model As the MoE-LGR has multiple language-group-specific routers in the decoder, we need to add multiple language-group-specific routers in the decoder of MMNMT accordingly to accommodate the initialization.\nMixed Initialization To combine the strengths of these basic models, we can perform mixed initialization. For example, we can use the encoder of the\ndense model and the decoder of the MoE-LGR model to initialize the MMNMT model, which could improve the ability of low-resource language translation as well as zero-shot translation."
        },
        {
            "heading": "4.3 Fine-tuning",
            "text": "After initializing the MMNMT model, we need to further fine-tune the entire model on the same training data used in the basic model pre-training to ensure that the various modules in the model cooperate with each other, especially under the mixed initialization condition. We fine-tune the MMNMT model with cross-entropy and load-balance objectives."
        },
        {
            "heading": "5 Experiments",
            "text": "We used the MoE branch of fairseq1 to implement our MoE models. We conducted extensive experiments to examine the effectiveness of the proposed MMNMT against MoE and dense model baselines in multilingual machine translation."
        },
        {
            "heading": "5.1 Experiment Setting for Many-to-Many Translation",
            "text": "Dataset We used the publicly available OPUS100 dataset which contains approximately 55M sentence pairs and 99 language pairs in our experiments. Since the test sets in OPUS-100 only covers 94 language pairs, we only used the data of these 94 language pairs for training. We employed the temperature-based sampling strategy (Aharoni et al., 2019) with T = 5 to balance corpus size for various language pairs. We used SentencePiece\n1https://github.com/facebookresearch/fairseq/tree/moe\n(Kudo and Richardson, 2018) for tokenization and the vocabulary size was 64K.\nModel Configurations Our dense MNMT model contained 12 Transformer encoder blocks and 12 Transformer decoder blocks, where the model dimension was 1024, the number of attention heads was 8. For the MoE model, MoE layer frequency of the MoE MNMT model was 2, the capacity factor was 1.25 in training and the eval-capacityfactor was set to 0.75 in inference. Every MoE layer had 32 experts and Top-2 routing strategy was used for expert routing. Other configurations were the same as the dense model. For the MoE-LGR model, we set Tmax = 15M and used the language embeddings learned by the dense model. The 95 languages were finally clustered into 4 language groups.\nTraining Configurations We conducted all experiments on 8 NVIDIA Tesla V100 32GB GPUs. For basic model pre-training and MMNMT finetuning, we used the Adam optimizer (Kingma and Ba, 2014) with learning rate = 2e\u22124, \u03b1 = 0.9, \u03b2 = 0.98. The learning schedule was Polynomial decay with the number of warm-up steps being set to 4000, the end learning rate was 1e\u22125, and the total number of updates was 100K. We set the training max tokens to 4096 per GPU and accumulated the gradients every 4 steps. The training objects were cross-entropy loss and load-balance loss with a weight of 0.1. The best checkpoint was selected according to the perplexity (ppl) on the validation set. The BLEU score was computed via sacrebleu (Post, 2018), and we employed langdetect2 toolkit for language identification on the OPUS-100 dataset to calculate language accuracy of target translations."
        },
        {
            "heading": "5.2 Main Results",
            "text": "In order to examine the effectiveness of different models, we evaluated the translation performance on the OPUS-100 test set and OPUS-100 zero-shot test set. We grouped languages into three categories according to the size of training data available for them in the training dataset: High (500K\u2264 size \u2264 1M, 100 directions), Medium (200K < size < 500K, 24 directions) and Low (\u2264 200K, 64 directions). The zero-shot test set covers six languages (Arabic, Chinese, Dutch, French, German, and Russian) and 30 zero-shot directions.\n2https://github.com/Mimino666/langdetect\nResults are shown in Table 2. The Wall Time is the training cost of models. The dense model and MoE model require 28 and 79 hours to complete 100K training steps respectively. For a fair comparison, we also provide results for a dense model that continues to train in 79 hours with 260K steps. The wall time of MMNMT model is in x+y format, x denotes the pre-training time of the corresponding basic model and y is the time of finetuning. We also extend the training time of some MMNMT models to conduct fair comparisons in terms of training hours. It can be seen that the MoE-LGR model is on a par with the MoE baseline on Any-to-En and En-to-Any translation, but achieves substantial improvements of 1.9 BLEU and 10.2% LangAcc over the MoE model on zeroshot translation (see (4) and (5) in Table 2). This is in line with our motivation in developing the MoE-LGR model to improve zero-shot translation via language-group-specific routers. All MMNMT models with different assembled modules are superior to both dense and MoE models. The MMNMT model Enc(1) \u2212Decrand, where the encoder is initialized with the encoder of the dense model and the decoder is randomly initialized, achieves an average improvement of 0.9 BLEU over the MoE model (see (4) and (8) in Table 2). The initialization from the encoder of the dense model significantly improves low-resource language translation by 2.4 BLEU on Any-to-En translation over the MoE model and is on a par with it on En-to-Any translation. Enc(1) \u2212 Dec(1) which uses the pretrained dense model to initialize both encoder and decoder, outperforms the MoE baseline by an average of 0.5 BLEU on Any-to-English translation and 0.2 BLEU on English-to-Any translation. Especially, Enc(1) \u2212Dec(1) achieves 34.6 BLEU on English-to-Any low-resource language translation. The Enc(1) \u2212 Dec(3) and Enc(1) \u2212 Dec(5) model achieve significant improvements on high-resource language translation. Specifically, Enc(1) \u2212Dec(5) outperforms the MoE baseline by 0.4 and 0.6 BLEU on Any-to-English and English-to-Any highresource language translation. These results suggest that the proposed MMNMT initialized with blocks from basic models is able to benefit from the advantages of these basic models."
        },
        {
            "heading": "5.3 Zero-Shot Results",
            "text": "Zero-shot translation enabled by multilingual NMT usually suffers from the off-target translation issue\n(Zhang et al., 2020), where tokens in the target translation are not in the right language. We evaluated baselines and our methods on OPUS-100 zeroshot test set (Zhang et al., 2020). Results are shown in Table 2. The MoE model achieves 5.3 BLEU and 24.0% language accuracy, slightly mitigating the off-target problem compared with the dense model. The MoE-LGR model improves BLEU and language accuracy from 5.3 to 7.2 and 24.0% to 34.2% respectively. The language group router encourages similar token routing behavior within a language group. This makes the token-to-experts assignment in the decoder more language-specific, therefore improving the accuracy of language generation. The Enc(1) \u2212 Dec(5) model achieves the best result on zero-shot translation, improving the BLEU score from 5.3 to 9.6 and language accuracy from 24.0% to 44.9%. This model initializes its decoder with the decoder of MoE-LGR, which itself achieves good performance on zero-shot translation."
        },
        {
            "heading": "5.4 Analysis on the Encoder Representations and Validation Loss Curve",
            "text": "In order to take a deep look into the improvements obtained by the initialization from the dense model,\nwe visualize the representations of the top layer of the encoder in Figure 4. We used the Flores-200 (Team et al., 2022) dataset, which is a many-tomany multilingual benchmark including 204 languages. We selected 4 high-resource languages (German, French, Russian and Chinese) and 4 lowresource languages (Afrikaans, Amharic, Tamil and Telugu) in Flores-200, and encoded all sentences by the encoder of three different models: (a) the dense model (b) the MoE model trained from scratch and (c) the MMNMT (Dense) model (i.e., Enc(1) \u2212Decrand in Table 2). We averaged the sequential representations of the sentences from the above 8 languages over the sequence dimension, and applied the t-SNE (Laurens and Hinton, 2008) dimensionality reduction algorithm to reduce the 1024 dimensions to two dimensions. Then we ploted the bivariate kernel density estimation based on the reduced 2-dim representations. According to Figure 4, the encoder top layer representations of the dense model are more compact than those of the MoE model. Russian representations seem to overlap with those of German, French, and Afrikaans. The MMNMT (Dense) model whose encoder is initialized by the encoder of the dense model, shows obviously better unified representations than the other two models. All 8 languages are close to each other in the shared space and gradually merged into a cluster.\nWe also plot the validation loss curve of the MMNMT (Dense) model during training in Figure 5. We observe an overall decrease in the validation loss of the MMNMT model compared with that of the MoE model ((b) in Figure 1), especially on lowresource language translation. This explains the significant improvements gained by the MMNMT model on low-resource language translation."
        },
        {
            "heading": "5.5 Analysis on the Token-to-Experts Assignments in the Decoder",
            "text": "To analyse the improvements of the MMNMT (MoE-LGR) model (i.e., Enc(1) \u2212 Dec(5) in Table 2) gained on zero-shot translation, we report detailed results of zero-shot translation in Table 4. Compared with MoE baselines, the MMNMT (MoE-LGR) model improves BLEU scores by an average of 4.2 and language accuracy by an average of 20.3%. Arabic, Russian, and Chinese have the most noticeable improvements, which may be due to the fact that these languages are not categorized into the same group as English, making them more differentiated in routing and effectively alleviating the off-target issue. We visualize the token-to-experts assignments of zero-shot translation in the last MoE-layer of the decoder for MoE and MMNMT (MoE-LGR) respectively in Figure 6, where the x-axis represents 32 experts, and the yaxis denotes the target language (e.g., the first row in (a) is the average token-to-experts assignments of De-Ar, Fr-Ar, Nl-Ar, Ru-Ar, Zh-Ar). As zeroshot translation usually suffers from the off-target issue (i.e., easily translated into English), we used the same source sentences to translate them into English to observe changes in token-to-experts assignments. We observe that the MoE model has a similar token-to-experts assignments on zero-shot and X-to-English translation (see (a) and (b) in Figure 6), while MMNMT learns different assignments (see (c) and (d) in Figure 6). Our proposed language-group-specific routers route tokens from different language groups to different experts, mitigating the off-target issue of zero-shot translation."
        },
        {
            "heading": "5.6 Adapting Off-the-Shelf Dense Checkpoints to Sparse Architectures Via MMNMT",
            "text": "A wide variety of models have been proposed to improve dense multilingual machine translation in the past few years (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Lin et al., 2020; Pan et al., 2021). Our framework facilitates the adaptation of these off-the-shelf dense checkpoints to MoEbased sparse architectures, the current dominant formalism for massively multilingual NMT. We consider public MNMT model checkpoints with available multilingual parallel training data. For this, we choose mRASP2 (Pan et al., 2021) for our dense-to-sparse adaptation experiments with off-the-shelf models.\nExperiment Settings We used PC32 dataset (Lin et al., 2020) which is used by mRASP2. PC32 is a English-centric multilingual parallel corpus which includes 32 language pairs and 64 translation directions in total. The details of the dataset, model and training configuration can be found in Appendix B.\nResults Results are shown in Table 3. We compared with five baselines, and the numbers in bracket are the number of model parameters and activated parameters at inference time. The dense and MoE models were trained on the PC32 dataset from scratch, and their model configuration were the same as mRASP2. mRASP2 (Pan et al., 2021), M2M-100 (Fan et al., 2021) and NLLB (Team et al., 2022) are all publicly released MNMT dense checkpoints, which were downloaded and evaluated without any changes. We applied our framework to both dense and mRASP2 models to obtain MMNMT (Dense) and MMNMT (mRASP2) re-\nspectively, where both the encoder and decoder of the adapted models were initialized by the counterpart modules from the corresponding models. As we can see in Table 3, mRASP2 is a very strong baseline and is consistently better than both dense and MoE baselines. MMNMT (Dense) achieves a consistent improvements compared with MoE baselines on Any-to-English translation (28.3 vs. 27.5). MMNMT (mRASP2) can further significantly improve translation performance over mRASP2, especially on Any-to-English translation (29.5 vs. 27.7). On English-to-Any translation, MMNMT (mRASP2) gains an average improvement of 0.7 BLEU and a substantial improvement of 1.1 BLEU on extremely low-resource language translation. These results suggest that our modularized MNMT framework can be successfully applied to off-theshelf MNMT models to recycle and upgrade them. Our MMNMT model achieves comparable performance with NLLB-1.3B on low-resource language translation. NLLB-1.3B uses far more training data than ours (only 108K sentence pairs in total) on extremely low-resource languages. It is hence reasonable that our model cannot compete with it on these languages. Note that our framework can also use the modules of NLLB-1.3B to achieve further performance improvements."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we have presented MMNMT, a modularized multilingual neural machine translation framework that is capable of flexibly assembling both dense and sparse blocks to achieve the best of\nboth worlds for high/low-resource language translation and zero-shot translation. Experiments and in-depth analyses demonstrate that our framework combined with different modules significantly outperforms the MoE and dense baselines on highand low-resource language translation as well as zero-shot translation.\nLimitations\nThe MMNMT model is able to incorporate different modules from different basic models to combine their strengths. However, these modules might be not consistent with each other, making them not able to fit together in a single model. We leave this to our future work for exploring new strategies."
        },
        {
            "heading": "Acknowledgments",
            "text": "The present research was partially supported by the Key Research and Development Program of Yunnan Province (Grant No. 202203AA080004). We would like to thank the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Languages Grouping Result",
            "text": "We built language-group-specific routers for MoE following section 4.1, the grouping results are shown in Table 5."
        },
        {
            "heading": "B The Experiment Setting for mRASP2 Adapting Experiments",
            "text": "Dataset We downloaded the binary file of PC32 dataset without RAS and checkpoint from the project website3. We grouped these languages into four categories (excluding English-Mongolian language pair) according to their corpus size: High (10M \u2264 size, 4 directions), Medium (1M \u2264 size < 10M, 32 directions), Low (100K \u2264 size < 1M, 16 directions), Extremely Low (size < 100K, 10 directions). We omitted Maltese and Esperanto to make all multilingual translation models comparable in test. We report the validation and test set used in mRASP2 adapting experiments in Table 6.\nModel Configurations Our dense MNMT model contained 12 Transformer encoder blocks and 12 Transformer decoder blocks, where the model dimension was 1024, the number of attention heads was 16. The position embeddings of encoder and decoder were learned during training. For the MoE model, MoE layer frequency of the MoE MNMT model was 2, the capacity factor was 1.25 in training and the eval-capacity-factor was set to 0.75 in inference. Every MoE layer had 32 experts and Top-2 routing strategy was used for expert routing. Other configurations were the same as the dense model.\nTraining Configurations We conducted all experiments on 8 NVIDIA Tesla V100 32GB GPUs. For dense model pre-training and MMNMT finetuning, we used the Adam optimizer (Kingma and Ba, 2014) with learning rate = 2e\u22124, \u03b1 = 0.9, \u03b2 = 0.98. The learning schedule was inverse sqrt decay with the number of warm-up steps being set to 4000. The total number of updates was 100K. We set the training max tokens to 2000 per GPU and accumulated the gradients every 23 steps. The training objects were cross-entropy loss and load-balance loss with a weight of 0.1. The best checkpoint was selected according to the perplexity (ppl) on the validation set. The BLEU score was computed vias sacrebleu (Post, 2018).\n3https://github.com/PANXiao1994/mRASP2"
        }
    ],
    "title": "MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks",
    "year": 2023
}