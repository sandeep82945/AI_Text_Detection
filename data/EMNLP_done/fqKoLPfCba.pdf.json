{
    "abstractText": "Opinion summarization is expected to digest larger review sets and provide summaries from different perspectives. However, most existing solutions are deficient in epitomizing extensive reviews and offering opinion summaries from various angles due to the lack of designs for information selection. To this end, we propose SUBSUMM, a supervised summarization framework for large-scale multi-perspective opinion summarization. SUBSUMM consists of a review sampling strategy set and a two-stage training scheme. The sampling strategies take sentiment orientation and contrastive information value into consideration, with which the review subsets from different perspectives and quality levels can be selected. Subsequently, the summarizer is encouraged to learn from the sub-optimal and optimal subsets successively in order to capitalize on the massive input. Experimental results on AmaSum and Rotten Tomatoes datasets demonstrate that SUBSUMM is adept at generating pros, cons, and verdict summaries from hundreds of input reviews. Furthermore, our in-depth analysis verifies that the advanced selection of review subsets and the two-stage training scheme are vital to boosting the summarization performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Han Jiang"
        },
        {
            "affiliations": [],
            "name": "Rui Wang"
        },
        {
            "affiliations": [],
            "name": "Zhihua Wei"
        },
        {
            "affiliations": [],
            "name": "Yu Li"
        },
        {
            "affiliations": [],
            "name": "Xinpeng Wang"
        }
    ],
    "id": "SP:ba95d40a6e01bf5025766557ddc663756e186590",
    "references": [
        {
            "authors": [
                "Ojas Ahuja",
                "Jiacheng Xu",
                "Akshay Gupta",
                "Kevin Horecka",
                "Greg Durrett."
            ],
            "title": "ASPECTNEWS: Aspect-oriented summarization of news documents",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Aspect-controllable opinion summarization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6578\u20136593, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Unsupervised opinion summarization with content planning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12489\u201312497.",
            "year": 2021
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Mirella Lapata"
            ],
            "title": "Unsupervised opinion summarization with noising and",
            "year": 2020
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "year": 2020
        },
        {
            "authors": [
                "Adithya Bhaskar",
                "Alexander R. Fabbri",
                "Greg Durrett"
            ],
            "title": "Prompted opinion summarization with gpt-3.5",
            "year": 2023
        },
        {
            "authors": [
                "Arthur Bra\u017einskas",
                "Mirella Lapata",
                "Ivan Titov."
            ],
            "title": "Few-shot learning for opinion summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4119\u20134135, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Arthur Bra\u017einskas",
                "Mirella Lapata",
                "Ivan Titov."
            ],
            "title": "Unsupervised opinion summarization as copycat-review generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151\u20135169, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Arthur Bra\u017einskas",
                "Mirella Lapata",
                "Ivan Titov."
            ],
            "title": "Learning opinion summarizers by selecting informative reviews",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9424\u20139442, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Arthur Brazinskas",
                "Ramesh Nallapati",
                "Mohit Bansal",
                "Markus Dreyer."
            ],
            "title": "Efficient few-shot finetuning for opinion summarization",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1509\u20131523, Seattle, United States. Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633\u20136649, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Eric Chu",
                "Peter Liu."
            ],
            "title": "MeanSum: A neural model for unsupervised multi-document abstractive summarization",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Edunov",
                "Myle Ott",
                "Michael Auli",
                "David Grangier",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Classical structured prediction losses for sequence to sequence learning",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "G\u00fcnes Erkan",
                "Dragomir R. Radev."
            ],
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "venue": "J. Artif. Int. Res., 22(1):457\u2013479.",
            "year": 2004
        },
        {
            "authors": [
                "Kavita Ganesan",
                "ChengXiang Zhai",
                "Jiawei Han."
            ],
            "title": "Opinosis: A graph based approach to abstractive summarization of highly redundant opinions",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages",
            "year": 2010
        },
        {
            "authors": [
                "Suyu Ge",
                "Jiaxin Huang",
                "Yu Meng",
                "Jiawei Han."
            ],
            "title": "Finesum: Target-oriented, fine-grained opinion summarization",
            "venue": "Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM \u201923, page 1093\u20131101, New York,",
            "year": 2023
        },
        {
            "authors": [
                "Jinbae Im",
                "Moonki Kim",
                "Hoyeop Lee",
                "Hyunsouk Cho",
                "Sehee Chung."
            ],
            "title": "Self-supervised multimodal opinion summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Hayate Iso",
                "Xiaolan Wang",
                "Yoshihiko Suhara",
                "Stefanos Angelidis",
                "Wang-Chiew Tan."
            ],
            "title": "Convex Aggregation for Opinion Summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3885\u20133903, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Wenjun Ke",
                "Jinhua Gao",
                "Huawei Shen",
                "Xueqi Cheng."
            ],
            "title": "Consistsum: Unsupervised opinion summarization with the consistency of aspect, sentiment and semantic",
            "venue": "Proceedings of the Fifteenth ACM International Conference on Web Search and",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference for Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif Mohammad."
            ],
            "title": "Bestworst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chunyuan Li",
                "Xiang Gao",
                "Yuan Li",
                "Baolin Peng",
                "Xiujun Li",
                "Yizhe Zhang",
                "Jianfeng Gao."
            ],
            "title": "Optimus: Organizing sentences via pre-trained modeling of a latent space",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu."
            ],
            "title": "SimCLS: A simple framework for contrastive learning of abstractive summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu",
                "Dragomir Radev",
                "Graham Neubig."
            ],
            "title": "BRIO: Bringing order to abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890\u20132903,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Sgdr: Stochastic gradient descent with restarts",
            "venue": "5th International Conference for Learning Representations, ICLR 2017, Palais des Congr\u00e8s Neptune, Toulon, France, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Jordan Louviere",
                "Terry Flynn",
                "A.A.J. Marley."
            ],
            "title": "Best-Worst Scaling: Theory, Methods and Applications",
            "venue": "Cambridge University Press.",
            "year": 2015
        },
        {
            "authors": [
                "Ziming Mao",
                "Chen Henry Wu",
                "Ansong Ni",
                "Yusen Zhang",
                "Rui Zhang",
                "Tao Yu",
                "Budhaditya Deb",
                "Chenguang Zhu",
                "Ahmed Awadallah",
                "Dragomir Radev."
            ],
            "title": "DYLE: Dynamic latent extraction for abstractive long-input summarization",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.",
            "year": 2013
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of NAACL-HLT 2019: Demonstrations.",
            "year": 2019
        },
        {
            "authors": [
                "Nadav Oved",
                "Ran Levy."
            ],
            "title": "PASS: Perturb-andselect summarizer for product reviews",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Bo Pang",
                "Erik Nijkamp",
                "Wojciech Kryscinski",
                "Silvio Savarese",
                "Yingbo Zhou",
                "Caiming Xiong."
            ],
            "title": "Long document summarization with top-down and bottom-up inference",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Romain Paulus",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "A deep reinforced model for abstractive summarization",
            "venue": "CoRR, abs/1705.04304.",
            "year": 2017
        },
        {
            "authors": [
                "Ofir Press",
                "Lior Wolf."
            ],
            "title": "Using the output embedding to improve language models",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 157\u2013163, Valencia, Spain.",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Rafal J\u00f3zefowicz",
                "Ilya Sutskever."
            ],
            "title": "Learning to generate reviews and discovering sentiment",
            "venue": "CoRR, abs/1704.01444.",
            "year": 2017
        },
        {
            "authors": [
                "Gaetano Rossiello",
                "Pierpaolo Basile",
                "Giovanni Semeraro."
            ],
            "title": "Centroid-based text summarization through compositionality of word embeddings",
            "venue": "Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source",
            "year": 2017
        },
        {
            "authors": [
                "Ori Shapira",
                "Ran Levy"
            ],
            "title": "Massive multidocument summarization of product reviews with weak supervision",
            "year": 2020
        },
        {
            "authors": [
                "Shichao Sun",
                "Wenjie Li."
            ],
            "title": "Alleviating exposure bias via contrastive learning for abstractive text summarization",
            "venue": "CoRR, abs/2108.11846.",
            "year": 2021
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey E. Hinton."
            ],
            "title": "Visualizing high-dimensional data using t-sne",
            "venue": "Journal of Machine Learning Research, 9:2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Ke Wang",
                "Xiaojun Wan."
            ],
            "title": "TransSum: Translating aspect and sentiment embeddings for selfsupervised opinion summarization",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 729\u2013742, Online. Association",
            "year": 2021
        },
        {
            "authors": [
                "Lu Wang",
                "Wang Ling."
            ],
            "title": "Neural network-based abstract generation for opinions and arguments",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Ronald J. Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Machine Learning, 8(3):229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Shusheng Xu",
                "Xingxing Zhang",
                "Yi Wu",
                "Furu Wei."
            ],
            "title": "Sequence level contrastive learning for text summarization",
            "venue": "CoRR, abs/2109.03481.",
            "year": 2021
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed."
            ],
            "title": "Big bird: Transformers for longer sequences",
            "venue": "Proceedings of the 34th Interna-",
            "year": 2020
        },
        {
            "authors": [
                "Wen Zhang",
                "Yang Feng",
                "Fandong Meng",
                "Di You",
                "Qun Liu."
            ],
            "title": "Bridging the gap between training and inference for neural machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4334\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Ming Zhong",
                "Pengfei Liu",
                "Yiran Chen",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Extractive summarization as text matching",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197\u20136208, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Radev"
            ],
            "title": "PageRank-like algorithm that extracts sentences based on graph centrality, (b) EXTSUM (Bra\u017einskas et al., 2021), which uses the same ROUGE greedy heuristic as in Liu and Lapata (2019); unsupervised abstractive models",
            "year": 2019
        },
        {
            "authors": [
                "MEANSUM (Chu",
                "Liu"
            ],
            "title": "2019), which generates opinion summary by reconstructing the mean of review embeddings, (d) COPYCAT (Bra\u017einskas et al., 2020b), a VAE summarizer with hierarchical continuous latent representations to model products",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A plethora of online resources has been appealing for techniques of automatic information mining. Opinion summarization, as a task of generalizing views from a group of documents (e.g., reviews, posts, and discussions) related to an entity and presenting them in text form, has received considerable attention. The summarization of user opinions is of great advantage to public opinion research, marketing analysis, and decision-making (Im et al., 2021). While circumventing the tedious documentby-document browsing, it also offers more significant details compared to a single sentiment rating (Wang and Wan, 2021).\n\u2217Corresponding author\nDue to the burgeoning amount of online reviews and user needs, opinion summarization is expected to (1) process larger sets of documents and (2) provide summaries from different perspectives. One mainstream solution models the cross-document relations with sentence or document representations, using mean function (Chu and Liu, 2019; Bra\u017einskas et al., 2020b; Li et al., 2020), convex combination (Iso et al., 2021), and graph (Erkan and Radev, 2004; Ganesan et al., 2010) as well as other hierarchical structures (Isonuma et al., 2021; Amplayo et al., 2021a). These approaches are proven to achieve remarkable results with a moderate amount of reviews, usually within 10 (Shapira and Levy, 2020); however, they perform unsatisfactorily when the number of reviews further increases, as they focus on the fusion rather than the selection of the information. Another solution concatenates the reviews for long-range language models (Beltagy et al., 2020; Zaheer et al., 2020; Mao et al., 2022; Pang et al., 2023) and Large Language Models (LLMs; OpenAI, 2023), which converts multi-document summarization into singledocument summarization (Bra\u017einskas et al., 2020a; Oved and Levy, 2021; Ke et al., 2022; Brazinskas et al., 2022; Bhaskar et al., 2023). Despite the benefit brought by the LLMs, these methods struggle to handle the overlong combined reviews, missing a step to select from them either. Bra\u017einskas et al. (2021) first proposes to select smaller subsets of input reviews and provides verdict, pros, and cons summaries, yet differentiated treatments of different perspectives are not reflected in their method. Limited by data, there are seldom works targeting large-scale and multi-perspective opinion summarization.\nTo address the problems, we propose SUBSUMM, a supervised summarization framework for large-scale and multi-perspective opinion summarization. SUBSUMM comprises a review sampling strategy set and a two-stage training scheme. The\nreview sampling strategies are formulated with sentiment analysis and contrastive information valuation. With different strategies, the review subsets from different angles and quality levels can be selected. Then, the two-stage training method enables the summarization model to learn from the sub-optimal and optimal review subsets successively to fully utilize the input reviews within the model capacity. During the training stage II, a contrastive loss term is incorporated to further boost the performance of the summarizer.\nBy coupling with SUBSUMM, the Pre-trained Language Model (PLM) outperforms previous state-of-the-art models and LLMs under zeroshot settings on the AmaSum and Rotten Tomatoes datasets in our experiments, which demonstrates the superiority of the proposal. Further analysis also proves the effectiveness of the two modules in SUBSUMM.\nThe contributions of this paper are as follows.\n\u2022 We propose a large-scale opinion summarization framework1 to address the challenge of summarizing large review sets and providing opinions from different perspectives by selecting valuable review subsets.\n\u2022 We present (1) a review sampling strategy set based on sentiment analysis and contrastive information valuation and (2) a two-stage training scheme promoting the digestion and absorption of the massive input.\n\u2022 We substantiate the effectiveness of the proposed opinion summarization framework SUBSUMM with sufficient experiments and indepth analysis on two opinion summarization datasets from different domains."
        },
        {
            "heading": "2 Related Work",
            "text": "Opinion Summarization As high-quality annotation for the large opinion corpora is expensive to obtain (Ge et al., 2023), most works of opinion summarization are unsupervised, summarizing a limited number of reviews. Among the abstractive approaches, VAE-based and synthetic-datasetbased models have the upper hand.\nThe VAE-based models (Chu and Liu, 2019; Bra\u017einskas et al., 2020b; Li et al., 2020; Iso et al.,\n1The code is available at https://github.com/ Salomeeeee/SubSumm.\n2021; Isonuma et al., 2021) summarize through the aggregation of the latent representations of the reviews. COOP (Iso et al., 2021) considers the convex combination of input review representations. These methods work well with fewer reviews, while they suffer a performance drop when processing numerous reviews.\nThe synthetic-dataset-based methods (Amplayo and Lapata, 2020; Bra\u017einskas et al., 2020a; Oved and Levy, 2021; Wang and Wan, 2021; Amplayo et al., 2021b; Ke et al., 2022; Brazinskas et al., 2022) transform the unsupervised task into a supervised task by constructing review-summary pairs from original data. PASS (Oved and Levy, 2021) applies systematic perturbations to the input reviews for more candidate summaries and trains a classifier to rank the candidates. CONSISTSUM (Ke et al., 2022) measures the distances between reviews from aspect, sentiment, and semantics to create highly relevant review-summary pairs. ADASUM (Brazinskas et al., 2022) first fine-tunes the PLM with a synthetic dataset, then performs finetuning in a few-shot manner. The idea of making full use of the original text is embodied thoroughly in these methods.\nBenefiting from the growth of annotated data for opinion summarization, there are some emergent studies on supervised methods. Bra\u017einskas et al. (2021) provide a large-scale opinion summarization dataset enabling supervised training. They formulate the task as jointly learning to select informative reviews and summarize the opinions, and their solution SELSUM is based on reinforcement learning (REINFORCE; Williams, 1992). Aiming at avoiding the challenges brought by reinforcement learning, we decouple the process of selection and summarization in this work.\nContrastive Learning Contrastive learning in automatic summarization (Cao and Wang, 2021; Xu et al., 2021; Sun and Li, 2021; Liu and Liu, 2021; Liu et al., 2022) also gives us much inspiration. CLIFF (Cao and Wang, 2021) creates negative samples with automatically generated erroneous summaries. SIMCLS (Liu and Liu, 2021) trains an extra model with contrastive learning to evaluate and rank the candidate summaries. BRIO (Liu et al., 2022) introduces contrastive learning to assign a dual role to the model, alleviating inference performance degradation. In this work, we explore contrastive learning for multi-document summarization rather than single-document sum-\nSentiment Analysis\nTraining stage I\nTraining stage II\nReview subset produced by Sentiment-Random Review subset produced by Sentiment-Information\nmarization, and a PLM is fine-tuned contrastively for the information valuation."
        },
        {
            "heading": "3 Methodology",
            "text": "We introduce SUBSUMM, a supervised framework for large-scale and multi-perspective opinion summarization, as illustrated in Fig. 1. SUBSUMM is composed of a review sampling strategy set regarding sentiment orientation and information value, as elucidated in Sec. 3.1; and a two-stage training scheme, where contrastive learning with candidate summaries is extra performed, see Sec. 3.2.\nGiven the entity set and the corresponding sample set, for every sample {R1:N , S}, the goal of opinion summarization is to learn a function f that takes the review set as input and outputs a summary as close as possible to the reference:\nS \u2190 f(R1:N ) (1)\nwhere R1:N is the original review set, and S is the reference opinion summary. This paper mainly discusses the situation where R1:N is too large to be processed with most language models. For review set R1:N , let R1:K be the review subset where K \u226a N ."
        },
        {
            "heading": "3.1 Review Sampling Strategy Set",
            "text": "Sentiment Analysis We leverage sentiment analysis to filter the reviews roughly. It is supposed that the reviews with similar sentiment orientations are less likely to conflict in content than those with\ncontrary sentiment orientations. On the other hand, the sentiment tendency of the summary can be controlled by adjusting the proportion of input reviews with different sentiments; in this way, multiple angles are formed.\nWe formulate sentiment analysis as a text classification task. The sentiment tags of reviews in R1:N are computed as:\npseni = SLM(Ri) (2)\nSeni = argmax j\npseni (j) (3)\nwhere SLM(\u00b7) is a PLM with a linear classification head, and pseni refers to the probability distribution over the sentiment classes of the review Ri. The class with the highest probability is taken as the sentiment tag Seni. We use the rating of each review in the dataset as the sentiment label and apply a negative log likelihood loss w.r.t. the sentiment distribution pseni . After fine-tuning, the sentiment tags of all the reviews can be obtained. Contrastive Information Valuation Information valuation has finer granularity than sentiment analysis. Intuitively, once a subset of reviews is selected for summarization, the closer the generation is to the reference, the more valuable the information in the subset may be.\nGiven the reference summary, the information value of an input review is tied to its similarity with the reference; ROUGE (Lin, 2004) is an appropriate metric to estimate such similarity. Thus we fit the ROUGE score of a review\nRi = {r(1)i , ..., r (|Ri|) i } by modeling the correlation between the review and the whole review set:\nh (1) i , ..., h (|Ri|) i = Enc(r (1) i , ..., r (|Ri|) i ) (4)\nhi = 1\n|Ri| |Ri|\u2211 k=1 h (k) i (5)\nCorri = 1 N \u2212 1 \u2211\n1\u2264j\u2264N,j \u0338=i hihj (6)\nwhere Enc(\u00b7) is a transformer (Vaswani et al., 2017) encoder, and h(k)i denotes the last hidden state of token r(k)i . The review representation hi is computed by averaging the last hidden states of the tokens in Ri. Corri is the correlation score between Ri and the review set R1:N . We refer to the leave-one-out setting in unsupervised opinion summarization, computing Corri by the dot product of hi and the mean representation of the others.\nMindful of the volume of the original review set, it is unfeasible to fit the distribution of the ROUGE score directly or employ list-wise loss. Therefore, we resort to a contrastive margin loss:\nL = N\u2211 i=1 \u2211 r(j)>r(i) max(0, Corrj \u2212 Corri + \u03bbij)\n(7) where r(i) accounts for the ranking of Ri when sorted by ROUGE(Ri, S) in descending order, and \u03bbij = \u03bb(r(j)\u2212 r(i)) is a margin varying with the rankings, defined following Zhong et al. (2020); Liu and Liu (2021); Liu et al. (2022). The pairwise loss allows the model to learn the ROUGE rankings of a large review set. After fine-tuning, we can get the estimated information values of all the reviews. Multi-level Review Sampling Strategies Drawing support from the sentiment analysis and the contrastive information valuation, diverse review sampling strategies can be formed to select R1:K out of R1:N . We find that it is not ideal to accomplish the task with a single optimal subset, which will be explained in Sec. 4.3. To tackle the problem, we introduce stochastic factors to develop sampling strategies of multiple quality levels.\nThe sampling strategy set consists of the following three strategies:\n\u2022 Random Sampling: Randomly sample K reviews from the original review set R1:N as the subset R1:K .\n\u2022 Sentiment-Random Sampling: Firstly, divide all reviews into positive and negative types according to their sentiment tags Seni. Secondly, the number of reviews of each type in R1:K is determined by the type of the reference summary:\n(K+,K\u2212) =  (K, 0), pros (0,K), cons (KN +\nN , KN\u2212\nN ), verdict (8)\nwhere (K+,K\u2212), (N+, N\u2212) stands for the numbers of positive and negative reviews in R1:K , R1:N ; K++K\u2212 = K, N++N\u2212 = N . Finally, randomly sample K+,K\u2212 reviews from the positive and negative types respectively for R1:K .\n\u2022 Sentiment-Information Ranking: Firstly, compute (K+,K\u2212) likewise. Secondly, sort the reviews in descending order by the estimated information value Corri in two types separately. Finally, take the top-K+\npositive reviews and the top-K\u2212 negative reviews for R1:K .\nThe quality of the corresponding review subsets should improve in sequence."
        },
        {
            "heading": "3.2 Two-Stage Training for Large-Scale Opinion Summarization",
            "text": "SUBSUMM embodies a two-stage training scheme encouraging the summarizer to learn from the suboptimal and optimal review subsets successively.\nIn stage I, we choose the sub-optimal strategy, Sentiment-Random Sampling to re-sample the review subset R\u03071:K at each training epoch and train the model with standard maximum likelihood estimation (MLE):\n\u03b8\u2217 = argmax \u03b8 \u2211 i log p\u03b8(S|R\u0307 (i) 1:K) (9)\nwhere \u03b8 denotes the parameters of the abstract model, and p\u03b8 represents the probability derived by the parameters. The cross entropy loss is defined over the reference sequence of length l as:\nLI = Lxent =\n\u2212 l\u2211\ni=1 \u2211 s\u2208V p\u2217(s|R\u03071:K , S<i) log p\u03b8(s|R\u03071:K , S<i)\n(10)\nwhere s can be any token in the vocabulary V , and p\u2217 refers to an one-hot distribution. S<i stands for a pre-defined start token and the first i\u22121 tokens of the reference summary. However, Standard MLE is prone to exposure bias since it heavily relies on the ground-truth sequence (Zhang et al., 2019). Meanwhile, whichever strategy is adopted, the reviews sampled are only a part of the original review set, where the information can be further exploited.\nIn stage II, we take a cue from the practice of assigning probability mass to candidate summaries during training (Liu et al., 2022). Theoretically, assigning probability mass to a summary means an opportunity for the summary to pass on knowledge to the model through backpropagation. Hence the range of probability mass allocation is essentially the range of model learning, and better candidate summaries ought to compete for more probability mass. We plan to reuse the original review set via the candidate summaries.\nTo start with, we slightly modify the optimal strategy (i.e., Sentiment-Information Ranking), as some perturbations are required to obtain various candidate summaries for contrastive learning:\n\u2022 Sentiment-Information Ranking (modified): After computing (K+,K\u2212) in Eq. 8, take the estimated information value Corri of each review as weight to sample K+,K\u2212 reviews from the positive and negative types severally.\nNext, the modified optimal strategy is repeatedly conducted to get M review subsets, with which M candidate summaries S\u03021, S\u03022, ..., S\u0302M are generated by the model from stage I. The review subset produced by the original optimal strategy, denoted by R\u03081:K , will be the training input. We again calculate the ROUGE scores of the reviews in R\u03081:K with the reference summary S to derive the rankings and apply a contrastive loss term similar to Eq. 7:\nLctr = M\u2211 i=1 \u2211 r(j)>r(i) max(0, Lhj \u2212 Lhi + \u03bbij)\n(11) where Lhi is the length-normalized likelihood of the candidate summary S\u0302i, which is defined following Liu et al. (2022):\nLh(S) =\n\u2211|S| i=1 log p\u03b8(si|R\u03081:K , S<i)\n|S|\u03b1 (12)\nHere \u03b1 is a length penalty hyperparameter. This term enforces the model to assign more probability mass to better candidate summaries.\nFinally, to maintain the generation ability of the pre-trained model, we follow Edunov et al. (2018) to use the multi-task loss:\nLII = Lxent + \u03b3Lctr (13)\nwhere \u03b3 is the weight of the contrastive loss term. By involving the candidate summaries in training, stage II raises the utilization rate of the original review set and alleviates the problem of exposure bias; it acts as a complement to stage I considering the addition of the review subsets with higher quality and the contrastive loss term.\nDuring inference, given a review set R1:N , SUBSUMM predicts the sentiment tag and information value of each review with the fine-tuned PLMs in Sec. 3.1, then selects the optimal review subset R1:K according to the Sentiment-Information Ranking strategy and summarizes the subset using the summarization model from stage II."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets We choose two opinion summarization datasets with large review sets as our testbed. The statistics are shown in Appendix A. AmaSum2 (Bra\u017einskas et al., 2021) is a product review dataset where each sample contains a large number of reviews and reference summaries written by professional reviewers. Unlike other datasets, AmaSum provides reference summaries from three perspectives, namely verdict, which is equivalent to general opinion summary; pros and cons, which summarize the most important positive and negative details. As shown in Table 6, with 4.2k tokens on average, the combined reviews in AMASUM are too long to summarize with most summarizers. We refer to the preprocessing in SELSUM but split the dataset into three partitions with different targets. Rotten Tomatoes3 (RT; Wang and Ling, 2016) is a large-scale movie review dataset. For each movie, a one-sentence critic consensus is constructed by an editor to summarize the opinions in professional critics, which is treated as the reference summary. We follow Amplayo et al. (2021b) to preprocess the dataset; the data in RT and the verdict partition of AmaSum are equally treated in our experiments.\n2https://github.com/abrazinskas/SelSum 3https://web.eecs.umich.edu/~wangluxy/\ndata.html\nBaselines Concerning the baselines, we select a series of competitive models for the two datasets.\nOn AmaSum dataset, the baselines include (1)\nunsupervised extractive models LEXRANK (Erkan and Radev, 2004) and EXTSUM (Bra\u017einskas et al., 2021); (2) unsupervised abstractive models MEANSUM (Chu and Liu, 2019) and COPYCAT (Bra\u017einskas et al., 2020b); (3) supervised abstractive models SELSUM, LONGFORMER (Beltagy et al., 2020), and BRIO; and (4) zero-shot solutions related to LLMs, including GPT-3.5-turbo (CHATGPT) as well as QG (Bhaskar et al., 2023) based on QFSumm (Ahuja et al., 2022) and GPT-3 (Brown et al., 2020).\nOn RT dataset, the extra baselines are (1) unsupervised extractive models W2VCENT (Rossiello et al., 2017), SNCENT (Amplayo and Lapata, 2020), and BERTCENT (Amplayo et al., 2021b); (2) unsupervised abstractive models OPINOSIS (Ganesan et al., 2010) and DENOISESUM (Amplayo and Lapata, 2020); (3) weakly supervised model PLANSUM (Amplayo et al., 2021b). We classify PLANSUM as a weakly-supervised summarizer since it uses additional information other than review text.\nA detailed introduction to the baselines is in Appendix B.\nImplementation Details We used RoBERTa-base (Liu et al., 2019) for the sentiment analysis, a BART-base (Lewis et al., 2020) encoder for the contrastive information valuation, and BART-base\nas the backbone of our summarizer and its variants. In all of the review sampling strategies, we selected K = 10 reviews for every subset, which is explained in Appendix D. All the following experiments were conducted on 2 Geforce RTX 3090 GPUs. For the hyperparameters and more details, please refer to Appendix C."
        },
        {
            "heading": "4.2 Results",
            "text": "Automatic Evaluation We used ROUGE-1/2/L as the evaluation metrics and reported the F-1 scores. For AmaSum, we evaluated pros, cons, and verdict separately. As shown in Table 1 and Table 2, SUBSUMM significantly outperforms other methods on both datasets. Specifically, there are\ntwo observations: (1) SUBSUMM excels in generating summary with obvious emotion tendency, i.e., pros and cons. We notice that the three targets in AmaSum are equally treated by all the baselines, indicating the lack of exploration into the difference between the perspectives. SUBSUMM samples only positive/negative reviews for pros/cons summary. As depicted in Fig. 2, the reviews sampled for pros and cons are distributed in different zones of the semantic space, with the reviews for verdict evenly scattered between them. It not only reduces inconsistencies, but also adds valuable information to the input, since positive reviews always point out more advantages of the product, and vice versa.\n(2) The supervised methods score generally higher than the LLM-related methods, and SUBSUMM has an edge over the congener supervised systems. Although LLMs possess strong versatility in text generation, fine-tuning standard PLMs on annotated data seems non-trivial for opinion summarization. Compared with the other supervised methods, SUBSUMM reuses the reference summaries through contrastive learning in both information valuation and the training stage II, thus comprehensively utilizing the annotations. Especially, LONGFORMER\u2019s sparse attention mechanism plays an implicit selection role, while the review sampling strategies of SUBSUMM consider the sentiment tendency and information value, which is more sophisticated and task-specific. Human Evaluation As a supplement to automatic evaluation, we conducted a user study using Best-Worst Scaling (BWS; Louviere et al., 2015) detailed in Appendix E. The four evaluated summaries were GOLD (reference) summary and summaries generated by SELSUM, BRIO, and SUBSUMM. The three criteria were Informativeness, Coherence, and Non-Redundancy.\nResults in Table 3 demonstrate the considerable\npractical value of our model. Regarding Informativeness, summaries from SUBSUMM display comparable, even more information than GOLD summaries. As for Coherence, SUBSUMM leaves the users the best reading experience with correct grammar and straightforward expression. In terms of Non-Redundancy, SUBSUMM does not present the most succinct summaries, but considering the first two criteria, the redundancy is still acceptable.\nWe further compared our model with the LLM via 50 head-to-head tests between SUBSUMM and CHATGPT. The test cases were randomly sampled from the two datasets (15 samples from each partition in AmaSum\u2019s test set and 5 samples from RT\u2019s test set), and the annotators were asked to make pair-wise comparisons without the reference summaries. The results are shown in Table 4. It seems that the users prefer the summaries generated by SUBSUMM. An obvious issue of CHATGPT is that it cannot control the output length within a few calls when the input is overlong. Consequently, most of the summaries generated are either excessively long or abruptly truncated with the maximum length argument fixed. In addition, though CHATGPT is qualified to produce fluent text, it suffers from more severe hallucination than our model, which may compromise its ROUGE scores. In Table 10 are some supporting cases."
        },
        {
            "heading": "4.3 Analysis",
            "text": "With the purpose of deeper insights into our proposal, we carried out some in-depth analysis exper-\niments on AmaSum, using the same metrics as in automatic evaluation. We also reported the results on RT in Appendix F.\nComparison between Sampling Strategies As aforementioned, the quality of the three strategies in the review sampling strategy set would elevate sequentially. To confirm, we compare summarizers from the training stage I with different sampling strategies in the upper block of Table 5. RAND, SENTI-RAND, and SENTI-INFO apply Random Sampling, Sentiment-Random Sampling, and Sentiment-Information Ranking in training and inference respectively; SENTI-RAND-INFO is trained with Sentiment-Random Sampling but infers on review subsets produced by Sentiment-Information Ranking.\nBy comparing RAND with SENTI-RAND, it can be seen that with the aid of sentiment analysis, the review subsets sampled appear more useful for the summaries with emotion tendencies. There is no clear improvement from SENTI-RAND to SENTI-INFO, so we add SENTI-RAND-INFO to ascertain the reason. SENTI-RAND-INFO and SENTIRAND only differ in the test input, while the former wins with a clear margin, suggesting SentimentInformation Ranking produces better review subsets. SENTI-RAND-INFO shares the same test input with SENTI-INFO but results in higher ROUGE scores, possibly because the stochastic factor prevents the potential over-fitting problem. It also drops a hint that employing diverse review subsets might promote the model performance.\nInsight into Two-Stage Training Scheme We investigate the gains from the two-stage training scheme through an ablation study. The variants in the bottom block of Table 5 share the same test input with SUBSUMM.\nOur experiments evidence that both stage I and stage II are significant to model performance, while the latter plays a greater role. We suppose that the two stages are complementary to each other: the standard MLE training in stage I functions as taskspecific initialization, and the multi-task learning in stage II passes on more knowledge to the model, mitigating the exposure bias problem.\nMoreover, we explore how the two-stage training scheme contributes to the summary quality by replacing the sub-optimal and optimal strategies in the two training stages with Random Sampling. It leads to observable performance decreases, yet they are slighter than those when stage I or II is directly removed. It can be inferred that besides the complementary training objectives and additional training steps, the sensible selection of the review subsets is also conducive to model training."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we put forward a supervised summarization framework for large-scale and multiperspective opinion summarization, SUBSUMM. SUBSUMM supports a two-stage training scheme based on a set of review sampling strategies of multiple quality levels. Our model surpasses the state-of-the-art models and LLM-related systems on AmaSum and RT, manifesting its superiority in dealing with plentiful reviews and displaying various points of view. The analysis experiments verify that both components of SUBSUMM help the summarizer achieve better results.\nIn the future, we are planning to (1) explore more review sampling strategies to fully learn the aspect information and (2) combine the proposed framework with LLMs and generalize it to other large-scale multi-input tasks.\nLimitations\nThere are also some limitations in SUBSUMM. In Table 1, the verdict partition, the ROUGE-2 F1score of our model does not outweigh that of SELSUM; the ROUGE-L F1-score of our model is slightly lower than that of CHATGPT.\nFirstly, since ROUGE-2 reflects 2-gram recall, we suspect that this is due to the absence of explicit\ndesigns for aspect learning in SUBSUMM, which causes the model to miss more 2-gram aspect terms than SELSUM (We noticed that SELSUM emphasizes aspect learning). Secondly, ROUGE-L is computed based on the longest common subsequence, which has something to do with the fluency of the generation. We find that there are some errors, like repetitions and incomplete first words in the summaries from SUBSUMM. Compared to the LLMs with extensive parameters, our proposal still has room for improvement in language modeling.\nEthics Statement\nWe used only publicly available datasets, artifacts, and figures. Nevertheless, we realize that the proposed framework may produce fabricated and potentially harmful contents, for the PLMs used are pre-trained on heterogeneous web corpora. Therefore, we recommend that the users cautiously apply the proposal and its by-products."
        },
        {
            "heading": "Acknowledgements",
            "text": "The work is partially supported by the National Nature Science Foundation of China (No. 61976160, 61906137, 61976158, 62076184, 62076182) and Shanghai Science and Technology Plan Project (No. 21DZ1204800) and Technology Research Plan Project of Ministry of Public and Security (Grant No. 2020JSYJD01)."
        },
        {
            "heading": "A Dataset Statistics",
            "text": "The statistics of the datasets after preprocessing are shown in Table 6. The average numbers are taken except for \"Entity\"."
        },
        {
            "heading": "B Baselines",
            "text": "On AmaSum dataset, the baselines are unsupervised extractive models (a) LEXRANK (Erkan and Radev, 2004), a PageRank-like algorithm that extracts sentences based on graph centrality, (b) EXTSUM (Bra\u017einskas et al., 2021), which uses the same ROUGE greedy heuristic as in Liu and Lapata (2019); unsupervised abstractive models (c) MEANSUM (Chu and Liu, 2019), which generates opinion summary by reconstructing the mean of review embeddings, (d) COPYCAT (Bra\u017einskas et al., 2020b), a VAE summarizer with hierarchical continuous latent representations to model products and individual reviews; supervised abstractive models (e) SELSUM, a model jointly learns to select informative subsets of reviews and summarizing the opinions, (f) LONGFORMER, a long-range model with an attention mechanism that scales linearly with sequence length, (g) BRIO, the state-of-theart model of general abstractive summarization; and LLM-related solutions (h) GPT-3.5-turbo, (i) QG (Bhaskar et al., 2023), a pipeline where reviews are summarized by QFSumm (Ahuja et al., 2022) and GPT-3 (Brown et al., 2020), specifically the text-curie-001 model successively.\nOn RT dataset are some other baselines: unsupervised extractive models (j) W2VCENT (Rossiello et al., 2017), (k) SNCENT (Amplayo and Lapata,\n2020), and (l) BERTCENT (Amplayo et al., 2021b), which take encodings from word2vec (Mikolov et al., 2013), LSTM-based model (Radford et al., 2017), and BERT (Devlin et al., 2019) as the input representations; unsupervised abstractive models (m) OPINOSIS (Ganesan et al., 2010), a graphbased model that leverages token-level redundancy to summarize text, (n) DENOISESUM (Amplayo and Lapata, 2020), which re-formulates the summarization task as a denoising task; and a weakly supervised model (o) PLANSUM (Amplayo et al., 2021b), which constructs the synthetic dataset with a Dirichlet distribution parametrized by a content planner.\nC Implementation Details\nThe codes we used for fine-tuning the PLMs in sentiment analysis, contrastive information valuation, and the training stage I were implemented with Fairseq (Ott et al., 2019) library. For sentiment analysis, we set the learning rate to 1e-05 and updated the model parameters with the Adam optimizer. For contrastive information valuation, the margin \u03bb in Eq. 7 was 1e-02. We set the learning rate to 3e-05 and adopted the Adam optimizer with the cosine learning rate scheduler (Loshchilov and Hutter, 2017). The minimum and maximum learning rates were 1e-08 and 3e-05.\nDuring the training stage I, the input reviews were independently encoded, and the concatenated hidden states of the reviews were attended by the decoder to predict the summary. Following Press and Wolf (2017), the token embeddings were shared across the encoder and decoder for regularization. We used the learning rate of 3e-05 and\nthe Adam optimizer (Kingma and Ba, 2015) with 5,000 warmup steps for model optimization. The decoding strategy was beam search with a beam size of 5 and trigram blocking (Paulus et al., 2017).\nIn the training stage II, the reviews in the subset were joined with the separator <s> before being fed to the model, and M = 16 candidate summaries were collected for every training sample. We used the Adam optimizer with the same learning rate scheduler as BRIO and changed the maximum learning rate to 1e-03. We used the margin \u03bb of 1e-03 in Eq. 11 for all experiments. For summary generation, we used beam search of size 5. Particularly, We distinguished between the length penalty hyperparameter \u03b1 in Eq. 12 and lenpen in the beam search algorithm: the former was fixed at 2.0, and the latter differed across the targets. Other detailed hyperparameters are listed in Table 7.\nFor the baselines, we adapted BRIO to the task of large-scale opinion summarization. Concretely, while preparing the input, we always sampled R1:K out of R1:N and joined the K reviews with the separator <s>. The input of BRIO and CHATGPT was the optimal review subset as in Sec. 3.2 due to the maximum input length of 4096; LONGFORMER (LED-base-16384) and QG received the original review set as input. The prompts for the LLMs are presented in Fig. 3."
        },
        {
            "heading": "D Hyperparameter Selection",
            "text": "The hyperparameter K has a significant impact on the performance of SUBSUMM. In this paper, we followed the setting of the baseline model SELSUM and inherited K = 10 on AmaSum dataset for comparability. Before working on RT dataset, we had conducted experiments with varying K and random review subsets to explore the best value."
        },
        {
            "heading": "K = 6 21.36 4.30 15.87",
            "text": ""
        },
        {
            "heading": "K = 8 22.16 4.89 16.62",
            "text": ""
        },
        {
            "heading": "K = 10 23.20 5.56 17.28",
            "text": ""
        },
        {
            "heading": "K = 12 22.54 5.40 16.58",
            "text": "From the results in Table 8, it can be inferred that a too-small value of K can cause information deficiency, and a too-large one may introduce the sparsity problem even after the review selection, so we didn\u2019t change the value of K."
        },
        {
            "heading": "E Human Evaluation",
            "text": "BWS is known to produce more reliable results than raking scales (Kiritchenko and Mohammad, 2017) and is widely used in opinion summarization studies. We randomly selected 30 samples from the pros, cons, and verdict partition of AmaSum\u2019s test set severally and recruited 6 volunteers. The volunteers were asked to choose one best and one worst summary from four summaries for three criteria and report the confidence of their choices. For each volunteer\u2019s response, the best model received +1, the worst model received -1, and the rest of the models received 0 scores. Taking the confidence as weight, the scores of 6 volunteers were weighted and summed to get the final scores.\nAbout the criteria, Informativeness tells if the summary presents opinions about specific aspects of the entity in the round, Coherence measures how easy the summary is to read and reflects if the summary follows a natural ordering of facts,\nand Non-Redundancy measures the repetitions and unnecessary contents in the summary."
        },
        {
            "heading": "F Experiment Results",
            "text": "The results of the analysis experiments on RT are reported in Table 9. We list a set of example summaries from SUBSUMM and other baselines on the AmaSum dataset in Table 10."
        }
    ],
    "title": "Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets",
    "year": 2023
}