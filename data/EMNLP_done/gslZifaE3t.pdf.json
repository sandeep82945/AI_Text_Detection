{
    "abstractText": "Transferability estimation has been attached to great attention in the computer vision fields. Researchers try to estimate with low computational cost the performance of a model when transferred from a source task to a given target task. Considering the effectiveness of such estimations, the communities of natural language processing also began to study similar problems for the selection of pre-trained language models. However, there is a lack of a comprehensive comparison between these estimation methods yet. Also, the differences between vision and language scenarios make it doubtful whether previous conclusions can be established across fields. In this paper, we first conduct a thorough survey of existing transferability estimation methods being able to find the most suitable model, then we conduct a detailed empirical study for the surveyed methods based on the GLUE benchmark. From qualitative and quantitative analyses, we demonstrate the strengths and weaknesses of existing methods and show that H-Score generally performs well with superiorities in effectiveness and efficiency. We also outline the difficulties of consideration of training details, applicability to text generation, and consistency to certain metrics which shed light on future directions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun Bai"
        },
        {
            "affiliations": [],
            "name": "Xiaofeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Chen Li"
        },
        {
            "affiliations": [],
            "name": "Hanhua Hong"
        },
        {
            "affiliations": [],
            "name": "Xi Xu"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        },
        {
            "affiliations": [],
            "name": "Wenge Rong"
        }
    ],
    "id": "SP:aee8d2a2b7132a990ad1071852ec12c1a7aafe03",
    "references": [
        {
            "authors": [
                "Andrea Agostinelli",
                "Michal P\u00e1ndy",
                "Jasper R.R. Uijlings",
                "Thomas Mensink",
                "Vittorio Ferrari"
            ],
            "title": "How stable are transferability metrics evaluations",
            "venue": "In Proceedings of the 17th European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Zaid Alyafeai",
                "Maged Saeed AlShaibani",
                "Irfan Ahmad."
            ],
            "title": "A survey on transfer learning in natural language processing",
            "venue": "CoRR, abs/2007.04239.",
            "year": 2020
        },
        {
            "authors": [
                "Siqi Bao",
                "Huang He",
                "Fan Wang",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "PLATO: Pre-trained dialogue generation model with discrete latent variable",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 85\u201396.",
            "year": 2020
        },
        {
            "authors": [
                "Yajie Bao",
                "Yang Li",
                "Shao-Lun Huang",
                "Lin Zhang",
                "Lizhong Zheng",
                "Amir Zamir",
                "Leonidas J. Guibas."
            ],
            "title": "An information-theoretic approach to transferability in task transfer learning",
            "venue": "Proceedings of the 2019 IEEE International Conference on Image",
            "year": 2019
        },
        {
            "authors": [
                "Elisa Bassignana",
                "Max M\u00fcller-Eberstein",
                "Mike Zhang",
                "Barbara Plank."
            ],
            "title": "Evidence > intuition: Transferability estimation for encoder selection",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Bolya",
                "Rohit Mittapalli",
                "Judy Hoffman."
            ],
            "title": "Scalable diverse model selection for accessible transfer learning",
            "venue": "Proceedings of the 2021 Annual Conference on Neural Information Processing Systems, pages 19301\u201319312.",
            "year": 2021
        },
        {
            "authors": [
                "Zuohui Chen",
                "Yao Lu",
                "Wen Yang",
                "Qi Xuan",
                "Xiaoniu Yang."
            ],
            "title": "Graph-based similarity of neural network representations",
            "venue": "CoRR, abs/2111.11165.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Nan Ding",
                "Xi Chen",
                "Tomer Levinboim",
                "Soravit Changpinyo",
                "Radu Soricut."
            ],
            "title": "Pactran: Pacbayesian metrics for estimating the transferability of pretrained models to classification tasks",
            "venue": "Proceedings of the 17th European Conference on Computer",
            "year": 2022
        },
        {
            "authors": [
                "Kshitij Dwivedi",
                "Jiahui Huang",
                "Radoslaw Martin Cichy",
                "Gemma Roig."
            ],
            "title": "Duality diagram similarity: A generic framework for initialization selection in task transfer learning",
            "venue": "Proceedings of the16th European Conference on Computer Vision, pages",
            "year": 2020
        },
        {
            "authors": [
                "Kshitij Dwivedi",
                "Gemma Roig."
            ],
            "title": "Representation similarity analysis for efficient task taxonomy & transfer learning",
            "venue": "Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition, pages 12387\u201312396.",
            "year": 2019
        },
        {
            "authors": [
                "Shangwei Guo",
                "Chunlong Xie",
                "Jiwei Li",
                "Lingjuan Lyu",
                "Tianwei Zhang."
            ],
            "title": "Threats to pre-trained language models: Survey and taxonomy",
            "venue": "CoRR, abs/2202.06862.",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "Proceedings fo the 10th International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jiaji Huang",
                "Qiang Qiu",
                "Kenneth Church."
            ],
            "title": "Exploiting a zoo of checkpoints for unseen tasks",
            "venue": "Proceedings of the 2021 Annual Conference on Neural Information Processing Systems, pages 19423\u2013 19434.",
            "year": 2021
        },
        {
            "authors": [
                "Long-Kai Huang",
                "Junzhou Huang",
                "Yu Rong",
                "Qiang Yang",
                "Ying Wei."
            ],
            "title": "Frustratingly easy transferability estimation",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, pages 9201\u20139225.",
            "year": 2022
        },
        {
            "authors": [
                "Shibal Ibrahim",
                "Natalia Ponomareva",
                "Rahul Mazumder."
            ],
            "title": "Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance",
            "venue": "Proceedings of the 2022 European Conference on Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Mohammadreza Iman",
                "Khaled Rasheed",
                "Hamid R. Arabnia."
            ],
            "title": "A review of deep transfer learning and recent advancements",
            "venue": "CoRR, abs/2201.09679.",
            "year": 2022
        },
        {
            "authors": [
                "Qiao Jin",
                "Zheng Yuan",
                "Guangzhi Xiong",
                "Qianlan Yu",
                "Huaiyuan Ying",
                "Chuanqi Tan",
                "Mosha Chen",
                "Songfang Huang",
                "Xiaozhong Liu",
                "Sheng Yu."
            ],
            "title": "Biomedical question answering: A survey of approaches and challenges",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey E. Hinton."
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, pages 3519\u20133529.",
            "year": 2019
        },
        {
            "authors": [
                "Nupur Kumari",
                "Richard Zhang",
                "Eli Shechtman",
                "JunYan Zhu."
            ],
            "title": "Ensembling off-the-shelf models for GAN training",
            "venue": "Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10641\u201310652.",
            "year": 2022
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "Yandong Li",
                "Xuhui Jia",
                "Ruoxin Sang",
                "Yukun Zhu",
                "Bradley Green",
                "Liqiang Wang",
                "Boqing Gong."
            ],
            "title": "Ranking neural checkpoints",
            "venue": "Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition, pages 2663\u20132673.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Yifei Luo",
                "Minghui Xu",
                "Deyi Xiong."
            ],
            "title": "Cogtaskonomy: Cognitively inspired task taxonomy is beneficial to transfer learning in NLP",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 904\u2013920.",
            "year": 2022
        },
        {
            "authors": [
                "Amiel Meiseles",
                "Lior Rokach."
            ],
            "title": "Source model selection for deep learning in the time series domain",
            "venue": "IEEE Access, 8:6190\u20136200.",
            "year": 2020
        },
        {
            "authors": [
                "Shuteng Niu",
                "Yongxin Liu",
                "Jian Wang",
                "Houbing Song."
            ],
            "title": "A decade survey of transfer learning (2010-2020)",
            "venue": "IEEE Transactions on Artificial Intelligence, 1(2):151\u2013166.",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing ChatGPT",
            "venue": "https:// openai.com/blog/chatgpt.",
            "year": 2022
        },
        {
            "authors": [
                "Michal P\u00e1ndy",
                "Andrea Agostinelli",
                "Jasper R.R. Uijlings",
                "Vittorio Ferrari",
                "Thomas Mensink."
            ],
            "title": "Transferability estimation using bhattacharyya class separability",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9162\u20139172.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Phang",
                "Thibault F\u00e9vry",
                "Samuel R. Bowman."
            ],
            "title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
            "venue": "CoRR, abs/1811.01088.",
            "year": 2018
        },
        {
            "authors": [
                "Jo Plested",
                "Tom Gedeon."
            ],
            "title": "Deep transfer learning for image classification: A survey",
            "venue": "CoRR, abs/2205.09904.",
            "year": 2022
        },
        {
            "authors": [
                "Joan Puigcerver",
                "Carlos Riquelme Ruiz",
                "Basil Mustafa",
                "C\u00e9dric Renggli",
                "Andr\u00e9 Susano Pinto",
                "Sylvain Gelly",
                "Daniel Keysers",
                "Neil Houlsby."
            ],
            "title": "Scalable transfer learning with expert models",
            "venue": "Proceedings of the 9th International Conference on Learning",
            "year": 2021
        },
        {
            "authors": [
                "Sam T. Roweis."
            ],
            "title": "EM algorithms for PCA and SPCA",
            "venue": "Proceedings of the 1997 Conference on Neural Information Processing Systems, pages 626\u2013 632.",
            "year": 1997
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Matthew E. Peters",
                "Swabha Swayamdipta",
                "Thomas Wolf."
            ],
            "title": "Transfer learning in natural language processing",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Rylan Schaeffer",
                "Brando Miranda",
                "Sanmi Koyejo"
            ],
            "title": "Are emergent abilities of large language models a mirage? CoRR, abs/2304.15004",
            "year": 2023
        },
        {
            "authors": [
                "Wenqi Shao",
                "Xun Zhao",
                "Yixiao Ge",
                "Zhaoyang Zhang",
                "Lei Yang",
                "Xiaogang Wang",
                "Ying Shan",
                "Ping Luo."
            ],
            "title": "Not all models are equal: Predicting model transferability in a self-challenging fisher space",
            "venue": "Proceedings of the 17th European Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Jianlin Su",
                "Jiarun Cao",
                "Weijie Liu",
                "Yangyiwen Ou."
            ],
            "title": "Whitening sentence representations for better semantics and faster retrieval",
            "venue": "CoRR, abs/2103.15316.",
            "year": 2021
        },
        {
            "authors": [
                "Anh Tuan Tran",
                "Cuong V. Nguyen",
                "Tal Hassner."
            ],
            "title": "Transferability and hardness of supervised classification tasks",
            "venue": "Proceedings of the 17th IEEE/CVF International Conference on Computer Vision, pages 1395\u20131405.",
            "year": 2019
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Tu Vu",
                "Tong Wang",
                "Tsendsuren Munkhdalai",
                "Alessandro Sordoni",
                "Adam Trischler",
                "Andrew MattarellaMicke",
                "Subhransu Maji",
                "Mohit Iyyer."
            ],
            "title": "Exploring and predicting transferability across NLP tasks",
            "venue": "Proceedings of the 2020 Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "7th International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Ke Xu",
                "Dayiheng Liu",
                "Yike Guo",
                "Jie Fu."
            ],
            "title": "Interactive natural language processing",
            "venue": "CoRR, abs/2305.13246.",
            "year": 2023
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Kaichao You",
                "Yong Liu",
                "Jianmin Wang",
                "Mingsheng Long."
            ],
            "title": "Logme: Practical assessment of pretrained models for transfer learning",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, pages 12133\u201312143.",
            "year": 2021
        },
        {
            "authors": [
                "Amir Zamir",
                "Alexander Sax",
                "William B. Shen",
                "Leonidas J. Guibas",
                "Jitendra Malik",
                "Silvio Savarese."
            ],
            "title": "Taskonomy: Disentangling task transfer learning",
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Kun Zhao",
                "Bohao Yang",
                "Chenghua Lin",
                "Wenge Rong",
                "Aline Villavicencio",
                "Xiaohui Cui"
            ],
            "title": "Evaluating open-domain dialogues in latent space with",
            "year": 2023
        },
        {
            "authors": [
                "Chengkun Zheng",
                "Guanyi Chen",
                "Chenghua Lin",
                "Ruizhe Li",
                "Zhi Chen."
            ],
            "title": "Affective decoding for empathetic response generation",
            "venue": "Proceedings of the 14th International Conference on Natural Language Generation, pages 331\u2013340.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advances in the community of Natural Language Processing (NLP) are heavily built on the effectiveness of Pre-trained Language Models (PLMs), especially on large ones (LLMs) (Zeng et al., 2023; OpenAI, 2022; Touvron et al., 2023; Wang et al., 2023). As the number of available PLMs continually grows, a critical question arises: \"Which PLM can make the performance of a downstream task best?\". The fine-tuning result on a task usually varies across different PLMs, and this variation becomes more pronounced in low-resource scenarios (Bassignana et al., 2022).\nBasically, the key to such model selection is to figure out the transferability between the model and the target task. Pioneering works conducted finetuning on every candidate model in a brute-force manner (Phang et al., 2018; Zamir et al., 2019). Though the true fine-tuning performance can be obtained in this way, expensive parameter optimization is practically prohibitive (Wolf et al., 2020). Thus, there is an urgent need to quantify the transferability at a low cost of computation. To this end, Transferability Estimation (TE), as an essential task of Transfer Learning (TL), has emerged as a key challenge with several solutions proposed in Computer Vision (CV) fields initially (Agostinelli et al., 2022). Recently, some of these remarkable approaches have also been applied to NLP tasks which show promising results on PLM selection (Bassignana et al., 2022; Vu et al., 2022).\nDespite a great number of surveys established for TL and PLMs (Niu et al., 2020; Plested and Gedeon, 2022; Guo et al., 2022), there is no comprehensive survey on TE yet, especially with the purpose of PLM selection. Therefore, this paper aims to fill this gap by providing a comprehensive and well-structured summary of recent progress. To ensure comprehensive coverage, a multi-stage approach is employed to identify and select the\nstudies included in this review. Firstly, an extensive literature search was carried out using online databases, such as Google Scholar and DBLP. The search terms used were carefully chosen to capture the key concepts and themes related to TE and PLMs. After retrieving an initial pool of nearly 100 articles, a thorough screening of titles, abstracts, and keywords was conducted to exclude irrelevant studies, leading to a final selection of 20 studies that met the predetermined criteria for inclusion.\nBased on these research, we present a method taxonomy. As shown in Fig. 1, according to the need for training on target task, we divide TE methods into: (1) Model Similarity-based Methods that assume the inter-model similarity reflects the transferability which require the model trained on target task (Dwivedi and Roig, 2019). (2) Training-free Methods that accelerate the estimation process by computing metrics free of target model training to examine the compatibility of the PLM\u2019s feature space on the target dataset (Ding et al., 2022). Then we conduct qualitative analysis for the applicability and provide empirical results on the GLUE benchmark (Wang et al., 2019) to manifest specific strengths and weaknesses in existing methods. We show that model similarity-based methods have the superiority of applicability to different target tasks, and training-free methods have the advantage over fast estimation. And for the methods simulating the dynamics of fine-tuning, they generally perform better. Besides, we analyze some factors that can affect the estimation effectiveness and efficiency including task type, sample size, feature dimension, target model as well as sample affinity function. The empirical observations demonstrate that H-Score (Bao et al., 2019) generally shows desired usability. Based on these investigations, we further exhibit some under-explored aspects to shed light on the future directions 1."
        },
        {
            "heading": "2 Related Work",
            "text": "Transfer Learning. Training robust supervised models from scratch is non-trivial, especially in low-resource scenarios (Jin et al., 2023). Aiming at transferring knowledge from a source task to a target task, TL can achieve superior performances on the target dataset by spending far less time and using far fewer data (Niu et al., 2020). Despite a good number of surveys available for TL (Ruder\n1The code is available at https://github.com/ba1jun/ model-selection-nlp.\net al., 2019; Niu et al., 2020; Alyafeai et al., 2020; Iman et al., 2022), these works mainly focus on \u201cwhat to transfer?\u201d and \u201chow to transfer?\u201d that describe specific transfer approaches. To the best of our knowledge, there is no comprehensive survey on TE yet, which seeks to answer the question of \u201cwhen to transfer?\u201d. This work is expected to fill this gap by shedding light on how to appropriately choose TE methods for PLMs practitioners. Transferability Estimation. To avoid exhaustive attempts on all pairs of source tasks and target tasks, TE provides efficient heuristics to exhibit the best-performing source task at a minor cost (Agostinelli et al., 2022). Originated in the field of CV, a great number of TE approaches, including model-similarity-based methods (Dwivedi and Roig, 2019), label-comparison-based methods (Tran et al., 2019) and source features-based methods (Ding et al., 2022), etc., have been proposed in the past few years. To adapt such techniques to PLM selection for NLP tasks, Bassignana et al. (2022) found the predictions of LogME can positively correlate with the true performances of candidate PLMs, and Vu et al. (2022) exhibited the model similarity computed by soft prompts reflects the transfer performance across different models. Built on these remarkable researches, we further review the TE methods and provide a comprehensive empirical study of them for PLM selection. Pre-trained Language Models. From BERT (Devlin et al., 2019) to LLaMA (Touvron et al., 2023), significant efforts have been put into scaling PLMs into LLMs and some abilities such as performing arithmetic, answering questions are emerging simultaneously (Schaeffer et al., 2023). Nevertheless, training and fine-tuning LLMs or even small ones require substantial computational resources which can limit accessibility to these models for researchers and developers with limited resources even with the help of parameter-efficient tuning (Hu et al., 2022). Based on these considerations, the efficient utilization of PLMs is still a problem worth studying. Thus we focus on the selection of PLMs in this work which aims at releasing the computing resources needed for exhaustive fine-tuning."
        },
        {
            "heading": "3 Transferability Estimation Taxonomy",
            "text": ""
        },
        {
            "heading": "3.1 Problem Formalization",
            "text": "Formally, given a pool of L candidate PLMs {\u03d5i}Li=1 and a target dataset D = {(xi, yi)|xi \u2208 X , yi \u2208 Y}Ni=1 containing N samples where each\n\u03d5i can encode the sample to pre-trained feature \u03d5i(xi) (usually the [CLS] embedding), the true performance Ti(D) can be measured by certain evaluation metrics after fine-tuning \u03d5i on D with careful tuning of hyper-parameters. The TE approach should produce a score Si(D) for each \u03d5i to approximate the true fine-tuning performance Ti(D). Intuitively, a well-designed method should return {Si(D)}Li=1 that correlates well with {Ti(D)}Li=1 under an acceptable burden, such that the topperforming PLM can be determined rapidly."
        },
        {
            "heading": "3.2 Model Similarity-based Methods",
            "text": "To avoid brute force fine-tuning, the model similarity-based methods are designed based on the assumption that a high similarity between two models correlates with a high degree of transferability between the tasks bonded to the models. To this end, one model \u03c8 fine-tuned on the target task, i.e., the target model, is required to compute its similarity to each candidate PLM. Therefore, the time consumption of fine-tuning can be significantly reduced to 1/L of brute force approach extra with a minor cost of model similarity computation.\nCurrently, the sample features output from models are mainly used to measure the inter-model similarity. Therefore, the target is to design a similarity function to maximize the correlation between finetuning performances and similarities between the pre-trained features {\u03d5(xi)}Ni=1 and target features {\u03c8(xi)}Ni=1. In terms of the similarity computation mechanism, existing functions can fall into samplewise similarity functions and graph-wise similarity functions:\nSample-wise Similarity Functions The main idea is to directly compute the similarity between features across models. Under the Direct Similarity Estimation (DSE) (Luo et al., 2022) framework, Vu et al. (2020) compute the affinity between the mean features as the model similarity, i.e., A( \u2211 i \u03d5(xi)/N, \u2211 i \u03c8(xi)/N) where A is the sample affinity function such as Euclidean and cosine distances, while Luo et al. (2022) utilize averaged sample affinities \u2211 iA(\u03d5(xi), \u03c8(xi))/N .\nGraph-wise Similarity Functions In the form of Duality Diagram Similarity (DDS) (Dwivedi et al., 2020) framework, the graph-wise functions first measure the affinities for every sample pair in each model feature space separately, then compute the similarity between the resulting affinity graphs G\u03d5 = (V\u03d5, E\u03d5) and G\u03c8 = (V\u03c8, E\u03c8) with the\nsample features as vertices, e.g., V\u03d5 = {\u03d5i}Ni=1, and the inter-sample affinities as edges, e.g., E\u03d5 = {A(\u03d5i, \u03d5j)|\u03d5i, \u03d5j \u2208 V\u03d5}. For instance, Representation Similarity Analysis (RSA) (Dwivedi and Roig, 2019), Graph-Based Similarity (GBS) (Chen et al., 2021), Kernel Alignment (KA) (Huang et al., 2021) and Centered Kernel Alignment (CKA) (Kornblith et al., 2019) which are only slightly different in the ways to compute inter-sample affinities and inter-graph similarities."
        },
        {
            "heading": "3.3 Training-free Methods",
            "text": "Although model similarity-based methods only need to fine-tune on the target task once, they still require a large load of computational cost. Therefore, the training-free methods try to directly compare the pre-trained features {\u03d5(xi)}Ni=1 with the true target labels {yi}Ni=1 by cheap metrics to further save the estimation time. According to whether directly measure the fine-tuning loss, the metrics can be divided into class separability metrics and loss approximation metrics.\nClass Separability Metrics These metrics intuitively examine whether pre-trained features are easy to separate according to their target labels, and assume that well-separated pre-trained features results in desired fine-tuning performance. Some of these metrics directly measure the separability of static pre-trained features. For example, MSC (Meiseles and Rokach, 2020) uses the mean intra-cluster distance and the mean nearestcluster distance to quantify the clustering quality of pre-trained features over target classes. Similarly, Puigcerver et al. (2021) rank the candidate PLMs by the test accuracy of kNN on pre-trained features via the leave-one-out cross-validation. PARC (Bolya et al., 2021) first computes the pairwise affinities between the pre-trained features of each pair of target samples, which is then compared with the pairwise label affinities of each pair of target samples to quantify the source feature space\u2019s fitness on target dataset. And GBC (P\u00e1ndy et al., 2022) uses the Bhattacharyya coefficient to measure the inter-class overlap of pre-trained features, where higher overlap means poorer separability. To further consider the fine-tuning dynamics by assuming the pre-trained features can be adjusted by an extra linear transformation, Kumari et al. (2022) train a cheap Logistic Regression (LR) model on pre-trained features to estimate how fitting the linearly transformed pre-trained features are for their\ntarget classes by LR\u2019s test accuracy.\nLoss Approximation Metrics Based on solid theoretical proof, these metrics try to directly approximate the fine-tuning loss that correlates with the fine-tuning performance well. Inspired by Euclidean information geometry, H-Score (Bao et al., 2019) approximates the optimal log-loss by interclass variance and feature redundancy that characterize the asymptotic error probability of using pre-trained features to estimate target labels. Ibrahim et al. (2022) then propose regularized Hscore which further shrinks the error that occurred when inverting the high-dimensional features using a pseudo-inverse. NLEEP (Li et al., 2021) first uses a Gaussian mixture model to attach a posterior distribution on Gaussian components to each pre-trained feature, then computes the likelihood from posterior distribution to target label to approximate that from pre-trained feature to target label. TransRate (Huang et al., 2022) approximates the correlation between pre-trained features and target labels by Mutual Information (MI) which has been proven an upper bound and a lower bound to the log-likelihood. There are also some metrics that involve fine-tuning dynamics. SFDA (Shao et al., 2022) simulates the dynamics by projecting the pre-trained features using Fisher Discriminant Analysis (FDA) to increase the class separability. Then, it approximates the log-likelihood by Bayes classification over projected features and also adds a self-challenging module to further measure the ability of the pre-trained models on hard samples. To avoid over-fitting problem of maximum likelihood estimation, LogME (You et al., 2021) turns to approximate the marginalized likelihood of label given pre-trained features over all possible linear transformation. More recently, motivated by learning theory, PACTran (Ding et al., 2022) minimizes the PAC-Bayesian upper bound over the log-loss."
        },
        {
            "heading": "4 Qualitative Analysis",
            "text": "To examine the applicability of each method, we qualitatively compare them as shown in Table 1 from the following perspectives: (1) Task Agnostic: the method does not require certain target task type; (2) Dynamic Consideration: the fine-tuning dynamics of pre-trained features are considered; (3) Free of Training: the method does not need fine-tuning on target task. Task Agnostic A widely applicable method should be able to deal with multiple task types such as classification, regression, and generation. Currently, only model similarity-based methods satisfy this property. However, the inter-model similarity is only aware of sample features and does not consider the output space of the task.\nDynamic Consideration Since fine-tuning appropriately adjust the representation of pre-trained features to adapt to the target task, the training dynamics are also a key factor. To date, only Logistic, LogME, SFDA, and PACTran assume that the pre-trained features can be adjusted by a linear transformation, while the fine-tuning process can be more diverse, e.g., adapter tuning (Houlsby et al., 2019) and prompt tuning (Lester et al., 2021), how to simulate different dynamics is under-explored.\nFree of Training The trained target model is mainly needed by model similarity-based methods. Although this is not a serious problem since the target model can be reused when estimating for different PLMs, it still takes a training time and whether model similarity-based methods perform stably on different target models is also doubtful."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "Datasets Following You et al. (2021), we validate TE methods on the GLUE benchmark (Wang et al., 2019) that is a collection of diverse Natural Language Understanding (NLU) tasks. Note, we remove the STS-B task since it is a regression task that is not suitable to most TE methods, the details of others are reported in Table 2.\nCandidate PLMs Since the HuggingFace Model Hub (Wolf et al., 2020) has already provided carefully tuned results on GLUE tasks for some models, we select 6 PLMs that have all the GLUE tasks\u2019 results following You et al. (2021): namely bert-base-uncased, bert-base-cased (Devlin et al., 2019), roberta-base (Liu et al., 2019) and their distilled versions which are distilbert-base-uncased, distilbert-base-cased and distilroberta-base (Sanh\net al., 2019) whose fine-tuning performances are as shown in Appendix A.\nMethods Setups Each candidate PLM to be finetuned is followed by a classification layer which takes the output embedding of BOS (Beginning of Sequence, e.g., [CLS] for BERT and <s> for RoBERTa) token as input, and all the TE methods utilize the same BOS embeddings as pre-trained features. Before estimating by certain methods, we also conduct dimensionality reduction on the sample features by Principal Component Analysis (PCA) (Roweis, 1997) since some methods\u2019 performances heavily depend on the number of feature dimensions. For model similarity-based methods, we also try different PLMs to train the target model (i.e., ALBERT: albert-base-v2, DeBERTa: debertabase and ELECTRA: electra-base-discriminator). Moreover, since some methods need to compute affinity graph which can be very time-consuming when there are too many samples, i.e., DDS, kNN, MSC, PARC, LFC, we limit the maximum number of samples that can be used by them to 10k. We run each method 5 times with different random seeds and report the mean results. For the implementation details, please refer to Appendix B.\nEvaluation To measure the deviation of TE methods\u2019 predictions to true fine-tuning performances, we use MRR and mean Spearman\u2019s rank correlation (\u00b5\u03c1) on all GLUE tasks. Among them, MRR reveals the average ranking of the best-performing PLM, and \u00b5\u03c1 evaluates the overall correlation between predicted score list {Si(D)}Li=1 and true performance list {Ti(D)}Li=1. Besides, the time consumption is measured by mean training time (\u00b5tt) that records the time of target model training and mean estimating time (\u00b5et) that tells the wall clock time of estimation value computing, which are both averaged over all GLUE tasks. Note that we omit the time of sample features encoding since this is the same across all methods."
        },
        {
            "heading": "6 Quantitative Analysis",
            "text": "Effectiveness and Efficiency The overall metric scores of all methods are reported in Table 3. For estimation effectiveness, although model similaritybased methods excel at adapting to different target tasks, they generally perform worse than trainingfree methods. Notably, DSE and PARC achieve the best MRR and \u00b5\u03c1 respectively, while they need to carefully select sample affinity function to produce\nsuch desired results. Another obvious observation is that Logistic, LogME, SFDA, and PACTran all result in a remarkable performance, which empirically validates the importance of fine-tuning dynamics. For estimation efficiency, model similaritybased methods all need considerable time consumption on target model training. Unless the target model training time is negligible compared to the whole PLM selection time, this kind of method has no advantage over the training-free method in terms of speed. Generally, the training-free methods run pretty fast, only the methods need to compute affinity graph will consume a lot of time (kNN, MSC, LFC, PARC), which have to limit the amount of data samples when they are employed on huge datasets. For detailed results on each task, please refer to Appendix C. Sensitivity to Task Type More detailed performances on three different types of tasks are also reported in Table 3. Generally, TE methods perform better on sentence pair tasks (paraphrase and inference) than on single sentence tasks (sentence classification), where 11 out of 14 TE methods result in \u00b5\u03c1 of sentence pair tasks superior to that of single sentence tasks. We speculate the reason is that most candidate PLMs used in this work are from the BERT family and the corresponding Next Sentence Prediction (NSP) (Devlin et al., 2019) pretraining task makes the [CLS] embedding more suitable for encoding a sentence pair, such that TE methods cannot fully understand the samples through the pre-trained features when meeting sentence classification task. Robustness to Fewer Samples The key strength of training-free methods lies in no need to target model training, while the total time consumption can be further reduced if the method also performs well when using only a small amount of data samples, such that the encoding time of ignored samples can be saved (Encoding for all GLUE datasets takes 1.73 hours in our case). To examine the data efficiency, we select 8 top-performing training-free methods and conduct PLM selection when different percentages of data are conditioned. Figure 2 illustrates the overall performance variation on GLUE\nof training-free methods as the data percentage changed. By employing a shrinkage-based estimator of covariance, regularized H-Score exhibits stable performance compared to the original H-score. And LogME also shows similar stability on both MRR and \u00b5\u03c1 since it is based on the marginalized likelihood which can alleviate the over-fitting problem on a small dataset. We can observe that kNN also performs stable on \u00b5\u03c1 while it needs careful selections of k and sample affinity function. However, a significant decrease in MRR occurred on almost all methods, indicating more advanced approaches are required to achieve accurate selection in low-resource scenarios.\nEffect of Feature Dimensions If a method performs best when conducted on the pre-trained features with the original dimension, then there is no need to employ dimensionality reduction and tune the reduced dimensions, which can further save the\nestimating time. As shown in Figure 3, we list the performance variation of top-8 performing trainingfree methods on different feature dimensions. It is observed that H-Score and regularized H-Score preferably enjoy original dimensions because the original feature space helps them to approximate the feature redundancy better, while the others all achieve the best results on smaller dimensions. For kNN, PARC, they need to measure sample affinity which may encounter the curse of dimensionality in high-dimensional scenes, thus performing better when the feature dimension is small meanwhile dimensionality reduction will not lose too much original information. For Logistic, LogME, SFDA, and PACTran which assume the pre-trained features can be linearly transformed, eliminating redundant feature dimensions can results in better estimation results.\nSensitivity to Different Target Models Although model similarity-based methods only need\nto train one target model, we actually have rich choices of PLM to train target models. An ideal method should produce similar results when different target models are implemented such that we can save the time required to try different target models. To examine the sensitivity to the target model, we conduct model similarity-based methods with different target models and show the results in Table 3. We can observe different behaviors in which DSE performs stably while DDS is very sensitive to the type of target model. Since the main difference between DSE and DDS is that the former computes inter-sample affinities across models while the latter compares the affinity graph across models, this observation reflects that the affinity graph from the target model may not well reflect the target task mechanism and directly measuring the affinity between features across models is preferable.\nEffect of Sample Affinity Function As introduced in Section 3, the implementation of DSE, DDS, kNN, MSC, LFC, and PARC require certain sample affinity functions. We try Euclidean, cosine, and correlation distances for the above methods to examine whether different functions will affect the methods and report the results in Table 4. Compared to Euclidean distance, cosine distance and correlation distance conduct extra normalization operations and thus results in more estimating time. However, except when applied to DSE, cosine and correlation distances generally exhibit superior performance than Euclidean distance. This observation reveals that the norm of the feature vector may result in anisotropic feature space and should not be taken into account to measure the sample affinity, which is also supported by (Su et al., 2021) that suggests the normalization operation can alleviate the anisotropy problem of PLMs."
        },
        {
            "heading": "7 Conclusion and Future Directions",
            "text": "This paper reviews the recent advances in TE that can be applied to PLM selection and presents a method taxonomy based on a thorough analysis. Moreover, comprehensive qualitative and quantitative comparisons between different approaches are provided to help understand their applicability in a number of aspects. We hope this survey can help people for the purpose of research or industry to choose desired PLM by appropriate TE methods.\nAlthough lots of efforts have been made as surveyed, there still remain some directions that deserve further investigation:\n(1) How to make the estimation approach aware of fine-tuning strategies and experimental hyper-parameters? The fine-tuning strategy usually needs to be determined under the acceptable computation burden, i.e. fully fine-tuning (optimizing all model parameters) or parameter-efficient tuning (optimizing part of model parameters). The actual fine-tuning strategy not only affects the training time and computation consumption but also the loss landscape of PLM which results in different target task performance (Bassignana et al., 2022). However, current approaches usually consider the situation of one strategy whose effectiveness can not be guaranteed in other situations. Therefore, making the estimation able to adapt to different fine-tuning strategies is worth further exploring. Besides, even if the best PLM can be accurately selected, one still needs exhaustive searching of training hyper-parameters to produce desired finetuning performance. It is also interesting to consider other important hyper-parameters such as learning rate and temperature when estimating the transferability.\n(2) How to adapt TE methods to text generation task? Although model similarity-based methods do not assume the type of target task since these methods only rely on the sample features, they neglect the information of the target label and thus the mapping from input space to output space is not well captured and the corresponding task can not be fully understood. However, taking label information into consideration for the text generation task is challenging since the length of output text changes and the one-to-many issue exists (Bao et al., 2020; Zheng et al., 2021; Zhao et al., 2023). Since currently LLMs conduct all tasks in the way of text generation and the number of LLMs is continually increasing, the TE method tailored for text generation is urgently needed.\n(3) How to make estimation results consistent with specific evaluation metrics? In our experiments, the TE methods are asked to correlate just one evaluation metric for GLUE datasets, e.g., Acc for QNLI. However, some tasks may have diverse metrics, e.g., NDCG, R@1 for ranking tasks, and sometimes one may focus on one of the metrics and the variations of these metrics are not necessarily consistent, such that the TE method\u2019s predictions can be confusing in these cases. Therefore, how to make TE methods aware of our interested metric is another direction worth exploring.\nLimitations\nThis work provides a comprehensive summary of existing TE methods. However, limited by our experimental conditions, we have to examine surveyed methods on a toy experimental setting where the following problems need to be improved: (1) Only small-scale PLMs form the candidate pool, the effectiveness of TE methods to select the bestperforming LLM is needed to be verified given the current popularity of LLMs. (2) Since most of existing TE methods only support target task of classification type, we determine the GLUE benchmark as the evaluation datasets, while the TE performances on regression task, structure prediction task and generation task are still under-explored."
        },
        {
            "heading": "A Fine-tuning Results on GLUE",
            "text": "B Implementation Details\nOur experimental machine contains an Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz and a NVIDIA GeForce RTX 3090 24G GPU. For the implementation of TE methods, since some methods\u2019 performance heavily depend on the number of feature dimensions, we reduce the feature dimension to [16, 32, 64, 128, 256, 512, 768] by PCA for each method to find their most suitable dimensions. For DSE, RSA, kNN, MSC, LFC, PARC that need to compute the affinities between sample features, we also try different sample affinity functions including cosine, euclidean and correlation distances. The implementation details of surveyed methods are as follows:\nDSE Among the averaged sample affinities (Luo et al., 2022) and the affinity between the mean features (Vu et al., 2020), we found that the former performs better. And DSE achieves the best results when DeBERTa is taken as target model, Euclidean distance is used to measure the sample affinity and the number of feature dimensions is 768. The corresponding computation is as Eq. 1.\nS(D) = \u2212 1 N N\u2211 i=1 \u2225(\u03d5(xi)\u2212 \u03c8(xi)\u22252 (1)\nDDS Among a number of instances of DDS framework (Dwivedi et al., 2020), RSA (Dwivedi and Roig, 2019) shows the best performance when DeBERTa is trained as target model, correlation distance is used and the number of feature dimensions is 512. Specifically, the pre-trained features and target features are first processed by z-score normalization. Then the pre-trained affinity graph and target affinity graph are computed by correlation distance. Finally, the lower triangular adjacent matrices of two graphs are compared by Spearman correlation coefficient as transferability score.\nMSC We use the code of silhouette_score from scikit-learn to implement MSC, which exhibits the best performance when cosine distance is used and the number of feature dimensions is 256.\nkNN We use the code of KNeighborsClassifier from scikit-learn and use the test accuracy of leave-oneout cross-validation to quantify the transferability. We tune the k in [1, 3, 5, 7], the method exhibits the best performance when k = 5, correlation distance is used and the number of feature dimensions is 64.\nPARC The computation process of PARC is similar to that of RSA except that the target affinity graph is replaced by affinity graph derived from samples\u2019 one-hot labels. We use the code from here and the best performance is achieved when correlation distance is used and the number of feature dimensions is 512.\nGBC It first uses Gaussian distribution to model each target class of samples which is parameterized by the in-class pre-trained features vectors. Then the averaged Bhattacharyya distance between every pair of different classes are used to measure the inter-class overlap as Eqs. 2 and 3:\nBC(pvi , pvj ) = \u222b \u221a pvi(\u03d5(x))pvj (\u03d5(x))dx (2)\nS(D) = \u2212 \u2211 i \u0338=j BC(pvi , pvj ) (3)\nwhere v is a specific value of target classes. We use the code from here and the most suitable number of feature dimensions is 64.\nLogistic We use the code of LogisticRegression from scikit-learn with the default hyper-parameters to classify the pre-trained features. The test accuracy of leave-one-out cross-validation is used to quantify the transferability and the most suitable number of feature dimensions is 64.\nH-Score As Eq. 4 shows, it first computes the covariance matrix over the feature dimensions of pretrained features and that over the feature dimensions of each target class\u2019s mean feature, then the trace of the dot-product between the inverse matrix of former and the latter is used to approximate the optimal log-loss. We use the code from here and the most suitable number of feature dimensions is 768.\nS(D) = tr(cov(\u03d5(X ))\u22121cov(EP (X|Y)[\u03d5(X )|Y])) (4)\nRegularized H-Score Compared to H-Score, it further solve the statistical problem of covariance estimation by shrinkage-based estimator (Ibrahim et al., 2022). The most suitable number of feature dimensions is 768.\nNLEEP It first uses Gaussian mixture model to fit the pre-trained features, then computes the Log Expected Empirical Prediction (LEEP) score between posterior distribution derived from fitted Gaussian mixture model and the target labels as Eq. 5:\nS(D) = 1 N N\u2211 i=1 log( \u2211 c\u2208C P (y|c)P (c|\u03d5(x))) (5)\nwhere c is the specific Gaussian component and C is the space of all components. We use the code from here where the number of Gaussian components is five times that of target classes and the most suitable number of feature dimensions is 64.\nTransRate It argues that the mutual information I(\u03d5(X ),Y) = H(\u03d5(X )) \u2212 H(\u03d5(X )|Y) between the pre-trained features and the target labels serves as a strong indicator for the performance of model. Since the mutual information is notoriously difficult to compute especially for continuous variables in high-dimensional settings, the authors turn to utilize rate distortion that is closely related to Shannon entropy. The code can be found in (Huang et al., 2022) and the most suitable number of feature dimensions before computing the mutual information is 64.\nLogME It uses marginal evidence of the target task P (y|\u03d5(x)) = \u222b P (w)P (y|\u03d5(x), w)dw where w is the weight of linear classifier. The prior P (w) is defined as Gaussian and P (y|\u03d5(x), w) is a Gaussian likelihood, then P (y|\u03d5(x)) can be analytically estimated. We use the code from here, and the most suitable number of feature dimensions is 512.\nSFDA It first projects the pre-trained features by regularized Fisher Discriminant Analysis such that the projected features can have better separability of target classes which can simulate the fine-tuning dynamics. Based on Bayes classification, the projecting weights can be used to compute the prediction probability of each sample on target label which is further used to compute the log-likelihood as the transferability. Moreover, confidential mix noise is further added to examine the model\u2019s ability to classify hard samples. We use the code from here and the most suitable number of feature dimensions is 512.\nPACTran It has three instances using the Dirichlet, Gamma and Gaussian as prior distributions respectively. Since the first two priors require the pre-training task head layer or non-negative features which are not the case in PLM selection, we implement PACTran with the more general Gaussian prior by the code from here, and tune the \u03bb and \u03c320 in [0.1, 1, 10] and [1, 10, 100, 1000]. The PACTran performs best when \u03bb = 1, \u03c320 = 10 and the number of feature dimensions is 64."
        },
        {
            "heading": "C TE Methods Performances on GLUE Tasks",
            "text": ""
        }
    ],
    "title": "How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey",
    "year": 2023
}