{
    "abstractText": "Paraphrase Identification (PI), a task of determining whether a pair of sentences express the same meaning, is widely applied in Information Retrieval and Question Answering. Data Augmentation (DA) is proven effective in tackling PI task. However, the majority of DA methods still suffer from two limitations: inefficiency and poor quality. In this study, we propose the Generative Boosting Training (GBT) approach for PI. GBT designs a boosting learning method for single model based on the human learning process, utilizing seq2seq model to perform DA on misclassified instances periodically. We conduct experiments on the benchmark corpora QQP and LCQMC, towards both English and Chinese PI tasks. Experimental results show that our method yields significant improvements on a variety of Pre-trained Language Model (PLM) based baselines with good efficiency and effectiveness. It is noteworthy that a single BERT model (with a linear classifier) can outperform the state-of-the-art PI models with the boosting of GBT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rui Peng"
        },
        {
            "affiliations": [],
            "name": "Zhiling Jin"
        },
        {
            "affiliations": [],
            "name": "Yu Hong"
        }
    ],
    "id": "SP:3eae0d345f1d5ae97e3bc7eb763e2ff50c3c8194",
    "references": [
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th SemEval, pages 1\u201314.",
            "year": 2017
        },
        {
            "authors": [
                "Qian Chen",
                "Xiaodan Zhu",
                "Zhen-Hua Ling",
                "Si Wei",
                "Hui Jiang",
                "Diana Inkpen."
            ],
            "title": "Enhanced LSTM for natural language inference",
            "venue": "Proceedings of the 55th ACL, pages 1657\u20131668.",
            "year": 2017
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for chinese bert",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504\u20133514.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL 2019, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Zhendong Dong",
                "Qiang Dong."
            ],
            "title": "Hownet-a hybrid language and knowledge resource",
            "venue": "NLPKE, 2003. Proceedings. 2003, pages 820\u2013824. IEEE.",
            "year": 2003
        },
        {
            "authors": [
                "Siddhant Garg",
                "Goutham Ramakrishnan."
            ],
            "title": "Bae: Bert-based adversarial examples for text classification",
            "venue": "arXiv preprint arXiv:2004.01970.",
            "year": 2020
        },
        {
            "authors": [
                "Yichen Gong",
                "Heng Luo",
                "Jian Zhang."
            ],
            "title": "Natural language inference over interaction space",
            "venue": "arXiv preprint arXiv:1709.04348.",
            "year": 2017
        },
        {
            "authors": [
                "Hua He",
                "Kevin Gimpel",
                "Jimmy Lin."
            ],
            "title": "Multiperspective sentence similarity modeling with convolutional neural networks",
            "venue": "Proceedings of EMNLP 2015, pages 1576\u20131586.",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of CVPR 2016, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Tong He",
                "Weilin Huang",
                "Yu Qiao",
                "Jian Yao."
            ],
            "title": "Text-attentional convolutional neural network for scene text detection",
            "venue": "IEEE TIP, 25(6):2529\u20132541.",
            "year": 2016
        },
        {
            "authors": [
                "Yutai Hou",
                "Yijia Liu",
                "Wanxiang Che",
                "Ting Liu."
            ],
            "title": "Sequence-to-sequence data augmentation for dialogue language understanding",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 1234\u20131245, Santa Fe, New",
            "year": 2018
        },
        {
            "authors": [
                "Baotian Hu",
                "Zhengdong Lu",
                "Hang Li",
                "Qingcai Chen."
            ],
            "title": "Convolutional neural network architectures for matching natural language sentences",
            "venue": "NIPS, 27.",
            "year": 2014
        },
        {
            "authors": [
                "Shankar Iyer",
                "Nikhil Dandekar",
                "Kornel Csernai"
            ],
            "title": "First quora dataset release: Question pairs",
            "year": 2017
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
            "year": 2020
        },
        {
            "authors": [
                "Seonhoon Kim",
                "Inho Kang",
                "Nojun Kwak."
            ],
            "title": "Semantic sentence matching with densely-connected recurrent and co-attentive information",
            "venue": "Proceedings of AAAI, volume 33, pages 6586\u20136593.",
            "year": 2019
        },
        {
            "authors": [
                "Varun Kumar",
                "Ashutosh Choudhary",
                "Eunah Cho."
            ],
            "title": "Data augmentation using pre-trained transformer models",
            "venue": "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18\u201326, Suzhou, China. Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Yuxuan Lai",
                "Yansong Feng",
                "Xiaohan Yu",
                "Zheng Wang",
                "Kun Xu",
                "Dongyan Zhao."
            ],
            "title": "Lattice cnns for matching based chinese question answering",
            "venue": "Proceedings of AAAI, volume 33, pages 6634\u20136641.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Bohan Li",
                "Yutai Hou",
                "Wanxiang Che."
            ],
            "title": "Data augmentation approaches in natural language processing: A survey",
            "venue": "AI Open.",
            "year": 2022
        },
        {
            "authors": [
                "Jinfeng Li",
                "Shouling Ji",
                "Tianyu Du",
                "Bo Li",
                "Ting Wang."
            ],
            "title": "Textbugger: Generating adversarial text against real-world applications",
            "venue": "arXiv preprint arXiv:1812.05271.",
            "year": 2018
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Xin Liu",
                "Qingcai Chen",
                "Chong Deng",
                "Huajun Zeng",
                "Jing Chen",
                "Dongfang Li",
                "Buzhou Tang."
            ],
            "title": "Lcqmc: A large-scale chinese question matching corpus",
            "venue": "Proceedings of the 27th ACL, pages 1952\u20131962.",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Boer Lyu",
                "Lu Chen",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "Let: Linguistic knowledge enhanced graph transformer for chinese short text matching",
            "venue": "Proceedings of the AAAI, volume 35, pages 13498\u201313506.",
            "year": 2021
        },
        {
            "authors": [
                "George A Miller."
            ],
            "title": "Wordnet: a lexical database for english",
            "venue": "Communications of the ACM, 38(11):39\u201341.",
            "year": 1995
        },
        {
            "authors": [
                "John Morris",
                "Eli Lifland",
                "Jack Lanchantin",
                "Yangfeng Ji",
                "Yanjun Qi."
            ],
            "title": "Reevaluating adversarial examples in natural language",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3829\u20133839, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "John Morris",
                "Eli Lifland",
                "Jin Yong Yoo",
                "Jake Grigsby",
                "Di Jin",
                "Yanjun Qi."
            ],
            "title": "TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Mueller",
                "Aditya Thyagarajan."
            ],
            "title": "Siamese recurrent architectures for learning sentence similarity",
            "venue": "Proceedings of AAAI, volume 30.",
            "year": 2016
        },
        {
            "authors": [
                "Liang Pang",
                "Yanyan Lan",
                "Xueqi Cheng."
            ],
            "title": "Match-ignition: Plugging pagerank into transformer for long-form text matching",
            "venue": "Proceedings of the 30th CIKM, pages 1396\u20131405.",
            "year": 2021
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Andreas R\u00fcckl\u00e9",
                "Jonas Pfeiffer",
                "Iryna Gurevych."
            ],
            "title": "Multicqa: Zero-shot transfer of self-supervised text matching models on a massive scale",
            "venue": "arXiv preprint arXiv:2010.00980.",
            "year": 2020
        },
        {
            "authors": [
                "Zhiguo Wang",
                "Wael Hamza",
                "Radu Florian."
            ],
            "title": "Bilateral multi-perspective matching for natural language sentences",
            "venue": "arXiv preprint arXiv:1702.03814.",
            "year": 2017
        },
        {
            "authors": [
                "Zhiguo Wang",
                "Haitao Mi",
                "Abraham Ittycheriah."
            ],
            "title": "Sentence similarity learning by lexical decomposition and composition",
            "venue": "arXiv preprint arXiv:1602.07019.",
            "year": 2016
        },
        {
            "authors": [
                "Chen Xu",
                "Jun Xu",
                "Zhenhua Dong",
                "Ji-Rong Wen."
            ],
            "title": "Semantic sentence matching via interacting syntax graphs",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 938\u2013949.",
            "year": 2022
        },
        {
            "authors": [
                "Kun Zhang",
                "Enhong Chen",
                "Qi Liu",
                "Chuanren Liu",
                "Guangyi Lv."
            ],
            "title": "A context-enriched neural network method for recognizing lexical entailment",
            "venue": "Proceedings of AAAI, volume 31.",
            "year": 2017
        },
        {
            "authors": [
                "Kun Zhang",
                "Le Wu",
                "Guangyi Lv",
                "Meng Wang",
                "Enhong Chen",
                "Shulan Ruan."
            ],
            "title": "Making the relation matters: Relation of relation learning network for sentence semantic matching",
            "venue": "Proceedings of AAAI, volume 35, pages 14411\u201314419.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "PI can be boiled down to a binary classification task, aiming to determine whether pair of sentences convey the same or similar meaning. It is a fundamental natural language understanding task with non-trivial challenges, serving as a practical technique in the field of information retrieval and question answering (Hu et al., 2014; Cer et al., 2017; R\u00fcckl\u00e9 et al., 2020; Pang et al., 2021).\nA variety of neural PI approaches have been proposed (Chen et al., 2017; Wang et al., 2017; Zhang et al., 2017; Gong et al., 2017; Lai et al., 2019; Kim et al., 2019). Recently, the large PLMbased Models are leveraged as crucial supportive encoders for neural PI (Devlin et al., 2019; Liu et al., 2019; Cui et al., 2021; Zhang et al., 2021;\n\u2217Corresponding author.\nLyu et al., 2021; Xu et al., 2022). The state-ofthe-art methods achieve significant improvements, increasing PI performance up to the accuracy of 91.6% and 88.3% on the benchmark corpora QQP (Iyer et al., 2017) and LCQMC (Liu et al., 2018).\nNo matter whether in the English or Chinese scenario, the advanced PI models still suffer from two types of cases, as shown in Table 1. The first ones are classified as \u201cisomorphic but heterogeneous\u201d (denoted as IBH0), which is referred to the pair of sentences holding similar syntactic structure but with different semantics (label \u201c0\u201d). Cases of this type tend to be literally consistent (smaller edit distance), easily confusing models to identify such pairs as \"Paraphrase\". The second ones are classified as \u201cisomerous but homogeneous\u201d (denoted as IBH1). These cases are opposite of IBH0, using different expressions to convey the same meaning (label \u201c1\u201d), which makes the model mispredict them as \"Non-paraphrase\" due to the huge literal differences. We conduct a pilot experiment on the QQP validation set. Among the incorrect predictions made by the fine-tuned BERT model, 35.9% of these cases correspond to IBH0, while 14.7% correspond to IBH11. Errors in these two types occupy 50.7% of all the errors made by the baseline model. It highlights the ineffectiveness of the\n1We use a Levenshtein similarity with a threshold of 0.6 to identify literal similarity (sentences with a similarity greater than 0.6 are considered literally similar).\ncurrent PI model in handling these cases. Advanced DA methods such as Adversarial Training have been proven to be effective in solving such cases (Hou et al., 2018; Kumar et al., 2020; Morris et al., 2020a; Li et al., 2022). However, advanced adversarial methods suffer from two limitations. The first one is inefficiency. The majority of these methods are based on greedy-based synonym replacement (Li et al., 2018; Ren et al., 2019; Jin et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020). They first need to use external knowledge bases (e.g., WordNet) to obtain lists of synonyms. And after that, a greedy-based search strategy is used to choose suitable words, which brings serious time costs. The second one is poor quality. Previous work (Morris et al., 2020a) has proven most of them may result in generating unnatural and unreadable sentences, or even huge semantic shifts from the original ones.\nWith the continuous improvement of PLMs, generative models, such as BART (Lewis et al., 2020), are capable of generating sentences that are in line with human expression habits with acceptable time costs. Despite the fact that using seq2seq as a DA method often requires additional data or special data conditions, PI data has a naturally generative feature with source and target sentences, making seq2seq a great DA method.\nWe suggest that the existing PI models can be further improved in a simple and effective way by strengthening learning on misclassified instances. Inspired by the human learning process, we believe that, just as humans learn knowledge from books, the model aims to learn previously unknown knowledge from training data. For the knowledge that is difficult to understand, we need to improve our understanding and cognition through repeated and extended learning, and so are the models.\nTherefore, in this study, we propose the Generative Boosting Training (GBT) method for single model, which utilizes seq2seq model as the DA method serving for our boosting learning algorithm. During the training process, GBT performs DA on the instances that the model did wrong in a period of training steps, and enforces the model to be retrained on these instances. In this way, GBT can enable the model to correct its errors and enhance its ability to solve hard cases in a timely manner, just like human learning.\nExperiments on the benchmarks of English QQP and Chinese LCQMC show that GBT improves var-\nious PLM models. It is noteworthy that a baseline model (with only a PLM encoder and a linear classifier) can outperform the advanced models with the boosting of GBT. Our contributions can be summarized as follow:\n\u2022 The proposed GBT brings stable and significant improvements over the current PLMsbased PI models.\n\u2022 GBT utilizes the seq2seq model as the DA method, greatly improving the efficiency and quality of example generation.\n\u2022 GBT is vest-pocket, which extends the training time by no more than 1 hour on an RTX3090 24GB GPU."
        },
        {
            "heading": "2 Related Work",
            "text": "PI has been widely studied due to its widespread application in downstream tasks. The existing approaches can be divided into representation-based and interaction-based approaches.\nThe representation-based approaches use a siamese architecture to encode two sentences into independent vectors of high-level features. On this basis, the semantic similarity is obtained by both feature vectors. He et al. (2015) propose a siamese CNN structure, and they verify the effectiveness of cosine similarity, Euclidean distance and elementwise difference. Wang et al. (2016) decompose two sentences to the sets of similar and different tokens to extract similar and distinguishable features by CNN. Lai et al. (2019) focus on the ambiguity caused by Chinese word segmentation and propose a lattice-based CNN model (LCNs). Lyu et al. (2021)\u2019s LET-BERT perceives multi-granularity interaction based on word lattice graph, where graph networks and external knowledge HowNet (Dong and Dong, 2003) are used, achieving state-of-theart performance in Chinese PI.\nAn interaction-based PI model is designed to perceive interaction features between sentences, and encode such information into the representations. Specifically, Wang et al. (2017) propose a BiMPM model under the matching-aggregation framework, which conducts one-to-many token-level matching operations to obtain interaction information. ESIM model proposed by Chen et al. (2017) performs interactive inference based on LSTM representations. ESIM sharpens the interaction information by means of element-wise dot product and subtraction. Inspired by ResNet (He et al., 2016a), Kim\net al. (2019) propose a densely-connected recurrent and co-attentive Network (DRCN), which combines residual operation with RNN and attention mechanism. Zhang et al. (2021) propose a Relation Learning Network (R2-Net) based on BERT, which is characterized by interactive relationship perception of multi-granularity linguistic units. The recently proposed ISG-BERT (Xu et al., 2022) integrates syntactic alignments and semantic matching signals of two sentences into an association graph to gain a fine granularity matching process.\nDA techniques such as Adversarial Training have received keen attention in solving semantic matching tasks. Ren et al. (2019) propose a Probability Weighted Word Saliency (PWWS), determining word substitution order by the word saliency and weighted by the classification probability, where WordNet (Miller, 1995) is used for synonym replacement. Similar to PWWS, Li et al. (2018) propose TextBugger, which finds word substitution order by Jacobian matrix and considers both character and token-level modification. The BERT-Attack proposed by Li et al. (2020) also applies a greedy word saliency strategy to find vulnerable words. It uses BERT-MLM for word generation instead of synonyms.\nWith the advancement of auto-regressive PLM, the seq2seq model is also used as a DA method in various scenarios (Hou et al., 2018; Kumar et al., 2020; Li et al., 2022). However, seq2seq is not generalized as a DA method such as synonym replacement, because it requires the data to have a generative condition, which is relatively strict. In this paper, we explore a generative augmentation method suitable for PI."
        },
        {
            "heading": "3 Approach",
            "text": "Let us first define the PI task in a formal way. Given a pair of sentences X1 = {x11, x12...x1n} and X2 = {x21, x22...x2n} , a PI model aims to determine whether X1 and X2 are paraphrases of each other. During training, each data example can be noted as (X1, X2, y), where y stands for a binary label indicating the relationship between X1 and X2: either \"Paraphrase\" (P) or \"Non-paraphrase\" (N). The goal of training is to optimize the PI model f(X1, X2) by minimizing the prediction loss.\nWe suggest that the learning process of the model closely parallels that of humans. The goal is to learn previously unknown knowledge from one instance, reinforce impressions, and use the\nlearned knowledge to infer other instances. In addition, models need to timely correct mistakes during the learning process.\nIn light of this, we devise a simple but effective training method GBT to mimic the human learning process. First, we leverage the inherent properties of PI data to transfer it into seq2seq training data, enabling the training of two independent generators. During training, both generators need to perform DA on the cases where the model made mistakes. Subsequently, we re-train the model on the augmented data at a certain interval."
        },
        {
            "heading": "3.1 Baseline Model",
            "text": "As introduced by Devlin et al. (2019), we adopt the model framework of a PLM-based encoder with a two-layer classifier as our baseline model. The model is designed to predict the binary relationship of X1 and X2. The input sequence is organized as X ={[CLS], X1, [SEP ], X2, [SEP ]} for BERTbased models and X ={<s>, X1, <\\s>, X2, <\\s>} for RoBERTa-based models, where [CLS], [SEP ], <s> and <\\s> serve as special tokens. Further, we input X into the PLM encoder to produce its contextual representations of each token. The [CLS] or <s> token is fed into linear classifier to estimate the probabilities p(y\u0302|X1, X2). The optimization objective is to minimize the cross-entropy loss:\nLPI = \u2212(y \u2217 log(p(y\u0302|X1, X2)) +(1\u2212 y) \u2217 log(1\u2212 p(y\u0302|X1, X2))).\n(1)"
        },
        {
            "heading": "3.2 Boosting Generator",
            "text": "For DA, we train two seq2seq models GP and GN as our generators. Given an input sentence, the GP aims to produce a paraphrased sentence that is literally different (IBH1). While GN is to generate a non-paraphrased sentence that is literally similar to the input sentence (IBH0).\nBART (Lewis et al., 2020): BART is a classical seq2seq model using the standard Transformer structure including a bidirectional encoder and an autoregressive decoder. It is pre-trained by corrupting text with an arbitrary noising function, and learning to reconstruct the original text. We use BART as the backbone model for training both generators GP and GN .\nData Preparation: We introduce the training data for two generators GP and GN . First, we split the original PI training set into Paraphrase (P ) and Non-Paraphrase (N ) by the given binary label. We take X1 as the source sentence and X2 as the target sentence no matter on P or N .\nFor data P , we aim to select paraphrased sentence pairs that X1 and X2 are literally different to train Gp. In this way, Gp is able to learn how to generate paraphrased sentences with different syntactic structures from the source sentence. To be specific, we calculate the BLEU2 score for each pair of X1 and X2 in data P . Then, we randomly select 50k examples from the top 50% of pairs with the lowest BLEU scores (a lower BLEU score indicates a greater dissimilarity). These selected examples are used as the training data for GP .\nWhile for data N , we select non-paraphrased sentence pairs that X1 and X2 are literally similar, which enables GN to produce target sentences with similar literal content but different semantics. For this, we also calculate the BLEU score of data N and randomly select 50k examples from the top 50% of pairs with the highest BLEU score as training data for GN .\nTraining Process: We train the generators GP and GN by fine-tuning the backbone model BART. Specifically, given training data (X1, X2), the source sentence X1 is fed into the encoder, and the decoder is required to generate tokens in target sentence X2 auto-regressively. The G\u03b8 is trained to maximize the log-likelihood:\nLGen(\u03b8) = T\u2211 t=1 log(p\u03b8(x 2 t |x21:t\u22121, X1)) (2)\nwhere x2t represents current generating word in X2, and x21:t\u22121 represents previous word in the ground-truth rather than word x\u030221:t\u22121 generated by the model. This training technique is known as teacher forcing.\n2Our choice of BLEU, instead of other evaluation metrics, is motivated by our specific aim to measure literal similarity. In this context, BLEU adequately satisfies our purpose and provides computational efficiency.\nThe generator is evaluated by the BLEU metric. The final BLEU value for fine-tuned Chinese generator is 51.5 (GP ) and 48.8 (GN ), while that for the English generator is 29.6 (GP ) and 27.7 (GN ). After training, the parameters of GP and GN are fixed for subsequent use, noted as Boosting Generator."
        },
        {
            "heading": "3.3 Boosting Training",
            "text": "Classical boosting learning methods perform ensemble learning. During training time, the integrated model serially adds new models to itself to strengthen the cases where the previous integrated model did wrong. In this way, the integrated model is able to enhance the ability to handle hard cases.\nDifferent from the classical boosting method, GBT repeatedly enhances the ability of a single model to solve hard cases without the massive cost of multiple models. In a round of learning, GBT records wrong instances during fixed t training steps (denoted as boosting interval t). The boosting interval t controls how often the model will reinforce learning on the wrong instances. In order to be corrected in time, the interval t should not be too large. Because the model will have a large deviation from the previous model that did wrong at that time due to parameter updating, the effect of re-learning wrong instances will also decrease. After every t learning steps, the model will enter boosting training mode from the normal training.\nIn boosting training mode, wrong cases will be augmented by GP and GN as shown in Figure 1. Note that different from Figure 1, we perform DA not only on X1 but also X2. Given a wrong instance (X1, X2, y), we will generate four extra cases according to it. First, GP produces paraphrase sentences of X1 and X2 respectively, and we note them as XP1 and X P 2 . GN is also used to generate non-paraphrase sentences of X1 and X2, denoting as XN1 and X N 2 . In this way, we are able to produce two IBH1 cases (X1, XP1 , 1) and (X2, XP2 , 1), also two IBH0 cases (X1, X N 1 , 0) and (X2, XN2 , 0) according to one wrong instance. The GBT learning process is shown in Algorithm 1. Note that we randomly select p% cases for boosting training (L9 in Algorithm 1). This is because the examples we generate are relatively difficult, and the proportion of difficult examples should not be too high. Besides, we did not start the first boosting training until the warm-up steps ends (L8 in Algorithm 1). We will discuss the boosting interval t, boosting ratio p, and the starting timing\nAlgorithm 1: Boosting Training Input: original example X = [X1;X2],\nlabel y, boosting generator GP (\u00b7), GN (\u00b7), boosting interval t, boosting ratio p, target model f(\u00b7), warm-up steps ws, training function train(\u00b7)\nOutput: None 1 Define train_steps ts, W of wrong cases set; 2 Begin training; 3 For X in TrainData Do: 4 ts += 1 5 train(f , X) 6 If f(X)! = y Do: 7 W.add(X) 8 If ts%t == 0 and ts >= ws Do: 9 W \u2013> randomly save p% cases\n10 Boosting_Train(f , W ) 11 W.clear() 12 Boosting_Train(f , W ): 13 For X in W Do: 14 GP (X1) \u2192 XP1 GP (X2) \u2192 XP2 15 GN (X1) \u2192 XN1 GN (X2) \u2192 XN2 16 W .add((X1, XP1 , 1), (X2, X P 2 , 1)) 17 W .add((X1, XN1 , 0), (X2, X N 2 , 0)) 18 train(f , W )\nin Section 4.5. The model is required to be re-trained on the wrong instances and their augmented ones to enhance performances. After that, the model will return to normal training steps to continue a new round of learning."
        },
        {
            "heading": "4 Experimentation",
            "text": ""
        },
        {
            "heading": "4.1 Corpora and Evaluation Metrics",
            "text": "We evaluate our PI models on two benchmark corpora, including Chinese LCQMC (Liu et al., 2018), as well as English QQP (Iyer et al., 2017). Both QQP and LCQMC are large-scale corpora for opendomain, where sentence pairs are collected from QA websites without rigorous selection. Each instance in both corpora is specified as a pair of sentences. The binary labels of \u201cPos\u201d (i.e., paraphrase) and \u201cNeg\u201d (i.e., non-paraphrase) are provided. Table 2 shows the statistical information of the corpora. The canonical splitting method of training, validation and test sets for the corpora is plainly stated, and we strictly adhere to it. We evaluate all the models in the experiments using accuracy (ACC.) and F1-score."
        },
        {
            "heading": "4.2 Hyperparameter Settings",
            "text": "Our experiments are conducted on HuggingFace\u2019s Transformers Library. We adopt an Adam optimizer with epsilon of 1e-8 and warm-up steps ratio of 10% for all tasks. For the generative model, we first fine-tune the BARTlarge models (the Chinese version is provided by fnlp3) on both benchmarks (Section 3.2). The batch size and learning rate are set to 16 and 2e-5. We set the epoch to 3 and the max sequence length is set to 100.\nFor Chinese PI tasks, we set the learning rate to 2e-5 and batch size to 32 for LCQMC. While for the English task, the learning rate and batch size are set to 2.5e-5 and 64. The fine-tuning epoch is set to 5 for LCQMC. While for QQP, we fine-tune the models for 50k steps, and checkpoints are evaluated every 2k steps. We set the boosting interval t = 500 steps and boosting ratio p = 25(%). All experiments are conducted on a single RTX 3090."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Table 3 and Table 4 show the main results on LCQMC and QQP. The reported performance of our models are average scores obtained in five runs with random seeds.\nLCQMC: The Chinese PI models are listed in Table 3. Our proposed GBT yields significant improvements (p-value < 0.05 in statistical significance test) over baselines, which demonstrates that GBT generalizes well when cooperating with different PLMs. The most substantial boosting is up to 3.5% (Acc.) and 2.5% (F1), achieving state-ofthe-art performance.\nIn addition, our models outperform all state-ofthe-art models, including the recently-proposed LET-BERT which addresses issues of complex Chinese word segmentation and ambiguity to some extent. It uses HowNet as the external knowledge base to provide explanations. While for GBT, it is noteworthy that we use no extra knowledge. GBT takes advantage of the existing data to train the Boosting Generator, and only costs some additional time for DA and boosting training during the training process. In this way, a baseline model without\n3https://nlp.fudan.edu.cn\n\u201c\u2662\u201d denotes the PLM-based baselines, while \u201c\u2663\u201d denotes GBT boosted models that obtain significant improvements (p-value < 0.05 in statistical significance test) over baselines.\nany enhancing component network can outperform the existing strongest model.\nQuora: Table 4 shows the performance of English PI on QQP. The previous work didn\u2019t report F1-scores on QQP. We evaluate our models with the F1 metric and report the performance to support future comparative studies. Similarly, GBT boosts baseline models stably on different configurations and PLMs, achieving state-of-the-art performance.\nThe previous approach R2-Net is strongest among the advanced models. Its structure contains both CNN and PLM-based encoder. Besides, it is learned by utilizing the label information of instances, and additionally trained by congeniality recognition among multiple instances. Our approach achieves comparable performance to R2Net when BERTbase is used as the backbone, but with no sophisticated structure and learning process. The recently proposed ISG model is another powerful model, using a graph model to integrate syntactic alignments and semantic matching signals. Our model outperforms ISG when RoBERTalarge is used with no extra parameters and structure. In the English scenario, GBT also brings strong improvements over the baseline models to achieve state-of-the-art performance.\nWe can observe that improvements in Chinese PI are more than in English. This is partly due to the effect of generator models under different language scenarios. The generation length in Chinese is\nshorter and the difficulty is lower, which makes the generation effect better. Therefore, the boosting effect for Chinese PI is better than that of English."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "We conduct ablation experiments over the baseline model using BERT as described in Section 3.1. Three expanded models are considered in this experiment. First, \"+Only Boosting Generator\" uses Boosting Generator to generate augmented cases on all training data no matter whether the model did it right or wrong, which makes the training data four times larger than before. Second, \"+ Only Boosting Train\" applies the Boosting Training algorithm in Algorithm 1 but only re-train on the original wrong cases with no Boosting Generator for DA. Last, the GBT combines both aforementioned approaches to enforce the GBT approach.\nTable 5 shows the performance obtained on test sets. It is found that the performance has sharply dropped with Boosting Generator performing on the full amount of data. This is due to the unbalanced proportion of difficult instances and redundant samples. While only applying Boosting Training simply on the original wrong cases brings substantial improvements. This demonstrates that despite not utilizing Boosting Generator for DA, our boosting training algorithm is an effective means of improving performance.\nLast, we prove that GBT combines Boosting\nTraining with Boosting Generator to achieve further improvements. Boosting Training algorithm finds suitable instances and timing for Boosting Generator to make DA, while the latter brings stronger improvements."
        },
        {
            "heading": "4.5 Boosting Timing and Ratio",
            "text": "As mentioned in Algorithm 1, we use boosting interval t to control the boosting timing, and boosting ratio p% to control how many instances should be augmented for difficulty balance. Figure 2 shows how the interval t and ratio p affect model learning.\nWe conduct experiments when the boosting interval t is in {100, 250, 500, 1000, 1ep}, where 1ep means we perform the boosting learning at the end of each epoch. As expected, the smaller the interval t brings better performance, because the model can correct mistakes and learn expanded knowledge (augmented examples) in a timely manner. However, frequent switching of training modes will lead to a certain amount of extra cost. Therefore, we believe t = 250 is a good choice.\nMoreover, we also test the boosting ratio p on both benchmarks where p takes values from {10, 25, 50, 75, 100} (%). We set boosting ratio p = 25% as a reasonable value. It can be observed that if p is too small, the enhanced effect will not be obvious. While if p is too large, it will lead to an imbalance in the proportion of difficult examples, eventually affecting the performances.\nJust like the human learning process, it is better to learn difficult knowledge after mastering a certain foundation instead of learning difficulties at the beginning. Therefore, we test three timings on when we should start first boosting training, respectively at 1) the beginning of the whole training, 2) after the first epoch (start at the second epoch) 3) after the end of the warm-up steps. As expected, start timing at 1) performs worst and 3) works best. That is why we do not start boosting training until the warm-up steps end (L8 in Algorithm 1)."
        },
        {
            "heading": "4.6 Comparison with advanced DA methods",
            "text": "Compared with advanced DA methods, GBT has the advantage of generating high-quality instances with low time cost because we use the seq2seq model (Boosting Generator). We replace the Boosting Generator with advanced DA methods to analyze their efficiency and performances as shown in Table 6. All adversarial methods we used as baselines are open-source and readily applicable to the PI task. We implement these methods using TextAttack4 (Morris et al., 2020b). We conduct experiments on the QQP dataset, because under the Chinese scenario, there are few DA-related researches that we can compare with. We compare our models to a series of strong arts, including:\n\u2022 PWWS (Ren et al., 2019): Based on the synonym replacement strategy, PWWS proposes a probability-weighted word salience approach to determine word replacement order. The synonyms are provided by WordNet.\n\u2022 TextBugger (Li et al., 2018): TextBugger finds important words by Jacobian matrix and greedy search. In order to ensure that the generated examples are literally and semantically consistent with the original ones, character and word-level modifications are considered.\n\u2022 BERT-Attack (BERTA) (Li et al., 2020): BERTA identifies vulnerable words by lossbased scores, which is to mask tokens one by one to see the loss changes. The BERT MLM is used to generate top-K candidates for each keyword, and another greedy algorithm is applied to confirm the final option. Note that\n4https://github.com/QData/TextAttack\ndefault K is set as 48, but the time cost is way too much and we regulate K as 8.\nAs mentioned, most previous DA methods are based on greedy search for word replacements, and there are also a few methods using MLM for word generation. However, most methods have been criticized for their efficiency or generation quality shortcomings (Morris et al., 2020a). With the progress of the generative model, we believe that the generative model is far better than rule-based methods under certain conditions, and it can perform well in sentence diversity and fluency.\nAs shown in Table 6, GBT has a much higher generation efficiency in Time cost. Our GBT with seq2seq generator is several times more efficient than advanced methods, proving that the generative model is able to achieve the expected effect. The generation quality can be evaluated through the comparison in performance. The model training on the instances generated by our Boosting Generator brings the strongest improvements. Meanwhile, GBT with our Boosting Generator produces fewer boosting instances, proving that our model has relatively fewer wrong instances during training.\nIn addition, it can also be found that despite replacing different DA methods, the model still gains substantial improvements, demonstrating the generalization performance of our Boosting Training algorithm. Moreover, when the DA method is replaced, our GBT is not limited to the PI task but can be generalized to any task."
        },
        {
            "heading": "4.7 Case study",
            "text": "GBT provides a time-efficient DA method as described in Table 6. Compared with other DA methods, the performance improvement of GBT proves the excellent generation quality to a certain extent. We show an English case in QQP generated with different DA methods in Table 7.\nAs shown, four examples are produced by using GP and GN to generate from X1 and X2 respec-\ntively (as described in Section 3.3). Our generated sentence is marked in blue. While the modifications of other DA methods are marked in red.\nPWWS and TextBugger replace the word \u201cremove\u201d with \u201cslay\u201d and \u201celiminate\u201d respectively. Although the generated sentence is semantically similar to the original sentence, it is unnatural and does not conform to normal pragmatic habits. BERTA replaces \u201cremove\u201d in both sentences with \u201creplace\u201d and \u201cerase\u201d, resulting in semantic shifts from the original pair. Moreover, all three methods generate \u201cParaphrase\u201d sentences that are literally similar to the original ones, which makes the model easy to recognize.\nIt can be observed that our generative approach can not only generate \u201cNon-Paraphrase\u201d examples but also produce more natural and diverse results. When generating \u201cParaphrase\u201d, it recognizes the semantics between \u201cHow\u201d and \u201cWhat is the best way\u201d are similar, changing the syntactic structure while preserving the same meaning (IBH1). While for \u201cNon-paraphrase\u201d, it switches How-type questions to Why-type questions, as to preserve similar literal structure while changing the semantics (IBH0). In this way, our GBT is able to bring better effectiveness during the training process."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this study, we propose a Generative Boosting Training (GBT) approach for PI. GBT combines the generative DA method with our boosting learning algorithm, which enables the model to correct mistakes and perform expanded learning in a timely manner during the learning process. GBT offers several advantages: high-quality and efficient DA for PI, simple and effective boosting learning algorithm, and state-of-the-art matching performance. Experimental results on both English and Chinese benchmarks verified the effectiveness and efficiency of GBT. In the future, we will explore more generalized generative DA methods to broaden the application of GBT to various tasks.\nLimitations\nGBT leverages the seq2seq model as a high-quality and efficient DA method. However, the seq2seq method requires data with the generative property (such as data for PI), which limits the generalization of our DA method to other tasks. In contrast, our boosting learning algorithm is a method that can be generalized to other tasks. Given the remarkable ability of Large Language Models (LLMs) to adhere to instructions, we can use LLMs to generate data tailored to the specific task by providing well-designed prompts. In this way, our GBT can achieve generalization to other tasks.\nFurthermore, GBT may not handle noisy examples with incorrect labels well, as it tends to repeatedly emphasize the influence of such examples during the training stage. This issue of mislabeling can be mitigated through data-cleaning approaches."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank all anonymous reviewers for their insightful comments. This work is supported by National Key R&D Program of China (2020YFB1313601) and National Science Foundation of China (62376182, 62076174)."
        }
    ],
    "title": "GBT: Generative Boosting Training Approach for Paraphrase Identification",
    "year": 2023
}