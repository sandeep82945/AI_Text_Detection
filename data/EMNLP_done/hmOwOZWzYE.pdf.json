{
    "abstractText": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joshua Ainslie"
        },
        {
            "affiliations": [],
            "name": "James Lee-Thorp"
        },
        {
            "affiliations": [],
            "name": "Michiel de Jong"
        },
        {
            "affiliations": [],
            "name": "Yury Zemlyanskiy"
        },
        {
            "affiliations": [],
            "name": "Federico Lebr\u00f3n"
        },
        {
            "affiliations": [],
            "name": "Sumit Sanghai"
        }
    ],
    "id": "SP:b3cc3a39651858f88981990303314b542b4a0078",
    "references": [
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations",
            "year": 2018
        },
        {
            "authors": [
                "Charlie Chen",
                "Sebastian Borgeaud",
                "Geoffrey Irving",
                "Jean-Baptiste Lespiau",
                "Laurent Sifre",
                "John Jumper."
            ],
            "title": "Accelerating large language model decoding with speculative sampling",
            "venue": "CoRR, abs/2302.01318.",
            "year": 2023
        },
        {
            "authors": [
                "nan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "Proceedings of the 2018 Conference of",
            "year": 2018
        },
        {
            "authors": [
                "Tri Dao",
                "Daniel Y. Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9."
            ],
            "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
            "venue": "CoRR, abs/2205.14135.",
            "year": 2022
        },
        {
            "authors": [
                "Michiel de Jong",
                "Yury Zemlyanskiy",
                "Joshua Ainslie",
                "Nicholas FitzGerald",
                "Sumit Sanghai",
                "Fei Sha",
                "William Cohen."
            ],
            "title": "FiDO: Fusion-in-decoder optimized for stronger performance and faster inference",
            "venue": "arXiv preprint arXiv:2212.08153.",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "Llm.int8(): 8-bit matrix multiplication for transformers at scale. CoRR, abs/2208.07339",
            "year": 2022
        },
        {
            "authors": [
                "Alexander R. Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir R. Radev."
            ],
            "title": "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of the 57th Conference of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh."
            ],
            "title": "GPTQ: accurate post-training quantization for generative pre-trained transformers",
            "venue": "CoRR, abs/2210.17323.",
            "year": 2022
        },
        {
            "authors": [
                "Google."
            ],
            "title": "Profile your model with cloud tpu tools",
            "venue": "https://cloud.google.com/tpu/docs/ cloud-tpu-tools. Accessed: 2022-11-11.",
            "year": 2020
        },
        {
            "authors": [
                "Jianping Gou",
                "Baosheng Yu",
                "Stephen J. Maybank",
                "Dacheng Tao."
            ],
            "title": "Knowledge distillation: A survey",
            "venue": "Int. J. Comput. Vis., 129(6):1789\u20131819.",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Heek",
                "Anselm Levskaya",
                "Avital Oliver",
                "Marvin Ritter",
                "Bertrand Rondepierre",
                "Andreas Steiner",
                "Marc van Zee"
            ],
            "title": "Flax: A neural network library and ecosystem for JAX",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "CoRR, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S. Weld",
                "Luke Zettlemoyer."
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Van-",
            "year": 2017
        },
        {
            "authors": [
                "Aran Komatsuzaki",
                "Joan Puigcerver",
                "James Lee-Thorp",
                "Carlos Riquelme Ruiz",
                "Basil Mustafa",
                "Joshua Ainslie",
                "Yi Tay",
                "Mostafa Dehghani",
                "Neil Houlsby"
            ],
            "title": "Sparse upcycling: Training mixtureof-experts from dense checkpoints",
            "year": 2022
        },
        {
            "authors": [
                "Yaniv Leviathan",
                "Matan Kalman",
                "Yossi Matias."
            ],
            "title": "Fast inference from transformers via speculative decoding",
            "venue": "CoRR, abs/2211.17192.",
            "year": 2022
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "C\u00edcero Nogueira dos Santos",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Bing Xiang."
            ],
            "title": "Abstractive text summarization using sequence-tosequence rnns and beyond",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural",
            "year": 2016
        },
        {
            "authors": [
                "Reiner Pope",
                "Sholto Douglas",
                "Aakanksha Chowdhery",
                "Jacob Devlin",
                "James Bradbury",
                "Anselm Levskaya",
                "Jonathan Heek",
                "Kefan Xiao",
                "Shivani Agrawal",
                "Jeff Dean."
            ],
            "title": "Efficiently scaling transformer inference",
            "venue": "arXiv preprint arXiv:2211.05102.",
            "year": 2022
        },
        {
            "authors": [
                "Markus Rabe."
            ],
            "title": "Memory-efficient attention",
            "venue": "https://github.com/google/flaxformer/ blob/main/flaxformer/components/ attention/memory_efficient_attention.py. Accessed: 2023-05-23.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Noam Shazeer."
            ],
            "title": "Fast transformer decoding: One write-head is all you need",
            "venue": "arXiv preprint arXiv:1911.02150.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "7th International Conference on Learning Representa-",
            "year": 2019
        },
        {
            "authors": [
                "Samuel Williams",
                "Andrew Waterman",
                "David A. Patterson."
            ],
            "title": "Roofline: an insightful visual performance model for multicore architectures",
            "venue": "Commun. ACM, 52(4):65\u201376.",
            "year": 2009
        },
        {
            "authors": [
                "Chenguang Zhu",
                "Yang Liu",
                "Jie Mei",
                "Michael Zeng."
            ],
            "title": "Mediasum: A large-scale media interview dataset for dialogue summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Autoregressive decoder inference is a severe bottleneck for Transformer models due to the memory bandwidth overhead from loading decoder weights and all attention keys and values at every decoding step (Shazeer, 2019; Pope et al., 2022; de Jong et al., 2022). The memory bandwidth from loading keys and values can be sharply reduced through multi-query attention (Shazeer, 2019), which uses multiple query heads but single key and value heads.\nHowever, multi-query attention (MQA) can lead to quality degradation and training instability, and it may not be feasible to train separate models optimized for quality and inference. Moreover, while some language models already use multiquery attention, such as PaLM (Chowdhery et al., 2022), many do not, including publicly available language models such as T5 (Raffel et al., 2020) and LLaMA (Touvron et al., 2023).\nThis work contains two contributions for faster inference with large language models. First, we\n\u2217Equal contribution. \u2020University of Southern California. Work done at Google\nResearch.\nshow that language model checkpoints with multihead attention (MHA) can be uptrained (Komatsuzaki et al., 2022) to use MQA with a small fraction of original training compute. This presents a cost-effective method to obtain fast multi-query as well as high-quality MHA checkpoints.\nSecond, we propose grouped-query attention (GQA), an interpolation between multi-head and multi-query attention with single key and value heads per subgroup of query heads. We show that uptrained GQA achieves quality close to multihead attention while being almost as fast as multiquery attention."
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 Uptraining",
            "text": "Generating a multi-query model from a multi-head model takes place in two steps: first, converting the checkpoint, and second, additional pre-training to allow the model to adapt to its new structure. Figure 1 shows the process for converting a multi-head checkpoint into a multi-query checkpoint. The projection matrices for key and value heads are mean pooled into single projection matrices, which we find works better than selecting a single key and value head or randomly initializing new key and value heads from scratch.\nThe converted checkpoint is then pre-trained for\na small proportion \u03b1 of its original training steps on the same pre-training recipe."
        },
        {
            "heading": "2.2 Grouped-query attention",
            "text": "Grouped-query attention divides query heads into G groups, each of which shares a single key head and value head. GQA-G refers to grouped-query with G groups. GQA-1, with a single group and therefore single key and value head, is equivalent to MQA, while GQA-H, with groups equal to number of heads, is equivalent to MHA. Figure 2 shows a comparison of grouped-query attention and multihead/multi-query attention. When converting a multi-head checkpoint to a GQA checkpoint, we construct each group key and value head by meanpooling all the original heads within that group.\nAn intermediate number of groups leads to an interpolated model that is higher quality than MQA but faster than MHA, and, as we will show, represents a favorable trade-off. Going from MHA to MQA reduces H key and value heads to a single key and value head, reducing the size of the key-value cache and therefore amount of data that needs to be loaded by a factor of H . However, larger models generally scale the number of heads, such that multi-query attention represents a more aggressive cut in both memory bandwidth and capacity. GQA lets us keep the same proportional decrease in bandwidth and capacity as model size increases.\nMoreover, larger models suffer relatively less from memory bandwidth overhead from attention, as the KV-cache scales with model dimension while model FLOPs and parameters scale with the square of model dimension. Finally, standard sharding for large models replicates the single key and value head by the number of model partitions (Pope\net al., 2022); GQA removes the waste from such partitioning. Therefore, we expect GQA to present a particularly good trade-off for larger models.\nWe note that GQA is not applied to the encoder self-attention layers; encoder representations are computed in parallel, and memory bandwidth is therefore generally not the primary bottleneck."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experimental setup",
            "text": "Configurations All models are based on the T5.1.1 architecture (Raffel et al., 2020), implemented with JAX (Bradbury et al., 2018), Flax (Heek et al., 2020), and Flaxformer1. For our main experiments we consider T5 Large and XXL with multi-head attention, as well as uptrained versions of T5 XXL with multi-query and grouped-query attention. We use the Adafactor optimizer with the same hyperparameters and learning rate schedule as T5 (Raffel et al., 2020). We apply MQA and GQA to decoder self-attention and cross-attention, but not encoder self-attention.\nUptraining Uptrained models are initialized from public T5.1.1 checkpoints. The key and value heads are mean-pooled to the appropriate MQA or GQA structure, and then pre-trained for a further \u03b1 proportion of original pre-training steps with the original pre-training setup and dataset from (Raffel et al., 2020). For \u03b1 = 0.05, training took approximately 600 TPUv3 chip-days.\nData We evaluate on summarization datasets CNN/Daily Mail (Nallapati et al., 2016), arXiv and PubMed (Cohan et al., 2018), MediaSum (Zhu et al., 2021), and Multi-News (Fabbri et al., 2019);\n1https://github.com/google/flaxformer\ntranslation dataset WMT 2014 English-to-German; and question answering dataset TriviaQA (Joshi et al., 2017). We do not evaluate on popular classification benchmarks such as GLUE (Wang et al., 2019) as autoregressive inference is less applicable for those tasks.\nFine-tuning For fine-tuning, we use a constant learning rate of 0.001, batch size 128, and dropout rate 0.1 for all tasks. CNN/Daily Mail and WMT use input length of 512 and output length 256. Other summarization datasets use input length 2048 and output length 512. Finally, TriviaQA uses input length 2048 and output length 32. We train until convergence and select the checkpoint with the highest dev performance. We use greedy decoding for inference.\nTiming We report time per sample per TPUv4 chip, as measured by xprof (Google, 2020). For timing experiments we use 8 TPUs with the largest batch size that fits up to 32 per TPU, and parallelization optimized separately for each model."
        },
        {
            "heading": "3.2 Main results",
            "text": "Figure 3 shows average performance over all datasets as a function of average inference time for MHA T5-Large and T5-XXL, and uptrained MQA and GQA-8 XXL models with uptraining proportion \u03b1 = 0.05. We see that a larger uptrained MQA model provides a favorable tradeoff relative to MHA models, with higher quality and faster inference than MHA-Large. Moreover, GQA achieves significant additional quality gains, achieving performance close to MHA-XXL with speed close to MQA. Table 1 contains full results for all datasets."
        },
        {
            "heading": "3.3 Ablations",
            "text": "This section presents experiments to investigate the effect of different modeling choices. We eval-\nuate performance on a representive subsample of tasks: CNN/Daily Mail, (short-form summarization), MultiNews (long-form summarization), and TriviaQA (question-answering).\nCheckpoint conversion Figure 4 compares the performance of different methods for checkpoint conversion. Mean pooling appears to work best, followed by selecting a single head and then random initialization. Intuitively, results are ordered by the degree to which information is preserved from the pre-trained model.\nUptraining steps Figure 5 shows how performance varies with uptraining proportion for T5 XXL with MQA and GQA. First, we note that GQA already achieves reasonable performance after conversion while MQA requires uptraining to\n54.4 54.6 54.8 55 55.2 55.4 55.6\nMean\nFirst\nRandom\nFigure 4: Performance comparison of different checkpoint conversion methods for T5-Large uptrained to MQA with proportion \u03b1 = 0.05. \u2018Mean\u2019 mean-pools key and value heads, \u2018First\u2019 selects the first head and \u2018Random\u2019 initializes heads from scratch.\nbe useful. Both MQA and GQA gain from 5% uptraining with diminishing returns from 10%.\nNumber of groups Figure 6 demonstrates the effect of the number of GQA groups on inference speed. For larger models the memory bandwidth overhead from the KV cache is less constraining (Shazeer, 2019), while the reduction in key-value size is sharper due to the increased number of heads. As a result, increasing the number of groups from MQA only results in modest slowdowns initially, with increasing cost as we move closer to MHA. We selected 8 groups as a favorable middle ground."
        },
        {
            "heading": "4 Related Work",
            "text": "This work is focused on achieving a better tradeoff between decoder quality and inference time through reducing the memory bandwidth overhead (Williams et al., 2009) from loading keys and values. Shazeer (2019) first proposed reducing this overhead through multi-query attention. Follow-up work showed that multi-query attention is espe-\ncially helpful for long inputs (Pope et al., 2022; de Jong et al., 2022). Rabe (2023) independently developed GQA with public implementation.\nA number of other methods have been proposed to reduce memory bandwidth overhead from keys and values, as well as parameters. Flash attention (Dao et al., 2022) structures the attention computation to avoid materializing the quadratic attention scores, reducing memory and speeding up training. Quantization (Dettmers et al., 2022; Frantar et al., 2022) reduces the size of weights and activations, including keys and values, by lowering precision. Model distillation (Hinton et al., 2015; Gou et al., 2021) instead reduces model size at a given precision, using data generated from the larger model to finetune the smaller model. Layersparse cross-attention (de Jong et al., 2022) eliminates most cross-attention layers which make up the primary expense for longer inputs. Speculative sampling (Chen et al., 2023; Leviathan et al., 2022) ameliorates the memory bandwidth bottleneck by proposing multiple tokens with a smaller model which are then scored in parallel by a larger model.\nFinally, the uptraining procedure we propose is inspired by Komatsuzaki et al. (2022), which uptrains standard T5 checkpoints into sparsely activated Mixture-of-Experts models."
        },
        {
            "heading": "5 Conclusion",
            "text": "Language models are expensive for inference primarily due to the memory bandwidth overhead from loading keys and values. Multi-query attention reduces this overhead at the cost of decreased model capacity and quality. We propose to convert multi-head attention models to multi-query models\nwith a small fraction of original pre-training compute. Moreover, we introduce grouped-query attention, an interpolation of multi-query and multi-head attention that achieves quality close to multi-head at comparable speed to multi-query attention.\nLimitations\nThis paper focuses on ameliorating the memory bandwidth overhead from loading keys and values. This overhead is most important when generating longer sequences, for which quality is inherently difficult to evaluate. For summarization we employ Rouge score, which we know is a flawed evaluation that does not tell the whole story; for that reason, it is difficult to be certain our trade-offs are correct. Due to limited computation, we also do not compare our XXL GQA model to a comparitive model trained from scratch, so we do not know the relative performance of uptraining vs training from scratch. Finally, we evaluate the impact of uptraining and GQA only on encoder-decoder models. Recently, decoder-only models are extremely popular, and since these models do not have separate self-attention and cross-attention, we expect GQA to have a stronger advantage over MQA.\nAcknowlegements\nWe thank Santiago Onta\u00f1\u00f3n, Afroz Mohiuddin, William Cohen and others at Google Research for insightful advice and discussion."
        },
        {
            "heading": "A Training Stability",
            "text": "We find that multi-query attention can lead to training instability during fine-tuning, in particular combined with long input tasks. We trained multiple T5-Large models with multi-query attention from scratch. In each case, pre-training suffered from frequent loss spikes and the final models diverged immediately when fine-tuning on long-input tasks. Uptrained multi-query attention models are more stable but still display high variance, so for multiquery models on unstable tasks we report average performance over three fine-tuning runs. Uptrained grouped-query attention models, however, appear to be stable, so we did not investigate futher on the root causes of multi-query instability."
        }
    ],
    "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "year": 2023
}