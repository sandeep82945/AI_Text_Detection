{
    "abstractText": "The classical deep clustering optimization methods basically leverage information such as clustering centers, mutual information, and distance metrics to construct implicit generalized labels to establish information feedback (weak supervision) and thus optimize the deep model. However, the resulting generalized labels have different degrees of errors in the whole clustering process due to the limitation of clustering accuracy, which greatly interferes with the clustering process. To this end, this paper proposes a general deep clustering optimization method from the perspective of empirical risk minimization, using the correlation relationship between the samples. Experiments on two classical deep clustering methods demonstrate the necessity and effectiveness of the method. Code is available at https://github.com/yangzonghao1024/DCGLU.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zonghao Yang"
        },
        {
            "affiliations": [],
            "name": "Wenpeng Hu"
        },
        {
            "affiliations": [],
            "name": "Yushan Tan"
        },
        {
            "affiliations": [],
            "name": "Zhunchen Luo"
        }
    ],
    "id": "SP:e04bf40a95e6bab8596986effc0c52e6623ba6fa",
    "references": [
        {
            "authors": [
                "Paul Albert",
                "Eric Arazo",
                "Noel E O\u2019Connor",
                "Kevin McGuinness"
            ],
            "title": "Embedding contrastive unsupervised features to cluster in-and out-of-distribution noise in corrupted image datasets",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi."
            ],
            "title": "Pattern recognition and machine learning, volume 4",
            "venue": "Springer.",
            "year": 2006
        },
        {
            "authors": [
                "Xiao Cai",
                "Feiping Nie",
                "Heng Huang."
            ],
            "title": "Multiview k-means clustering on big data",
            "venue": "Twenty-Third International Joint conference on artificial intelligence.",
            "year": 2013
        },
        {
            "authors": [
                "Mathilde Caron",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Matthijs Douze."
            ],
            "title": "Deep clustering for unsupervised learning of visual features",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pages 132\u2013149.",
            "year": 2018
        },
        {
            "authors": [
                "Jianlong Chang",
                "Lingfeng Wang",
                "Gaofeng Meng",
                "Shiming Xiang",
                "Chunhong Pan."
            ],
            "title": "Deep adaptive image clustering",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 5879\u20135887.",
            "year": 2017
        },
        {
            "authors": [
                "Dorin Comaniciu",
                "Peter Meer."
            ],
            "title": "Mean shift: A robust approach toward feature space analysis",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence, 24(5):603\u2013619.",
            "year": 2002
        },
        {
            "authors": [
                "Alice Coucke",
                "Alaa Saade",
                "Adrien Ball",
                "Th\u00e9odore Bluche",
                "Alexandre Caulier",
                "David Leroy",
                "Cl\u00e9ment Doumouro",
                "Thibault Gisselbrecht",
                "Francesco Caltagirone",
                "Thibaut Lavril"
            ],
            "title": "Snips voice platform: an embedded spoken language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Francesco De Comit\u00e9",
                "Fran\u00e7ois Denis",
                "R\u00e9mi Gilleron",
                "Fabien Letouzey."
            ],
            "title": "Positive and unlabeled examples help learning",
            "venue": "International conference on algorithmic learning theory, pages 219\u2013230. Springer.",
            "year": 1999
        },
        {
            "authors": [
                "Arthur P Dempster",
                "Nan M Laird",
                "Donald B Rubin."
            ],
            "title": "Maximum likelihood from incomplete data via the em algorithm",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1\u201322.",
            "year": 1977
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Marthinus Du Plessis",
                "Gang Niu",
                "Masashi Sugiyama."
            ],
            "title": "Convex formulation for learning from positive and unlabeled data",
            "venue": "International conference on machine learning, pages 1386\u20131394. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Marthinus C Du Plessis",
                "Gang Niu",
                "Masashi Sugiyama."
            ],
            "title": "Analysis of learning from positive and unlabeled data",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "Martin Ester",
                "Hans-Peter Kriegel",
                "J\u00f6rg Sander",
                "Xiaowei Xu"
            ],
            "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
            "venue": "In kdd,",
            "year": 1996
        },
        {
            "authors": [
                "K Chidananda Gowda",
                "G Krishna."
            ],
            "title": "Agglomerative clustering using the concept of mutual nearest neighbourhood",
            "venue": "Pattern recognition, 10(2):105\u2013112.",
            "year": 1978
        },
        {
            "authors": [
                "Amir Hadifar",
                "Lucas Sterckx",
                "Thomas Demeester",
                "Chris Develder."
            ],
            "title": "A self-training approach for short text clustering",
            "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 194\u2013199.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengqiu He",
                "Wenliang Chen",
                "Yuyi Wang",
                "Wei Zhang",
                "Guanchun Wang",
                "Min Zhang."
            ],
            "title": "Improving neural relation extraction with positive and unlabeled learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7927\u20137934.",
            "year": 2020
        },
        {
            "authors": [
                "Wenpeng Hu",
                "Ran Le",
                "Bing Liu",
                "Jinwen Ma",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Translation vs",
            "venue": "dialogue: A comparative analysis of sequence-to-sequence modeling. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4111\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Zongmo Huang",
                "Yazhou Ren",
                "Xiaorong Pu",
                "Lifang He."
            ],
            "title": "Non-linear fusion for self-paced multiview clustering",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pages 3211\u2013 3219.",
            "year": 2021
        },
        {
            "authors": [
                "Ryuichi Kiryo",
                "Gang Niu",
                "Marthinus C Du Plessis",
                "Masashi Sugiyama."
            ],
            "title": "Positive-unlabeled learning with non-negative risk estimator",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Jens Lehmann",
                "Robert Isele",
                "Max Jakob",
                "Anja Jentzsch",
                "Dimitris Kontokostas",
                "Pablo N Mendes",
                "Sebastian Hellmann",
                "Mohamed Morsey",
                "Patrick Van Kleef",
                "S\u00f6ren Auer"
            ],
            "title": "Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia",
            "year": 2015
        },
        {
            "authors": [
                "Ting-En Lin",
                "Hua Xu",
                "Hanlei Zhang."
            ],
            "title": "Discovering new intents via constrained deep adaptive clustering with cluster refinement",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8360\u20138367.",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Lison",
                "Serge Bibauw."
            ],
            "title": "Not all dialogues are created equal: Instance weighting for neural conversational models",
            "venue": "arXiv preprint arXiv:1704.08966.",
            "year": 2017
        },
        {
            "authors": [
                "Bing Liu",
                "Wee Sun Lee",
                "Philip S Yu",
                "Xiaoli Li."
            ],
            "title": "Partially supervised classification of text documents",
            "venue": "ICML, volume 2, pages 387\u2013394. Sydney, NSW.",
            "year": 2002
        },
        {
            "authors": [
                "J MacQueen."
            ],
            "title": "Classification and analysis of multivariate observations",
            "venue": "5th Berkeley Symp. Math. Statist. Probability, pages 281\u2013297.",
            "year": 1967
        },
        {
            "authors": [
                "Minlong Peng",
                "Xiaoyu Xing",
                "Qi Zhang",
                "Jinlan Fu",
                "Xuanjing Huang."
            ],
            "title": "Distantly supervised named entity recognition using positive-unlabeled learning",
            "venue": "arXiv preprint arXiv:1906.01378.",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Yazhou Ren",
                "Carlotta Domeniconi",
                "Guoji Zhang",
                "Guoxian Yu."
            ],
            "title": "Weighted-object ensemble clustering: methods and analysis",
            "venue": "Knowledge and Information Systems, 51(2):661\u2013689.",
            "year": 2017
        },
        {
            "authors": [
                "Yazhou Ren",
                "Xiaohui Hu",
                "Ke Shi",
                "Guoxian Yu",
                "Dezhong Yao",
                "Zenglin Xu."
            ],
            "title": "Semi-supervised denpeak clustering with pairwise constraints",
            "venue": "Pacific Rim International Conference on Artificial Intelligence, pages 837\u2013850. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Yazhou Ren",
                "Uday Kamath",
                "Carlotta Domeniconi",
                "Zenglin Xu."
            ],
            "title": "Parallel boosted clustering",
            "venue": "Neurocomputing, 351:87\u2013100.",
            "year": 2019
        },
        {
            "authors": [
                "Meitar Ronen",
                "Shahaf E Finder",
                "Oren Freifeld."
            ],
            "title": "Deepdpm: Deep clustering with an unknown number of clusters",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9861\u20139870.",
            "year": 2022
        },
        {
            "authors": [
                "Jianlin Su",
                "Jiarun Cao",
                "Weijie Liu",
                "Yangyiwen Ou."
            ],
            "title": "Whitening sentence representations for better semantics and faster retrieval",
            "venue": "arXiv preprint arXiv:2103.15316.",
            "year": 2021
        },
        {
            "authors": [
                "Rui Xia",
                "Xuelei Hu",
                "Jianfeng Lu",
                "Jian Yang",
                "Chengqing Zong."
            ],
            "title": "Instance selection and instance weighting for cross-domain sentiment classification via pu learning",
            "venue": "Twenty-Third International Joint Conference on Artificial Intelligence.",
            "year": 2013
        },
        {
            "authors": [
                "Junyuan Xie",
                "Ross Girshick",
                "Ali Farhadi."
            ],
            "title": "Unsupervised deep embedding for clustering analysis",
            "venue": "International conference on machine learning, pages 478\u2013487. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Bo Yang",
                "Xiao Fu",
                "Nicholas D Sidiropoulos",
                "Mingyi Hong."
            ],
            "title": "Towards k-means-friendly spaces: Simultaneous deep learning and clustering",
            "venue": "international conference on machine learning, pages 3861\u2013 3870. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Xu Yang",
                "Cheng Deng",
                "Feng Zheng",
                "Junchi Yan",
                "Wei Liu."
            ],
            "title": "Deep spectral clustering using dual autoencoder network",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4066\u20134075.",
            "year": 2019
        },
        {
            "authors": [
                "Dejiao Zhang",
                "Feng Nan",
                "Xiaokai Wei",
                "Shangwen Li",
                "Henghui Zhu",
                "Kathleen McKeown",
                "Ramesh Nallapati",
                "Andrew Arnold",
                "Bing Xiang."
            ],
            "title": "Supporting clustering with contrastive learning",
            "venue": "arXiv preprint arXiv:2103.12953.",
            "year": 2021
        },
        {
            "authors": [
                "nen"
            ],
            "title": "2022) and Deep Adaptive Clustering (DAC) (Chang et al., 2017). DeepDPM is a deep nonparametric clustering method that can adapt to k-value changes by dynamically adjusting the split and merge operations. DAC transforms the clus",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "The classical deep clustering optimization methods basically leverage information such as clustering centers, mutual information, and distance metrics to construct implicit generalized labels to establish information feedback (weak supervision) and thus optimize the deep model. However, the resulting generalized labels have different degrees of errors in the whole clustering process due to the limitation of clustering accuracy, which greatly interferes with the clustering process. To this end, this paper proposes a general deep clustering optimization method from the perspective of empirical risk minimization, using the correlation relationship between the samples. Experiments on two classical deep clustering methods demonstrate the necessity and effectiveness of the method. Code is available at https://github.com/yangzonghao1024/DCGLU."
        },
        {
            "heading": "1 Introduction",
            "text": "Cluster analysis plays a role in machine learning and data mining as it can classify data into different groups in an unsupervised way. Over the past decades, a large number of clustering methods with shallow models have been proposed (Ren et al., 2019; Comaniciu and Meer, 2002; Ren et al., 2018; Bishop and Nasrabadi, 2006; Ren et al., 2017; Cai et al., 2013; Huang et al., 2021). The performance of them on the complex data is limited due to the poor power of feature learning.\nRecently, deep learning based clustering approach (referred to deep clustering) aims at effectively extracting more clustering-friendly features from data and performing clustering with learned features simultaneously (Chang et al., 2017; Albert et al., 2022; Ronen et al., 2022).\nAfter in-depth research, we find that most deep clustering models are usually optimized through\n\u2217 Corresponding authors.\nspecific supervision information. We call it generalized supervision in clustering which is implemented through generalized labels in most cases. The generalized labels usually have different generation method according to different deep clustering algorithm. We will elaborate them in Section 2. Here, we argue that the generalized labels are full of noise due to the performance limitation of clustering model and estimation methods especially at the beginning of clustering progress, which will disturb the recognition of clustering group and influence the clustering result. To this end, inspired by Positive and unlabeled Learning (PU learning), we propose a novel method to reduce the impact of noise in generalized labels, namely Deep Clustering optimization from Generalized Labeled and Unlabeled data (DCGLU for short).\nThe proposed DCGLU divides the clustering samples into high confidence samples and low confidence samples according to the confidence level of the samples been correctly clustered. Obviously, the generalized labels of low confidence samples tend to have more errors (as the decision confidence usually related to accuracy (Hu et al., 2020)). In this case, we propose to regard the low confidence samples as unlabeled samples and the high confidence samples as labeled samples, then learning the model in the same fashion with PU learning to reduce the errors in generalized supervision and thus improve the clustering performance.\nOverall, our contributions can be summarized as follows:(1) it proposes a new problem that exists in most deep clustering approaches and formulate it as a problem of generalized supervision with generalized label, which will facilitate the research of deep clustering; (2) it proposes a general deep clustering optimization method, which can be leveraged to reduce the impact of noise in clustering; (3) experiments show the proposed DCGLU can improve the clustering features and clustering results of strong baselines."
        },
        {
            "heading": "2 Problem Formulation",
            "text": "Given the unlabeled text dataset D = {xi, i = 1, ..., N}, where xi represents the ith text in the corpus, our goal is to cluster N samples into k different classes by an unsupervised method. Each sample in D is represented by a feature vector. We use the pre-trained language model Bert (Devlin et al., 2018) to obtain the representation ei = BERT (xi) of each text.\nAs discussed in Section 1, the deep model in deep clustering methods is usually optimized by specific supervised information. We refer the specific supervision to generalized supervision, which is basically realized by generalized labels. The generalized label generation method varies among different clustering methods 1: 1) The cluster center used in K-means-based methods (Yang et al., 2017; Xie et al., 2016) can be considered as the generalized label; 2) Spectral clustering correlation methods (Chang et al., 2017; Yang et al., 2019) use the distance similarity matrix between samples to minimize the similarity distance between samples, the similarity between samples can be regarded as generalized labels; 3) Gaussian Mixture Model methods (Ronen et al., 2022) assume that the samples in each class obey an independent Gaussian distribution. The center of the Gaussian distribution can be regard as the generalized label; 4) (Caron et al., 2018) use the clustering assignment results as pseudo-label information to optimize feature generation. Thus clustering assignment results are the generalized labels.\nFormally, deep clustering algorithm will generate a target distribution P , which is used to indicate the probability distribution of samples of the same category, and optimize the clustering model through stochastic gradient descent algo-\n1please note that the generalized label is not necessarily a discrete classification label, but also a continuous measurement, such as distance\nrithm. Hence, the generated target distribution P can be considered as the distribution of the generalized labels Y \u2032 of the samples. The process of fine-tuning the deep clustering model is the process of generalized label exploitation, which can be described formally by the experience risk R\u03b4(f):\nR\u03b4(f) = 1\nn n\u2211 i=1 L ( f (xi) , y \u2032 i ) (1)\nwhere L is the distance metric function of f(x) and the generalized label y\u2032, (xi, y\u2032i) \u223c P ."
        },
        {
            "heading": "3 Method",
            "text": "The noises in generalized label Y \u2032 result in error propagation problem. A common used method for the problem is instance weighting (Lison and Bibauw, 2017): R\n\u2032 \u03b4(f) = 1 n \u2211n i=1wi \u00b7\nL (f (xi) , y\u2032i). wi is the importance estimation weight of each sample. Clearly, there are two main challenges for instance weighting, (1) it reduces the impact of noise while ignoring a large amount of information in data with small wi; (2) the estimation of wi is very tricky due to the accuracy limitation of unsupervised clustering.\nFor challenge (2), we propose to use the confidence of samples been correctly clustered (easy estimation, see appendix A.1) to measure the quality of generalized labels. Note that the confidence cannot replace noise estimation as the generalized labels tend to have little errors with high confidence but the contrary is not necessarily true in most cases. To solve the above problem and challenge (1), we regard the samples with low confidence as the unlabeled data as we cannot make sure the correctness of their generalized labels.2 Therefore, the samples are divided into high-confidence labeled samples and unlabeled (low confidence) samples according to their confidence levels by hyper-parameter t. 3 Then we adapt PU learning (Liu et al., 2002) to our case for model optimization. Here we directly provide the final adaptation formula, for detailed\n2According to the maximum entropy principle, the optimal treatment for samples with unknown distribution is to make no assumptions, in which case the probability distribution is the most uniform and the risk of prediction is minimal.\n3We set t to 0.8 in the experiments which performs well in different data sets and for different models.\nderivation process, please refer to Appendix A.2:\nR\u0302pu(f) = \u03c0pR\u0302 + p (f) + max { 0, R\u0302\u2212u (f)\u2212 \u03c0pR\u0302\u2212p (f) } R\u0302+p (f) = 1\nnp np\u2211 i=1 L ( f (xpi ) , y \u2032p + ) R\u0302\u2212p (f) = 1\nnp np\u2211 i=1 L ( f (xpi ) , y \u2032p \u2212 )\nR\u0302\u2212u (f) = 1\nnu nu\u2211 i=1 L ( f\u0304 (xui ) ,\u22121\n) (2)\nwhere y\u2032p+ is the multi-class generalized label corresponding to the high confidence sample; in general, y\u2032p\u2212 can be expressed as y \u2032p \u2212 = 1 \u2212 y \u2032p + ; \u03c0p is the prior probability of positive samples. Overall, we give the clustering optimization algorithm in the Algorithm 1 in appendix A.2, and give the architecture of DCGLU in Figure 1. Clearly, DCGLU realizes decoupling from the clustering model, therefore it can be applied to most clustering algorithms."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "We apply the DCGLU method on two strong deep clustering method: DAC (Chang et al., 2017) and DeepDPM (Ronen et al., 2022), and then evaluate them on two publicly available text datasets: SNIPS (Coucke et al., 2018)(7 classes) and DBPedia (Lin et al., 2020) (14 classes). We further take K-Means(KM) (MacQueen, 1967) and agglomerative clustering(AG) (Gowda and Krishna, 1978), SAE-KM, BERT-KM, Deep embedding clustering(DEC) (Xie et al., 2016), Deep clustering network(DCN) (Yang et al., 2017), etc as the baselines into comparison to show the competitiveness\nof DCGLU. We keep our primary experimental setup consistent with what DeepDPM4 and DAC5 reported in their original paper. For more details of datasets, baselines and implement settings please refer to Appendix B."
        },
        {
            "heading": "4.2 Compared with strong baselines",
            "text": "Table 1 shows the clustering results of strong baselines and the effectiveness of DCGLU. We can draw the following observations.\nFirstly, the proposed DCGLU has significant improvement on two different kinds of deep clustering methods DAC and DeepDPM (with p-value < 0.01 on paired t-test). The consistency improvement of multiple algorithms, evaluation metrics and datasets indicates that DCGLU has better effectiveness and universality.\nSecondly, we can see that 1) on the SNPIS dataset, with the help of DCGLU, DeepDPM succeeds in significantly outperforming the strongest baseline system (DEC) participating in the comparison (with p-value < 0.01 on paired t-test); 2) on the DBPedia dataset, DCGLU makes DAC as well as DeepDPM, the top two powerful systems, get better results. This indicates that the error propagation problem in generalized supervision described in this paper has a large impact on the performance of deep models and is prevalent (even in very strong clustering systems), while DCGLU can deal with the problem and get better results.\nIn Section 3, we discussed that the instance weighting method can mitigate the noise problem in generalized labels, unfortunately it is difficult to\n4https://github.com/BGU-CS-VIL/DeepDPM 5https://github.com/thuiar/CDAC-plus\nestimate the noise of generalized labels accurately in the clustering algorithm. This paper uses confidence to select high confidence labeled samples and unlabeled samples. In this case, can we use confidence as weight to mitigate the noise problem? The answer is yes but we cannot get better clustering results in experiments. Taking DAC algorithm as an example, compared with the original DAC algorithm, instance weighting (regard the confidence level as the instance weight) decreases 3.31,3.79,2.63 percentage points for NMI, ARI, and ACC metrics on SNIPS dataset, and 7.19,6.59,6.16 percentage points for the three metrics on DBPedia dataset, respectively. This is because the generalized labels with low confidence level are not necessarily noise, so the instance weighting using confidence level will ignore a lot of relevant information, which makes the clustering effect appear a certain degree of degradation."
        },
        {
            "heading": "4.3 More evaluations",
            "text": "We evaluate the representation ability of DCGLU as learning better clustering features is very important for deep clustering algorithm (Caron et al., 2018). Good representation of clustering indicate the clustering results can be further improved by fine-tuning or distance-based clustering methods, etc. In the experiments, we leverage K-Means and DEC as the post clustering method based on the features learned by DCGLU, and get consistency and significant improvements on the two datasets (with p-value < 0.01 on paired t-test), which indicates DCGLU can learn better representations. For more details please see Appendix C.1.\nThe \u03c0p in Equation 2 is the key hyper-parameter in DCGLU, we conducted a detailed analysis to guide its setting in specific applications. Fortunately, we found that using a constant \u03c0p (\u03c0p = 0.5) can already achieve good results, which greatly reduces the conditions of DCGLU applications. For more details please see Appendix C.2."
        },
        {
            "heading": "5 Related Work",
            "text": "Deep clustering. Researchers have proposed many clustering methods, including center-ofmass-based clustering (MacQueen, 1967), densitybased clustering (Ester et al., 1996; Comaniciu and Meer, 2002), agglomerative clustering (AG) (Gowda and Krishna, 1978) and so on, but these traditional clustering methods fix sample features, and the clustering performance of the model\ntends to be poor when the extracted sample features are poor. In recent years, researchers (Xie et al., 2016; Yang et al., 2017) have combined deep neural networks to focus their work in deep clustering by feature clustering jointly. (Chang et al., 2017) proposed Deep Adaptive Cluster(DAC), which transforms the clustering problem into a binary pairwiseclassification framework to determine whether samples belong to the same class. (Hadifar et al., 2019) proposed to learn discriminative features from both an autoencoder and a sentence embedding, and then update the weights of the encoder network using assignments of clustering algorithm as supervision. (Zhang et al., 2021) proposed Supporting Clustering with Contrastive Learning (SCCL) to leverage contrastive learning to promote better separation. (Ronen et al., 2022) proposed Deep Dirichlet Process Mixture(DeepDPM) to adapt to changes in the k-values of clustering categories by dynamically performing split-and-merge operations. In this paper, we focus on the influence of noise in clustering which is not explicitly investigated before.\nPU learning. Positive-unlabeled (PU) learning has been studied for a long time (De Comit\u00e9 et al., 1999; Du Plessis et al., 2014, 2015; Kiryo et al., 2017). For PU learning (Liu et al., 2002) is defined as follows: given a set of positive samples P and a set of unlabeled samples U , which contain hidden positive and negative samples, construct a binary classifier to classify the samples. PU learning is used in many natural language processing applications, (Xia et al., 2013) combined instance selection and instance weighting to apply PU learning to cross-domain sentiment classification; (Peng et al., 2019) explored the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries; (He et al., 2020) presented a method to improve the performance of distant supervision relation extraction with PU Learning.\nIn this paper, inspired by PU learning, we propose a novel method (DCGLU) to improve the performance of text clustering, which 1) transform the multi-classification problem in clustering into binary classification; 2) establish a binary classification selector; 3) from the perspective of empirical risk minimization, dig the correlation between generalized labeled and unlabeled samples. Clearly, there are some major differences between PU learning and DCGLU."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose the concept of generalized supervision and generalized labels in clustering which can help to study the impact of noise in clustering and thus improve the performance of clustering. Based on the generalized supervision and labels, we propose a deep clustering optimization method, namely Deep Clustering optimization from Generalized Labeled and Unlabeled data (DCGLU) which can be leveraged to most deep clustering methods of text. As we discussed in related works, DCGLU different from existing clustering and clustering optimization approaches. The experimental results demonstrate the necessity and effectiveness of the method."
        },
        {
            "heading": "Acknowledgement",
            "text": "We would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by National Natural Science Foundation of China (No.62206308, No.61976221)."
        },
        {
            "heading": "Limitations",
            "text": "Due to the limitations imposed by the data representation methods, in future work, we will attempt to apply our method to image data."
        },
        {
            "heading": "A Detailed Explanations",
            "text": ""
        },
        {
            "heading": "A.1 Method of Confidence Estimation",
            "text": "Compared with the estimation of generalized labels, the method of obtaining confidence is relatively simple. For examples, in the method based on the cluster center, the closer the sample is to the cluster center, the higher the possibility of belonging to the center, so the confidence level can be acquired by the distance; in the method based on the similarity between samples, the higher the similarity between samples, the higher the possibility of the sample pair belonging to the same category, thus the confidence level can be acquired by the similarity between samples."
        },
        {
            "heading": "A.2 Adapting PU Learning to Deep Clustring",
            "text": "According to the problem of learning from unlabeled data discussed in Section 3, we find that PU learning has more in-depth research, which can promote the identification of samples with similar characteristics of positive data among unlabeled samples. In the scenario of clustering, high-confidence labeled samples and unlabeled (low confidence) samples can be regard as the positive samples and unlabeled samples in PU learning respectively. The classical PU learning (Du Plessis et al., 2014) relies on the binary unbiased risk estimator to mine the correlation between unlabeled samples and positive samples:\nR\u0302pu(f) = \u03c0pR\u0302 + p (f)\u2212 \u03c0pR\u0302\u2212p (f) + R\u0302\u2212u (f) (3)\nwhere \u03c0p is the prior probability of positive samples. Among Eq. (3),\nR\u0302+p (f) = 1\nnp np\u2211 i=1 L (f (xpi ) ,+1) (4)\nR\u0302\u2212p (f) = 1\nnp np\u2211 i=1 L (f (xpi ) ,\u22121) (5)\nR\u0302\u2212u (f) = 1\nnu nu\u2211 i=1 L (f (xui ) ,\u22121) (6)\nwhere {+1,\u22121} corresponds to the labels of positive and unlabeled samples, respectively.\nHere we introduce PU learning to deep clustering to help use the generalized labels and extend it to multiple classes to adapt for the application scenario of the clustering, in which high confidence labeled samples are considered as positive samples\nand unlabeled (low confidence) samples are considered as unlabeled samples, thus:\nR\u0302+p (f) = 1\nnp np\u2211 i=1 L ( f (xpi ) , y \u2032p + ) (7)\nR\u0302\u2212p (f) = 1\nnp np\u2211 i=1 L ( f (xpi ) , y \u2032p \u2212 )\n(8)\nwhere,y\u2032p+ is the multi-class generalized label corresponding to the high confidence sample; Eq.(8) indicates that the high confidence sample does not belong to the loss of the currently assigned generalized label, in general, y\u2032p\u2212 can be expressed as y\u2032p\u2212 = 1 \u2212 y \u2032p + . Based on the above development method, R\u0302\u2212u (f) can be extended to:\nR\u0302\u2212u (f) = 1\nnu nu\u2211 i=1 L ( f (xui ) , y \u2032u \u2212 )\n(9)\nHowever, there is no reliable multi-class generalized label for low confidence samples. Similarly, based on the maximum entropy principle, we assume that the generalized label y\u2032p of each unlabeled (low confidence) sample obeys a uniform distribution, i.e., the unlabeled sample expects the minimum under all category labels, so for the unlabeled sample,we transform Eq.(9) into:\nR\u0302\u2212u (f) = 1\nnu nu\u2211 i=1 L ( f\u0304 (xui ) ,\u22121 ) (10)\nwhere f\u0304 (xui ) is the mean of the predicted probabilities of unlabeled (low confidence) samples over all categories. The extension method, including Eq. (7), Eq.(8), and Eq.(10), can be approximated as a way to complete the learning of high confidence categorized samples and uncategorized (low confidence) samples by transforming the multi-category task split into multiple binary categories. Further, to prevent overfitting we use the non-negative risk estimator (Kiryo et al., 2017), which transforms Eq.(3) into:\nR\u0302pu(f) = \u03c0pR\u0302 + p (f) + max { 0, R\u0302\u2212u (f)\u2212 \u03c0pR\u0302\u2212p (f) } (11)\nBy substituting Eq.(7), Eq.(8), and Eq.(10) into Eq.(11), the loss function of the optimized deep clustering model proposed in this paper can be obtained, which is called Deep Clustering optimization from Generalized Labled and Unlabeled learning algorithm(DCGLU). It can be added as a regularization term into the currently commonly\nused deep clustering algorithm, with good generalization ability:\nLtotal = Lori + \u03bb \u00b7 R\u0302pu(f) (12)\nWhere Lori is the original loss of deep clustering algorithm, \u03bb is the blend factor, and R\u0302pu(f) is the loss function of deep clustering optimization model proposed in this paper.\nAlgorithm 1 DCGLU Input: Dataset X = {xi}ni=1, xi is the i\nth text in X . Parameter: Clustering model parameter \u03b8, hyper-parameters confidence threshold t (0 \u2264 t \u2264 1) and positive class-prior probability \u03c0p. Output: Cluster label ci for each xi \u2208 X . 1: Initialization:Initialize the cluster network parameters; 2: for number of training steps do 3: Sample batch Xk from X , k indicates the batch id; 4: Input to clustering model f ,Get probability of sample\npairs or samples p; 5: if p > t then 6: Get high confidence samples X hk from Xk as the positive samples X pk ; 7: Calculate the empirical risk of the positive samples R\u0302+p (f) and R\u0302\u2212p (f);//Eq.(7) and Eq.(8) 8: else 9: Get low confidence samples X lk from Xk as the\nunlabeled samples Xuk ; 10: Calculate the empirical risk of the unlabeled samples R\u0302\u2212u (f);//Eq.(10) 11: end if 12: if R\u0302\u2212u (f)\u2212 \u03c0pR\u0302\u2212p (f) \u2265 0 then 13: Set gradient \u2207\u03b8Lori +\u2207\u03b8R\u0302pu(f); 14: else 15: Set gradient \u2207\u03b8Lori +\u2207\u03b8(\u03c0pR\u0302\u2212p (f)\u2212 R\u0302\u2212u (f)); 16: end if 17: Update \u03b8 by an external SGD-like stochastic optimization algorithm; 18: end for 19: for all xi \u2208 X do 20: li := f (xi); 21: ci := argmaxh (lih); 22: end for"
        },
        {
            "heading": "B Experiment Setup",
            "text": ""
        },
        {
            "heading": "B.1 Datasets",
            "text": "We conduct experiments on two publicly available text datasets. The detailed statistics are shown in Table 2.\nSNIPS It derives from (Coucke et al., 2018) and is a personal voice assistant dataset containing 14,484 voices, divided into seven categories altogether.\nDBpedia It contains 14 non-overlapping classes of ontology selected from DBPedia 2015 (Lehmann et al., 2015). We follow (Lin et al., 2020), which contained 1,000 samples in each classes."
        },
        {
            "heading": "B.2 Baseline and Evaluation Metrics",
            "text": "We follow (Lin et al., 2020), and chose the baseline of the unsupervised part for comparison:\nKM and AG. K-means(KM) (MacQueen, 1967) and Agglomerative Clustering(AC) (Gowda and Krishna, 1978) are classical clustering algorithms, and here we represent the sentences with the averaged pre-trained 300-dimensional word embeddings from GloVe (Pennington et al., 2014).\nSAE-KM. We encode the sentences with the stacked autoencoder (SAE), and then execute kmeans.\nBERT-KM. We encode the sentences by BERT (Devlin et al., 2018), and then execute k-means.\nDEC. Deep embedding clustering(DEC) (Xie et al., 2016) learns the mapping from data space to low-dimensional feature space, and uses tdistribution iteration to fine-tune clustering model.\nDCN. Deep clustering network(DCN) (Yang et al., 2017) follows the idea of DEC and adds regularization term in the optimization process.\nAs an optimization strategy, in order to show the performance of our algorithm, we select two advanced deep clustering algorithms of different kinds as the main methods to test DCGLU: Deep Dirichlet Process Mixture (DeepDPM) (Ronen et al., 2022) and Deep Adaptive Clustering (DAC) (Chang et al., 2017). DeepDPM is a deep nonparametric clustering method that can adapt to k-value changes by dynamically adjusting the split and merge operations. DAC transforms the clustering problem into binary pairwise-classification framework to judge whether the samples belong to the same category.\nTo evaluate the experimental results, we choose three common clustering measures: Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and clustering accuracy (ACC). To calculate clustering accuracy, we use the Hungarian algorithm (Kuhn, 1955) to find the best alignment between the predicted cluster label and the ground-\ntruth label. The higher the score of all metrics, the better the clustering performance.\nB.3 Implement Details For both DeepDPM and DAC, we use the same BERT model to get feature vectors of text data, but as the adaptation of DeepDPM is limited for highdimensional features, in accordance with their suggestions, we use Bert-Whitening (Su et al., 2021) to reduce the dimension of text features. Text features H is reduced from 768 to 64 dimensions. For fairer comparison, we maintain the original dataset settings of (Ronen et al., 2022) and (Lin et al., 2020). Finally, we report the average results of each algorithm over ten runs.\nDeepDPM is divided into clustering module and subclustering module. The model is optimized by introducing a new loss caused by ExpectationMaximization (EM) (Dempster et al., 1977) in Bayesian GMM, where every E-step is followed by a standard M-step. However, we believe that the probability of E-step generation is too incredible, and there are many misdivided samples. Therefore, we sharpen the probability of E-step generation, select high confidence labeled samples and other samples as unlabeled samples to optimize the clustering effect. DAC transforms the clustering problem into a binary pairwise-classification framework to determine whether the samples belong to the same class, so we optimize the clustering effect by treating two samples with high similarity as high-confidence labeled sample pairs and those with low similarity as unlabeled sample pairs. Our experiments conduct on pytorch6 version 1.11.0 and 1.0.1 respectively. All experiments were performed on the NVIDIA Ge-Force RTX-2080Ti Graphical Card with 10G graphical memory."
        },
        {
            "heading": "C Experimental Results",
            "text": ""
        },
        {
            "heading": "C.1 Effect on clustering features",
            "text": "Learning better clustering features is one of the important directions of deep clustering algorithm research (Caron et al., 2018), on which the clustering effect can be further improved by fine-tuning or\n6https://pytorch.org\ndistance-based clustering methods, etc. To explore more deeply the effect of DCGLU on clustering features, we explored the experimental results of different variants of the DAC algorithm, in which DAC-KM clusters the embedded features learned by DAC using the K-Means model, and DAC-DEC combines the idea of DEC (Xie et al., 2016) model to fine-tune and improve the clustering assignment by expectation maximization iterations.\nAs can be seen from Table 3 of the experimental results, each method has a certain improvement on both datasets after combining the DCGLU optimization method, which is more obvious on the DBPedia dataset, which indicates that the DCGLU method is able to optimize the clustering effect of the DAC algorithm itself, in addition to obtaining better clustering features. Note that DCGLU has limited improvement on the SNPIS dataset, and we believe this is because the performance improvement on the clustering representation cannot be efficiently transferred to the final clustering performance after post-processing, therefore the limited improvement can already indicate that DCGLU has some improvement on the clustering features."
        },
        {
            "heading": "C.2 Effect of \u03c0p",
            "text": "In the PU learning domain, \u03c0p denotes that the prior of positive samples can be estimated by the ratio of positive samples in the data, however, in DCGLU, the accuracy and recall of high confidence labeled sample estimates in the generalized labels of most deep clustering algorithms keep changing as the clustering algorithm learns(generally, it will be promoted first and then remain stable). This would result in \u03c0p changing dynamically throughout the training process and thus difficult to estimate. Fortunately, we found that using a constant \u03c0p can already achieve good results, which greatly reduces the conditions of DCGLU applications.\nThe effects of different \u03c0p on the experimental results are shown in Figure 2, where \u03c0p \u2208 {0.1, 0.3, 0.5, 0.7, 0.9}. It can be seen that for different \u03c0p, the three evaluation metrics of the two data sets show the same trend of change, and the optimal effect is reached at \u03c0p = 0.5. When \u03c0p\nis overestimated, there is a significant decrease in the experimental results, while the experimental results tend to be stable when \u03c0p is small, so we suggest not to use a larger \u03c0p. This is because in the clustering process, there are still a small number of errors in the high confidence labeled samples. Hence, under the condition of ensuring performance, a smaller \u03c0p should be used to reduce the probability of introducing errors in DCGLU, so as to improve model stability and clustering effect."
        },
        {
            "heading": "C.3 Effect of time efficiency",
            "text": "DCGLU does not increase the complexity of the primary clustering algorithm in terms of efficiency.\nWe conduct a runtime complexity experiment, and Table 4 shows the run time. \u201cProportion(DCGLU Time/Base Time)\u201d indicates the proportion of the calculation time of DCGLU in the entire program running time. As can be seen from the table, our method does not bring much additional time over the entire run time. This is because DCGLU has a time complexity of O(N), and at the same time, the additional spatial complexity brought by DCGLU is O(N)."
        }
    ],
    "title": "Improved Training of Deep Text Clustering",
    "year": 2023
}