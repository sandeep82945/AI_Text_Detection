{
    "abstractText": "Large-scale video-language pre-training has made remarkable strides in advancing videolanguage understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame. TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding. Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuhuai Ren"
        },
        {
            "affiliations": [],
            "name": "Sishuo Chen"
        },
        {
            "affiliations": [],
            "name": "Shicheng Li"
        },
        {
            "affiliations": [],
            "name": "Xu Sun"
        },
        {
            "affiliations": [],
            "name": "Lu Hou"
        }
    ],
    "id": "SP:752c1fb63652e10abbe9c4ec5509966c61faff0c",
    "references": [
        {
            "authors": [
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Georg Heigold",
                "Chen Sun",
                "Mario Lucic",
                "Cordelia Schmid."
            ],
            "title": "Vivit: A video vision transformer",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6816\u20136826.",
            "year": 2021
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "A. Brown",
                "Andrew Zisserman."
            ],
            "title": "Condensed movies: Story based retrieval with contextual embeddings",
            "venue": "ArXiv, abs/2005.04208.",
            "year": 2020
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman."
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1708\u20131718.",
            "year": 2021
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman."
            ],
            "title": "A clip-hitchhiker\u2019s guide to long video retrieval",
            "venue": "ArXiv, abs/2205.08508.",
            "year": 2022
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Bolya",
                "Cheng-Yang Fu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Christoph Feichtenhofer",
                "Judy Hoffman."
            ],
            "title": "Token merging: Your vit but faster",
            "venue": "ArXiv, abs/2210.09461.",
            "year": 2022
        },
        {
            "authors": [
                "Qingqing Cao",
                "Bhargavi Paranjape",
                "Hanna Hajishirzi."
            ],
            "title": "Pumer: Pruning and merging tokens for efficient vision language models",
            "venue": "ArXiv, abs/2305.17530.",
            "year": 2023
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Kumar Sharma",
                "Nan Ding",
                "Radu Soricut."
            ],
            "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize longtail visual concepts",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Dongsheng Chen",
                "Chaofan Tao",
                "Lu Hou",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "Litevl: Efficient videolanguage learning with enhanced spatial-temporal modeling",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Feng Cheng",
                "Xizi Wang",
                "Jie Lei",
                "David J. Crandall",
                "Mohit Bansal",
                "Gedas Bertasius."
            ],
            "title": "Vindlu: A recipe for effective video-and-language pretraining",
            "venue": "ArXiv, abs/2212.05051.",
            "year": 2022
        },
        {
            "authors": [
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Yang Liu",
                "Samuel Albanie",
                "Marius Leordeanu",
                "Hailin Jin",
                "Andrew Zisserman."
            ],
            "title": "Teachtext: Crossmodal generalized distillation for text-video retrieval",
            "venue": "2021 IEEE/CVF International Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805.",
            "year": 2019
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Linjie Li",
                "Zhe Gan",
                "Kevin Lin",
                "William Yang Wang",
                "Lijuan Wang",
                "Zicheng Liu."
            ],
            "title": "Violet : End-to-end video-language transformers with masked visual-token modeling",
            "venue": "ArXiv, abs/2111.12681.",
            "year": 2021
        },
        {
            "authors": [
                "Yuying Ge",
                "Yixiao Ge",
                "Xihui Liu",
                "Dian Li",
                "Ying Shan",
                "Xiaohu Qie",
                "Ping Luo."
            ],
            "title": "Bridging videotext retrieval with multiple choice questions",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16146\u201316155.",
            "year": 2022
        },
        {
            "authors": [
                "Satya Krishna Gorti",
                "No\u00ebl Vouitsis",
                "Junwei Ma",
                "Keyvan Golestan",
                "Maksims Volkovs",
                "Animesh Garg",
                "Guangwei Yu."
            ],
            "title": "X-pool: Cross-modal language-video attention for text-video retrieval",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and",
            "year": 2022
        },
        {
            "authors": [
                "Saurabh Goyal",
                "Anamitra R. Choudhury",
                "Saurabh Raje",
                "Venkatesan T. Chakaravarthy",
                "Yogish Sabharwal",
                "Ashish Verma."
            ],
            "title": "Power-bert: Accelerating bert inference via progressive word-vector elimination",
            "venue": "International Conference on Machine Learning.",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u2019ar",
                "Ross B. Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick."
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738.",
            "year": 2020
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Oliver Wang",
                "Eli Shechtman",
                "Josef Sivic",
                "Trevor Darrell",
                "Bryan C. Russell."
            ],
            "title": "Localizing moments in video with natural language",
            "venue": "2017 IEEE International Conference on Computer Vision (ICCV), pages 5804\u20135813.",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Kenji Hata",
                "Frederic Ren",
                "Li Fei-Fei",
                "Juan Carlos Niebles."
            ],
            "title": "Dense-captioning events in videos",
            "venue": "2017 IEEE International Conference on Computer Vision (ICCV), pages 706\u2013715.",
            "year": 2017
        },
        {
            "authors": [
                "Jie Lei",
                "Tamara L. Berg",
                "Mohit Bansal."
            ],
            "title": "Revealing single frame bias for video-and-language learning",
            "venue": "ArXiv, abs/2206.03428.",
            "year": 2022
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L. Berg",
                "Mohit Bansal",
                "Jingjing Liu."
            ],
            "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Dongxu Li",
                "Junnan Li",
                "Hongdong Li",
                "Juan Carlos Niebles",
                "Steven C.H. Hoi."
            ],
            "title": "Align and prompt: Video-and-language pre-training with entity prompts",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4943\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven C.H. Hoi."
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning.",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath R. Selvaraju",
                "Akhilesh Deepak Gotmare",
                "Shafiq R. Joty",
                "Caiming Xiong",
                "Steven C.H. Hoi."
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "ArXiv, abs/2107.07651.",
            "year": 2021
        },
        {
            "authors": [
                "Yanghao Li",
                "Chaoxia Wu",
                "Haoqi Fan",
                "Karttikeya Mangalam",
                "Bo Xiong",
                "Jitendra Malik",
                "Christoph Feichtenhofer."
            ],
            "title": "Mvitv2: Improved multiscale vision transformers for classification and detection",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Jia Ning",
                "Yue Cao",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Han Hu."
            ],
            "title": "Video swin transformer",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3192\u2013 3201.",
            "year": 2021
        },
        {
            "authors": [
                "Stuart P. Lloyd."
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE Trans. Inf. Theory, 28:129\u2013136.",
            "year": 1982
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Ming Zhong",
                "Yang Chen",
                "Wen Lei",
                "Nan Duan",
                "Tianrui Li."
            ],
            "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval",
            "venue": "Neurocomputing, 508:293\u2013304.",
            "year": 2021
        },
        {
            "authors": [
                "Yiwei Ma",
                "Guohai Xu",
                "Xiaoshuai Sun",
                "Ming Yan",
                "Ji Chao Zhang",
                "Rongrong Ji."
            ],
            "title": "X-clip: Endto-end multi-grained contrastive learning for videotext retrieval",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Dmitrii Marin",
                "Jen-Hao Rick Chang",
                "Anurag Ranjan",
                "Anish K. Prabhu",
                "Mohammad Rastegari",
                "Oncel Tuzel."
            ],
            "title": "Token pooling in vision transformers",
            "venue": "ArXiv, abs/2110.03860.",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Miech",
                "Ivan Laptev",
                "Josef Sivic."
            ],
            "title": "Learning a text-video embedding from incomplete and heterogeneous data",
            "venue": "ArXiv, abs/1804.02516.",
            "year": 2018
        },
        {
            "authors": [
                "Andreea-Maria Oncescu",
                "Jo\u00e3o F. Henriques",
                "Yang Liu",
                "Andrew Zisserman",
                "Samuel Albanie"
            ],
            "title": "Queryd: A video dataset with high-quality text",
            "year": 2020
        },
        {
            "authors": [
                "Rui Qian",
                "Tianjian Meng",
                "Boqing Gong",
                "Ming-Hsuan Yang",
                "Huisheng Wang",
                "Serge Belongie",
                "Yin Cui."
            ],
            "title": "Spatiotemporal contrastive video representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Benlin Liu",
                "Jiwen Lu",
                "Jie Zhou",
                "Cho-Jui Hsieh."
            ],
            "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification",
            "venue": "Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Esteban Real",
                "Jonathon Shlens",
                "Stefano Mazzocchi",
                "Xin Pan",
                "Vincent Vanhoucke."
            ],
            "title": "Youtubeboundingboxes: A large high-precision humanannotated data set for object detection in video",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern",
            "year": 2017
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Lei Li",
                "Xuancheng Ren",
                "Guangxiang Zhao",
                "Xu Sun."
            ],
            "title": "Delving into the openness of CLIP",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Junyang Lin",
                "Guangxiang Zhao",
                "Rui Men",
                "An Yang",
                "Jingren Zhou",
                "Xu Sun",
                "Hongxia Yang."
            ],
            "title": "Learning relation alignment for calibrated cross-modal retrieval",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Aston Zhang",
                "Yi Zhu",
                "Shuai Zhang",
                "Shuai Zheng",
                "Mu Li",
                "Alexander J. Smola",
                "Xu Sun."
            ],
            "title": "Prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition",
            "venue": "volume abs/2304.04704.",
            "year": 2023
        },
        {
            "authors": [
                "Michael S. Ryoo",
                "A.J. Piergiovanni",
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Anelia Angelova"
            ],
            "title": "Tokenlearner: What can 8 learned tokens do for images and videos? ArXiv, abs/2106.11297",
            "year": 2021
        },
        {
            "authors": [
                "Dachuan Shi",
                "Chaofan Tao",
                "Anyi Rao",
                "Zhendong Yang",
                "Chun Yuan",
                "Jiaqi Wang."
            ],
            "title": "Crossget: Crossguided ensemble of tokens for accelerating visionlanguage transformers",
            "venue": "ArXiv, abs/2305.17455.",
            "year": 2023
        },
        {
            "authors": [
                "Yuchong Sun",
                "Hongwei Xue",
                "Ruihua Song",
                "Bei Liu",
                "Huan Yang",
                "Jianlong Fu."
            ],
            "title": "Long-form videolanguage pre-training with multimodal temporal contrastive learning",
            "venue": "ArXiv, abs/2210.06031.",
            "year": 2022
        },
        {
            "authors": [
                "Zhan Tong",
                "Yibing Song",
                "Jue Wang",
                "Limin Wang."
            ],
            "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pretraining",
            "venue": "ArXiv, abs/2203.12602.",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Yixiao Ge",
                "Rui Yan",
                "Yuying Ge",
                "Xudong Lin",
                "Guanyu Cai",
                "Jianping Wu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou."
            ],
            "title": "All in one: Exploring unified video-language pre-training",
            "venue": "ArXiv, abs/2203.07303.",
            "year": 2022
        },
        {
            "authors": [
                "Junke Wang",
                "Dongdong Chen",
                "Zuxuan Wu",
                "Chong Luo",
                "Luowei Zhou",
                "Yucheng Zhao",
                "Yujia Xie",
                "Ce Liu",
                "YuGang Jiang",
                "Lu Yuan."
            ],
            "title": "Omnivl: One foundation model for image-language and video-language tasks",
            "venue": "ArXiv, abs/2209.07526.",
            "year": 2022
        },
        {
            "authors": [
                "Qiang Wang",
                "Yanhao Zhang",
                "Yun Zheng",
                "Pan Pan",
                "Xiansheng Hua."
            ],
            "title": "Disentangled representation learning for text-video retrieval",
            "venue": "ArXiv, abs/2203.07111.",
            "year": 2022
        },
        {
            "authors": [
                "Chaoxia Wu",
                "Philipp Kr\u00e4henb\u00fchl."
            ],
            "title": "Towards long-form video understanding",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1884\u20131894.",
            "year": 2021
        },
        {
            "authors": [
                "Hu Xu",
                "Gargi Ghosh",
                "Po-Yao Huang",
                "Dmytro Okhonko",
                "Armen Aghajanyan",
                "Florian Metze Luke Zettlemoyer Christoph Feichtenhofer."
            ],
            "title": "Videoclip: Contrastive pre-training for zero-shot video-text understanding",
            "venue": "Conference on Empirical Methods in",
            "year": 2021
        },
        {
            "authors": [
                "Jiarui Xu",
                "Shalini De Mello",
                "Sifei Liu",
                "Wonmin Byeon",
                "Thomas Breuel",
                "Jan Kautz",
                "X. Wang."
            ],
            "title": "Groupvit: Semantic segmentation emerges from text supervision",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages",
            "year": 2022
        },
        {
            "authors": [
                "Hongwei Xue",
                "Tiankai Hang",
                "Yanhong Zeng",
                "Yuchong Sun",
                "Bei Liu",
                "Huan Yang",
                "Jianlong Fu",
                "Baining Guo."
            ],
            "title": "Advancing high-resolution videolanguage representation with large-scale video transcriptions",
            "venue": "2022 IEEE/CVF Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Hongwei Xue",
                "Yuchong Sun",
                "Bei Liu",
                "Jianlong Fu",
                "Rui Song",
                "Houqiang Li",
                "Jiebo Luo."
            ],
            "title": "Clip-vip: Adapting pre-trained image-text model to video-language representation alignment",
            "venue": "ArXiv, abs/2209.06430.",
            "year": 2022
        },
        {
            "authors": [
                "Shen Yan",
                "Tao Zhu",
                "Zirui Wang",
                "Yuan Cao",
                "Mi Zhang",
                "Soham Ghosh",
                "Yonghui Wu",
                "Jiahui Yu."
            ],
            "title": "Video-text modeling with zero-shot transfer from contrastive captioners",
            "venue": "ArXiv, abs/2212.04979.",
            "year": 2022
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Just ask: Learning to answer questions from millions of narrated videos",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1666\u20131677.",
            "year": 2020
        },
        {
            "authors": [
                "Qinghao Ye",
                "Guohai Xu",
                "Ming Yan",
                "Haiyang Xu",
                "Qi Qian",
                "Ji Chao Zhang",
                "Fei Huang."
            ],
            "title": "Hitea: Hierarchical temporal-aware video-language pre-training",
            "venue": "ArXiv, abs/2212.14546.",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Yu",
                "D. Xu",
                "Jun Yu",
                "Ting Yu",
                "Zhou Zhao",
                "Yueting Zhuang",
                "Dacheng Tao."
            ],
            "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering",
            "venue": "ArXiv, abs/1906.02467.",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "Merlot: Multimodal neural script knowledge models",
            "venue": "Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Zhihan Zhang",
                "Zhiyi Yin",
                "Shuhuai Ren",
                "Xinhang Li",
                "Shicheng Li."
            ],
            "title": "Dca: Diversified co-attention towards informative live video commenting",
            "venue": "Natural Language Processing and Chinese Computing.",
            "year": 2020
        },
        {
            "authors": [
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Ramazan Gokberk Cinbis",
                "David F. Fouhey",
                "Ivan Laptev",
                "Josef Sivic."
            ],
            "title": "Cross-task weakly supervised learning from instructional videos",
            "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "LCAP"
            ],
            "title": "Hyperparameters. The model is pre-trained for 5 epochs with the Adam (Kingma and Ba, 2015) with a weight decay of 5e-2. The batch size is 384 and the momentum",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Video-language modeling aims to learn semantic alignment between video and language in a joint representation space (Xu et al., 2021; Lei et al., 2021) to facilitate downstream tasks including text-video retrieval, video question answering (VideoQA), and video captioning. Unlike text, which can be represented concisely as a sequence of words with dense semantics, video input consists of much longer sequences due to its 3D properties and the redundancy in space-time information (He\n1Our code is available at https://github.com/ RenShuhuai-Andy/TESTA.\net al., 2021; Tong et al., 2022). In fact, the number of visual tokens processed by Transformer-based models (Fu et al., 2021; Cheng et al., 2022; Ye et al., 2022; Li et al., 2021a; Wang et al., 2022b) can be over 150\u00d7 more than text tokens.2 This poses an efficiency bottleneck for video-language understanding, especially for long-form videos lasting more than 30 seconds (Wu and Kr\u00e4henb\u00fchl, 2021; Sun et al., 2022).\nTo encode long videos within limited computing budgets, previous approaches can be broadly categorized into two types: (1) Sparse Sampling (Lei et al., 2021; Sun et al., 2022; Lei et al., 2022). This method reduces the number of visual tokens by sampling very few frames from the raw video.3 However, sparse sampling sacrifices rich temporal dynamics and storyline information, which limits model performance. (2) Offline Encoding (Luo et al., 2021; Bain et al., 2022). It allows processing more frames within the same computation budgets by constraining the interaction between visual tokens. It first uses an off-the-shelf image encoder (Dosovitskiy et al., 2020; Radford et al., 2021) to encode each frame independently, then uses a temporal module to aggregate all the frame features. However, the frame features encoded offline may not be well adapted to downstream tasks in various domains. Additionally, the postaggregation mechanism also prohibits the full fusion of frame features (Cheng et al., 2022). Considering that both sufficient input frames and full temporal-spatial modeling in an end-to-end manner are pivotal for optimal performance, a natural question arises: Are there better approaches to achieve efficient video coding without compromising on either of these aspects?\n2For example, in the QuerYD dataset, a long-form video with 96 sampled frames at a resolution of 224 \u00d7 224 pixels generates around 19K visual tokens after patchification, while the corresponding caption contains only 128 text tokens.\n3For instance, sample 4 frames from more than 5.4K frames for ActivityNet Captions dataset (Krishna et al., 2017).\nIn this paper, we propose an efficient method named TEmporal-Spatial Token Aggregation (TESTA) inspired by Token Merging (ToMe) (Bolya et al., 2022). Specifically, TESTA samples input frames densely, but progressively aggregates similar visual tokens during video encoding to reduce the token number and computational overhead. As shown in Fig. 1, our aggregation operates separately in temporal and spatial dimensions, allowing for the merging of similar frames as well as similar patches within each frame. This reduces ToMe\u2019s complexity from O((T2 H 16 W 16 ) 2) to O(T 2 + (H16 W 16 )\n2), making it more efficient for encoding longer videos. After aggregation, around 75% visual tokens can be reduced and thus the video encoding is accelerated. To achieve this, we use the bipartite matching algorithm. Specifically, we select a set of tokens and then find their most similar counterparts from the remaining set. Finally, we aggregate the features of these pairs through mean pooling. This aggregation-based mechanism has three advantages: First, it does not incorporate additional parameters and is amenable to parallelism, which significantly improves the training and inference efficiency; Second, our method (1) adaptively condenses video semantics rather than directly discarding input information, (2) retains full end-to-end spatiotemporal fusion, which both ensure the performance. Third, compared to convolution-based feature down-sampling methods (Liu et al., 2021; Li et al., 2021c), our aggregation trajectory can be easily tracked and\nrecovered. The aggregated tokens often correspond to higher-level semantics (e.g., objects, scenes, and events), making them more interpretable and even grounded in language.\nBuilding upon TESTA, we design a pre-trained video-language model with a temporal and spatial token aggregation module in each video encoder block. We evaluate our model on paragraphto-video retrieval and long-form VideoQA tasks. When using an equal number of input frames, our model improves computing efficiency by 1.7 times while maintaining comparable performance. When accessing more frames, our model exhibits strong scalability and achieves significant performance gains compared to previous state-of-the-art methods (e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie)."
        },
        {
            "heading": "2 Related Work",
            "text": "Video-Language Pre-trained Models. Benefitting from large-scale video-text datasets (Bain et al., 2021; Xue et al., 2021) and advances in Transformer model design (Gorti et al., 2022; Ren et al., 2021; Fu et al., 2021; Zellers et al., 2021; Wang et al., 2022a), pre-trained Video-Language Models (VidLMs) (Chen et al., 2022; Sun et al., 2022; Cheng et al., 2022) have demonstrated impressive performance in video-language understanding tasks. VidLMs typically comprise a video encoder and a text encoder, which encode video-text pairs into a shared feature space to learn the semantic alignment between video and language. Additionally, a text decoder can be added after the video\nencoder for tasks such as video captioning and VideoQA (Yan et al., 2022; Zhang et al., 2020).\nEfficient Video Transformer. A Transformerbased video encoder typically pachifies each video into massive visual tokens, which will cause prohibitive computation costs for full self-attention with quadratic computational complexity. Therefore, research on efficient video Transformers has always been active. Representative work like TimeSFormer (Bertasius et al., 2021) and ViViT (Arnab et al., 2021) propose to factorize the spatial and temporal dimensions of the input, then separately apply spatial and temporal attention. Video Swin Transformer (Liu et al., 2021) keeps the joint temporal-spatial attention but restricts it within a local 3D window. Orthogonal to the advances of efficient Transformer architectures, our TESTA aggregates token features from the spatial and temporal dimensions, which reduces the size of input features for each Transformer block and can further boost the efficiency of video encoding.\nFeature Aggregation in Video Transformers. Existing feature aggregation methods can be broadly categorized into two branches. Temporally, frame features can be encoded by a pre-trained image encoder and aggregated using self-attention, joint-attention, or mean pooling for post-temporal modeling purposes (Bain et al., 2022; Luo et al., 2021). Spatially, previous work explored merging similar patches in the image or aggregating tokens into additional proxy tokens (Bolya et al., 2022; Shi\net al., 2023; Cao et al., 2023; Xu et al., 2022; Ryoo et al., 2021; Marin et al., 2021). In contrast, we propose a unified mechanism to simultaneously aggregate frames and patches. Our method gradually aggregates features during video encoding, improving efficiency while ensuring sufficient interaction between features in both space and time."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we first introduce our videolanguage pre-trained model and its architecture in \u00a7 3.1. To improve the efficiency of encoding longform videos, we propose a novel temporal-spatial token aggregation mechanism (\u00a7 3.2). Finally, we present the pre-training objectives in \u00a7 3.3."
        },
        {
            "heading": "3.1 Model Architecture",
            "text": "Inspired by prevalent VidLMs (Li et al., 2022, 2021b), our model consists of three encoders and one decoder for video-language representation learning. Figure 2 shows the model architecture.\nText Encoder. The text encoder is a uni-modal encoder similar to BERT (Devlin et al., 2019). A [CLS] token is prepended at the beginning of the input text to represent its global feature.\nVideo-grounded Text Encoder. This is a crossmodal encoder. Compared to the uni-modal text encoder, we add a cross-modal module to each encoder layer to enable information flow from video to language. We insert an [ENC] token before the\ninput text to condense the cross-modal information from both video and language.\nVideo-grounded Text Decoder. This is a crossmodal decoder with causal self-attention for autoregressive text generation.\nVideo Encoder. This is a uni-modal encoder. Given a raw video, the visual input V \u2208 RT\u00d7H\u00d7W\u00d73 is a sequence of T RGB frames of size H \u00d7W sampled from the video. Each frame is split into L non-overlapping patches4 following ViT (Dosovitskiy et al., 2020). To represent the global video feature, an additional [CLS] token is also used. Our video encoder is similar to TimeSFormer (Bertasius et al., 2021) with the Divided Space-Time Attention. Specifically, each video encoder block captures the temporal relations across frames using Temporal Attention and fuses the spatial information of objects, scenes, etc., within each frame using Spatial Attention. In contrast to TimeSFormer, we improve the efficiency of video encoding by equipping each video encoder block with a Temporal Aggregation Module and a Spatial Aggregation Module, which we will introduce in \u00a7 3.2."
        },
        {
            "heading": "3.2 Temporal-Spatial Token Aggregation",
            "text": "Videos have heavy spatiotemporal redundancy (He et al., 2021; Tong et al., 2022). On one hand, some activities (e.g., conversations) can persist across multiple frames with little visual variations. On the other hand, some scenes like background often contain numerous indistinguishable patches in each frame. Aggregating these similar frames and patches can simplify video feature representation and accelerate video encoding.\nAccordingly, we introduce a Temporal Aggregation Module (TAM) and a Spatial Aggregation Module (SAM), i.e., the yellow modules in Figure 2. After each aggregation, TAM reduces RT frames while SAM reduce RS patches, where RT and RS are hyper-parameters to control the tradeoffs between performance and efficiency. TAM and SAM are incorporated into each block of the video encoder, aggregating tokens progressively to reduce their number. For the i-th Transformer block, let V \u2208 RTi\u00d7Li\u00d7D represents the input video feature, where Ti, Li, D denote the number of frames, the number of patches per frame, and the dimension of the token feature, respectively. The output video\n4The size of each patch is P \u00d7 P , and the L patches span the entire frame (L = HW/P 2).\nfeature after temporal and spatial token aggregation is V\n\u2032 \u2208 R(Ti\u2212RT )\u00d7(Li\u2212RS)\u00d7D, resulting in a smaller size and reducing the computing burden for subsequent blocks. After the forward process with M encoder blocks, the final number of visual tokens is reduced to (T \u2212MRT )\u00d7 (L\u2212MRS)."
        },
        {
            "heading": "3.2.1 Objects for Aggregation",
            "text": "Our video encoder based on TESTA involves two types of tokens for aggregation: patch tokens and frame tokens. Recall that each frame is divided into a sequence of patches, which are treated as patch tokens. To ensure a formally unified aggregation algorithm, we define frame tokens as pseudo tokens to represent each frame by averaging all the patch tokens within it. When merging two frame tokens, the corresponding L patches [p(1)1 , . . . ,p (1) L ] in frame-1 and L patches [p(2)1 , . . . ,p (2) L ] in frame-2 are merged, resulting in L patches [p\n(1&2) 1 , . . . ,p (1&2) L ]. As our aggregation strategy is agnostic to the token type, we refer to both patch tokens and frame tokens as \u201ctokens\u201d throughout the rest of the paper, without loss of generality."
        },
        {
            "heading": "3.2.2 Aggregation Strategy",
            "text": "Recall that given a sequence of N tokens, our target is to reduce R tokens after each aggregation operation.5 To achieve this, we can greedily merge two tokens with the highest similarity and then repeat R times, or merge N tokens into N\u2212R clusters using clustering algorithms such as k-means (Lloyd, 1982). However, these iteration-based methods are not suited for parallelism and can slow down encoding speed (Bolya et al., 2022). Therefore, we resort to the bipartite matching method. We first partition the N tokens into two disjoint sets A and B with R and N \u2212R tokens, respectively. The R tokens in the set A are selected elaborately as the tokens to be reduced. For each token in the set A, we find its most similar token from the set B, then merge them by averaging their features. As a result, the remaining N \u2212 R tokens in the set B form a new sequence as the output.\nFor similarity calculation, we utilize the attention keys (K) of tokens as features and measure their similarity using cosine similarity. The attention keys contain summarized information intended for use in QKV self-attention, yielding accurate similarity measures (Bolya et al., 2022).\n5For temporal aggregation, N = T and R = RT , for spatial aggregation, N = L and R = RS .\nIn practice, we introduce two aggregation algorithms, i.e., importance-based aggregation and geometry-based aggregation.\nImportance-based Aggregation. In this algorithm, we pick out the least important R tokens into the set A for aggregation, so as to minimize the negative effects of token reduction. The importance of the token xi is measured by the following score function Si, which is defined as the product of the attention it receives from the other tokens\u2211N\nj=1,j \u0338=iAji:\nSi = N\u2211 j=1,j \u0338=i Aji = N\u2211 j=1,j \u0338=i softmax( QK\u22a4\u221a d )ji, (1)\nwhere Aji is the attention score from token xj to xi, Q and K represent Queries and Keys in selfattention, respectively.\nGeometry-based Aggregation. In practice, we notice that adjacent tokens have a larger similarity and should be merged. However, these adjacent tokens also have similar importance scores and thus are prone to be grouped into the same set in importance-based strategy, which hinders their aggregation. To address this issue, we partition the N tokens in an alternative way inspired by Bolya et al. (2022), thus assigning adjacent tokens to different sets A and B. As shown in the left panel in Figure 2, for each token x(A)i in the set A, we find its most similar token x(B)j from the set B to construct a pair (x\n(A) i , x (B) j ) and record their similarity. After that, we select R pairs with the greatest similarity and merge the two tokens in the top-R pairs. Finally, we concatenate the tokens in the two sets back into one sequence as the output.\nThe above aggregation algorithms are parameterfree, and can be easily plugged into a Transformerbased video encoder. We conduct our aggregation during both training and testing. Although the token similarity calculation brings additional computing overhead, it is negligible compared to the efficiency gained by reducing token numbers."
        },
        {
            "heading": "3.2.3 Novelty over Token Merging",
            "text": "Our work is inspired by Token Merging (ToMe) (Bolya et al., 2022), which also proposes to reduce video tokens by merging similar ones. However, we differentiate ourselves from ToMe in two significant ways:\nVideo Token Definition. ToMe uses joint spacetime tokens (2\u00d7 16\u00d7 16 cubes), while our TESTA defines frame tokens (representing entire frames) and patch tokens (16\u00d7 16 2D patches) for decoupled aggregation. This tailored token design is more efficient for modeling long-form videos.\nAggregation Method. ToMe performs global aggregation over all tokens, resulting in a complexity of O((T2 H 16 W 16 )\n2). This becomes impractical for long-form video and causes out-of-memory issues beyond 16 frames. In contrast, TESTA uses divided aggregation in time and space, reducing complexity to O(T 2 + (H16 W 16 )\n2). This allows efficient encoding of much longer videos (more than 128 frames under the same computation quota). The divided scheme also better captures spatial and temporal semantics, resulting in improved performance on long-form video understanding tasks (to be shown in \u00a7 4.7)."
        },
        {
            "heading": "3.3 Pre-training Objectives",
            "text": "We use the following three classic pre-training objectives, i.e., video-text contrastive loss, video-text matching loss, and captioning loss. Please refer to Appendix A for more details."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Implementation Details",
            "text": "To pre-train our TESTA model, we start by initializing it with the BLIP (12-layer ViT-B/16) checkpoint (Li et al., 2022), with the exception of the temporal attention, which is copied from the spatial attention weights. We use around 5M image-text and video-text pairs from two datasets for pre-training. See Appendix A for more details.\nFor downstream fine-tuning, we uniformly sample either 32 or 96 frames, each with a resolution of 224\u00d7 224 pixels (196 patches per frame with a patch size of 16). To achieve approximately a 50% reduction in computation cost, we employ different hyper-parameters for aggregation. Specifically, for 96-frame inputs, we set RT to 4 and RS to 8, while for 32-frame inputs, RT is 1 and RS is 12. We use geometry-based aggregation by default since it achieves better performance. Please refer to Appendix B for more fine-tuning details."
        },
        {
            "heading": "4.2 Downstream Task Setups",
            "text": "We finetune and evaluate TESTA on two downstream tasks of paragraph-to-video retrieval and\nlong-form VideoQA. For paragraph-to-video retrieval, we use four datasets: DiDeMo (Hendricks et al., 2017), QuerYD (Oncescu et al., 2020), ActivityNet Captions (Krishna et al., 2017), and Condensed Movie (Bain et al., 2020). For long-form VideoQA, we use ActivityNet-QA (Yu et al., 2019). The details of these datasets are shown in Appendix C."
        },
        {
            "heading": "4.3 Paragraph-to-Video Retrieval",
            "text": "Table 1 demonstrates the performance of TESTA on two challenging and under-explored paragraphto-video retrieval datasets, QuerYD and Condensed Movie, which involve videos with lengthy durations (over 200 seconds on average). For 32-frame video inputs, TESTA achieves Recall@1 of 77.0 on QuerYD and 21.5 on Condensed Movie, surpassing previous SOTA methods by 7.3 and 3.1, respectively. In terms of computational complexity, TESTA exhibits a significantly lower GFLOPs of 420 compared to Frozen (Bain et al., 2021) and VINDLU (Cheng et al., 2022). While LFVILA (Sun et al., 2022) operates with even fewer GFLOPs (298), it necessitates feature aggregation within a fixed local window, which can potentially undermine semantic integrity after concentration. In contrast, our model enables the adaptive merging of features with high similarity in the global scope, resulting in improved performance (+7.6\nR@1 on average compared to LF-VILA).\nGiven the importance of incorporating more input frames for long video understanding tasks, we finetune TESTA using 96-frame inputs and further promote R@1 to 83.4 on QuerYD and 24.9 on Condensed Movie. This exhibits strong scalability of our model (see Appendix D for a detailed analysis). Additionally, we report the results of TESTA without token aggregation, which serves as an upper bound for TESTA\u2019s performance. Although preserving full visual tokens yields higher recall, it requires 1.8 times more GLFOPs compared to TESTA. As the number of input frames increases from 32 to 96, the GFLOPs of TESTA w/o agg. exceed 2300, but the performance gain diminishes (only +0.8 R@1 on QuerYD). This indicates the superiority of our method in aggregating redundant tokens in long sequence inputs.\nTable 2 demonstrates model performance on DiDeMo and ActivityNet Caption, which consist of shorter videos (\u223c100 seconds on average) and are considered less challenging. For 32-frame inputs, TESTA with 5M pre-training data achieves 57.7 R@1 on DiDeMo, which even surpasses the models pre-trained with over 100M data. By increasing the number of frames to 96, TESTA achieves R@1 of 59.2 on DiDeMo and 53.7 on ActivityNet, outperforming previous SOTA methods by 2.7 and 2.6, respectively."
        },
        {
            "heading": "4.4 Long-Form Video Question-Answering",
            "text": "Table 3 showcases the performance of TESTA on ActivityNet-QA (using 96-frame). The accuracy of TESTA is 45.0%, which is 3.2% higher than the previous SOTA, Singularity (Lei et al., 2022). This demonstrates that our method eliminates redundant information while integrating crucial visual cues to accurately answer the posed questions."
        },
        {
            "heading": "4.5 Zero-shot Generalizability",
            "text": "In Table 4, we show the zero-shot performance of pre-trained CLIP4clip, BLIP, and TESTA on three datasets (32 frames). Although our TESTA is initialized by the BLIP checkpoint, it consistently outperforms BLIP (as well as CLIP4clip) after our pre-training, achieving average improvements of +14.1, +2.9, and +3.8 on QuerYD, DiDeMo, and ActivityNet respectively. This indicates our substantial gains on long-form video datasets are not solely due to the strong BLIP checkpoint, but also owing to our temporal modeling and pre-training on video data."
        },
        {
            "heading": "4.6 Ablation Study",
            "text": "We perform an extensive ablation study and analysis on various crucial components in our aggregation algorithm to examine their impacts.\nToken Aggregation v.s. Token Pruning. We first compare the performance and efficiency of\n-1\ntoken aggregation and token pruning (Rao et al., 2021). Regarding pruning, we calculate the importance score (Eq. (1)) for each token and prune the least important R tokens following previous methods (Goyal et al., 2020). We finetune our pre-trained model on QuerYD without token aggregation, then apply token aggregation and pruning in an off-the-shelf manner for test evaluation. The results are presented in the first block of Table 5. In comparison to the vanilla model (no aggregation), both pruning and aggregation decrease computation costs, with only 58% GFLOPs and 66% GPU memory. However, the performance degradation of our token aggregation is much smaller than that of pruning (\u22122.2 v.s. \u22128.4 in terms of average recall), suggesting that aggregation better preserves the valuable visual semantics within videos.\nAblation on the Aggregation Strategy. To investigate the effectiveness of different aggregation strategies, we report the performance of TESTA using importance-based and geometry-based aggregation methods. The results in the middle block of Table 5 show that the simplest geometry-based aggregation method achieves the best Recall@1 of 83.4, outperforming the other method by 3.2. This confirms our hypothesis that adjacent tokens exhibit greater similarity and should be assigned to separate sets for aggregation.\nAblation on the Aggregation Dimension. We compare the performance of three aggregation methods: (1) temporal only, (2) spatial only, and (3) both temporal and spatial. To ensure a roughly equal computational overhead, we adjust RS and RT accordingly. The results in the bottom block of Table 5 show that performing token aggregation on\na single dimension leads to excessive dilution of information, while the information in other dimensions becomes overly redundant. This imbalance hurts the performance of the model. Therefore, our approach, with incorporates both temporal and spatial aggregation, achieves the best outcomes.\nAdditionally, Appendix E discusses the impact of the number of reduced tokens RT and RS . Appendix F analyzes the properties of aggregated tokens by probing their similarity."
        },
        {
            "heading": "4.7 Comparison to Token Merging",
            "text": "We directly compare the performance of ToMe (Bolya et al., 2022) and TESTA by initializing both models from the BLIP pre-trained checkpoint and fine-tuning them on QuerYD. As we noted in \u00a7 3.2.3, due to the extremely high computational complexity of ToMe\u2019s global attention, increasing the number of input frames can lead to out-of-memory issues without token aggregation (w/o agg.). Therefore, we limit the number of input frames to 16. Besides, We set the hyperparameter R (number of reduced tokens) to ensure matched GFLOPs. Specifically, for ToMe, R = 197, while for TESTA, RT = 1 and RS = 2. The results in Table 6 illustrate TESTA\u2019s efficiency and effectiveness for long-form video understanding, which can be attributed to our tailored design for divided spatial-temporal modeling. In comparison to ToMe, our approach achieves higher recall with fewer GFLOPs, regardless of whether token aggregation is applied."
        },
        {
            "heading": "4.8 Visualization",
            "text": "Figure 3 provides a visualization of temporal and spatial aggregation on the DiDeMo dataset. TESTA effectively aggregates tokens with highly-similar semantics, demonstrating its strong interpretability. From a temporal perspective, TESTA aggregates a sequence of frames captured during continuous lens movement (first 3 frames). It also condenses similar frames of athletes waiting for the game (last 3 frames). From a spatial perspective, TESTA merges the patches belonging to the same scenes (e.g., sky, baseball park) and the same objects (e.g., billboard, back of the audience\u2019s head). More examples can be found in Appendix G.\nIn Figure 4, we further show that TESTA enables grounding of language to the aggregated visual to-\nkens (Ren et al., 2023b,a). Given the phrase query in the caption, it achieves the highest similarity of its oracle region formed by our aggregation, facilitating fine-grained alignment between phrases and regions."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we present TESTA, an efficient method for long-form video-language understanding. By aggregating similar frames and patches, TESTA effectively condenses video semantics and accelerates video encoding. Experimental results on paragraph-to-video retrieval and VideoQA tasks demonstrate that TESTA outperforms previous SOTA methods by a considerable margin.\nLimitations\nTo facilitate future research, we analyze the limitations and possible solutions in our work. (1) Due to limited computing resources, we do not use long-form video pre-training datasets such as HDVILA (Xue et al., 2021) or incorporate TESTA in pre-training. We believe long video pre-training with TESTA could greatly improve pre-training efficiency and obtain a video-language model with better performance. (2) For aggregation efficiency, we only use video-side features to merge visual tokens. We believe that leveraging text signals for aggregation could make the final encoded features more suitable for downstream tasks. (3) Our model training only uses coarse objectives such as VTC, VTM, and CAP (Eq. (2)-(4)) on video-text pairs. Considering TESTA can aggregate tokens into objects, scenes, events, etc., training with fine-grained alignment functions (Ren et al., 2021; Wang et al., 2022c) could help some tasks like action localization and video object detection (Zhukov et al., 2019; Real et al., 2017), on which we will perform more explorations in future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank all the anonymous reviewers for their constructive comments, and Rundong Gao and Lei Li for their valuable suggestions in preparing the manuscript. This work is supported in part by a Huawei Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author of this paper."
        },
        {
            "heading": "A Pre-training Details",
            "text": "A.1 Pre-training Datasets.\nWe perform pre-training on two datasets: WebVid2M (Bain et al., 2021) containing 2.5M video-text pairs and Conceptual Captions (CC3M) (Changpinyo et al., 2021) consisting of 3M image-text pairs. We include CC3M to improve spatial representations of videos as suggested by Li et al. (2021a). We duplicate images from CC3M for 8 times to make static videos. For WebVid-2M, we randomly sample 8 frames for each video instance. Because a small fraction of video and image URLs from the original datasets are no longer available, the total number of pre-training samples is around 5M. In the pre-training phase, we do not perform token aggregation since the number of frames in the pre-training video data is relatively small.\nA.2 Detailed Pre-training Objectives.\nWe use the following three classic pre-training objectives.\nVideo-Text Contrastive Loss. Given a batch of B video-text pairs, the contrastive objective aims to pull together the paired videos and texts while pushing apart the others with dissimilar semantics in the feature space. Let vi and ti represent the [CLS] feature of the video and text, respectively.\nThe video-to-text contrastive loss LV2T is:\nLV2T = \u2212 1\nB B\u2211 i=1 log exp(v\u22a4i ti/\u03c4)\u2211 j exp(v \u22a4 i tj/\u03c4) ,\nwhere \u03c4 is a learnable temperature parameter. Similarly, the text-to-video contrastive loss LT2V is:\nLT2V = \u2212 1\nB B\u2211 i=1 log exp(t\u22a4i vi/\u03c4)\u2211 j exp(t \u22a4 i vj/\u03c4) .\nThe video-text contrastive loss is defined as:\nLVTC = 1\n2 (LV2T + LT2V). (2)\nIn the implementation LVTC, the negative sample features are extracted from a queue of recent samples encoded by a momentum encoder (He et al., 2020). Moreover, a momentum distillation regularization loss (Li et al., 2021b) is added to LVTC for the sake of the potential positives in the negative pairs.\nVideo-Text Matching Loss. Video-text matching aims to predict whether a pair of video and text is matched or not. For the i-th video-text pair, we first obtain their joint video-text embedding of the [ENC] token from the video-grounded text encoder. We then use this embedding to generate a two-class probability pi, and calculate the videotext matching loss LVTM as:\nLVTM = 1\nB B\u2211 i=1 CE(yi,pi). (3)\nHere yi is a one-hot vector representing the groundtruth label, and CE(\u00b7, \u00b7) is the cross-entropy loss. In the implementation of LVTM, we apply online contrastive hard negative mining (Li et al., 2021b). We refer readers to the ALBEF paper (Li et al., 2021b) for a comprehensive introduction to momentum distillation and online contrastive hard negative mining.\nCaptioning Loss. This objective activates the video-grounded text decoder to predict the precise tokenized caption c in an autoregressive way:\nLCAP = \u2212 M\u2211 i=1 logP (ci | c<i, V ) , (4)\nwhere M is the text length. Combining Eq. (2)-(4), the overall objective can be formulated as:\nL = LVTC + LVTM + LCAP. (5)\nA.3 Hyperparameters.\nThe model is pre-trained for 5 epochs with the Adam (Kingma and Ba, 2015) with a weight decay of 5e-2. The batch size is 384 and the momentum queue size is 57600. The pre-training is conducted on four nodes with 32 NVIDIA V100 GPUs (32 GB memory per GPU) in total and each epoch lasts around 6 hours. The learning rate is linearly warmed up from 1e-6 to 5e-6 in the first 5000 steps and then gradually cosine decayed to 5e-7 in the remaining steps. Temporally consistent random spatial augmentation (Qian et al., 2021) is applied and mixed precision is used for efficient training."
        },
        {
            "heading": "B Fine-tuning Details",
            "text": "The downstream fine-tuning is conducted on 8 NVIDIA V100 GPUs. The learning rate is 1e-5 with a warmup ratio of 0.1. The batch size is 16 and the momentum queue size is 32. We fine-tune our model for 10 epochs with the Adam optimizer and a weight decay of 0.05. For paragraph-to-video retrieval, we use LVTC and LVTM as training objectives. For evaluating paragraph-to-video retrieval models, we select the top 128 candidates based on the video-text feature similarity and then rerank the selected candidates by their pairwise VTM scores. For video-QA, we use the cross-entropy loss for maximizing the generation probability of the correct answer and rank the candidates by their generation probabilities for evaluation."
        },
        {
            "heading": "C Downstream Datasets",
            "text": "We finetune and evaluate TESTA on two downstream tasks of paragraph-to-video retrieval and long-form VideoQA. The details of these datasets are shown in Table 7.\nFor paragraph-to-video retrieval, we use 4 datasets of DiDeMo (Hendricks et al., 2017), QuerYD (Oncescu et al., 2020), ActivityNet Captions (Krishna et al., 2017), and Condensed Movie (Bain et al., 2020). We evaluate text-tovideo retrieval, where the text acts as the query, in terms of R@k, which means the recall (%) of the target video through K retrieval efforts.\nFor long-form VideoQA, we use ActivityNetQA (Yu et al., 2019). The metric is accuracy (%)."
        },
        {
            "heading": "D Recall-GFLOPs Tradeoff of Various Pre-trained Models",
            "text": "In Figure 7, we analyze the tradeoff between recall and GFLOPs for various pre-trained models. The curve of our TESTA is located in the upper left corner, indicating that our model achieves a superior Recall-GFLOPs tradeoff compared to other pre-trained models.\nFurthermore, Figure 7 presents the model performance with different input frames. Surprisingly, increasing the number of input frames from 32 to 96 has minimal impact on the performance of Singularity (Lei et al., 2022) and Frozen (Bain et al., 2021), and even slightly reduced the recall of ALPRO (Li et al., 2021a) and VINDLU (Cheng et al., 2022). In contrast, our TESTA exhibits linear improvement in performance with the number of input frames, demonstrating superior scalability."
        },
        {
            "heading": "E Ablation on the Number of Reduced Tokens",
            "text": "In our TESTA (\u00a7 3.2), RT and RS specify the number of tokens to be reduced for the temporal and spatial aggregation module, separately. To investigate the influence of these two hyper-parameters, we vary the number of RT and RS , then report the average GFLOPs (blue bars) and recall (red star) on the QuerYD dataset. Figure 6 illustrates the results. On one hand, GFLOPs decrease linearly as R6 increases, indicating that increasing the reduced token number can improve the efficiency of video encoding. On the other hand, merging too many tokens with large R (e.g., RT = 10) will lose semantic information in the final encoded video representation, thus leading to a declined average recall.\nWe evaluate more cases with various RT and RS configurations, and plot the GFLOPs-Recall tradeoff in Figure 7. Based on these results and analysis, we determined the default configuration for our TESTA, i.e., RT = 4 & RS = 8 and for 96-frame inputs, and RT = 1 & RS = 12 for 32-frame inputs. This configuration helps our model achieve approximately a 50% reduction in computation cost without significant performance decline."
        },
        {
            "heading": "F Token Similarity Analysis",
            "text": "We probe the properties of the aggregated tokens by analyzing their similarity. In Figure 8, we count the average similarity between tokens from different blocks, different dimensions (frame tokens or patch tokens), and different aggregation results (aggregated or disaggregated).\nFor patch tokens (in orange), the overall similarity between them is large (higher than 0.5), indicating considerable spatial redundancy. Meanwhile, the aggregated patch tokens (in dark orange) have a very high similarity of 0.96, which ensures the semantic purity of the aggregated patch tokens.\n6Here we use R to refer to RT or RS for brevity.\nWhile for frame tokens (in blue), their similarity decreases as the number of blocks increases, which may yield aggregated frames with mixed and diverse semantics. Nevertheless, recall that our frame token is a pseudo token (\u00a7 3.2.1) obtained by averaging patch features, which does not elaborately model frame semantics. Therefore, compared to patch tokens, the representation of frame token and their similarity measure needs improvement, which we regard as future work."
        },
        {
            "heading": "G More Visualization of Aggregation",
            "text": "In this section, we provide more qualitative results of our TESTA for video-language understanding. Figure 9 shows another 4 case on the DiDeMo dataset. TESTA effectively aggregates tokens with highly-similar semantics, demonstrating its strong interpretability."
        }
    ],
    "title": "TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
    "year": 2023
}