{
    "abstractText": "Thanks in part to the availability of copious annotated resources for some entity categories, existing studies have achieved superior performance in multimodal named entity recognition (MNER). However, in the real-world scenario, it is infeasible to enumerate all entity categories in advance. Therefore, in this paper, we formulate a new few-shot multimodal named entity recognition (FewMNER) task, which aims to effectively locate and identify named entities for a text-image pair only using a small number of labeled examples. Further, we explore the merit of in-context learning (ICL) and propose a novel framework to deal with FewMNER, where three points are taken into account: i.e., converting visual modality, selecting useful examples, and designing an effective task demonstration. Specifically, we first employ an image caption model to convert images into textual descriptions, enabling large language models to absorb information from visual modality. Then, we use the ranking of the sum of similarity rankings from both text and image modalities to select k-nearest examples, which form a demonstration context. Finally, we utilize the MNER definition and the meaning of each entity category as effective instruction. Extensive experimental results demonstrate that our framework outperforms baselines under several few-shot settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenran Cai"
        },
        {
            "affiliations": [],
            "name": "Qianlong Wang"
        },
        {
            "affiliations": [],
            "name": "Bin Liang"
        },
        {
            "affiliations": [],
            "name": "Bing Qin"
        },
        {
            "affiliations": [],
            "name": "Min Yang"
        },
        {
            "affiliations": [],
            "name": "Kam-Fai Wong"
        },
        {
            "affiliations": [],
            "name": "Ruifeng Xu"
        }
    ],
    "id": "SP:23568a2d31a54e41be2d0857e4d61abe53311386",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "Hailin Chen",
                "Amrita Saha",
                "Shafiq Joty",
                "Steven C.H. Hoi."
            ],
            "title": "Learning label modular prompts for text classification in the wild",
            "venue": "Proceedings of EMNLP, pages 1677\u20131690.",
            "year": 2022
        },
        {
            "authors": [
                "Shuguang Chen",
                "Gustavo Aguilar",
                "Leonardo Neves",
                "Thamar Solorio."
            ],
            "title": "Can images help recognize entities? a study of the role of images for multimodal ner",
            "venue": "Proceedings of W-NUT, pages 87\u201396.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Lei Li",
                "Yunzhi Yao",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Good visual guidance make a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction",
            "venue": "Findings of",
            "year": 2022
        },
        {
            "authors": [
                "Damai Dai",
                "Yutao Sun",
                "Li Dong",
                "Yaru Hao",
                "Shuming Ma",
                "Zhifang Sui",
                "Furu Wei."
            ],
            "title": "Why can GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers",
            "venue": "Proceedings of ME-FoMo.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL, pages 4171\u2013 4186.",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Guangwei Xu",
                "Yulin Chen",
                "Xiaobin Wang",
                "Xu Han",
                "Pengjun Xie",
                "Haitao Zheng",
                "Zhiyuan Liu."
            ],
            "title": "Few-NERD: A few-shot named entity recognition dataset",
            "venue": "Proceedings of ACL, pages 3198\u20133213.",
            "year": 2021
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Lei Li",
                "Zhifang Sui."
            ],
            "title": "A survey on in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Fritzler",
                "Varvara Logacheva",
                "Maksim Kretov."
            ],
            "title": "Few-shot classification in named entity recognition task",
            "venue": "Proceedings of SAC, pages 993\u2013 1000.",
            "year": 2019
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A. Smith",
                "Mari Ostendorf."
            ],
            "title": "Incontext learning for few-shot dialogue state tracking",
            "venue": "Findings of EMNLP, pages 2627\u20132643.",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for GPT-3",
            "venue": "In Proceedings of DeeLIO,",
            "year": 2022
        },
        {
            "authors": [
                "Di Lu",
                "Leonardo Neves",
                "Vitor Carvalho",
                "Ning Zhang",
                "Heng Ji."
            ],
            "title": "Visual attention model for name tagging in multimodal social media",
            "venue": "Proceedings of ACL, pages 1990\u20131999.",
            "year": 2018
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "In Proceedings of EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Seungwhan Moon",
                "Leonardo Neves",
                "Vitor Carvalho."
            ],
            "title": "Multimodal named entity recognition for short social media posts",
            "venue": "Proceedings of NAACL, pages 852\u2013860.",
            "year": 2018
        },
        {
            "authors": [
                "Yuxin Peng",
                "Jinwei Qi",
                "Yuxin Yuan."
            ],
            "title": "Modality-specific cross-modal similarity measurement with recurrent attention network",
            "venue": "IEEE TIP, 27(11):5585\u20135599.",
            "year": 2018
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of NAACL, pages 2655\u2013 2671.",
            "year": 2022
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg",
            "year": 2022
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Hong Qian",
                "Xuanjing Huang",
                "Xipeng Qiu."
            ],
            "title": "Black-box tuning for language-model-as-a-service",
            "venue": "Proceedings of ICML, pages 20841\u201320855.",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "NIPS, 33:5776\u20135788.",
            "year": 2020
        },
        {
            "authors": [
                "Xinyu Wang",
                "Min Gui",
                "Yong Jiang",
                "Zixia Jia",
                "Nguyen Bach",
                "Tao Wang",
                "Zhongqiang Huang",
                "Kewei Tu."
            ],
            "title": "Ita: Image-text alignments for multi-modal named entity recognition",
            "venue": "Proceedings of NAACL, pages 3176\u20133189.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Yang",
                "Arzoo Katiyar."
            ],
            "title": "Simple and effective few-shot named entity recognition with structured nearest neighbor learning",
            "venue": "Proceedings of EMNLP, pages 6365\u20136375.",
            "year": 2020
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang",
                "Li Yang",
                "Rui Xia."
            ],
            "title": "Improving multimodal named entity recognition via entity span detection with unified multimodal transformer",
            "venue": "Proceedings of ACL, pages 3342\u20133352.",
            "year": 2020
        },
        {
            "authors": [
                "Dong Zhang",
                "Suzhong Wei",
                "Shoushan Li",
                "Hanqian Wu",
                "Qiaoming Zhu",
                "Guodong Zhou."
            ],
            "title": "Multimodal graph fusion for named entity recognition with targeted visual guidance",
            "venue": "Proceedings of AAAI, pages 14347\u201314355.",
            "year": 2021
        },
        {
            "authors": [
                "Qi Zhang",
                "Jinlan Fu",
                "Xiaoyu Liu",
                "Xuanjing Huang."
            ],
            "title": "Adaptive co-attention network for named entity recognition in tweets",
            "venue": "Proceedings of AAAI, pages 5674\u20135681.",
            "year": 2018
        },
        {
            "authors": [
                "Xin Zhang",
                "Jingling Yuan",
                "Lin Li",
                "Jianquan Liu."
            ],
            "title": "Reducing the bias of visual objects in multimodal named entity recognition",
            "venue": "Proceedings of WSDM, page 958\u2013966.",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Zhang",
                "Shi Feng",
                "Chenhao Tan."
            ],
            "title": "Active example selection for in-context learning",
            "venue": "Proceedings of EMNLP, pages 9134\u20139148. A Appendix A.1 Impact of Different Output Format",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multimodal Named Entity Recognition (MNER) aims to identify named entities of different categories from the text with extra image assistance. Consider the example in Figure 1(a), we need to recognize three named entities from the text,\n\u2217 Corresponding author\n\"Suarez\" (PER), \"Barcelona\" and \"La Liga\" (ORG), to finish the MNER task. Most existing methods commonly employ pre-trained models followed by fine-tuning to accomplish the MNER task and achieve superior performance (Lu et al., 2018; Yu et al., 2020; Zhang et al., 2021; Chen et al., 2022b). In terms of existing research efforts, their superior performance generally relies on sufficient annotated data, which is time-consuming and laborintensive. In addition, in practice, entity categories will continue to emerge rather than remain fixed. Therefore, it is impractical to define all entity categories in advance.\nTo address these issues, motivated by the fewshot Named Entity Recognition (FewNER) task that involves learning unseen entity categories from a small number of labeled examples (Fritzler et al., 2019), we extend the MNER task to the few-shot field, named the few-shot multimodal named entity recognition (FewMNER) task, which aims to locate and identify named entities for a text-image pair only using a small number of labeled examples. As illustrated in Figure 1(b), the 2-shot MNER task\naims to accomplish the MNER task based on two labeled text-image pair examples.\nFurther, to address the FewMNER task, we propose leveraging the powerful in-context learning (ICL) capability of the large language model (LLM). Specifically, we argue that this paradigm can provide a promising direction for solving the FewMNER task by learning from a few examples in context without training. However, there are three problems while solving FewMNER using the in-context learning paradigm: (i) For the FewMNER task, each sample is represented by textual and visual modalities, while the input of LLM is limited to natural language. Thus, we first require seeking ways to convert visual modality into natural language form. (ii) The key to performing ICL is to select a few examples to form a demonstration context. Although there are some example selection studies (Chen et al., 2022a; Min et al., 2022) targeting text classification tasks, selecting some useful examples for ICL in multimodal scenarios has not been approached. (iii) In addition, good demonstration designing precisely is essential to obtain satisfactory performance. Unlike simple classification tasks, the task instruction and output format of MNER need to be constructed according to the extractive nature.\nTo apply ICL to solve the FewMNER task, we propose corresponding solutions to the abovementioned problems. First, we employ an image caption model (Wang et al., 2022a) to generate textual descriptions from images, which not only converts images into natural language form but also aligns image features into the text space. Second, for selecting examples, we design an efficient sorting algorithm based on image and text similarity ranks, which can mitigate the similarity bias caused by different modality models. Then, we utilize this algorithm to select top-k examples with the highest similarity to the current test sample. Third, the demonstration design consists of two parts: instruction construction and demonstration construction (Dong et al., 2023). The former aims to inform LLM about the current task. To provide more detailed information, we add the description of entity category meaning to the instruction. The latter is to define the demonstration template and order selected examples into the demonstration template. For the demonstration template, it consists of three components: image description, sentence, and output, where output is the label information.\nThen, we pack selected top-k examples into the demonstration template in ascending order of similarity rank, such that the most similar example is nearest to the current test sample. Finally, we concatenate instruction, demonstration, and the test sample as the input and feed it into LLM to obtain the prediction output.\nThe contributions of this paper are as follows:\n\u2022 We are the first to extend the MNER task to the few-shot field and explore the potential of the in-context learning paradigm for this task.\n\u2022 To adapt the in-context learning paradigm to the FewMNER task, we address three related problems and propose a framework to accomplish this task.\n\u2022 Through comparison with previous competitive methods, our framework exhibits a significant advantage in this task. We also conduct extensive analysis experiments to reveal the impact of various factors on its performance and provide novel insights for future research."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Multimodal Named Entity Recognition",
            "text": "Multimodal Named Entity Recognition (MNER) aims to discover named entities in the unstructured text and classify them into pre-defined types with the help of an auxiliary image. Existing studies could be divided into two categories: cross-modal interaction-based methods and image conversionbased methods. The former tends to carry out crossmodal interaction using an attention mechanism and to combine textual representation with image representation for MNER. For example, some studies (Lu et al., 2018; Moon et al., 2018; Zhang et al., 2018) first applied LSTM and CNN to extract text and image features, respectively. Then attention is adopted to fuse two modal features to derive textual representation in order to complete entity labeling. In addition to modeling the interaction between text and images, a few studies (Chen et al., 2022b; Zhang et al., 2021) leveraged the semantic correlation between tokens and object regions to derive the final token representations for MNER. The latter (Chen et al., 2021; Wang et al., 2022b) first aims at converting images and extracting textualized information from them such as captions in order to align image features to the text space. Then this textualized information derived from an\nimage is concatenated with the input text to yield the final token representation for completion entity recognition. Despite their promising results, they generally depend on a large amount of annotated data, which is inadequate in generalizing the ability to locate and identify entities to unseen entity categories."
        },
        {
            "heading": "2.2 In-Context Learning",
            "text": "With the scaling of the pre-trained model from 110M parameters (Devlin et al., 2019) to over 500B parameters (Smith et al., 2022), the ability of the model has been greatly improved, especially the understanding ability, fluency, and quality of generation. Many studies have demonstrated that large language models (LLMs) have shown an in-context learning ability (Brown et al., 2020), which is learning from a few context examples without training. Although various LLMs (e.g., GPT-3, ChatGPT) have been trained, they are all closed-source and only accessible internally or via paid API services. How to effectively utilize the in-context learning ability of LLMs is an important question. Recently, some studies (Sun et al., 2022; Hu et al., 2022; Zhang et al., 2022) treat LLMs as a service and utilize the in-context learning ability to finish the few-shot and even zero-shot tasks."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Task Definition",
            "text": "Given a text t and its correlated image v as input, the fewMNER task only applies a small number of\nlabeled examples to detect a series of entities in t and classify them into pre-defined categories.\nFollowing most existing in-context learning work (Dong et al., 2023), we formulate this task as a generation task. A large language model M takes a generation sequence of the maximum score as prediction output conditioning on the context C. For the k-shot MNER task, C contains instruction I and k examples, where C = {I, s (v1, t1, y1) , . . . , s (vk, tk, yk)}, s is demonstration template and {y1, . . . , yk} is a set of free text phrases as the label. Therefore, for the given test sample x = {v, t}, the prediction output y\u0302 can be expressed as:\ny\u0302 = maxPM(y | C, x). (1)"
        },
        {
            "heading": "3.2 Overall Architecture",
            "text": "Figure 2 illustrates the overall architecture of our framework for the 2-shot MNER task, which contains three main components: (1) Retrieve example module, which utilizes k-nearest neighbors of text and image to select examples. (2) Demonstration designing module, which includes instruction construct and demonstration construct. (3) Predict module, which applies a large language model to generate prediction results without training."
        },
        {
            "heading": "3.3 Retrieve Example Module",
            "text": "Previous works (Rubin et al., 2022; Liu et al., 2022) have demonstrated that selecting similar examples to the current test sample can enhance the performance of LLM. However, these methods only con-\nsider textual similarity scores, which are insufficient for the FewMNER task due to the multimodal nature of FewMNER. Besides, different modality models introduce bias (i.e., different similarity score distributions for text and image (Peng et al., 2018)). To this end, we propose an efficient selection method based on text and image similarity ranks, which can mitigate the bias described above."
        },
        {
            "heading": "3.3.1 Image Similarity Rank",
            "text": "Given an test image vtest and candidate set D, where D contains N text-image pair and D = {(v1, t1), (v2, t2), ..., (vN , tN )}. We first adopt the pre-trained vision model ViT (Dosovitskiy et al., 2021) to obtain the representation of the whole image, including test image vtest and candidate image set Dv = {v1, v2, ..., vN}:\nHv = ViT(vtest), (2)\nV = ViT(Dv), (3)\nwhere Hv \u2208 Rdh is the image representation of vtest and V \u2208 RN\u00d7dh is the embedding matrix of Dv. Then, we calculate the cosine similarity of the test image representation Hv and the image representation of the whole candidate set V, and record the rank of each candidate image set Dv.\nSv = Cosine(Hv,V), (4)\nRv = Rank(Sv), (5)\nwhere Sv \u2208 RN , Rv \u2208 RN and Riv \u2208 [1, N ]."
        },
        {
            "heading": "3.3.2 Text Similarity Rank",
            "text": "Given a test text ttest, we first utilize the pretrained language model such as MiniLM (Wang et al., 2020) as text extractor to map text ttest and candidate text set Dt = {t1, t2, ..., tN} into a dwdimensional embedding:\nHt = MiniLM(ttest), (6)\nT = MiniLM(Dt), (7)\nwhere Ht \u2208 Rdw is the sentence representation of ttest and T \u2208 RN\u00d7dw is the embedding matrix of Dt. Then, we calculate the cosine similarity of the test text representation Ht and the text representation of the whole candidate set T, and record the rank of each candidate text set Dt.\nSt = Cosine(Ht,T), (8)\nRt = Rank(St), (9)\nwhere St \u2208 RN and Rt \u2208 RN and Rit \u2208 [1, N ]."
        },
        {
            "heading": "3.3.3 Sorting Based on Both Similarity Ranks",
            "text": "According to the similarity rank results of image and text modalities Rv and Rt, we sum two rankings and sort them to get the final ranking result.\nR = Rank(Rv +Rt), (10)\nwhere R \u2208 RN and Ri \u2208 [1, N ]. Compared with sorting based on the sum of image and text similarity scores, sorting based on both similarity ranks considers the bias introduced by different modality pre-trained models. Through analyzing the distribution of image similarity scores Sv and text similarity scores St, we observe that the image similarity scores are generally higher than the text similarity scores. Sorting based on both similarity ranks can effectively address this issue. Finally, we take topk examples with the highest similarity ranking as selected examples.\n\u03c3 = Top-K(R), (11)\nwhere \u03c3 are the indices of top-k similarity ranking, and \u03c3={\u03c31, ..., \u03c3k}."
        },
        {
            "heading": "3.4 Demonstration Designing Module",
            "text": "Following the in-context learning paradigm (Dong et al., 2023), it consists of two parts: instruction construction and demonstration construction."
        },
        {
            "heading": "3.4.1 Instruction Construction",
            "text": "We use the definition of the MNER task as the instruction, which helps LLM understand the current task and is shown as follows: You are a smart and intelligent Multimodal Named Entity Recognition (MNER) system. I will provide you the definition of the entities you need to extract, the sentence from where your extract the entities, the image description from image associated with sentence and the output format with examples.\nTo provide more detailed information for LLM, we describe the meaning of each entity category as follows: 1.PERSON: Short name or full name of a person from any geographic region; 2.ORGANIZATION: An organized group of people with a particular purpose, such as a business or a government department; 3.LOCATION: Names of any geographic location, like cities, countries, continents, districts, etc; 4.MISCELLANEOUS: Name entities that do not belong to the previous three groups PERSON, ORGANIZATION, and LOCATION.\nFinally, we concatenate the task and entity category definitions as instruction I.\nI = {task definition, category definition}. (12)"
        },
        {
            "heading": "3.4.2 Demonstration Construction",
            "text": "As shown in the demonstration designing module in Figure 2, the demonstration template contains image description, sentence, and output. To obtain the image description, we employ the OFA model (Wang et al., 2022a) to convert images into text captions. The sentence is the original text input. The output is initially constructed by concatenating the entity and the category, taking the test sample in Figure 2 as an example, the initial output is \"World Cup is miscellaneous.\". However, this leads to disordered outputs, such as predicting categories that are not among the four predefined ones, despite the instruction specifying them. To address this issue, we adopt a dictionary-based format that explicitly defines the output structure as {\"PER\": [], \"ORG\": [], \"LOC\": [], \"MISC\": []}. We find that this approach effectively standardizes the output format1.\nFinally, the top-k selected examples are fed into the demonstration template in ascending order such that the most similar example is nearest to the current test sample.\nD = {s(D\u03c3kv ,D \u03c3k t ,D\u03c3ky ), ..., s(D\u03c31v ,D \u03c31 t ,D\u03c31y )}.\n(13)"
        },
        {
            "heading": "3.5 Predict module",
            "text": "Given the instruction and demonstration, we concatenate them into the whole context C. Then, we feed context C and test sample {vtest, ttest} into LLM and select the most probable generated sequence as the predicted output.\nC = {I,D}, (14) y\u0302 = maxPLLM(y | C, s(vtest, ttest)). (15)\nFinally, we decode the prediction output y\u0302 into a list according to the dictionary format and complete the k-shot MNER task."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We conduct experiments on two public multimodal named entity recognition (MNER) benchmark datasets, Twitter2015 and Twitter2017. Two MNER datasets are constructed by (Yu et al., 2020). Each example consists of a text and an associated image in the two MNER datasets. The statistics of two MNER datasets are shown in Table 1.\n1To support this statement, we perform experiments comparing the two different output formats (detailed results in Appendix A.1)."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "We randomly select 10, 50, 100, and all examples from the training set of two MNER datasets, respectively, and denote them as D10, D50, D100, and Dall sets. In this paper, we compare fine-tuning and few-shot methods with our framework. For fine-tuning methods, we utilize examples of the entire set to train models. For few-shot methods, we apply examples of the entire set as the train support and query sets and construct 4-way k-shot setting for training. Here, the support and query sets of few-shot baselines used for training and inference have the same entity categories. For our framework, we select the k examples from the entire set to perform the k-shot MNER task without training."
        },
        {
            "heading": "4.3 Model Settings",
            "text": "For the feature extraction model, we employ clipvit-base-patch322 and all-MiniLM-L6-v23 to embed each image and text as a 512-dimensional and 768-dimensional embedding, respectively. For the image caption process, we employ the ofa-imagecaption-coco-large-en4 model to generate image description. For LLM, we choose the gpt-3.5-turbo (i.e., ChatGPT) as the backbone of our framework."
        },
        {
            "heading": "4.4 Comparison Models",
            "text": "For fine-tuning methods, we adopt the following baselines: (1) UMT (Yu et al., 2020), which employs a transformer layer with a multimodal interaction module to capture the inter-modality dynamics between tokens and images for MNER; (2) UMGF (Zhang et al., 2021), which applies a unified multimodal graph approach to capture semantic relationships between tokens and visual objects and performs entity labeling; (3) HVPNeT (Chen et al., 2022b), which utilizes a hierarchical visual prefix fusion network for the visual-enhanced entity; (4)\n2https://huggingface.co/openai/ clip-vit-base-patch32\n3https://huggingface.co/sentence-transformers/ all-MiniLM-L6-v2\n4https://modelscope.cn/models/damo/ofa_ image-caption_coco_large_en\nDebiasCL (Zhang et al., 2023), which proposes a de-bias contrastive learning-based approach for MNER and studies modality alignment enhanced by cross-modal contrastive learning.\nFor the few-shot methods, we apply the following baselines: (1) ProtoBERT (Ding et al., 2021), which employs a prototypical network with a backbone of BERT encoder; (2) StructShot (Yang and Katiyar, 2020), which uses token-level nearest neighbor classification and structured inference."
        },
        {
            "heading": "4.5 Main Results",
            "text": "We report the main experimental results in Table 2 and draw the following conclusions.\n(1) Our framework significantly outperforms the fine-tuning methods on D10, D50 and D100 sets (except for HVPNeT on D100 set of Twitter2015). For example, in terms of F1, our framework outperforms UMT by 55.78% and 65.30%, UMGF by 55.23% and 67.46%, HVPNeT by 36.40% and 35.53%, and DebiasCL by 54.83% and 65.75% on D10 set of two MNER datasets. These show that\nour framework effectively exploits the in-context learning potential of large language models.\n(2) Compared with few-shot baselines such as ProtoBERT and StructShot, our framework achieves the best results in all few-shot settings (i.e., 2-shot, 4-shot, and 8-shot). This indicates that methods based on in-context learning are preferable in the FewMNER task, and thus exploring congenial methods based on in-context learning can lead to improved performance in this task.\n(3) We observe that the performance of our framework improves as the size of D increases. This is because a larger retrieval set provides more opportunities for the test sample to find similar examples.\n(4) Our framework still lags behind the finetuning methods under the Dall set."
        },
        {
            "heading": "4.6 Ablation Study",
            "text": "To analyze the impact of instruction and demonstration on the performance of our framework, we conduct ablation experiments and report detailed\nresults in Table 3. The results reveal that our framework achieves the best performance when combining instruction and demonstration, which suggests that both components are beneficial. Compared with removing instruction, removing demonstration leads to more performance degradation. This shows that the demonstration is crucial for our framework.\nFurthermore, we also conduct ablation experiments on the way of selecting examples. Sorting based on the sum of image and text similarity ranks outperforms sorting based on the sum of image and text similarity scores (i.e., w/ score). This is because the latter does not account for the bias introduced by different modality models."
        },
        {
            "heading": "4.7 Analysis",
            "text": "Image Modality Analysis. To explore the effect of image description on the FewMNER task, we conduct experiments with single-modality (i.e., text) and multi-modality (i.e., text and image) and show results in Table 4. For a fair comparison, both settings use the same instruction and demonstration, but the single-modality setting discards the image description. We observe that the multimodality setting outperforms the single-modality setting, especially on the PER category. The reason is that the image caption model tends to generate sentences related to people, which provide useful cues for identifying PER category entities.\nDifferent Examples Analysis. To analyze the impact of different examples, we compare three methods of examples selection (i.e., similar, dissimilar, and random) in the 4-shot setting on D50 set. The similar method selects examples that have the highest similarity to the test sample, while the dissimilar method selects examples that have the lowest similarity. The random method selects examples uniformly at random. The results are shown in Table 5. We observe that the similar method achieves the best performance, followed by the random method, and the dissimilar method performs the worst. This indicates that selecting similar examples to form the demonstration is beneficial for the FewMNER task.\nImpact of Examples Sort. To investigate the impact of example sort on performance, we utilize the same examples to compare three methods of sorting (i.e., ascending, descending, and random) in the 4-shot setting on D50 set. The results are shown in Table 6. The ascending sort method, which places the most similar example nearest to the current test sample, outperforms the other methods. This suggests that ascending sort examples by their similarity can improve performance. The reason is that the most similar example can provide more relevant information for the current prediction.\nImpact of the Number of Examples. To explore the impact of the number of examples on perfor-\nmance, we conduct experiments with different the number of examples (i.e., 0, 2, 4, 8, 16, 32-shot) on D50 set and show results in Table 9. On the Twitter2017 dataset, we observe that the F1 generally increases with the number of examples, and our framework achieves the best score with 32 examples. Comparing 0-shot with 32-shot, the latter outperforms the former by 13.84% in F1.\nRecently, some works have attempted to explain the ICL capability of LLMs. Dai et al. (2023) interprets language models as meta-optimizers and views ICL as a form of implicit fine-tuning. This is consistent with our findings that more examples can enhance the performance, as more examples lead to more optimization steps. On the Twitter2015 dataset, we observe a similar trend as on the Twitter2017 dataset, but our framework achieves the best score with 16 examples. The reason is that increasing the number of examples may introduce more dissimilar examples. These fluctuations indicate that more examples can have a positive effect on performance if they are sufficiently similar.\nError analysis. In this section, we aim to analyze the factors that affect the performance of our framework. As shown in Table 7, we report\nthe performance of four categories in 2-shot, 4- shot, and 8-shot settings on D50 set. We find that our framework performs poorly on MISC category, which is significantly lower than PER, LOC, and ORG categories. The reason is that MISC is a miscellaneous category, defined as name entities that do not belong to the previous three categories. The annotation of MISC category entities depends on the preference of annotators. Relying only on the in-context learning ability of LLM and a few examples is not sufficient to learn this preference.\nMoreover, we analyze the boundary error and category error and perform a detailed analysis of wrong predictions. We classify wrong predictions into two types: boundary errors and category errors5. We count the number of errors for each category and report results in Table 8. We observe that increasing the number of examples significantly reduces boundary errors. Specifically, comparing 2-shot with 8-shot, the latter reduces the proportion of boundary errors by 3.80% and 5.09% on two datasets, respectively. Besides, increasing the number of examples does not reduce category errors. This is an interesting finding and demonstrates that more examples mainly improve the boundary ability of ICL, rather than category ability."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we formulate multimodal named entity recognition as a few-shot learning problem,\n5When the predicted entity is boundary-misspecified, we classify it as a boundary error. When the boundary of the entity is completely correct, but the category is incorrectly identified, we classify it as a category error.\nnamed few-shot multimodal named entity recognition (FewMNER), to extend entity detection to unseen entity categories. To tackle FewMNER, we propose a framework based on in-context learning by addressing three problems. Experimental results show that our framework outperforms baselines in several few-shot settings. Moreover, we conduct analysis experiments and find that selecting similar examples, sorting them in ascending order, and using more examples improve the performance of in-context learning. We also perform error analysis and observe that increasing the number of examples reduces boundary errors but not category errors. These results provide novel insights for future work on the FewMNER task.\nLimitations\nAlthough the proposed framework significantly outperforms several strong baselines on the FewMNER task, it suffers from the following limitations:\n\u2022 In the case of using the full amount of data, our framework still lags behind fine-tuning methods. We can utilize some data to domain fine-tune LLM before applying our framework, which may further improve the performance of in-context learning on the FewMNER task. This is a direction for future efforts.\n\u2022 Unfortunately, due to API limitations, we are unable to obtain results from the more powerful GPT4 model, which has a multimodal function. Further experiments and analysis are required.\nWe believe that addressing the above limitations can further improve the performance of our framework."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially supported by the National Natural Science Foundation of China 62006062 and 62176076), Natural Science Foundation of GuangDong 2023A1515012922, Shenzhen Foundational Research Funding JCYJ20200109113441941, Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies 2022B1212010005."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Impact of Different Output Format\nTo explore the influence of different output formats, we perform experiments comparing dictionary-based output format and sentence-based output format and show results in Table 10-11. We can obverse that compared with the sentencebased output format, the dictionary-based output format achieves 4.64% and 8.17% higher F1 in 4-shot setting on D50 set of Twitter2015 and Twitter2017 datasets, respectively, which demonstrates the dictionary-based output format is more suitable for in-context learning paradigm on ChatGPT. Perhaps this is related to the pre-training of ChatGPT on code, as this code-like output format is more comprehensible for ChatGPT.\nA.2 Perturbation Analysis We conduct a perturbation analysis to examine the effect of label noise on in-context learning. We randomly replace different proportions of labels with other labels in the examples and measure the impact of label noise on the F1 score of our framework. The results are shown in Table 12. As expected, the F1 score decreases as the label replacement ratio increases. When all the example labels are replaced (i.e., 100%), the F1 score drops by 3.94% and 10.01% on two datasets, respectively. However, the F1 score does not drop to zero even when all the example labels are replaced, which indicates that the LLM has a strong intrinsic ability for this task and is not completely lost by the examples.\nA.3 Analyze Our Framework from Task Perspective\nWe here provide an analysis of why our framework performs well from the perspective of the FewMNER task. First, we model the FewMNER task as a generative task, which is more direct than the baseline methods that model the MNER task as a sequence labeling task. Sequence labeling methods not only require fine-grained prediction but also need to learn the association between different labels. Second, our framework utilizes image caption as a bridge to alleviate the gap between image modality and text modality, which can skip the process of information interaction between different modalities. Third, the four category concepts of people, location, organization, and miscellaneous in FewMNER tasks can be understood by LLM. Our framework can be readily adapted to the task objectives. For these reasons, our framework achieves good performance on the FewMNER task under the condition of small samples."
        }
    ],
    "title": "In-context Learning for Few-shot Multimodal Named Entity Recognition",
    "year": 2023
}