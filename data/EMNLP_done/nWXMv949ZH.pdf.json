{
    "abstractText": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, LOGICLM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a selfrefinement module, which utilizes the symbolic solver\u2019s error messages to revise symbolic formalizations. We demonstrate LOGIC-LM\u2019s effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, LOGICLM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-ofthought prompting. Our findings suggest that LOGIC-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Liangming Pan"
        },
        {
            "affiliations": [],
            "name": "Alon Albalak"
        },
        {
            "affiliations": [],
            "name": "Xinyi Wang"
        },
        {
            "affiliations": [],
            "name": "William Yang Wang"
        }
    ],
    "id": "SP:424d0f20d0c30b1f45e0793a48d893304210180c",
    "references": [
        {
            "authors": [
                "Lise Getoor"
            ],
            "title": "Hinge-loss markov random fields",
            "year": 2017
        },
        {
            "authors": [
                "Le-Wen Cai",
                "Wang-Zhou Dai",
                "Yu-Xuan Huang",
                "YuFeng Li",
                "Stephen H. Muggleton",
                "Yuan Jiang."
            ],
            "title": "Abductive learning with ground knowledge base",
            "venue": "Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI), pages",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen."
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "CoRR, abs/2211.12588.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "CoRR, abs/2304.05128.",
            "year": 2023
        },
        {
            "authors": [
                "Peter Clark",
                "Oyvind Tafjord",
                "Kyle Richardson."
            ],
            "title": "Transformers as soft reasoners over language",
            "venue": "Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI), pages 3882\u20133890.",
            "year": 2020
        },
        {
            "authors": [
                "William F Clocksin",
                "Christopher S Mellish."
            ],
            "title": "Programming in PROLOG",
            "venue": "Springer Science & Business Media.",
            "year": 2003
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "CoRR, abs/2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Leonardo Mendon\u00e7a de Moura",
                "Nikolaj S. Bj\u00f8rner."
            ],
            "title": "Z3: an efficient SMT solver",
            "venue": "Proceedings of the 14th International Conference of Tools and Algorithms for the Construction and Analysis of Systems (TACAS), volume 4963 of Lecture Notes in Computer",
            "year": 2008
        },
        {
            "authors": [
                "Leonardo Mendon\u00e7a de Moura",
                "Soonho Kong",
                "Jeremy Avigad",
                "Floris van Doorn",
                "Jakob von Raumer."
            ],
            "title": "The lean theorem prover (system description)",
            "venue": "Proceedings of the 25th International Conference on Automated Deduction (ICAD), volume 9195 of",
            "year": 2015
        },
        {
            "authors": [
                "Herbert B Enderton."
            ],
            "title": "A mathematical introduction to logic",
            "venue": "Elsevier.",
            "year": 2001
        },
        {
            "authors": [
                "Bruce Frederiksen."
            ],
            "title": "Applying expert system technology to code reuse with pyke",
            "venue": "PyCon: Chicago.",
            "year": 2008
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "PAL: program-aided language models",
            "venue": "Proceedings of the International Conference on Machine Learning (ICML), volume 202,",
            "year": 2023
        },
        {
            "authors": [
                "Olga Golovneva",
                "Moya Chen",
                "Spencer Poff",
                "Martin Corredor",
                "Luke Zettlemoyer",
                "Maryam Fazel-Zarandi",
                "Asli Celikyilmaz."
            ],
            "title": "ROSCOE: A suite of metrics for scoring step-by-step reasoning",
            "venue": "Proceedings of the 11th International Conference on",
            "year": 2023
        },
        {
            "authors": [
                "Nitish Gupta",
                "Kevin Lin",
                "Dan Roth",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "Neural module networks for reasoning over text",
            "venue": "Proceedings of the 8th International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Shafiq R. Joty",
                "Alexander R. Fabbri",
                "Wojciech Kryscinski",
                "Xi Victoria Lin",
                "Caiming Xiong",
                "Dragomir Radev."
            ],
            "title": "FOLIO: natural language reasoning with first-order logic",
            "venue": "CoRR, abs/2209.00840.",
            "year": 2022
        },
        {
            "authors": [
                "Joy He-Yueya",
                "Gabriel Poesia",
                "Rose E Wang",
                "Noah D Goodman."
            ],
            "title": "Solving math word problems by combining language models with symbolic solvers",
            "venue": "CoRR, abs/2304.09102.",
            "year": 2023
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 1049\u20131065.",
            "year": 2023
        },
        {
            "authors": [
                "Albert Qiaochu Jiang",
                "Sean Welleck",
                "Jin Peng Zhou",
                "Timoth\u00e9e Lacroix",
                "Jiacheng Liu",
                "Wenda Li",
                "Mateja Jamnik",
                "Guillaume Lample",
                "Yuhuai Wu."
            ],
            "title": "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Diaz",
                "Salvador Abreu"
            ],
            "title": "Fifty years",
            "year": 2022
        },
        {
            "authors": [
                "Vipin Kumar"
            ],
            "title": "Algorithms for constraint",
            "year": 1992
        },
        {
            "authors": [
                "Zhou",
                "Yue Zhang"
            ],
            "title": "Evaluating the logi",
            "year": 2023
        },
        {
            "authors": [
                "Jiayuan Mao",
                "Chuang Gan",
                "Pushmeet Kohli",
                "Joshua B. Tenenbaum",
                "Jiajun Wu."
            ],
            "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "venue": "Proceedings of the 7th International Conference on Learning",
            "year": 2019
        },
        {
            "authors": [
                "Kostas S. Metaxiotis",
                "Dimitris Askounis",
                "John E. Psarras."
            ],
            "title": "Expert systems in production planning and scheduling: A state-of-the-art survey",
            "venue": "Journal of Intelligent Manufacturing, 13(4):253\u2013260.",
            "year": 2002
        },
        {
            "authors": [
                "Chess",
                "John Schulman."
            ],
            "title": "Webgpt: Browserassisted question-answering with human feedback",
            "venue": "CoRR, abs/2112.09332.",
            "year": 2021
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "der",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)",
            "year": 2022
        },
        {
            "authors": [
                "der",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Lawrence C. Paulson."
            ],
            "title": "Isabelle - A Generic Theorem Prover (with a contribution by T",
            "venue": "Nipkow), volume 828 of Lecture Notes in Computer Science. Springer.",
            "year": 1994
        },
        {
            "authors": [
                "David Poole",
                "Alan K. Mackworth."
            ],
            "title": "Artificial Intelligence - Foundations of Computational Agents",
            "venue": "Cambridge University Press.",
            "year": 2010
        },
        {
            "authors": [
                "Connor Pryor",
                "Charles Dickens",
                "Eriq Augustine",
                "Alon Albalak",
                "William Yang Wang",
                "Lise Getoor."
            ],
            "title": "Neupsl: Neural probabilistic soft logic",
            "venue": "Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI), pages 4145\u20134153.",
            "year": 2023
        },
        {
            "authors": [
                "Danilo Neves Ribeiro",
                "Shen Wang",
                "Xiaofei Ma",
                "Henghui Zhu",
                "Rui Dong",
                "Deguang Kong",
                "Juliette Burger",
                "Anjelica Ramos",
                "Zhiheng Huang",
                "William Yang Wang",
                "George Karypis",
                "Bing Xiang",
                "Dan Roth"
            ],
            "title": "STREET: A multi-task struc",
            "year": 2023
        },
        {
            "authors": [
                "Danilo Neves Ribeiro",
                "Shen Wang",
                "Xiaofei Ma",
                "Henry Zhu",
                "Rui Dong",
                "Deguang Kong",
                "Juliette Burger",
                "Anjelica Ramos",
                "William Yang Wang",
                "Zhiheng Huang",
                "George Karypis",
                "Bing Xiang",
                "Dan Roth"
            ],
            "title": "2023b. STREET: A multi-task structured reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Richardson",
                "Pedro M. Domingos."
            ],
            "title": "Markov logic networks",
            "venue": "Machine Learning, 62(12):107\u2013136.",
            "year": 2006
        },
        {
            "authors": [
                "John Alan Robinson."
            ],
            "title": "A machine-oriented logic based on the resolution principle",
            "venue": "The Journal of the ACM (JACM), 12(1):23\u201341.",
            "year": 1965
        },
        {
            "authors": [
                "Abulhair Saparov",
                "He He."
            ],
            "title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "venue": "Proceedings of the 11th International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Murray Shanahan."
            ],
            "title": "Talking about large language models",
            "venue": "CoRR, abs/2212.03551.",
            "year": 2022
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface",
            "venue": "CoRR, abs/2303.17580.",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "REPLUG: retrieval-augmented black-box language models",
            "venue": "CoRR, abs/2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Arfa Tabassum",
                "Arul Menezes",
                "Arun Kirubarajan",
                "Asher Mullokandov",
                "Ashish Sabharwal",
                "Austin Herrick",
                "Avia Efrat",
                "Aykut Erdem",
                "Ayla Karakas"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "year": 2022
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum."
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3645\u20133650.",
            "year": 2019
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi",
                "Peter Clark."
            ],
            "title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "venue": "Findings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3621\u20133634.",
            "year": 2021
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi Mishra",
                "Peter Clark."
            ],
            "title": "Entailer: Answering questions with faithful and truthful chains of reasoning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2078\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jidong Tian",
                "Yitian Li",
                "Wenqing Chen",
                "Liqiang Xiao",
                "Hao He",
                "Yaohui Jin."
            ],
            "title": "Weakly supervised neural symbolic learning for cognitive tasks",
            "venue": "Proceedings of 36th Conference on Artificial Intelligence (AAAI), pages 5888\u20135896.",
            "year": 2022
        },
        {
            "authors": [
                "Xingyao Wang",
                "Sha Li",
                "Heng Ji."
            ],
            "title": "Code4struct: Code generation for few-shot structured prediction from natural language",
            "venue": "CoRR, abs/2210.12810.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "Proceedings of the 11th International Confer-",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "CoRR, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Albert Qiaochu Jiang",
                "Wenda Li",
                "Markus N. Rabe",
                "Charles Staats",
                "Mateja Jamnik",
                "Christian Szegedy."
            ],
            "title": "Autoformalization with large language models",
            "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyu Yang",
                "Jia Deng",
                "Danqi Chen."
            ],
            "title": "Generating natural language proofs with verifier-guided search",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Xi Ye",
                "Qiaochu Chen",
                "Isil Dillig",
                "Greg Durrett."
            ],
            "title": "Satisfiability-aided language models using declarative prompting",
            "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2023
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Siyuan Wang",
                "Duyu Tang",
                "Zenan Xu",
                "Daya Guo",
                "Yining Chen",
                "Jiahai Wang",
                "Jian Yin",
                "Ming Zhou",
                "Nan Duan."
            ],
            "title": "Analytical reasoning of text",
            "venue": "Findings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V. Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "Proceedings",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Logical reasoning is a cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments (Huang and Chang, 2023). It plays a central role in intelligent systems for problem-solving, decision-making, and critical thinking. Recently, large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022a; OpenAI, 2023) have exhibited emergent ability to \u201creason\u201d like human (Wei et al., 2022a). When prompted with step-wise explanations of reasoning (\u201cchain of thoughts\u201d), or a simple prompt \u201cLet\u2019s think step by step.\u201d, these models are able to answer questions with explicit reasoning steps (Wei et al., 2022b; Kojima et al., 2022).\n1Code and data are publicly available at https://github. com/teacherpeterpan/Logic-LLM.\nDespite the advances of LLMs, they still struggle with complex logical reasoning problems (Liu et al., 2023b). Recent studies (Golovneva et al., 2023; Ribeiro et al., 2023b; Lyu et al., 2023) found that LLMs occasionally make unfaithful reasoning, i.e., the derived conclusion does not follow the previously generated reasoning chain. While chain-of-thought may imitate human reasoning processes, the fundamental nature of LLMs remains that of black-box probabilistic models, lacking a mechanism to guarantee the faithfulness of reasoning (Shanahan, 2022). In contrast, symbolic inference engines, such as expert systems (Metaxiotis et al., 2002), are faithful and transparent because the reasoning is based on symbolic-represented knowledge and follows well-defined inference rules that adhere to logical principles. The main obstacle is how to accurately translate a problem into symbolic representations, considering the inherent ambiguity and flexibility of natural language. This is precisely where LLMs excel, making LLMs a promising complement to symbolic solvers.\nThis drives our exploration of neuro-symbolic methods that integrate LLMs with symbolic reasoning. As illustrated in Figure 1, we present LOGIC-\nLM, a novel framework that decomposes a logical reasoning problem into three stages: Problem Formulation, Symbolic Reasoning, and Result Interpretation. During problem formulation, an LLM converts the natural language description of the problem into an appropriate symbolic formulation, identifying key entities, facts, and rules present in the problem statement. Subsequently, at the symbolic reasoning stage, a deterministic symbolic solver performs inference on the symbolic formulation. Lastly, a result interpreter explains the output and maps it to the correct answer. By incorporating LLMs with symbolic solvers, we can exploit the robust natural language understanding capabilities of LLMs to precisely represent the problem using symbolic representations, while also taking advantage of the logical faithfulness and transparency offered by symbolic solvers. To improve the accuracy of the symbolic parsing, we also incorporate the idea of self-refinement to iteratively revise the generated logical form using the error messages from the symbolic solver as feedback.\nWe showcase the adaptability and effectiveness of LOGIC-LM on five logical reasoning datasets: ProofWriter (Tafjord et al., 2021), PrOntoQA (Saparov and He, 2023), FOLIO (Han et al., 2022), AR-LSAT (Zhong et al., 2022), and the LogicalDeduction dataset from BigBench (Srivastava et al., 2022). These datasets cover a wide range of logical reasoning problems, including: \u2022 Deductive Reasoning problems \u2022 First-Order Logic (FOL) reasoning problems \u2022 Constraint Satisfaction Problems (CSP) \u2022 Analytical Reasoning (AR) problems\nWe integrate four types of symbolic inference tools tailored to these problems: 1) logic programming engine that supports deductive reasoning through forward/backward chaining; 2) FOL inference engine that derives new conclusions based on FOL rules and facts, 3) constraint optimization engine that provides solvers for CSP over finite domains, and 4) boolean satisfiability problem (SAT) solver that solves analytical reasoning problems.\nOur evaluations show that the strategy of integrating LLMs with symbolic solvers performs significantly better than purely relying on LLMs for logical reasoning, with an average improvement of 39.2% over the standard prompting and 18.4% over the chain-of-thought prompting (\u00a7 4.1). We also find that LOGIC-LM becomes increasingly effective as the required reasoning depth increases\n(\u00a7 4.3). Finally, by analyzing the impact of selfrefinement, we highlight the effectiveness of incrementally revising symbolic formalizations when interacting with the symbolic solver (\u00a7 4.4)."
        },
        {
            "heading": "2 Related Work",
            "text": "Language Models for Logical Reasoning. Recent works in adapting LLMs for logical reasoning tasks can be broadly categorized into two groups: 1) fine-tuning approaches that optimize LLMs\u2019 reasoning ability through fine-tuning or training specialized modules (Clark et al., 2020; Tafjord et al., 2022; Yang et al., 2022), and 2) in-context learning approaches that design special prompts to elicit LLMs\u2019 step-by-step reasoning capabilities. Typical methods include chain-of-thought prompting (Wei et al., 2022b; Wang et al., 2023) that generates explanations before the final answer and the least-tomost prompting (Zhou et al., 2023) that breaks the problem down into simpler components that can be solved individually. Both the above approaches perform reasoning directly over natural language (NL), providing greater flexibility than symbolicbased reasoning. However, the intrinsic complexity and ambiguity of NL also bring undesired issues such as unfaithful reasoning and hallucinations.\nDifferent from prior works, we use symbolic language as the basic unit of reasoning. This effectively transfers the burden of executing complex, precise reasoning from LLMs to more reliable, interpretable external symbolic solvers. Simultaneously, we leverage the strong in-context learning ability of LLMs to formulate the NL-based problem into suitable symbolic representations, thus maintaining the benefit of flexibility.\nAlthough prior works (Mao et al., 2019; Gupta et al., 2020; Manhaeve et al., 2021; Cai et al., 2021; Tian et al., 2022; Pryor et al., 2023) also propose neuro-symbolic methods to combine neural networks with symbolic reasoning, these methods suffer from limitations such as hand-crafted or specialized module designs that are not easily generalizable, or brittleness due to the difficulty of optimization. In contrast, we propose a more generalizable framework that integrates modern LLMs with symbolic logic without the need for training or designing complex problem-specific modules.\nTool-augmented Language Models. Language models have inherent limitations such as the inability to access up-to-date information, take actions, or perform precise mathematical reasoning. To\naddress this, recent work has begun to augment language models with access to external tools and resources, such as the information retriever (Nakano et al., 2021; Shi et al., 2023; Lazaridou et al., 2022), calculator (Cobbe et al., 2021), code interpreter (Wang et al., 2022), planner (Liu et al., 2023a), and other pre-trained models (Shen et al., 2023). Recent works (Gao et al., 2023; Chen et al., 2022) have achieved improved performance on arithmetic reasoning tasks by generating Python programs that specify the reasoning procedure as chained commands in the order of execution. However, this idea has not been extended to logical reasoning problems, primarily due to the challenge of representing their highly \u201cnon-linear\u201d reasoning procedure (e.g., hypothesizing, case-by-case analysis, and the process of elimination) with functional programming. Our work provides a novel way to solve this within the framework of augmented LLMs. Instead of parsing the problem-solving procedure as programs, we only describe the problem with symbolic language using LLMs and then offload the reasoning to external symbolic solvers.\nAuto-Formalization. The concept of converting natural language into symbolic representations has been widely adopted in auto-formalization for mathematical reasoning (Wu et al., 2022; Drori\net al., 2022; He-Yueya et al., 2023; Jiang et al., 2023). These works demonstrate the proficiency of LLMs in translating a considerable fraction of mathematical problems into formal specifications defined in tools like SymPy (Meurer et al., 2017), Isabelle/HOL (Paulson, 1994), and Lean (de Moura et al., 2015). Mathematical reasoning can be considered a specialized subset of logical reasoning, primarily focused on numeric deductions. Due to this numeric specificity, mathematical problems are often more readily translatable to symbolic forms. In contrast, logical reasoning covers a wider array of problem types, often requiring a deeper understanding of world knowledge and commonsense for effective parsing into symbolic forms. Despite plenty of works studying mathematical reasoning, our work pioneers in extending the concept of autoformalization to a broader range of logical reasoning tasks with modern LLMs."
        },
        {
            "heading": "3 LOGIC-LM",
            "text": "As shown in Figure 2, the inputs of our model are a logical reasoning problem P described in natural language, along with a goal G in the form of a multiple-choice or free-form question. LOGIC-LM then follows a problem formulation-and-reasoning paradigm to solve the problem.\nIn the Problem Formulation stage, we prompt an LLM to translate the problem and the goal into a task-specific symbolic language. In the Symbolic Reasoning stage, we call a deterministic symbolic solver, e.g., a logic programming engine, to obtain a symbolic-represented answer. Finally, an LLM- or rule-based Result Interpreter is responsible for translating the answer back to natural language. Using this approach, the reasoning is guaranteed to be faithful as long as the problem formulation is correct since the answer A is the result of executing deterministic algorithms (e.g., forward/backward-chaining) embedded within the symbolic reasoner. Compared to previous methods based on chain-of-thought, our framework reduces the burden of LLMs by shifting their focus from \u201csolving the problem by reasoning step-by-step\u201d to \u201crepresenting the problem in symbolic language\u201d."
        },
        {
            "heading": "3.1 Problem Formulator",
            "text": "Intuitively, LLMs may struggle with directly solving complex reasoning problems. However, they have demonstrated a notable ability to comprehend textual inputs and translate them into formal programs, such as mathematical equations (He-Yueya et al., 2023) or Python codes (Gao et al., 2023). We posit that this capability to formulate problems into different languages can be extended to symbolic languages as well. We leverage the few-shot generalization ability of LLMs to achieve this. By providing the LLM with detailed instructions about the grammar of the symbolic language, alongside a few demonstrations as in-context examples, we observe that LLMs, like InstructGPT (Ouyang et al., 2022b) and GPT-4 (OpenAI, 2023), can effectively follow the instructions to identify key entities, facts, and rules present in the problem statement, and then translate these elements into symbolic language following our defined grammar.\nSpecifically, we use four different symbolic formulations to cover four common types of logical reasoning problems: deductive reasoning, firstorder logic reasoning, constraint satisfaction problem, and analytical reasoning. These formulations provide a foundation for translating natural language-based problem statements. By defining additional problem-specific formulations, our framework retains the flexibility to accommodate a wider range of reasoning tasks. Next, we will delve into the grammar of each symbolic formulation. Examples of each problem type are in Figure 2.\nLogic Programming (LP) Language. Deductive reasoning typically starts from known facts and rules, and iteratively makes new inferences until the goal statement can be proved or disproved (Poole and Mackworth, 2010). The Prolog logic programming language (Clocksin and Mellish, 2003; K\u00f6rner et al., 2022) is arguably the most prominent symbolic language to describe deductive reasoning problems. We adopt its grammar to represent a problem as facts, rules, and queries. \u2022 Facts: a fact F is a simple statement with a predicate and a set of arguments, formulated as P (a1, \u00b7 \u00b7 \u00b7 , an), where P is the predicate name and each argument ai can be a variable, entity, number, or bool. For example, Age(Peter, 31) means \u201cPeter\u2019s age is 31\u201d, and MadeOfIron(Nails, True) represents the fact \u201cNails are made of iron\u201d. \u2022 Rules: rules are written in the form of clauses: F1\u2227\u00b7 \u00b7 \u00b7\u2227Fm \u2192 Fm+1\u2227\u00b7 \u00b7 \u00b7\u2227Fn, where each Fi is a fact and the rule means \u201cif the facts F1, \u00b7 \u00b7 \u00b7 , Fm are true, then the facts Fm+1 \u00b7 \u00b7 \u00b7Fn are also true.\u201d \u2022 Queries: a query Q is simply another fact required to be proved based on known facts and rules.\nFirst-Order Logic (FOL). While the logic programming language efficiently represents common deductive reasoning problems, it may fail to represent more complex first-order logic (FOL) problems. To address this, we also include the FOL grammar (Enderton, 2001) in Appendix A. A problem is then parsed into a list of FOL formulas, which are divided into Premises (the known information from the problem) and Conclusion (the unknown formula to be proved). An example sentence and its FOL formula are given in Table 1.\nConstraint Satisfaction (CSP). Constraint satisfaction problems (CSPs) (Kumar, 1992) aims to find the value assignment of a set of objects that satisfy a number of constraints. A CSP is often defined as a triple (X,D,C), where X = {x1, \u00b7 \u00b7 \u00b7 , xn} is a set of variables, D = {D1, \u00b7 \u00b7 \u00b7 , Dn} is a set of their respective domains of values, and C = {C1, \u00b7 \u00b7 \u00b7 , Cm} is a set of constraints. Each variable xi can take on the values in the nonempty domain Di. Every constraint Cj is a pair \u27e8tj , Rj\u27e9, where tj \u2282 X is a subset of k variables and Rj is a k-ary relation on the corresponding subset of domains Dj . We use the above syntax to define a CSP problem as variables, domains, and constraints. An example is given in both Figure 2 and Table 1.\nBoolean Satisfiability (SAT) Formulation. SAT is the problem of deciding if there is an assignment to the variables of a Boolean formula such that the formula is satisfied. Many analytical reasoning problems can be formulated as SAT problems. We adopt the grammar defined in Ye et al. (2023) to formulate an SAT problem P as (\u03a6, T ,Q), where \u03a6 is a set of constraints defined under the theory T , and Q is the query of interest.\nTable 1 summarizes the four types of logical reasoning problems, their typical datasets, and the symbolic formulation used to represent each type of problem. We also give an example of a natural language statement with its corresponding symbolic formulation for each type. Appendix C shows the full prompts we use for the problem formulator. To teach LLMs to better align each statement with its corresponding symbolic form, we use the format SYMBOLIC_FORMULA ::: NL_STATEMENT in in-context examples to enable better grounding."
        },
        {
            "heading": "3.2 Symbolic Reasoner",
            "text": "After the problem formulator parses the problem P and the goal G into symbolic representations P\u0302 and G\u0302, we call a deterministic external solver depending on the task, to obtain the answer A. Table 1 summarizes the symbolic solvers we use for each type of logical reasoning problem.\nLP System. For deductive reasoning, we incorporate the Pyke expert system (Frederiksen, 2008), which makes inferences based on the logic programming language. In response to a query, Pyke first creates a knowledge base, populating it with known facts and rules. Subsequently, it applies forward- and backward-chaining algorithms to infer new facts and substantiate the goal.\nFOL Prover. We use Prover92 as the FOL inference engine. Prover9 is an automated theorem prover that supports first-order logic and equational logic. It initially converts FOL statements to conjunctive normal form (CNF) and then performs resolution (Robinson, 1965) on the CNF to deduce whether a conclusion is true, false, or unknown.\nCSP Solver. Solving a CSP is to find value assignments for all variables that satisfy all given constraints. Commonly used algorithms for this task include backtracking, constraint propagation, and local search variants. To this end, we incorporate the python-constraint3 package which offers solvers for CSPs over finite domains.\nSAT Solver. For solving SAT problems, we use the Z3 theorem prover (de Moura and Bj\u00f8rner, 2008), a satisfiability modulo theories (SMT) solver developed by Microsoft4. The SMT solver provides algorithms to determine whether a set of mathematical formulas is satisfiable. It generalizes the SAT problems to more complex formulas involving real numbers, integers, and various data structures such as lists, arrays, bit vectors, and strings. A lot of real-world analytical reasoning problems can be represented as problems of solving a system of equations."
        },
        {
            "heading": "3.3 Self-Refiner",
            "text": "For complex problems, generating the correct logical form may become challenging for LLMs. To address this, we introduce a self-refinement module that learns to modify inaccurate logical for-\n2https://www.cs.unm.edu/~mccune/prover9/ 3https://github.com/python-constraint/\npython-constraint 4https://github.com/Z3Prover/z3\nmulations using the error messages from the symbolic reasoner as feedback. Recent works (Chen et al., 2023; Madaan et al., 2023) have adopted similar ideas to improve code generation, by teaching LLMs to debug their predicted programs via fewshot demonstrations. Here we extend this idea to refine generated logic representations. If the symbolic solver returns an execution error, we instruct the LLM to refine the incorrect logical form, by prompting it with the erroneous logic form, the solver\u2019s error message, and a set of demonstrations showing common error cases (e.g., a free variable is not bounded to any quantifier in FOL) and their remedies. We run this process iteratively until either no error messages are returned, or the maximum number of allowable revisions is reached."
        },
        {
            "heading": "3.4 Result Interpreter",
            "text": "Finally, the result interpreter translates the results returned from the symbolic solver back to a natural language answer. For certain problems, this can be achieved through predefined rules; for example, mapping Entailment to true. However, this process can be more complex for CSPs, e.g., translating {convertible: 1, tractor: 2, minivan: 3} to \u201cthe convertible is the oldest.\u201d. To handle these varying levels of complexity, we designed both rule-based and LLM-based result interpreters. Details of the result interpreter are given in Appendix D."
        },
        {
            "heading": "4 Experiments",
            "text": "Datasets. We evaluate LOGIC-LM on five common logical reasoning datasets, as follows.\nPrOntoQA (Saparov and He, 2023) is a recent synthetic dataset created to analyze the capacity of LLMs for deductive reasoning. We use the hardest fictional characters version of the dataset, based on the results in Saparov and He (2023). Each version is divided into different subsets depending on the number of reasoning hops required. We use the hardest 5-hop subset for evaluation. Each question in PrOntoQA aims to validate a new fact\u2019s veracity, such as \u201cTrue or false: Alex is not shy.\u201d.\nProofWriter (Tafjord et al., 2021) is another commonly used dataset for deductive logical reasoning. Compared with PrOntoQA, the problems are expressed in a more naturalistic language form. We use the open-world assumption (OWA) subset in which each example is a (problem, goal) pair and the label is one of {PROVED, DISPROVED, UNKNOWN}. The dataset is divided into five parts,\neach part requiring 0, \u2264 1, \u2264 2, \u2264 3, and \u2264 5 hops of reasoning, respectively. We evaluate the hardest depth-5 subset. To reduce overall experimentation costs, we randomly sample 600 examples in the test set and ensure a balanced label distribution.\nFOLIO (Han et al., 2022) is a challenging expert-written dataset for logical reasoning. The problems are mostly aligned with real-world knowledge and use highly natural wordings, and the questions require complex first-order logic reasoning to solve. We use the entire FOLIO test set for evaluation, consisting of 204 examples.\nLogicalDeduction is a challenging logical reasoning task from the BigBench (Srivastava et al., 2022) collaborative benchmark. The problems are mostly about deducing the order of a sequence of objects from a minimal set of conditions. We use the full test set consisting of 300 examples.\nAR-LSAT (Zhong et al., 2022) is a dataset that collects all analytical logic reasoning questions from the Law School Admission Test from 1991 to 2016. We use the test set which has 231 multiplechoice questions. AR-LSAT is particularly challenging, with state-of-the-art models only achieving performance slightly better than random guessing (Liang et al., 2022; Ribeiro et al., 2023a).\nWe convert all examples into a standard multiplechoice format, comprising a problem statement, a question, and potential answers, as shown in Figure 2. We also select 1-5 examples from the training set of each dataset as in-context examples. Detailed data statistics are in Appendix B.\nBaselines. We compare our model against two baselines that depend solely on LLMs for logical reasoning: 1) Standard LLMs, which leverage incontext learning to directly answer the question; and 2) Chain-of-Thought (CoT) (Wei et al., 2022b), which adopts a step-by-step problem-solving approach, generating explanations before providing the final answer. We separately evaluate the settings that ChatGPT (gpt-3.5-turbo), GPT-3.5 (text-davinci-003) (Ouyang et al., 2022a) and GPT-4 (gpt-4) (OpenAI, 2023) serve as the underlying LLMs for all models. To ensure fair comparisons, we use the same in-context examples for all models. For reproducible results, we set the temperature to 0 and select the response with the highest probability from LLMs. Since all examples are formed as multiple-choice questions, we evaluate model performance based on the accuracy of selecting the correct answer."
        },
        {
            "heading": "4.1 Main Results",
            "text": "We report the results of LOGIC-LM (without selfrefinement) and baselines in Table 2. For LOGICLM, a symbolic solver does not return an answer when there are grammar errors in the symbolic formulation. For these un-executable cases, we fall back on using chain-of-thought to predict the answer. We have three major observations.\n1. Logic-LM significantly outperforms standard LLMs and CoT across all datasets. With GPT3.5, our method outperforms standard LLM on all datasets, with an average improvement of 39.2%. This highlights the benefit of combining LLMs with external symbolic solvers for logical reasoning. LOGIC-LM also improves CoT by a large margin of 18.4% on average, showing that offloading the reasoning to symbolic solvers greatly improves faithfulness compared with pure language-based reasoning with CoT.\n2. GPT-4 outperforms GPT-3.5 by a large margin of 48.46% on average for the standard prompting. This aligns with the assertion that the main enhancement of GPT-4 lies in its ability to carry out complex reasoning (OpenAI, 2023). Although this may indicate that the logical reasoning capability can be boosted by scaling up the LLM, we observe that GPT-4 still makes numerous unfaithful reasoning errors. By delegating the reasoning to symbolic solvers, our method can further improve GPT-4 by an average of 24.98% and 10.44% for standard prompting and CoT prompting, respectively.\n3. While integrating CoT generally enhances LLM performance, we find its benefits comparatively less substantial or even negative on FOLIO, LogicalDeduction, and AR-LSAT, with a modest improvement of 11.75%, 9.41%, and -3.2%, respectively. On the contrary, the benefits of CoT on ProntoQA and ProofWriter are 51.59% and 33.82%, respectively. A plausible explanation is"
        },
        {
            "heading": "FOLIO",
            "text": ""
        },
        {
            "heading": "AR-LSAT",
            "text": "that CoT emulates human forward-chain reasoning: beginning with known facts and sequentially deriving new conclusions until the goal is met. This reasoning style aligns well with problems in the PrOntoQA and ProofWriter datasets. However, FOL and CSP problems often necessitate more sophisticated reasoning strategies that are \u201cnonlinear\u201d compared to standard forward-chain reasoning. These include hypothesizing, conditioning, recursive inference, and the process of elimination. Compared to CoT, the integration of symbolic solvers is better suited to these reasoning styles, hence yielding a more marked improvement on FOLIO (+21.85%), LogicalDeduction (+45.67%), and AR-LSAT (+24.14%)."
        },
        {
            "heading": "4.2 Effectiveness of Problem Formulator",
            "text": "We then evaluate how well LLM can translate a given problem into the symbolic formulation used by each symbolic solver. In Table 3, we report the percentage of symbolic formulations that are executable by the corresponding symbolic solver for\n1\n57.7 52.6 47.3 38.3 33.5\n76.3 73.6\n65.3 59.4\n51\n81.7 77.3\n73.6 71.3 71.1\n30\n40\n50\n60\n70\n80\n90\n0 1 2 3 5 Reasoning Depth\nStandard CoT Logic-LMAccuracy\nFigure 3: Accuracy of different models for increasing size of reasoning depth on the ProofWriter dataset.\neach dataset (Exe_Rate). Generally, LLM demonstrates high proficiency in transcribing problems into symbolic formats, evidenced by its near 100% Exe_Rate on ProntoQA, ProofWriter, and LogicalDeduction. However, the high performance on these datasets is somewhat anticipated, given that their problems are mostly synthetically generated, limiting language variability. When it comes to datasets comprising real-world, expertly crafted problems, such as FOLIO and AR-LSAT, GPT4\u2019s performance is notably less promising, with Exe_Rate scores of 79.9% and 32.6% respectively. This discrepancy underscores the inherent challenges associated with converting real-world problems into their logical equivalents. Exe_Rate only reflects the grammar correctness of the logical form. We also report the accuracy of the executable samples (Exe_Acc) to measure the semantic correctness. We find that logical forms generated by GPT-4 generally achieve high Exe_Acc, even for the most challenging AR-LSAT dataset. Such performance accentuates the potential of symbolic solvers in bolstering the model\u2019s logical reasoning prowess, contingent on the precise translation of problems into symbolic forms."
        },
        {
            "heading": "4.3 Robustness of Reasoning",
            "text": "Incorporating symbolic solvers also leads to more robust reasoning. To illustrate this, we report the performance of LOGIC-LM and baselines for questions of varying complexity levels. We randomly selected 300 examples from each subset of ProofWriter, ensuring a balanced label distribution. The problems in these subsets require 0, <=1, <=2, <=3, and <=5 hops of reasoning, respectively. The results, shown in Figure 3, indicate that LOGIC-LM becomes increasingly effective as the required reasoning depth increases. For exam-\n57.87\n61.27 62.25 64.56 63.84\n70.58\n78.92 78.43 79.9 79.41\n55\n60\n65\n70\n75\n80\n85\n0 1 2 3\nCoT (GPT-3.5) Logic-LM (GPT-3.5) CoT (GPT-4) Logic-LM (GPT-4)\nRounds GPT-3.5 66.7% 79.4% 82.4% 84.3%\nGPT-4 79.9% 85.3% 85.3% 85.8%\nAccuracy\nFigure 4: The accuracy for different rounds of selfrefinement, with the corresponding executable rates.\nple, LOGIC-LM outperforms CoT by 7.1%, 5.0%, 12.7%, 20.0%, and 39.4% on depth-0, depth-1, depth-2, depth-4, and depth-5 problems, respectively. In LOGIC-LM, multi-step logical reasoning is delegated to external symbolic solvers, thereby transitioning the challenge of LLM from problemsolving to problem representation. Ideally, the complexity of formally representing a problem statement in logical form should remain relatively constant, regardless of whether the questions require simple or complex reasoning. The trends in Figure 3 validate this assumption. The performance of Standard and CoT declines precipitously with the escalation of problem complexity. However, this trend is less prominent for LOGIC-LM, indicating that the robust reasoning capabilities provided by external solvers substantially mitigate performance degradation for complex reasoning problems."
        },
        {
            "heading": "4.4 Impact of Self-Refinement",
            "text": "In Table 3, we find that self-refinement is effective in fixing the in-executable symbolic formulations, increasing the Exe_Rate by 5.01 on average. For an in-depth analysis, we then evaluate the accuracy and Exe_Rate across different rounds of selfrefinement on FOLIO, namely, 0 (no refinement), 1, 2, and 3 rounds. The results are in Figure 4.\nWe find that as the rounds of self-refinement increase, the percentage of executable formulations consistently increases, leading to an enhancement in the final performance. This suggests that selfrefinement serves as an effective tool in aiding the LLM to accurately frame the problem. However, the accuracy tends to stagnate in subsequent rounds, even though the Exe_Rate continues to increase. This can be attributed to the type of feedback received by the self-refiner, which is the error mes-\nsage from the symbolic solver. This feedback aids in converting \u201cinvalid\u201d symbolic representations into valid ones. However, a valid symbolic representation does not necessarily equate to a \u201ccorrect\u201d problem formulation that accurately represents the problem. This issue could be tackled by enhancing the self-refiner to incorporate feedback beyond the error message, e.g., a reward signal from an additional module evaluating the accuracy of a generated symbolic form. We leave this as a promising direction for future exploration."
        },
        {
            "heading": "4.5 Case Study",
            "text": "In Figure 5, we show an example of the symbolic representations generated by GPT-4, together with the predicted answer. In general, LOGIC-LM has demonstrated a potent capacity to interpret complex problems into symbolic forms. Nonetheless, there remain certain difficulties in accurately understanding the semantics of the problem.\nWe further analyze some error cases in Figure 6 of Appendix E. Example 1 shows a case where GPT-4 generates an incorrect FOL representation, stemming from its inability to define appropriate predicates. Here, instead of creating the predicate EasternWildTurkey, the model generates a constant, WildTurkey(eastern), in which WildTurkey is the predicate and eastern is the constant. While this representation is valid in isolation, it does not interact well with subsequent constants. This inconsistency is a recurring issue in GPT-4\u2019s symbolic form generation, illustrating that the model sometimes struggles to maintain an overarching understanding of the problem when forming logical symbols. Example 3 highlights a case where GPT-4 struggles to interpret specific\nexpressions accurately. In this case, the model fails to distinguish between the meanings of \u201cbelow\u201d and \u201cabove\u201d, resulting in an incorrect constraint Dan > Eve. Example 4 exemplifies GPT-4\u2019s challenge with fully grasping the rules of FOL grammar, evidenced by the invalid generated formula: Rating(subway, y) \u2227 y > 9. These error cases underscore that transforming problems into logical forms remains a challenging task for modern LLMs, due to the intricacies of FOL formulation, the innate flexibility of natural language, and the complexity of global problem comprehension."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this work, we propose a novel approach to address logical reasoning problems by combining large language models with symbolic solvers. We introduce Logic-LM, one instantiation of such a framework, and demonstrate how it significantly improves performance over pure LLMs and chainof-thought prompting techniques.\nWhile Logic-LM has proven to be a capable system, it can be further improved with extension to more flexible and powerful logic systems. For example, statistical relational learning (SRL) systems such as Markov logic networks (Richardson and Domingos, 2006) and probabilistic soft logic (Bach et al., 2017) have demonstrated great promise in reasoning under uncertainty and integration with our framework would enable even more adaptive problem-solving capabilities. Additionally, our method can be extended to reasoning problems requiring commonsense, which remains a significant challenge as they often require reasoning over complex and ambiguous rules."
        },
        {
            "heading": "Limitations",
            "text": "We identify two main limitations of LOGIC-LM. First, LOGIC-LM relies on translating reasoning problems into logical formats that can be tackled by symbolic solvers. As a consequence, the model\u2019s applicability is inherently bounded by the expressiveness of the symbolic solver, for example, not all problems can be easily encoded in first-order logic. Nevertheless, this limitation can be mitigated by integrating a more diverse set of symbolic solvers. The flexible design of LOGIC-LM facilitates this integration. The wide range of reasoning tasks that we can instantiate our LOGIC-LM framework on shows its general applicability.\nSecond, LOGIC-LM depends on in-context learning coupled with self-refinement to convert a natural language (NL) problem into the symbolic representation. While this method has proven to be effective, it may face difficulties when dealing with logical representations with intricate grammar structures, such as probabilistic soft logic. This arises from the difficulty in conveying complex grammatical rules to the language model through a limited number of demonstrations within a constrained context size. As a potential solution, future works could explore the development of specialized modules to enhance the mapping between NL and symbolic language, e.g., fine-tuning LLMs with synthetic data generated via symbolic solvers."
        },
        {
            "heading": "Ethics Statement",
            "text": "The use of large language models requires a significant amount of energy for computation for training, which contributes to global warming (Strubell et al., 2019). Our work performs few-shot in-context learning instead of training models from scratch, so the energy footprint of our work is less. The large language models whose API we use for inference, especially GPT-4, consume significant energy."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Science Foundation Award #2048122. The views expressed are those of the authors and do not reflect the official policy or position of the US government."
        },
        {
            "heading": "A Syntax for First-order Logic (FOL)",
            "text": ""
        },
        {
            "heading": "B Dataset Statistics",
            "text": ""
        },
        {
            "heading": "C Prompt Examples",
            "text": "In this section we provide examples of the prompts used for each dataset and method. Prompts for standard in-context learning contain 2 demonstrations consisting of 3 parts each: a context, a question, and options. Prompts for chain-of-thought prompting contain 2 demonstrations consisting of 5 parts each: a task description, a context, a question, options, and a chain of reasoning. Prompts for LogicLM contain 2 demonstrations with 5 parts each: a task description, a context, a question, options, and a domain-specific symbolic program. For brevity, we show only a single demonstration for each setting in the following sections."
        },
        {
            "heading": "C.1 PrOntoQA Prompts",
            "text": "Standard In-Context Learning Context: Jompuses are not shy. Jompuses are yumpuses. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) Zumpuses are rompuses. Max is a yumpus.\nQuestion: Is the following statement true or false? Max is sour.\nOptions: A) True B) False\nThe correct option is: B\nChain-of-Thought Prompting Task Description: Given a problem statement as contexts , the task is to answer a logical reasoning question.\nContext: Jompuses are not shy. Jompuses are yumpuses. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) Zumpuses are rompuses. Max is a yumpus.\nQuestion: Is the following statement true or false? Max is sour.\nOptions: A) True B) False\nReasoning: Max is a yumpus. Each yumpus is a dumpus. (\u00b7 \u00b7 \u00b7 more reasoning here \u00b7 \u00b7 \u00b7 ) Tumpuses are not sour. So Max is not sour.\nThe correct option is: B\nLogic-LM Task Description: You are given a problem description and a question. The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic form\nContext: Each jompus is fruity. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) Rompuses are zumpuses. Alex is a tumpus.\nQuestion: True or false: Alex is not shy.\nPredicates: Jompus (\\$x, bool) ::: Does x belong to Jompus? (\u00b7 \u00b7 \u00b7 more predicates here \u00b7 \u00b7 \u00b7 ) Zumpus (\\$x, bool) ::: Does x belong to Zumpus?\nFacts: Tumpuses(Alex , True)\nRules: Jompus($x, True) >>> Fruity($x, True) (\u00b7 \u00b7 \u00b7 more rules here \u00b7 \u00b7 \u00b7 ) Dumpus (\\$x, True) >>> Rompus (\\$x, True)\nQuery: Shy(Alex , False)"
        },
        {
            "heading": "C.2 ProofWriter Prompts",
            "text": "Standard In-Context Learning Context: The cow is blue. The cow is round. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) If the cow is cold and the cow visits the lion then the lion sees the squirrel.\nQuestion: Based on the above information , is the following statement true , false , or unknown? The tiger is not young.\nOptions: A) True B) False C) Unknown\nThe correct option is: B\nChain-of-Thought Prompting Task Description: Given a problem statement as contexts , the task is to answer a logical reasoning question.\nContext: The cow is blue. The cow is round. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) If the cow is cold and the cow visits the lion then the lion sees the squirrel.\nQuestion: Based on the above information , is the following statement true , false , or unknown? The tiger is not young.\nOptions: A) True B) False C) Unknown\nReasoning: The tiger likes the cow. The tiger likes the squirrel. (\u00b7 \u00b7 \u00b7 more reasoning here \u00b7 \u00b7 \u00b7 ) If something is nice and it sees the tiger then it is young. So the tiger is young.\nThe correct option is: B\nLogic-LM Task Description: You are given a problem description and a question. The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic form\nContext: Anne is quiet. Erin is furry. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) All red people are young.\nQuestion: Based on the above information , is the following statement true , false , or unknown? Anne is white.\nPredicates: Quiet($x, bool) ::: Is x quiet? Furry($x, bool) ::: Is x furry? (\u00b7 \u00b7 \u00b7 more predicates here \u00b7 \u00b7 \u00b7 ) White($x, bool) ::: Is x white? Young($x, bool) ::: Is x young?\nFacts: Quite(Anne , True) ::: Anne is quiet. (\u00b7 \u00b7 \u00b7 more facts here \u00b7 \u00b7 \u00b7 ) White(Harry , True) ::: Harry is white.\nRules: Young($x, True) >>> Furry($x, True) ::: Young people are furry. (\u00b7 \u00b7 \u00b7 more rules here \u00b7 \u00b7 \u00b7 ) Red($x, True) >>> Young($x, True) ::: All red people\nare young.\nQuery: White(Anne , True) ::: Anne is white"
        },
        {
            "heading": "C.3 FOLIO Prompts",
            "text": "Standard In-Context Learning Context: All people who regularly drink coffee are dependent on caffeine. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) If Rina is not a person dependent on caffeine and a student , then Rina is either a person dependent on caffeine and a student , or neither a person dependent on caffeine nor a student.\nQuestion: Based on the above information , is the following statement true , false , or uncertain? Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug.\nOptions: A) True B) False C) Uncertain\nThe correct option is: A\nChain-of-Thought Prompting Task Description: Given a problem statement as contexts , the task is to answer a logical reasoning question.\nContext: The Blake McFall Company Building is a commercial warehouse listed on the National Register of Historic Places. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) John works at the Emmet Building.\nQuestion: Based on the above information , is the following statement true , false , or uncertain? The Blake McFall Company Building is located in Portland , Oregon.\nOptions: A) True B) False C) Uncertain\nReasoning: The Blake McFall Company Building is another name for the Emmet Building. (\u00b7 \u00b7 \u00b7 more reasoning here \u00b7 \u00b7 \u00b7 ) Therefore , the Blake McFall Company Building is located in Portland , Oregon.\nThe correct option is: A\nLogic-LM Task Description: Given a problem description and a\nquestion. The task is to parse the problem and the question into first -order logic formulas. The grammar of the first -order logic formula is defined as follows:\n1) logical conjunction: expr1 \u2227 expr2 2) logical disjunction: expr1 \u2228 expr2 3) logical exclusive disjunction: expr1 \u2295 expr2 4) logical negation: \u00acexpr1 5) expr1 implies expr2: expr1 \u2192 expr2 6) expr1 if and only if expr2: expr1 \u2194 expr2 7) logical universal quantification: \u2200 x 8) logical existential quantification: \u2203 x Output format: logic form ::: description\nContext: All people who regularly drink coffee are dependent on caffeine. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) If Rina is not a person dependent on caffeine and a student , then Rina is either a person dependent on caffeine and a student , or neither a person dependent on caffeine nor a student.\nQuestion: Based on the above information , is the following statement true , false , or uncertain? Rina is either a person who jokes about being addicted to caffeine or is unaware that caffeine is a drug.\nPredicates: Dependent(x) ::: x is a person dependent on caffeine (\u00b7 \u00b7 \u00b7 more predicates here \u00b7 \u00b7 \u00b7 ) Student(x) ::: x is a student\nPremises: \u2200x (Drinks(x) \u2192 Dependent(x)) ::: All people who\nregularly drink coffee are dependent on caffeine.\n(\u00b7 \u00b7 \u00b7 more premises here \u00b7 \u00b7 \u00b7 ) \u2200x (Jokes(x) \u2192 \u00acUnaware(x)) ::: No one who jokes\nabout being addicted to caffeine is unaware that caffeine is a drug.\nConclusion: Jokes(rina) \u2295 Unaware(rina) ::: Rina is either a\nperson who jokes about being addicted to caffeine or is unaware that caffeine is a drug."
        },
        {
            "heading": "C.4 LogicalDeduction Prompts",
            "text": "Standard In-Context Learning Context: The following paragraphs each describe a set of seven objects arranged in a fixed order. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) Eve finished below Ada. Rob finished below Joe.\nQuestion: Which of the following is true?\nOptions: A) Ana finished third. B) Eve finished third. C) Ada finished third. D) Dan finished third. E) Rob finished third. F) Amy finished third. G) Joe finished third.\nThe correct option is: A\nChain-of-Thought Prompting Task Description: Given a problem statement as contexts , the task is to answer a logical reasoning question.\nContext: The following paragraphs each describe a set of five objects arranged in a fixed order. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) The raven is the third from the left.\nQuestion: Which of the following is true?\nOptions: A) The quail is the rightmost. B) The owl is the rightmost. C) The raven is the rightmost. D) The falcon is the rightmost. E) The robin is the rightmost.\nReasoning: The owl is the leftmost. This means owl is not the rightmost. (\u00b7 \u00b7 \u00b7 more reasoning here \u00b7 \u00b7 \u00b7 ) This means raven is also not the rightmost. So, the answer is: A) The quail is the rightmost.\nThe correct option is: A\nLogic-LM Task Description: You are given a problem description. The task is to parse the problem as a constraint satisfaction problem , defining the domain , variables , and contraints.\nContext: The following paragraphs each describe a set of three objects arranged in a fixed order. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) The minivan is newer than the convertible.\nQuestion: Which of the following is true?\nOptions: A) The station wagon is the second -newest. B) The convertible is the second -newest. C) The minivan is the second -newest.\nDomain: 1: oldest 3: newest\nVariables: station\\_wagon [IN] [1, 2, 3] convertible [IN] [1, 2, 3] minivan [IN] [1, 2, 3]\nConstraints: station\\_wagon == 1 ::: The station wagon is the oldest. minivan > convertible ::: The minivan is newer than the convertible. AllDifferentConstraint ([ station\\_wagon , convertible ,\nminivan ]) ::: All vehicles have different values.\nQuery: A) station\\_wagon == 2 ::: The station wagon is the second -newest. B) convertible == 2 ::: The convertible is the second -newest. C) minivan == 2 ::: The minivan is the second -newest\n."
        },
        {
            "heading": "C.5 AR-LSAT Prompts",
            "text": "Standard In-Context Learning Context: During a single week , from Monday through\nFriday , tours will be conducted of a company 's three divisions: Operations , Production , and Sales. Exactly five tours will be conducted that week , one each day. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) If the Operations division is toured on Thursday , then the Production division is toured on Friday.\nQuestion: Which one of the following CANNOT be true of the week's tour schedule?\nOptions: A) The division that is toured on Monday is also toured on Tuesday. B) The division that is toured on Monday is also toured on Friday. C) The division that is toured on Tuesday is also toured on Thursday. D) The division that is toured on Wednesday is also toured on Friday. E) The division that is toured on Thursday is also\ntoured on Friday.\nThe correct option is: C\nChain-of-Thought Prompting Task Description: Given a problem statement as contexts , the task is to answer a logical reasoning question.\nContext: During a single week , from Monday through Friday , tours will be conducted of a company 's three divisions: Operations , Production , and Sales. Exactly five tours will be conducted that week , one each day. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) If the Operations division is toured on Thursday , then the Production division is toured on Friday.\nQuestion: Which one of the following CANNOT be true of the week's tour schedule?\nOptions: A) The division that is toured on Monday is also toured on Tuesday. B) The division that is toured on Monday is also toured on Friday. C) The division that is toured on Tuesday is also toured on Thursday. D) The division that is toured on Wednesday is also toured on Friday. E) The division that is toured on Thursday is also\ntoured on Friday.\nReasoning: Since Thursday and Friday already have tours planned , only Monday , Tuesday and Wednesday tours need to be determined. (\u00b7 \u00b7 \u00b7 more reasoning here \u00b7 \u00b7 \u00b7 ) A different division is toured on Thursday. Therefore , the final answer is C.\nThe correct option is: C\nLogic-LM Task Description: You are given a problem description. The task is to parse the problem as a constraint satisfaction problem , defining the domain , variables , and contraints.\nContext: A travel magazine has hired six interns - Farber , Gombarick , Hall , Jackson , Kanze , and Lha - to assist in covering three stories: Romania , Spain , and Tuscany. (\u00b7 \u00b7 \u00b7 more context here \u00b7 \u00b7 \u00b7 ) Jackson is assigned to Tuscany. Kanze is not assigned to Spain.\nQuestion: Which one of the following interns CANNOT be assigned to Tuscany?\nOptions: (A) Farber (B) Gombarick (C) Hall (D) Kanze (E) Lha\nDeclarations: stories = EnumSort ([Romania , Spain , Tuscany ]) assistants = EnumSort ([ photographer , writer ]) (\u00b7 \u00b7 \u00b7 more declarations here \u00b7 \u00b7 \u00b7 ) trained = Function ([ interns] -> [assistants ])\nConstraints: trained(Gombarick) == trained(Lha) ::: Gombarick and Lha will be trained in the same field trained(Farber) != trained(Kanze) ::: Farber and Kanze will be trained in different fields (\u00b7 \u00b7 \u00b7 more contraints here \u00b7 \u00b7 \u00b7 ) assigned(Jackson) == Tuscany ::: Jackson is assigned to Tuscany assigned(Kanze) != Spain ::: Kanze is not assigned\nto Spain\nOptions: is_unsat(assigned(Farber) == Tuscany) ::: (A) is_unsat(assigned(Gombarick) == Tuscany) ::: (B) is_unsat(assigned(Hall) == Tuscany) ::: (C) is_unsat(assigned(Kanze) == Tuscany) ::: (D) is_unsat(assigned(Lha) == Tuscany) ::: (E)"
        },
        {
            "heading": "D Result Interpreter Implementation",
            "text": "For PrOntoQA and ProofWriter, the Pyke logic programming engine returns the inferred value of the variable in the query or Unknown if the variable cannot be determined. For example, for the query ConductElectricity(Nail, x), Pyke may return x =True. By comparing with the goal statement ConductElectricity(Nail, False), we can know that goal to be proved is False. For FOLIO, the FOL inference engine directly returns the veracity label of the goal as ENTAILMENT, CONTRADICTION, and CONTINGENT, which can be mapped to True, False, and Unknown, respectively. For LogicalDeduction, the solver returns all the possible value assignments in an array. We write rules to parse each option into the corresponding value and check it is in the generated array. For ARLSAT, we attempt to separately prove each option to find the correct answer.\nE Example Generations of LOGIC-LM"
        }
    ],
    "title": "LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
    "year": 2023
}