{
    "abstractText": "Retrieval augmentation enhances generative language models by retrieving informative exemplars relevant for output prediction. However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget. In this work, we improve retrieval-augmented parsing for complex graph problems by exploiting two unique sources of information (1) structural similarity and (2) model uncertainty. We propose Structure-aware and Uncertainty-Guided Adaptive Retrieval (SUGAR) that first quantify the model uncertainty in graph prediction and identify its most uncertain subgraphs, and then retrieve exemplars based on their structural similarity with the identified uncertain subgraphs. On a suite of real-world parsing benchmarks with non-trivial graph structure (SMCalflow and E-commerce), SUGAR exhibits a strong advantage over its classic counterparts that do not leverage structure or model uncertainty.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zi Lin"
        },
        {
            "affiliations": [],
            "name": "Quan Yuan"
        },
        {
            "affiliations": [],
            "name": "Panupong Pasupat"
        },
        {
            "affiliations": [],
            "name": "Jeremiah Liu"
        },
        {
            "affiliations": [],
            "name": "Jingbo Shang"
        }
    ],
    "id": "SP:f691f4297411f9aa8e04a2cbf6a65960527a065f",
    "references": [
        {
            "authors": [
                "Striplin",
                "Yu Su",
                "Zachary Tellman",
                "Sam Thomson",
                "Andrei Vorobev",
                "Izabela Witoszko",
                "Jason Wolfe",
                "Abby Wray",
                "Yuchen Zhang",
                "Alexander Zotov."
            ],
            "title": "Task-oriented dialogue as dataflow synthesis",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Laura Banarescu",
                "Claire Bonial",
                "Shu Cai",
                "Madalina Georgescu",
                "Kira Griffitt",
                "Ulf Hermjakob",
                "Kevin Knight",
                "Philipp Koehn",
                "Martha Palmer",
                "Nathan Schneider."
            ],
            "title": "Abstract Meaning Representation for sembanking",
            "venue": "Proceedings of the 7th Linguistic",
            "year": 2013
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Shu Cai",
                "Kevin Knight."
            ],
            "title": "Smatch: an evaluation metric for semantic feature structures",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748\u2013752, Sofia, Bulgaria. Association",
            "year": 2013
        },
        {
            "authors": [
                "Ruixiang Cui",
                "Rahul Aralikatte",
                "Heather Lent",
                "Daniel Hershcovich."
            ],
            "title": "Compositional generalization in multilingual semantic parsing over Wikidata",
            "venue": "Transactions of the Association for Computational Linguistics, 10:937\u2013955.",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Yacine Jernite",
                "Ethan Perez",
                "David Grangier",
                "Jason Weston",
                "Michael Auli."
            ],
            "title": "ELI5: Long form question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558\u20133567, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Flickinger",
                "Emily M. Bender",
                "Stephan Oepen."
            ],
            "title": "Towards an encyclopedia of compositional semantics: Documenting the interface of the English Resource Grammar",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and",
            "year": 2014
        },
        {
            "authors": [
                "Vivek Gupta",
                "Akshat Shrivastava",
                "Adithya Sagar",
                "Armen Aghajanyan",
                "Denis Savenkov."
            ],
            "title": "RetroNLU: Retrieval augmented task-oriented semantic parsing",
            "venue": "Proceedings of the 4th Workshop on NLP for Conversational AI, pages 184\u2013196,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Luyu Gao",
                "Zhiqing Sun",
                "Qian Liu",
                "Jane Dwivedi-Yu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Active retrieval augmented generation",
            "venue": "arXiv preprint arXiv:2305.06983.",
            "year": 2023
        },
        {
            "authors": [
                "Robert T Kasper."
            ],
            "title": "A flexible interface for linking applications to penman\u2019s sentence generator",
            "venue": "Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, Pennsylvania, February 21-23, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal"
            ],
            "title": "Decomposed prompting: A modular",
            "year": 2022
        },
        {
            "authors": [
                "Haoran Li",
                "Abhinav Arora",
                "Shuohui Chen",
                "Anchit Gupta",
                "Sonal Gupta",
                "Yashar Mehdad."
            ],
            "title": "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Zi Lin",
                "Jeremiah Liu",
                "Jingbo Shang."
            ],
            "title": "Neuralsymbolic inference for robust autoregressive graph parsing via compositional uncertainty quantification",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Zi Lin",
                "Jeremiah Zhe Liu",
                "Jingbo Shang."
            ],
            "title": "Towards collaborative neural-symbolic graph semantic parsing via uncertainty",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 4160\u20134173, Dublin, Ireland. Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Zi Lin",
                "Du Phan",
                "Panupong Pasupat",
                "Jeremiah Zhe Liu",
                "Jingbo Shang."
            ],
            "title": "On compositional uncertainty quantification for seq2seq graph parsing",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory Cooper",
                "Milos Hauskrecht."
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 29.",
            "year": 2015
        },
        {
            "authors": [
                "Stephan Oepen",
                "Kristina Toutanova",
                "Stuart Shieber",
                "Christopher Manning",
                "Dan Flickinger",
                "Thorsten Brants."
            ],
            "title": "The LinGO redwoods treebank: Motivation and preliminary applications",
            "venue": "COLING 2002: The 17th International Conference on Compu-",
            "year": 2002
        },
        {
            "authors": [
                "Juri Opitz."
            ],
            "title": "SMATCH++: Standardized and extended evaluation of semantic graphs",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1595\u20131607, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Yuan Zhang",
                "Kelvin Guu."
            ],
            "title": "Controllable semantic parsing via retrieval augmentation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7683\u20137698, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A Smith",
                "Mike Lewis."
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "arXiv preprint arXiv:2210.03350.",
            "year": 2022
        },
        {
            "authors": [
                "Linlu Qiu",
                "Peter Shaw",
                "Panupong Pasupat",
                "Tianze Shi",
                "Jonathan Herzig",
                "Emily Pitler",
                "Fei Sha",
                "Kristina Toutanova."
            ],
            "title": "Evaluating the impact of model scale for compositional generalization in semantic parsing",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Machel Reid",
                "Graham Neubig."
            ],
            "title": "Learning to model editing processes",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3822\u20133832, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane A. Yu",
                "Zhengbao Jiang",
                "Fabio Petroni",
                "Patrick Lewis",
                "Gautier Izacard",
                "Qingfei You",
                "Christoforos Nalmpantis",
                "Edouard Grave",
                "Sebastian Riedel."
            ],
            "title": "PEER: A collaborative language model",
            "venue": "The Eleventh International Conference on",
            "year": 2023
        },
        {
            "authors": [
                "Hinrich Schutze",
                "Christopher D Manning",
                "Prabhakar Raghavan."
            ],
            "title": "Introduction to information retrieval",
            "venue": "Cambridge University Press.",
            "year": 2008
        },
        {
            "authors": [
                "Ivan Stelmakh",
                "Yi Luan",
                "Bhuwan Dhingra",
                "MingWei Chang."
            ],
            "title": "ASQA: Factoid questions meet long-form answers",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8273\u20138288, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Oriol Vinyals",
                "\u0141 ukasz Kaiser",
                "Terry Koo",
                "Slav Petrov",
                "Ilya Sutskever",
                "Geoffrey Hinton."
            ],
            "title": "Grammar as a foreign language",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Sean Welleck",
                "Ximing Lu",
                "Peter West",
                "Faeze Brahman",
                "Tianxiao Shen",
                "Daniel Khashabi",
                "Yejin Choi."
            ],
            "title": "Generating sequences by learning to self-correct",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Dongqin Xu",
                "Junhui Li",
                "Muhua Zhu",
                "Min Zhang",
                "Guodong Zhou."
            ],
            "title": "Improving AMR parsing with sequence-to-sequence pre-training",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2501\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629.",
            "year": 2022
        },
        {
            "authors": [
                "Yury Zemlyanskiy",
                "Michiel de Jong",
                "Joshua Ainslie",
                "Panupong Pasupat",
                "Peter Shaw",
                "Linlu Qiu",
                "Sumit Sanghai",
                "Fei Sha."
            ],
            "title": "Generate-and-retrieve: Use your predictions to improve retrieval for semantic parsing",
            "venue": "Proceedings of the 29th Inter-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Retrieval augmentation enhances generative language models by retrieving informative exemplars relevant for output prediction. However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget. In this work, we improve retrieval-augmented parsing for complex graph problems by exploiting two unique sources of information (1) structural similarity and (2) model uncertainty. We propose Structure-aware and Uncertainty-Guided Adaptive Retrieval (SUGAR) that first quantify the model uncertainty in graph prediction and identify its most uncertain subgraphs, and then retrieve exemplars based on their structural similarity with the identified uncertain subgraphs. On a suite of real-world parsing benchmarks with non-trivial graph structure (SMCalflow and E-commerce), SUGAR exhibits a strong advantage over its classic counterparts that do not leverage structure or model uncertainty."
        },
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have demonstrated remarkable capabilities as effective few-shot learners (Brown et al., 2020). Recently, a new learning paradigm called in-context learning has been developed. Under this paradigm, the model is given a prompt including test inputs and a few related exemplars, and can generate outputs directly without updating its parameters. A typical approach to obtaining these exemplars is to retrieve training examples similar to the test input (Pasupat et al., 2021; Gupta et al., 2022). However, for realistic parsing tasks with large output graphs and non-trivial structure (e.g., dialogue-oriented semantic parsing), the\n\u2020 Co-senior authors. \u2021 Work done at Google.\ninput similarity alone may not be effective in identifying the most informative exemplars for aiding graph prediction. As an example mentioned in Qiu et al. (2022), for the test input \u201cSchedule a meeting with my manager\u201d, it is more similar to example \u201cSchedule a meeting with Alex\u201d than \u201cWho is my manager\u201d, while the latter one contains an important action for searching an org chart which is also required by the test input.\nIn this paper, we explore effective approaches to improve the generalization performance of retrieval-augmented LLMs for parsing complex graphs. Specifically, we consider exploiting two sources of information uniquely available to this problem: (1) structural similarity between output subgraphs, and (2) model uncertainty in graph component prediction. The motivations behind our approach are two empirical investigations on LLM graph parsing (presented in Section 3): (a) Inadequacy of sequence-similarity retrieval (Section 3.1). When output graphs exhibit non-trivial structure, the exemplars retrieved based on sequence similarity is less effective than those based on graph similarity, even when the similarity is computed with respect to the gold output graphs 1. (b) LLM uncertainty correlates with performance (Section 3.2). We conduct a exploratory study of the quality of LLM uncertainty as an indicator of its generalization performance in graph prediction, and identified a monotonic between model uncertainty v.s. accuracy for node and edge prediction (Figure 2). This implies that model uncertainty can serve as an effective signal for identifying the subgraphs that the model is struggling to predict, thereby helping the retrieval algorithm to efficiency identify the most effective examplars for aiding model prediction, especially when the output graph is large.\nBased on the above observations, we propose\n1This is in contrast to earlier work where sequence similarity is already sufficient for simple sentences with shallow output structures (e.g., TOPS) (Zemlyanskiy et al., 2022)\nStructure-aware and Uncertainty-Guided Adaptive Retrieval (SUGAR), a retrieval-augmented LLM inference framework for complex graph parsing that incorporates both structural similarity and model uncertainty into retrieval procedure (Section 4). Operating in an iterative manner, SUGAR first identifies uncertainty regions in the model\u2019s graph prediction from the previous iteration, and adaptively retrieves exemplars based on their graph similarity with the identified uncertainty subgraphs (Figure 1, Section 4). In this way, SUGAR is able to better target model weaknesses in structural prediction by retrieving the most informative exemplars, which appears to be especially valuable in the setting of large and complex output graphs given limited retrieval budget.\nOn a suite of real-world complex graph parsing benchmarks (i.e., SMCalFlow and Ecommerce), SUGAR exhibits distinct strength over its classic counterparts that do not leverage uncertainty or structure, bringing clear performance improvement over the base model across iterations even when other retrieval-augmentation methods become counterproductive (Section 5). Further indepth analysis revealed the interesting role of exemplar quality on model uncertainty, the effectiveness of uncertainty as an early-stopping signal for retrieval iterations, and verifies the effectiveness of structural retrieval in improving model confidence (Section 6)2.\n2Open-source code may be found at https://github. com/google/uncertainty-baselines."
        },
        {
            "heading": "2 Related Work",
            "text": "Retrieval-Augmented Parsing. Sequence-tosequence (seq2seq) models have achieved stateof-the-art performance on many natural language processing tasks including complex parsing, e.g., dialogue-oriented semantic parsing and meaning representation parsing (Vinyals et al., 2015; Xu et al., 2020; Cui et al., 2022; Lin et al., 2022b,a). The general approach is to treat the output structure as a sequence and fine-tune a seq2seq model to learn the mapping between input sentences and output structures. To reduce the reliance on largescaled annotated data, several work augment the input with retrieved exemplars from the training data, with different strategy to select exemplars.\nFor unsupervised retrieval, Pasupat et al. (2021) and Gupta et al. (2022) retrieve exemplars with similar input encodings from a pre-trained neural encoder. Zemlyanskiy et al. (2022) retrieves exemplars for which the input and output (from the preliminary prediction) has high TF-IDF similarity with the sample input. The above work mainly focused on fine-tuning settings. For supervised retrieval, Rubin et al. (2022) suggest to use language models themselves to label examples that can serve as good prompts, and train a prompt retriever from this signal. In this work, we focus on unsupervised retrievers that do not rely on additional training data beyond the candidate pool they retrieve from.\nIterative Retrieval. While LLMs can generate coherent outputs via single-time retrieval-based augmentation, they often fall short in addressing more complex tasks. To address this, there have been various attempts to retrieve exemplars more than one time. Most of the work focused on ad-\ndressing long-form outputs such as long-form question answering tasks (Fan et al., 2019; Stelmakh et al., 2022). The basic idea is to decomposing complex question into several easier sub-questions, and iteratievely retrieving relevant information from knowledge agents for each sub-quesitons (Press et al., 2022; Yao et al., 2022; Khot et al., 2022). Based on this line of work, FLARE (Jiang et al., 2023) further proposes to actively retrieving when the sub-answer contains low-confident tokens.\nHowever, iterative retrieval for parsing complex structured outputs is less explored. The core challenge lies in the non-sequential nature of the output structure such as tree or graph, which means it cannot be simply decomposed sequentially. In this work, we aim at addressing complex parsing tasks, and target on progressively improving model\u2019s prediction by iteratively retrieving relevant examplars for model\u2019s uncertain sub-structures. As we will show in Section 3, this can not be achieved without incorporating structure and uncertainty."
        },
        {
            "heading": "3 Motivations",
            "text": "In this section, we present two ablation studies which serve as motivations for our methods."
        },
        {
            "heading": "3.1 Structural Similarity Matters",
            "text": "The first question is what to retrieve. Here we study how different similarity functions perform on different semantic parsing tasks, which is under in-context learning settings using GPT3.5 with 10 exemplars in the prompt.\nSpecifically, we test input sentence similarity using Universal Sentence Encoder (USE) (Cer et al., 2018) and BM25 (Schutze et al., 2008), and output similarity using BM25 and SMATCH (Cai and Knight, 2013). Note that SMATCH is the only metric that considers structural similarity beyond simple token overlapping in the output (more details in Appendix A). We choose three different\nsemantic parsing tasks with output structures from simple to complex, including (1) MTOP (Li et al., 2021): a user intent slot filling dataset which can be simplified as a sequence labeling task; (2) SMCalFlow (Andreas et al., 2020): a dataset of semantically detailed annotations of task-oriented natural dialogues, which can be taken as a tree parsing task; (3) Redwoods-Ecommerce (Ecommerce for short) (Oepen et al., 2002): a dataset of annotated meaning representation (outputs are directed acyclic graphs) based on English Resource Grammar (Flickinger et al., 2014), which is a DAG parsing task.\nThe evaluation results are shown in Table 1. We can observe gaps between standard retrieval (based on input similarity) versus oracle retrieval (based on output similarity), a finding that aligns with Qiu et al. (2022). Furthermore, as the output structure gets more complex (from MTOP to Ecommerce), it becomes more important to have exemplars that are similar in output structure, compared to just input similarity or sequence-level output similarity. This is because sequence-level similarity metrics only consider token overlapping, and ignore syntactic or semantic relationships in the output structure. As output becomes more complex, these similarity metrics are less likely to find structural similar exemplars. Therefore, it is important to have a structure-aware retriever for complex parsing tasks."
        },
        {
            "heading": "3.2 Model Uncertainty Matters",
            "text": "However, retrieving exemplars based structural similarity can have several challenges. First, in the initial settings we do not have access to gold output structures. This can be solved by getting some preliminary prediction using retrievals with similar inputs as proposed in Zemlyanskiy et al. (2022). Second, retrieving similar outputs based on the entire structures can be ineffective and may introduce unwanted noise. Given the emerging challenges, we are investigating if it is possible to measure struc-\ntural similarity only on necessary sub-structures. If so, which part to retrieve? Our hypothesis is that we can retrieve only when model is uncertain about some sub-structure predictions, which are very likely to be flawed.\nTo validate our hypothesis, our second ablation study analyzes LLM\u2019s behavior in predicting structure components in terms of model uncertainty. Specifically, we explore the correlation between model probability and performance for in-context learning model (more details can be found in Appendix B). As shown in Figure 2, high model probability generally corresponds to high performance and vice versa. Our study confirms that model uncertainty is effective for detecting prediction errors. This means that we can retrieve structurally similar exemplars targeting on these uncertain substructures, which can help to address the flawed parts in the prediction."
        },
        {
            "heading": "4 SUGAR: Structure-aware and Uncertainty-Guided Adaptive Retrieval",
            "text": "This section describes details of SUGAR for parsing complex structures. Typically, the output structure is a semantic graph that is rooted, directed, acyclic and labeled (Opitz, 2023). Problem Formulation. We aim to solve a graph parsing problem that maps from a natural language utterance x to a target graph representation G = \u27e8N,E\u27e9, where N is the set of nodes and E \u2208 N\u00d7N is the set of edges. For seq2seq models, G is represented as a linearized graph string y. Following Lin et al. (2023), we adopt PENMAN annotation (Kasper, 1989) to linearize all graph structures in this work, which is a serialization format for the directed, rooted graphs used to encode semantic dependencies (details for graph linearization can be found in Appendix C).\nFigure 1 shows an overview of SUGAR and Algorithm 1 summarizes the detailed process. In Section 4, we illustrate the retrieval process based\nAlgorithm 1 Retrieval-augmented Inference with SUGAR Precompute:\nExamplar pool P = {(xc, yc)}|P |c=1 Subgraph pool Pg = \u222acSd(yc) \u25b7 (Sec. 4.1.3)\nInput: Test input x Output: Final graph prediction y\u0302i Initialize: E0 = base_retriever(x) \u25b7 Initial retrieval. y\u03020 = LLM(x,E0) \u25b7 Initial prediction.\n# Iterative retrieval with early stopping. for i \u2208 [1, ..., max_iter] do\n# Graph uncertainty quantification (Sec. 4.1.1). {pv}v = graph_component_probability(y\u0302i\u22121) # Confidence-based early stopping. if pv > conf_threshold \u2200v then\nreturn y\u0302i\u22121 # Construct uncertainty subgraphs (Sec. 4.1.2). {y\u0302ik}k = get_uncertain_subgraph({pv}v) # Structure-aware retrieval (Sec. 4.1.3). Ei = \u2205 for y\u0302ik \u2208 {y\u0302ik}k do\nEik = structure_aware_retrieval(y\u0302ik , Pg)) Ei.add(Eik)\n# Retrieval-augmented prediction. y\u0302i = LLM(x,Ei, y\u0302i\u22121)\non preliminary predictions y\u0302i at step i (the retrieval process in Figure 1), which includes three steps: (1) graph uncertainty quantification for y\u0302i (Section 4.1.1); (2) uncertain subgraph construction, i.e., y\u0302ik (Section 4.1.2); (3) structure-aware retrieval for y\u0302ik (Section 4.1.3). The retrieval process is operated in an iterative manner, which enables a progressive improvement for model\u2019s prediction by iteratively retrieve exemplars based on model\u2019s predictive uncertainty from the previous turn (Section 4.2)."
        },
        {
            "heading": "4.1 Retrieval Process",
            "text": ""
        },
        {
            "heading": "4.1.1 Graph Uncertainty Quantification",
            "text": "Recent year witnessed the success of applying seq2seq models to graph parsing tasks, where the outputs are compositionally structured (e.g., a graph or a tree). However, these seq2seq approaches pose a technical challenge in properly quantifying the model uncertainty for graph prediction. This is because the autoregressive seq2seq probability is not well-suited for describing model uncertainty in predicting elements or substructures of the output graph, where the probabilistic graphical model (PGM) is a more suitable formalism. To address this issue, we leverage Graph Autoregressive Process (GAP) proposed by Lin et al. (2023) to allow the correspondence between seq2seq output probability to PGM probability, i.e., assigning\nmodel probability for a node or edge on the graph. Specifically, given an input sequence x and output sequence y = y1y2 \u00b7 \u00b7 \u00b7 yN that refers to a graph G = \u27e8N,E\u27e9, GAP can map the token-level autoregressive distribution\np(y|x) = N\u220f i=1 p(yi|y<i, x)\nto graphical model likelihood\np(G|x) = \u220f v\u2208G p(v| pa(v), x)\n= \u220f n\u2208N p(n|pa(n), x) \u2217 \u220f e\u2208E p(e|pa(e), x)\nwhere p(v|pa(v), x) is the conditional probability for graph elements v with respect to their parents pa(v) in G."
        },
        {
            "heading": "4.1.2 Uncertain Subgraph Construction",
            "text": "To leverage uncertainty in the model prediction p(G|x) for efficient retrieval, we consider the concept of uncertain subgraph which is a subgraph that contains:\n\u2022 Uncertain element. We consider a graph element v \u2208 G to be uncertain if its probability p(v|pa(v), x) is below a certain threshold \u03f5.\n\u2022 Relatively-confident neighbors. Given a uncertain element v and a subgraph sd(v) that surrounds v and with maximum depth d. We define the relatively-confident neighbors of v as the subset cd(v) \u2282 sd(v) whose probability is above a certain threshold \u03f5.\nAs shown, by coupling the uncertain element v with its relatively-confident graph neighbor cd(v), uncertain subgraphs y\u0302v = {v} \u222a cd(v) provides the retrieval algorithm fine-grained and contextualized information about model uncertainty in the prediction of graph elements (see Figure 3 for an example). In practice, to limit the size of uncertain subgraphs and keep the cost of structural similarity computation within a feasible range, we set a parameter d to control the maximum depth of uncertain subgraphs y\u0302v."
        },
        {
            "heading": "4.1.3 Structure-aware Retrieval",
            "text": "To identify informative graph exemplars that best address the model uncertainty in predicting graph elements, we leverage the uncertain graph introduced above and consider a retrieval policy using\ny\u0302v\u2019s as the query (with the uncertain element v masked out) to retrieve structurally similar exemplars.\nSpecifically, we consider the typical setting where there is a retrieval candidates pool P = {(xc, yc)}|P |c=1 which are pairs of input sentences xc and output graphs yc. To perform structure-aware retrieval, we first prepare a subgraph retrieval pool Pg = \u222a|P |c=1Sd(yc), where Sd(yc) = {ycj}j is the set of all depth-d subgraphs of yc. Then, at inference time, given every uncertain subgraph ud(v), we retrieve {yc\u2032,j\u2032}c\u2032,j\u2032 \u2282 Pg based on their graph similarity with ud(v), which eventually leads to the set of exemplars {(xc\u2032 , yc\u2032)}c\u2032 that will be used for the retrieval-augmented inference.\nPractical Implementation. In this work, we consider the SMATCH metric (Cai and Knight, 2013) for computing graph similarity. Given a query graph with Nq nodes and k candidate graphs with Nc nodes each, the time complexity of the graph matching algorithm is O(k \u2217 Nq \u2217 Nc). In practice, the size of Nq is controlled by d, and k can be significantly reduced by first pre-filter the candidate pool using a fast heuristic metric (e.g., atom similarity) 3."
        },
        {
            "heading": "4.2 Improve Parsing Performance with",
            "text": "Uncertainty-aware Iterative Retrieval\nDue to its uncertainty-aware nature, SUGAR introduced in Section 4.1 can be applied iteratively to\n3In our experiment, we choose d = 3 and retrieve based on 1,000 candidate graph pool, and our exception time is around 5 iterations per seconds.\nmodel prediction, by continuously retrieving new exemplars to address model uncertainty in the previous iteration until model reached a satisfactory level of confidence. This is analogous to the iterative refinement approaches in the recent literature where a model\u2019s initial generation can improved by additional self-correction steps (Reid and Neubig, 2022; Schick et al., 2023; Welleck et al., 2023; Jiang et al., 2023).\nIn the experiment (Section 6.2), we study model performance under different retrieval and refinement strategies, validating that incorporating structure and uncertainty information are both important for improving parsing performance under iterative refinement."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Datasets & Model Settings",
            "text": "Datasets. In this paper, we use two complex semantic parsing datasets, including dialogue-oriented semantic parsing and graph-based grammar parsing.\n\u2022 SMCalFlow. SMCalFlow (Andreas et al., 2020) is a large corpus of semantically detailed annotations of task-oriented natural dialogues. The annotation uses dataflow computational graphs, composed of rich set of both general and application specific functions, to represent user requests as rich compositional expressions.\n\u2022 Redwoods-Ecommerce (Ecommerce). The LinGO Redwoods Treebank is a collection of hand-annotated corpora for an English grammar consisting of more then 20 datasets (Oepen et al., 2002). We choose the Ecommerce subset of Redwoods which consists of email messages composed by the online customers. The graph output of this subset is sufficiently complex, and is considered out-of-distribution compared to the standard training split of Redwoods based on Wall Street Journal (Lin et al., 2022b).\nModels & Base Retrievers. We choose GPT3.5 (text-davinci-003; Ouyang et al., 2022) as our large language models for in-context learning settings. To initialize SUGAR prediction, we consider three choices of base retrievers to be used for initializing SUGAR prediction: (1) Random, (2) CASPERuse (Pasupat et al., 2021) that is based on Universal Sentence Encoder (USE) (Cer et al.,\n2018), and (3) BM25 (Robertson et al., 2009). Recall that these retrievers will only be used to initialize the first round of SUGAR prediction, and not used in subsequent iterations (Algorithm 1). Baselines. Due to the iterative nature of SUGAR, prediction results are comparable to base retrievers mentioned above using the same total number of exemplars. For example, base retriever using 8 exemplars is comparable to SUGAR at iteration 1 (5 base exemplars plus 3 iterative exemplars). However, due to the sequence limitation, it is unfeasible to add large number of exemplars in one prompt, which makes it impossible to get results for base retrievers using more than 10 exemplars. This also highlights the advantage of iterative retrieval as it can get rid of sequence length limitation. Another benefit is that it can get the model\u2019s intermediate results towards the final optimal prediction, and more exemplars are added to concentrate on these intermediate results\u2019 weak parts.\nWe also consider two iterative variant of baselines that is based on output similarity: GandRiter (Zemlyanskiy et al., 2022) that retrieves examplars based on BM25 with both input sentence and predicted graphs (weight \u03b1=0.5)4, and also Oracle that retrieves examplars based on graph similarity with gold subgraphs. Further details about settings of model, candidate pool and prompts can be found in Appendix D, and Appendix E reports additional supplementary studies on much smaller models T5 (Raffel et al., 2020) for out-of-domain and low-resource settings."
        },
        {
            "heading": "5.2 Results",
            "text": "The evaluation results are shown in Figure 4. As shown, SUGAR progressively improves the model prediction across iterations, even in the setting where its iterative counterparts becomes counterproductive (e.g, Base=BM2.5 on Ecommerce). Specifically, SUGAR significantly outperforms its base retrievers with the same retrieval budget (Base@8), improving the base retriever CASPERuse and BM25 by 26.61% and 20.36% respectively in absolute percentage on SMCalflow, and 17.58% and 12.37% respectively in absolute percentage on Ecommerce.\nIn Appendix F, to understand if the benefit of SUGAR is orthogonal to base model choice, and cannot be surpassed by simply upgrading model\n4In ablation analysis Section 6.2, we also consider a variant of GandR that incorporates model uncertainty.\nmodel architecture, we conduct the following two sets of experiments: (1) a comparison between SUGAR and its baseline variants based on older variants of GPT models (e.g., text-davinci-002) which differ in training method. (2) comparison of baseline methods under GPT-4 v.s. SUGAR under GPT-3.5. The experimental results have shown that SUGAR successfully improves all LLMs in terms of exact match and graph similarities. Furthermore, SUGAR with GPT3.5 also works better than GPT4 using base retriever. This concretely shows our method provides non-trivial improvement beyond what can be achieved by simply upgrading architecture.\nTable 2 shows some exemplars (sentence inputs) from different retrievers. We can find that compared to other retrievers, SUGAR can accurately retrieve exemplars that closely align with the uncertain parts of the model prediction, which is significant in addressing the uncertainties that are inherent in the model inference, thereby refining the model\u2019s prediction."
        },
        {
            "heading": "6 Analysis: the Role of Uncertainty",
            "text": ""
        },
        {
            "heading": "6.1 Uncertainty Quality Matters",
            "text": "We notice that SUGAR does not work very well on the random base retriever, and this might be due to the relatively poor uncertainty quality of the random base retriever in comparison to the other two base retrievers. To validate our hypothesis, we further evaluate the uncertainty quality of the three base retrievers.\nA common approach to evaluate model\u2019s uncer-\ntainty quality is to measure its calibration performance, i.e., whether the model\u2019s predictive uncertainty is indicative of the predictive error, e.g., expected calibration error (ECE; Naeini et al., 2015). Based on ECE, Lin et al. (2023) propose Compositional Expected Calibration Error (CECE) to measures the difference in expectation between the model\u2019s predictive performance on graph elements and their actual match to the gold graph, which can better reflect model\u2019s behavior in predicting graph structures. Table 3 reports the CECE results (based on all graph elements, nodes, and edges).\nWe can find that the calibration for random base retriever is consistently worse than BM25 and CASPERuse, indicating that the relativelyconfident neighbors in uncertain subgraphs for random retriever might fail to capture accurate contexts for iterative retrieval. This also indicates that improving LLMs\u2019 calibration can be a fruitful venue for improving retrieval-based data augmentation."
        },
        {
            "heading": "6.2 Uncertainty as a Early Stopping Signal",
            "text": "Given that the parsing generation in Figure 1 to edit previous model prediction is a zero-shot process, i.e., the LLM has not been provided with any examples to improve model prediction at each step, thus the model has a tendency to keep making edits to predictions from previous iterations, even if they are already accurate. At this stage, uncertainty becomes a significant signal for stopping the iteration process.\nSpecifically, SUGAR will terminate the iteration retrieval process and return the last model predic-\ntion once a satisfactory level of confidence has been achieved. Additionally, we incorporate model uncertainty as a stopping signal for the GandRiter baseline (Table 4), and we consistently observe an improvement compared to the baseline that doesn\u2019t utilize uncertainty as a stopping signal. This further validates the crucial role of uncertainty in the iterative retrieval process."
        },
        {
            "heading": "6.3 Uncertainty across Retrieval Iterations",
            "text": "Two additional interesting questions could be (1) what type of graph requires more iterations? and (2) how does uncertainty evolve across different retrieval iterations?\nWe first explore the correlation between number of iterations and graph complexity and report the results in Table 5 (base retriever is CASPERuse). As we expected, a graph of higher complexity typically demands a greater number of iterations before achieving a satisfactory level.\nWe then visualize some graphs at different iterations and explore the progression of uncertainty levels (see details in Appendix G). Generally, we notice that an uncertain graph elements will become less uncertain as iterations progress. However, we also observe occasional fluctuations in the uncertainty, which can trigger instability in neighboring contexts. This means that as uncertain elements become certain, the contexts around them may lose some degree of confidence. We reserve further exploration of this phenomenon for future work, which we believe is a promising direction for understanding LLMs\u2019 calibration in complex structure generation."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this work, we present Structural-aware and Uncertainty-Guided Adaptive Retrieval (SUGAR), a new retrieval-augmented parsing framework us-\ning LLMs for complex graphs. This work deepens the current practice of retrieval-augmented models for complex structures by incorporating information related to the model\u2019s uncertainty of graph component prediction and structural similarity of output subgraphs. Experimental results on two complex seq2seq semantic parsing tasks, i.e., SMCalFlow and E-commerce, have demonstrated the practical effectiveness of the proposed approach in the modern setting of graph parsing with pretrained LLMs.\nOur future work includes considering more advantage graph similarity metric beyond SMATCH (e.g., incorporates additional similarity metric between the space of node and edge properties), and also larger scale and more fine-grained evaluation on graph parsing benchmarks with distinct properties (e.g., long-tail generalization, graph of specific families) to further elucidate in what setting is the graph similarity most effective. Furthermore, it is also of interest to study the generalization of this approach to a broader class of modern program synthesis problems, e.g., code generation."
        },
        {
            "heading": "Acknowledgement",
            "text": "Our work is sponsored in part by National Science Foundation Convergence Accelerator under award OIA-2040727 as well as generous gifts from Google, Adobe, and Teradata. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon. We thank Deepak Ramachandran for helpful discussion.\nLimitations\nThis work focuses on advanced inference procedures to improve retrieval-augmented LLMs for complex graph parsing problems. The work is limited in two aspects due to the nature of LLM setup: (1) our evaluation has focused on the in-context learning setting where the LLM is not fine-tuned on domain-specific data. Although a standard setting of modern LLMs, it is still of scientific interest to understand the interplay between parameter finetuning and the effectiveness of retrieval-augmentation procedures, which we leave for future work. (2) This work has focused on GPT3.5 which was one of the strongest and openly available LLMs at the time of the writing. As the behavior of LLM can be impacted by its pretraining procedures, it is also of interest to generalize this study to a wider class of LLMs. (3) Finally, the graph similarity metric considered in this work (i.e., SMATCH) has a computational complexity quadratic in graph size. The current work mitigates the issue by restricting its attention to degree-d subgraphs, with the caveat of limiting SUGAR\u2019s ability to reason about similarity in the global graph structure. Therefore identifying practical and more computationally efficient structure similarity metrics can further improve the scalability of the SUGAR approach."
        },
        {
            "heading": "A Graph Matching Algorithm in",
            "text": "SMATCH\nIn general, finding the largest common subgraph is a well-known computationally intractable problem in graph theory. However, for graph parsing problems where graphs have labels and a simple tree-like structure, some efficient heuristics are proposed to approximate the best match by a hillclimbing algorithm (Cai and Knight, 2013). The initial match is modified iteratively to optimize the total number of matches with a predefined number of iterations (default value set to 5). This algorithm is very efficient and effective, it was also used to calculate the SMATCH score in Cai and Knight (2013)."
        },
        {
            "heading": "B Model Calibration",
            "text": "To explore the model calibration for parsing complex graph structure under in-context learning settings, we plot histgrams to how the correlation between model probability and performance. Specifically, we use base retrievers USE-input, BM25input and Random, and test them for GPT3.5\u2019s incontext learning settings on SMCalflow and Ecommerce respectively. Results are shown in Figure 5. As can be seen from figures, the model is generally calibrated, i.e., high probability generally corresponds to high performance and vice versa. Our\nstudy confirms that model uncertainty is effective for detecting prediction errors. This means that we can retrieve structurally similar exemplars targeting on these uncertain substructures, which can help to address the flawed parts in the prediction."
        },
        {
            "heading": "C Graph Linearization",
            "text": "We use PENMAN notation for graph linearzation, which is originally called Sentence Plan Notation in the PENMAN project (Kasper, 1989). PENMAN is a serialization format for the directed, rooted graphs used to encode semantic dependencies, mostly notably in the Abstract Meaning Representation (AMR) framework (Banarescu et al., 2013). It looks similar to Lisp\u2019s S-Expression in using parentheses to indicate nested structures.\nTo make PENMAN notation compatible with the seq2seq learning, we adopted a variable-free version of PENMAN which was first proposed in Lin et al. (2022b). Table 6 shows some variable-free PENMAN linearized examples for the two semantic parsing datasets we adopt in our experiments."
        },
        {
            "heading": "D Detailed Experiment Settings",
            "text": "Parameter Settings For the uncertain threshold \u03f5, considering that at the initial stage, the model\u2019s predictions are relatively weak, we set a warm up schedule for \u03f5. Specifically, \u03f51 = 0.5, \u03f52 = 0.8, \u03f53 = 0.9. We set subgraph max depth d = 3. For each model prediction, the number of uncertain subgraphs k = 3, and we will retrieve 1 exemplars for each uncertain subgraph.\nModels For in-context learning settings, considering the impressive performance achieved by GPT3.5 (Ouyang et al., 2022), we test our methods on text-davinci-003. For fine-tuning settings, we choose T5 (Raffel et al., 2020) as our pretrained model, which is a pre-trained sequence-tosequence Transformer model that has been widely used in many NLP applications. We use the opensourced T5X5, which is a new and improved implementation of T5 codebase in JAX and Flax. Specifically, we use the official pretrained T5-Large (770 million parameters).\n5https://github.com/google-research/t5x\nTraining Data For in-context learning, as we access the model through a paid API6, and there is a limitation for sequence length, we sample a subset of 1,000 test examples from each datasets for test set. For SMCalFlow, we only consider firstturn dialogue in order to reduce sequence length. The candidate pool for SMCalflow is a set of 2,000 examples sampled from the standard training set. The candidate pool for Ecommerce is the standard development set (1.7K examples).\nFor fine-tuning settings, the training data for SMCalFlow is a set of 2,000 examples sampled from the standard training set, which also serves as the candidate pool for retrieval, and the test set is the standard development set (15K examples). The training set for Ecommerce is the Redwoods\u2019 indomain dataset on Wall Street Journal (34K examples), the test set and candidate pool are the standard test and development set of Ecommerce (1.1K and 1.7 examples respectively).\nPrompt Design There are two prompts adopted in the in-context settings using GPT3.5. The first one contains exemplars from the base retriever to generate the preliminary prediction at the initial stage (prompt 1 shown in Figure 6). The second one contains exemplars from uncertainty-guided retrieval and incorporates the prediction from the previous step (prompt 2 shown in Figure 7)."
        },
        {
            "heading": "E Fine-tuning Results",
            "text": "We conduct a supplementry evaluation for finetuning settings, where we focus on low-resource (SMCalFlow) and out-of-domain (Ecommerce) settings7. Specifically, we compare SUGAR with two\n6https://api.openai.com/v1/completions 7Since the cost of fine-tuning is higher, we only need one\niteration for fine-tuning settings.\nbaseline retrievers: (1) CASPER using Universal Sentence Encoder (CASPER-USE; Pasupat et al., 2021); (2) GandR considering input and output similarity using BM25 with weight \u03b1 = 0.5 (Zemlyanskiy et al., 2022).\nThe evaluation results are reported in Table 7. As can be seen, other retreivers that do not consider structural similarity all fail on these two complex parsing tasks, i.e., perform even worse than base model without retrieval, while SUGAR can persistently improve base model on both datasets. Specifically, SUGAR achieves error reduction rate as 6.72% and 9.45% on SMCalflow and Ecommerce respectively, and improves exact match rate by 2.72% and 8.38% respectively."
        },
        {
            "heading": "F Results with Other LLMs",
            "text": "The results of SUGAR with text-davinci-002 and GPT4 are shown in Table 9. We can see that: (1) SUGAR successfully improves all LLMs in terms of exact match and graph similarities. Note that SUGAR Iter 1 is comparable to BM25@8 given the same number of exemplars, and SUGAR Iter 2 and Iter 3 can further improve the results without increasing the general sequence length of the prompt; (2) SUGAR with GPT3.5 also works better than GPT4 using base retriever. This con-\ncretely shows our method provides non-trivial improvement beyond what can be achieved by simply upgrading architecture. We will experiment with SUGAR+GPT4 in future work (cannot do it right now due to API limitations)."
        },
        {
            "heading": "G Sample Graph Prediction Visualizations",
            "text": "Some sample graph prediction visualizations on Ecommerce dataset using CASPER-USE are shown in Table 8. We can observe that as iteration goes, the confidence scores of uncertain graph elements generally increase and the number of uncertain graph elements generally decreases.\nHowever, we also observe occasional fluctuations in the uncertainty, which can trigger instability in neighboring contexts. For example, the\nnode pron from the first to second iteration in the second example (the probability decreases from 0.9381 to 0.8746). We reserve this observation for further exploration in future work, which we believe is a promising direction for understanding LLMs\u2019 calibration in complex structure generation.\nSentence I have received two shipments from you for one order.\nIter1\nIter2\nIter3 Sentence I need the instruction about returning goods and repayment.\nIter1\nIter2\nIter3"
        }
    ],
    "title": "Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty",
    "year": 2023
}