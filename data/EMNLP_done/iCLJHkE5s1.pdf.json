{
    "abstractText": "The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like TransformerXL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAiningfree Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haofei Yu"
        },
        {
            "affiliations": [],
            "name": "Cunxiang Wang"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        },
        {
            "affiliations": [],
            "name": "Wei Bi"
        }
    ],
    "id": "SP:0a1c6413a02098c23842803eac439ab91b78b0cc",
    "references": [
        {
            "authors": [
                "Amanda Bertsch",
                "Uri Alon",
                "Graham Neubig",
                "Matthew R Gormley."
            ],
            "title": "Unlimiformer: Longrange transformers with unlimited length input",
            "venue": "arXiv preprint arXiv:2305.01625.",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever."
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509.",
            "year": 2019
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime G Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov."
            ],
            "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Giannis Daras",
                "Nikita Kitaev",
                "Augustus Odena",
                "Alexandros G Dimakis."
            ],
            "title": "Smyrf-efficient attention using asymmetric clustering",
            "venue": "Advances in Neural Information Processing Systems, 33:6476\u20136489.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Yunsu Kim",
                "Duc Thanh Tran",
                "Hermann Ney"
            ],
            "title": "When and why is document-level context useful in neural machine translation",
            "venue": "In Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Nikita Kitaev",
                "Lukasz Kaiser",
                "Anselm Levskaya."
            ],
            "title": "Reformer: The efficient transformer",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Matt Mahoney"
            ],
            "title": "Large text compression benchmark",
            "year": 2011
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher."
            ],
            "title": "Pointer sentinel mixture models",
            "venue": "International Conference on Learning Representations.",
            "year": 2016
        },
        {
            "authors": [
                "Hao Peng",
                "Jungo Kasai",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Zhaofeng Wu",
                "Lingpeng Kong",
                "Roy Schwartz",
                "Noah A Smith."
            ],
            "title": "Abc: Attention with bounded-memory control",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Hao Peng",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Roy Schwartz",
                "Noah Smith",
                "Lingpeng Kong."
            ],
            "title": "Random feature attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Micha\u0142 Pietruszka",
                "\u0141ukasz Borchmann",
                "\u0141ukasz Garncarek."
            ],
            "title": "Sparsifying transformer models with trainable representation pooling",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Aurko Roy",
                "Mohammad Saffar",
                "Ashish Vaswani",
                "David Grangier."
            ],
            "title": "Efficient content-based sparse attention with routing transformers",
            "venue": "Transactions of the Association for Computational Linguistics, 9:53\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "\u00c9douard Grave",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Adaptive attention span in transformers",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Da Ju",
                "Spencer Poff",
                "Stephen Roller",
                "Arthur Szlam",
                "Jason Weston",
                "Angela Fan."
            ],
            "title": "Not all memories are created equal: Learning to forget by expiring",
            "venue": "International Conference on Machine Learning, pages 9902\u20139912. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Donald Metzler."
            ],
            "title": "Efficient transformers: A survey",
            "venue": "ACM Computing Surveys, 55(6):1\u201328.",
            "year": 2022
        },
        {
            "authors": [
                "Apoorv Vyas",
                "Angelos Katharopoulos",
                "Fran\u00e7ois Fleuret."
            ],
            "title": "Fast transformers with clustered attention",
            "venue": "Advances in Neural Information Processing Systems, 33:21665\u201321674.",
            "year": 2020
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma."
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "arXiv preprint arXiv:2006.04768.",
            "year": 2020
        },
        {
            "authors": [
                "Lesly Miculicich Werlen",
                "Dhananjay Ram",
                "Nikolaos Pappas",
                "James Henderson."
            ],
            "title": "Documentlevel neural machine translation with hierarchical attention networks",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Lin Zheng",
                "Chong Wang",
                "Lingpeng Kong."
            ],
            "title": "Linear complexity randomized self-attention mechanism",
            "venue": "International Conference on Machine Learning, pages 27011\u201327041. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zheng",
                "Jianbo Yuan",
                "Chong Wang",
                "Lingpeng Kong."
            ],
            "title": "Efficient attention via control variates",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Yuchen Eleanor Jiang",
                "Peng Cui",
                "Tiannan Wang",
                "Zhenxin Xiao",
                "Yifan Hou",
                "Ryan Cotterell",
                "Mrinmaya Sachan."
            ],
            "title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "venue": "arXiv preprint arXiv:2305.13304.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Transformer-based models (Kenton and Toutanova, 2019; Liu et al., 2019; Raffel et al., 2020; Lan et al., 2019; Brown et al., 2020) have achieved remarkable performance over the past few years. The key component of these model architectures is the attention mechanism (Vaswani et al., 2017). However, the original attention design struggles to efficiently handle long sequences, which becomes particularly problematic in scenarios such as document-level translation (Werlen et al., 2018; Kim et al., 2019) and large-scale text generation (Zhou et al., 2023), as its time and space computation costs increase quadratically with the sequence length (Tay et al., 2022). The primary factor for this elevated computational complexity can be traced back to the multiplication between queries and keys used in\n\u2217Work done during internship at Tencent AI Lab. \u2020 Co-first Author. \u2021 The correponding author.\nQ ue\nry\nKey\nMemory Selection\nthe attention module. In general, the time complexity for calculation is O(N2d) if a transformer model with d dimensions is set up with an input consisting of N tokens.\nTo tackle this computation bottleneck, numerous efforts have been made. The first line of work is to find a new efficient expression to compute the attention score. Despite the advancements made, these methods often compromise performance, thus paving the way for alternative solutions. Efficient architectures that provide an approximate expression of attention have been explored widely (Wang et al., 2020; Peng et al., 2022b,a; Choromanski et al., 2021; Zheng et al., 2022b,a). The second line of work is to keep the calculation expression the same and use an external structure like hash function (Kitaev et al., 2019; Daras et al., 2020), clustering (Roy et al., 2021; Vyas et al., 2020) and memory selector (Pietruszka et al., 2022; Dai et al., 2019; Bertsch et al., 2023; Sukhbaatar et al., 2021, 2019; Child et al., 2019) to find the suitable subset of queries and keys in the long sequence for attention calculation.\nOur work falls into the second category, in which we propose a training-free memory selection mechanism to select suitable tokens for attention computation. Specifically, we focus on pushing Transformer-XL (Dai et al., 2019) architecture to a better position by selecting higher-quality tokens inside its memory. Based on our initial investigation, we construct a memory subset by selecting 50% of the memories with the largest attention values and maintaining the same performance. It indicates that a large portion of information in memory is not fully utilized. This motivates us to explore better methods to optimize memory usage.\nIllustrated in Figure 1, we propose a TRAiningfree Memory Selection method (TRAMS) that can be directly plugged into memory-based long-range language models and reduces the time complexity of computing attention matrix. Through experiments on two language modeling benchmark datasets, namely word-level WikiText-103 (Merity et al., 2016) and character-level enwik8 (Mahoney, 2011), we achieve an improvement in the model\u2019s performance, as demonstrated by a 0.19 perplexity (ppl) drop in WikiText-103 and a 0.017 reduction in bits-per-character (bpc) in enwik8.\nTo our knowledge, we are the first to design a training-free memory selection method based on Transformer-XL architecture.1"
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 Problem Definition",
            "text": "We use h \u2208 RN\u00d7d to represent the input hidden states for the attention module, o \u2208 RN\u00d7d to represent the output hidden states for the attention module, m \u2208 RM\u00d7d to represent the memory hidden states used in the attention calculation. We use WQ, WK , WV to represent the trainable projection matrix in the attention module. We define d for the dimension of the model, M for the memory size, and N for the input size. The attention calculation process can be formally written as o = Attn(h,m).\nWith the above annotations, the problem of memory selection can be defined as choosing a subset of hidden states memory m\u0303 from the memory m that brings the minimum difference to the transformer layer output but with a smaller memory size.\nm\u0303\u2217 = argmin m\u0303\u2282m \u2225Attn(h,m\u0303) \u2212Attn(h,m)\u2225 (1)\n1Source code for this paper is available at https://github.com/lwaekfjlk/TRAMS."
        },
        {
            "heading": "2.2 Attention Reformulation",
            "text": "In a memory-augmented language model, the standard attention mechanism between input hidden states and memory hidden states can be written as:\nAttn(h,m) = softmax( QK\u22ba \u221a d )V (2)\nwhere Q = hWQ is the product of target token hidden states h and query projection matrix WQ; K = mWK is the product of memory token hidden states m and key projection matrix WK ; V = mWV is also the product of memory token hidden states m and value projection matrix WV .\nDifferent from the well-known attention score calculation, we can compute this attention formula in a different order but maintain the same result:\nQK\u22ba = (hW \u22baQ)(mW \u22ba K) \u22ba\n= (h)(W \u22baQWKm) = (h)(mW \u22baKWQ) \u22ba (3)\nThus, we define Q\u2032 = h as the reformulated query for this attention expression and K \u2032 =mW TKWQ as the reformulated keys for attention. With this reformulation, we transfer all attention-related parametric information onto reformulated key vectors."
        },
        {
            "heading": "2.3 Transformer Hidden Space",
            "text": "Since h is the input of the current transformer layer and also the output of the previous transformer layer, it is the result of the last layer\u2019s Layernorm operation. We can define the coordinate-wise average of h as \u00b5 and the coordinate-wise standard deviation of h as \u03c3. Expressions can be written as:\n\u00b5 = 1\nd\nd\n\u2211 i=1\nhi \u2248 0, \u03c3 =\n\u00bf \u00c1 \u00c1\u00c01\nd\nd\n\u2211 i=1 (hi \u2212 \u00b5)2 \u2248 1 (4)\nSince the mean value for the hidden states h is around zero, we can confirm the hidden states vectors are approximately orthogonal to the 1\u20d7 vector and the L2 norm of hidden states is around \u221a d.\nWith this approximation, we can expand our reformulated attention score as:\nQ\u2032K \u2032\u22ba = (h)(mW \u22baKWQ) \u22ba\n= \u2223\u2223Q\u2032\u2223\u2223 \u22c5 \u2223\u2223K \u2032\u2223\u2223 \u22c5 cos \u27e8Q\u2032,K \u2032\u27e9 \u2248 \u221a d \u22c5 \u2223\u2223K \u2032\u2223\u2223 \u22c5 cos \u27e8Q\u2032,K \u2032\u27e9 (5)\nwhere \u2225Q\u2032\u2225 stands the L2 norm for Q\u2032 and \u2225K \u2032\u2225 stands for the L2 norm for K \u2032. Based on Fig 2,\nThe red distribution represents the query norm. The blue distribution represents the key norm.\nwe see that reformulated query norm \u2223\u2223Q\u2032\u2223\u2223 has a much sharper distribution compared with key norm \u2223\u2223K \u2032\u2223\u2223, indicating reformulated query norm can be approximated by a constant factor.\n2.4 Training-free Memory Selection (TRAMS)\nOur target for memory selection is to recover the complete attention score with as few memory tokens as possible. This problem is equivalent to finding the subset of memory tokens that have the highest attention scores with queries. We propose a heuristic method to perform token-level selection based on a memory-independent metric in this section.\nThere are two crucial components for calculating the attention score after approximating \u2223\u2223Q\u2032\u2223\u2223 with a constant factor: the norm of the reformulated keys \u2223\u2223K \u2032\u2223\u2223 and the angles between the reformulated keys and queries arccos \u27e8Q\u2032,K \u2032\u27e9, which is proved in Khandelwal et al. (2019). Commonly, we believe that arccos \u27e8Q\u2032,K \u2032\u27e9 is the more important factor in general. Yet, if we use the ranking of attention score value for all query and key pairs as ground-truth ranking, based on Fig 3, we empirically discovered that rankings based on key norms and rankings based on angles produce close Spearman correlation scores when only taking the highest 1% attention scores into account. Therefore, it indicates that we can rank our memory tokens based on \u2223\u2223K \u2032\u2223\u2223 solely to gain a relatively good performance when we desire top 1% attention scores with queries in our memories instead of all.\nAdditionally, we discovered that relying solely on a large norm isn\u2019t sufficient as a constraint. Specifically, keys that are nearer to 1\u20d7 tend to yield a higher attention score. To address this, we introduce a combined metric: s = cos \u27e8K \u2032, 1\u20d7\u27e9 \u2223\u2223K \u2032\u2223\u2223.\nThis metric allows us to identify tokens that can produce very high attention scores when paired with the appropriate query (owing to a high value of \u2223\u2223K \u2032\u2223\u2223) and very low scores when paired with an unsuitable query. This is due to the near orthogonality to the query space, as indicated by a small angle with 1\u20d7, which is orthogonal to the query space."
        },
        {
            "heading": "3 Experiments",
            "text": "We introduce the compared methods and report the main results and analysis on different attention variants for inference in this section. Datasets details for WikiText-103 and enwik8 benchmarks and their evaluation metric details are included in Appendix A. The details of the model that we built memory selection on can be seen in Appendix B."
        },
        {
            "heading": "3.1 Compared Methods",
            "text": "Transformer+RPE (Vaswani et al., 2017): the vanilla transformer baseline with relative position embedding that is the same as Transformer-XL. Therefore, the only difference between this model and Transformer-XL is the additional memories. More information related to relative position embedding can be seen in Appendix C. Transformer-XL (Dai et al., 2019): a specificdesigned architecture for long-range language modeling. It includes relative position embedding and recurrent memories per layer. Memory slots are filled with hidden states from previous time steps."
        },
        {
            "heading": "3.2 Experimental Settings",
            "text": "We compare our methods with the TransformerXL (Dai et al., 2019) under the same size of memory (m = 200) for attention calculation. For the input token length n for both models, we keep the same as in (Dai et al., 2019) (n = 64). Additionally,\nModel #Total Mem #Selected Mem PPL (\u2193)\nWikiText-103\nthe memory selection process is performed on a memory pool with the size of M . Our model and the Transformer-XL share the model parameters but have different inference strategies."
        },
        {
            "heading": "3.3 Main Results",
            "text": "The main results of WikiText-103 and enwik8 datasets are shown in Table 1. Without additional training or additional parameters, we gain 0.19 improvement in perplexity and 0.017 improvement for bit-per-character with our TRAMS mechanism. We implement p-test by inferencing on multiple model checkpoints and prove that our results are significant (p < 0.05)."
        },
        {
            "heading": "4 Discussions",
            "text": "Is TRAMS vulnerable to the selection of hyperparameters? There are three hyper-parameters in TRAMS: the memory pool size M that TRAMS is able to select from; the selected memory size m that is used in the forward process; and the input token size n that is involved in both backward and forward process.\nFrom the ablation study on M , Figure 4 suggests an optimal range between 300 to 400 for the memory pool size. Beyond this range, enlarging the memory pool often leads to the selection of irrelevant tokens, deteriorating our performance. Regarding m, Figure 5 indicates that TRAMS witnesses a substantial drop in perplexity when the memory size selected is about 25%. Selecting a larger portion does not yield further improvement. This is consistent with Figure 3, where TRAMS excels by concentrating on the top 10% of results. Lastly, in the study on n, Figure 6 shows that as the target token length decreases, the efficacy of memory selection improves.\nWhat is the inference cost compared to Transformer-XL? Since there is no training part in our model, we focus on discussing the inference cost. Compared with Transformer-XL, our model requires storing a larger memory pool to do memory selection. Therefore, the memory cost of our method would be larger. When it comes to timing cost, our model has an additional memory token norm computation memory sorting operations, and memory selection operations for each layer. These extra operations require extra inference time. Table 2 shows the GPU memory cost and wall-clock time for the Transformer-XL baseline and our model. Our model requires slightly more GPU memory usage and around 50% additional inference time for memory selection.\nHow does TRAMS benefit from memory selection? Memory selection helps the model pick tokens with higher attention scores with the queries, thus increasing the average memory utilization. Quantitatively, our method improves the average attention probability by 24.25% for the same size of memory compared with Transformer-XL.\nDoes each layer hold the same importance? Based on Figure 7, we show the ablation study when applying memory selection on each layer while remaining other layers the same. There is an observable drop when we apply the memory selection on the deeper layers starting from Layer 13 while we do not observe a clear influence when applying memory selection on shallow layers."
        },
        {
            "heading": "5 Case Study",
            "text": "To have an understanding of what kind of context should be selected, we provide one example case to understand specifically what kind of tokens in the memory would be selected. Based on Table 3, we can see that most of the selected memory tokens are low-frequency words. Those low-frequency words like \u201cJohn\" in the memory would be beneficial for the prediction of \u201cJohn\" in the target sequence."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we formulate the problem of memory selection in transformer architecture and reformulate the attention calculation process to obtain our self-defined queries and keys. After that, we propose a query-independent metric that utilizes memory hidden states to implement a training-free"
        },
        {
            "heading": "He appeared in the television series Judge",
            "text": "WikiText-103. text indicates that this word in memory sequence is selected and used in the forward pass. text indicates that this word in the target sequence benefits from the memory.\nmemory selector. Our experiments indicate that this method offers a simple yet effective means of identifying valuable memory tokens. Exploring optimal memory selection strategies for large language models is a promising avenue for future research. Additionally, integrating trainable parameters into these models as memory selectors presents another exciting direction for future work.\nLimitations\nOur study has a couple of main limitations. First, we are currently focusing on the Transformer-XL architecture, but there are many other models with different sizes we haven\u2019t tried. It indicates that our findings could be limited to typical transformer architecture. Second, our method has many hyperparameters including M , m, and n. Adjusting them can greatly change how well our model works. A careful calibration is thus necessary, and one must tread cautiously to strike a balance and achieve the desired performance, which could be time-consuming and computationally expensive.\nEthics Statement\nThere are no recognized potential risks."
        },
        {
            "heading": "A Dataset and Evaluation Metrics",
            "text": "WikiText-103 (Merity et al., 2016) is a commonly used word-level language modeling benchmark. It has an average length of 3.6 thousand tokens per article and includes 28 thousand Wikipedia articles. This word-level dataset has a vocabulary size of around 260K. We use the same data pre-processing setting in Dai et al. (2019) for this dataset. We use perplexity as our metric. Enwik8 (Mahoney, 2011) is a character-level language modeling benchmark. This dataset contains 100M unprocessed Wikipedia characters. The train set, dev set, and test set include 80M, 10M, and 10M characters separately. enwik8 has no preprocessing stage and is directly used. bpc (bit per character) is defined as an evaluation metric and we report results on both the dev set and test set."
        },
        {
            "heading": "B Training Configurations",
            "text": "Since we do inference experiments based on a trained model, we separately train two TransformerXL models for WikiText-103 and enwik8. For the training stage, we use Adam (Kingma and\nBa, 2014) to optimize with a batch size=60, learning rate=2.5e-4, target length=150, memory length=150, and a cosine scheduler without warmup steps.\nWhen it comes to a different dataset, we use different Transformer-XL architecture. For WikiText-103, we use a 16-layer transformer architecture with 10 heads, 410 hid dim, 0.1 dropout ratio, 0.0 attention dropout ratio, 2100 inner dim, and adaptive softmax mechanism. For enwik8, we propose a 12-layer transformer architecture with 8 heads, 512 hid dim, 0.1 dropout ratio, 0.0 attention dropout ratio, and 2048 inner dim. Both models are trained for 350K steps.\nA batch size=10 and target length=150 are fixed for all inference experiments to avoid unfair comparison. All experiments including training and inference are conducted using 4 2080Ti GPUs. It takes 280 GPU hours to train the enwik8 model checkpoint. It takes 61 GPU hours to train the WikiText-103 model checkpoint."
        },
        {
            "heading": "C Relative Position Embedding",
            "text": "Concerning positional encodings, we maintain the same results with Transformer-XL. The positional encodings include learnable parameters of Ri\u2212j , u, and v. Typically, Ri\u2212j is derived from a learnable r network included in the model. The advantage of using this design when computing the attention score is that it avoids temporal confusion caused by indexing the same position and considers the relative distance between two tokens. The formula for attention score calculation with relative position embedding can be written as:\nAxli,j =X \u22ba i W \u22ba q W E k Xj +X \u22ba i W \u22ba q W R k Ri\u2212j\n+u\u22baWEk Xj + v \u22baWRk Ri\u2212j (6)\nMoreover, after doing ablation studies on relative position embedding, we found that Ri\u2212j contributes the most to the result and u, v only has a small influence on the final performance. The existence of Ri\u2212j leads to the exponentially decayed attention probability distribution related to a memory position. As a result, we base our memory selection on the Axli,j which includes positional information instead of the pure X\u22bai W \u22ba q W E k Xj . To be noticed, all concepts related to qK are all equipped with position embedding instead of a simple dot product."
        }
    ],
    "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling",
    "year": 2023
}