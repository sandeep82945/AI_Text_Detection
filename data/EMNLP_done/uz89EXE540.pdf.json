{
    "abstractText": "We present a neuro-symbolic approach to selflearn rules that serve as interpretable knowledge to perform relation linking in knowledge base question answering systems. These rules define natural language text predicates as a weighted mixture of knowledge base paths. The weights learned during training effectively serve the mapping needed to perform relation linking. We use popular masked training strategy to self-learn the rules. A key distinguishing aspect of our work is that the masked training operate over logical forms of the sentence instead of their natural language text form. This offers opportunity to extract extended context information from the structured knowledge source and use that to build robust and human readable rules. We evaluate accuracy and usefulness of such learned rules by utilizing them for prediction of missing kinship relation in CLUTRR dataset and relation linking in a KBQA system using SWQ-WD dataset. Results demonstrate the effectiveness of our approach its generalizability, interpretability and ability to achieve an average performance gain of 17% on CLUTRR dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shajith Ikbal"
        },
        {
            "affiliations": [],
            "name": "Udit Sharma"
        },
        {
            "affiliations": [],
            "name": "Hima Karanam"
        },
        {
            "affiliations": [],
            "name": "Sumit Neelam"
        },
        {
            "affiliations": [],
            "name": "Ronny Luss"
        },
        {
            "affiliations": [],
            "name": "Dheeraj Sreedhar"
        },
        {
            "affiliations": [],
            "name": "Pavan Kapanipathi"
        },
        {
            "affiliations": [],
            "name": "Naweed Khan"
        },
        {
            "affiliations": [],
            "name": "Kyle Erwin"
        },
        {
            "affiliations": [],
            "name": "Ndivhuwo Makondo"
        },
        {
            "affiliations": [],
            "name": "Ibrahim Abdelaziz"
        },
        {
            "affiliations": [],
            "name": "Achille Fokoue"
        },
        {
            "affiliations": [],
            "name": "Alexander Gray"
        },
        {
            "affiliations": [],
            "name": "Maxwell Crouse"
        },
        {
            "affiliations": [],
            "name": "Subhajit Chaudhury"
        },
        {
            "affiliations": [],
            "name": "Chitra K Subramanian"
        }
    ],
    "id": "SP:0a04290096fb7716b8e695d180d6dde19fdfee96",
    "references": [
        {
            "authors": [
                "Laura Banarescu",
                "Claire Bonial",
                "Shu Cai",
                "Madalina Georgescu",
                "Kira Griffitt",
                "Ulf Hermjakob",
                "Kevin Knight",
                "Philipp Koehn",
                "Martha Palmer",
                "Nathan Schneider."
            ],
            "title": "Abstract meaning representation for sembanking",
            "venue": "LAW@ACL.",
            "year": 2013
        },
        {
            "authors": [
                "Chandra Bhagavatula",
                "Jena D. Hwang",
                "Doug Downey",
                "Ronan Le Bras",
                "Ximing Lu",
                "Lianhui Qin",
                "Keisuke Sakaguchi",
                "Swabha Swayamdipta",
                "Peter West",
                "Yejin Choi"
            ],
            "title": "I2d2: Inductive knowledge distillation with neurologic and self-imitation",
            "year": 2023
        },
        {
            "authors": [
                "Mihaela A. Bornea",
                "Ram\u00f3n Fern\u00e1ndez Astudillo",
                "Tahira Naseem",
                "Nandana Mihindukulasooriya",
                "I. Abdelaziz",
                "Pavan Kapanipathi",
                "Radu Florian",
                "Salim Roukos."
            ],
            "title": "Learning to transpile amr into sparql",
            "venue": "ArXiv, abs/2112.07877.",
            "year": 2021
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "et.al"
            ],
            "title": "Language models are few-shot learners. ArXiv, abs/2005.14165",
            "year": 2020
        },
        {
            "authors": [
                "Yubo Chen",
                "Yunqi Zhang",
                "Changran Hu",
                "Yongfeng Huang."
            ],
            "title": "Jointly extracting explicit and implicit relational triples with reasoning pattern enhanced binary pointer network",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "ArXiv, abs/2003.10555.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805.",
            "year": 2019
        },
        {
            "authors": [
                "Dennis Diefenbach",
                "Thomas Pellissier Tanon",
                "Kamal Deep Singh",
                "Pierre Maret."
            ],
            "title": "Question answering benchmarks for wikidata",
            "venue": "International Workshop on the Semantic Web.",
            "year": 2017
        },
        {
            "authors": [
                "Mohnish Dubey",
                "Debayan Banerjee",
                "Debanjan Chaudhuri",
                "Jens Lehmann."
            ],
            "title": "Earl: Joint entity and relation linking for question answering over knowledge graphs",
            "venue": "ArXiv, abs/1801.03825.",
            "year": 2018
        },
        {
            "authors": [
                "Claire Gardent",
                "Anastasia Shimorina",
                "Shashi Narayan",
                "Laura Perez-Beltrachini."
            ],
            "title": "Creating training corpora for nlg micro-planners",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "vatsa Bhargav",
                "Mo Yu"
            ],
            "title": "Leveraging abstract meaning representation for knowledge base question answering",
            "venue": "In Findings",
            "year": 2020
        },
        {
            "authors": [
                "Hunter Lang",
                "Hoifung Poon."
            ],
            "title": "Self-supervised self-supervision by combining deep learning and probabilistic logic",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(6):4978\u20134986.",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdel rahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2019
        },
        {
            "authors": [
                "Xueling Lin",
                "Haoyang Li",
                "Hao Xin",
                "Zijian Li",
                "Lei Chen."
            ],
            "title": "Kbpearl: A knowledge base population system supported by joint entity and relation linking",
            "venue": "Proc. VLDB Endow., 13:1035\u20131049.",
            "year": 2020
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yang Liu",
                "Xuan Zhu."
            ],
            "title": "Learning entity and relation embeddings for knowledge graph completion",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2015
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Gray."
            ],
            "title": "Leveraging semantic parsing for relation linking over knowledge bases",
            "venue": "International Workshop on the Semantic Web.",
            "year": 2020
        },
        {
            "authors": [
                "Khandelwal",
                "Srinivas Ravishankar",
                "Sairam Gurajada",
                "Maria Chang",
                "Rosario A. Uceda-Sosa",
                "Salim Roukos",
                "Alexander G. Gray",
                "Guilherme Lima",
                "Ryan Riegel",
                "Francois P.S. Luus",
                "L.V. Subramaniam"
            ],
            "title": "Sygma: A system for generalizable and modular",
            "year": 2022
        },
        {
            "authors": [
                "Maximilian Nickel",
                "Lorenzo Rosasco",
                "Tomaso A. Poggio."
            ],
            "title": "Holographic embeddings of knowledge graphs",
            "venue": "ArXiv, abs/1510.04935.",
            "year": 2015
        },
        {
            "authors": [
                "Jeff Z. Pan",
                "Mei Zhang",
                "Kuldeep Singh",
                "F.V. Harmelen",
                "Jinguang Gu",
                "Zhi Zhang."
            ],
            "title": "Entity enabled relation linking",
            "venue": "International Workshop on the Semantic Web.",
            "year": 2019
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "North American Chapter of the Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Reid Pryzant",
                "Ziyi Yang",
                "Yichong Xu",
                "Chenguang Zhu",
                "Michael Zeng"
            ],
            "title": "Automatic rule induction for interpretable semi-supervised learning",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw J. Purgal",
                "David M. Cerna",
                "C. Kaliszyk."
            ],
            "title": "Differentiable inductive logic programming in high-dimensional space",
            "venue": "ArXiv, abs/2208.06652.",
            "year": 2022
        },
        {
            "authors": [
                "Meng Qu",
                "Junkun Chen",
                "Louis-Pascal Xhonneux",
                "Yoshua Bengio",
                "Jian Tang."
            ],
            "title": "Rnnlogic: Learning logic rules for reasoning on knowledge graphs",
            "venue": "ArXiv, abs/2010.04029.",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan"
            ],
            "title": "Improving language understanding by generative pretraining",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam M. Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "ArXiv, abs/1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel."
            ],
            "title": "End-toend differentiable proving",
            "venue": "ArXiv, abs/1705.11040.",
            "year": 2017
        },
        {
            "authors": [
                "Gaetano Rossiello",
                "Faisal Chowdhury",
                "Nandana Mihindukulasooriya",
                "Owen Cornec",
                "Alfio Gliozzo."
            ],
            "title": "Knowgl: Knowledge generation and linking from text",
            "venue": "arXiv preprint arXiv:2210.13952.",
            "year": 2022
        },
        {
            "authors": [
                "Gaetano Rossiello",
                "Nandana Mihindukulasooriya",
                "I. Abdelaziz",
                "Mihaela A. Bornea",
                "A. Gliozzo",
                "Tahira Naseem",
                "Pavan Kapanipathi."
            ],
            "title": "Generative relation linking for question answering over knowledge bases",
            "venue": "ArXiv, abs/2108.07337.",
            "year": 2021
        },
        {
            "authors": [
                "Ali Reza Sadeghian",
                "Mohammadreza Armandpour",
                "Patrick Ding",
                "Daisy Zhe Wang."
            ],
            "title": "Drum: End-to-end differentiable rule mining on knowledge graphs",
            "venue": "ArXiv, abs/1911.00055.",
            "year": 2019
        },
        {
            "authors": [
                "Ahmad Sakor",
                "Isaiah Onando Mulang",
                "Kuldeep Singh",
                "Saeedeh Shekarpour",
                "Maria-Esther Vidal",
                "Jens Lehmann",
                "S. Auer."
            ],
            "title": "Old is gold: Linguistic driven approach for entity and relation linking of short text",
            "venue": "North American Chapter of the As-",
            "year": 2019
        },
        {
            "authors": [
                "Ahmad Sakor",
                "Kuldeep Singh",
                "Anery Patel",
                "MariaEsther Vidal"
            ],
            "title": "2019b. Falcon 2.0: An entity and relation linking tool over wikidata",
            "venue": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management",
            "year": 2019
        },
        {
            "authors": [
                "P. Sen",
                "B.W.S.R. de Carvalho",
                "I. Abdelaziz",
                "P. Kapanipathi",
                "S. Roukos",
                "A. Gray."
            ],
            "title": "Logical neural networks for knowledge base completion with embeddings & rules",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Prithviraj Sen",
                "Breno W. Carvalho",
                "Ryan Riegel",
                "Alexander G. Gray."
            ],
            "title": "Neuro-symbolic inductive logic programming with logical neural networks",
            "venue": "ArXiv, abs/2112.03324.",
            "year": 2021
        },
        {
            "authors": [
                "Koustuv Sinha",
                "Shagun Sodhani",
                "Jin Dong",
                "Joelle Pineau",
                "William L Hamilton."
            ],
            "title": "Clutrr: A diagnostic benchmark for inductive reasoning from text",
            "venue": "arXiv preprint arXiv:1908.06177.",
            "year": 2019
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "Tie-Yan Liu."
            ],
            "title": "Mass: Masked sequence to sequence pretraining for language generation",
            "venue": "International Conference on Machine Learning.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhihong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "Rotate: Knowledge graph embedding by relational rotation in complex space",
            "venue": "ArXiv, abs/1902.10197.",
            "year": 2018
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "\u00c9ric Gaussier",
                "Guillaume Bouchard."
            ],
            "title": "Complex embeddings for simple link prediction",
            "venue": "International Conference on Machine Learning.",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam M. Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "arXiv preprint arXiv:1710.10903.",
            "year": 2017
        },
        {
            "authors": [
                "Zhen Wang",
                "Jianwen Zhang",
                "Jianlin Feng",
                "Zheng Chen."
            ],
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2014
        },
        {
            "authors": [
                "Ledell Yu Wu",
                "Fabio Petroni",
                "Martin Josifoski",
                "Sebastian Riedel",
                "Luke Zettlemoyer."
            ],
            "title": "Scalable zero-shot entity linking with dense entity retrieval",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2019
        },
        {
            "authors": [
                "Bishan Yang",
                "Wen tau Yih",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng."
            ],
            "title": "Embedding entities and relations for learning and inference in knowledge bases",
            "venue": "CoRR, abs/1412.6575.",
            "year": 2014
        },
        {
            "authors": [
                "Fan Yang",
                "Zhilin Yang",
                "William W. Cohen."
            ],
            "title": "Differentiable learning of logical rules for knowledge base reasoning",
            "venue": "NIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Fan Yang",
                "Zhilin Yang",
                "William W. Cohen."
            ],
            "title": "Differentiable learning of logical rules for knowledge base reasoning",
            "venue": "NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Mo Yu",
                "Wenpeng Yin",
                "Kazi Saidul Hasan",
                "C\u00edcero Nogueira dos Santos",
                "Bing Xiang",
                "Bowen Zhou."
            ],
            "title": "Improved neural relation detection for knowledge base question answering",
            "venue": "Annual Meeting of the Association for Computational",
            "year": 2017
        },
        {
            "authors": [
                "Shuai Zhang",
                "Yi Tay",
                "Lina Yao",
                "Qi Liu."
            ],
            "title": "Quaternion knowledge graph embeddings",
            "venue": "Neural Information Processing Systems.",
            "year": 2019
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "et.al"
            ],
            "title": "Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "We present a neuro-symbolic approach to selflearn rules that serve as interpretable knowledge to perform relation linking in knowledge base question answering systems. These rules define natural language text predicates as a weighted mixture of knowledge base paths. The weights learned during training effectively serve the mapping needed to perform relation linking. We use popular masked training strategy to self-learn the rules. A key distinguishing aspect of our work is that the masked training operate over logical forms of the sentence instead of their natural language text form. This offers opportunity to extract extended context information from the structured knowledge source and use that to build robust and human readable rules. We evaluate accuracy and usefulness of such learned rules by utilizing them for prediction of missing kinship relation in CLUTRR dataset and relation linking in a KBQA system using SWQ-WD dataset. Results demonstrate the effectiveness of our approach - its generalizability, interpretability and ability to achieve an average performance gain of 17% on CLUTRR dataset."
        },
        {
            "heading": "1 Introduction",
            "text": "Relation linking is a key step in many Natural Language Processing (NLP) tasks including Semantic Parsing, Triple Extraction, and Knowledge Base Question Answering (KBQA). Its goal is to accurately map predicate components of a natural language (NL) text segment onto their corresponding predicate (i.e., relational) elements within a knowledge base (KB). Such a mapping would enable question answering systems to exploit the deep\n\u2020IBM Research, Bangalore/Gurugram, India. \u2021IBM Research, T. J. Watson Research Center, Yorktown Heights, New York, United States. \u00a7IBM Research, Johannesburg, South Africa. \u2217Correspondence e-mails: shajmoha@in.ibm.com, udit.sharma@in.ibm.com, hkaranam@in.ibm.com\nreasoning capabilities of symbolic systems. For example, by accurately mapping predicate component of the NL text shown in Figure 1, i.e., share-screen, to its corresponding relational elements within KB, we should be able to successfully utilize KBQA systems to answer questions like Whether Kate Winslet shared screen with Leonardo DiCaprio? and Who all have shared screen with Kate Winslet?1. However, learning these mappings is challenging for various reasons: lexical variations, modeling deficiencies to handle complex linking involving multiple KB predicates2, and limited training data to build models in a supervised fashion.\nVarious approaches have been tried in the past work, including graph-based approaches (Pan et al.,\n1Provided entity mentions in the NL text are also appropriately mapped to their corresponding entity elements in the KB, through entity linking (Wu et al., 2019). For example, in Figure 1, entity mention Kate Winslet in NL text is mapped to corresponding KB (Wikidata) entity wd:Q202765.\n2On several occasions, there is no single relational element but multiple connected relational elements within the KB that actually correspond to a NL text predicate. These connected relational elements constitute a path within the KB. For example, two connected relational elements as in Figure 3 (i.e., one-hop path involving a reverse KB relation edge cast-member and a forward KB relation edge cast-member) correspond to NL text predicate share-screen. Note that a formal expression of this KB path is given in Figure 2, i.e., cast-member\u22121(x, z) & cast-member(z, y).\n2019; Dubey et al., 2018), linguistic features based approaches (Sakor et al., 2019a,b; Lin et al., 2020), and recently neural approaches that achieve stateof-the-art performance (Rossiello et al., 2021; Mihindukulasooriya et al., 2020; Yu et al., 2017). However, these approaches have the following drawbacks: they are (a) data hungry requiring huge amount of supervised data for training, (b) non-interpretable by design, and (c) non-trivial to adapt and generalize to new domains or knowledge graphs. In this paper, we propose a neuro-symbolic approach that formulates relation linking as a rule learning problem in the space of structured knowledge sources. Our approach tries to learn human readable rules of the form illustrated in Figure 2 and use them to perform relation linking, thus resulting in explainable relation linking model. In contrast to the existing approaches that build linking models in a supervised fashion, we adopt a self-supervised learning approach that offer benefits: does not require annotating large amounts of training data, and offers generalization across domains.\nFor self-supervision we use masked training, a popularly used self-supervised training strategy for large language models. However, in contrast to the use of NL text as in the large language models (LLMs), masked training in our approach operates on logical statements of the sentences. In this work, we use triples3 extracted from semantically parsed output of the sentences as the logical statements. An example sentence in its NL form and its corresponding triple form (logical statement) are shown in Figure 1. The masking procedure when applied on a triple may end up masking any of the three elements, i.e., subject, object, or predicate. Figure 1 shows an illustration of the predicate component\n3Triple format: predicate(source entity, destination entity), where source entity and destination entity are also alternatively referred to as subject and object, respectively.\ngetting masked. Given that the three elements of a triple play different semantic roles, the masked training need to be adapted to suit the role played by the element being masked. As will be seen later in the paper, such differences reflect mainly on the loss computation during training.\nThe main contributions of our work are:\n1. The first self-supervised rule learning approach that uses an adaptation of masked language objective for logical forms extracted from text. We have applied this to relation linking, a prominent task for understanding natural language.\n2. The output of our model is completely interpretable; the set of rules obtained to perform relation linking can explain the predictions of the model. An example of a learnt rule for the SWQ dataset is shown in Figure 2.\n3. We evaluate our approach on two datasets CLUTRR and SWQ demonstrating (a) Generalizability over two different knowledge bases as context on the datasets, (b) Effectiveness of relation linking where results show significant gains (17% gain in average performance) for CLUTRR across different test sets along with superior systematic generalization and competitive results on SWQ, (c) Interpretability, by showing qualitative results of the learnt rules."
        },
        {
            "heading": "2 Related Work",
            "text": "Relation linking has been important for various NLP tasks such as semantic parsing, knowledge graph induction (Gardent et al., 2017; Chen et al., 2021; Rossiello et al., 2022; Lin et al., 2020) and knowledge base question answering (Rossiello et al., 2021; Kapanipathi et al., 2020; Neelam et al., 2022). Prior to the surge of generative models, relation linking was addressed either by graph traversal based (Pan et al., 2019; Dubey et al., 2018) or by linguistic-features based methodologies (Sakor et al., 2019a,b; Lin et al., 2020). Several learning based approaches to relation linking have been proposed (Mihindukulasooriya et al., 2020; Yu et al., 2017; Bornea et al., 2021). Most recent approaches to relation linking have focused on generative models (Rossiello et al., 2021). These approaches are data hungry and non-interpretable. In contrast, our work is a self-supervised rule learning based approach for relation linking. The learnt rules are\nhuman readable and they can be learnt for different knowledge bases as context for different domains.\nRule learning, specifically in terms of detecting relations has also been popular for knowledge base completion (KBC) task. KBC problems are addressed in two different ways in the literature. Rule based KBC learns explainable rules that can be used to predict the missing links in the knowledge base (Sen et al., 2021; Qu et al., 2020; Yang et al., 2017a; Sadeghian et al., 2019; Purgal et al., 2022; Rockt\u00e4schel and Riedel, 2017). These rules are learned for predicates in the knowledge base in terms of other predicates and paths in the knowledge base. Our work uses the rule model and some of the ideas from the rule based KBC to build self supervised explainable rules for linking text elements to knowledge base relations. There have also been several learning based methods that use vector embeddings of the entities and relationships and then use them to predict any missing relationships (Nickel et al., 2015; Yang et al., 2014; Wang et al., 2014; Lin et al., 2015; Trouillon et al., 2016; Sun et al., 2018; Zhang et al., 2019); they have the same drawbacks as neural relation linking approaches. There are many recent works that use semi-supervised/unsupervised learning for rules and knowledge induction (Pryzant et al., 2022; Bhagavatula et al., 2023; Lang and Poon, 2021; Zhu and Li).\nTransformers (Vaswani et al., 2017) have given rise to a wide range of masked pre-training models that try to learn latent representation of words. Masked language modeling has gained popularity in building large pre-trained models that learn word representations that can easily be adapted to a variety of tasks. Masked language training has resulted in building various model architectures like encoder only models which learn a vector representation of tokens using masked training (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020). Several Encode-decoder LLMs are built using similar training strategy (Song et al., 2019; Lewis et al., 2019; Soltan et al., 2022; Raffel et al., 2019). Recently decoder only models built using similar masked language training are gaining popularity (Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022; Scao et al., 2022). In this paper we take inspiration from masked language training on text and use the ideas to perform similar masking on logic statements to learn rules."
        },
        {
            "heading": "3 Our Approach",
            "text": "Our approach to relation linking is a rule-learning framework where rules learned in a self-supervised fashion are used to map from \u2018NL text segments\u2019 to the \u2018elements of structured knowledge\u2019. For self-supervised rule learning, we use masked training. A distinctive aspect of the masked training in our approach is that it is applied over the logical form of the input text. When logical form is not readily available, we first convert NL text into its logical form using a semantic parser, before applying masked training. In this section, first we briefly describe the conversion of input text to logical statements, and then describe the proposed rule model and its self-supervised training."
        },
        {
            "heading": "3.1 Logical Statements",
            "text": "In this work, we use triples extracted from sentences as their logical form4. Note that logical forms are expressed as predicate(source entity, destination entity), as illustrated in Figure 1. To extract such logical forms from NL text, we use the semantic parsing approach described in (Neelam et al., 2022), which performs an Abstract Meaning Representation (AMR) (Banarescu et al., 2013) based \u03bb-expression extraction. For simple sentences, \u03bbexpressions can be converted directly into triples5. Figure 1 shows an illustration of a NL text and its corresponding logical form. Different elements of the triple represent distinct segments of the NL text. Since we use an AMR based approach to derive \u03bbexpressions, predicate components of the triple are typically a propbank predicate. Relation linking is expected to map this predicate component onto the corresponding knowledge-base elements."
        },
        {
            "heading": "3.2 Rule Model",
            "text": "Towards the goal of relation linking, our rule model is formulated as a function that maps a weighted mixture of \u2018knowledge base predicates\u2019 onto a \u2018predicate component of the NL text\u2019. This enables interpretable prediction of the set of knowledge base predicates that the text predicate should get linked to. Figure 2 gives an illustration of a simple rule.\nLet us assume a triple predi(s, d) extracted from NL text, where s is the source entity, d is the destination entity and predi is the text predicate. The\n4We use triples and logical form interchangeably. 5We also employ heuristics to handle the AMR and \u03bb-\nexpression computation failures and errors.\ngoal is to ground predi to its corresponding knowledge base predicates using relation linking model. Our proposed rule model to achieve this is defined as:\npredi(s, d) = f  N\u2211 j=1 wij pathj(s, d)  (1) for i \u2208 {1, . . . ,M} where f(\u00b7) denotes a function such as Sigmoid and pathj denotes a path in the knowledge base that passes through one or more knowledge base predicate edges. pathj(s, d) can take values 0 or 1 depending upon presence or absence of a path pathj connecting source entity s and destination entity d. Depending upon the length of the path, i.e., number of predicate edges and the intermediate entities, paths could be categorized as k-hop, where k denotes the count of intermediate entities in the path. An example of a 1-hop path between entities in Wikidata is shown in Figure 3. If there are L unique knowledge base predicates, i.e., {pl}, 1 \u2264 l \u2264 L, the sets of 0, 1 and 2-hop paths are defined as follows:\n0-hop paths = {pl(s, d)}, 1 \u2264 l \u2264 L\n1-hop paths = {pl(s, \u2217) : pm(\u2217, d), 1 \u2264 l \u2264 L, 1 \u2264 m \u2264 L\n2-hop paths = {pl(s, \u2217) : pm(\u2217, \u2217) : pn(\u2217, d)}, 1 \u2264 l \u2264 L, 1 \u2264 m \u2264 L, 1 \u2264 n \u2264 L\nwhere \u2217 denote any possible entity. Note that paths always originate from a source entity and end at a destination entity. Also note that, the constituent knowledge base predicates in a path could also be inverse (reverse), denoted by superscript \u22121. For example, path in Figure 3 contains an inverse relation,\nwhich is denoted as cast member\u22121 in the corresponding path expression given in Figure 2. All the knowledge base paths of various lengths, i.e., from 0-hop paths until k-hop paths put together, constitute the overall set of paths considered in our rule learning framework. Let us assume there are N such paths in total. Given that the knowledge base paths pathj are defined in terms of the knowledge base predicates, our rule model in (1) effectively defines a mapping between the \u2019NL text predicates\u2019 and the \u2019knowledge base predicates\u2019. During learning, we want to estimate weights wij so that rule model in (1) can be used for relation linking.\nA neural implementation of each rule in (1) is given in Figure 4. Note that there is one such network each for every rule in (1), and all these networks are trained together during learning. Note that the parameters of our rule model, denoted by W , is a matrix of learnable weights:\nW = {wij}, 1 \u2264 i \u2264 M, 1 \u2264 j \u2264 N\nwhere each row corresponds to the set of path weights of a specific rule. In the next section, we describe masked training used to estimate these weights in a self-supervised manner."
        },
        {
            "heading": "3.3 Self-Supervised Rule Learning",
            "text": "Masked training in large language model training involves masking certain parts of the input text and having the model predict those masked parts by treating the rest of the (unmasked) text as its context. The model parameters are adjusted during training to effectively model the context so it can act as a proxy for the masked part. In our approach masked training is performed on the logical form of the input text as detailed below."
        },
        {
            "heading": "3.3.1 Masked Training on Logical Forms",
            "text": "Logical statements are composed of distinct elements playing distinct semantic roles. For example, triples are composed of a predicate, a source entity (also called subject) and a destination entity (also called object). The masking procedure could mask any of these elements:\n1. Predicate masking - predicate component of the triple is masked\n2. Source entity masking - source entity component of the triple is masked\n3. Destination entity masking - destination entity element of the triple is masked\nFigure 1 gives an illustration of predicate masking. Similar to the large language model training, masked training for rule learning also aims to adjust the parameters of the rule model so that it can accurately predict the masked elements from the unmasked context.\nHowever, the distinctive aspect of our approach is in the context computation. For the context, in addition to the unmasked elements of the logical statement, we also use paths within the knowledge base that are associated to those unmasked elements. A set of such associated paths are obtained by grounding the unmasked elements of the logical statement to the knowledge base. For example, given Wikidata as the knowledge base, we first use entity linking (Wu et al., 2019) to link the unmasked source and destination entities of the triple to corresponding Wikidata entities, as illustrated in Figure 1. Then the Wikidata paths originating from unmasked source entity and ending at unmasked destination entity are computed and used as the context. Figure 3 gives an illustration of a Wikidata path, through entity node corresponding to Titanic movie, for the unmasked entities of Figure 1. Note that such use of knowledge base paths as the context help establish the mapping between the NL text predicates and the knowledge base predicates, which indeed is the goal of our relation linking model. Next we describe rule model parameter estimation procedure for different masking scenarios."
        },
        {
            "heading": "3.3.2 Rule Model Estimation",
            "text": "Learning procedures for the 3 masking scenarios differ mainly in the way training loss is computed. In all 3 masking scenarios we use rule models to predict the masked element. As discussed\nabove, knowledge base paths associated with the unmasked elements of the triple are used as the context for the prediction. However, the set of such paths differ based on which element is masked and which elements are unmasked and could be linked to the knowledge base. Training loss functions are defined to improve prediction accuracy over the training iterations, as described below:\na) Predicate Masking: As illustrated in Figure 5, when the predicate is masked, unmasked source and destination entities are linked to the corresponding elements in the knowledge base through entity linking. Then the set of all paths that connect these linked entities are computed and used as context. Let j \u2208 E denote the set of indices of knowledge base paths that connect the linked source and destination entities. These paths when applied to all the rules in (1), corresponding to all the text predicates predi(.), 1 \u2264 i \u2264 M , would result in scores for each text predicate as below:\nsi = f \u2211 j\u2208E wij pathj(s, d)  , 1 \u2264 i \u2264 M. For accurate prediction of the masked predicate by the model, the score should be highest (1.0) for the text predicate being masked and should be the lowest (0.0) for all others. Accordingly, let ti, 1 \u2264 i \u2264 M denote the target scores, where the score corresponding to the index of the masked predicate is 1.0 and the rest of the scores are 0.0. We compare the estimated scores against these target scores, to compute the training loss that could be used further to update the model weights through stochastic gradient descent (SGD). In actual, we use a crossentropy training loss computed as below:\nloss = N\u2211 j=1 ti log(si)\nb) Source Entity Masking: As illustrated in Figure 6 when the source entity is masked, unmasked destination entity is linked to the corresponding element in the knowledge base, through entity linking. Then the set of all paths that end at the linked destination entity are computed for use as the context. Note that all these paths together have a set of associated origination entities that could potentially be considered as the prediction for the masked source entity. Let S = {s1, s2, ..., sE} denote the set of all such candidate source entities. Let Ee correspond to the set of indices of all paths that originate from the candidate source entity se and end at the unmasked linked destination entity. These paths when applied to a specific rule in (1) that corresponds to the unmasked text predicate predi, would give a score for se as below:\nse = \u2211 j\u2208Ee wij pathj(se, d), 1 \u2264 e \u2264 E\nA list of such scores are computed for each candidate source entity in S. Note that among these scores the one corresponding to candidate entity that is same as the masked source entity should be highest (1.0) and the score for the rest should be the lowest (0.0). Accordingly, target scores are te, 1 \u2264 e \u2264 E, where the score for the index of the masked entity is set to 1.0 and the rest are set to 0.0. We then compute training loss by comparing the estimated scores against the target scores. A cross-entropy training loss computed as below is used to update the rule model weights through stochastic gradient descent (SGD):\nloss = \u2211 e\u2208S te log(se)\nc) Destination Entity Masking: Destination entity masking is similar to that of the source entity masking, except that when the destination entity\nis masked, unmasked source entity is linked to the knowledge base and the set of all paths originating from the linked source entity are computed as context, resulting in the associated candidate destination entities. Then scores for all candidate destination entities are computed in a similar fashion to further compute the training loss."
        },
        {
            "heading": "3.4 Inference",
            "text": "Note that, once trained, the right hand side of the rule model in Equation (1) directly gives human readable information of the relation linking, highlighting the interpretability/explainability aspect of our model. Each rule gives information of the knowledge base paths (that in turn are composed of the knowledge base predicates) that the text predicate should get linked to, with path weights denoting the linking confidence. During inference, for example while using our rule model to perform QA, the information being asked in the NL question would correspond to the missing element in the corresponding logical form. Hence the goal is to estimate candidates for that missing element and score them using our rule model. The scoring procedure is similar to that for the masked elements during masked training as described in Section 3.3.2. TopK scoring candidates are chosen as the answers estimates of the rule model."
        },
        {
            "heading": "3.5 Scaling",
            "text": "The number of paths N is exponential in the number of predicates L that exist in the knowledge base. As a result, solving problem (1) becomes prohibitive with large N and hence we explore methods for reducing the search space of paths. In other words, we learn a set of paths P with |P| << N that contains the optimal paths, allowing us to replace the summation in problem in Equation (1) to a summation over P , and thereby reducing the computation required to find the solution.\nWe make use of a Chain of Mixtures (CoM) model, also known as Neural Logic Programming (Yang et al., 2017b) or edge-based rules (Sen et al., 2022), to learn P . In CoM, a rule is represented by a conjunction of mixtures of predicates. Specifically, rather than define a weight per path, for each of M predicates we define a weight per predicate per hop to get rules of the form:\npredCoMi (s, d) = g ( f (\u2211M j=1w i j,0pj(s, r1) ) ,\n(2) f (\u2211M\nj=1w i j,1pj(r1, r2)\n) ,\nf (\u2211M\nj=1w i j,2pj(r2, d) )) for i \u2208 {1, . . . ,M} where now wij,k is a weight for rule i (i.e., for the ith predicate) for predicate j on the hop k of the rule being learned, and g(. . .) is a function approximating a conjunction such as a product. Equation (2) is an example 2-hop rule where r1 and r2 can take the values of any entities in the knowledge base. The number of weights learned in a CoM rule is M(K+1) where K is the number of hops in the rule, which greatly reduces the number of weights learned in problem (1).\nWhile CoM requires learning much fewer parameters than our rule model (1), it is less interpretable because it is not known which paths are actually relevant to the rule. CoM rules suggest that any path that can be obtained from predicates with positive weights in the hops is possible. For example, suppose there are 8 predicates in the knowledge base and the set of strictly positive weights in a rule of the form (2) is {wi0,1, wi0,3, wi0,4, wi1,1, wi1,2, wi2,2, wi2,4, wi2,8}. Then such a chain rule only allows rules of the form pj(s, r1) \u2227 pk(r1, r2) \u2227 pl(r2, e) where j \u2208 {1, 3, 4}, k \u2208 {1, 2}, l \u2208 {2, 4, 8}. with 8 predicates, there are a total of 8 \u00d7 8 \u00d7 8 = 512 possible paths, but the CoM reduces this to 3 \u00d7 2 \u00d7 3 = 18 possible paths. Given these possible paths, a more interpretable rule of the form Equation (1) can be learned more efficiently since the summation can be taken over 18 paths instead of N = 512 paths."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate our approach on two datasets as described below that involve utilization of two distinct types of knowledge sources.\na) CLUTRR (Sinha et al., 2019) is a benchmark dataset for evaluating the reasoning capabilities of a Natural Language Understanding (NLU) system, where the goal is to infer kinship relations between source and target entities based on short stories. It contains both graph and textual representations of these stories. Each story consists of clauses (chains of relations) of length ranging from 2 to 10. The benchmark tests the model\u2019s ability for: i) Systematic Generalization which tests how well the model generalizes beyond the combination of rules seen during training. The test set consists of clauses\nof length up to 10, while the train set has clauses of length up to 4. ii) Model robustness which tests how well model performs in the presence of noisy facts. It consists of three kinds of noise: a) Supporting facts: It adds alternative equivalent paths between source and target entities. b) Irrelevant facts: It adds a path which originates on the path between the source and target entity but ends at an entity not in that path. c) Disconnected facts: It adds a path which is completely disconnected from the path between the source and target entity.\nWe considered graph stories with only clause length 2 for training rule models in a selfsupervised fashion. This results in multiple onehop rules along with their confidence scores. These rules can be used to handle stories of clause length 2 at the time of inference. For stories with clause length greater than 2, we combine relations within the path from source to target entity using Algorithm 1 to compute the path scores. A n-length path R1, ..., Rn from source entity to target entity can be reduced by invoking infer(1, n). It utilises two sub-modules predict(Ri, Rj) and score(Ri, Rj) which returns prediction and confidence scores for merging Ri and Rj using one-hop rule model.\nAlgorithm 1: Inference procedure using one-hop rule model\nOutput :Rij : Prediction for path Ri ...Rj , Sij : Prediction score for path Ri ...Rj Function infer(i, j): if i == j then\nreturn Ri, 1 else\nRij \u2190 null, Sij \u2190 0 for k \u2190 i+ 1 to j by 1 do\nRik\u22121, Sik\u22121\u2190 infer(i, k \u2212 1) Rkj , Skj \u2190 infer(k, j) S\u2032ij \u2190 Sik\u22121* Skj*score(Rik\u22121, Rkj) R\u2032ij \u2190 predict(Rik\u22121, Rkj) if S\u2032ij > Sij then\nRij \u2190 R\u2032ij , Sij \u2190 S\u2032ij end return Rij , Sij\nend\nb) SWQ-WD (Diefenbach et al., 2017) is a Question Answering (QA) dataset with simple questions built to evaluate the QA accuracy of KBQA systems that use Wikidata as the knowledge source. This dataset fits our relation linking evaluation goal because it consists of simple questions with single text predicates, hence it is possible to perform a focused evaluation of the relation linking task, reducing the impact of uncertainties of other components of KBQA systems such as semantic parsing\nand entity linking. SWQ-WD has 13894 train set questions and 5622 test set questions. We use train set to self-learn rules and report relation linking accuracy on the test set. Answer prediction using our rule model is described in Section 3.4."
        },
        {
            "heading": "4.2 Baselines and Metrics",
            "text": "For both the datasets, we compared our method against the best available baseline as per our knowledge. For CLUTRR, we compared against GAT (Velic\u030ckovic\u0301 et al., 2017), a neural graph representation framework that learns representations through self-attention over the neighbourhood. For SWQ, we compared against SYGMA (Neelam et al., 2022), a modular and generalizable pipeline based approach consisting of KB-specific and KBagnostic modules for KBQA.\nFor CLUTRR experiments, where relation linking performance is directly evaluated, we use a simple accuracy metric of accuracy = success_count/total_count. For SWQ experiments, where the relation linking performance is not directly evaluated but through its influence on the question answering (QA) accuracy, we use the metric of Recall which is a fraction of correct answers that the system is able to estimate."
        },
        {
            "heading": "4.3 Code Release",
            "text": "The code and the experimental setup used to train and evaluate our self-supervised rule learning based approach for relation linking is available at https://github.com/IBM/self-supervisedrule-learning."
        },
        {
            "heading": "5 Results",
            "text": "a) CLUTRR: Figure 7 shows (in upper part) sample 1-hop rules learned from CLUTRR dataset. Note that these rules are of form as in Equation (1). For CLUTRR, there were 22 predicates and so each 1-hop rule was learned over N = 22 \u00d7 22 = 484 possible paths. Figure 8 shows corresponding CoM rules for the CLUTRR dataset. For example, the CoM rule for son implies possible 1-hop rules that start with either son, husband, daughter, or wife and end with either son or brother, offering 4 \u00d7 2 = 8 possible rules for son. Rules of the form (1) could thus be rather learned over only these 8 possible rules and capture the same two interpretable rules for son seen in the CLUTRR part of Figure 7. Overall, rules for the 22 different relations search over 484 paths each (10648 total possible rules); CoM reduced that search to 118 to-\ntal possible rules with perfect coverage (i.e., CoM results include all optimal rules as a subset) for all but 2 relations.\nFigure 9 compares the Systematic generalization performance of our approach against GAT (Velic\u030ckovic\u0301 et al., 2017) on CLUTRR dataset with test set clause length ranging from 2 to 10. The results suggest that the proposed approach trained with stories of clause length k = 2 generalizes better in comparison to GAT trained with stories of clause length k = 2, 3 and stories of clause length k = 2, 3, 4. Table 1 compares the model robustness of our approach against GAT on CLUTRR dataset for differing noise scenarios during training and inference. The results suggest that the proposed approach outperforms GAT for most scenarios, achieving an average absolute accuracy improvement of 17%.\nb) SWQ-WD: Figure 7 shows (in lower part) sample 0/1-hop rules learned from SWQ dataset, of form as in Equation (1). Interestingly, for text predicate film-language it is able to learn rules that associate it with the language spoken/written by the director or screen writer or cast members of the film. Table 2 compares QA accuracy of our approach against SYGMA (Neelam et al., 2022). Our approach achieves comparable QA performance (as given in the first two lines), in spite of SYGMA using supervised training for its relation linking module - whereas our approach is a self-supervised approach (not using any labelled data for training). Note that rule learning is prone to inaccurate entity linking6. Thus, when ground truth entities are\n6Note that inaccuracies both in AMR parsing and entity linking could lead to erroneous logical statements. However, SWQ sentences are simple and hence their AMR parsing is straight forward, with SMATCH score of 83.0% (Neelam et al., 2022). Thus, incorrect logical statements for SWQ are largely due to entity linking errors. In case of CLUTRR, GAT baseline used the readily available ground truth logical forms of the sentences that we also used for fair comparison.\nused, our approach performs marginally better than SYGMA (as shown in the last two rows). Note that when ground truth entities are used, QA performance is more reflective of the relation linking performance, thus asserting the effectiveness of our relation linking model."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this paper, we presented a novel masked training based self rule learning approach for relation linking. The proposed approach takes inspiration from the masked training of large language models and rule learning in knowledge base completion to create a rule learning model that results in a relation linking model that is interpretable and generalizable to any knowledge base and domain. Our evaluation on two datasets CLUTRR (rule learning for kinship relations) and SWQ (relation linking for Wikidata) shows that our model is able to produce high quality rules for relation linking that generalize across domains. Our next steps are to scale to learn from large amount of data, extend the framework beyond triples. We see a lot of potential for our rule learning framework beyond relation linking, like general knowledge acquisition."
        },
        {
            "heading": "7 Limitations",
            "text": "Proposed approach is an initial attempt at taking masked training to logic statements and using knowledge graph as context. Current approach described is only taking single triple to learn relationship linking problem for KBQA, but overall approach has potential to extend to larger logic statements. Extending the work to handle logic statements with more than one triple is a future direction that we are looking into. In the current approach we assumed semantic parses to be perfect and available, but in reality they can be noisy resulting in noisy rule model. Handling noise coming from the parsing is another main extension we would like to look into as part of our future work."
        }
    ],
    "title": "Self-Supervised Rule Learning to Link Text Segments to Relational Elements of Structured Knowledge",
    "year": 2023
}