{
    "abstractText": "Visual Relation Extraction (VRE) is a powerful means of discovering relationships between entities within visually-rich documents. Existing methods often focus on manipulating entity features to find pairwise relations, yet neglect the more fundamental structural information that links disparate entity pairs together. The absence of global structure information may make the model struggle to learn longrange relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledge-guided relation Extraction (GOSE) framework. GOSE initiates by generating preliminary relation predictions on entity pairs extracted from a scanned image of the document. Subsequently, global structural knowledge is captured from the preceding iterative predictions, which are then incorporated into the representations of the entities. This \u201cgenerate-capture-incorporate\u201d cycle is repeated multiple times, allowing entity representations and global structure knowledge to be mutually reinforced. Extensive experiments validate that GOSE not only outperforms existing methods in the standard fine-tuning setting but also reveals superior cross-lingual learning capabilities; indeed, even yields stronger data-efficient performance in the low-resource setting. The code for GOSE will be available at https://github.com/chenxn2020/GOSE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiangnan Chen"
        },
        {
            "affiliations": [],
            "name": "Qian Xiao"
        },
        {
            "affiliations": [],
            "name": "Juncheng Li"
        },
        {
            "affiliations": [],
            "name": "Duo Dong"
        },
        {
            "affiliations": [],
            "name": "Jun Lin"
        },
        {
            "affiliations": [],
            "name": "Xiaozhong Liu"
        },
        {
            "affiliations": [],
            "name": "Siliang Tang"
        }
    ],
    "id": "SP:57710541e0b8ec463904f23f08540909a2a43df5",
    "references": [
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Nan Yang",
                "Saksham Singhal",
                "Wenhui Wang",
                "Xia Song",
                "Xian-Ling Mao",
                "He-Yan Huang",
                "Ming Zhou."
            ],
            "title": "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
            "venue": "NAACL-HLT, pages",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "\u00c9douard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "ACL,",
            "year": 2020
        },
        {
            "authors": [
                "Lei Cui",
                "Yiheng Xu",
                "Tengchao Lv",
                "Furu Wei."
            ],
            "title": "Document AI: benchmarks, models and applications",
            "venue": "arXiv preprint arXiv:2111.08609.",
            "year": 2021
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi"
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "Hsiao-Wuen Hon."
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "Proceedings of NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Zhangxuan Gu",
                "Changhua Meng",
                "Ke Wang",
                "Jun Lan",
                "Weiqiang Wang",
                "Ming Gu",
                "Liqing Zhang."
            ],
            "title": "Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "year": 2022
        },
        {
            "authors": [
                "Tao Huang",
                "Lang Huang",
                "Shan You",
                "Fei Wang",
                "Chen Qian",
                "Chang Xu."
            ],
            "title": "Lightvit: Towards lightweight convolution-free vision transformers",
            "venue": "arXiv preprint arXiv:2207.05557.",
            "year": 2022
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei."
            ],
            "title": "Layoutlmv3: Pre-training for document ai with unified text and image masking",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Wonseok Hwang",
                "Jinyeong Yim",
                "Seunghyun Park",
                "Sohee Yang",
                "Minjoon Seo."
            ],
            "title": "Spatial dependency parsing for semi-structured document information extraction",
            "venue": "Findings of ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Guillaume Jaume"
            ],
            "title": "FUNSD: A dataset for form understanding in noisy scanned documents",
            "venue": "ICDAR, volume 2, pages 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "Chenliang Li",
                "Bin Bi",
                "Ming Yan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si."
            ],
            "title": "StructuralLM: Structural pre-training for form understanding",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng Li",
                "Minghe Gao",
                "Longhui Wei",
                "Siliang Tang",
                "Wenqiao Zhang",
                "Mengze Li",
                "Wei Ji",
                "Qi Tian",
                "TatSeng Chua",
                "Yueting Zhuang."
            ],
            "title": "Gradientregulated meta-prompt learning for generalizable vision-language models",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "Juncheng Li",
                "Xin He",
                "Longhui Wei",
                "Long Qian",
                "Linchao Zhu",
                "Lingxi Xie",
                "Yueting Zhuang",
                "Qi Tian",
                "Siliang Tang."
            ],
            "title": "Fine-grained semantically aligned vision-language pre-training",
            "venue": "Advances in neural information processing systems, 35:7290\u20137303.",
            "year": 2022
        },
        {
            "authors": [
                "Juncheng Li",
                "Kaihang Pan",
                "Zhiqi Ge",
                "Minghe Gao",
                "Hanwang Zhang",
                "Wei Ji",
                "Wenqiao Zhang",
                "TatSeng Chua",
                "Siliang Tang",
                "Yueting Zhuang."
            ],
            "title": "Fine-tuning multimodal llms to follow zeroshot demonstrative instructions",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Juncheng Li",
                "Siliang Tang",
                "Fei Wu",
                "Yueting Zhuang."
            ],
            "title": "Walking with mind: Mental imagery enhanced embodied qa",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia, pages 1211\u2013 1219.",
            "year": 2019
        },
        {
            "authors": [
                "Juncheng Li",
                "Siliang Tang",
                "Linchao Zhu",
                "Haochen Shi",
                "Xuanwen Huang",
                "Fei Wu",
                "Yi Yang",
                "Yueting Zhuang."
            ],
            "title": "Adaptive hierarchical graph reasoning with semantic coherence for video-and-language inference",
            "venue": "Proceedings of the IEEE/CVF Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng Li",
                "Siliang Tang",
                "Linchao Zhu",
                "Wenqiao Zhang",
                "Yi Yang",
                "Tat-Seng Chua",
                "Fei Wu."
            ],
            "title": "Variational cross-graph reasoning and adaptive structured semantics learning for compositional temporal grounding",
            "venue": "IEEE Transactions on Pattern Analysis",
            "year": 2023
        },
        {
            "authors": [
                "Juncheng Li",
                "Xin Wang",
                "Siliang Tang",
                "Haizhou Shi",
                "Fei Wu",
                "Yueting Zhuang",
                "William Yang Wang."
            ],
            "title": "Unsupervised reinforcement learning of transferable meta-skills for embodied navigation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision",
            "year": 2020
        },
        {
            "authors": [
                "Xin Eric Wang"
            ],
            "title": "Compositional temporal grounding with structured variational crossgraph correspondence learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Juncheng Li",
                "Junlin Xie",
                "Linchao Zhu",
                "Long Qian",
                "Siliang Tang",
                "Wenqiao Zhang",
                "Haochen Shi",
                "Shengyu Zhang",
                "Longhui Wei",
                "Qi Tian",
                "Yueting Zhuang"
            ],
            "title": "Dilated context integrated network with crossmodal consensus for temporal emotion localization",
            "year": 2022
        },
        {
            "authors": [
                "Yuliang Liu",
                "Zhang Li",
                "Hongliang Li",
                "Wenwen Yu",
                "Yang Liu",
                "Biao Yang",
                "Mingxin Huang",
                "Dezhi Peng",
                "Mingyu Liu",
                "Mingrui Chen",
                "Chunyuan Li",
                "Xucheng Yin",
                "Cheng lin Liu",
                "Lianwen Jin",
                "Xiang Bai"
            ],
            "title": "On the hidden mystery of ocr in large multimodal",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo."
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "jani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Proceedings of the Annual Conference on Neural Information Processing",
            "year": 2019
        },
        {
            "authors": [
                "Qiming Peng",
                "Yinxu Pan",
                "Wenjin Wang",
                "Bin Luo",
                "Zhenyu Zhang",
                "Zhengjie Huang",
                "Yuhui Cao",
                "Weichong Yin",
                "Yongfeng Chen",
                "Yin Zhang",
                "Shikun Feng",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "Ernie-layout: Layout knowledge enhanced",
            "year": 2022
        },
        {
            "authors": [
                "Michael Ryoo",
                "AJ Piergiovanni",
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Anelia Angelova."
            ],
            "title": "Tokenlearner: Adaptive space-time tokenization for videos",
            "venue": "Advances in Neural Information Processing Systems, pages 12786\u201312797.",
            "year": 2021
        },
        {
            "authors": [
                "Guozhi Tang",
                "Lele Xie",
                "Lianwen Jin",
                "Jiapeng Wang",
                "Jingdong Chen",
                "Zhen Xu",
                "Qianying Wang",
                "Yaqiang Wu",
                "Hui Li."
            ],
            "title": "MatchVIE: Exploiting match relevancy between entities for visual information extraction",
            "venue": "IJCAI, pages 1039\u20131045.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Donald Metzler."
            ],
            "title": "Efficient transformers: A survey",
            "venue": "CoRR, abs/2009.06732.",
            "year": 2020
        },
        {
            "authors": [
                "Jiapeng Wang",
                "Lianwen Jin",
                "Kai Ding."
            ],
            "title": "LiLT: A simple yet effective language-independent layout transformer for structured document understanding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages",
            "year": 2022
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Wengang Zhou",
                "Jianzhuang Liu",
                "Houqiang Li."
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Peng Xu",
                "Wenqi Shao",
                "Kaipeng Zhang",
                "Peng Gao",
                "Shuo Liu",
                "Meng Lei",
                "Fanqing Meng",
                "Siyuan Huang",
                "Yu Qiao",
                "Ping Luo"
            ],
            "title": "Lvlm-ehub: A comprehensive evaluation benchmark for large visionlanguage models",
            "year": 2023
        },
        {
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "LayoutLM: Pre-training of text and layout for document image understanding",
            "venue": "ACM-SIGKDD, pages 1192\u20131200.",
            "year": 2020
        },
        {
            "authors": [
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Furu Wei."
            ],
            "title": "LayoutXLM: Multimodal pre-training for multilingual visually-rich document understanding",
            "venue": "arXiv preprint arXiv:2104.08836.",
            "year": 2021
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiyang Dai",
                "Bin Xiao",
                "Lu Yuan",
                "Jianfeng Gao."
            ],
            "title": "Focal attention for long-range interactions in vision transformers",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Shukang Yin",
                "Chaoyou Fu",
                "Sirui Zhao",
                "Ke Li",
                "Xing Sun",
                "Tong Xu",
                "Enhong Chen"
            ],
            "title": "A survey on multimodal large language models",
            "year": 2023
        },
        {
            "authors": [
                "Hang Zhang",
                "Yeyun Gong",
                "Yelong Shen",
                "Weisheng Li",
                "Jiancheng Lv",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Poolingformer: Long document modeling with pooling attention",
            "venue": "Proceedings of ICML.",
            "year": 2021
        },
        {
            "authors": [
                "Yue Zhang",
                "Zhang Bo",
                "Rui Wang",
                "Junjie Cao",
                "Chen Li",
                "Zuyi Bao."
            ],
            "title": "Entity relation extraction as dependency parsing in visually rich documents",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Visual Relation Extraction (VRE) is a powerful means of discovering relationships between entities within visually-rich documents. Existing methods often focus on manipulating entity features to find pairwise relations, yet neglect the more fundamental structural information that links disparate entity pairs together. The absence of global structure information may make the model struggle to learn longrange relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledge-guided relation Extraction (GOSE) framework. GOSE initiates by generating preliminary relation predictions on entity pairs extracted from a scanned image of the document. Subsequently, global structural knowledge is captured from the preceding iterative predictions, which are then incorporated into the representations of the entities. This \u201cgenerate-capture-incorporate\u201d cycle is repeated multiple times, allowing entity representations and global structure knowledge to be mutually reinforced. Extensive experiments validate that GOSE not only outperforms existing methods in the standard fine-tuning setting but also reveals superior cross-lingual learning capabilities; indeed, even yields stronger data-efficient performance in the low-resource setting. The code for GOSE will be available at https://github.com/chenxn2020/GOSE."
        },
        {
            "heading": "1 Introduction",
            "text": "Visually-rich document understanding (VrDU) aims to automatically analyze and extract key information from scanned/digital-born documents, such as forms and financial receipts (Jaume et al., 2019; Cui et al., 2021). Since visually-rich documents (VrDs) usually contain diverse structured information, Visual Relation Extraction (VRE), as\n\u2217 Work done during an internship at DAMO Research, Alibaba Group.\n\u2020Corresponding author.\na critical part of VrDU, has recently attracted extensive attention from both the academic and industrial communities (Jaume et al., 2019; Xu et al., 2021; Hwang et al., 2021; Wang et al., 2022a). The VRE task aims to identify relations between semantic entities in VrDs, severing as the essential basis of mapping VrDs to structured information which is closer to the human comprehension process of the VrDs (Zhang et al., 2021b). Recently, inspired by the success of pre-training in visually-rich document understanding (Li et al., 2021a; Xu et al., 2020; Wang et al., 2022a), many fine-tuning works predict relation for each entity pair independently, according to local semantic entity representations derived from the pre-trained model.\nAlthough existing VRE methods have achieved promising improvement, they ignore global structure information, i.e. dependencies between entity pairs. Without considering global structure knowledge, the model may easily predict conflicted links and struggle to capture longrange relations. Taking the state-of-the-art model LiLT (Wang et al., 2022a) as an example, as shown in Figure 1(a), although each relational entity pair predicted by the LiLT may make sense semantically, there are conflicted relational entity pairs such as (Labels, Blue) and (Closures, White), as well as (Tear Tape, White) and (Cartons, White), whose link crosses each other from a global view.\nThis phenomenon indicates that methods only using local features do not have the sufficient discriminatory ability for conflicted predictions. Furthermore, as shown in Figure 1(b), even though LiLT accurately identifies the relational entity pairs (No.OF STORES, 19) and (No.OF STORES, 18), LiLT still hardly learns long-range relations such as (No.OF STORES, 16) and (No.OF STORES, 17). Our intuition is that global structure knowledge can help the model learn long-range relations. The model can predict the relational entity pair (No.OF STORES, 17) by analogy with the global structure consistency between (No.OF STORES, 17) and (No.OF STORES, 18).\nIn this paper, we present the first study on leveraging global structure information for visual relation extraction. We focus on how to effectively mine and incorporate global structure knowledge into existing fine-tuning methods. It has the following two challenges: (1) Huge Mining Space. Considering N entities in a VrD, the computational complexity of capturing dependencies between entity pairs is quadratic to entity pair size (N2 \u00d7N2). So it is difficult to mine useful global structure information in the lack of guidance. (2) Noisy mining process. Since the process of mining dependencies between entity pairs relies on initial entity representations and lacks direct supervision, global structure information learned by the model is likely to contain noise. Mined noisy global structure knowledge by a model can in turn impair the performance of the model, especially when the model has low prediction accuracy in the early training stage.\nTo this end, we propose a general global structure knowledge-guided visual relation extraction method named GOSE, which can efficiently and accurately capture dependencies between entity pairs. GOSE is plug-and-play, which can be flexibly equipped to existing pre-trained VrDU models. Specifically, we first propose a global structure knowledge mining (GSKM) module, which can mine global structure knowledge effectively and efficiently. The GSKM module introduces a novel spatial prefix-guided self-attention mechanism, which takes the spatial layout of entity pairs as the attention prefix to progressively guide mining global structure knowledge in a local-global way. Our intuition is the spatial layout of entity pairs in VrDs may be a valuable clue to uncovering global structure knowledge. As shown in\nFigure 1(a), we can recognize crossover between entity pairs in 2D space by computing the spatial layout of linking lines. Furthermore, in order to increase the robustness of GOSE and handle the noisy mining process, we introduce an iterative learning strategy to combine the process of entity representations learning and global structure mining. The integration of global structure knowledge can help refine entity embeddings, while better entity embeddings can help mine more accurate global structure knowledge.\nIn summary, the contributions of our work are as follows:\n\u2022 We propose a global structure knowledgeguided visual relation extraction method, named GOSE. It can use the spatial layout as a clue to mine global structure knowledge effectively and leverage the iterative learning strategy to denoise global structure knowledge.\n\u2022 GOSE can be easily applied to existing pretrained VrDU models. Experimental results on the standard fine-tuning task over 8 datasets show that our method improves the average F1 performance of the previous SOTA models by a large margin: LiLT(+14.20%) and LayoutXLM(+12.88%).\n\u2022 We further perform comprehensive experiments covering diverse settings of VRE tasks, such as cross-lingual learning, and lowresource setting. Experimental results illustrate advantages of our model, such as crosslingual transfer and data-efficient learning."
        },
        {
            "heading": "2 Preliminaries",
            "text": "In this section, we first formalize the visually-rich document relation extraction task and then briefly introduce how the task was approached in the past."
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "The input to the VRE task is a scanned image of a document. Each visually rich document contains a set of semantic entities, and each entity is composed of a group of words and coordinates of the bounding box. We use a lowercase letter e to represent semantic entity, where e = {[w1, w2, ..., wk], [x1, y1, x2, y2]}. The sequence [w1, w2, ..., wk] means the word group, x1/x2 and y1/y2 are left/right x-coordinates and top/down y-coordinates respectively. The corresponding boldface lower-case letter e indicates its\nembedding. Let E and R represent the set of entities and relations respectively, where E = {ei}Ni=1, R = {(ei, ej)} \u2286 E \u00d7 E , (ei, ej) mean the keyvalue entity pair and the directed link from ei to ej . So each visually rich document can be denoted as D = {E ,R}. The goal of VRE task is to determine whether a relation (link) exists between any two semantic entities. Notably, the semantic entity may exist relations with multiple entities or does not have relations with any other entities."
        },
        {
            "heading": "2.2 Fine-tuning for Visual Relation Extraction",
            "text": "Inspired by the success of pre-training in visuallyrich document understanding, most existing methods (Jaume et al., 2019; Xu et al., 2021; Wang et al., 2022a) fine-tune pre-trained VrDU model for VRE task. These methods take entity representations from a pre-trained VrDU model as input and then train a binary classifier to predict relations for all possible semantic entity pairs. Specifically, these methods project entity representations to key/value features respectively by two FFN layers. The features of key and value are concatenated and fed into a binary classifier to compute loss."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we describe the proposed framework named GOSE in detail. As shown in Figure 2, our method consists of three main modules, a Base module, a Relation Feature Generation (RFG)\nmodule, and a Global Structure Knowledge Mining (GSKM) module. In Section 3.1, we present how the Base module can be combined with a pretrained VrDU model to generate initial key/value features. In Section 3.2, we introduce the RFG module to generate relation features. Then, in Section 3.3, we elaborate on how the GSKM module mines global structure knowledge in a local-global way. We further provide a theoretical analysis to better understand the efficient and effective features of the GSKM module. Finally, we show how to apply our iterative learning strategy to mine accurate global structure knowledge in Section 3.4."
        },
        {
            "heading": "3.1 Base Module",
            "text": "Given a visually rich document, the Base module first generates preliminary key/value features, which is same as most fine-tuning methods (Xu et al., 2021; Gu et al., 2022; Wang et al., 2022a). Specifically, we use a pre-trained VrDU model to obtain semantic entity representations. Then entity representation e is fed into two separated FeedForward Networks (FFN) to generate the initial key feature and value feature (denoted as H(0)k and H (0) v respectively), as written with Eq. (1):\nH (0) k = Wkeye + bkey, H(0)v = Wvaluee + bvalue, (1)\nwhere Wkey/value \u2208 R2dh\u00d7dh are trainable weights and bkey/value \u2208 Rdh are trainable biases."
        },
        {
            "heading": "3.2 Relation Feature Gneration (RFG) Module",
            "text": "Taking key/value features as input, the RFG module generates relation features for all entity pairs. We denote key/value features at the t-th iteration as H\n(t) k and H (t) v respectively, where H (t) k/v \u2208 R N\u00d7dh . At the t-th iteration, the RFG module concatenates H\n(t) k and H (t) v as input and uses a bi-affine classifier to compute the relation logits of each entity pairs with Eq.(2):\nl(t) = H (t) k W1H (t) v +H (t) k W2, (2)\nwhere l(t) \u2208 RN\u00d7N\u00d72 denotes relation logits at the t-th iteration. Then we employ a FFN block to generate a unified relation feature map (denoted as R(t) \u2208 RN\u00d7N\u00d7dh), which contains relation features of all entity pairs. the relation feature map is calculated as follows:\nR(t) = Wrl (t) + br, (3)\nwhere Wr \u2208 R2\u00d7dh is trainable weight and br \u2208 Rdh is trainable biases."
        },
        {
            "heading": "3.3 Global Structure Knowledge Mining (GSKM) module",
            "text": "The GSKM module mines the global structure knowledge on the relation feature map in a localglobal way. As illustrated in Figure 2, this module consists of a spatial prefix-guided local selfattention (SPLS) layer and a global interaction layer. The SPLS layer first partitions the relation feature map into multiple non-overlapping windows and then performs spatial prefix-guided local self-attention independently in each window. The global interaction layer brings long-range dependencies to the local self-attention with negligible computation cost."
        },
        {
            "heading": "3.3.1 Spatial Prefix-guided Local Self-attention (SPLS) Layer",
            "text": "To address the challenge of mining global structure knowledge in a large computational space, we consider the spatial layout information of entity pairs in VrDs to guide self-attention computation. Most of existing VrDU methods independently encode the spatial information of each entity in the pretraining phase (i.e., the coordinate information of the bounding boxes in 2D space). However, it is necessary to insert spatial layout information of entity pairs explicitly in the fine-tuning phase (i.e.,\nthe spatial information of linking lines between bounding boxes in 2D space). Therefore, inspired by the success of prompt learning (He et al., 2022; Li et al., 2023a) we propose a spatial prefix-guided local self-attention (SPLS) layer to mine global structure knowledge in a parameter-efficient manner. Spatial Prefix Construction. We calculate spatial geometric features (denoted as S \u2208 RN\u00d7N\u00d7dh) of linking lines between each entity pair as the spatial prefix. As shown in Figure 2(b), for an entity pair (ei, ej), we calculate the direction and Euclidean distance of the line linking from the vertice (xi, yi) of the bounding box of ei to the same vertice (xj , yj) of ej as follows:\ngi,j = [W\u03b8\u03b8(i, j);Wdd(i, j)], d(i, j) = \u221a (xi \u2212 xj)2 + (yi \u2212 yj)2,\n\u03b8(i, j) = arctan yj \u2212 yi xj \u2212 xi ,\n(4)\nwhere W\u03b8/d \u2208 R1\u00d7 dh 6 are trainable weights. Therefore the spatial geometric features Si,j of entity pair (ei, ej) is calculated as follows:\nSi,j = [g tl i,j ; g ct i,j ; g br i,j ], (5)\nwhere gtl, gct, gbr indicate top-left, center, and bottom-right points respectively. Then we treat S as the spatial prefix and compute attention on the hybrid keys and values. Spatial Prefix-guided Attention. We first partition R(t) into non-overlapping windows and then perform spatial prefix-guided local attention within each local window. The variant formula of selfattention with the spatial prefix as follows 1:\nR (t) local = softmax ( R(t)Wq[SW s k ;R (t)Wk] \u22a4) [ SW sv\nR(t)Wv ] = softmax(Q(t)R [Sk;K (t) R ] \u22a4) [ Sv V\n(t) R ] = (1\u2212 \u03bb(R(t)))Attn(Q(t)R ,K (t) R ,V\n(t) R )\ufe38 \ufe37\ufe37 \ufe38\nstandard attention\n+ \u03bb(R(t))Attn((Q(t)R , Sk, Sv)\ufe38 \ufe37\ufe37 \ufe38 spatial-prefix guided attention ,\n(6)\n\u03bb(R(t)) =\n\u2211 i exp(Q (t) R S\n\u22a4 k )i\u2211\ni exp(Q (t) R S \u22a4 k )i + \u2211 j exp(Q (t) R K (t)\u22a4 R )j ,\n(7)\n1Without loss of generalization, we ignore the scaling factor \u221a d of the softmax operation for the convenience of explanation.\nwhere R(t)local refers to the local attention output at the t-th iteration, \u03bb(R(t)) denotes the scalar for the sum of normalized attention weights on the key and value vectors from spatial prefix."
        },
        {
            "heading": "3.3.2 Global Interaction Layer (GIL)",
            "text": "After the SPLS layer effectively aggregates local correlations with window priors, we introduce a global interaction layer to bring long-range dependencies to the local self-attention. As illustrated in Figure 2, we use learnable global tokens T \u2208 RM\u00d7dh to compute the global interaction at the t-th iteration as follows:\nT\u0302 (t) = Attn(Q(t)T ,K (t) R ,V (t) R ), R (t) global = Attn(Q (t) R ,K (t) T\u0302 , V (t) T\u0302 ),\n(8)\nwhere R(t)global refers to the global interaction at the t-th iteration. T will be updated in the same way as Hk/v throughout the iterative learning process. Subsequently, we compute R(t+1) and employ the mean pooling operation to obtain the context-aware key and value features as:\nR(t+1) = R (t) local +R (t) global,\nH\u0302 (t+1) k/v = mean-pooling(R\n(t+1)), (9)\nwhere H\u0302(t+1)k/v contain global structure information. Analysis of GSKM. Here we give some analysis to help better understand GSKM, especially effective and efficient features. Effectiveness. GSKM can effectively learn global structure knowledge guided by spatial layout information in VrDs. As shown in Eq. 6, the first term Attn(Q(t)R ,K (t) R ,V (t) R ) is the standard attention in the content side, whereas the second term represents the 2D spatial layout guidelines. In this sense, our method implements 2D spatial layout to guide the attention computation in a way similar to linear interpolation. Specifically, the GSKM module down-weights the original content attention probabilities by a scalar factor (i.e., 1\u2212\u03bb) and redistributes the remaining attention probability \u03bb to attend to spatial-prefix guided attention, which likes the linear interpolation. Efficiency. GSKM can reduce the computation complexity N4 to N2 \u00d7 S2, where S denotes window size. In the SPLS layer, with a relation feature map R(t) \u2208 RN\u00d7N\u00d7dh as input, we partition R(t) into non-overlapping windows with shape (NS \u00d7 N S , S \u00d7 S, dh) to reduce the computation\ncomplexity N4 of self-attention to (NS \u00d7 N S ) \u00d7 (S \u00d7 S)2 = N2 \u00d7 S2, where S denotes window size. Meanwhile, the computation complexity of the global interaction layer (N2\u00d7M ) is negligible, as the number of global tokens M is much smaller than the window size S2 in our method."
        },
        {
            "heading": "3.4 Iterative Learning",
            "text": "To alleviate the noisy mining process, we further propose an iterative learning strategy to enable global structure information and entity embeddings mutually reinforce each other. Specifically, we incorporate global structure knowledge into entity representations through a gating mechanism:\ng = sigmoid(Wg[H (t) k/v; H\u0302 (t+1) k/v ] + bg)\nH (t+1) k/v = H (t) k/v + g \u00b7 H\u0302 (t+1) k/v\n(10)\nFinally, these new key and value features are fed back to the classifier for the next iteration. After repeating this iterative process K times, we get updated key and value features H(K)k/v to compute final logits l(K). Finally, we calculate Binary Cross Entropy (BCE) loss based on l(K) as follows 2:\nL = N\u2211 i=1 N\u2211 j=1 \u2113(l (K) i,j , yi,j) (11)\nwhere yi,j \u2208 [0, 1] is binary ground truth of the entity pair (ei, ej), \u2113(., .) is the cross-entropy loss."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we perform detailed experiments to demonstrate the effectiveness of our proposed method GOSE among different settings. Besides the standard setting of typical language-specific fine-tuning (section 4.2), we further consider more challenging settings to demonstrate the generalizability of GOSE such as cross-lingual zero-shot transfer learning (section 4.3) and few-shot learning (section 4.4). Before discussing the results, we provide the details of the experimental setup below."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1.1 Datasets",
            "text": "FUNSD (Jaume et al., 2019) is a scanned document dataset for form understanding. It has 149 training samples and 50 test samples with various layouts.\n2To make a fair comparison with baselines, we use the same binary classification training strategy.\nXFUND (Xu et al., 2021) is a multilingual form understanding benchmark. It includes 7 languages with 1,393 fully annotated forms. Each language includes 199 forms. where the training set includes 149 forms, and the test set includes 50 forms."
        },
        {
            "heading": "4.1.2 Baselines",
            "text": "We use the following baselines: (1) text-only pre-trained models without structure information: XLM-RoBERT (Conneau et al., 2020), InfoXLM (Chi et al., 2021); (2) layout-aware pre-trained VrDU models with local structure information: LayoutXLM (Xu et al., 2021), XYLayoutLM (Gu et al., 2022), LiLT (Wang et al., 2022a). All of the experimental results of these baselines are from their original papers directly, except for the results on the few-shot learning task 3."
        },
        {
            "heading": "4.1.3 Experiment Implementation",
            "text": "We use the bi-affine classifier and binary classification training loss over all datasets and settings following (Wang et al., 2022a) for a fair comparison. The entity representation is the first token vector in each entity. For the few-shot learning task, we randomly sample training samples over each shot five times with different random seeds,\n3We re-implement the results using the official code.\nand report the average performance under five sampling times for a fair comparison. More details of the training hyper-parameters can be found in Appendix A."
        },
        {
            "heading": "4.2 Language-specific Fine-tuning",
            "text": "We compare the performance of GOSE applied to the language-specific fine-tuning task. The experimental results are shown in Table 1. First, all VRE methods with structure information outperform the text-only models XLM-RoBERT and InfoXLM, which indicates structure information plays an important role in the VrDU. Second, while pre-trained VrDU models have achieved significant improvement over text-only models, our method still outperforms them by a large margin. This phenomenon denotes that incorporating global structure information is generally helpful for VRE. Compared to the SOTA method LiLT (Wang et al., 2022a), GOSE achieves significant improvements on all language datasets and has an increase of 14.20% F1 accuracy on the average performance. Third, we further observe that our GOSE is model-agnostic, which can consistently improves diverse pre-trained models\u2019 relation extraction performance on all datasets. For example, GOSE has an improvement of 12.88% F1 accuracy on average performance compared to LayoutXLM (Xu et al., 2021)."
        },
        {
            "heading": "4.3 Cross-lingual Transfer Learning",
            "text": "We evaluate GOSE on the cross-lingual zero-shot transfer learning task. In this setting, the model is only fine-tuned on the FUNSD dataset (in English) and evaluated on each specific language dataset. We present the evaluation results in Table 2. It can be observed that GOSE significantly outperforms its competitors and consistently improve diverse backbone encoders\u2019 relation extraction performance on all datasets. This verifies that GOSE can capture the common global structure information invariance among different languages and transfer it to other languages for VRE. We observe that the performance improvement of GOSE(LayoutXLM) is not as significant as GOSE(LiLT). This may be attributed to that the architecture of LiLT decoupling the text and layout information makes it easier to learn language-independent structural information, which is consistent with the observation of previous works (Wang et al., 2022a). We further evaluate GOSE on the Multilingual learning task, the results are shown in Appendix B."
        },
        {
            "heading": "4.4 Few-shot Learning",
            "text": "Previous experiments illustrate that our method achieves improvements using full training samples. We further explore whether GOSE could mine global structure information in the low-resource setting. Thus, we compare with the previous SOTA model LiLT on few-shot settings. The experimental results in Figure 3 indicate that the average performance of GOSE still outperforms the SOTA model LiLT. Notably, our GOSE achieves a comparable average performance (67.82%) on FUNSD dataset using only 64 training samples than LiLT does (62.76%) using full training samples, which further proves that our proposed method can more efficiently leverage training samples. This success may be attributed to the incorporation of global\nstructure knowledge can improve the generalization of the model."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "Effectiveness of individual components. We further investigate the effectiveness of different modules in our method. we compare our model with the following variants in Table 3.\n(1) w/o GIL. In this variant, we remove the global interaction layer from GSKM. This change means that the GSKM module only performs local selfattention. The results shown in Table 3 suggest that our GIL can encourage the GSKM module to better exploit dependencies of entity pairs.\n(2) w/o Spatial-Prefix. In this variant, we remove the spatial prefix-guided interaction. This change causes a significant performance decay. This suggests the injection of additional spatial layout information can guide the attention mechanism attending to entity pairs with similar spatial structure and thus help the model learn global structural knowledge.\n(3) w/o GSKM. In this variant, we remove the GSKM module from GOSE. This change means the model only obtains context-aware relation features through a mean-pooling operation. The results shown in Table 3 indicate that although the mean-pooling operation can achieve a performance improvement, the GSKM module can mine more useful global structure information. Ablation of Iteration Rounds. The highlight of our GOSE is mining global structure knowledge and refining entity embeddings iteratively. We argue that these two parts can mutually reinforce each other: the integration of global structure knowledge can help refine entity embeddings. On the contrary, better entity embeddings can help mine more accurate global structure knowledge. Thus, we evaluate the influence of the iteration rounds. The results are shown in 5(a), which indicates GOSE usually achieves the best results within small number of\niteration rounds. In addition, we investigate the performance of different information under multiple iterations in Appendix C. Ablation of Global Tokens. We further investigate the effect of the number of global tokens in the GIL on our model. The results are shown in Figure 5 (b), which denotes GOSE can achieve optimal results within a small number of global tokens, while keeping efficiency."
        },
        {
            "heading": "4.6 Further Analysis",
            "text": "Case Study. To better illustrate the effectiveness of global structure knowledge, we conduct the specific case analysis on the VRE task as shown in Figure 4. Through the visualization of examples, we can notice: (1) as shown in Figure 4(a), GOSE can greatly mitigate the prediction of conflicted links which reveals that our method can capture global structure knowledge to detect conflicted interactions between entity pairs. (2) as shown in Figure 4(b), GOSE can learn long-range relations by analogy with the linking pattern of entity pairs, while keeping a good recall. Notably, it is also difficult for GOSE to predict long-range relations where is not sufficient global structure knowledge. For example, GOSE does not predict well relations\nof the entity \"Section A\", due to there are few topto-bottom and one-to-many linking patterns.\nVisualization of Attention over Spatial Information. To illustrate the effect of our proposed spatial prefix-guided local self-attention. We calculate the attention scores over the spatial information for the document in Figure 5(a), i.e., the spatial-prefix guided attention weights using Equation (6). As shown in Figure 6, the entity pair (TO:, Sam Zolot) pays more attention towards the entity pair (FROM:, D.J.Landro) and (DATE:, 2-DEC97). This phenomenon indicates that the injection of additional spatial layout information of entity pairs can guide the attention mechanism attending to entity pairs with similar spatial structure, thereby enhancing the capacity of the model to discern precise dependencies between entity pairs."
        },
        {
            "heading": "5 Related Works",
            "text": ""
        },
        {
            "heading": "5.1 Visual Relation Extraction",
            "text": "Visual relation extraction (VRE) aims at identifying relations between semantic entities from visuallyrich documents. Early methods based on graph neural networks (Zhang et al., 2021b; Tang et al., 2021; Li et al., 2021b, 2022b, 2023c) learned node features by treating semantic entities as nodes in\na graph. Recently, most studies (Li et al., 2020; Wang et al., 2022a; Gu et al., 2022; Huang et al., 2022b), used the self-supervised pre-training and fine-tuning techniques to boost the performance on document understanding. Although they have achieved significant improvement on VIE tasks, especially on the semantic entity recognition (SER) task (Peng et al., 2022). VRE remains largely underexplored and is also a challenging task. In this paper, we focus on mining global structure knowledge to guide relation extraction. To the best of our knowledge, our work is the first attempt to exploit dependencies of entity pairs for this task."
        },
        {
            "heading": "5.2 Efficient Transformers",
            "text": "Efficient transformers (Dong et al., 2019; Li et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Ryoo et al., 2021; Zhang et al., 2021a; Li et al., 2022c) are a class of methods designed to address the quadratic time and memory complexity problems of vanilla self-attention. More similar to us are methods that leverage down-sampling to reduce the resolution of the sequence, such as window-based vision transformers (Liu et al., 2021; Yang et al., 2021; Huang et al., 2022a; Wang et al., 2022b). Different from existing methods, we propose the spatial prefix-guided attention mechanism, which leverages spatial layout properties of VrDs to guide GOSE to mine global structure knowledge."
        },
        {
            "heading": "6 Discussion",
            "text": "Recently, The Multimodal Large Language Model (MLLM) (Li et al., 2022a; Yin et al., 2023) has emerged as a pioneering approach. MLLMs leverage powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as following zero-shot demonstrative instructions (Li et al., 2023b) and OCR-free math reasoning (Zhu et al., 2023; Dai et al., 2023), are rare in traditional methods. Several studies (Xu et al., 2023; Liu et al., 2023) have conducted comprehensive evaluations of publicly available large multimodal models. These investigations reveal that MLLMs still struggle with the VIE task. In this paper, we introduce and empirically validate that global structural knowledge is useful for visually-rich document information extraction. Our insights have the potential to shape the advancement of large model technology in the domain of visually-rich documents."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we present a general global structure knowledge-guided relation extraction method for visually-rich documents, which jointly and iteratively learns entity representations and mines global dependencies of entity pairs. To the best of our knowledge, GOSE is the first work leveraging global structure knowledge to guide visual relation extraction. Concretely, we first use a base module that combines with a pre-trained model to obtain initial relation predictions. Then, we further design a relation feature generation module that generates relation features and a global structure knowledge mining module. These two modules perform the \"generate-capture-incorporate\u201d process multiple times to mine and integrate accurate global structure knowledge. Extensive experimental results on three different settings (e.g., standard fine-tuning, cross-lingual learning, low-resource setting) over eight datasets demonstrate the effectiveness and superiority of our GOSE."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to express gratitude to the anonymous reviewers for their kind comments. This work has been supported in part by the Zhejiang NSF (LR21F020004), Key Research and Development Projects in Zhejiang Province (No. 2023C01030, 2023C01032), NSFC (No. 62272411), National Key Research and Development Program of China (2018AAA0101900), Ant Group and AlibabaZhejiang University Joint Research Institute of Frontier Technologies. Our work was also supported by Scientific Research Fund of Zhejiang Provincial Education Department.\nLimitations\nThe proposed work still contains several limitations to address in future work, as follows: Method. One limitation of our method is that it cannot capture global structure information from informative visual clues. The visual features in visually-rich documents such as font size and color may provide diverse structure information. For example, section titles in resumes and job ads are often in fonts different from the content. We leave this for future work. Task. We only evaluate the visual relation extraction task covering diverse settings. Due to the limited budget and computation resources, we cannot\nafford evaluation on more tasks related to visuallyrich documents. We will plan to evaluate the proposed approach on more visual information extraction tasks such as semantic entity recognition."
        },
        {
            "heading": "A Hyperparameters",
            "text": "All of our experiments are performed on one NVIDIA 3090 GPU with the PyTorch (Paszke et al., 2019) framework. We use a mini-batch AdamW (Loshchilov and Hutter, 2018) optimizer with a weight decay of 0.1. The model is trained with a batch size of 2. The window size is fixed to 64. In the Language-Specific Fine-tuning experiments for all languages, the learning rate, steps are set to 6.25\u00d710\u22126,2\u00d7104 accordingly on LiLT encoder, while 2.5\u00d7 10\u22125,4\u00d7 104 on the Layoutxlm encoder. In the multilingual learning experiments, we use the full language-specific data for training, with total steps 1.6\u00d7 105. In the cross-lingual zero-shot transfer learning experiments, we directly evaluate the model, which was trained in the previous language-specific experiments, on the XFUND dataset."
        },
        {
            "heading": "B Multilingual Learning",
            "text": "We evaluate GOSE on the multilingual learning setting. In this setting, the model is fine-tuned with all 8 languages simultaneously and evaluated on each specific language. From the experimental results shown in Table 4, we can find that although this setting further improves the baseline model performance compared to the language-specific finetuning, our method GOSE once again outperforms its counterparts by a large margin. We hold that the superior performance of our method GOSE can be attributed to the fact that the previous method still does not learn sufficient global structure information in multilingual learning. This finding also demonstrates that mining global structure information is beneficial for the VRE task."
        },
        {
            "heading": "C Effect of Different Information",
            "text": "We show the performance of our GOSE (LiLT) under multiple iterations in Figure 7. We can observe\nthat (1) when the iterative learning process begins, both global structural features and entity representations progressively enhance and mutually reinforce each other. (2) In scenarios where the number of iteration rounds stands at zero, i.e., without iterative learning. The performance of global structure information is poor. This may be because the initial entity representations obtained from the pre-trained model are not strong, thus the mined structural information without iterative optimization is noisy. (3) As the number of iteration rounds increases to a certain point, the performance of the model decreases. This phenomenon can be attributed to too many iteration rounds that can cause mined global structural information to become over-smoothing thus affecting the final performance of our model."
        }
    ],
    "title": "Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document",
    "year": 2023
}