{
    "abstractText": "Traditional event detection methods require predefined event schemas. However, manually defining event schemas is expensive and the coverage of schemas is limited. To this end, some works study the event type induction (ETI) task, which discovers new event types via clustering. However, the setting of ETI suffers from two limitations: event types are not linked into the existing hierarchy and have no semantic names. In this paper, we propose a new research task named Event Ontology Completion (EOC), which aims to simultaneously achieve event clustering, hierarchy expansion and type naming. Furthermore, we develop a HierarchicAL STructure EvOlution Network (HALTON) for this new task. Specifically, we first devise a Neighborhood Contrastive Clustering module to cluster unlabeled event instances. Then, we propose a Hierarchy-Aware Linking module to incorporate the hierarchical information for event expansion. Finally, we generate meaningful names for new types via an In-Context Learning-based Naming module. Extensive experiments indicate that our method achieves the best performance, outperforming the baselines by 8.23%, 8.79% and 8.10% of ARI score on three datasets1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pengfei Cao"
        },
        {
            "affiliations": [],
            "name": "Yupu Hao"
        },
        {
            "affiliations": [],
            "name": "Yubo Chen"
        },
        {
            "affiliations": [],
            "name": "Kang Liu"
        },
        {
            "affiliations": [],
            "name": "Jiexin Xu"
        },
        {
            "affiliations": [],
            "name": "Huaijun Li"
        },
        {
            "affiliations": [],
            "name": "Xiaojian Jiang"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        }
    ],
    "id": "SP:b79048473dafe373f71bb9f7e2427b19f597211f",
    "references": [
        {
            "authors": [
                "Amit Bagga",
                "Breck Baldwin."
            ],
            "title": "Entity-based cross-document coreferencing using the vector space model",
            "venue": "COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics.",
            "year": 1998
        },
        {
            "authors": [
                "Collin F. Baker",
                "Charles J. Fillmore",
                "John B. Lowe."
            ],
            "title": "The Berkeley FrameNet project",
            "venue": "COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics.",
            "year": 1998
        },
        {
            "authors": [
                "Christopher DB Burt",
                "Simon Kemp",
                "Martin A Conway."
            ],
            "title": "Themes, events, and episodes in autobiographical memory",
            "venue": "Memory & Cognition, 31:317\u2013 325.",
            "year": 2003
        },
        {
            "authors": [
                "Pengfei Cao",
                "Yubo Chen",
                "Jun Zhao",
                "Taifeng Wang."
            ],
            "title": "Incremental event detection via knowledge consolidation networks",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 707\u2013717. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Pengfei Cao",
                "Zhuoran Jin",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Zero-shot cross-lingual event argument extraction with language-oriented prefix-tuning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 12589\u201312597.",
            "year": 2023
        },
        {
            "authors": [
                "Nathanael Chambers."
            ],
            "title": "Event schema induction with a probabilistic entity-driven model",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807. Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Nathanael Chambers",
                "Dan Jurafsky."
            ],
            "title": "Templatebased information extraction without the templates",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 976\u2013986. Association",
            "year": 2011
        },
        {
            "authors": [
                "Yubo Chen",
                "Liheng Xu",
                "Kang Liu",
                "Daojian Zeng",
                "Jun Zhao."
            ],
            "title": "Event extraction via dynamic multipooling convolutional neural networks",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
            "year": 2015
        },
        {
            "authors": [
                "Klaas Dellschaft",
                "Steffen Staab."
            ],
            "title": "On how to perform a gold standard based evaluation of ontology learning",
            "venue": "Proceedings of the International Semantic Web Conference, pages 228\u2013241. Springer.",
            "year": 2006
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "George Doddington",
                "Alexis Mitchell",
                "Mark Przybocki",
                "Lance Ramshaw",
                "Stephanie Strassel",
                "Ralph Weischedel."
            ],
            "title": "The automatic content extraction (ACE) program \u2013 tasks, data, and evaluation",
            "venue": "Proceedings of the Fourth International Conference",
            "year": 2004
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Event extraction by answering (almost) natural questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 671\u2013683. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Carl Edwards",
                "Heng Ji."
            ],
            "title": "Semi-supervised new event type induction and description via contrastive loss-enforced batch attention",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3805\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross B. Girshick."
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,",
            "year": 2020
        },
        {
            "authors": [
                "Lifu Huang",
                "Taylor Cassidy",
                "Xiaocheng Feng",
                "Heng Ji",
                "Clare R. Voss",
                "Jiawei Han",
                "Avirup Sil."
            ],
            "title": "Liberal event extraction and event schema induction",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2016
        },
        {
            "authors": [
                "Lifu Huang",
                "Heng Ji."
            ],
            "title": "Semi-supervised new event type induction and event detection",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 718\u2013724. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Lawrence Hubert",
                "Phipps Arabie."
            ],
            "title": "Comparing partitions",
            "venue": "Journal of classification, pages 193\u2013218.",
            "year": 1985
        },
        {
            "authors": [
                "Heng Ji",
                "Ralph Grishman."
            ],
            "title": "Refining event extraction through cross-document inference",
            "venue": "Proceedings of ACL-08: HLT, pages 254\u2013262. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Xiaomeng Jin",
                "Manling Li",
                "Heng Ji."
            ],
            "title": "Event schema induction with double graph autoencoders",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Paul Kingsbury",
                "Martha Palmer."
            ],
            "title": "Propbank: the next level of treebank",
            "venue": "Proceedings of Treebanks and lexical Theories, volume 3.",
            "year": 2003
        },
        {
            "authors": [
                "Manling Li",
                "Sha Li",
                "Zhenhailong Wang",
                "Lifu Huang",
                "Kyunghyun Cho",
                "Heng Ji",
                "Jiawei Han",
                "Clare Voss."
            ],
            "title": "The future is not one-dimensional: Complex event schema induction by graph modeling for event prediction",
            "venue": "Proceedings of the 2021",
            "year": 2021
        },
        {
            "authors": [
                "Manling Li",
                "Qi Zeng",
                "Ying Lin",
                "Kyunghyun Cho",
                "Heng Ji",
                "Jonathan May",
                "Nathanael Chambers",
                "Clare Voss."
            ],
            "title": "Connecting the dots: Event graph schema induction with path language modeling",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Sha Li",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Documentlevel event argument extraction by conditional generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
            "year": 2021
        },
        {
            "authors": [
                "Sha Li",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Open relation and event type discovery with type abstraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6864\u20136877. Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Tianle Li",
                "Xueguang Ma",
                "Alex Zhuang",
                "Yu Gu",
                "Yu Su",
                "Wenhu Chen."
            ],
            "title": "Few-shot in-context learning on knowledge base question answering",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Jian Liu",
                "Yubo Chen",
                "Kang Liu",
                "Wei Bi",
                "Xiaojiang Liu."
            ],
            "title": "Event extraction as machine reading comprehension",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1641\u20131651. Association",
            "year": 2020
        },
        {
            "authors": [
                "Minqian Liu",
                "Shiyu Chang",
                "Lifu Huang."
            ],
            "title": "Incremental prompting: Episodic memory prompt for lifelong event detection",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 2157\u20132165. International Committee on",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Heyan Huang",
                "Ge Shi",
                "Bo Wang."
            ],
            "title": "Dynamic prefix-tuning for generative template-based event extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5216\u20135228.",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Zhunchen Luo",
                "Heyan Huang."
            ],
            "title": "Jointly multiple events extraction via attention-based graph information aggregation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1247\u20131256. Association",
            "year": 2018
        },
        {
            "authors": [
                "Zichen Liu",
                "Hongyuan Xu",
                "Yanlong Wen",
                "Ning Jiang",
                "HaiYing Wu",
                "Xiaojie Yuan."
            ],
            "title": "TEMP: Taxonomy expansion with dynamic margin loss through taxonomy-paths",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Yubo Ma",
                "Zehao Wang",
                "Mukai Li",
                "Yixin Cao",
                "Meiqi Chen",
                "Xinze Li",
                "Wenqi Sun",
                "Kunquan Deng",
                "Kun Wang",
                "Aixin Sun",
                "Jing Shao."
            ],
            "title": "MMEKG: Multi-modal event knowledge graph towards universal representation across modalities",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Kiem-Hieu Nguyen",
                "Xavier Tannier",
                "Olivier Ferret",
                "Romaric Besan\u00e7on."
            ],
            "title": "Generative event schema induction with entity disambiguation",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
            "year": 2015
        },
        {
            "authors": [
                "Trung Minh Nguyen",
                "Thien Huu Nguyen."
            ],
            "title": "One for all: Neural joint modeling of entities and events",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, pages 6851\u20136858. AAAI Press.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin."
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 815\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Satoshi Sekine."
            ],
            "title": "On-demand information extraction",
            "venue": "Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 731\u2013738. Association for Computational Linguistics.",
            "year": 2006
        },
        {
            "authors": [
                "Jiaming Shen",
                "Yunyi Zhang",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Corpus-based open-domain event type induction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5427\u20135440. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyi Song",
                "Ann Bies",
                "Stephanie Strassel",
                "Tom Riese",
                "Justin Mott",
                "Joe Ellis",
                "Jonathan Wright",
                "Seth Kulick",
                "Neville Ryant",
                "Xiaoyi Ma."
            ],
            "title": "From light to rich ERE: Annotation of entities, relations, and events",
            "venue": "Proceedings of the The 3rd Workshop on",
            "year": 2015
        },
        {
            "authors": [
                "Tian-Xiang Sun",
                "Xiang-Yang Liu",
                "Xi-Peng Qiu",
                "Xuan-Jing Huang."
            ],
            "title": "Paradigm shift in natural language processing",
            "venue": "Machine Intelligence Research, 19(3):169\u2013183.",
            "year": 2022
        },
        {
            "authors": [
                "Joshua B Tenenbaum",
                "Charles Kemp",
                "Thomas L Griffiths",
                "Noah D Goodman."
            ],
            "title": "How to grow a mind: Statistics, structure, and abstraction",
            "venue": "science, 331(6022):1279\u20131285.",
            "year": 2011
        },
        {
            "authors": [
                "Laurens Van Der Maaten."
            ],
            "title": "Accelerating t-sne using tree-based algorithms",
            "venue": "The Journal of Machine Learning Research, 15(1):3221\u20133245.",
            "year": 2014
        },
        {
            "authors": [
                "Sijia Wang",
                "Mo Yu",
                "Shiyu Chang",
                "Lichao Sun",
                "Lifu Huang."
            ],
            "title": "Query and extract: Refining event extraction as type-oriented binary decoding",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 169\u2013182. Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Ziqi Wang",
                "Xu Han",
                "Wangyi Jiang",
                "Rong Han",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Peng Li",
                "Yankai Lin",
                "Jie Zhou."
            ],
            "title": "MAVEN: A Massive General Domain Event Detection Dataset",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "Nan Xu",
                "Hongming Zhang",
                "Jianshu Chen."
            ],
            "title": "Ceo: Corpus-based open-domain event ontology induction",
            "venue": "arXiv preprint arXiv:2305.13521.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Yu",
                "Heng Ji",
                "Prem Natarajan."
            ],
            "title": "Lifelong event detection with knowledge transfer",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5278\u2013 5290. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Quan Yuan",
                "Xiang Ren",
                "Wenqi He",
                "Chao Zhang",
                "Xinhe Geng",
                "Lifu Huang",
                "Heng Ji",
                "Chin-Yew Lin",
                "Jiawei Han."
            ],
            "title": "Open-schema event profiling for massive news corpora",
            "venue": "Proceedings of the 27th ACM International Conference on Information and",
            "year": 2018
        },
        {
            "authors": [
                "Kai Zhang",
                "Yuan Yao",
                "Ruobing Xie",
                "Xu Han",
                "Zhiyuan Liu",
                "Fen Lin",
                "Leyu Lin",
                "Maosong Sun."
            ],
            "title": "Open hierarchical relation extraction",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Jun Zhao",
                "Tao Gui",
                "Qi Zhang",
                "Yaqian Zhou."
            ],
            "title": "A relation-oriented clustering method for open relation extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9707\u20139718. Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zhao",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Transformer: A general framework from machine translation to others",
            "venue": "Machine Intelligence Research, pages 1\u201325.",
            "year": 2023
        },
        {
            "authors": [
                "Zhun Zhong",
                "Enrico Fini",
                "Subhankar Roy",
                "Zhiming Luo",
                "Elisa Ricci",
                "Nicu Sebe."
            ],
            "title": "Neighborhood contrastive learning for novel class discovery",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Zuo",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao",
                "Weihua Peng",
                "Yuguang Chen."
            ],
            "title": "Improving event causality identification via selfsupervised representation learning on external causal statement",
            "venue": "Findings of the Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "\u2022 BERTScore (Zhang"
            ],
            "title": "2020) computes the semantic similarity between generated names and ground-truth labels by using BERT to obtain contextual representations",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Automated real-world event detection is a crucial task towards mining fast-evolving event knowledge. Existing methods (Ji and Grishman, 2008; Chen et al., 2015; Du and Cardie, 2020; Wang et al., 2022) typically require a pre-defined event schema along with massive human-labeled data for model learning. Despite the tremendous success, manually defining an event schema is especially expensive and labor-intensive, which requires experts to\n\u2217Corresponding author. 1Code is available at https://github.com/CPF-NLPR/\nHALTON.\nexamine amounts of raw data in advance to specify potential event types. Besides, as new events are happening every day (Cao et al., 2020; Yu et al., 2021; Liu et al., 2022a), it is neither realistic nor scalable to define all event schemas in advance.\nTo get rid of the above problems, some researchers study the task of event type induction (ETI), which aims to discover new event types from an input corpus (Yuan et al., 2018; Huang and Ji, 2020; Shen et al., 2021). The task is generally formulated as a clustering problem, where each cluster represents an event type (cf. Figure 1(a)). Existing methods typically utilize probabilistic generative models (Chambers, 2013; Nguyen et al., 2015), adhoc clustering algorithms (Sekine, 2006; Huang et al., 2016) or neural networks (Huang and Ji, 2020; Shen et al., 2021; Li et al., 2022) to induce event clusters. Despite these successful efforts for clustering, the ETI setting inevitably suffers from two limitations in real applications:\nEvent types are not linked into the existing hierarchy: These methods only divide unlabeled event instances into several isolated clusters, without linking newly discovered types to an existing event ontology (i.e., an event hierarchy)2. Some studies about human cognition find that people tend to organize real-world events in a hierarchical way (Burt et al., 2003; Tenenbaum et al., 2011), ranging from coarse-grained (i.e., top-level) events to finegrained (i.e., bottom-level) events. Moreover, the ontologies of most knowledge bases also adopt hierarchical organization forms of event types (Baker et al., 1998; Kingsbury and Palmer, 2003). The hierarchical forms represent events at different granularity and abstraction levels, which helps people quickly understand related scenarios. For example, according to the event hierarchy in Figure 1(b), we can easily gain the overall picture of the Justice scenario, which may involve multiple events, such as Sue, Arrest and Sentence. Therefore, it is very necessary to establish and maintain the event hierarchy. However, since new events emerge rapidly and incessantly, it is impractical to manually add newly discovered types into the event ontology. Therefore, how to automatically expand the existing event hierarchy with new event types is an important problem.\nEvent types have no semantic names: Most ETI methods only assign numbers (i.e., type number) to the new event types, and lack the ability to generate human-readable type names. To enable new event types to be used in downstream tasks, it is inevitable to assign meaningful names for them in advance. For example, the event type name is required for training event extraction models (Li et al., 2021b) and constructing event knowledge graphs (Ma et al., 2022). Although the event type name is important, previous studies only focus on event clustering and ignore the type naming (Huang et al., 2016; Huang and Ji, 2020; Shen et al., 2021). As a result, the discovered event types cannot be directly applied to downstream applications, and extra human efforts are needed to conduct secondary labeling for the new types. Thus, how to automatically generate meaningful names for new event types is also a problem worth exploring.\nIn the light of the above restrictions, we propose a new task named Event Ontology Completion (EOC). Given a set of unlabeled event instances,\n2Event ontology denotes the hierarchical organization structure of known event types, which is usually incomplete.\nthe task requires that the model simultaneously achieves the following goals: (1) Event Clustering: dividing the unlabeled instances into several clusters; (2) Hierarchy Expansion: linking new event types (i.e., predicted clusters) into an existing event hierarchy; and (3) Type Naming: generating semantically meaningful names for new event types. As shown in Figure 1(b), the EOC model aims to divide the unlabeled instances into three clusters, and link the clusters to the Root and Justice node of the event hierarchy. Meanwhile, the three new event types are named Life, Sue and Arrest, respectively. Compared to ETI, EOC requires models to complete the event ontology, instead of only event clustering. Therefore, the proposed task is more useful and practical, but it is also more challenging.\nTo this end, we propose a novel method named HierarchicAL STructure EvOlution Network (HALTON) for this new task. Concretely, we first devise a Neighborhood Contrastive Clustering module for event clustering. The module utilizes a neighborhood contrastive loss to boost clustering for both supervised and unsupervised data. Intuitively, in a semantic feature space, neighboring instances should have a similar type, and pulling them together makes clusters more compact. Then, we propose a Hierarchy-Aware Linking module for hierarchy expansion. The module uses a dynamic path-based margin loss to integrate the hierarchical information into event representations. Compared with the static margin, the dynamic margin can capture the semantic similarities of event types in the hierarchy, which is conducive to hierarchy expansion. Finally, we design an In-Context Learningbased Naming module for type naming. The module elicits the abstraction ability of large language models (LLMs) via in-context learning to generate human-readable names for discovered event types. Extensive experiments on three datasets show that our proposed method brings significant improvements over baselines.\nTo summarize, our contributions are: (1) As a seminal study, we propose a new research task named event ontology completion, and introduce baselines and evaluation metrics for three task settings, including event clustering, hierarchy expansion and type naming. (2) We devise a novel method named Hierarchical Structure Evolution Network (HALTON), which achieves task goals via the collaboration of three components, namely neighborhood contrastive clustering, hierarchy-\naware linking and in-context learning-based naming. It can serve as a strong baseline for the research on the task. (3) Experimental results indicate that our method substantially outperforms baselines, achieving 8.23%, 8.79% and 8.10% improvements of ARI score on three datasets."
        },
        {
            "heading": "2 Task Formulation",
            "text": "The EOC task assumes that there is an incomplete event ontology T , which is constructed by experts in advance. The ontology is a tree-like structure, where leaf nodes denote known event types. Given an unlabeled dataset Du = {xui }Mi=1 and an estimated number of unknown types Mu, the goals of EOC include: (1) Event Clustering, dividing the unlabeled instances into Mu groups; (2) Hierarchy Expansion, linking each cluster C to the corresponding position of the hierarchy T ; and (3) Type Naming, generating a human-readable name for each cluster C. Following Li et al. (2022), we use golden triggers for event clustering3. To enable the model to achieve the above goals, we leverage a labeled dataset Dl = {(xli, yli)}Ni=1 to assist model learning. The types set of the labeled dataset is denoted as Y l. The event types in Y l belong to known types, which correspond to the leaf nodes of the event ontology T ."
        },
        {
            "heading": "3 Methodology",
            "text": "Figure 2 shows the overall architecture of HALTON, which consists of three major components: (1) Neighborhood Contrastive Clustering (\u00a73.1), which learns discriminative representations for event clustering; (2) Hierarchy-Aware Linking (\u00a73.2), which attaches newly discovered event types to the existing event hierarchy; and (3) In-Context Learning-based Naming (\u00a73.3), which generates event type names via in-context learning. We will illustrate each component in detail."
        },
        {
            "heading": "3.1 Neighborhood Contrastive Clustering",
            "text": "Encoding Instances Given the impressive performance of pre-trained language models on various NLP tasks (Sun et al., 2022; Zhao et al., 2023), we utilize BERT (Devlin et al., 2019) to encode input sentences. Since the trigger may contain multiple tokens, we conduct a max-pooling operation over\n3How to identify event triggers is not our focus in this paper. Actually, our method can be combined with any event detection model to extract event triggers.\nBERT outputs to obtain the event representation:\nh1, . . . ,hn = BERT(x) h = Max-Pooling(hs, . . . ,he),\n(1)\nwhere x denotes the input sentence. n is the length of the input sentence. s and e represent the start and end positions of the trigger, respectively.\nBase Losses In this way, we obtain the event representations of labeled and unlabeled instances, denoted as {hli}Ni=1 and {huj }Mj=1, respectively. We feed the representations of labeled instances into a softmax function for prediction, and utilize the cross-entropy loss to train the model:\nLce = \u2212 1\nN N\u2211 i=1 yli \u00b7 log(softmax(hli)), (2)\nwhere yli is a one-hot vector representing the golden label of the instance xli. For unlabeled instances, we use the K-means algorithm to obtain their pseudo labels:\ny\u0302u = K-means(hu) \u2208 {1, . . . ,Mu}. (3)\nSince the order of clusters often changes in multiple clustering, it is not readily to use cross-entropy loss for training the model on unlabeled instances. Instead, we compute pair-wise pseudo labels, according to the clustering result:\nqij = 1{y\u0302ui = y\u0302uj }, (4)\nwhere qij denotes whether xui and x u j belong to the same cluster. We input the representations of unlabeled instances into a classifier to obtain predicted distributions {pui }Mi=1. Intuitively, if a pair of instances output similar distributions, it can be assumed that they are from the same cluster. Therefore, we use the pair-wise Kullback-Leibler (KL) divergence to evaluate the distance between two unlabeled instances:\ndij = KL(pui ||puj ) + KL(puj ||pui ). (5)\nIf xui and x u j belong to different clusters, their predicted distributions are expected to be different. Thus, we modify standard binary cross-entropy loss by incorporating the hinge-loss function (Zhao et al., 2021):\nLbce = 1\nC2M \u2211 i,j (qijdij + (1\u2212 qij)max(0, \u03b1\u2212 dij)), (6)\nwhere \u03b1 is a hyper-parameter for the hinge loss. C2M denotes the number of combinations.\nNeighborhood Contrastive Loss Since contrastive learning is a very effective representation learning technique (He et al., 2020; Zhong et al., 2021; Zuo et al., 2021), we propose a neighborhood contrastive loss to learn more discriminative representations from both the labeled and unlabeled data. Concretely, for each instance xi, we select its top-K nearest neighbors in the embedding space to form a neighborhood Ni. The instances in Ni should share a similar type as xi, which are regarded as its positives. The neighborhood contrastive loss for unlabeled instances is defined as follows:\nLncu = \u2212 1\nM M\u2211 i=1 1 K \u2211 j\u2208Ni log exp(sim(hui ,h u j )/\u03c4)\u2211M k \u0338=i exp(sim(h u i ,h u k)/\u03c4) ,\n(7)\nwhere sim(\u00b7, \u00b7) is the similarity function (e.g., dot product). \u03c4 is the temperature scalar. For labeled instances, the positives set is expanded with the instances having the same event type. Thus, the neighborhood contrastive loss for labeled instances is written as follows:\nLncl = \u2212 1\nN N\u2211 i=1 1 |N li | \u2211 j\u2208N li log exp(sim(hli,h l j)/\u03c4)\u2211N k \u0338=i exp(sim(h l i,h l k)/\u03c4) .\n(8)\nwhere N li denotes the positives set for the labeled instance xli."
        },
        {
            "heading": "3.2 Hierarchy-Aware Linking",
            "text": "Dynamic Path-based Margin Loss To better accomplish the hierarchy expansion, we use the margin loss (Schroff et al., 2015; Liu et al., 2021) to integrate hierarchy information into event representations. To this end, we devise a dynamic pathbased margin loss. In detail, given two known event types yli and y l j , we randomly sample two instances\nfrom type yli, which serve as anchor instance a and positive instance p, respectively. We also randomly sample an instance from type ylj as negative instance n. The loss encourages a dynamic margin between the positive pair (a, p) and the negative pair (a, n), which is computed as follows:\nLdpm = \u2211\n(yli,y l j)\u2208S\nmax(0, sim(ha,hn)\n+ \u03b3(yli, y l j)\u2212 sim(ha,hp)),\n(9)\nwhere S denotes the set of the combination of any two known event types. To more accurately reflect the similarity between two types in a hierarchy, the margin \u03b3(yli, y l j) is computed based on the paths:\n\u03b3(yli, y l j) = |PATH(yli) \u222a PATH(ylj)| |PATH(yli) \u2229 PATH(ylj)| \u2212 1, (10)\nwhere PATH(yli) represents the set containing the nodes on the path from the root event to the type yli. If the intersection set of the two paths is smaller (i.e., less common super-classes), the margin will become larger. Therefore, compared with the static margin, the dynamic margin can capture the semantic similarities of event types in the hierarchy, which is effective for event clustering and hierarchy expansion. We reach the final loss function by combining the above terms:\nLf = Lce + Lbce + Lncu + Lncl + Ldpm. (11)\nGreedy Expansion Strategy After training the model using the final loss function, we can discover new event types and link them to the existing ontology via a greedy expansion algorithm (Zhang et al., 2021). Specifically, for each new event type (i.e., predicted cluster), starting from the root node,\nwe compute the similarity between the new event type and its children nodes. Then, we select the event type (i.e., node) with the highest similarity to repeat the above process. The search process terminates if the similarity does not increase compared to the previous layer. The similarity between the new event type and an existing event type is computed as follows:\nS(yn, ye) =\n\u2211 xu\u2208Pn \u2211 xv\u2208Pe sim(hu,hv) |Pn||Pe| , (12)\nwhere yn is a new event type (i.e., event cluster) and ye is an existing event type. Pn and Pe denote the sets of event instances belonging to yn and ye, respectively."
        },
        {
            "heading": "3.3 In-Context Learning-based Naming",
            "text": "To obtain a human-readable name for each predicted cluster, we propose an in-context learningbased naming technique, which elicits the naming ability of LLMs by providing a few demonstrative instances (Li et al., 2023). We first construct the prompt for LLMs. Figure 3 shows an example of the prompt, which includes three parts:\nTask Description is a short description of the task. We devise a simple and effective version, i.e., \u201cGenerate the type name according to the given text and event trigger. The generated name should be one clear and brief word.\u201d\nIn-Context Examples consist of the sentence, event trigger, path, question and answer. As shown in Figure 3, the starting point of the path is the root node of the hierarchy, and the ending point is the\nparent node of the type. The question is \u201cAccording to this, what is the type name?\u201d.\nIncomplete Entry is filled by LLMs, whose composition is similar to the in-context examples. Intuitively, if the text provides more relevant information about the event, the model will give more accurate predictions. Thus, we select the instance closest to the cluster centroid as the sentence. The path information is obtained via the hierarchy-aware linking module. As for the answer part, we leave it blank for LLMs to complete.\nThen, the constructed prompt is input into the LLMs (i.e., ChatGPT) for type name generation. This overall training and inference procedure is detailed in Appendix A."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "So far, there is no benchmark for evaluating EOC models. Based on three widely used event detection datasets, namely ACE (Doddington et al., 2004), ERE (Song et al., 2015), and MAVEN (Wang et al., 2020), we devise the following construction method: for the ACE dataset, we regard the top 10 most popular types are regarded as known types and the remaining 23 event types as unknown types. For the ERE dataset, we also set the top 10 most popular types as seen and the remaining 28 types as unseen. For the MAVEN dataset, we select the top 60 most frequent types to alleviate long-tail problem, where the top 20 most popular event types serve as known types and the remaining 40 types are regarded as unknown types. For the three datasets, the event hierarchy is a treelike structure constructed by known types. We list known and unknown types in Appendix B."
        },
        {
            "heading": "4.2 Event Clustering Evaluations",
            "text": "Baselines We compare our HALTON with the following methods: (1) SS-VQ-VAE (Huang and Ji, 2020) utilizes vector quantized variational autoencoder to learn discrete latent representations for seen and unseen types. (2) ETYPECLUS (Shen et al., 2021) jointly embeds and clusters predicateobject pairs in a latent space. (3) TABS (Li et al., 2022) designs a co-training framework that combines the advantage of type abstraction and tokenbased representations.\nEvaluation Metrics Following previous ETI works (Huang and Ji, 2020; Li et al., 2022), we\nadopt several standard metrics to evaluate event clustering results, including Adjusted Rand Index (ARI) (Hubert and Arabie, 1985), BCubed-F1 (Bagga and Baldwin, 1998), Normalized Mutual Information (NMI) and Accuracy. The detailed descriptions are in Appendix C.2.\nResults Table 1 shows the event clustering results on the three datasets, from which we can observe that our method HALTON outperforms all the baselines by a large margin, and achieves new stateof-the-art performance. For example, compared with the strong baseline TABS (Li et al., 2022),\nour method achieves 8.23%, 8.79% and 8.10% improvements of ARI score on the three datasets, respectively. The significant performance gain over the baselines demonstrates that the HALTON is very effective for event clustering. We attribute it to that our method can learn discriminative representations via the neighborhood contrastive loss."
        },
        {
            "heading": "4.3 Hierarchy Expansion Evaluations",
            "text": "Baselines Since the ETI methods cannot tackle the hierarchy expansion, we augment ETI baselines with the greedy expansion (GE) algorithm,\nnamely X+GE, where X is the ETI method. Besides, we also devise two representative baselines: (1) Type_Similarity, which links new types based on the similarity between representations of new types and known type names. (2) LLMs_Prompt, which devises prompts to leverage LLMs for linking. We describe more details in Appendix D.1.\nEvaluation Metrics To measure hierarchy expansion performance, we utilize the taxonomy metric (Dellschaft and Staab, 2006), which is originally proposed to evaluate taxonomy structure. For each cluster, the metric compares the predicted position and the golden position in the existing ontology. We report the taxonomy precision (Taxo_P), recall (Taxo_R) and F1-score (Taxo_F1). More detailed descriptions about the metric are in Appendix D.2.\nResults The hierarchy expansion results are shown in Table 2, with the following observations: (1) Our method HALTON has a great advantage over the baselines. For example, compared with the TABS+GE, our method achieves 12.07% improvements of Taxo_F1 with predicted clusters on the MAVEN dataset. Even given golden clusters (i.e., same clustering results), our method still outperforms the baselines. It indicates that the hierarchical information captured by the dynamic path-based margin loss can provide guidance for hierarchy expansion. (2) Our method outperforms Type_Similarity, which proves that the greedy expansion algorithm is effective. Besides, our method improves more significantly on the MAVEN dataset. We guess that hierarchical information is more useful for hierarchy expansion in more complex scenarios."
        },
        {
            "heading": "4.4 Type Naming Evaluations",
            "text": "Baselines We compare our method with the TABS model that uses the abstraction mechanism to generate type names. In addition, we also develop two competitive baselines: (1) T5_Template, which designs the template and uses T5 (Raffel et al., 2020) to fill it. (2) Trigger_Sel, which randomly selects a trigger from clusters as the type name. Appendix E.1 describes more details.\nEvaluation Metrics To our best knowledge, there is no evaluation metrics designed for event type name generation. We adopt two metrics: (1) Rouge-L (Lin, 2004), which measures the degree of matching between generated names and groundtruth names (i.e., hard matching). (2) BERTScore (Zhang et al., 2020), which computes the semantic similarity between generated name and the groundtruth (i.e., soft matching). The math formulas of Rouge-L and BERTScore are in Appendix E.2.\nResults We present the type naming results in Table 3. From the results, we can observe that our method HALTON significantly outperforms all the baselines on the three datasets. For example, compared with the second best-performing model Trigger_Sel, our method achieves 3.23%, 2.74% and 3.59% improvements of Rouge-L score on the three datasets, respectively. It indicates that our method can generate type names that are more similar to ground-truth names. The reason is that the proposed in-context learning-based naming technique can better elicit the abstraction abilities in LLMs for type naming."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "To demonstrate the effectiveness of each component, we conduct ablation studies on the ACE dataset, which is shown in Table 4. We observe that the performance drops significantly if we remove the neighborhood contrastive (NC) loss. It indicates the NC loss plays a key role in event clustering. Without the dynamic path-based margin\n(DPM) loss, the performance is also degraded, suggesting the hierarchical information can provide guidance for hierarchy expansion. In addition, the cross-entropy (CE) and binary cross-entropy (BCE) losses are also useful, which is conducive to training the model by using labeled and unlabeled data."
        },
        {
            "heading": "4.6 Visualization",
            "text": "Event Clustering To better understand our method, we visualize the features for event clustering using t-SNE (Van Der Maaten, 2014) on the ERE dataset. The results are shown in Figure 4. Although TABS can learn separated features to some degree, it divides the instances with red colors into two clusters. By contrast, our method can generate more discriminative representations, which proves the effectiveness of our method for event clustering.\nHierarchy Expansion To intuitively show the process of hierarchy expansion, we visualize the workflow of linking the new type Sentence to the existing event hierarchy via our method, as shown in Figure 5. As we can see, our method computes the similarity between the new type and known types in a top-down manner, and links the new event type to the correct position in the existing event ontology. In addition, the greedy expansion strategy provides better interpretability for the expansion process."
        },
        {
            "heading": "4.7 Case Study of Type Naming",
            "text": "Table 5 shows case studies, where our method and baselines generate event type names for the unlabeled instances. For the first example, the event trigger is similar to the golden type name. Our method and the baselines can produce type names that are semantically similar to golden names. For the second example, it is more challenging. All the baselines fail to generate correct type names. By contrast, our method successfully generates the type name that is almost identical to the ground truth. It demonstrates that the in-context learningbased naming module is very effective."
        },
        {
            "heading": "5 Related Work",
            "text": "Although event extraction has met with remarkable success (Ji and Grishman, 2008; Liu et al., 2018; Nguyen and Nguyen, 2019; Liu et al., 2020, 2022b; Cao et al., 2023), it usually requires that hand-crafted event schemas and annotations are given in advance. Since manually defining event schemas is labor-intensive and fails to generalize to new scenarios, some researchers have attempted to explore the ETI task (Chambers, 2013; Huang et al., 2016; Li et al., 2020, 2021a, 2022; Jin et al., 2022; Xu et al., 2023; Edwards and Ji, 2023). Typical approaches utilize probabilistic generative models (Chambers, 2013; Nguyen et al., 2015), adhoc clustering techniques (Chambers and Jurafsky, 2011) and neural networks (Huang and Ji, 2020; Shen et al., 2021) to induce event clusters. Yuan et al. (2018) study the event profiling task and utilizes a Bayesian generative model to obtain clusters. Shen et al. (2021) design an unsupervised method to generate salient event types by clus-\ntering predicate-object pairs. Recently, Li et al. (2022) propose a co-training framework to combine abstraction-based and token-based representations for the task.\nDespite these successful efforts, existing methods cannot link new event types to the existing ontology, and lack the ability to generate meaningful names for new event types."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we define a new event ontology completion task, aiming at simultaneously achieving event clustering, hierarchy expansion and type naming. Furthermore, we propose a hierarchical structure evolution network (HALTON), which achieves the goals via collaboration between neighborhood contrastive clustering, hierarchy-aware linking and in-context learning-based naming. Experimental results on three datasets show that our method brings significant improvements over baselines.\nLimitations\nIn this paper, the size of used datasets is relatively small and the datasets are most in the newswire genre. To facilitate further research on this task, constructing a large-scale and high-quality dataset is an important research problem. In addition, similar to the event type induction, the proposed event ontology completion task also requires labeled instances for training models and constructing the existing event ontology. The ultimate goal of the event ontology completion task is to automatically construct the event ontology structure from scratch. We plan to address the event ontology completion task in the unsupervised scenario."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank anonymous reviewers for their insightful comments and suggestions. This work is supported by the National Key Research and Development Program of China (No.2020AAA0106400), the National Natural Science Foundation of China (No.62176257, No.61976211), the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No.XDA27020100 ), the Youth Innovation Promotion Association CAS, and Yunnan Provincial Major Science and Technology Special Plan Projects (No.202202AD080004)."
        },
        {
            "heading": "A Training and Inference Procedure",
            "text": "Algorithm 1 The HALTON Method\nRequire: Labeled dataset Dl = {(xli, yli)} and unlabeled dataset Du = {xui } for training, another unlabeled instances Du\u0302 = {xu\u0302i } for inference, existing event ontology T , model parameters \u0398, learning rate \u03b7. Ensure: Optimized model parameters, and completed event hierarchy.\n1: for epoch\u2190 1 to L do 2: Compute the Lf on Dl and Du; 3: Optimize model parameters via gradient descent \u0398 = \u0398\u2212 \u03b7\u2207\u0398Lf ; 4: end for 5: Cluster unlabeled data Du\u0302 via trained model; 6: Link each cluster to T using the greedy expan-\nsion algorithm; 7: Generate type names via in-context learning-\nbased naming module."
        },
        {
            "heading": "B Known and Unknown Types",
            "text": "B.1 ACE Dataset\nThe known event types include: Trial-Hearing, Die, Transfer-Money, Injure, End-Position, Elect, Meet, Phone-Write, Transport, and Attack.\nThe unknown event types include: Merge-Org, Start-Org, Declare-Bankruptcy, End-Org, Pardon, Extradite, Execute, Fine, Sentence, Appeal, Convict, Sue, Release-Parole, Arrest-Jail, ChargeIndict, Acquit, Demonstrate, Start-Position, Nominate, Transfer-Ownership, Marry, Divorce, and Be-Born.\nB.2 ERE Dataset\nThe known event types include: Attack, TransportPerson, Transfer-Money, Contact, Die, Broadcast, Transfer-Ownership, Meet, End-Position, and Correspondence.\nThe unknown event types include: ArrestJail, Start-Position, Trial-Hearing, Elect, ChargeIndict, Artifact, Transaction, Demonstrate, Sentence, Marry, Convict, Transport-Artifact, Be-Born, Release-Parole, Injure, Sue, Pardon, Nominate, Execute, Start-Org, End-Org, Divorce, Acquit, Extradite, Merge-Org, Appeal, Fine, and DeclareBankruptcy.\nB.3 MAVEN Dataset The known event types include: Causation, Process_start, Attack, Hostile_encounter, Catastrophe, Motion, Competition, Killing, Process_end, Social_event, Conquering, Statement, Self_motion, Arriving, Destroying, Coming_to_be, Bodily_harm, Death, Creating, and Military_operation.\nThe unknown event types include: Damaging, Cause_change_of_strength, Cause_change_of_position_on_a_scale, Hold, Control, Earnings_and_losses, Getting, Becoming, Arranging, Know, Preventing_or_letting, Presence, Escaping, Defending, Action, Motion_directional, Cause_to_be_included, Change, Traveling, Placing, Participation, Influence, Change_of_leadership, Judgment_communication, Expressing_publicly, Name_conferral, Request, Giving, Supporting, Recording, Removing, Agree_or_refuse_to_act, Using, Supply, Communication, Reporting, Choosing, Sending, Bringing, and Departing."
        },
        {
            "heading": "C Baselines and Evaluation Metrics for Event Clustering",
            "text": "C.1 Baselines \u2022 SS-VQ-VAE (Huang and Ji, 2020) first uses the\nBERT to encode the event trigger, and then predicts the type by looking up a dictionary of discrete latent representations. It also utilizes a variational autoencoder to avoid overfitting problem.\n\u2022 ETYPECLUS (Shen et al., 2021) first selects salient predicates and object to represent events. Then, it leverages a dictionary to disambiguate predicate senses. Finally, it embeds and clusters the events in a latent spherical space.\n\u2022 TABS (Li et al., 2022) proposes an abstractionbased representation, which is complementary to the token-based representation of events. It devises a prompt to elicit semantic knowledge in pre-trained language models for clustering.\nC.2 Evaluation Metrics \u2022 ARI (Hubert and Arabie, 1985) measures the\nsimilarity between two cluster assignments. The number of pairs in the same (different) clusters is denoted as TP (TN). The ARI is computed as follows:\nARI = RI\u2212 E(RI)\nmax RI\u2212 E(RI) , RI =\nTP + TN\nNe ,\nwhere Ne is the total number of instances. E(RI) is the expectation of the RI.\n\u2022 NMI is the normalized mutual information score, which is calculated as follows:\nNMI = 2\u00d7 MI(Y ;C) H(Y ) + H(C) ,\nwhere Y and C denote the ground truth and predicted clusters, respectively. H(\u00b7) is the entropy function. MI(Y ;C) denotes the mutual information between Y and C.\n\u2022 BCubed (Bagga and Baldwin, 1998) averages the precision and recall of each instance. The B-Cubed precision is defined as follows:\nBCubed-P = 1\nNe Ne\u2211 i=1 |C(ei) \u2229 Y (ei)| |C(ei)| ,\nwhere Y (\u00b7) is the mapping function from an instance to its ground truth cluster. Similarly, we can compute the B-Cubed recall. The B-Cubed F1 is calculated by their harmonic average.\n\u2022 Accuracy estimates the quality of clustering by finding a permutation from predicted cluster labels to the ground-truth that gives the highest accuracy:\nAccuracy = max \u03c3\u2208Perm(k)\n1\nNe Ne\u2211 i=1 1(y\u2217i = \u03c3(yi)),\nwhere k is the number of clusters. Perm(k) denote all permutation functions."
        },
        {
            "heading": "D Baselines and Evaluation Metrics for Hierarchy Expansion",
            "text": "D.1 Baselines \u2022 Type_Similarity first computes the prototype for\nthe new type by averaging all instance representations belonging to the type. Then, it uses the BERT (Devlin et al., 2019) to encode known type names. Finally, it links the new type to the existing ontology based on the similarity between the prototype and known type representations.\n\u2022 LLMs_Prompt first devises a prompt, and then utilizes the LLMs (i.e., ChatGPT) to fill it. The prompt is defined as follows:\nThe existing event ontology consists of these event types, including T1, T2, ..., TN. Please link the\nnew event type to the correct position of the event ontology. The answer should be one of these existing event type names. The following is an example:\n\u2013 Trigger: trigger1, Sentence: s1 \u2013 Answer: one known type\nTrigger: trigger2, Sentence: s2, Answer: .\nD.2 Evaluation Metrics \u2022 Taxonomy metric (Dellschaft and Staab, 2006)\ncompares the predicted position of the clusters and the golden position in the hierarchy. The taxonomy precision (Taxo_P) is formulated as follows:\nTaxo_P = 1 |C| \u2211 t\u2208C |u(tp) \u2229 u(tg)| |u(tp)| ,\nwhere C are predicted clusters. tp and tg denote the predicted and golden positions of the event type t, respectively. u(tp) is the union of all the ancestors and itself of the predicted position tp. We can compute the recall (Taxo_R) in a similar way."
        },
        {
            "heading": "E Baselines and Evaluation Metrics for Type Naming",
            "text": "E.1 Baselines \u2022 T5_Template devises a template and utilizes T5\n(Raffel et al., 2020) to fill it. The template is defined as follows:\n\u27e8Context\u27e9. According to this, the trigger word of this [MASK] is \u27e8Trigger\u27e9. In the template, \u27e8Context\u27e9 represents the text that describes the event. \u27e8Trigger\u27e9 is a placeholder that is replaced by the actual trigger in the prototype instance. [MASK] is expected to be filled with the type name.\n\u2022 Trigger_Sel randomly selects an event trigger from clusters as the new type name.\nE.2 Evaluation Metrics \u2022 Rouge-L (Lin, 2004) measures the degree of\nmatching based on the longest common subsequence between generated names and golden type names, which can be computed as follows:\nPlcs = LCS(X,Y )\nn\nRlcs = LCS(X,Y )\nm\nFlcs = (1 + \u03b22)PlcsRlcs Rlcs + \u03b22Plcs ,\nwhere X is the golden type name, and Y denotes the generated name. m and n denote the length of X and Y , respectively. LCS(X,Y ) is the longest common subsequence between X and Y . \u03b2 is a hyper-parameter.\n\u2022 BERTScore (Zhang et al., 2020) computes the semantic similarity between generated names and ground-truth labels by using BERT to obtain contextual representations. The precision is formulated as follows:\nP = 1 |Y | \u2211 yi\u2208Y max xj\u2208X xTj yi,\nwhere X and Y denote the the ground-truth label and generated type names, respectively. yi is the embedding of i-th token in Y . After symmetrically calculating the recall, we can get the BERTScore F1 based on their harmonic average.\nFor the two evaluation metrics, the generated type names and golden type names are both composed of node names from the root to the leaf in the ontology tree."
        },
        {
            "heading": "F Augment Baselines with In-Context Learning-based Naming",
            "text": "We augment the event clustering baselines with the proposed in-context learning-based naming module. The results are shown in Table 6. From the table, we can observe that the three baselines with\nthe type naming technique can achieve better or comparable performance than the original TABS. It indicates that the proposed in-context learningbased naming module is very effective.\nG Visualization of Event Clustering\nIn section 4.6, we show the feature visualization of TABS and our method for event clustering. In this section, we present the visualization result of the ETYPECLUS, which is shown in Figure 6. From the result, we can see that the ETYPECLUS fails to distinguish the unlabeled instances. By contrast, our method can learn discriminative features, which proves the effectiveness of our method.\nH Implementation Details\nIn our implementations, our method uses the HuggingFace\u2019s Transformers library4 to implement the the uncased BERT base and T5 base models. The learning rate is initialized as 1e-4 with a linear decay. We utilize the Adam algorithm (Kingma and Ba, 2014) to optimize model parameters. The batch size is set to 128. The hyper-parameter for the hinge loss in BCE loss is set to 2. The number of neighbors K is set to 3. The number of training epochs is 100. Each experiment is conducted on NVIDIA RTX A6000 GPUs.\n4https://github.com/huggingface/transformers"
        }
    ],
    "title": "Event Ontology Completion with Hierarchical Structure Evolution Networks",
    "year": 2023
}