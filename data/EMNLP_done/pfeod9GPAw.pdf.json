{
    "abstractText": "Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. This paper first presents a thorough evaluation of ChatGPT\u2019s performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-thengenerate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT\u2019s capabilities in faithful summarization using two-stage approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haopeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiao Liu"
        },
        {
            "affiliations": [],
            "name": "Jiawei Zhang"
        }
    ],
    "id": "SP:4ec39c0012f43856a5108698868949cff40f8f6d",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jianpeng Cheng",
                "Mirella Lapata."
            ],
            "title": "Neural summarization by extracting sentences and words",
            "venue": "arXiv preprint arXiv:1603.07252.",
            "year": 2016
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "arXiv preprint arXiv:1804.05685.",
            "year": 2018
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Pengfei Liu",
                "Hiroaki Hayashi",
                "Zhengbao Jiang",
                "Graham Neubig."
            ],
            "title": "Gsum: A general framework for guided neural abstractive summarization",
            "venue": "arXiv preprint arXiv:2010.08014.",
            "year": 2020
        },
        {
            "authors": [
                "Tanya Goyal",
                "Junyi Jessy Li",
                "Greg Durrett."
            ],
            "title": "News summarization and evaluation in the era of gpt-3",
            "venue": "arXiv preprint arXiv:2209.12356.",
            "year": 2022
        },
        {
            "authors": [
                "Som Gupta",
                "Sanjai Kumar Gupta."
            ],
            "title": "Abstractive summarization: An overview of the state of the art",
            "venue": "Expert Systems with Applications, 121:49\u201365.",
            "year": 2019
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tomas Kocisky",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in neural information processing systems, pages 1693\u20131701.",
            "year": 2015
        },
        {
            "authors": [
                "Byeongchang Kim",
                "Hyunwoo Kim",
                "Gunhee Kim."
            ],
            "title": "Abstractive summarization of reddit posts with multi-level memory networks",
            "venue": "arXiv preprint arXiv:1811.00783.",
            "year": 2018
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "arXiv preprint arXiv:1910.12840.",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Eduard Hovy"
            ],
            "title": "Automatic evaluation of summaries using n-gram co-occurrence",
            "year": 2003
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "arXiv preprint arXiv:1908.08345.",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu",
                "Dragomir Radev",
                "Graham Neubig."
            ],
            "title": "BRIO: Bringing order to abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890\u20132903,",
            "year": 2022
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou."
            ],
            "title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "venue": "arXiv preprint arXiv:2303.15621.",
            "year": 2023
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "Thirty-first AAAI conference on artificial intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Caglar Gulcehre",
                "Bing Xiang"
            ],
            "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
            "venue": "arXiv preprint arXiv:1602.06023",
            "year": 2016
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "arXiv preprint arXiv:1808.08745.",
            "year": 2018
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language",
            "year": 2021
        },
        {
            "authors": [
                "Mathieu Ravaut",
                "Shafiq Joty",
                "Nancy F Chen."
            ],
            "title": "Summareranker: A multi-task mixture-of-experts reranking framework for abstractive summarization",
            "venue": "arXiv preprint arXiv:2203.06569.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Gallinari Patrick",
                "Lamprier Sylvain",
                "Piwowarski Benjamin",
                "Staiano Jacopo",
                "Wang Alex."
            ],
            "title": "Questeval: Summarization asks for fact-based evaluation",
            "venue": "arXiv preprint arXiv:2103.12693.",
            "year": 2021
        },
        {
            "authors": [
                "Danqing Wang",
                "Pengfei Liu",
                "Yining Zheng",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Heterogeneous graph neural networks for extractive document summarization",
            "venue": "arXiv preprint arXiv:2004.12393.",
            "year": 2020
        },
        {
            "authors": [
                "Fei Wang",
                "Kaiqiang Song",
                "Hongming Zhang",
                "Lifeng Jin",
                "Sangwoo Cho",
                "Wenlin Yao",
                "Xiaoyang Wang",
                "Muhao Chen",
                "Dong Yu."
            ],
            "title": "Salience allocation as guidance for abstractive summarization",
            "venue": "arXiv preprint arXiv:2210.12330.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Xu",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Discourse-aware neural extractive model for text summarization",
            "venue": "arXiv preprint arXiv:1910.14142.",
            "year": 2019
        },
        {
            "authors": [
                "Xianjun Yang",
                "Yan Li",
                "Xinlu Zhang",
                "Haifeng Chen",
                "Wei Cheng."
            ],
            "title": "Exploring the limits of chatgpt for query or aspect-based text summarization",
            "venue": "arXiv preprint arXiv:2302.08081.",
            "year": 2023
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang."
            ],
            "title": "HEGEL: Hypergraph transformer for long document summarization",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10167\u201310176, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang."
            ],
            "title": "Contrastive hierarchical discourse graph for scientific document summarization",
            "venue": "Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023), pages 37\u201347, Toronto, Canada.",
            "year": 2023
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang."
            ],
            "title": "DiffuSum: Generation enhanced extractive summarization with diffusion",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 13089\u201313100, Toronto, Canada. Association",
            "year": 2023
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang"
            ],
            "title": "2023c. Summit: Iterative text summarization via chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Semih Yavuz",
                "Wojciech Kryscinski",
                "Kazuma Hashimoto",
                "Yingbo Zhou."
            ],
            "title": "Improving the faithfulness of abstractive summarization via entity coverage control",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Faisal Ladhak",
                "Esin Durmus",
                "Percy Liang",
                "Kathleen McKeown",
                "Tatsunori B Hashimoto."
            ],
            "title": "Benchmarking large language models for news summarization",
            "venue": "arXiv preprint arXiv:2301.13848.",
            "year": 2023
        },
        {
            "authors": [
                "Ming Zhong",
                "Pengfei Liu",
                "Yiran Chen",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Extractive summarization as text matching",
            "venue": "arXiv preprint arXiv:2004.08795.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Document summarization aims to compress text material while retaining its most salient information. With the increasing amount of publicly available text data, automatic summarization approaches have become increasingly important. These approaches can be broadly classified into two categories: abstractive and extractive summarization. While abstractive methods (Nallapati et al., 2016; Gupta and Gupta, 2019) have the advantage of producing flexible and less redundant summaries, they often struggle with generating ungrammatical or even nonfactual contents (Krys\u0301cin\u0301ski et al., 2019; Zhang et al., 2022b). In contrast,\nextractive summarization directly selects sentences from the source document to form the summary, resulting in summaries that are grammatically correct and faithful to the original text.\nThe growing interest in applying advanced large language models (LLM) such as ChatGPT 1 for text summarization tasks has sparked significant attention. A recent study by (Goyal et al., 2022) compared GPT-3 with traditional fine-tuning methods and found that, despite lower Rouge scores, human annotators preferred the GPT-3 generated text. Another study by (Zhang et al., 2023d) conducted a comprehensive analysis of large language models for news summarization and found that the generated summaries were comparable to those produced by humans. However, existing research (Yang et al., 2023; Luo et al., 2023) has only focused on abstractive summary approaches, and the performance of ChatGPT for extractive summarization remains an open question. Moreover, the hallucination problem has dramatically hindered the practical use of abstractive summarization systems, highlighting the need to explore extractive summarization with LLMs for faithful summaries.\nIn this study, we comprehensively evaluate ChatGPT\u2019s performance on extractive summarization and investigate the effectiveness of in-context learning and chain-of-thought explanation approaches. Our experimental analysis demonstrates that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics. Additionally, we observe that using an extract-then-generate pipeline with ChatGPT yields large performance improvements over abstractive baselines in terms of summary faithfulness.\nThe main contributions of this paper are: 1) This study represents the first attempt to extend the ap-\n1https://chat.openai.com/chat\nplication of ChatGPT to extractive summarization and evaluate its performance. 2) We investigate the effectiveness of in-context learning and chain-ofthought reasoning approaches for extractive summarization using ChatGPT. 3) We further extend the extraction step to abstractive summarization and find that the extract-then-generate framework could improve the generated summary faithfulness by a large margin compared to abstractive-only baselines without hurting summary qualities."
        },
        {
            "heading": "2 Related Work",
            "text": "Most extractive summarization works formulate the task as a sequence classification problem and use sequential neural models with diverse encoders such as recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2016) and pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2023b). Another group of works formulated extractive summarization as a node classification problem and applied graph neural networks to model inter-sentence dependencies (Xu et al., 2019; Wang et al., 2020; Zhang et al., 2022a, 2023a).\nSeveral studies also explored the use of large language models (Brown et al., 2020) for summarization. Goyal et al. (2022) found that while the former obtained slightly lower Rouge scores, human evaluators preferred them. Likewise, Zhang et al. (2023d) reported that large language modelgenerated summaries were on par with humanwritten summaries in the news domain. In addition, Yang et al. (2023) explored the limits of ChatGPT on query-based summarization other than generic summarization. Luo et al. (2023) explored the use of ChatGPT as a factual inconsistency evaluator for abstractive text summarization. Zhang et al. (2023c) proposed a self-evaluation and revisement framework with ChatGPT. While most of the existing research has focused on abstractive summarization, this work aims to investigate the applicability of ChatGPT to extractive summarization and examine whether extractive methods could enhance abstractive summarization faithfulness."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "Extractive summarization systems form a summary by identifying and concatenating the most salient sentences from a given document. These approaches have gained widespread traction in various real-world applications owing to their ability\nto produce accurate and trustworthy summaries devoid of grammatical inconsistencies.\nFormally, given a document d consisting of n sentences, the goal of an extractive summarization system is to produce a summary s comprising of m(m \u226a n) sentences, by directly extracting relevant sentences from the source document. Most existing work formulates it as a sequence labeling problem, where the sentences are selected by model M based on the probability of whether it should be included in the summary s:\ns\u0302 = argmax s\npM (s | d). (1)\nIn the training of supervised summarization models, it is common to employ a greedy algorithm, as described in (Nallapati et al., 2017), to generate extractive ground-truth labels (ORACLE) by selecting multiple sentences that maximize the ROUGE score compared to the gold summary."
        },
        {
            "heading": "3.2 In-context Learning",
            "text": "Recent studies have shown that large language models have strong few-shot performance on various downstream tasks, known as in-context learning (ICL) (Brown et al., 2020). The standard ICL prompts a large language model, M , with a set of k exemplar document-summary pairs and predicts a summary s\u0302 for the document by:\ns\u0302 = argmax s pM (s | d, {(d1, s1)...(dk, sk)}). (2)\nBesides simple input-output pairs, previous works also show that including explanations and chain-of-thought (COT) reasoning in prompts (Nye et al., 2021; Wei et al., 2022) also benefits language models, represented as:\ns\u0302 = argmax s\npM (s | d,C), (3)\nwhere C = {(d1, e1, s1)...(dk, ek, sk)} is the set of input-explanation-output triplets in prompts. Besides zero-shot setting, this study also investigates the impact of in-context learning on extractive summarization, with and without explanations."
        },
        {
            "heading": "3.3 Extract-abstract Summarization",
            "text": "It is not new to use extractive summaries to guide abstractive summary generations (Dou et al., 2020; Wang et al., 2022). Here we also propose to use LLM in a two-stage manner: extract salient sentences to form extractive summaries (sE) first, and\nthen ask the LLM to generate summaries guided by the extractive summaries, represented as:\np(s | d) = T\u220f t=1 p ( st | s<t, d, sE ) , (4)\nwhere s<t denotes the previous generated tokens before step t. We explore the extract-then-generate pipeline in this study, aiming to alleviate the hallucination problems in LLM summary generation."
        },
        {
            "heading": "4 Experiments and Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Settings",
            "text": "Datasets: We chose four publicly available benchmark datasets as listed in Table 2, ensuring that they are consistent with previous fine-tuning approaches. CNN/DailyMail (Hermann et al., 2015) is the most widely-adopted summarization dataset that contains news articles and corresponding highlights as summaries. We use the non-anonymized version and follow the common training/validation/testing splits (287,084/13,367/11,489). XSum (Narayan\net al., 2018) is a one-sentence news summarization dataset with professionally written summaries. We follow the common splits (204,045/11,332/11,334). PubMed (Cohan et al., 2018) is a scientific paper summarization dataset and we use the introduction section as the article and the abstract section as the summary following (Zhong et al., 2020) with common splits (83,233/4,946/5,025). Reddit (Kim et al., 2018) is a highly abstractive dataset collected from social media platforms with a split (41,675/645/645). Evaluation: We conducted an evaluation of ChatGPT\u2019s summarization performance utilizing ROUGE (Lin and Hovy, 2003) following previous studies. We also employ a GPT-based evaluation metric G-EVAL (Liu et al., 2023). To investigate the faithfulness of the summaries, we employed common metrics FactCC (Krys\u0301cin\u0301ski et al., 2019) and QuestEval (Scialom et al., 2021).\nWe selected the best prompts on a dev set of 50 examples and randomly sampled 1000 examples from each test set of the original dataset for evaluation. The detailed prompts used in the experiments and more details about the experimental setup can be found in Table 4 and Appendix B."
        },
        {
            "heading": "4.2 Experiments Results",
            "text": "The overall results are shown in Table 1. The upper block includes extractive results and SOTA scores from MatchSum (Zhong et al., 2020). The lower block includes abstractive results and SOTA scores from BRIO (Liu et al., 2022) for CNN/DM and XSum, SummaReranker (Ravaut et al., 2022) for\nReddit, and GSum (Dou et al., 2020) for PubMed.\nIt is observed that ChatGPT generally achieves lower ROUGE scores in comparison to previous fine-tuning methods for all datasets under both extractive and abstractive settings, but achieves higher scores in terms of LLM-based evaluation metric G-EVAL. The findings are consistent with the previous conclusion in (Goyal et al., 2022; Zhang et al., 2023d). We also observe that ChatGPT-Ext outperforms ChatGPT-Abs in two extractive datasets CNN/DM and PubMed while performing worse in the other two abstractive datasets. We argue the results are due to the bias within the reference summaries of the dataset and the limit of ROUGE scores. Nonetheless, we notice that despite being primarily designed for generation tasks, ChatGPT achieves impressive results in extractive summarization, which requires comprehension of the documents. The decoder-only structure of ChatGPT doesn\u2019t degrade its comprehension capability compared to encoder models like BERT. We also find that the ROUGE score gap between ChatGPT and SOTA fine-tuned baselines are smaller in the extractive setting than in the abstractive setting.\nThe results also indicate that in-context learning and reasoning are generally beneficial for the extractive summarization task across four datasets in different domains. We only observe performance degradation for in-context learning on the XSum dataset. We argue that the degradation comes from the short ORACLE of XSum, which brings more confusion with a few ORACLE examples. However, with chain-of-thought reasoning explanations, ChatGPT can better understand the pattern and thus shows improvements with in-context reasoning. More in-context learning results could be found\nin Table 5 in Appendix."
        },
        {
            "heading": "4.3 Extract Then Generate",
            "text": "We conduct further experiments to examine the effectiveness of the extract-then-generate framework as presented in Table 3.\nThe results show large improvements in summary factual consistency across all four datasets with the extract-then-generate framework. Notably, the FactCC scores are extremely low for generateonly baselines (less than 10 percent), highlighting the hallucination problems of ChatGPT-based summarization, where ChatGPT tends to make up new content in the summary. Nevertheless, the extractthen-generate framework effectively alleviates the hallucination problem of abstractive summaries by guiding the summary generation process with extracted salient sentences from the documents. We also find that guiding ChatGPT summary generation with its own extracted summaries leads to similar summary faithfulness improvements compared to guiding generation with ORACLE.\nIn terms of summary quality, the results demonstrate that the performance of ChatGPT improves largely in terms of ROUGE scores when grounded with the ORACLE summaries. However, the ROUGE score performance of the extract-thengenerate framework relies heavily on the extractive performance when grounded with its own extractive summaries. In summary, the extract-thengenerate framework could effectively improve the summary faithfulness with similar or even better summary quality."
        },
        {
            "heading": "4.4 Positional Bias",
            "text": "Lead bias is a common phenomenon in extractive summarization, especially in the news domain, where early parts of an article often contain the most salient information. As shown in Figure 1, we find that the position distribution of the ChatGPT extracted summary sentences is skewed towards a higher position bias than the ORACLE sentences. In addition, in-context learning brings more positional bias to the summaries. The results indicate that LLMs may rely on superficial features like sentence positions for extractive summarization."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper presents a thorough evaluation of ChatGPT\u2019s performance on extractive summarization across four benchmark datasets. The results indicate ChatGPT\u2019s strong potential for the task and the possibility of generating factual summaries using the extract-generate framework. Overall, this study suggests that ChatGPT is a powerful tool for text summarization, and we hope the insights gained from this work can guide future research in this area.\nLimitations\nInstead of conducting experiments on the entire test set, we randomly sample 1000 examples from each dataset test set due to budget limits. Previous research efforts (Goyal et al., 2022; Zhang et al., 2023d) have also been limited in their testing of GPT-3 on a small number of instances.\nOur experimental results are mainly evaluated with various automatic metrics (summary quality and faithfulness). We plan to include a human study to further verify the conclusions in the future.\nWe only use gpt-3.5-turbo model from openAI API as an instance of large language models. The\nemphasis of the paper is to explore extractive summarization and extract-then-generate pipeline with LLM, but not compare different open and closed LLMs."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is partially supported by NSF through grants IIS-1763365 and IIS-2106972.\nWe express our gratitude to the anonymous reviewers for their valuable reviews and feedback."
        },
        {
            "heading": "A Prompts",
            "text": "Here we list prompts used in our experiments for extracted and generated summaries in Table 4. Note that according to OpenAI\u2019s document, the model could receive two categories of prompts: system prompt and user prompt, where the system prompt functions as the global instruction to initialize the model and the user prompt as the question proposed by users."
        },
        {
            "heading": "B Experimental Setup",
            "text": "We employed the gpt-3.5-turbo model2 for the generation and assessment of summaries, maintaining a temperature setting of 0 to ensure reproducibility.\nRegarding the datasets, a random sampling method was adopted where 1000 samples were chosen for each dataset for experimental purposes. Furthermore, a smaller subset of 50 samples was utilized for the discovery of optimal prompts and hyperparameters. The random seed was established at 101 to promote consistency.\nIn accordance with established research, the ROUGE3 F-1 scores were implemented as the automatic evaluation metrics (Lin and Hovy, 2003). To be specific, the ROUGE-1/2 scores serve as measures of summary informativeness, while the ROUGE-L score gauges the fluency of the summary. In addition to these measures, a GPT model was integrated as a summary evaluator to mimic human evaluation processes. This evaluator was designed to assess the summary based on a comprehensive analysis of coherence, consistency, fluency, and efficiency. The findings from each experiment are reported as single-run results.\nThe experiments involving each dataset, which includes 1000 examples, will run for 1.5 hours to perform both inference and evaluation.\nC In-context Learning Results\nThe detailed in-context learning results are shown in Table 5"
        },
        {
            "heading": "D Document length Analysis",
            "text": "We further investigate the influence of document length on the summarization performance, as pre-\n2https://platform.openai.com/docs/guides/gpt/chatcompletions-api\n3ROUGE: https://pypi.org/project/rouge-score/\nsented in Figure 2. Our findings suggest that ChatGPT maintains consistent performance across documents of different lengths, indicating the model\u2019s robustness in the context of extractive summarization."
        },
        {
            "heading": "E Case Study",
            "text": "Here we show the ChatGPT-generated summaries with different prompt settings in Table 6 for one example from the CNNDM dataset."
        }
    ],
    "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation",
    "year": 2023
}