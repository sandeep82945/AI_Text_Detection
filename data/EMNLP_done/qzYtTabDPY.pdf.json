{
    "abstractText": "Zipf (1935) posited that wordforms are optimized to minimize utterances\u2019 communicative costs. Under the assumption that cost is given by an utterance\u2019s length, he supported this claim by showing that words\u2019 lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance\u2019s information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word\u2019s length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.\u2019s derivation does not minimize CCH\u2019s cost, but rather a lower bound, which we term CCH\u2193. We propose a novel derivation, suggesting an improved way to minimize CCH\u2019s cost. Under this method, we find that a language\u2019s word lengths should instead be proportional to the surprisal\u2019s expectation plus its variance-tomean ratio. Experimentally, we compare these three communicative cost functions: Zipf\u2019s, CCH\u2193, and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal\u2019s expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf\u2019s longstanding hypothesis holds. https://github.com/tpimentelms/ optimality-of-word-lengths",
    "authors": [
        {
            "affiliations": [],
            "name": "Tiago Pimentel"
        },
        {
            "affiliations": [],
            "name": "Clara Meister"
        },
        {
            "affiliations": [],
            "name": "Ethan Gotlieb Wilcox"
        },
        {
            "affiliations": [],
            "name": "Kyle Mahowald"
        },
        {
            "affiliations": [],
            "name": "Ryan Cotterell"
        }
    ],
    "id": "SP:6f9015ea38e852d15174f56f29348c3c8d6bf83f",
    "references": [
        {
            "authors": [
                "Alan Bell",
                "Daniel Jurafsky",
                "Eric Fosler-Lussier",
                "Cynthia Girand",
                "Michelle Gregory",
                "Daniel Gildea."
            ],
            "title": "Effects of disfluencies, predictability, and utterance position on word form variation in English conversation",
            "venue": "The Journal of the Acoustical Society",
            "year": 2003
        },
        {
            "authors": [
                "Christian Bentz",
                "Ramon Ferrer-i-Cancho."
            ],
            "title": "Zipf\u2019s law of abbreviation as a language universal",
            "venue": "Proceedings of the Leiden Workshop on Capturing Phylogenetic Algorithms for Linguistics. Universit\u00e4t T\u00fcbingen.",
            "year": 2016
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "An interview on minimalism",
            "venue": "Cambridge University Press.",
            "year": 2002
        },
        {
            "authors": [
                "Uriel Cohen Priva."
            ],
            "title": "Informativity affects consonant duration and deletion rates",
            "venue": "Laboratory Phonology, 6(2):243\u2013278.",
            "year": 2015
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Joy A. Thomas."
            ],
            "title": "Elements of Information Theory",
            "venue": "John Wiley & Sons, Ltd.",
            "year": 2005
        },
        {
            "authors": [
                "Huteng Dai",
                "Richard Futrell."
            ],
            "title": "Simple induction of (deterministic) probabilistic finite-state automata for phonotactics by stochastic gradient descent",
            "venue": "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology,",
            "year": 2021
        },
        {
            "authors": [
                "August Fenk",
                "Gertraud Fenk"
            ],
            "title": "Konstanz im Kurzzeitged\u00e4chtnis - Konstanz im sprachlichen Informationsflu\u00df? Zeitschrift f\u00fcr Experimentelle und Angewandte Psychologie, 27(3):400\u2013414",
            "year": 1980
        },
        {
            "authors": [
                "Edward Gibson",
                "Richard Futrell",
                "Steven T. Piantadosi",
                "Isabelle Dautriche",
                "Kyle Mahowald",
                "Leon Bergen",
                "Roger Levy."
            ],
            "title": "How efficiency shapes human language",
            "venue": "Trends in Cognitive Sciences, 23(5):389\u2013 407.",
            "year": 2019
        },
        {
            "authors": [
                "Kyle Gorman."
            ],
            "title": "Generative Phonotactics",
            "venue": "Ph.D. thesis, University of Pennsylvania.",
            "year": 2013
        },
        {
            "authors": [
                "Peter Grzybek."
            ],
            "title": "Word length",
            "venue": "The Oxford Handbook of the Word, pages 89\u2013119. Oxford University Press.",
            "year": 2015
        },
        {
            "authors": [
                "Mandy Guo",
                "Zihang Dai",
                "Denny Vrande\u010di\u0107",
                "Rami Al-Rfou."
            ],
            "title": "Wiki-40B: Multilingual language model dataset",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 2440\u2013 2452, Marseille, France. European Language Re-",
            "year": 2020
        },
        {
            "authors": [
                "Bruce Hayes",
                "Colin Wilson."
            ],
            "title": "A maximum entropy model of phonotactics and phonotactic learning",
            "venue": "Linguistic Inquiry, 39(3):379\u2013440.",
            "year": 2008
        },
        {
            "authors": [
                "David A. Huffman."
            ],
            "title": "A method for the construction of minimum-redundancy codes",
            "venue": "Proceedings of the IRE, 40(9):1098\u20131101.",
            "year": 1952
        },
        {
            "authors": [
                "Jasmeen Kanwal."
            ],
            "title": "Word length and the principle of least effort: language as an evolving, efficient code for information transfer",
            "venue": "Ph.D. thesis, The University of Edinburgh, Edinburgh, UK.",
            "year": 2018
        },
        {
            "authors": [
                "Jasmeen Kanwal",
                "Kenny Smith",
                "Jennifer Culbertson",
                "Simon Kirby."
            ],
            "title": "Zipf\u2019s law of abbreviation and the principle of least effort: Language users optimise a miniature lexicon for efficient communication",
            "venue": "Cognition, 165:45\u201352.",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2015
        },
        {
            "authors": [
                "Alexander Koplenig",
                "Marc Kupietz",
                "Sascha Wolfer."
            ],
            "title": "Testing the relationship between word length, frequency, and predictability based on the German reference corpus",
            "venue": "Cognitive Science, 46(6):e13090.",
            "year": 2022
        },
        {
            "authors": [
                "Taku Kudo."
            ],
            "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375,",
            "year": 2018
        },
        {
            "authors": [
                "Natalia Levshina."
            ],
            "title": "Frequency, informativity and word length: Insights from typologically diverse corpora",
            "venue": "Entropy, 24(2).",
            "year": 2022
        },
        {
            "authors": [
                "Roger P. Levy",
                "Tim Florian Jaeger."
            ],
            "title": "Speakers optimize information density through syntactic reduction",
            "venue": "Advances in Neural Information Processing Systems, pages 849\u2013856.",
            "year": 2007
        },
        {
            "authors": [
                "T. Linder",
                "V. Tarokh",
                "K. Zeger."
            ],
            "title": "Existence of optimal prefix codes for infinite source alphabets",
            "venue": "IEEE Transactions on Information Theory, 43(6):2026\u20132028.",
            "year": 1997
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Patrick Haller",
                "Lena J\u00e4ger",
                "Ryan Cotterell",
                "Roger Levy."
            ],
            "title": "Revisiting the uniform information density hypothesis",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 963\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Stephan C. Meylan",
                "Thomas L. Griffiths."
            ],
            "title": "The challenges of large-scale, web-based language datasets: Word length and predictability revisited",
            "venue": "Cognitive Science, 45(6):e12983.",
            "year": 2021
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Sonia Petrini",
                "Antoni Casas-i-Mu\u00f1oz",
                "Jordi Cluet-iMartinell",
                "Mengxue Wang",
                "Christian Bentz",
                "Ramon Ferrer-i-Cancho."
            ],
            "title": "The optimality of word lengths",
            "venue": "Theoretical foundations and an empirical study. arXiv preprint arXiv:2208.10384.",
            "year": 2022
        },
        {
            "authors": [
                "Sonia Petrini",
                "Antoni Casas-i-Mu\u00f1oz",
                "Jordi Cluet-iMartinell",
                "Mengxue Wang",
                "Christian Bentz",
                "Ramon Ferrer-i-Cancho."
            ],
            "title": "Direct and indirect evidence of compression of word lengths",
            "venue": "Zipf\u2019s law of abbreviation revisited. arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Steven T. Piantadosi",
                "Harry Tily",
                "Edward Gibson."
            ],
            "title": "Word lengths are optimized for efficient communication",
            "venue": "Proceedings of the National Academy of Sciences, 108(9):3526\u20133529.",
            "year": 2011
        },
        {
            "authors": [
                "Steven T. Piantadosi",
                "Harry Tily",
                "Edward Gibson."
            ],
            "title": "The communicative function of ambiguity in language",
            "venue": "Cognition, 122(3):280\u2013291.",
            "year": 2012
        },
        {
            "authors": [
                "Steven T. Piantadosi",
                "Harry J. Tily",
                "Edward Gibson."
            ],
            "title": "The communicative lexicon hypothesis",
            "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society, volume 31, pages 2582\u20132587.",
            "year": 2009
        },
        {
            "authors": [
                "Tiago Pimentel",
                "Rowan Hall Maudslay",
                "Damian Blasi",
                "Ryan Cotterell."
            ],
            "title": "Speakers fill lexical semantic gaps with context",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4004\u20134015,",
            "year": 2020
        },
        {
            "authors": [
                "Tiago Pimentel",
                "Clara Meister",
                "Elizabeth Salesky",
                "Simone Teufel",
                "Dami\u00e1n Blasi",
                "Ryan Cotterell."
            ],
            "title": "A surprisal\u2013duration trade-off across and within the world\u2019s languages",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Tiago Pimentel",
                "Clara Meister",
                "Simone Teufel",
                "Ryan Cotterell."
            ],
            "title": "On homophony and r\u00e9nyi entropy",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8284\u20138293, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Tiago Pimentel",
                "Irene Nikkarinen",
                "Kyle Mahowald",
                "Ryan Cotterell",
                "Dami\u00e1n Blasi"
            ],
            "title": "2021c. How (non)optimal is the lexicon",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Claude E. Shannon."
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell System Technical Journal, 27(3):379\u2013423.",
            "year": 1948
        },
        {
            "authors": [
                "Bengt Sigurd",
                "Mats Eeg-Olofsson",
                "Joost Van Weijer."
            ],
            "title": "Word length, sentence length and frequency \u2013 Zipf revisited",
            "venue": "Studia Linguistica, 58(1):37\u201352.",
            "year": 2004
        },
        {
            "authors": [
                "Sean Trott",
                "Benjamin Bergen"
            ],
            "title": "Why do human languages have homophones? Cognition, 205:104449",
            "year": 2020
        },
        {
            "authors": [
                "Sean Trott",
                "Benjamin Bergen"
            ],
            "title": "Languages are efficient, but for whom? Cognition, 225:105094",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Gejza Wimmer",
                "Reinhard K\u00f6hler",
                "R\u00fcdiger Grotjahn",
                "Gabriel Altmann."
            ],
            "title": "Towards a theory of word length distribution",
            "venue": "Journal of Quantitative Linguistics, 1(1):98\u2013106.",
            "year": 1994
        },
        {
            "authors": [
                "George K. Zipf."
            ],
            "title": "The Psychobiology of Language",
            "venue": "London: Routledge.",
            "year": 1935
        },
        {
            "authors": [
                "George K. Zipf."
            ],
            "title": "Human Behavior and the Principle of Least Effort",
            "venue": "Addison-Wesley Press.",
            "year": 1949
        }
    ],
    "sections": [
        {
            "text": "https://github.com/tpimentelms/ optimality-of-word-lengths"
        },
        {
            "heading": "1 Introduction",
            "text": "Zipf proposed the idea that languages are optimized to minimize their expected utterance length (Zipf, 1935).1 Under this hypothesis, a word\u2019s length should be inversely proportional to its frequency. Indeed, this relationship has been attested across a wide variety of the world\u2019s languages (Grzybek, 2015; Bentz and Ferrer-i-Cancho, 2016, inter alia).\n1We will refer to this hypothesis as ZIPF.\nIn subsequent work, Piantadosi et al. (2011) offered a complementary account of communicative cost. Starting from the hypothesis that information rate should be roughly constant during communication (UID; Fenk and Fenk, 1980; Levy and Jaeger, 2007), they argue that word lengths should make information rates as close as possible to a hypothetical channel capacity, where the channel refers to the means by which information is transferred from one person to another. We term this the channel capacity hypothesis (CCH).2 They conclude that lengths should be proportional to a word\u2019s expected surprisal instead.3\nAs the communicative efficiency of language provides important insights into human cognition (Gibson et al., 2019), Piantadosi et al.\u2019s finding that word lengths are better explained by average surprisal than frequency has been influential. However, there are shortcomings: First, the manner in which Piantadosi et al. finds a solution which minimizes the cost associated with CCH is not formally\n2CCH is one of the many instantiations of the uniform information density (UID) hypothesis. We introduce this new terminology here to make the hypothesis\u2019 name more descriptive.\n3Surprisal is defined as negative log-probability in context.\nspecified. And second, Piantadosi et al.\u2019s empirical results have been shown to be sensitive to a number of methodological decisions, such as the choice of text-encoding (e.g., ascii vs. unicode), the inclusion of non-conventional wordforms and other orthographic conventions of a language (Meylan and Griffiths, 2021; Levshina, 2022). Thus, there remain fundamental open questions about the relationship between communicative efficiency and word length. Here, we aim to clarify both theoretical and empirical aspects of this relationship.\nTheoretically, we offer a novel, formal derivation of Piantadosi et al.\u2019s claim. We find that Piantadosi et al. (2011) optimize not for the objective under the CCH, but for a lower bound on it instead; we call this the CCH\u2193 objective. We then provide a closed-form expression for the function that determines word lengths under CCH: Word lengths should be proportional to the expected surprisal plus its variance-to-mean ratio. Importantly, we derive the solution above by framing the problem of assigning wordforms as the optimization of a cost function.4 By instantiating this optimization problem with the objectives posited by each hypothesis (ZIPF, CCH, and CCH\u2193), we can compute their word length predictions within a single, unified framework.\nEmpirically, we offer a large-scale comparison of ZIPF\u2019s, CCH\u2193\u2019s, and CCH\u2019s word length predictions across 13 typologically diverse languages. Notably, we use neural language models to estimate words\u2019 surprisals, which provides more accurate estimates than the n-gram models relied on by prior work on this topic (Piantadosi et al., 2011; Meylan and Griffiths, 2021; Levshina, 2022). We find strong evidence (see Fig. 1) that languages are optimized to minimize their utterance lengths: A word\u2019s frequency (ZIPF\u2019s prediction) offers stronger predictive power over word lengths than either the surprisal\u2019s expected value (CCH\u2193\u2019s prediction) or expected surprisal plus variance-tomean ratio (CCH\u2019s prediction). We conclude that Zipf\u2019s longstanding theory stands strong."
        },
        {
            "heading": "2 The Lexicalization Problem",
            "text": "Zipf (1935, 1949) posited that the lexicon is optimized for communication, taking the needs of both speakers and listeners into account. In this section, we formalize a slice of this optimization\n4As we will make explicit, we relax some optimization constraints to be able to derive closed-form solutions. These solutions will thus lead to lower bounds on the total cost.\nproblem. First, we assume a fixed (but potentially infinite) vocabulary W of words, each of which we denote as w \u2208 W , and a fixed alphabet \u03a3. Given a vocabulary and alphabet, we define a lexicon as a function that outputs a wordform for each word; we denote a lexicon as \u03d5 : W \u2192 \u03a3\u2217 and a wordform as \u03d5(w) \u2208 \u03a3\u2217. Note that we distinguish between a word, which is an abstract notion or concept, and its wordform, which is its orthophonological realization. Further, let p(w, c) be a language\u2019s joint probability distribution over these words and their prior linguistic context c \u2208 W\u2217.5 Finally, let cost[\u03d5](w, c) be a cost function that, given a lexicon, outputs the communicative cost of a word in context. It is often suggested that the only attribute of a wordform \u03d5(w) that the function cost[\u03d5] is concerned with is its length |\u03d5(w)|, where | \u00b7 | : \u03a3\u2217 \u2192 Z+. We now define the optimization problem proposed by Zipf as follows.\nDefinition 1. The lexicalization problem is the task of finding an optimal lexicon \u03d5\u2217, which minimizes cost[\u03d5]. This lexicon can be described formally as the solution to\n\u03d5\u2217 =argmin \u03d5 E p(w,c) cost[\u03d5](w, c)\nsubject to \u03d5 \u2208 \u03a6\u2113 (1)\nwhere \u03a6\u2113 is the set of valid \u03d5 for language \u2113.\nThere are many assumptions that one could make about \u03a6\u2113\u2019s characteristics. We make a few explicit in the following remark.\nRemark 1. We take the set \u03a6\u2113 to include all lexicons which: 1 only produce phonotactically valid wordforms,6 2 respect morphological composition,7 and 3 are uniquely decodable.8\nAnother implicit constraint 4 regarding valid \u03d5\u2014 which comes from our specification of the output\n5We define this distribution formally in App. A. 6Phonotactics tells us how phones can be combined to create wordforms in a language. If we denote the set of all possible phonotactically valid wordforms in language \u2113 as L\u2113 \u2282 \u03a3\u2217, this means that the image of \u03d5 is contained in L\u2113.\n7Roughly, if the concepts represented by w and w\u2032 overlap in a dimension that is captured by \u2113\u2019s morphology (e.g., plurality in English) then their wordforms \u03d5(w) and \u03d5(w\u2032) are likely to also partially overlap.\n8This condition is perhaps too strict. Homophony, for instance, is when \u03d5(w) = \u03d5(w\u2032) for w \u0338= w\u2032 and will, in general, make natural languages not uniquely decodable. However, if two words never appear in the same context, natural languages may still be uniquely decodable even in the presence of homophony. We note that whether or not natural languages are optimized for being unambiguous is contentions (Chomsky, 2002; Piantadosi et al., 2012; Pimentel et al., 2020, 2021b; Trott and Bergen, 2020, 2022).\nspace of \u03d5\u2014is that these mappings only produce integer-length wordforms.\nIn the subsequent sections, we consider relaxations of eq. (1) to arrive at simple solutions regarding the lengths provided by optimal lexica. Specifically, we partially relax constraint 1 and fully relax constraint 2 when deriving a lexicon with minimal utterance length. Further, when deriving optimal results for both CCH and CCH\u2193, we also fully relax constraints 1 , 3 , and 4 .9 Note that, as in all optimization problems, removing constraints always yields a lower bound on the expected cost we obtain under an optimal lexicon.10"
        },
        {
            "heading": "3 Revisiting Zipf\u2019s Law of Abbreviation",
            "text": "Zipf (1935, 1949) posited a specific form that the cost function in eq. (1) should take. Concretely, he posited that lexica were optimized with the goal of minimizing speakers\u2019 utterance lengths, which can be written as cost[\u03d5](w, c) = |\u03d5(w)| in our notation. In an attempt to formalize his position, he proposed his eponymous law of abbreviation:\n|\u03d5zipf(w)| \u221d \u2212 log p(w) (2)\nOver the years, Zipf\u2019s law of abbreviation has been empirically investigated numerous times (Wimmer et al., 1994; Sigurd et al., 2004; Kanwal et al., 2017; Koplenig et al., 2022; Levshina, 2022; Petrini et al., 2022, 2023). We now present a formal derivation of Zipf\u2019s law of abbreviation by viewing it as an instantiation of the lexicalization problem. Hypothesis 1. Zipf\u2019s hypothesis predicts that communication is made optimal by the mapping \u03d5zipf that satisfies:\n\u03d5zipf =argmin \u03d5 E p(w,c)\n|\u03d5(w)|\nsubject to \u03d5 \u2208 \u03a6\u2113 (3)\nIf we relax constraints 1 and 2 in Remark 1, then the optimal solution to eq. (3) can be achieved by Huffman coding (Huffman, 1952).11 We know that this optimal solution\u2019s word lengths respect:\n|\u03d5zipf(w)| \u2264 \u2212 log|\u03a3| p(w) + 1 (4) 9Explicitly, by relaxing 4 , we allow |\u03d5(\u00b7)| to take on continuous values. Such a relaxation destroys \u03d5\u2019s interpretation as assigning wordforms that live in \u03a3\u2217. However, it turns a combinatorial optimization problem into a continuous one and allows us to apply tools from calculus.\n10Pimentel et al. (2021c) estimate eq. (1) while respecting all four constraints, but restricted to cost[\u03d5](w, c) = |\u03d5(w)|.\n11By Kraft-McMillan\u2019s inequality, constraining our solution to not only be uniquely decodable, but to be prefix free, adds nothing in terms of length (Cover and Thomas, 2005).\nwhich can be roughly approximated as |\u03d5zipf(w)| \u2248 \u2212 log|\u03a3| p(w). Unfortunately, empirical evidence suggests that this solution, which suggests the proportionality constant in eq. (2) equals 1, is not representative of how natural languages behave (Pimentel et al., 2021c). It thus gives us little insight into how actual wordforms should behave.\nFortunately, we can derive a more interesting result where the proportionality in eq. (2) still holds by only partially relaxing 1 from Remark 1. We first assume a very simplistic model of phonotactics. Given an alphabet \u03a3 of phones, let L\u2113 \u2282 \u03a3\u2217 be the set of phonotactically valid wordforms in language \u2113. Note that this assumes deterministic phonotactics (Gorman, 2013; Dai and Futrell, 2021).12 Further, define PREFIXES(L\u2113) def = {\u03b1<t | \u03b1 \u2208 L\u2113, t \u2264 |\u03b1|} to be the set of all prefixes in this language. Assumption 1. The constant phonotactic assumption assumes there exists a K \u2208 Z>0 such that K \u2264 |\u03a3| and, for every string \u03b1\u2208 PREFIXES(L\u2113), there exist exactly K symbols {\u03c3k}Kk=1 for which \u03b1\u03c3k \u2208 PREFIXES(L\u2113).\nIn words, Assumption 1 says that there are exactly K valid symbols with which every phonotactically valid prefix can be extended. Given this assumption, we can now find a solution to eq. (3), which only partially relaxes the phonotactic constraint in Remark 1. Theorem 1. The minimization problem given in Hypothesis 1 with constraint 2 relaxed can be solved by Huffman coding13 with K symbols. The optimal solution is given by\n|\u03d5zipf(w)| = |\u03d5huffK (w)| (5a)\n\u2264 \u2212 1 log|\u03a3|K log|\u03a3| p(w) + 1 (5b)\nProof. The proof is available in App. C. \u25a0\nTheorem 1 makes precise the sense in which we claim to have derived Zipf\u2019s law of abbreviation. Under the rough approximation |\u03d5zipf(w)| \u2248 \u2212 log|\u03a3| p(w)log|\u03a3| K , the proportionality in eq. (2) is realized through the unknown constant 1/log|\u03a3| K.\n12Non-deterministic models of phonotactics are also popular; see (Hayes and Wilson, 2008) for a classic study.\n13Huffman coding is an efficient O (|W| log |W|)algorithm that returns the exact solution to this problem. Huffman coding, however, requires a finite W , which may not always be the case in theory. Linder et al. (1997) proved the existence of an optimal code which respects eq. (4) for distributions with infinite support but finite entropy. See Pimentel et al. (2021c) for more discussion."
        },
        {
            "heading": "4 Revisiting Piantadosi et al. (2011)",
            "text": "What\u2019s wrong with Zipf\u2019s law of abbreviation? The solution in eq. (5) is only optimal if one believes that cost[\u03d5](w, c) = |\u03d5(w)| is the true objective underlying the lexicalization problem. However, more recent work on communicative efficiency (e.g., Piantadosi et al., 2009, 2011) has proposed that speakers may intend to optimize another objective instead. Specifically, one can take the perspective that language is an exchange of information via a noisy communication channel, where information is operationalized as a word\u2019s surprisal H(w | c) = \u2212 log p(w | c). This channel has an inherent capacity C \u2208 R>0 at which information can be transmitted while the receiver is still able to effectively decode the underlying message. Under this perspective, optimal communication happens when a word\u2019s information rate (H(w|c)|\u03d5(w)| , in bits per character) is kept as close as possible to C. A word\u2019s channel deviation is then the difference between its information rate and channel capacity. This hypothesis can thus be stated within the framework of the lexicalization problem by defining the cost[\u03d5](w, c) of a lexicon as a function of the channel deviation.\nHypothesis 2. The channel capacity hypothesis predicts that communication is made optimal by the mapping \u03d5cch that satisfies:\n\u03d5cch =argmin \u03d5 E p(w,c) dist ( H(w | c) |\u03d5(w)| ,C ) subject to \u03d5 \u2208 \u03a6\u2113\n(6)\nwhere dist(x, y) is a function that quantifies how far x is from y.14\nIntuitively, eq. (6) penalizes lexica where the length of a word causes its information rate to deviate from the channel capacity. Thus, \u03d5cch will generate word lengths which produce information rates that are as uniform as possible. It follows that it can be categorized under the larger umbrella of the uniform information density hypothesis (UID; Fenk and Fenk, 1980; Levy and Jaeger, 2007). As discussed by Meister et al. (2021), however, UID has several potential interpretations, only one of which involves maximizing the use of a communication channel. Here, we will only discuss it under\n14We consider dist(\u00b7) functions which satisfy the first two axioms required by true distance metrics: dist(x, y) = 0 \u21d0\u21d2 x = y (identity of indiscernibles) and dist(x, y) \u2265 0 (non-negativity), but which are not necessarily symmetric and do not necessarily satisfy the triangle inequality.\nthis perspective, and assume that its operationalization is given by eq. (6)."
        },
        {
            "heading": "4.1 Optimal Word Lengths",
            "text": "The exact solution to eq. (6) depends on the choice of dist. In this section, we assume a quadratic distance function, i.e., dist(x,C) = (x \u2212 C)2. Efficient lexica should thus minimize the expected value of the square of the channel deviation under p(w, c) (i.e., its mean squared error). We now derive a closed-form expression for CCH-optimal word lengths under this cost function. As in Theorem 1, we relax the morphological 2 constraint. Beyond this, we also relax the phonotactic 1 , unique-decodability 3 , and the integer-length 4 constraints. Note that, unlike in Theorem 1, we need to relax 4 here because we have no efficient combinatorial algorithm to solve eq. (6).\nTheorem 2. Under Hypothesis 2, if we relax 1 , 2 , 3 and 4 , the optimal word lengths are given by\n|\u03d5cch(w)| = 1\nC\nE p(c|w)\n[ H2(w | c) ] E\np(c|w) [H(w | c)]\n(7)\nProof. The proof is available in App. D. \u25a0\nWe note that Eq. (7) is equivalent to the expected surprisal plus a variance-to-mean ratio.15"
        },
        {
            "heading": "4.2 Choices of Distance",
            "text": "In the above section, we assumed a quadratic penalty for a word\u2019s channel deviation. There is, however, no inherent reason why dist should be quadratic. We thus examine alternative ways to quantify the deviation between a word\u2019s information rate and the channel capacity. Different choices of dist will then each define a cost function through cost[\u03d5](w, c) = dist ( H(w|c) |\u03d5(w)| ,C ) .\nAny specific utterance should fall in one of three cases: First, a word\u2019s information rate may be at capacity, i.e., when H(w|c)|\u03d5(w)| = C. In this case, there are no CCH-based costs. As the capacity is a real number, however, this is virtually impossible in practice. Second, information rate may be below capacity. This will imply an opportunity cost on communication: speakers will need more time to produce their utterances than desired, which is not ideal from the perspective of communicative efficiency (Levy and Jaeger, 2007; Kanwal, 2018).\n15This can be seen via the following manipulations: E[x2] E[x] = E [x] + E[x2]\u2212E[x]2 E[x] = E [x] + V[x] E[x]\nThird, information rate may be above capacity. This again implies a cost on communication; since communicating above a channel\u2019s capacity is provably noisy (Shannon, 1948), there may be communication faults which will either lead to the wrong meaning being conveyed, or will require a potential retransmission of the message.\nThe quadratic distance function that we have proposed above assumes a symmetric cost, where communication above or below capacity are equally harmful. It is, however, reasonable to assume that the cost associated with communicating above capacity may be higher than the opportunity cost of communicating below it. This leads us to propose costs based on the following generalized distance function:\ndist(x,C) = { \u03bb (x\u2212 C)2 if x > C (x\u2212 C)2 else (8)\nwhere \u03bb \u2208 R>0. Under this generalized distance function, any value \u03bb > 1 will imply a larger penalty for communicating above than below capacity. Further, when \u03bb = 1 we recover the symmetric quadratic distance function proposed earlier.\nNotably, when assuming this generalized distance function, there is no capacity-agnostic closedform value to which word lengths should be proportional. Here, we find CCH\u03bb-optimal lengths with a two step process: (i) given a large set of surprisal values paired with their word lengths, we find what the optimal capacity is for a language; (ii) we then use a gradient descent-based optimizer to find the optimal lengths under that capacity."
        },
        {
            "heading": "4.3 Piantadosi et al.\u2019s (2011) Lower Bound",
            "text": "In their paper, Piantadosi et al. offer a similar argument to the one proposed in this section. They state, however, that the optimal word lengths follow:\n|\u03d5cch\u2193(w, c)| \u221d H(w | C) (9)\nwhere H(w | C) is the surprisal of word w, marginalized over all contexts. While Piantadosi et al. intended to find a solution which minimizes the cost associated with CCH, they actually do something else. We find that Piantadosi et al.\u2019s proposal optimizes a different instantiation of the lexicalization problem, one that does not use the objective that formally corresponds to the CCH hypothesis.16 We give the objective Piantadosi et al.\u2019s proposal is the solution to below as its own hypothesis.\n16See Cohen Priva (2015), however, for a discussion on how average surprisal could still predict a word\u2019s duration beyond individual surprisal effects.\nHypothesis 3. Piantadosi et al. predict that communication is made optimal by the mapping \u03d5cch\u2193 that satisfies:\n\u03d5cch\u2193 =argmin \u03d5 E p(w,c) dist ( H(w | C) |\u03d5(w)| ,C ) subject to \u03d5 \u2208 \u03a6\u2113 (10)\nWe now give the connection between Hypothesis 3 and eq. (9) in the following theorem.\nTheorem 3. Under Hypothesis 3, if we further relax 1 , 2 , 3 and 4 , the optimal word lengths are given by\n|\u03d5cch\u2193(w)| = 1\nC H(w | C) (11)\nProof. Using \u03d5 = \u03d5cch\u2193 as given by eq. (11), we get dist(\u00b7,C) = 0 for all words when evaluating the objective in eq. (10). By definition, this is the minimum for any dist. \u25a0\nNote that dist ( H(w|C) |\u03d5(w)| ,C ) is constant with respect to individual contexts c. Thus, the expectation in eq. (10) can simply be taken over the unigram distribution, p(w). Moreover, if dist is a convex function, then, we can use Jensen\u2019s inequality to show that eq. (10) lower-bounds eq. (6).17 We therefore denote Piantadosi et al.\u2019s hypothesis and solution CCH\u2193.\nProposition 1. Given a convex dist function and any \u03d5 \u2208 \u03a6\u2113, the cost optimized by CCH\u2193 in Hypothesis 3 lower-bounds CCH\u2019s cost in Hypothesis 2\nE p(w,c) dist ( H(w | c) |\u03d5(w)| ,C ) \u2265 E\np(w,c) dist ( H(w | C) |\u03d5(w)| ,C ) (12) Proof. The proof is available in App. E. \u25a0\nWe now provide an example to show how CCH\u2193\u2019s solution does not minimize\ndist ( H(w|c) |\u03d5(w)| ,C ) under the distribution p(w, c).\nExample 1. Let there be a word with a surprisal of 2 bits in ten distinct contexts, and a surprisal of 24 bits in a single context; assume all eleven contexts are equiprobable. The word\u2019s average surprisal is thus 4 bits (i.e., 10\u00b72+2411 ). Further, assume we\n17Note that this lower bound is with respect to the function being minimized in our optimization problem. It is therefore in addition to the lower bound that comes from relaxing this optimization problem\u2019s constraints.\nhave a channel with capacity C = 2. According to Theorem 3, we have |\u03d5cch\u2193(w)| = H(w|C) C = 2, which under the CCH objective (eq. (6)) gives us an expected cost of 10 (i.e., 1011 ( 2 2\u22122) 2+ 111 ( 24 2 \u22122)\n2). If we choose word lengths according to Theorem 2 instead, we get that the length should be |\u03d5cch(w)| = 7. This results in a cost under the CCH objective of roughly 2.86."
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Estimating Word Length Predictions",
            "text": "To evaluate the different hypotheses, we test how well their predictions about word lengths align with the lengths of real languages\u2019 wordforms. These predictions require computing surprisals (either unigram or contextual), which are defined according to the true probability distribution p (either as a function of p(w), or p(w | c); the distribution p is defined more precisely in App. A). While we do not have access to the true probability distribution p, we do have samples from it. We use the following estimators of eqs. (5), (7) and (11):\n\u0302|\u03d5zipf(w)| = \u2212 log q(w) (13a) \u0302|\u03d5cch\u2193(w)| = \u2212 1 |Dw| \u2211\nc\u2032\u2208Dw\nlog q(w | c\u2032) (13b)\n\u0302|\u03d5cch(w)| = \u2212\n\u2211 c\u2032\u2208Dw\n( log q(w | c\u2032) )2 \u2211\nc\u2032\u2208Dw log q(w | c\u2032)\n(13c)\nwhere Dw = {c\u2032 | (c\u2032, w\u2032) \u2208 D, w\u2032 = w}, and D is our corpus, which we assume to be sampled from the distribution p. In practice, our corpus D is composed of data from one out of 13 languages from 5 language families in Wiki40B (Guo et al., 2020).\nDistribution q is our estimate of p, which we implement using language models. We use: normalized count statistics to estimate the unigram distribution p(w), and transformer models for p(w | c). Our data and models are described in detail in App. B.18 Note that we omit unknown constants from eqs. (13a) to (13c) because we only consider scale-invariant evaluation metrics.\n18In our main set of experiments, we filter the set of words we analyze to only include the top 25k most frequent words in a language which have wordforms composed of characters in the language\u2019s alphabet; we use alphabet\u2019s as defined in homoglyph: https://pypi.org/project/homoglyphs/. We also pre-tokenize data with language-specific UnigramLM tokenizers, and sum subword surprisals when necessary to get per-word values."
        },
        {
            "heading": "5.2 Evaluation Metrics",
            "text": "Even with access to the true p, comparing the word length predictions of the different theories above would be non-trivial. Language evolution is a dynamic and noisy process: Even if one of the above optimization pressures has acted during the creation of languages\u2019 lexica, it is unlikely that they are perfectly optimal with respect to that pressure. We thus cannot simply evaluate whether languages match our predictions exactly. Rather, we can instead measure if the general trends predicted by the different hypotheses match the trends observed in natural language. We will rely on a number of metrics to evaluate our results. Taken together these metrics should allow us to draw conclusions on which theory (if any) best correlates with observed word lengths.\nSpearman Correlation. First, we follow prior work (Piantadosi et al., 2011; Meylan and Griffiths, 2021; Levshina, 2022) and use the Spearman correlation to assess the quality of each word-length hypothesis. A positive attribute of this correlation is that it can account for nonlinear relationships, potentially accounting for non-linear optimization obstacles. This metric, however, has a significant drawback: Namely, all wordforms contribute equally to its computation. If we evaluate large enough corpora using Spearman correlations, we will therefore consider vocabularies W mostly dominated by low-frequency and uncommon wordforms, such as typos, specialized terms, and names. Yet arguably, when evaluating the different hypotheses, a given word should be weighted according to its usage (i.e, frequency) in a given language, as this is the case in our various optimization problems; a word\u2019s impact on the lexicalization problem\u2019s objective is a function of its frequency. This is perhaps one of the reasons why prior work has limited their analyses to only consider a subset of the most common words per language (Piantadosi et al., 2011), a design choice that we likewise employ in our main experiments.\nPearson Correlation. As a second metric, we evaluate the Pearson correlation between our predictions and actual word lengths. Pearson\u2019s correlation has similar drawbacks to Spearman\u2019s, differing from it only in that its value reflects the strength of linear relationships.\nWeighted Mean Squared Error (MSE). As a third metric, we use weighted MSE, which\navoids the main drawbacks of the previous metrics. We fit linear regression models (without a bias term) to predict a language\u2019s word lengths using our ZIPF, CCH, or CCH\u2193 estimators as the sole predictor. Importantly, we weight each squared error term by that words\u2019 frequency (both during this model\u2019s training and evaluation). This design choice makes our method more robust to the set of words being evaluated, since the inclusion of exponentially many low-frequency words should not substantially affect weighted MSE. Note that this procedure is equivalent to measuring the predictive power of each hypothesis, while assuming eqs. (5), (7) and (11) predict an expected length, and that word lengths are normally distributed around these expected values."
        },
        {
            "heading": "6 Results",
            "text": "Our main results are presented in Fig. 1 and 2. In short, Fig. 1 shows that words\u2019 frequencies offer stronger predictive power of word lengths (as evinced by smaller MSE) than either of the surprisal-dependent metrics. This result provides evidence for ZIPF\u2019s hypothesis over either CCH or CCH\u2193. This result is particularly surprising since we improve on CCH\u2019s optimal word length predictions, but ZIPF\u2019s hypothesis still provides the best predictions.19 A similar result can be seen in Fig. 2, where frequency offers the strongest correlation with lengths (in terms of both Pearson and Spearman), in all languages but English. Notably, in our results, some languages even have a negative correlation between the two surprisal-based measures and actual word lengths. We now turn to analyzing different methodological choices that could impact our results."
        },
        {
            "heading": "6.1 Sensitivity to Tokenization",
            "text": "The first design choice that we analyze here is the choice of tokenizer that we use to preprocess our data. As cross-entropies are necessarily larger or equal to entropies,20 it is reasonable to expect that our language model surprisal estimates may be, on average, larger than true surprisals. While we do not know the exact per-token nature of this difference, it is conceivable that using UnigramLM tokenization could compound it: On average,\n19We improve CCH\u2019s optimal word length predictions over prior work both theoretically, by optimizing CCH as opposed to CCH\u2193, and empirically, by using stronger language models.\n20Cross-entropy is the (probability-weighted) average of the surprisal estimates from our language model.\nlonger words will naturally decompose into more subword units, and so when adding subword surprisals, the total error of a word\u2019s surprisal estimate may correlate with its number of subword units.\nTo assess the impact of this potential systematic error in our estimates, we thus re-train our models using a vocabulary of 32k full words, replacing any word not in this set with an unk symbol, which is necessary when working with finite vocabularies. Under this model, all analyzed words are encoded using a single \u201csubword\u201d unit. We then re-analyze the three hypotheses as before. In Fig. 3 (top), we see that a word\u2019s frequency is still a better predictor of its length than the quantities put forth by other hypotheses. Further, in the only case in which CCH\u2193 offers better predictions than ZIPF (English, as evinced by higher Spearman correlations), their performance difference is now lower than before.21\nWe also estimate ZIPF\u2019s unigram surprisals using tokenized counts, i.e., where we count subword tokens to estimate frequencies instead of directly counting the full words in our training set. We then estimate the suprisal of a word as the sum of the surprisals of its subwords, thus assuming independence between them. We display these new results in Fig. 3 (bottom) under the name Zipf (subwords). We see here that this tokenization scheme increases our measured correlations, and Zipf (subwords) presents the strongest correlations in all languages. Perhaps surprisingly, tokenization seems to not influence MSE as much."
        },
        {
            "heading": "6.2 Sensitivity to Word Filtering Protocol",
            "text": "Next, we analyze our results\u2019 sensitivity with respect to how we select the set of words we analyze. Specifically, for our analyses so far we have only considered words whose wordform is composed\n21These correlations are, respectively, 0.09 vs. 0.21 with ZIPF and CCH\u2193 when using UnigramLM. After switching to full words they are 0.09 vs. 0.14.\nexclusively of characters in its language\u2019s alphabet. We now run similar analyses, but including either: All white-space-separated words in a language\u2019s test set, or all white-space-separated words with no punctuation symbols.22 We denote these conditions as: \u03a3\u2217\u03b1 when selecting alphabet-only words, \u03a3\u2217\u25e6 when selecting no-punctuation words, and \u03a3\u2217 when selecting all words. We display results under each condition in Fig. 4. We see that across these various protocols, ZIPF\u2019s hypothesis remains the most predictive.23\nAdditionally, we consider the impact of including only the top 25k most frequent words in our analysis. In Fig. 5, we present MSE values computed when using sets composed from the top 10k most frequent words, to entire test sets. Notably, we again see that frequency remains the best pre-\n22We consider punctuation to be any of: !\"#$%&\u2019()*+,- ./:;<=>?@[\\]\u02c6_\u2018{|}\u02dc.\n23In App. G\u2019s Fig. 9, we also see that Spearman correlation is considerably more sensitive to filtering protocols than MSE.\ndictor of word length. In App. G\u2019s Fig. 10, we display results per language for MSE and Spearman correlation. There, we see that MSE rates frequency best on all languages and across all evaluated setups. Spearman correlation evaluated on few word types similarly rates frequency over CCH or CCH\u2193 predictions (again, except on English). When evaluated on full test-sets, Spearman correlation shows a less straightforward conclusion: While ZIPF still achieves the highest correlation in most languages, CCH\u2193 achieves stronger correlations in Italian, Spanish and Russian. At this stage, however, the evaluated sets are dominated by lowfrequency words, which may not be representative of the evaluated languages."
        },
        {
            "heading": "6.3 Sensitivity to Model Quality",
            "text": "Finally, we investigate how our model quality influences our results. We train new models on subsets of our training sets to get language models\nof different qualities. We then use these models to assess whether there is a relationship between model quality and a hypothesis\u2019 predictive power. In addition to the models estimated using the full training sets, we thus train 7 new transformer and unigram models per language, each using from 1 million to 1 billion training tokens in log-uniform intervals. We plot the predictive power of each hypothesis (ZIPF\u2019s, CCH\u2193\u2019s and CCH) vs. the language model\u2019s cross-entropy in Fig. 6.24 Unintuitively, surprisal estimates of better models (i.e., with lower cross-entropies) provide worse predictors of word length. An additional analysis suggests that the surprisal estimates of worse language models are more strongly correlated with frequency (see Fig. 7 in App. G), which may justify this unituitive result since frequencies are most predictive of word lengths in our experiments. ZIPF\u2019s hypothesis, on the other hand, is robust to the quality of the used unigram model."
        },
        {
            "heading": "6.4 Sensitivity to Cost Function",
            "text": "In our last set of experiments, we analyze the impact of our choice of quadratic cost function in our results. Using the generalized cost function in eq. (8), we derive optimal word length predictions using values of \u03bb from 1 to 5 in 0.25 intervals. We present their MSE and Spearman correlations in App. G\u2019s Fig. 13. While there seems to be a slight tendency for CCH to be more predictive for larger values of \u03bb, ZIPF still has the most predictive power of the different hypotheses.\n24We show per-language plots evaluated with both MSE and Spearman correlation in App. G\u2019s Fig. 11. We do not quantify our unigram models\u2019 quality, but assume that they increase monotonically with the size of the corpus on which they were estimated. We show a similar plot, but with the number of training tokens on the x-axis, in App. G\u2019s Fig. 12."
        },
        {
            "heading": "7 Discussion",
            "text": "The answer to what drives the distribution of word lengths in lexica has long been considered important for understanding the evolution and function of language (see Gibson et al., 2019 for a review). Across multiple languages and various methodological choices, our results support Zipf\u2019s law of abbreviation over other potential explanations as a driving factor in the development of lexica.\nThese findings deviate from Piantadosi et al., who found average surprisal to be a stronger predictor of word lengths. We hypothesize that this is because of methodological choices. Specifically, Piantadosi et al. derive surprisal estimates from language models that are now outdated (in terms of their quality), and we found that, when CCH\u2019s predictions were computed using worse surprisal estimates, they had stronger correlations with length than when using better estimates. Like prior work on this topic (Meylan and Griffiths, 2021; Levshina, 2022), our analyses suggest the sensitivity of Piantadosi et al.\u2019s results to methodological choices.\nWhat do these results tell us about the communicative optimization of natural language lexica? In short, our results suggest lexica are optimized to minimize expected utterance lengths. Notably, other linguistic properties may be optimized towards other notions of communicative efficiency. While a word\u2019s duration is mainly determined by its wordform, speakers can still modulate this duration to a certain extent; such a modulation could target CCH. In fact, prior work has shown a correlation between surprisal and duration (Bell et al., 2003; Aylett and Turk, 2004; Pimentel et al., 2021a)."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we formalize the problem of assigning wordforms based on different notions of communicative efficiency, which we term the lexicalization problem. Under this framework, we describe the optimization problem related to the channel capacity hypothesis, and, in doing so, we show that Piantadosi et al.\u2019s predictions optimized for only a lower bound on CCH, rather than on the true objective. Further, while considering relaxed versions of the lexicalization problem, we derive optimal word length values for Zipf\u2019s hypothesis and CCH. We then empirically evaluate CCH\u2019s, CCH\u2193\u2019s and ZIPF\u2019s predictions in 13 languages. Our results strongly support ZIPF\u2019s hypothesis: Word lengths are optimized to minimize utterance lengths.\nLimitations\nA limitation of our work is that, when deriving optimal word lengths under CCH and CCH\u2193, we relax: the phonotactic 1 , morphological composition 2 , unique decodability 3 and the integer-length 4 requirements. In the case of 3 , if a language\u2019s channel capacity is large, this might lead to poorer predictions under both these theories. Deriving optimal word lengths while considering this constraint is left as an open problem for future work. In the case of 4 , it is arguably unrealistic to consider continuous-length wordforms. This issue could be addressed by using a linear program to solve problems of the form eq. (1). This, as well as considering the role of phonotactics 1 and morphological composition 2 in CCH, is likewise left for future work. Further, we note that while we relax all four constraints to derive CCH- and CCH\u2193-optimal word lengths, we only relax 2 (and partially 1 ) to derive ZIPF-optimal lengths. This could realistically impact the fact that Zipf\u2019s hypothesis seems to have more predictive power over word lengths.\nAnother limitation is that our analyses focus solely on written data from Wikipedia. We recommend future work investigates how these findings generalize to spoken or signed languages, and to other text genres. Finally, while we use a typologically diverse sample of languages, it is still skewed towards Eurasian languages. This is because the large amount of text needed for training state-of-the-art language models\u2014 necessary to estimate entropy\u2014are not available in many languages. Expanding the set of languages analyzed here would be necessary to confirm the generality of our results."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers and metareviewer for their feedback on this paper. Tiago Pimentel also thanks Hope McGovern and Simone Teufel for helpful comments on different stages of writing this manuscript. Tiago Pimentel is funded by a Facebook PhD Fellowship. Clara Meister is funded by a Google PhD Fellowship. Ethan Gotlieb Wilcox would like to acknowledge support from an ETH Postdoctoral Fellowship."
        },
        {
            "heading": "B Data and Models",
            "text": "Data. The corpora used throughout our analyses come from Wiki40B (Guo et al., 2020). This dataset is composed of cleaned text from Wikipedia articles in more than 40 languages, out of which we select a subset of 13 for our analysis. Our selection includes: German, Greek, English, Spanish, Estonian, Finnish, Hebrew, Italian, Korean, Dutch,\nNorwegian, Russian, and Turkish. These span five language families: Afro-Asiatic, Indo-European, Koreanic, Turkic, and Uralic. The data for each language comes pre-split into a training, validation and test set. We fit our models using the first two sets, while performing our analyses exclusively on the test-sets. As discussed above, the set of analyzed words may make a large difference in the measured correlations. In our main set of experiments, we filter the set of words we analyze to only include wordforms composed of characters in the language\u2019s alphabet.25 Table 1 (in App. F) includes the number of word types and tokens used per language in our analyses.\nModels. To estimate the unigram distribution p(w), we use a simple MLE estimator: the (normalized) count statistics from our training set. To estimate contextual probabilities p(w | c), we use an autoregressive language model p\u03b8. Specifically, we train monolingual transformers in each language using fairseq (Ott et al., 2019) with its default language modeling hyper-parameters. Our transformers (Vaswani et al., 2017) have 6 layers, a hidden size of 512, and 8 attention heads per layer. Further, they can attend to a context size of at most 512 tokens, and we train them with a dropout of 0.1, and a batch size of 64. We optimize our models using Adam (Kingma and Ba, 2015) with a learning rate of 5\u00d7 10\u22124, weight decay of 0.01, and 4k warmup steps. In our main set of experiments, we further pre-tokenize each language\u2019s text using language-specific tokenizers fit (using the UnigramLM algorithm; Kudo, 2018) on their respective training sets, with a vocabulary of 32k subword units. We then compute per-word surprisals by adding the surprisals of all the subwords that the word is composed of. (We also consider other tokenization schemes, as described in \u00a76.1.)"
        },
        {
            "heading": "C Proof of Theorem 1",
            "text": "Before proving Theorem 1, we provide a lemma which will be useful for it. In words, we prove a length-preserving bijection between L\u2113 and \u2206\u2217 for an alphabet K such that |\u2206| = K. Lemma 1. Under the constant phonotactic assumption, there exists an alphabet \u2206 with cardinality K such that, for every N \u2265 0, \u2206N is isomorphic to L(N)\u2113 , where L (N) \u2113 is the set of\n25We use alphabet\u2019s as defined in homoglyph: https:// pypi.org/project/homoglyphs/.\nphonotactically valid wordforms with length N .\nProof. First, it is clear that |\u2206N | = |\u2206|N = KN . We now prove the same for L(N)\u2113 by induction.\nBase case (N = 0). The set of 0-length phonotactically valid strings includes only the empty string {\u03b5}. It follows that: |L(0)\u2113 | = 1 = K 0.\nInductive step (N > 0). By the inductive hypothesis, we have that |L(N\u22121)\u2113 | = K\nN\u22121. By Assumption 1, each element in L(N\u22121)\u2113 has K possible continuations in L(N)\u2113 . It follows that |L(N)\u2113 | = |L (N\u22121) \u2113 |K = K N .\nSince \u2206N and L(N)\u2113 have the same number of elements for every N \u2265 0, there exists an isomorphism between them. \u25a0\nGiven the lemma above, we are now in a position to prove Theorem 1.\nTheorem 1. The minimization problem given in Hypothesis 1 with constraint 2 relaxed can be solved by Huffman coding with K symbols. The optimal solution is given by\n|\u03d5zipf(w)| = |\u03d5huffK (w)| (5a)\n\u2264 \u2212 1 log|\u03a3|K log|\u03a3| p(w) + 1 (5b)\nProof. Since L(N)\u2113 is isomorphic to \u2206 N for every N \u2265 0, there exists a length-preserving bijection \u03c8 between L\u2113 and \u2206 (by Lemma 1). By Huffman\u2019s (1952) algorithm, we can construct an encoding that satisfies\n|\u03c8(\u03d5zipf(w))| \u2264 \u2212 log|\u2206| p(w) + 1 (15)\nHowever, because \u03c8 is length-preserving, |\u03c8(\u03d5zipf(w))| = |\u03d5zipf(w)|. As an upper bound, we thus have\n|\u03d5zipf(w)| \u2264 \u2212 log|\u2206| p(w) + 1 (16a)\n= \u2212 1 log|\u03a3|K log|\u03a3| p(w) + 1 (16b)\n\u25a0"
        },
        {
            "heading": "D Proof of Theorem 2",
            "text": "Theorem 2. Under Hypothesis 2, if we relax 1 , 2 , 3 and 4 , the optimal word lengths are given by\n|\u03d5cch(w)| = 1\nC\nE p(c|w)\n[ H2(w | c) ] E\np(c|w) [H(w | c)]\n(7)\nProof. We can easily derive these optimal word lengths from eq. (6) by taking its derivative with respect to a specific word\u2019s length, and setting it to zero. First, we rewrite it for mathematical convenience as:\nE p(w) E p(c|w) ( H(w | c) |\u03d5(w)| \u2212 C )2\n(17)\nwhere we make the quadratic cost function explicit. We note this function is convex, and so if we find a point where its derivative is zero, we also find its global minimum. We now take its derivative with respect to a specific word\u2019s length |\u03d5(w)| and set this derivative to zero:\np(w) E p(c|w)\n[ 2 ( H(w | c) |\u03d5(w)| \u2212 C ) H(w | c) |\u03d5(w)|2 ] = 0\n(18) where we note that all terms involving other words will have derivative zero (with respect to this specific word w\u2019s length). As the expectation is a linear operation, we can rewrite this equation as:\nE p(c|w) [ H2(w | c) |\u03d5(w)|3 ] = E p(c|w) [ C H(w | c) |\u03d5(w)|2 ] (19)\nNote that both the length and capacity are constant with respect to the expectation over contexts. Isolating the length term, thus, we get:\n|\u03d5(w)| = 1 C\nE p(c|w)\n[ H2(w | c) ] E\np(c|w) [H(w | c)]\n(20)\nThis completes the proof. \u25a0"
        },
        {
            "heading": "E Proof of Proposition 1",
            "text": "Proposition 1. Given a convex dist function and any \u03d5 \u2208 \u03a6\u2113, the cost optimized by CCH\u2193 in Hypothesis 3 lower-bounds CCH\u2019s cost in Hypothesis 2\nE p(w,c) dist ( H(w | c) |\u03d5(w)| ,C ) \u2265 E\np(w,c) dist ( H(w | C) |\u03d5(w)| ,C ) (12)\nProof. It can be easily shown by Jensen\u2019s inequal-\nity that for any choice of \u03d5:\nE p(w,c) dist ( H(w | c) |\u03d5(w)| ,C ) (21a)\n= E p(w) E p(c|w) dist ( H(w | c) |\u03d5(w)| ,C ) (21b)\n\u2265 E p(w) dist  Ep(c|w) [H(w | c)] |\u03d5(w)| ,C  (21c) = E\np(w) dist ( H(w | C) |\u03d5(w)| ,C ) (21d)\nwhich completes the proof. \u25a0"
        },
        {
            "heading": "F Data Statistics",
            "text": "We provide dataset statistics in Table 1."
        },
        {
            "heading": "G Further Results",
            "text": "For a more detailed reading, we provide MSE and Spearman correlation plots similar to Fig. 1 and 2\u2019s but as bar plots in Fig. 8. We also provide perlanguage results:\n\u2022 as a function of the word filtering protocol used in our analysis in Fig. 9;\n\u2022 as a function of the number of word types included in our analysis in Fig. 10;\n\u2022 as a function of our language model\u2019s crossentropy in Fig. 11; and\n\u2022 as a function of the number of tokens used to train our language models and to get word count statistics in Fig. 12.\nWe also provide results when CCH is defined using generalized distfunctions, i.e., for several values of \u03bb, in Fig. 13. Finally, we show the Spearman correlation between CCH and CCH\u2193 and unigram surprisal as a function of the used language model\u2019s quality in Fig. 7."
        }
    ],
    "title": "Revisiting the Optimality of Word Lengths",
    "year": 2023
}