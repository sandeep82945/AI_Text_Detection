{
    "abstractText": "3D referring expression comprehension is a task to ground text representations onto objects in 3D scenes. It is a crucial task for indoor household robots or augmented reality devices to localize objects referred to in user instructions. However, existing indoor 3D referring expression comprehension datasets typically cover larger object classes that are easy to localize, such as chairs, tables, or doors, and often overlook small objects, such as cooking tools or office supplies. Based on the recently proposed diverse and high-resolution 3D scene dataset of ARKitScenes, we construct the ARKitSceneRefer dataset focusing on small daily-use objects that frequently appear in real-world indoor scenes. ARKitSceneRefer contains 15k objects of 1, 605 indoor scenes, which are significantly larger than those of the existing 3D referring datasets, and covers diverse object classes of 583 from the LVIS dataset. In empirical experiments with both 2D and 3D state-of-theart referring expression comprehension models, we observed the task difficulty of the localization in the diverse small object classes. ARKitSceneRefer dataset is available at: https: //github.com/ku-nlp/ARKitSceneRefer",
    "authors": [
        {
            "affiliations": [],
            "name": "Shunya Kato"
        },
        {
            "affiliations": [],
            "name": "Shuhei Kurita"
        },
        {
            "affiliations": [],
            "name": "Chenhui Chu"
        },
        {
            "affiliations": [],
            "name": "Sadao Kurohashi"
        }
    ],
    "id": "SP:f1bcb632dffa8ef92a78823bb072d93216cce962",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Ahmed Abdelreheem",
                "Fei Xia",
                "Mohamed Elhoseiny",
                "Leonidas J. Guibas."
            ],
            "title": "ReferIt3D: Neural listeners for fine-grained 3d object identification in real-world scenes",
            "venue": "Proceedings of the 16th European Conference on Computer Vision",
            "year": 2020
        },
        {
            "authors": [
                "Daichi Azuma",
                "Taiki Miyanishi",
                "Shuhei Kurita",
                "Motoki Kawanabe."
            ],
            "title": "Scanqa: 3d question answering for spatial scene understanding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Gilad Baruch",
                "Zhuoyuan Chen",
                "Afshin Dehghan",
                "Tal Dimry",
                "Yuri Feigin",
                "Peter Fu",
                "Thomas Gebauer",
                "Brandon Joffe",
                "Daniel Kurz",
                "Arik Schwartz",
                "Elad Shulman"
            ],
            "title": "ARKitscenes - a diverse real-world dataset for 3d indoor scene understanding",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko."
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "Computer Vision \u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August",
            "year": 2020
        },
        {
            "authors": [
                "Angel Chang",
                "Angela Dai",
                "Thomas Funkhouser",
                "Maciej Halber",
                "Matthias Niessner",
                "Manolis Savva",
                "Shuran Song",
                "Andy Zeng",
                "Yinda Zhang."
            ],
            "title": "Matterport3D: Learning from RGB-D data in indoor environments",
            "venue": "International Conference on 3D Vision",
            "year": 2017
        },
        {
            "authors": [
                "Dave Zhenyu Chen",
                "Angel X Chang",
                "Matthias Nie\u00dfner."
            ],
            "title": "Scanrefer: 3d object localization in rgb-d scans using natural language",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV). Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Dave Zhenyu Chen",
                "Qirui Wu",
                "Matthias Nie\u00dfner",
                "Angel X. Chang."
            ],
            "title": "D3net: A speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Xinlei Chen",
                "Hao Fang",
                "Tsung-Yi Lin",
                "Ramakrishna Vedantam",
                "Saurabh Gupta",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft coco captions: Data collection and evaluation server",
            "venue": "CoRR, abs/1504.00325.",
            "year": 2015
        },
        {
            "authors": [
                "Zhenfang Chen",
                "Lin Ma",
                "Wenhan Luo",
                "KwanYee Kenneth Wong."
            ],
            "title": "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2019
        },
        {
            "authors": [
                "Volkan Cirik",
                "Taylor Berg-Kirkpatrick",
                "LouisPhilippe Morency."
            ],
            "title": "Refer360\u25e6: A referring expression recognition dataset in 360\u25e6 images",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7189\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner."
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2017
        },
        {
            "authors": [
                "Jiajun Deng",
                "Zhengyuan Yang",
                "Tianlang Chen",
                "Wengang Zhou",
                "Houqiang Li."
            ],
            "title": "Transvg: Endto-end visual grounding with transformers",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1769\u20131779.",
            "year": 2021
        },
        {
            "authors": [
                "Martin Ester",
                "Hans-Peter Kriegel",
                "J\u00f6rg Sander",
                "Xiaowei Xu."
            ],
            "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
            "venue": "Proceedings of the Second International Conference on Knowledge Discovery and Data Min-",
            "year": 1996
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick."
            ],
            "title": "Lvis: A dataset for large vocabulary instance segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Ayush Jain",
                "Nikolaos Gkanatsios",
                "Ishita Mediratta",
                "Katerina Fragkiadaki."
            ],
            "title": "Bottom up top down detection transformers for language grounding in images and point clouds",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Haojun Jiang",
                "Yuanze Lin",
                "Dongchen Han",
                "Shiji Song",
                "Gao Huang."
            ],
            "title": "Pseudo-q: Generating pseudo language queries for visual grounding",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
            "year": 2022
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Mannat Singh",
                "Yann LeCun",
                "Gabriel Synnaeve",
                "Ishan Misra",
                "Nicolas Carion."
            ],
            "title": "Mdetr - modulated detection for end-to-end multi-modal understanding",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Sahar Kazemzadeh",
                "Vicente Ordonez",
                "Mark Matten",
                "Tamara Berg."
            ],
            "title": "ReferItGame: Referring to objects in photographs of natural scenes",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787\u2013",
            "year": 2014
        },
        {
            "authors": [
                "Shuhei Kurita",
                "Naoki Katsura",
                "Eri Onami."
            ],
            "title": "Refego: Referring expression comprehension dataset from first-person perception of ego4d",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15214\u201315224.",
            "year": 2023
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang",
                "Kai-Wei Chang",
                "Jianfeng Gao"
            ],
            "title": "Grounded language-image pre-training",
            "year": 2022
        },
        {
            "authors": [
                "Zhenyang Li",
                "Ran Tao",
                "Efstratios Gavves",
                "Cees G.M. Snoek",
                "Arnold W.M. Smeulders."
            ],
            "title": "Tracking by Natural Language Specification",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2017
        },
        {
            "authors": [
                "Xuejing Liu",
                "Liang Li",
                "Shuhui Wang",
                "Zheng-Jun Zha",
                "Li Su",
                "Qingming Huang."
            ],
            "title": "Knowledgeguided pairwise reconstruction network for weakly supervised referring expression grounding",
            "venue": "Proceedings of the 27th ACM International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Edward Loper",
                "Steven Bird."
            ],
            "title": "Nltk: The natural language toolkit",
            "venue": "arXiv preprint cs/0205028.",
            "year": 2002
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Junyu Luo",
                "Jiahui Fu",
                "Xianghao Kong",
                "Chen Gao",
                "Haibing Ren",
                "Hao Shen",
                "Huaxia Xia",
                "Si Liu."
            ],
            "title": "3d-sps: Single-stage 3d visual grounding via referred point progressive selection",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Junhua Mao",
                "Jonathan Huang",
                "Alexander Toshev",
                "Oana Camburu",
                "Alan Yuille",
                "Kevin Murphy."
            ],
            "title": "Generation and comprehension of unambiguous object descriptions",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "Pushmeet Kohli Nathan Silberman",
                "Derek Hoiem",
                "Rob Fergus."
            ],
            "title": "Indoor segmentation and support inference from rgbd images",
            "venue": "ECCV.",
            "year": 2012
        },
        {
            "authors": [
                "Songyou Peng",
                "Kyle Genova",
                "Chiyu Jiang",
                "Andrea Tagliasacchi",
                "Marc Pollefeys",
                "Thomas Funkhouser"
            ],
            "title": "Openscene: 3d scene understanding with open vocabularies",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2023
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Bryan A. Plummer",
                "Liwei Wang",
                "Chris M. Cervantes",
                "Juan C. Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik."
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "2015 IEEE International",
            "year": 2015
        },
        {
            "authors": [
                "Yuankai Qi",
                "Qi Wu",
                "Peter Anderson",
                "Xin Wang",
                "William Yang Wang",
                "Chunhua Shen",
                "Anton van den Hengel."
            ],
            "title": "Reverie: Remote embodied visual referring expression in real indoor environments",
            "venue": "Proceedings of the IEEE Conference on",
            "year": 2020
        },
        {
            "authors": [
                "David Rozenberszki",
                "Or Litany",
                "Angela Dai."
            ],
            "title": "Language-grounded indoor 3d semantic segmentation in the wild",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV).",
            "year": 2022
        },
        {
            "authors": [
                "Shuran Song",
                "Fisher Yu",
                "Andy Zeng",
                "Angel X Chang",
                "Manolis Savva",
                "Thomas Funkhouser."
            ],
            "title": "Semantic scene completion from a single depth image",
            "venue": "Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition.",
            "year": 2017
        },
        {
            "authors": [
                "Carter",
                "Jesus Briales",
                "Tyler Gillingham",
                "Elias Mueggler",
                "Luis Pesqueira",
                "Manolis Savva",
                "Dhruv Batra",
                "Hauke M. Strasdat",
                "Renzo De Nardi",
                "Michael Goesele",
                "Steven Lovegrove",
                "Richard Newcombe"
            ],
            "title": "The Replica dataset: A digital replica of indoor",
            "year": 2019
        },
        {
            "authors": [
                "Sanjay Subramanian",
                "William Merrill",
                "Trevor Darrell",
                "Matt Gardner",
                "Sameer Singh",
                "Anna Rohrbach."
            ],
            "title": "ReCLIP: A strong zero-shot baseline for referring expression comprehension",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Mingjie Sun",
                "Jimin Xiao",
                "Eng Gee Lim",
                "Si Liu",
                "John Y. Goulermas"
            ],
            "title": "Discriminative triad",
            "year": 2021
        },
        {
            "authors": [
                "Johanna Wald",
                "Armen Avetisyan",
                "Nassir Navab",
                "Federico Tombari",
                "Matthias Nie\u00dfner."
            ],
            "title": "Rio: 3d object instance re-localization in changing indoor environments",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages",
            "year": 2019
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Yanmin Wu",
                "Xinhua Cheng",
                "Renrui Zhang",
                "Zesen Cheng",
                "Jian Zhang"
            ],
            "title": "Eda: Explicit textdecoupling and dense alignment for 3d visual and language learning",
            "year": 2022
        },
        {
            "authors": [
                "Mutian Xu",
                "Pei Chen",
                "Haolin Liu",
                "Xiaoguang Han."
            ],
            "title": "To-scene: A large-scale dataset for understanding 3d tabletop scenes",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Yunhan Yang",
                "Xiaoyang Wu",
                "Tong He",
                "Hengshuang Zhao",
                "Xihui Liu."
            ],
            "title": "Sam3d: Segment anything in 3d scenes",
            "venue": "arXiv preprint arXiv:2306.03908.",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Faisal Ahmed",
                "Zicheng Liu",
                "Yumao Lu",
                "Lijuan Wang."
            ],
            "title": "Unitab: Unifying text and box outputs for grounded vision-language modeling",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Licheng Yu",
                "Patrick Poirson",
                "Shan Yang",
                "Alexander C. Berg",
                "Tamara L. Berg."
            ],
            "title": "Modeling context in referring expressions",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), pages 69\u201385.",
            "year": 2016
        },
        {
            "authors": [
                "Zhihao Yuan",
                "Xu Yan",
                "Zhuo Li",
                "Xuhao Li",
                "Yao Guo",
                "Shuguang Cui",
                "Zhen Li."
            ],
            "title": "Toward explainable and fine-grained 3d grounding through referring textual phrases",
            "venue": "arXiv preprint arXiv:2207.01821.",
            "year": 2022
        },
        {
            "authors": [
                "Zhihao Yuan",
                "Xu Yan",
                "Yinghong Liao",
                "Ruimao Zhang",
                "Sheng Wang",
                "Zhen Li",
                "Shuguang Cui."
            ],
            "title": "Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Lichen Zhao",
                "Daigang Cai",
                "Lu Sheng",
                "Dong Xu."
            ],
            "title": "3dvg-transformer: Relation modeling for visual grounding on point clouds",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2928\u20132937.",
            "year": 2021
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Rohit Girdhar",
                "Armand Joulin",
                "Philipp Kr\u00e4henb\u00fchl",
                "Ishan Misra."
            ],
            "title": "Detecting twenty-thousand classes using image-level supervision",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2020) found that using color and normal information improves performance in 3D models",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "3D referring expression comprehension (REC) is an essential task of understanding 3D scenes and localizing objects in scenes into easy-to-interpret text representations. It has numerous applications, such as robotics and augmented reality. Recently, sophisticated datasets have been proposed for this purpose (Chen et al., 2020; Wald et al., 2019; Qi et al., 2020). These datasets are based on object segmentations in 3D scenes and cover relatively large objects, such as furniture in indoor scenes.\nHowever, when we develop robots that follow instructions and perform indoor household tasks, such robots are expected to find out and localize typically small objects that are required for household tasks. For example, for developing cooking\nrobots, robots are expected to find cooking tools, foods, and ingredients. Similarly, to develop autonomous laundry robots, they need to find small objects such as clothes. We assume that finding these small objects in 3D scenes is meaningful for household robots, although these objects are difficult to be captured and often overlooked from the existing real-world scan-based 3D scene REC datasets.\nIn this study, we propose a new REC dataset named ARKitSceneRefer in 3D scenes in that we concentrate on the fineness and diversity of the objects referred to in instructions. ARKitSceneRefer is based on the recently proposed ARKitScenes dataset. ARKitScenes (Baruch et al., 2021) is a finegrained photo-realistic 3D scan for diverse 1, 661 venues and 5, 047 scenes. Based on ARKitScenes, we extract small objects that are not covered in the previous 3D scene datasets well. We first apply the 2D object detector Detic (Zhou et al., 2022) with LVIS (Gupta et al., 2019) object classes for the video frames (i.e., 2D images) from which 3D\nscenes in ARKitScenes are constructed. Next, we extract object labels and positions and map them to 3D scenes with ray-casting and clustering by DBSCAN (Ester et al., 1996). We confirm that most small objects are detected in 3D scenes with this approach. We then annotate referring expressions that people use to locate the objects via Amazon Mechanical Turk, while manually revising the incorrectly detected object labels. Figure 1 shows an example of our ARKitSceneRefer dataset. We finally collect 1, 605 scenes with 583 object classes and more than 15k objects and their corresponding referring expressions.\nIn addition, we conduct experiments with both 2D and 3D models to localize objects in ARKitSceneRefer. Our 2D models are based on the state-of-the-art 2D REC models MDETR (Kamath et al., 2021) and OFA (Wang et al., 2022). Our 3D models are based on an adaptation of the stateof-the-art 3D REC models ScanRefer (Chen et al., 2020) and 3DVG-Transformer (Zhao et al., 2021) on our dataset.\nOur contributions are as follows: (i) creating the first object localization dataset concentrating on the small objects in daily indoor scenes upon the high-resolution 3D scene dataset of ARKitScenes, (ii) attaching more than 15k referring expressions with human annotations with a significantly large number of object classes, and (iii) comparisons with the state-of-the-art 2D and 3D REC models on ARKitSceneRefer."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 3D and Language",
            "text": "Recently, several photorealistic 3D indoor scene datasets (Nathan Silberman and Fergus, 2012; Song et al., 2017; Dai et al., 2017; Wald et al., 2019; Straub et al., 2019; Ramakrishnan et al., 2021; Rozenberszki et al., 2022) have been constructed. ScanNet (Dai et al., 2017) consists of 1, 513 RGB-D scans of 707 unique indoor environments with estimated camera parameters and semantic segmentation. 3RScan (Wald et al., 2019) consists of 1, 482 RGB-D scans of 478 environments across multiple time steps, including objects whose positions change over time and annotations of object instances and 6DoF mappings. ARKitScenes (Baruch et al., 2021) is the recently proposed high-resolution 3D scene dataset based on Apple\u2019s LiDER scanner. ARKitScenes consists of 5, 047 high-resolution RGB-D scans of\n1, 661 unique indoor environments and provides high-quality depth maps and 3D-oriented bounding boxes.\nBased on these 3D indoor-scene datasets, several language-related 3D scene understanding datasets have been proposed. For 3D visual grounding or 3D REC, ScanRefer (Chen et al., 2020) and ReferIt3D (Achlioptas et al., 2020) have been proposed. These datasets are based on ScanNet and annotated with referring expressions for objects in 3D scenes. They are also used for the 3D dense captioning task. Similarly, the 3D question answering dataset ScanQA (Azuma et al., 2022) was proposed based on ScanNet. Yuan et al. (2022) extended 3D visual grounding to 3D phrase-aware grounding with phrase-level annotations from existing 3D visual grounding datasets (Chen et al., 2020; Achlioptas et al., 2020). Qi et al. (2020) annotated language instructions based on Matterport3D (Chang et al., 2017) and proposed remote embodied visual referring expression in real 3D indoor environments. Xu et al. (2022) proposed a large-scale 3D synthetic indoor dataset TO-Scene focusing on tabletop scenes. Unlike these datasets, our dataset focuses on a broader category of indoor small objects in real-world 3D scenes. Our dataset is more challenging because small objects are harder to recognize."
        },
        {
            "heading": "2.2 Referring Expression Comprehension",
            "text": "REC is the task of localizing a target object corresponding to a referring expression. In 2D REC (Kazemzadeh et al., 2014; Plummer et al., 2015; Yu et al., 2016; Mao et al., 2016), models find the target object region specified by textual referring expression in an image. Deng et al. (2021) use images with bounding boxes and queries for supervised REC. TransVG (Deng et al., 2021) is a transformer-based framework for 2D visual grounding, outperforming existing one-stage and two-stage methods. These fully supervised REC, However, depends on large annotated datasets. Weakly supervised methods (Liu et al., 2019; Sun et al., 2021) don\u2019t require manually annotated bounding boxes and unsupervised methods (Jiang et al., 2022) that require neither manually annotated bounding boxes nor queries have also been studied. Pseudo-Q (Jiang et al., 2022) proposed a method for generating pseudo queries with objects, attributes, and spatial relationships as key components, outperforming the weakly supervised\nmethods.\nRecently, pre-training on large vision-andlanguage datasets become popular in imageunderstanding tasks. Many existing 2D REC methods (Li* et al., 2022; Yang et al., 2022; Subramanian et al., 2022; Kamath et al., 2021; Wang et al., 2022) relied on some pre-trained models. MDETR (Kamath et al., 2021) is an end-to-end text-modulated detector derived from DETR (Carion et al., 2020) and achieved good performances on scene understanding tasks. OFA (Wang et al., 2022) is a unified sequence-to-sequence pre-trained model that unifies multiple multi-modal tasks such as image captioning, VQA, visual grounding, and text-to-image generation. OFA achieved state-of-the-art performances on several vision-andlanguage tasks, including the REC. In addition to 2D REC, Video-REC (Li et al., 2017; Chen et al., 2019) become a major task. First-person vision REC of RefEgo (Kurita et al., 2023) shares the similar difficulties with 3D REC problems. Both OFA and MDETR are utilized in Kurita et al. (2023).\nCompared to 2D and video REC, 3D REC is an emerging task. Two-stage (Chen et al., 2020; Zhao et al., 2021; Yuan et al., 2021) and singlestage (Luo et al., 2022) methods have been proposed for 3D REC. Two-stage methods generate object proposals and then match them with the query. These methods have the disadvantage that they don\u2019t take the query into account when generating object proposals. To address this disadvantage, single-stage methods conduct language-aware\nkey point selection. Other approaches (Chen et al., 2022; Jain et al., 2022; Wu et al., 2022) have been proposed for further improvements of the matching. D3Net (Chen et al., 2022) unified dense captioning and REC in a self-critical manner. In this study, we adapt two-stage models for the proposed task."
        },
        {
            "heading": "3 Dataset",
            "text": "We describe the methods to construct the ARKitSceneRefer dataset in this section."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "We construct the ARKitSceneRefer dataset based on ARKitScenes (Baruch et al., 2021), which is a large-scale 3D indoor scene dataset. ARKitScenes has comparably higher resolutions in 3D scenes, and thus it is suitable for our task that targets small object localization. Note that, in 3D scenes, some small objects often become unclear and difficult to recognize. However, most of them can be clearly detected and classified in corresponding 2D images. The performance of object detection in 2D images has been improved significantly, making it possible to find small objects in 2D images. Therefore, in this study, we detect target objects in the video frames (i.e., 2D images) where the 3D scene is constructed and then localize them in 3D scenes. Figure 2 shows our dataset construction pipeline. In the following subsections, we describe each step in detail."
        },
        {
            "heading": "3.1.1 Object Detection",
            "text": "In this step, we detect objects in video frames from which the 3D scene is constructed. The results of object detection are used to select target objects in the dataset. Detic (Zhou et al., 2022) is used as the object detection model. Detic is trained on both object detection and image classification datasets. With that, Detic expands the vocabulary of object detection, reduces the performance gap between rare classes and all classes in standard LVIS (Gupta et al., 2019) benchmarks, and achieves state-of-theart performance. Detic provides more detailed class information than conventional models trained on MSCOCO (Chen et al., 2015), which is helpful for the next class selection step, and the instance segmentation corresponding to each 2D bounding box, which is helpful for obtaining 3D bounding boxes. We used pre-trained Detic in LVIS, MSCOCO, and ImageNet-21K.1 Because the same object appears in chronologically close video frames, it is unnecessary to perform object detection on all frames. Therefore, we conduct object detection only for 1/10 frames uniformly sampled from all frames."
        },
        {
            "heading": "3.1.2 Projection of 2D Objects to 3D",
            "text": "After we obtain the bounding box of objects detected by Detic, we project the position of the object point in video frames into the world coordinate of the 3D space using the provided intrinsic and extrinsic parameters of ARKitScenes. We first project the camera position for each video frame into the world coordinate with the extrinsic parameters. The position of the detected objects is then projected by the intrinsic parameters of the camera. Here, two problems exist: the distance between the camera and the object remains unknown, and the projections of the same object from multiple video frames don\u2019t always converge on a single point because of the noise of the projection parameters. It is also important to isolate bounding boxes for different objects because the Detic results often contain multiple same-class-label objects in a video frame.\nTherefore, we apply the ray-casting and simple clustering-based approach for summarizing multiple projections for a single object in the 3D scene. We use the ray-casting from the camera point to the ARKitScenes mesh to obtain the first intersection of the mesh and the ray to the target object. Here we use the line from the camera point to the center\n1https://dl.fbaipublicfiles.com/detic/Detic_ LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth\nof the bounding boxes as the \u201cray.\u201d By doing so, we obtain multiple ray-mesh intersections for each scene. We then make clusters by DBSCAN (Ester et al., 1996) to summarize the intersections and create object points in 3D space. For clustering, we make clusters for the same class-label objects. We also impose a threshold of 0.05m of distance for making a single cluster to DBSCAN to keep the resolution of small objects in scenes. As a result of clustering, we obtain 68.25 clusters in a single scene on average on the validation set. Note that we didn\u2019t use the clusters that consist of fewer numbers of intersections for further analyses and annotations.\nFinally, we use 2D instance segmentations in the images corresponding to each 3D object point to assign 3D bounding boxes. Similar to obtaining 3D object points, we use ray-casting for each pixel of the instance segmentation to project it onto the 3D scene. To reduce computational complexity, ray casting is only conducted for pixels whose positions are divisible by 5. To eliminate noise, the top 5% and bottom 5% of the projected instance segmentation coordinates are removed, and the minimum and maximum coordinates within that range are considered as the 3D bounding box. Note that the 3D bounding boxes are assumed to be parallel to each axis because an object rotation is not taken into account in ARKitSceneRefer."
        },
        {
            "heading": "3.1.3 Object Selection",
            "text": "The object selection step consists of three sub-steps: class selection, scene selection, and target object selection. Firstly, as large objects and small objects are mixed in the results of object detection, we conduct class selection to select small objects. We choose the class of small objects based on the criteria that they can be grasped with both hands (e.g., coffee maker, laptop computer, and microwave oven). As a result of object selection, the number of object classes decreases from the Detic object detection results of 1, 151 to 794. Next, we conduct scene selection to guarantee the diversity of 3D scenes. We select one scene per room. In the same room, the scene with the largest number of objects and more than 20 objects is selected. As a result, the number of scenes decreases from 5, 047 to 1, 615. Finally, we conduct target object selection to decide on the objects to be annotated. The number of target objects is set to 20 for each 3D scene. In order to increase the number of objects in rarely detected classes, instead of choosing tar-\nget objects randomly, we select objects from lowfrequent classes to high-frequent classes according to the detected class frequency. As a result, the number of the entire object class of ARKitSceneRefer becomes 612."
        },
        {
            "heading": "3.1.4 Annotation",
            "text": "With the above steps, we obtain the position information of small objects from 3D scenes without any human annotations. Then we annotate referring expressions on Amazon Mechanical Turk as the last step for constructing our dataset. Before the annotation, we conduct a qualification test and create a worker pool to ensure the quality of our dataset. In the annotation website, We present workers a 3D scene viewer with 3D bounding boxes made in Sec. 3.1.2, three images containing the target object with 2D bounding boxes detected by Detic, and the class of the target object. The images are randomly selected from video frames containing large detected bounding boxes from target objects. A 2D bounding box for the target object is shown in each image, aiming to make it easier for workers to recognize the location of the target object. The 3D bounding boxes of all objects and all target objects of each task are shown in the 3D scene so that workers can clearly recognize the target object and other objects. While referring to the 3D scene and 2D images with target objects, workers\nare asked to annotate referring expressions for the target objects; the referring expression for a target object must be clear enough to distinguish the target object from other objects. One task contains five target objects. Workers are restricted to countries of USA, Canada, UK, and Australia. Workers are also asked to choose 3D bounding boxes in terms of whether their position is almost correct as 3D bounding boxes sometimes become noisy. As a result, workers answered 48.15% of the 3D bounding boxes as correct. The wrong bounding boxes are excluded from the final dataset. Furthermore, as the class obtained from Detic is sometimes wrong, the workers are asked to choose the correct class in the case that the class originally shown in the interface was wrong. The class vocabulary used for class correction is also LVIS, which is the same as the one used in Detic. As a result, workers modified 40.72% classes throughout the annotation tasks, and the number of classes in our dataset became 583. Note that 4.93% small objects wrongly detected by Detic are modified by the worker to larger object classes, and thus not all annotated target objects belong to small object classes.2"
        },
        {
            "heading": "3.2 Dataset Statistics",
            "text": "Finally, we collected 9.69 objects on average for each scene in 1, 605 3D scenes in ARKitScenes and 15, 553 referring expressions for these objects. Each object has a 14.43 average length of the referring expression. The referring expression covers 538 classes of indoor objects. Table 1 shows a comparison of our data with existing 3D referring expression datasets. Our dataset is significantly larger than existing datasets in two aspects, 3D scenes, and object classes. Figure 3 shows distributions of the number of objects and average volumes of each class in major 30 classes comparing our dataset with ScanRefer. We can see that ScanRefer includes many object classes for relatively large objects, such as \u201ctable\u201d and \u201cdoor.\u201d Compared to ScanRefer, ARKitSceneRefer includes many object classes for small objects, such as \u201cbottle\u201d and \u201cbox.\u201d Moreover, the volumes in ARKitSceneRefer are not more than 0.10m3, while the volumes in ScanRefer are significantly greater than 0.10m3. The distribution indicates that our dataset is successfully focused on small objects. Figure 4 shows the most commonly used words for nouns, verbs, adjectives, and adverbs classified by NLTK (Loper\n2See Appendix A for our annotation interface.\nand Bird, 2002). In our dataset, \u201cwall\u201d and \u201csink\u201d are commonly used as nouns, \u201changing\u201d and \u201csitting\u201d as verbs, \u201cwhite\u201d and \u201cblack\u201d as adjectives, and \u201cclose\u201d and \u201cdirectly\u201d as adverbs. Note that NLTK rarely fails to classify the part of speech, such as \u201coven\u201d classified into adverbs. We further split our dataset into training, validation, and test sets. Table 2 shows the statistics of our dataset after the split.3"
        },
        {
            "heading": "4 Model",
            "text": "Following the previous 3D referring expression studies (Chen et al., 2020; Zhao et al., 2021; Luo et al., 2022), we compare 2D to 3D REC models.4"
        },
        {
            "heading": "4.1 2D Models",
            "text": "Our 2D models are based on MDETR (Kamath et al., 2021) and OFA (Wang et al., 2022), which are state-of-the-art 2D REC models. We first apply 2D REC models, which take a video frame and a referring expression as input and predict the bounding box corresponding to the referring expression in the video frame. Then the centers of the predicted bounding boxes in video frames are projected onto the 3D scene and clustered using\n3More details are provided in Appendix B and C. 4See Appendix D for a formulation of the task.\nthe same method presented in Sec. 3.1.2. Finally, the center of the cluster with the most points is regarded as the center of the predicted target object on the 3D scene. Note that 2D models can\u2019t predict 3D bounding boxes because these models don\u2019t generate 2D instance segmentation maps."
        },
        {
            "heading": "4.2 3D Models",
            "text": "Our 3D models are based on ScanRefer (Chen et al., 2020) and 3DVG-Transformer (Zhao et al., 2021), which are state-of-the-art 3D REC models. We customize both ScanRefer and 3DVGTransformer to fit to our task. Specifically, we don\u2019t adopt the vote regression loss introduced in ScanRefer because there are no fine-grained instance segmentation labels in ARKitScenes, which means we define the object detection loss Ldet as Ldet = 0.5Lobjn-cls + Lbox + 0.1Lsem-cls, where Lobjn-cls, Lbox, and Lsem-cls respectively represent the objectness binary classification loss, box regression loss, and semantic classification loss, all of which are introduced in ScanRefer. Our loss function is defined as followings:\nL = \u03b1Lloc + \u03b2Ldet + \u03b3Lcls (1)\nwhere Lloc and Lcls respectively represent the localization loss and the language-to-object classification loss, all of which are introduced in ScanRefer, and \u03b1, \u03b2, and \u03b3 represent the weights for each loss. Note that the loss function of 3DVG-Transformer are based on ScanRefer, but the weights are customized. We use the same weights introduced in ScanRefer and 3DVG-Transformer."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Evaluation Metrics",
            "text": "Following ScanRefer and Refer360\u00b0 (Cirik et al., 2020), we employ two evaluation metrics. The first metric is IoU@k, where the predicted 3D bounding box is considered correct if its Intersection over Union (IoU) with the ground truth 3D bounding box is equal to or greater than the threshold value k. This metric has been widely adopted in existing studies on REC. We set the threshold values k to 0.05, 0.15, 0.25, and 0.5. The second metric is Dist@l, which considers the predicted 3D bounding box as correct if the distance between its center and the center of the ground truth 3D bounding box is equal to or less than the threshold value l. We use threshold values l of 0.1, 0.3, and 0.5. Note that units of IoU@k and Dist@l are percentiles."
        },
        {
            "heading": "5.2 Settings",
            "text": "2D Models We used MDETR and OFAlarge finetuned on RefCOCOg (Mao et al., 2016). We compared the following methods:\n\u2022 MDETR-random and OFA-random: We randomly sampled input 1/10 video frames used to construct the 3D scene of ARKitScenes. Note that the target object may not appear in the randomly sampled video frames. If the target object is not contained in a video frame, the 2D REC models may localize irrelevant regions, leading to noises.\n\u2022 OFA-Detic: This is a heuristic-based method. OFA-Detic conducted object detection on video frames by Detic, and then used only video frames that contained the detected class appearing in the referring expression. If no class is included in the referring expression, we used the same randomly sampled video frames as OFA-random. Note that as we also used Detic for dataset construction, this method is biased. We leave the comparison of using other object detectors for video frame selection as future work.\nWe used DBSCAN (Ester et al., 1996) algorithm for clustering. We set the maximum distance between points in a cluster to 0.02m, and the minimum number of points that make up a cluster to 1.\n3D Models We used NLTK (Loper and Bird, 2002) for tokenization. We used GloVe (Pennington et al., 2014) to convert tokens in a referring\nexpression to word embeddings. Then all word embeddings in the referring expression are concatenated and input to the 3D models. ScanRefer was trained for 200 epochs on the batch size of 32, and 3DVG-Transformer was trained for 500 epochs on the batch size of 8. The initial learning rate was 1e\u22123, and AdamW (Loshchilov and Hutter, 2019) was used for optimization. Following ScanRefer, we applied random flipping, random rotation in the range of [\u22125\u00b0,5\u00b0], random scaling in the range of [e\u22120.1, e0.1], and random translation in the range of [-0.5m, 0.5m] to point clouds for data augmentation. The input features for the 3D models were xyz coordinates, colors, and normals, where the number of vertices in the point cloud was 200, 000. All experiments were conducted on 1 NVIDIA A100 GPU."
        },
        {
            "heading": "5.3 Quantitative Analysis",
            "text": "Tables 3 and 4 present the evaluation results of 2D and 3D models on ARKitSceneRefer, respectively.5\nIoU For 3D models, 3DVG-Transformer outperformed ScanRefer by a large margin. However, both of these models achieved lower performance than that on previous datasets. For example, in terms of IoU@0.25, 3DVG-Transformer achieved 45.90% on the ScanRefer validation set while only 2.21% on our validation set, which suggested that our dataset is insanely difficult compared to the existing datasets.\nDist Comparing 2D models of MDETR-random and OFA-random to 3D models for Dist@0.1 and Dist@0.3, MDETR-random and OFA-random outperformed 3D models. However, 3D models were comparable to 2D models for Dist@0.5. This is because the 3D models can make predictions based on the entire 3D scene. Even if the target object is not recognizable, the approximate position can be guessed. OFA-Detic significantly outperformed all\n5More discussions can be found in Appendix E.\nother methods, indicating the importance of selecting video frames that contain the target object for the 2D model."
        },
        {
            "heading": "5.4 Qualitative Analysis",
            "text": "Figure 5 shows the comparison of localization results of 3DVG-Transformer and OFA-Detic. In the leftmost example, both 3DVG-Transformer and OFA-Detic successfully localized the referred object. Relatively large objects that are unique in the scene were easy to localize accurately. However, in the second example from the left, only 3DVGTransformer successfully localized the referred object. This suggests that 2D models, which rely on local information from video frames, struggled to consider the entire 3D scene simultaneously, resulting in overlooking relatively small objects. In the third example from the left, only OFA-Detic successfully localized the referred object. This suggests that 3D localizing models faces difficulties in accurately localizing quite small objects such as bottles. In the rightmost example, both 3DVGTransformer and OFA-Detic failed to localize the referred object. This suggests that objects in complicated scenes are still difficult to localize even with current best models."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduced a new 3D REC dataset, ARKitSceneRefer, for small objects. ARKitSceneRefer consists of 15, 553 referring expressions for 1, 605 scenes in ARKitScenes. We found that conventional 3D models cannot get high accuracy on our dataset. We also confirmed that the performance of the 2D models varied significantly depending on the input video frames. In the future, we plan to use the confidence scores of 2D models for image selection. We hope that our dataset will be useful in the 3D REC community."
        },
        {
            "heading": "7 Limitations",
            "text": "Dataset ARKitSceneRefer only provides one referring expression per object, which is less than in previous works. Additionally, some objects in\nthe 3D scenes of ARKitScenes fail to reconstruct accurately, which is common to ScanNet, resulting in missing parts or low resolution.\nHuman Annotation To ensure the quality of the dataset, we conducted a qualification test to gather highly skilled workers. However, still, subjectivity rarely leads to occasional errors in human annotations. Particularly in this paper, selecting accurate 3D bounding boxes is susceptible to such influences.\n2D Models In this paper, we utilized off-the-shelf 2D models that were fine-tuned on RefCOCOg. These models already exhibit impressive performance, but we can expect further improvement on our task by fine-tuning them on our dataset. In our experiments, we employed simple heuristic video frame selection methods. It can potentially enhance accuracy to implement more optimized video frame selection methods tailored to our task.\n3D Models ARKitScenes lacks semantic segmentation maps, which leads to the omission of the vote regression loss employed by ScanRefer and 3DVG-Transformer. Consequently, in our experiments, there is a possibility that the object detector is not fully optimized. However, there has been significant progress in recent research on 3D scene understanding (Peng et al., 2023; Yang et al., 2023). Leveraging these advancements to generate highquality pseudo-labels could improve 3D model performance."
        },
        {
            "heading": "8 Ethical Statements",
            "text": "The annotations were conducted on Amazon Mechanical Turk, and we ensured that the workers received fair compensation, taking into account market rates. As we utilized existing data and didn\u2019t collect any newly scanned data, the workers\u2019 privacy can be well protected. The annotation process was conducted in compliance with the procedures established by our institution."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by JSPS KAKENHI Grant Number JP23H03454 and JP22K17983, and by JST PRESTO Grant Numjber JPMJPR20C2."
        },
        {
            "heading": "A Mturk Annotation Interface",
            "text": "Figure 6 present an example of our annotation interface. Workers can see 3D scenes, object classes, and 3D bounding boxes. They can also rotate and zoom 3D scenes interactively."
        },
        {
            "heading": "B Unique/Multiple Objects",
            "text": "One of the major challenges for 3D REC is multiple objects with the same class as the target object can appear in the same scene. Basically, it\u2019s hard to determine unique/multiple objects precisely because we sampled target objects before revising classes from Detic. Instead, we can determine unique/multiple objects on the dataset before the target object selection, which means the analysis is a little noisy. We confirmed that class-unique and class-multiple objects in the whole dataset are 61.5% and 38.5%, respectively. As a result, the model should focus on not only object words but also other descriptions (e.g., other objects and relationships) in order to achieve higher scores. Furthermore, the performances of unique and multiple objects on the test set by 3DVG-Transformer were 1.75% and 2.16% on IoU@0.25 and 12.69% and 13.00% on Dist@0.3, respectively. It might be because of the difference in the amount of the data. Specifically, we selected low-frequency objects so that our dataset could cover extensive object classes, leading to the fact that the average number of objects in the whole dataset was 94.11 for class-unique and 199.45 for class-multiple, respectively."
        },
        {
            "heading": "C Human Score on the Small Dataset",
            "text": "To verify the dataset quality, we randomly tested 50 samples from val and test datasets. We carefully checked whether the objects were detectable and classified them into five categories: (i) we can localize objects in 3D scenes without video frames (ii) we can localize objects in 3D scenes referring to video frames. (iii) we can\u2019t localize objects because of the ambiguity of referring expressions. (iv) we can\u2019t localize objects because of the incorrectness of bounding boxes. (v) It\u2019s hard to localize objects. We confirmed that 29 of 50 objects can be localized by referring expressions and 3D scenes. Furthermore, an additional five objects can be localized by using video frames. This result indicated that the performances of 3D REC models were much lower than those of humans. We also confirmed that 42\nof 50 objects ((i)+(ii)+(iii)) are detectable, which certified the quality of small object representations. Although ARKitScenes provides high-quality 3D scenes, we didn\u2019t use them in this paper because of the lack of computational resources. Indeed, we confirmed that we would not be able to see high-quality 3D scenes on the web browser. Moreover, not only our paper but also other papers (e.g., ScanRefer and 3DVG-Transformer) adopted downsampling of 3D scenes before feeding them into models to reduce the GPU memory. We believe that future improvements in computational resources would make it possible to handle high-quality 3D scenes while we can use the same annotations of ARKitSceneRefer."
        },
        {
            "heading": "D Task",
            "text": "We introduce a text-based small object localization task in 3D scenes. Our task is completely different from existing 3D referring expression tasks in terms of the object size, which means existing tasks mostly focus on large objects. In our task, the input of the 3D REC model is a fine-grained 3D indoor scene and a referring expression that clearly describes the target object. 3D scenes are represented by xyz coordinates, colors, and normals. The model predicts the 3D bounding box of the target object as:\n3DVGsmall(Scene,Query) = Box (2)\nwhere Box \u2208 R6 represents the xyz coordinates, height, width, and depth of the 3D bounding box."
        },
        {
            "heading": "E Discussion",
            "text": "Number of Points on 3D Scenes When the 3D models localize small objects, the number of vertices in the point cloud should be a very important parameter compared to localizing large objects. As shown in Table 5, we investigated how the number of vertices in the point cloud affects the performance. We used 3DVG-Transformer because its performance was better than ScanRefer. However, the performance was comparable as we reduced the number of vertices. This is because the object detectors employed by 3D models were not optimized because of the lack of semantic segmentation maps.\n3D Features Chen et al. (2020) found that using color and normal information improves performance in 3D models. Therefore, we conducted experiments to verify this claim in our task. We\ntrained the model with four features: (i) coordinates (xyz), (ii) coordinates and colors (xyz+rgb), (iii) coordinated and normals (xyz+normal), (iv) coordinates, colors, and normals (xyz+rgb+normal), for the 3DVG-Transformer model. As shown in Table 6, rgb features were not effective in our task. This is because our dataset focuses on small objects and handles a wide range of object classes, making it more difficult to associate rgb features with objects. By contrast, normal features were slightly effective in our task.\nAdditionally, we compared 3DVG-Transformer trained with coordinates, colors, and normals (xyz+color+normal) with a model trained with coordinates, colors, normals, and multiview features (xyz+color+normal+multivew). For the reduction of computational costs, we changed the number of points, epochs, and object proposals from 200, 000 to 25, 000, from 500 to 200, and from 1, 024 to 256, respectively. As shown in Table 7, 3DVG-Transformer (xyz+color+normal) was comparable to 3DVGTransformer (xyz+color+normal+multiview). This showed that the importance of 3D scenes might be comparable with video frames.\nUpper Bound of 2D Models We focused on that MDETR and OFA achieve high accuracy in 2D REC tasks, but the performance is significantly lower in our task. Therefore, we investigated the upper bound of the OFA-based 2D model, which outperforms the MDETR-based model, as shown in Table 3. We further conducted experiments on the following two settings:\n\u2022 OFA-oracle: We conducted object detection on video frames by Detic, and then used only video frames with the detected class corresponding with the one annotated in the ARKitSceneRefer.\n\u2022 OFA-upper: We used only the video frames used for annotating referring expressions by crowdsourcing workers.\nAs shown in Table 8, OFA-oracle and OFA-upper were superior to OFA-random significantly. OFA-\noracle was slightly superior to OFA-Detic because many referring expressions include the object class in themselves. The results showed that if we use video frames with target objects, the model can be further improved in our task."
        }
    ],
    "title": "ARKitSceneRefer: Text-based Localization of Small Objects in Diverse Real-World 3D Indoor Scenes",
    "year": 2023
}