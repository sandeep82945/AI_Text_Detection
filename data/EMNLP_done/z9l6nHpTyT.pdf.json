{
    "abstractText": "Adapting a large language model for multipleattribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task. In this paper, we address this challenge by introducing Adapter-TST, a framework that freezes the pre-trained model\u2019s original parameters and enables the development of a multiple-attribute text style transfer model. Using BART or T5 as the backbone model, Adapter-TST utilizes different neural adapters to model different types of attribute information, similar to a plug-in connected to the base model. Our method allows control over multiple attributes (e.g. sentiment, tense, active or passive voice) and configures the adapters\u2019 architecture to generate multiple outputs in respect to attributes or compositional editing on the same sentence. We evaluate the proposed model on both traditional sentiment transfer and multiple-attribute transfer tasks. The experiment results demonstrate that Adapter-TST outperforms all the state-of-the-art baselines with significantly less computational resources. We have also empirically shown that each adapter is able to characterize specific stylistic attributes effectively and can be configured to perform compositional editing. The code and datasets can be found in https://github. com/Social-AI-Studio/Adapter-TST.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiqiang Hu Roy"
        },
        {
            "affiliations": [],
            "name": "Ka-Wei Lee"
        },
        {
            "affiliations": [],
            "name": "Nancy F. Chen"
        }
    ],
    "id": "SP:eaa236255c723b360ebf6e043774d6d4af6dd8ce",
    "references": [
        {
            "authors": [
                "Liqun Chen",
                "Shuyang Dai",
                "Chenyang Tao",
                "Haichao Zhang",
                "Zhe Gan",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Guoyin Wang",
                "Ruiyi Zhang",
                "Lawrence Carin."
            ],
            "title": "Adversarial text generation via featuremover\u2019s distance",
            "venue": "Advances in Neural Information",
            "year": 2018
        },
        {
            "authors": [
                "Ning Dai",
                "Jianze Liang",
                "Xipeng Qiu",
                "Xuan-Jing Huang."
            ],
            "title": "Style transformer: Unpaired text style transfer without disentangled latent representation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5997\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zhenxin Fu",
                "Xiaoye Tan",
                "Nanyun Peng",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Style transfer in text: Exploration and evaluation",
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence.",
            "year": 2018
        },
        {
            "authors": [
                "Navita Goyal",
                "Balaji Vasan Srinivasan",
                "Anandhavelu N",
                "Abhilasha Sancheti."
            ],
            "title": "Multi-style transfer with discriminative feedback on disjoint corpus",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Bin He",
                "Di Zhou",
                "Jinghui Xiao",
                "Qun Liu",
                "Nicholas Jing Yuan",
                "Tong Xu"
            ],
            "title": "Integrating graph contextualized knowledge into pre-trained language models. arXiv preprint arXiv:1912.00147",
            "year": 2019
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqiang Hu",
                "Roy Ka-Wei Lee",
                "Charu C Aggarwal",
                "Aston Zhang."
            ],
            "title": "Text style transfer: A review and experimental evaluation",
            "venue": "ACM SIGKDD Explorations Newsletter, 24(1):14\u201345.",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqiang Hu",
                "Roy Kaa-Wei Lee",
                "Nancy F Chen"
            ],
            "title": "2022b. Are current task-oriented dialogue systems able to satisfy impolite users? arXiv preprint arXiv:2210.12942",
            "year": 2022
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Zhiting Hu",
                "Olga Vechtomova",
                "Rada Mihalcea."
            ],
            "title": "Deep learning for text style transfer: A survey",
            "venue": "Computational Linguistics, 48(1):155\u2013205.",
            "year": 2022
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751.",
            "year": 2014
        },
        {
            "authors": [
                "Chih-Te Lai",
                "Yi-Te Hong",
                "Hong-You Chen",
                "Chi-Jen Lu",
                "Shou-De Lin."
            ],
            "title": "Multiple text style transfer by using word-level conditional generative adversarial network with two-phase training",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
            "year": 2019
        },
        {
            "authors": [
                "Guillaume Lample",
                "Sandeep Subramanian",
                "Eric Michael Smith",
                "Ludovic Denoyer",
                "Marc\u2019Aurelio Ranzato",
                "Y-Lan Boureau"
            ],
            "title": "Multipleattribute text rewriting",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Anne Lauscher",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Anna Korhonen",
                "Goran Glava\u0161"
            ],
            "title": "Informing unsupervised pretraining with external linguistic knowledge",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Dianqi Li",
                "Yizhe Zhang",
                "Zhe Gan",
                "Yu Cheng",
                "Chris Brockett",
                "Bill Dolan",
                "Ming-Ting Sun."
            ],
            "title": "Domain adaptive text style transfer",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Juncen Li",
                "Robin Jia",
                "He He",
                "Percy Liang."
            ],
            "title": "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Wei Liu",
                "Xiyan Fu",
                "Yue Zhang",
                "Wenming Xiao."
            ],
            "title": "Lexicon enhanced Chinese sequence labeling using BERT adapter",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Lajanugen Logeswaran",
                "Honglak Lee",
                "Samy Bengio."
            ],
            "title": "Content preserving text generation with attribute controls",
            "venue": "Advances in Neural Information Processing Systems, pages 5103\u20135113.",
            "year": 2018
        },
        {
            "authors": [
                "Fuli Luo",
                "Peng Li",
                "Jie Zhou",
                "Pengcheng Yang",
                "Baobao Chang",
                "Zhifang Sui",
                "Xu Sun."
            ],
            "title": "A dual reinforcement learning framework for unsupervised text style transfer",
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yiwei Lyu",
                "Paul Pu Liang",
                "Hai Pham",
                "Eduard Hovy",
                "Barnab\u00e1s P\u00f3czos",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "StylePTB: A compositional benchmark for fine-grained controllable text style transfer",
            "venue": "Proceedings of the 2021 Conference",
            "year": 2021
        },
        {
            "authors": [
                "Aman Madaan",
                "Amrith Setlur",
                "Tanmay Parekh",
                "Barnabas Poczos",
                "Graham Neubig",
                "Yiming Yang",
                "Ruslan Salakhutdinov",
                "Alan W Black",
                "Shrimai Prabhumoye."
            ],
            "title": "Politeness transfer: A tag and generate approach",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "Adapterhub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Shrimai Prabhumoye",
                "Yulia Tsvetkov",
                "Ruslan Salakhutdinov",
                "Alan W Black."
            ],
            "title": "Style transfer through back-translation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866\u2013876.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Blog, 1(8):9.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Sudha Rao",
                "Joel Tetreault."
            ],
            "title": "Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "Advances in neural information processing systems, pages 6830\u20136841.",
            "year": 2017
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour."
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in neural information processing systems, 12.",
            "year": 1999
        },
        {
            "authors": [
                "Bakhtiyar Syed",
                "Gaurav Verma",
                "Balaji Vasan Srinivasan",
                "Anandhavelu Natarajan",
                "Vasudeva Varma."
            ],
            "title": "Adapting language models for non-parallel authorstylized rewriting",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
            "year": 2020
        },
        {
            "authors": [
                "John Vineet",
                "Lili Mou",
                "Hareesh Bahuleyan",
                "Olga Vechtomova."
            ],
            "title": "Disentangled representation learning for non-parallel text style transfer",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 424\u2013434.",
            "year": 2019
        },
        {
            "authors": [
                "Ruize Wang",
                "Duyu Tang",
                "Nan Duan",
                "Zhongyu Wei",
                "Xuanjing Huang",
                "Jianshu Ji",
                "Guihong Cao",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
            "venue": "Findings of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yunli Wang",
                "Yu Wu",
                "Lili Mou",
                "Zhoujun Li",
                "Wenhan Chao."
            ],
            "title": "Harnessing pre-trained neural networks with rules for formality style transfer",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Jingfei Du",
                "William Yang Wang",
                "Veselin Stoyanov."
            ],
            "title": "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model",
            "venue": "arXiv preprint arXiv:1912.09637.",
            "year": 2019
        },
        {
            "authors": [
                "Di Yin",
                "Shujian Huang",
                "Xin-Yu Dai",
                "Jiajun Chen."
            ],
            "title": "Utilizing non-parallel text for style transfer by making partial comparisons",
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 5379\u20135386. AAAI Press.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "ERNIE: Enhanced language representation with informative entities",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Zhirui Zhang",
                "Shuo Ren",
                "Shujie Liu",
                "Jianyong Wang",
                "Peng Chen",
                "Mu Li",
                "Ming Zhou",
                "Enhong Chen."
            ],
            "title": "Style transfer as unsupervised machine translation",
            "venue": "arXiv, pages arXiv\u20131808.",
            "year": 2018
        },
        {
            "authors": [
                "Jake Zhao",
                "Yoon Kim",
                "Kelly Zhang",
                "Alexander M Rush",
                "Yann LeCun."
            ],
            "title": "Adversarially regularized autoencoders",
            "venue": "35th International Conference on Machine Learning, ICML 2018, pages 9405\u20139420. International Machine Learning Society (IMLS).",
            "year": 2018
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Duyu Tang",
                "Jiahai Wang",
                "Jian Yin",
                "Nan Duan."
            ],
            "title": "UserAdapter: Few-shot user learning in sentiment analysis",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1484\u20131488, Online. Association for",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Adapting a large language model for multipleattribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task. In this paper, we address this challenge by introducing Adapter-TST, a framework that freezes the pre-trained model\u2019s original parameters and enables the development of a multiple-attribute text style transfer model. Using BART or T5 as the backbone model, Adapter-TST utilizes different neural adapters to model different types of attribute information, similar to a plug-in connected to the base model. Our method allows control over multiple attributes (e.g. sentiment, tense, active or passive voice) and configures the adapters\u2019 architecture to generate multiple outputs in respect to attributes or compositional editing on the same sentence. We evaluate the proposed model on both traditional sentiment transfer and multiple-attribute transfer tasks. The experiment results demonstrate that Adapter-TST outperforms all the state-of-the-art baselines with significantly less computational resources. We have also empirically shown that each adapter is able to characterize specific stylistic attributes effectively and can be configured to perform compositional editing. The code and datasets can be found in https://github. com/Social-AI-Studio/Adapter-TST."
        },
        {
            "heading": "1 Introduction",
            "text": "Motivation. Text style transfer (TST) is a popular natural language generation task that aims to change the stylistic properties (e.g., sentiment, formality, tense, voice) of the text while preserving the style-independent content (Hu et al., 2022a). Existing studies explore performing text style transfer on attributes like age, or gender (Lample et al., 2019), sentiment (Li et al., 2018; Luo et al., 2019; Fu et al., 2018), formality (Rao and Tetreault, 2018), politeness (Madaan et al., 2020; Hu et al., 2022b), and\nauthor writing style (Syed et al., 2020). Nevertheless, most of the existing TST studies are confined to single-attribute TST tasks.\nFew works have explored multiple-attribute TST tasks, where TST models are designed to control and transfer text in multiple target stylistic attributes. Lample et al. (2019) attempts style transfer with multiple attributes by conditioning on the average embedding of each target attribute and using a combination of denoising autoencoder (DAE) and back-translation techniques. Goyal et al. (2021) propose an approach to initialize an encoderdecoder setup with a transformer-based language model that is pre-trained on a generic corpus and enhances its capability of re-writing to multiple target style dimensions by utilizing multiple style-\naware language models as discriminators.\nA possible approach to perform single and multiple attribute TST tasks is to leverage large pretrained language models (PLMs). The PLMs have been pre-trained on large corpora, which allows them to capture natural language\u2019s syntactic and semantic information. This characteristic of PLMs makes them well-suited for TST tasks, where the model needs to understand the content and style of the input text. Syed et al. (2020) fine-tune a denoising autoencoder (DAE) for the stylized re-writing task by initializing the encoder and decoder with a pre-trained language model trained on Masked Language Modeling (MLM) objectives (Devlin et al., 2019). Wang et al. (2019) fine-tune GPT-2 model (Radford et al., 2019) using the text formality transfer rules harnessed from analyzing the GYAFC parallel dataset (Rao and Tetreault, 2018). The fine-tuned GPT-2 model was subsequently used to transfer the formality of text (e.g., informal to formal text). However, fine-tuning PLMs for multiple-attribute TST remains challenging as a significant amount of computational resources and style-labeled data are required to perform TST for each stylistic attribute.\nResearch Objectives. To address these research gaps, we propose Adapter-TST, a parameterefficient framework that utilizes BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) as the backbone model and trains neural adapters to capture multiple stylistic attributes for multiple-attribute TST. During the training of Adapter-TST, we freeze the original parameters of the pre-trained BART or T5 model and only update the parameters of adapters to relax the dependence on computational resources and supervised data. The proposed Adapter-TST model is flexible to handle different settings of multiple-attribute TST by configuring the connection method among adapters. Figure 1 illustrates the different settings of multiple-attribute TST tasks. Paralleling the adapters in AdapterTST can generate multiple outputs in the corresponding target style simultaneously (setting b) and stacking the adapters for compositional editing in terms of different target styles at the same time (setting c). We conduct experiments on the traditional sentiment transfer task and multiple-attribute TST tasks, including multiple stylistic attribute outputs and compositional editing. Results of automatic and human evaluations show that AdapterTST can outperform the state-of-the-art baselines\nto transfer and generate high-quality text with lower computational resources.\nContributions. We summarize our contributions as follows: (i) We introduce an AdapterTST, which is a parameter-efficient framework that can perform multiple-attribute TST tasks with significantly lower computational resources. (ii) Included in the Adapter-TST are two TST configurations, parallel and stacking, which support multiple-output TST and compositional editing, respectively. (iii) We conducted extensive experiments on real-world datasets. The automatic and human evaluation results show that Adapter-TST can outperform the state-of-the-art baselines to transfer and generate high-quality text."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Text Style Transfer",
            "text": "TST is an emerging research topic that has garnered attention from computational linguists and computer science researchers. The recent comprehensive survey (Hu et al., 2022a; Jin et al., 2022) summarizes the existing TST approaches.\nWhile the majority of existing studies have focused on performing TST on single attributes such as sentiment (Li et al., 2018; Luo et al., 2019; Fu et al., 2018) or formality (Rao and Tetreault, 2018), recent studies have also explored multiple-attribute TST tasks, where TST models are designed to control and transfer text in multiple target stylistic attributes. Lample et al. (2019) attempts style transfer with multiple attributes by conditioning on the average embedding of each target attribute and using a combination of denoising autoencoder (DAE) and back-translation techniques. Goyal et al. (2021) propose an approach to initialize an encoder-decoder setup with a transformer-based language model that is pre-trained on a generic corpus and enhances its capability of re-writing to multiple target style dimensions by utilizing multiple style-aware language models as discriminators. In this study, we contribute to this limited multipleattribute TST literature by proposing an alternative approach to generate multiple stylistic outputs and perform compositional editing efficiently.\nDue to the lack of parallel training data, most existing TST methods are designed to train with non-parallel style-labeled sentences as input. A popular line of TST approaches aims to disentangle the text\u2019s content and style in the latent space to perform TST (Shen et al., 2017; Zhao et al., 2018;\nFu et al., 2018; Chen et al., 2018; Logeswaran et al., 2018; Yin et al., 2019; Lai et al., 2019; Vineet et al., 2019). Another common approach is to leverage PLMs. For instance, Syed et al. (2020) fine-tune a denoising autoencoder (DAE) for the stylized re-writing task by initializing the encoder and decoder with a pre-trained language model trained on Masked Language Modeling (MLM) objectives (Devlin et al., 2019). Wang et al. (2019) fine-tune GPT-2 model (Radford et al., 2019) using the text formality transfer rules harnessed from analyzing the GYAFC parallel dataset (Rao and Tetreault, 2018). The fine-tuned GPT-2 model was subsequently used to transfer the formality of text (e.g., informal to formal text). However, finetuning PLMs for multiple-attribute TST remains challenging as a significant amount of computational resources is required to perform the task; multiple PLMs need to be fine-tuned for the different attributes to perform multiple-attribute TST. In this study, we overcome this limitation by proposing Adapter-TST, which is a parameter-efficient framework that leverages on PLMs but requires significantly lesser computational resources to perform multiple-attribute TST."
        },
        {
            "heading": "2.2 Adapter-based Models",
            "text": "PLMs, pre-trained on large-scale text corpus with unsupervised objectives, have established state-ofthe-art performances on various NLP downstream tasks. Many studies fine-tune PLMs with language modeling and downstream task objectives to obtain better performance (Zhang et al., 2019b; Lauscher et al., 2019; He et al., 2019; Xiong et al., 2019).To leverage the powerful PLMs more efficiently, Houlsby et al. (2019) add adapter layers, small neural networks, into each transformer layer to obtain near state-of-the-art performance on the GLUE benchmark while updating only the parameters of adapters. Inspired by this work, more adapter-based models (Wang et al., 2021; Liu et al., 2021; Zhong et al., 2021) are proposed to inject task-specific knowledge into PLMs with adapters. Inspired by the adapter architecture, we propose Adapter-TST, which trains different neural adapters to capture different stylistic attributes to perform the multiple-attribute TST. The proposed adapter framework has two configurations that support multiple stylistic attribute outputs and compositional editing."
        },
        {
            "heading": "3 Methodology",
            "text": "This section proposes Adapter-TST, which adds neural adapters into each transformer layer to capture different attribute information for multipleattribute TST. We first introduce the adapter structure used in Adapter-TST and its parameter efficiency. Subsequently, we explain how the adapters are configured for different multiple-attribute TST settings, namely, multiple stylistic attribute outputs and compositional editing. Finally, we describe the training objectives of Adapter-TST."
        },
        {
            "heading": "3.1 Adapter Structure",
            "text": "We present an adapter structure in Figure 2. The adapter consists of a bottleneck that contains few parameters relative to the attention and feedforward layers in the original model. A skip connection is applied across two projection layers. In our proposed Adapter-TST, these adapters will be trained to capture different stylistic attributes. In contrast to Houlsby et al. (2019), adding the adapter module twice to each transformer layer, we propose simplifying the approach by just adding the adapter layer into each transformer once, making our AdapterTST\u2019s architecture more parameter efficient.\nWe use BART-large (24-layer, 1024-hidden, 16- heads, 406M parameters) or T5-large (24-layer, 1024-hidden, 16-heads, 770M parameters) as the backbone model in Adapter-TST. As for each adapter layer, we denote the hidden dimensions of the down-projection and up-projection layers as Hd = 64 and Hu = 1024. The bottleneck adapter layers are plugged into each layer of BART-large or T5-large, and different adapter layers do not share parameters. Thus the total number of parameters\nfor each attribute-specific adapter is about 3.17M, which is only 0.78% of the original BART-large model and 0.41% of the T5-large model, making the training process parameter efficient. Note that the original parameters of BART-large or T5-large are frozen during multiple-attribute TST training, and only the parameters of adapters are trainable and initialized randomly."
        },
        {
            "heading": "3.2 Adapter-TST Configurations",
            "text": "Adapter-TST has two configurations, parallel and stack, which support two multiple-attribute TST task settings: multiple stylistic attribute outputs and compositional editing, respectively. To better understand the two Configurations of Adapter-TST, we take the multiple-attribute TST task with tense and voice attributes as an example. Tense has three attribute values (Future, Past, Present), while Voice has two attribute values (Passive, Active). Thus, we add five attribute-specific adapters Adapter(Future, Past, Present, Passive, Active) to the base model for all the possible attribute values, respectively. Each adapter is employed to learn to generate sentences with corresponding attribute values while preserving the semantic content of the inputs.\nParallel Connection. We define the multiple stylistic attribute outputs as follows: given a sentence x = {x1, ..., xn} with n tokens and ytense, yvoice labels, the Adapter-TST model is required to generate multiple outputs with all possible other attribute values at the same time. For instance, as shown in Figure 1(b), given an input sentence with present tense and active voice, the multiple-attribute TST models need to generate three sentences in the past tense, future tense, and passive voice simultaneously. The multiple stylistic attribute output setting requires TST models to capture all the stylistic attributes and have the capability of performing style transfer among the attribute values. Adapter-TST performs the multiple stylistic attribute output by utilizing the Parallel connection configuration shown in Figure 3(a). Specifically, we plug the paralleled adapters Parallel(Future, Past, Present, Passive, Active) into each transformer layer of the base model. During training, each training sample passes all the attribute-specific adapters, but adapters will take different actions according to the attribute values of input sentences. The adapter learns to reconstruct the input sentence for training samples with the same attribute value as an adapter. Conversely, when training samples with different attribute val-\nues, the adapter learns to transfer the attribute of the input sentence while preserving the original content. The outputs of all the adapters are concatenated together to the next layer. The replication is only performed once in the first transformer layer. In the latter transformer layers, we distribute the hidden states to corresponding adapters to make sure that the input of an adapter in the current layer is the output of the adapter with the same attribute value in the preceding layer.\nStack Connection. Compositional editing requires TST models to change multiple attributes simultaneously while preserving the original content. For instance, as shown in Figure 1(c), given an input sentence with present tense and active voice, the multiple-attribute TST models need to generate one sentence both in future tense and passive voice. Adapter-TST performs compositional editing by using the Stack connection method shown in Figure 3(b), where adapters belonging to the same attribute are parallelized because a sentence should only contain one attribute value for a specific attribute. Specifically, we have Parallel(Future, Past, Present) and Parallel(Passive, Active) for tense and voice attributes. The two sets of paralleled adapters are stacked as Stack(Parallel(Future, Past, Present), Parallel(Passive, Active)) to learn to transfer multiple attributes. Similar to the Parallel connection method, the hidden states are replicated according to the number of adapters in the Parallel connection module. It\u2019s worth noting that, to demonstrate the attribute-specific adapters captured the attribute information, we only use the Stack connection method in inference time. During inference, we reload the parameters of adapters trained in multiple stylistic attribute outputs tasks and change the connection among the adapters to Stack."
        },
        {
            "heading": "3.3 Training Objectives.",
            "text": "The TST task aims to transfer the style of inputs while preserving the original semantic content. Thus, we train Adapter-TST with classification loss Lcls for style transfer and reconstruction Lrec for content preservation. During training, the original parameters of BART-large or T5-large are frozen, and only the parameters of adapters are trainable.\nClassification Loss Lcls: The classification loss ensures that the transferred sentence conforms to the target attribute value. To this end, we first pre-train a TextCNN-based (Kim, 2014) binary at-\ntribute classifier D for each attribute, then apply the pre-trained attribute classifiers to guide the updates of adapters\u2019 parameters such that the output sentence is predicted to be in the target style:\nLcls = \u2212E(x,y)\u223cD[logP (yt|x\u2032)] (1)\nwhere x\u2032 is sampled from the distribution of model outputs at each decoding time step, and yt is the target attribute value. Policy gradient algorithm (Sutton et al., 1999) is used to for discrete training with the attribute classifiers.\nReconstruction Loss Lrec: The reconstruction loss attempts to preserve the original content information in the transferred sentences. Specifically, the loss function constricts the adapters to capture informative features to reconstruct the original sentence using the learned representations. Formally, we define Lrec as follows:\nLrec = \u2212logP (x|zi, yi) (2)\nwhere yi is the i-th attribute value of the input sentence, zi denotes the hidden representation extracted by the corresponding adapter. The input sentences are only reconstructed by the corresponding adapter and transferred by other adapters.\nPutting them together, the final joint training loss L is as follows:\nL = (1\u2212 \u03bb)Lrec + \u03bbLcls (3)\nWhere \u03bb is a balancing hyper-parameter to ensure that the transferred sentence has the target style while preserving the original content."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setting",
            "text": "Datasets. We evaluate the proposed AdapterTST model on sentiment transfer and multipleattribute TST tasks using the Yelp1 and StylePTB (Lyu et al., 2021) datasets, respectively. We adopt the train, development, and test split for the Yelp dataset as (Luo et al., 2019). Lyu et al. (2021) introduce StylePTB 2, a large-scale benchmark with compositions of multiple-attribute TST tasks which allow the modeling of fine-grained stylistic changes. In our experiments, we choose four subsets for multiple-attribute TST: Tense-Voice, TensePP-Front\u2194Back, Tense\u2013PP-Removal, and TenseADJADV-Removal. Specifically, the four subsets\n1https://github.com/luofuli/DualRL 2https://github.com/lvyiwei1/StylePTB\ninclude five attributes, tense with three attribute values (Future, Past, Present), voice with two attribute values (Passive, Active), proposition position with two attribute values (Front, Back), proposition removal with two attribute values (Adding, Removing), and adjectives&adverbs removal with two attribute values (Adding, Removing). Table 1 shows the training, validation, and test splits of the Yelp and StylePTB datasets used in our experiments.\nBaselines. For sentiment transfer, we benchmark Adapter-TST against nine state-of-the-art TST models: BackTrans (Prabhumoye et al., 2018), CrossAlign (Shen et al., 2017), DualRL (Luo et al., 2019), Unpaired (Li et al., 2019), UnsuperMT (Zhang et al., 2018), Style Transformer (Dai et al., 2019), DeleteOnly, Template, and Del&Retri (Li et al., 2018). For multiple stylistic attribute outputs task, Style Transformer (Dai et al., 2019), a transformer-based model for single-attribute TST, is selected as a baseline. We train multiple Style Transformer models for each attribute and perform style transfer separately. For compositional editing, we use the trained Style Transformer models to perform sequential editing, which transfers one attribute after another to compare results with our model. We term this baseline as Sequential Style Transformer setup.\nTraining. The experiments were performed on an Ubuntu 20.04.3 LTS system with 24 cores, 128 GB RAM, and Nvidia RTX 3090. The model implementation is based on AdapterHub (Pfeiffer et al., 2020) and Huggingface Transformers (Wolf et al., 2020). For the balancing hyper-parameter \u03bb, we choose the best-performed one from (0.9, 1) as the BART-large and T5-large models can copy the input without training with TST objectives."
        },
        {
            "heading": "4.2 Automatic Evaluation",
            "text": "We evaluate the proposed model and baselines on three criteria commonly used in TST studies: transfer strength, content preservation, and fluency. An attribute classifier is first pre-trained to predict the attribute label of the input sentence. The classifier is subsequently used to approximate the style trans-\nfer accuracy (ACC) of the sentences\u2019 transferred attributes by considering the target attribute value as the ground truth. To quantitatively measure the amount of original content preserved after style transfer operations, we employ BERTscore (Zhang et al., 2019a) between style-transferred and original sentences. For fluency, We use GPT-2 (Radford et al., 2019) to measure the perplexity (PPL) of transferred sentences. The sentences with smaller PPL scores are considered more fluent. Finally, we compute the geometric mean of ACC, BERTscore, and 1/PPL. We take the inverse of the calculated perplexity score because a smaller PPL score corresponds to better fluency. When there is more than one accuracy in the multiple-attribute TST tasks, we use the average accuracy to compute G-score."
        },
        {
            "heading": "4.3 Automatic Evaluation Results",
            "text": "Table 2 shows the performance of the AdapterTST model and the baselines on the sentiment transfer task. Adapter-TST has achieved the best G-score, outperforming the baselines. We observe that Adapter-TST achieves comparable performance on transfer strength and content preservation with 97.3% transfer accuracy and 0.89 BERTscore by only updating the parameters of adapters. With the impressive generative ability of the pre-trained BART-large and T5-large models, the AdapterTST model can generate high-quality text in terms of fluency and completeness. The experiment results demonstrate Adapter-TST\u2019s ability to perform TST well and efficiently with fewer training parameters.\nTable 3 presents the results of the proposed Adapter-TST model and Style Transformer baselines for the multiple stylistic attribute output task. Our Adapter-TST model achieves the highest G-score across all four datasets by simultaneously modeling multiple attributes using different\nadapters. Adapter-TST performs well in transferring tense attributes, surpassing the baselines on three datasets. However, modeling multiple attributes together proves to be a more challenging task. While Adapter-TST exhibits a slight performance gap compared to the Style Transformer model in terms of transfer accuracy, it excels in generating fluent and coherent sentences while preserving the original content. This advantage allows Adapter-TST to outperform the baselines in content preservation and fluency. It is also worth noting that training multiple Style Transformers for the multiple-attribute TST tasks is computationally inefficient and expensive, unlike Adapter-TST.\nTo demonstrate that the attribute-specific adapters capture the corresponding attribute information, we evaluate the proposed Adapter-TST model on the compositional editing task. Note that the parameters of adapters trained in the multiple stylistic attribute outputs task are reloaded, and the connection method is changed to Stack for compositional editing. Table 4 shows the performance of the Adapter-TST and Sequential Style Transformer on the compositional editing task. The Adapter-TST model achieves the highest G-score across four datasets, similar to the results obtained in the multiple stylistic attribute output task. We observe that the average G-score of the multiple\nstylistic attribute outputs task is 2.24, significantly higher than compositional editing\u2019s average Gscore of 1.89. The difference in the average Gscore highlights the challenge of the compositional editing task. Interestingly, Adapter-TST achieves comparable performance on style transfer accuracy over attributes, indicating that the attribute-specific adapters effectively capture the stylistic attributes."
        },
        {
            "heading": "4.4 Human Evaluation",
            "text": "We conducted a human-based evaluation study to assess the performance of the Adapter-TST model in handling multiple-attribute TST tasks. The study involved randomly sampling 200 sentences from the Tense-Voice dataset. Both Adapter-TST and the baselines were used to generate multiple stylistic attribute outputs and perform compositional editing on the sampled sentences. Two linguistic researchers evaluated the generated sentences based on three criteria used in automated evaluation. To measure transfer strength, evaluators indicated whether the sentences were in the target attribute value (e.g., future tense, passive voice) using a true/false indicator. For content preservation, evaluators rated the amount of preserved content on a 5-point Likert scale, ranging from no content preserved (1) to all content preserved (5). Fluency was assessed on a 5-point Likert scale, where 1 represented unreadable sentences with numerous\ngrammatical errors, and 5 indicated perfect and fluent sentences. To reduce biases, the model names were concealed, and the order of the models was randomized when displaying the generated sentences. This ensured that evaluators were unaware of which model generated the sentences they were evaluating."
        },
        {
            "heading": "4.5 Human Evaluation Results",
            "text": "Table 5 shows the evaluation results. The style transfer accuracy of the models was computed using the binary feedback from the evaluators. The average scores for the criteria of content preservation and fluency were calculated using the 5-point Likert scores. Adapter-TST is observed to outperform the baselines in content preservation, fluency, and G-score. Adapter-TST is also rated to generate more syntactically sound and fluent sentences compared to the baselines. We can also observe that there is still a style transfer accuracy drop of Adapter-TST on attribute Voice when modeling multiple attributes at the same time. These results align with the automatic evaluations and demonstrate Adapter-TST\u2019s effectiveness in performing multiple-attribute TST well and efficiently."
        },
        {
            "heading": "5 Case Study",
            "text": "We conducted case studies to showcase the style transferred outputs of both the Adapter-TST and Style Transformer models. Randomly sampled examples and their corresponding outputs are presented in Table 6, specifically for the Tense-Voice dataset. Our findings reveal that Adapter-TST successfully transfers the style while preserving the content and sentence structure in multiple-attribute TST tasks. In contrast, the Style Transformer model generates sentences with grammatical errors, making it challenging to determine if the style transfer was successful. Moreover, the Style Transformer model performs poorly in the task of compositional editing due to its inherent complexity. Despite the difficulty of compositional editing, Adapter-TST is capable of generating fluent sentences that preserve the original content."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduced a parameter-efficient framework, Adapter-TST with different neural adapters to capture different attribute information for multiple-attribute TST tasks. During training, the original parameters of BART-large were frozen, and only the adapters\u2019 parameters were optimized to relax the dependence on computational resources and supervised data. We conducted extensive ex-\nperiments on traditional sentiment transfer and multiple-attribute TST tasks. The automatic and human-based evaluation results showed that the attribute-specific adapters in Adapter-TST is able to capture relevant stylistic attributes to transfer the style while preserving the original content successfully. Our case studies also demonstrated that Adapter-TST was able to generate high-quality text in the target style. For future work, we will continue to improve TST models\u2019 ability to model multiple attributes in terms of quality and efficiency. We will also explore plugging Adapter-TST on other PLMs and evaluate its effectiveness."
        },
        {
            "heading": "7 Limitations",
            "text": "This work has two limitations. First, there is a style transfer accuracy reduction on one of the attributes, while the proposed model models multiple attributes simultaneously. Explorations on improving TST models\u2019 ability to handle multipleattribute TST tasks and the dependency among attributes are potential directions in this field. Second, even though we have frozen the parameters of the pre-trained BART-large model to improve parameter efficiency, we still need to run BART-large model to extract representations for performing TST tasks."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "The ethical implications of using large language models trained on data containing unchecked biases are acknowledged. As with any generative task, style transfer also has the potential for misuse, such as fact distortion, plagiarism, etc. The paper aims to demonstrate the academic utility of the proposed framework. This solution must be paired with strict checks for misrepresentation, offensiveness, and bias to adhere to ethical standards."
        }
    ],
    "title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer",
    "year": 2023
}