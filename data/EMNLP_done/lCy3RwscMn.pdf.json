{
    "abstractText": "We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these questions. We call this representation Natural Language Learned Features (NLLF). NLLF is generated by a small transformer language model (e.g., BERT) that has been trained in a Natural Language Inference (NLI) fashion, using weak labels automatically obtained from a Large Language Model (LLM). We show that the LLM normally struggles for the main task using in-context learning, but can handle these easiest subtasks and produce useful weak labels to train a BERT. The NLI-like training of the BERT allows for tackling zeroshot inference with any binary question, and not necessarily the ones seen during the training. We show that this NLLF vector not only helps to reach better performances by enhancing any classifier, but that it can be used as input of an easy-to-interpret machine learning model like a decision tree. This decision tree is interpretable but also reaches high performances, surpassing those of a pre-trained transformer in some cases. We have successfully applied this method to two completely different tasks: detecting incoherence in students\u2019 answers to open-ended mathematics exam questions, and screening abstracts for a systematic literature review of scientific papers on climate change and agroecology.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Felipe Urrutia"
        },
        {
            "affiliations": [],
            "name": "Cristian Buc"
        },
        {
            "affiliations": [],
            "name": "Valentin Barriere"
        }
    ],
    "id": "SP:5ea3072cf5ae13f69bd863afb38fd110bae4f95d",
    "references": [
        {
            "authors": [
                "Alejandro Barredo Arrieta",
                "Natalia D\u00edaz-Rodr\u00edguez",
                "Javier Del Ser",
                "Adrien Bennetot",
                "Siham Tabik",
                "Alberto Barbado",
                "Salvador Garc\u00eda",
                "Sergio Gil-L\u00f3pez",
                "Daniel Molina",
                "Richard Benjamins"
            ],
            "title": "Explainable artificial intelligence (xai): Concepts, tax",
            "year": 2020
        },
        {
            "authors": [
                "Valentin Barriere."
            ],
            "title": "Hybrid Models for Opinion Analysis in Speech Interactions",
            "venue": "ICMI, pages 647\u2013 651.",
            "year": 2017
        },
        {
            "authors": [
                "Valentin Barriere",
                "Guillaume Jacquet."
            ],
            "title": "CoFE : A New Dataset of Intra-Multilingual Multi-target Stance Classification from an Online European Participatory Democracy Platform",
            "venue": "AACL-IJCNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Leo Breiman",
                "Jerome Friedman",
                "Richard Olshen",
                "Charles Stone."
            ],
            "title": "Classification and regression trees",
            "venue": "wadsworth int. Group, 37(15):237\u2013251.",
            "year": 1984
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Jos\u00e9 Ca\u00f1ete",
                "Gabriel Chaperon",
                "Rodrigo Fuentes",
                "JouHui Ho",
                "Hojin Kang",
                "Jorge P\u00e9rez."
            ],
            "title": "Spanish Pre-Trained BERT Model and Evaluation Data",
            "venue": "PML4DC at ICLR 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Davide Castelvecchi"
            ],
            "title": "Can we open the black box of ai? Nature News, 538(7623):20",
            "year": 2016
        },
        {
            "authors": [
                "Julien Colin",
                "Thomas Fel",
                "R\u00e9mi Cad\u00e8ne",
                "Thomas Serre."
            ],
            "title": "What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods",
            "venue": "Advances in Neural Information Processing Systems, 35:2832\u20132845.",
            "year": 2022
        },
        {
            "authors": [
                "Rahul Kumar Dass",
                "Nick Petersen",
                "Marisa Omori",
                "Tamara Rice Lave",
                "Ubbo Visser."
            ],
            "title": "Detecting racial inequalities in criminal justice: towards an equitable deep learning approach for generating and interpreting racial categories using mugshots",
            "venue": "AI &",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Shizhe Diao",
                "Pengcheng Wang",
                "Yong Lin",
                "Tong Zhang."
            ],
            "title": "Active prompting with chain-ofthought for large language models",
            "venue": "arXiv preprint arXiv:2302.12246.",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Fel",
                "Melanie Ducoffe",
                "David Vigouroux",
                "Remi Cadene",
                "Mikael Capelle",
                "Claire Nicodeme",
                "Thomas Serre."
            ],
            "title": "Don\u2019t Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis",
            "venue": "CVPR.",
            "year": 2023
        },
        {
            "authors": [
                "Felix Antoine Fortin",
                "Fran\u00e7ois Michel De Rainville",
                "Marc And\u0155e Gardner",
                "Marc Parizeau",
                "Christian Gag\u0144e."
            ],
            "title": "DEAP: Evolutionary algorithms made easy",
            "venue": "Journal of Machine Learning Research, 13:2171\u20132175.",
            "year": 2012
        },
        {
            "authors": [
                "Leilani H. Gilpin",
                "David Bau",
                "Ben Z. Yuan",
                "Ayesha Bajwa",
                "Michael Specter",
                "Lalana Kagal."
            ],
            "title": "Explaining explanations: An overview of interpretability of machine learning",
            "venue": "Proceedings - 2018 IEEE 5th International Conference on Data Science and",
            "year": 2018
        },
        {
            "authors": [
                "Bryce Goodman",
                "Seth Flaxman."
            ],
            "title": "European union regulations on algorithmic decision making and a \"right to explanation",
            "venue": "AI Magazine, 38(3):50\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Anna Monreale",
                "Salvatore Ruggieri",
                "Franco Turini",
                "Fosca Giannotti",
                "Dino Pedreschi"
            ],
            "title": "A survey of methods for explaining black box models",
            "venue": "ACM computing surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "David Gunning",
                "Mark Stefik",
                "Jaesik Choi",
                "Timothy Miller",
                "Simone Stumpf",
                "Guang-Zhong Yang"
            ],
            "title": "Xai\u2014explainable artificial intelligence",
            "venue": "Science robotics,",
            "year": 2019
        },
        {
            "authors": [
                "Zellig S. Harris."
            ],
            "title": "Distributional Structure",
            "venue": "Word, 10(2-3):146\u2013162.",
            "year": 1954
        },
        {
            "authors": [
                "James B Heaton",
                "Nick G Polson",
                "Jan Hendrik Witte."
            ],
            "title": "Deep learning for finance: deep portfolios",
            "venue": "Applied Stochastic Models in Business and Industry, 33(1):3\u201312.",
            "year": 2017
        },
        {
            "authors": [
                "S\u00e9rgio Jesus",
                "Catarina Bel\u00e9m",
                "Vladimir Balayan",
                "Jo\u00e3o Bento",
                "Pedro Saleiro",
                "Pedro Bizarro",
                "Jo\u00e3o Gama."
            ],
            "title": "How can i choose an explainer?: An Application-grounded Evaluation of Post-hoc Explanations",
            "venue": "FAccT 2021 - Proceedings of the 2021",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems, 35:22199\u2013 22213.",
            "year": 2022
        },
        {
            "authors": [
                "Brenden Lake",
                "Marco Baroni."
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "International conference on machine learning, pages 2873\u20132882. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Shiyang Li",
                "Jianshu Chen",
                "Yelong Shen",
                "Zhiyu Chen",
                "Xinlu Zhang",
                "Zekun Li",
                "Hong Wang",
                "Jing Qian",
                "Baolin Peng",
                "Yi Mao"
            ],
            "title": "2022a. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726",
            "year": 2022
        },
        {
            "authors": [
                "Siyan Li",
                "Riley Carlson",
                "Christopher Potts."
            ],
            "title": "Systematicity in gpt-3\u2019s interpretation of novel english noun compounds",
            "venue": "arXiv preprint arXiv:2210.09492.",
            "year": 2022
        },
        {
            "authors": [
                "Charles Lovering",
                "Ellie Pavlick."
            ],
            "title": "Unit testing for concepts in neural networks",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1193\u2013 1208.",
            "year": 2022
        },
        {
            "authors": [
                "Scott Lundberg",
                "Su-In Lee."
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "NIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Faithful chain-of-thought reasoning",
            "venue": "arXiv preprint arXiv:2301.13379.",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "Advances in Neural Information Processing Systems, 35:17359\u201317372.",
            "year": 2022
        },
        {
            "authors": [
                "Beau Norgeot",
                "Benjamin S Glicksberg",
                "Atul J Butte."
            ],
            "title": "A call for deep-learning healthcare",
            "venue": "Nature medicine, 25(1):14\u201315.",
            "year": 2019
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Ameida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "C L Mar",
                "Jacob Hilton",
                "Amanda Askell",
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "rot",
                "\u00c9douard Duchesnay"
            ],
            "title": "Scikit-learn: Machine Learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "year": 2023
        },
        {
            "authors": [
                "Ansh Radhakrishnan",
                "Karina Nguyen",
                "Anna Chen",
                "Carol Chen",
                "Carson Denison",
                "Danny Hernandez",
                "Esin Durmus",
                "Evan Hubinger",
                "Jackson Kernion",
                "Kamil\u0117 Luko\u0161i\u016bt\u0117"
            ],
            "title": "Question decomposition improves the faithfulness of model-generated reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Yuval Reif",
                "Roy Schwartz."
            ],
            "title": "Fighting Bias with Bias: Promoting Model Robustness by Amplifying Dataset Biases",
            "venue": "Findings of ACL: ACL 2023, pages 13169\u201313189.",
            "year": 2023
        },
        {
            "authors": [
                "Cynthia Rudin."
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature machine intelligence, 1(5):206\u2013215.",
            "year": 2019
        },
        {
            "authors": [
                "Stuart Russell",
                "Daniel Dewey",
                "Max Tegmark."
            ],
            "title": "Research priorities for robust and beneficial artificial intelligence",
            "venue": "AI Magazine, 36(4):105\u2013114.",
            "year": 2015
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "True fewshot learning with prompts\u2014a real-world perspective",
            "venue": "Transactions of the Association for Computational Linguistics, 10:716\u2013731.",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": "Why Should I Trust You?\u201d Explaining the Predictions of Any Classifier",
            "venue": "KDD \u201916 Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery",
            "year": 2016
        },
        {
            "authors": [
                "Tomer Ullman."
            ],
            "title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "venue": "arXiv preprint arXiv:2302.08399.",
            "year": 2023
        },
        {
            "authors": [
                "Felipe Urrutia",
                "Roberto Araya."
            ],
            "title": "Who \u2019 s the Best Detective ? LLMs vs",
            "venue": "MLs in Detecting Incoherent Fourth Grade Math Answers. arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Felipe Ignacio Urrutia Vargas",
                "Roberto Araya."
            ],
            "title": "Automatic detection of incoherent written responses to open-ended mathematics questions of fourth graders",
            "venue": "Preprint, pages 0\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Jannis Vamvas",
                "Rico Sennrich."
            ],
            "title": "X-stance: A Multilingual Multi-Target Dataset for Stance Detection",
            "venue": "SwissText.",
            "year": 2020
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rebecca Wexler."
            ],
            "title": "When a computer program keeps you in jail: How computers are harming criminal justice",
            "venue": "New York Times, 13.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Wenpeng Yin",
                "Jamaal Hay",
                "Dan Roth."
            ],
            "title": "Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach",
            "venue": "EMNLP-IJCNLP 2019, pages 3914\u20133923.",
            "year": 2019
        },
        {
            "authors": [
                "Hongyu Zhao",
                "Kangrui Wang",
                "Mo Yu",
                "Hongyuan Mei."
            ],
            "title": "Explicit planning helps language models in logical reasoning",
            "venue": "arXiv preprint arXiv:2303.15714.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction and Related Work",
            "text": "The use of AI models is becoming increasingly pervasive in today\u2019s society. As such, applications of these models have seen the light in domains where decisions can have dramatic consequences such as healthcare (Norgeot et al., 2019),\n1Code available at https://github.com/ furrutiav/nllf-emnlp-2023\njustice (Dass et al., 2022), or finances (Heaton et al., 2017). This pervasiveness is intrinsically linked to the huge success of Deep Learning models, which have shown to scale particularly well to complex decision-making problems. However, this success comes at a cost. To solve complex (high-stakes) decisions, these models develop inner hidden representations which are hard to understand and interpret even by researchers implementing the models (Castelvecchi, 2016).\nRegulations like the European Union\u2019s \u201cRight to Explanation\u201d (Goodman and Flaxman, 2017) or Russell et al. (2015) requirements for safe and secure AI in safety-critical tasks fostered advances in explainability. Therefore, there has been a growing interest in developing techniques allowing to explain the mechanisms subtending these so-called \"black-box\" models (Guidotti et al., 2018; Fel et al., 2022). Notably, this endeavor has given rise to an entire research field called Explainable AI (XAI), which focuses on providing human-interpretable information on the models\u2019 behavior (Gunning et al., 2019; Arrieta et al., 2020).\nExplainable Deep Learning in Natural Language Processing (NLP) can be decomposed in two categories: representational and practical. Representational XAI in NLP focuses on understanding the underlying structure of the representations. For instance, studies have shown that Transformer-based architectures develop abstract symbolic, or compositional, representations (Lovering and Pavlick, 2022; Li et al., 2022b). Similarly, it has been shown that conceptual knowledge can have sparse representations, which can thus be located and edited to induce different predictions (Meng et al., 2022). Practical methods can analyze the outputs of the models, for example when perturbing the inputs (Tulio Ribeiro et al., 2016; Fel et al., 2023; Lundberg and Lee, 2017). Recently, practical XAI in NLP focuses on prompting to increase explainability. For instance, chain-of-thought prompting is a\nmethod that implements a sequence of interposed NLP steps leading to a final answer (Wei et al., 2022; Wang et al., 2022; Zhao et al., 2023; Lyu et al., 2023). This method has the advantage to provide insights on the logical reasoning steps behind a model\u2019s behavior, and thus allows to understand (at a higher level) the predictive success or failure of LLM (Zhou et al., 2022; Diao et al., 2023; Wang et al., 2022).\nAlthough advances in XAI within the NLP domain have offered interesting insights on the underlying mechanisms and representational structures of LLM, there has been a recent push to solely focus on interpretable models for high-stakes decisions (Rudin, 2019). This push is motivated by dramatic errors made by these models in real life situations, such as assessing criminal risk at large scale (Angwin et al., 2016) or incorrectly denying bail for criminals (Wexler, 2017). The main rationale behind this idea holds in that there will always be a certain level of error (or information loss) associated to the explanations of black-box models. Indeed, these explanations can, by definition, only partially incorporate information of the model\u2019s reasoning process.\nFollowing Rudin (2019), we make the difference between an explainable black-box model and an interpretable white-box model. Explainability relies on algorithms aiming to explain the model predictions by showing cues to the user like LIME, or other ad-hoc methods (Fel et al., 2022; Colin et al.,\n2022). Interpretability relies on the possibility to know exactly why the model is making a prediction because they are inherent to the prediction and faithful to what the model actually computes. However, methods like CoT which should be interpretable (because outputting explanations with their predictions) have not always shown to give faithful explanations (Radhakrishnan et al., 2023). It is also arguable that our method relies on learned representations from a BERT, which decreases its overall interpretability.\nMotivation and Contributions In this work, we aim to reconcile the abilities of black-box LLM and interpretable machine learning (ML) models, by leveraging the impressive zero-shot abilities of LLM, instructed (Peng et al., 2023; Ouyang et al., 2022; Chung et al., 2022) or not (Wei et al., 2021), to improve the performance of ML models in more complex tasks. One particularly stunning feature of LLM is compositional systematicity (Lake and Baroni, 2018; Bubeck et al., 2023): the ability to decompose concepts into their constituent parts that can be recombined to produce entirely novel concepts. Such compositional representations is at the basis of systematic generalization in novel contexts (Brown et al., 2020). For instance, experimental work has shown that LLM are zero-shot learners, and can further improve this skill when encouraged to reason sequentially (Kojima et al., 2022a).\nIn our approach schematize in colors on Figure 1, we leverage the ability of LLM to decompose\ncomplex tasks in simpler sub-tasks, and use these sub-tasks with a medium-size language model to create interpretable features (the Binary Subtask Questions (BSQ) and Natural Language Learned Features (NLLF) in green) and to improve the performance of ML classifiers. This classifier can be a simple interpretable model such as a Decision Tree with a readable decision path (in blue). The uniqueness of our work is to combine the strength of LLM and the explainability of ML classifiers, as opposed to similar previous work which as mainly focused on leveraging LLM to increase the reasoning abilities of smaller LM (Li et al., 2022a).\nThis work makes three main contributions. First, compared with chain-of-thoughts methods, which are often computationally expensive and are effective in solving certain types of reasoning problems, our approach is a computationally cheap and universal solution. Indeed, it can be applied to any problem that can be reasonably decomposed in simpler tasks (see methods below). Second, we present a method that allows simpler and interpretable models to solve complex reasoning tasks; tasks which are usually outside the realm of solutions for these type of models. Third, we show that those interpretable models can surprisingly outperform stateof-the-art LLM models in reasoning-based classification tasks.\nTo demonstrate the usefulness of the proposed method we focus on two different languages tasks that requires high levels of reasoning, and can be decomposed in subtasks with lower difficulty levels of reasoning, in English and in Spanish. We aim to show the generalization power of our method by (i) classifying the coherence of fourth grade students\u2019 answers to mathematical questions (Urrutia Vargas and Araya, 2023; Urrutia and Araya, 2023) (IAD: Incoherent Answer Detection), (ii) classifying scientific papers regarding a topic of interest in the context of a systematic literature review about agroecology and climate change (SAC: Scientific Abstract Classification)."
        },
        {
            "heading": "2 Methods",
            "text": "This section describes the different parts of the proposed method, which are summarized in Figures 2 and 3. Subsection 2.1 shows how to utilize an instructed LLM to generate natural language binary subtasks questions that can be useful to solve a more complex task. Subsection 2.2 (in green in Figure 3) explains the process to leverage the\nzero-shot ability of a LLM in order to label examples regarding the binary subtasks. Next, subsection 2.3 (in orange in Figure 3) contains details on how we train a BERT-like model (Devlin et al., 2018; Ca\u00f1ete et al., 2020) in a natural language inference (NLI) fashion to resolved those subtasks. Finally, subsections 2.4 and 2.5 (in blue in Figure 3) describe the process to generate interpretable representations and how to integrate them in an explainable model to solve the main task."
        },
        {
            "heading": "2.1 Lower-level Subtasks Generation",
            "text": "In this step, we are generating C Binary Subtask Questions (BSQs), which are using a LLM. This step is not mandatory as a human practitioner could do it manually.\nIn order to identify subtasks of the main task, we randomly select a small percentage pq of samples from the training set. With the help of a LLM that we prompt using an instruction-based template (visible in Appendix E), we generate a set of 5 basic binary questions per sample useful to solve the main task as shown in Figure 2.\nThis process lead to a large set of Q binary questions obtained from the pq subpart of the dataset. In order to reduce the redundancy in this large set, similar questions were manually grouped together into C groups, and each group was reformulated into a unique general question and verifiable yes/no binary questions. This process leaves us with a set of C questions."
        },
        {
            "heading": "2.2 Zero-shot Subtasks Labeling with an LLM",
            "text": "In this phase, we generate labels on some of the training examples regarding the C subtasks. In order to achieve this, we are leveraging the zero-shot\nlearning capacity of LLM to solve simple tasks. By prompting an LLM with samples from the training dataset with a low-level subtask binary question, we are able to annotate each example according to the C binary subtasks. In the end, we obtain synthetics labels on a limited percentage pl of the training set, which totals to C \u00d7 pl percents of the initial training dataset size. The template of the prompts can be seen in Appendix E."
        },
        {
            "heading": "2.3 Natural Language Learned Feature Generator Training",
            "text": "In this stage, we use the examples tagged with low-level weak labels obtained through a large model, to fine-tune a smaller BERT-like transformer model2in a natural language inference (NLI) way. Given that the BSQs can be expressed and inserted in natural language inside the transformer, an NLI-type of inference means that the text to classify and the BSQ are seen as premise and hypothesis. It has two strong advantages: (i) it leverages the semantic knowledge encoded during pre-training to understand the label, (ii) it can be applied in a zero-shot manner using new labels formulated as natural language binary question (Yin et al., 2019; Vamvas and Sennrich, 2020; Barriere and Jacquet, 2022).\nIn the end, this model is able to predict, for every pair of sample associated with any binary question, if the answer to the question is yes or no. We call this model Natural Language Learned Feature Generator (NLLFG). More details are available in Appendix A.\n2which has 1000 times less parameters than the LLM used beforehand"
        },
        {
            "heading": "2.4 Natural Language Learned Feature Generation",
            "text": "BSQ augmentation Only C question were used for the training of the NLLFG because of budget cost, but way more might be used to represent a sample as the NLLFG can generate an answer to a question never seen during training, in a zeroshot way. Hence, we augment the set of questions by adding new questions from the expert domain: we used all the available BSQs before the manual clustering, translation of expert linguistics features into natural language, paraphrases of the C BSQs and human-made BSQs. This leads to a set of C+ questions.\nNLLF construction For every example, we use the NLLFG with the C+ binary questions in order to create a vector of NLLF. The vector of NLLF was constructed by taking the sigmoid of the logit instead of the softmax, in order to keep the information about the confidence of the prediction for both classes (i.e., sometimes the predictions are far away from the decision hyperplan for both classes). Which means that for each BSQ, there are two values between 0 and 1: one representing the probability of the Yes answer, and one representing the probability of the No answer. This gave us a NLLF of size 2C+.\nFeature selection Finally, we ensure to select only the most effective features by removing the ones predicting non useful representations with feature selection. We employed a genetic algorithm for feature selection (Fortin et al., 2012), using decision trees as backbones. We executed the algorithm in a 15-fold cross-validation setting and selected the features that were selected at least one third of\nthe times. In other words, we removed the BSQs that lead to unsure answers from the model."
        },
        {
            "heading": "2.5 NLLF-boosted Explainable Model",
            "text": "the NLLF vector generated from a sample is a controlled representation, which can then be added to augment any existing classifier. In this work, given that we are focusing on interpretable modelization, we chose to use it as input of a Decision Tree (Breiman et al., 1984). We also enhanced the NLLF representation with Expert Features (EF) derived from linguistic patterns, which are known to be very precise but generalize poorly. In this way, we take the best of both world with an hybrid model benefiting from the robustness and high accuracy of deep transformers as well as the fine-grained precision of linguistic rules (Barriere, 2017)."
        },
        {
            "heading": "3 Experiment and Results",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "We used two datasets in order to validate our model. First, we present a dataset of abstracts and titles of English scientific articles, labeled regarding their pertinence towards a systematic literature review on Climate Change and Agroecology. Second, we present a dataset of coherent and incoherent students answers to open-ended questions of a mathematical test.\nScientific Abstract Classification To evaluate our method on complex text, we use a dataset annotated in the context of a systematic literature review about the impact of agroecological practices on climate change mitigation and climate change adaptation.3 More than 15k articles were retrieved from the Web of Science database using an extensive set of keywords related to Agroecology and Climate Change. The first 2,000 articles were tagged by two annotators, using the title and abstract of the article, regarding whether or not the article was relevant for the systematic literature review. If there was no consensus between the two annotators, a third annotator was called to arbitrate, which happened the case 14% of the time. The articles with missing abstracts were removed from this study, which left a total of 1,983 articles, from which 50.1% labeled as included and 49.9 % labeled as excluded.\n3article to be published from the Agroecology research group of Sant\u2019Anna di Pisa: https://www. santannapisa.it/en/centro-di-ricerca/ scienze-delle-piante/agroecology\nIncoherent Answer Detection To evaluate our method on special domain text, we focus on the task of coherence detection in students\u2019 answers to an open-ended mathematical test questions. We used the dataset of Urrutia Vargas and Araya (2023) composed of 15,435 answers to 700+ different open-ended questions collected using the online e-learning platform ConectaIdeas. The answers\u2019 (in)coherence were manually annotated by several teachers. The test set only contained examples that were annotated similarly by at least three annotators. Both the train and test datasets are imbalanced between the classes, with respectively 13.3% and 20.1% of incoherent examples."
        },
        {
            "heading": "3.2 Baselines and proposed methods",
            "text": "In this section, we describe the baseline models that we evaluated in our experiments.\nVanilla ChatGPT Because this model is known to have good performances at zero- and few-shot inference, we evaluate it in a 0/4-shot prompt strategies.\nCoT ChatGPT Chain-of-Thought has been shown to improve the performances of LLM for reasoning tasks. Hence, we enhance the model with this technique. We used the technique of Kojima et al. (2022b) for the zero-shot CoT.\nSelf-ask ChatGPT Self-ask (Press et al., 2023) enhances compositional reasoning by explicitly formulating and answering follow-up questions before addressing the initial query to significantly reduces the compositionality gap.\nBERT We evaluate different models based on a BERT transformers (Devlin et al., 2018), processing the raw text. The Vanilla version connects one fully-connected layer after the [CLS] token. The other versions concatenate (previously extracted) expert features and NLLF with the [CLS] representation. For the IAD dataset, which is in Spanish, we used the Spanish version of BERT called BETO (Ca\u00f1ete et al., 2020).\nDecision Tree Decision trees were used as explainable models, with low height and only interpretable features. We used the same features as for BETO, plus added Bag-of-N-Grams (BoNG, variant of the Bag-of-Words; Harris, 1954) in order to model the text content.\nModel Variant Params Explainability IAD SAC Prec. Rec. F1 Prec. Rec. F1\nChatGPT\n0-shot\n\u223c 1011\n\u2717 19.70 76.47 31.33 76.57 50.27 35.23 4-shots \u2717 24.80 90.44 38.92 66.92 51.66 38.92 0-shots CoT \u2713 23.14 84.56 36.33 44.93 46.31 41.59 4-shots CoT \u2713 42.18 85.29 56.45 65.00 63.26 62.72 0-shots SA \u2713 21.29 82.35 33.84 63.65 55.15 48.13 4-shots SA \u2713 51.71 77.94 62.17 70.31 62.42 59.50\nBERT Vanilla\n\u223c 108 \u2717 58.47 78.68 67.08 67.74 67.80 67.72\nEF \u2717 78.40 72.06 75.10 67.65 66.93 66.90 NLLF \u2717 67.10 76.47 71.48 68.97 68.98 68.75 NLLF+EF \u2717 80.49 72.79 76.45 73.66 73.61 73.63\nDecision\nBoNG\n\u223c 102\n\u2713 100.0 8.09 14.97 65.38 65.13 65.15 EF \u2713 83.33 66.18 73.77 68.18 66.49 64.95 NLLF \u2713 75.00 44.12 55.56 62.41 62.43 62.25 Tree NLLF+EF \u2713 85.22 72.06 78.09 68.02 68.01 67.75\nNLLF+BoNG \u2713 82.28 47.79 60.47 66.21 66.26 66.20 NLLF+EF+BoNG \u2713 85.22 72.06 78.09 68.17 67.43 67.41\nTable 1: Precision, Recall and F1-score of all the configurations and models for Incoherent Answer Detection (IAD); and (Macro) Precision, Recall and F1-score of all the configurations and models for the Scientific Abstract Classification (SAC). Using Expert Features (EF), NLLF, and Bag-of-N-Grams (BoNG)."
        },
        {
            "heading": "3.3 Experimental Protocol",
            "text": ""
        },
        {
            "heading": "3.3.1 Dataset splitting",
            "text": "Scientific Abstract Classification We randomly split the data into a training, a validation and a test sets following the proportion 70/10/20.\nIncoherent Answer Detection We train the classifiers on the 2019 data and tested on a sample of 677 perfect-labeled answers from the 2017 dataset. The study used the different open-ended questions and answers, but the same definition of incoherence throughout, despite different students and teachers in each year.\nNLLFG training For each task, we randomly split 90% of the weakly labeled examples into a training set and keep the last 10% for the model validation."
        },
        {
            "heading": "3.3.2 Evaluation Metrics",
            "text": "Scientific Abstract Classification As both the classes are important, we have adopted the macroaveraged precision, recall, and F1-score metrics as our evaluation criteria.\nIncoherent Answer Detection To maintain consistency with the aforementioned work (Urrutia Vargas and Araya, 2023; Urrutia and Araya, 2023), we have adopted precision, recall, and F1score metrics for the positive class (incoherent) as our evaluation criteria."
        },
        {
            "heading": "3.3.3 Method parameters",
            "text": "Scientific Abstract Classification We used a pq of 1.3% (21 examples) to generate BSQ and a pl of 10% to train the NLLFG. C was set up to 13 questions, and the number of questions for the generation C+ was 109 questions.\nIncoherent Answer Detection We used a pq of .15% (21 examples) to generate BSQ and a pl of 10% to train the NLLFG. C was set up to 10 questions, and the number of questions for the generation C+ obtained was 66."
        },
        {
            "heading": "3.3.4 Implementation",
            "text": "The transformers library (Wolf et al., 2019) was used to access the pre-trained model and to train our models. We used BERT and BETO4 as backbones for the NLLFG.The decision trees were trained using scikit-learn (Pedregosa et al., 2012). We used the 03/23/23 version of ChatGPT (Ouyang et al., 2022) as LLM. Other details can be found in Appendix B and C."
        },
        {
            "heading": "3.4 Results",
            "text": "The results are visible in Table 1 for respectively the IAD task and the SAC task. The last six columns detail the precision, recall and F1-score of the \u201cincoherent\u201d class for the IAD task, and the macro precision, recall and F1-score for the SAC task. In both cases, the best results overall are\n4bert-base-cased and bert-base-spanish-wwm-cased\nthe ones from models enhanced by NLLF and EF, reach the F1-scores of 78.09% and 73.63%.\nChatGPT For the SAC task, ChatGPT models display high precision, but overall low recall, and very low F1-score coming from a low F1-score for the exclude task, except for the 4-shot + CoT /SAC versions that reach a F1-score of 62.72% / 59.50%. This is because ChatGPT tends to categorize almost all of the articles as included. For the IAD task, ChatGPT models display poor precision and F1-score metrics, but overall high recall, meaning these models tend to categorize most of the answers as incoherent. Moreover, as expected, the 4-shots + SAC variant outperform all other ChatGPT variants in F1-score (62.17%).\nBERT BERT-like models display the high metrics across the board in precision, recall and F1score. In particular, the models incorporating both NLLF and EF obtains the highest overall F1score (76.45% and 73.63%) in both tasks. Surprisingly, enhancing the transformer with NLLF (resp. EF) provokes a drop in the performances for the IAD (resp. SAC) task. Note however, that BERT models belong to the class of models that are not explainable.\nDecision Tree The DT models also reached high performances across the board (with the exception of the BoNG variant for the IAD task). Specifically, the variant using NLLF+EF displays highest F1-score (78.09% and 67.75%) in that model class, and it is notable that adding BoNG features does not improve the performances. The DT models are simple and fully interpretable, and significantly outperforms a LLM like ChatGPT, while reaching performance metrics competitive with a deep learning (black-box) model. This approach provides interpretable steps to explain decision making within the tree (see Appendix H). The DTs using EF have very competitive results. Even though it is not relying on deep neural nets, it needs many complex handcrafted features coming from expert knowledge (Table 9)."
        },
        {
            "heading": "4 Model Analysis",
            "text": ""
        },
        {
            "heading": "4.1 NLLF Accuracies",
            "text": "We quantify the error of the output of the decision tree using classical metrics, but not the error on the input of the tree, which is the error when creating the NLLF. Here we analyze how accurate were the\nNLLF generated by the BERT-like model, and also the weak labels by the LLM.\nNLLFG Training We analyze the performance of the NLLFG on the validation set during it\u2019s training in order to quantify how good a NLI-like BERT transformer can reproduce the weak labels of an LLM. From the results shown in Table 2, we can see that the performances for the IAD task are much higher than the ones of the SAC task.\nTask Label Prec. Rec. F1 Acc.\nSAC Yes 74 86 79\n73 No 71 52 60\nIAD Yes 97 96 97\n95 No 92 94 93"
        },
        {
            "heading": "1. Does the article assess the impact of agroecology on nitrogen dynamics?",
            "text": ""
        },
        {
            "heading": "2. Does the article discuss the impact of methane (CH4) emissions?",
            "text": ""
        },
        {
            "heading": "3. Does the article discuss the role of agroecology in enhancing climate resilience in agricultural systems?",
            "text": ""
        },
        {
            "heading": "4. Does the article address limitations, challenges, and potential risks?",
            "text": ""
        },
        {
            "heading": "5. Does the article have a specific geographic focus?",
            "text": ""
        },
        {
            "heading": "6. Does the article address policy implications?",
            "text": ""
        },
        {
            "heading": "7. Does the article cover climate change adaptation?",
            "text": ""
        },
        {
            "heading": "8. Does the article assess agroecological practices' impact on climate change?",
            "text": ""
        },
        {
            "heading": "9. Does the article discuss agroecological practices?",
            "text": ""
        },
        {
            "heading": "10. Does the article focus primarily on climate change mitigation?",
            "text": ""
        },
        {
            "heading": "11. Does the study specifically evaluate greenhouse gas emissions?",
            "text": ""
        },
        {
            "heading": "12. Does the article comprehensively cover climate change and environmental aspects?",
            "text": "is not using as input a class label but the probability of each label, which contains more information."
        },
        {
            "heading": "4.2 Features",
            "text": "Decision Tree Selected Features The DTs combining distinct features (see last three rows of Table 1) are free to select the features they deem best to solve the main decision-making task. For the IAD task, the decision tree combining NLLF + EF considers 25 features of which 14 are NLLF and 11 are EF. An exemplar is shown in Figure 15 on the Appendix, where the tree starts by checking for less than 3 tokens and the presence of pronouns. Otherwise, it checks whether the answer provides evidence or reasons, vowels, binary words, use of calculations, and adequate knowledge of the topic raised in the question. For the SAC task, the decision tree combining NLLF + EF considers 22 features of which 14 are NLLF and 8 are EF.\nCorrelation with Main Tasks Labels In the IAD task, we observed that some classes correlated with some EFs more strongly than with NLLFs, due to meticulous feature design versus intuitive BSQ design, respectively. However, when looking at the results of the SAC task, it is possible to validate that our approach is functioning even to tasks that lack a known powerful, hand-crafted feature\ndesign, but just with some keywords spotting using regular expressions.\nCausality In addition to provide interpretability, our method tends to foster causal learning. Indeed, by allowing the user to directly write the features to use as input, this method prevents the model to rely too heavily on latent correlational patterns that are specifically associated to certain classes (Gilpin et al., 2018; Angwin et al., 2016). Nevertheless, feature selection still relies on data distribution which makes the system not completely causal even though it tends to."
        },
        {
            "heading": "4.3 Path visualization",
            "text": "The possible path are visible in Figure 15 for the IAD task and in Figure 16 for the SAC task. All the possible paths are composed of mixed type of features with EF and NLLF.\nFor the SAC task, we can see that the first decision derives from the answer to \"Does the abstract address the relationship between agroecological practices and climate change?\" which allow to coarsely separate the samples between the two classes. Then, if the prefix \"convent\" is contained in the text, it is 5.6 times more probable (158/28) that the text comes from include class according to the GINI value. Examples of decisions with their associated paths in the tree are shown in Figures 13 and 14."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "We proposed a new method to leverage low-level reasoning knowledge related to a more complex task from a large language model, and integrate this into a smaller transformer. The transformer has been trained in a way that it can be used for zeroshot inference with any low-level reasoning having\nthe task formulated in natural language. This allows the practitioners to formulate easily their own features related about the task. The Natural Language Learned Feature vector can then be used as a representation in any other classifier. We show that it is easy to train an interpretable model like a Decision Tree, leading to both competitive results and interpretability. Our method can be applied to any predictive task using text as input. Future work should focus on investigating the potential impacts of this approach in real-world educational settings, and especially inspect the preferences of the practitioners regarding different explainability methods in order to help them taking complex decision (Jesus et al., 2021).\nLimitations\nAlthough, on paper, our method is universal, we need to show that our results can generalize to other tasks where LLM (such as ChatGPT) struggle in their reasoning process, e.g., theory of mind tasks (Ullman, 2023). Moreover, our approach has been demonstrated on binary classification, and it remains to demonstrate that our approach can scale well to more categories. Otherwise, more complex tasks like multi-hop reasoning would be a late target for our system, as a simple classifier cannot solve this as it is, which would require many adaptations.\nDespite the fact that we obtained promising results with our approach, the performance of both BERT and the decision tree using NLLF alone was not exceptional. This may be affected by the performance of NLLFG. In particular, we used a limited set of examples to train our NLLFG. It is possible that training with a large set of answers and BSQs, or using a prompt-based approach (Schick and Sch\u00fctze, 2022) useful in few-shot setting, may improve the results. Especially, we have also seen during our experiments that the number of examples shown to the NLLFG during its training was correlated with its performances, for this reason we would like to monitor the performances of the NLLFG when trained with more weak labels from the LLM.\nFinally, we claim that choosing the features fosters causality but without rigorous experimentation. This could be tested by using a dataset with a deliberately inflated bias (Reif and Schwartz, 2023).\nEthics Statement\nThis work is in compliance with the ACL Ethics Policy as it allows to create models that might be more interpretable and more causal."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to thank Angelica Marchetti for her useful help on creating the expert features for the SAC dataset, and for the manual assessment of the weak labels in Table 3. This work was funded by National Center for Artificial Intelligence CENIA FB210017, Basal ANID."
        },
        {
            "heading": "A NLI-like Training",
            "text": "The BSQ (a binary subtask question in natural language) were integrated inside the transformer input as follow:\n[CLS] Input text [SEP] BSQ [SEP] .\nWe used a pre-trained BERT (resp. BETO) model in English (resp. Spanish) language for the SAC (resp. IAD) task. We fine-tuned one only model for all the subtasks, by integrating the subtask as string and using the binary low-level subtasks labels Yes or No as Entailment and Contradiction in NLI. We trained the model for 7 epochs, using a batch size of 16 and the Cross Entropy loss, with the Adam optimizer and a learning rate of 8 \u00d7 10\u22125."
        },
        {
            "heading": "B Decision Tree Training",
            "text": "we used the gini impurity as criterion to optimized and fixed the maximum depth of the tree to 5 in order to keep it comprehensible for the practitioners.\nDuring the learning phase, we used the Gini impurity as the criterion to optimized and fixed the maximum depth of the tree to 5 in order to keep it comprehensible for the practitioners. For the the model with BoNG only, we augmented the maximum depth to 10 because of the sparsity of the features. The model with NLLF-BoNG had a minimum impurity of 1.2 \u00d7 10\u22123, while the other models had a minimum impurity decrease of zero.\nThe BoNG were also implemented using scikitlearn, we choose 1000 as the number max of feature in order to keep the dimension small and computed the tf-idf for each n-gram."
        },
        {
            "heading": "C BERT fine-tuning",
            "text": "We used 8 epochs, a batch size of 32 and the Cross Entropy loss, with the Adam optimizer and a learning rate of 1\u00d710\u22125, and 5\u00d710\u22126 for the augmented transformers because of the concatenation of highlevel features before the output layer. We selected the best model on the validation set using best accuracy for SAC and best loss for IAD (because the\nModel Label Prec. Rec. F1 Acc.\nChatGPT Yes 71 89 79\n78 No 88 68 77\nNLLFG Yes 60 96 74\n68 No 92 43 59\nCoT Yes 78 85 81\n79 No 81 72 76"
        },
        {
            "heading": "G Expert Features",
            "text": ""
        },
        {
            "heading": "F Binary Questions",
            "text": ""
        },
        {
            "heading": "E Prompt Templates",
            "text": ""
        },
        {
            "heading": "D Chain-of-Thought for Weak Labels",
            "text": "Template: Question generation\n1\nTitle: Title . Abstract: Abstract .\n2\nBased on the abstract/title, Low-level question (answer \u201cYes\u201d or \u201cNo\u201d)\nTemplate: Subtask labelisation\n1\n2\nYou are an expert on agroecology and impact on climate change. You have to decide which article is include or excluded in our literature review. Based on the example, give me 5 very general (i.e., independent of the given example), verifiable and short attributes (in yes/no question format) in the abstract and/or title to verify that the following article is Label in the literature review of studies that assess the impact of agroecological practices on climate change mitigation or climate change adaptation.\nTitle: Title . Abstract: Abstract .\nTemplate: Question generation\n1\nTitle: Title . Abstract: Abstract .\n2\nBased on the abstract/title, Low-level question (answer \u201cYes\u201d or \u201cNo\u201d)\nTemplate: Subtask labelisation\n1\n2\nYou are an expert on agroecology and impact on climate change. You have to decide which article is include or excluded in our literature review. Based on the example, give me 5 very general (i.e., independent of the given example), verifiable and short attributes (in yes/no question format) in the abstract and/or title to verify that the following article is Label in the literature review of studies that assess the impact of agroecological practices on climate change mitigation or climate change adaptation.\nTitle: Title . Abstract: Abstract .\nTemplate: Question generation 1\nTitle: Title . Abstract: Abstract .\n2\nBased on the abstract/title, Low-level question (answer \u201cYes\u201d or \u201cNo\u201d)\nTemplate: Subtask labelisation\n1\n2\nYou are an expert on agroecology and impact on climate change. You have to decide which article is include or excluded in our literature review. Based on the example, give me 5 very general (i.e., independent of the given example), verifiable and short attributes (in yes/no question format) in the abstract and/or title to verify that the following article is Label in the literature review of studies that assess the impact of agroecological practices on climate change mitigation or climate change adaptation.\nTitle: Title . Abstract: Abstract ."
        },
        {
            "heading": "H Examples of Decision Process in the Tree",
            "text": "We show in Figures 13 and 14 some examples along with the decision tree prediction and precision on the pathway it used.\nTemplate: Question generation\n1\nTitle: Title . Abstract: Abstract .\n2\nBased on the abstract/title, Low-level question (answer \u201cYes\u201d or \u201cNo\u201d)\nTemplate: Subtask labelisation\n1\n2\nYou are an expert on agroecology and impact on climate change. You have to decide which article is include or excluded in our literature review. Based on the example, give me 5 very general (i.e., independent of the given example), verifiable and short attributes (in yes/no question format) in the abstract and/or title to verify that the following article is Label in the literature review of studies that assess the impact of agroecological practices on climate change mitigation or climate change adaptation.\nTitle: Title . Abstract: Abstract .\nTemplate: Question generation 1\nTitle: Title . Abstract: Abstract .\n2\nBased on the abstract/title, Low-level question (answer \u201cYes\u201d or \u201cNo\u201d)\nTemplate: Subtask labelisation\n1\n2\nYou are an expert on agroecology and impact on climate change. You have to decide which article is include or excluded in our literature review. Based on the example, give me 5 very general (i.e., independent of the given example), verifiable and short attributes (in yes/no question format) in the abstract and/or title to verify that the following article is Label in the literature review of studies that assess the impact of agroecological practices on climate change mitigation or climate change adaptation.\nTitle: Title . Abstract: Abstract ."
        },
        {
            "heading": "Id Abstract",
            "text": "Abstract-1 Cropping is responsible for substantial emissions of greenhouse gasses (GHGs) worldwide through the use of fertilizers and through expansion of agricultural land and associated carbon losses. Especially in sub-Saharan Africa (SSA), GHG emissions from these processes might increase steeply in coming decades, due to tripling demand for food until 2050 to match the steep population growth. This study assesses the impact of achieving cereal self-sufficiency by the year 2050 for 10 SSA countries on GHG emissions related to different scenarios of increasing cereal production, ranging from intensifying production to agricultural area expansion. We also assessed different nutrient management variants in the intensification. Our analysis revealed that irrespective of intensification or extensification, GHG emissions of the 10 countries jointly are at least 50% higher in 2050 than in 2015. Intensification will come, depending on the nutrient use efficiency achieved, with large increases in nutrient inputs and associated GHG emissions. However, matching food demand through conversion of forest and grasslands to cereal area likely results in much higher GHG emissions. Moreover, many countries lack enough suitable land for cereal expansion to match food demand. In addition, we analysed the uncertainty in our GHG estimates and found that it is caused primarily by uncertainty in the IPCC Tier 1 coefficient for direct N2O emissions, and by the agronomic nitrogen use efficiency (N-AE). In conclusion, intensification scenarios are clearly superior to expansion scenarios in terms of climate change mitigation, but only if current N-AE is increased to levels commonly achieved in, for example, the United States, and which have been demonstrated to be feasible in some locations in SSA. As such, intensifying cereal production with good agronomy and nutrient management is essential to moderate inevitable increases in GHG emissions. Sustainably increasing crop production in SSA is therefore a daunting challenge in the coming decades\nAbstract-2 Paddy rice cultivation is an important source of global anthropogenic methane emissions. Drainage the flooded soils can reduce methane substantially, but N2O emission occur concurrently, which would offset the reduction of methane emission. It remains unclear how mid-season drainage affects the global warming potential (GWP) of CH4 and N2O emissions. In this study, a meta-analysis was conducted to investigate the effect of mid-season drainage on GWP and the factors that control the response of GWP to mid-season drainage. Results showed that mid-season drainage decreased CH4 emission by 52% while increased N2O emission by 242%. The GWP under mid-season drainage decreased by 47% compared to continuously flooding. The yield-scaled GWP under mid-season drainage decreased by 48%. Mid-season drainage had no effect on rice grain yield. Although soil drainage times and organic matter amendment are important factors affecting CH4 and N2O emissions in rice paddy field, the study showed that neither of them had effect on the response of GWP to mid-season drainage. The reduction rate of the GWP under mid-season drainage increased when N fertilization application rate increases from 50 kg ha(-1) to > 200 kg ha(-1). This study demonstrated that CH4 is still a dominant greenhouse gas in rice paddies under water management with mid-season drainage. Nitrogen fertilization is an important factor that regulates the response of GWP to mid-season drainage. High nitrogen fertilization rate would decrease the overall emission of CH4 and N2O under mid-season drainage. However, increasing drainage times or applying organic fertilizer under mid-season does not change the overall emission rate of CH4 and N2O\nAbstract-3 This study aimed to evaluate greenhouse gas (GHG) emissions from conventional cultivation (S1) of seedless lime (SL) fruit in Hau Giang province, in the Mekong Delta region of Vietnam. We adjusted the scenarios by replacing 25% and 50% of nitrogen chemical fertilizer with respective amounts of N-based organic fertilizer (S2 and S3). Face-to-face interviews were conducted to collect primary data. Life cycle assessment (LCA) methodology with the cradle to gate approach was used to estimate GHG emission based on the functional unit of one hectare of growing area and one tonnage of fresh fruit weight. The emission factors of agrochemicals, fertilizers, electricity, fuel production, and internal combustion were collected from the MiLCA software, IPCC reports, and previous studies. The S1, S2, and S3 emissions were 7590, 6703, and 5884 kg-CO2 equivalent (CO(2)e) per hectare of the growing area and 273.6, 240.3, and 209.7 kg-CO(2)e for each tonnage of commercial fruit, respectively. Changing fertilizer-based practice from S1 to S2 and S3 mitigated 887.0-1706 kg-CO(2)e ha(-1) (11.7-22.5%) and 33.3-63.9 kg-CO(2)e t(-1) (12.2-25.6%), respectively. These results support a solution to reduce emissions by replacing chemical fertilizers with organic fertilizers\nAbstract-4 No-tillage (NT) and the introduction of cover crops, owing to their positive effects on soil organic carbon (SOC) sequestration and crop yields, are potential agricultural practices that both support food security under the new realities of climate change and alleviate greenhouse gas (GHG) emissions. However, the effects of the combination of long-term NT systems and cover crops on non-carbon dioxide (CO2) emissions and SOC sequestration have not been adequately documented, particularly in East Asia. We conducted a split-plot field experiment involving two tillage systems [NT and moldboard plowing (MP)] and three cover crops, namely, fallow (FA), hairy vetch (HV), and rye (RY). NT had slightly higher soybean yield than MP, although tillage methods and cover crop treatments had no significant effects on soybean yield. Cover crop treatments rather than tillage methods significantly affected methane (CH4) emissions; under FA and RY treatments, we observed CH4 uptakes, whereas under HV, we observed CH4 emissions. In contrast, rather than cover crop treatments, tillage methods affected nitrous oxide (N2O) emissions. Higher WFPS and soil bulk density under NT resulted in significantly higher annual N2O emissions than those under MP. However, under NT, the annual SOC sequestration rate significantly increased compared with that under MP, the global warming potential (GWP) caused by CH4 and N2O emissions was fully offset by net CO2 retention under NT. Additionally, treatment under NT reduced net GWP and yield-scaled GWP to a significantly greater degree than did treatment under MP. Treatments under NT with RY cover crop had the lowest net GWP (-2324 kg CO2 equivalent ha(-1) year(-1)) and yield-scaled GWP (-1037 kg CO2 equivalent Mg-1 soybean yield). These findings suggest that treatments under NT with cover crop systems-especially RY cover crop-in the long-term organic soybean field maintains sustainable crop production and reduces net GWP and yield-scaled GWP, which will be an effective climate-smart agriculture practice in the humid, subtropical regions prevailing in Kanto, Japan\nTable 5: Abstracts used in the prompt templates for the SAC task.\nQuestions Origin\nDoes the abstract mention any terms starting with \u2019bio\u2019? Ling Does the abstract specifically mention CH4? Ling Does the abstract discuss emissions? Ling Does the abstract discuss reducing something? Ling Does the abstract make reference to the concept of cover? Ling Is the concept of intercropping mentioned in the abstract? Ling Does the abstract discuss strategies? Ling Does the abstract address the topic of GHG emissions? Ling\nDoes the abstract refer to the application of a type of organic fertilisation practice? Hum Does the abstract refer to the impact (or effect) of these practices on Nitrogen/N2O/nitrogen oxide emissions? Hum Does the abstract refer to the impact (or effect) of these practices on the carbon sequestration in the soil? Hum Does the abstract refer to the impact (or effect) of these practices on Carbon/CH4/methane emissions? Hum Does the abstract refer to the application of one or more Climate-Smart Agriculture practices? Hum Does the abstract refer to the impact (or effect) of these practices on climate change mitigation? Hum Does the abstract refer to the application of one or more Sustainable Rice Intensification practices? Hum Does the abstract refer to the impact (or effect) of these practices on climate change adaptation? Hum Does the abstract refer to the impact (or effect) of these practices on greenshouse gasses (GHG) emissions? Hum Does the abstract refer to the application of a type of Bio-control practice? Hum Does the abstract refer to the application of one or more agroecological practices? Hum Does the abstract refer to the application of a type of ecological or mechanical weed management practice? Hum Does the abstract refer to the application of one or more Diversified farming practices? Hum Does the abstract evaluate agroecological practices\u2019 impact on climate change? Hum Does the abstract mention any organic agriculture practices being applied? Hum Does the abstract discuss the substitution of different varieties or cultivars? Hum Does the abstract explore the connection between agroecological practices and climate change? Hum Does the abstract analyze how agroecological systems affect climate change? Hum Does the abstract mention the replacement of various varieties or cultivars? Hum Does the abstract mention the implementation of Sustainable Rice Intensification practices? Hum Does the abstract address how these practices affect soil carbon storage? Hum Does the abstract discuss the application of Regenerative agriculture methods? Hum Does the abstract discuss a form of Residues management practice? Hum Does the abstract mention the use of intercropping practices? Hum Does the abstract mention the use of Regenerative agriculture practices? Hum Do the contents of the abstract pertain to agroecological practices? Hum Is the abstract discussing the application of agroecological methods? Hum Does the abstract mention the application of cover crops or mulching? Hum Does the abstract discuss implementing a type of water collection practice? Hum\nTable 8: Binary subtasks questions and their origin (human-made or natural language translation of linguistics rules) before the feature selection process for the SAC task.\nFi na\nl L ab el A ns w er to a n O pe nen de d Q ue st io n\nQ : A\ngu st\nin a\nha s\n4 pl\nas ti\nc bo\nxe s\nin\nw hi\nch s\nhe k\nee ps\n5 d\nol ls\nin e\nac h.\nH er\nco\nus in\nto ld\nh er\nth at\ns he\nh as\n1 8\ndo lls\nin\nto ta\nl. Is\nw ha\nt A gu\nst in\na' s\nco us\nin s\nay s\nco rr\nec t,\nw ha\nt o pe\nra ti\non s\nho ul\nd sh e pe rf or m to g et th e re su lt ? W ha t op er at io n sh ou ld s he p er fo rm to\ng et\nth e\nre su\nlt ?\nE xp\nla in\nin y\nou r\now n\nw or\nds .\nA : a\ngu ti\nna h\nas 2\n0 do\nlls\nLe ng\nth o\nf t he\nan\nsw er\n> 15\n\u2264 8\n3. 2%\n\u2264 1\n3. 3%\n= 0\n\u2264 1\nIn c.\n\u2714\nLe ng\nth o\nf t he\nan\nsw er\nTh e\nan sw\ner d\noe s\nno t c\non si\nde r\nal l\npo ss\nib le\no pt\nio ns\no r\npr ov\nid e\na ra\nti on\nal e\nfo r\ndi sc\nar di\nng\nin co\nrr ec\nt o pt\nio ns\nM ax\nim um\nn um\nbe r\nof c\non so\nna nt s re pe at ed in w or\nds\nof th\ne an\nsw er\n> 15\n\u2264 1\n3. 3%\n< 4\nIn c.\n\u2714\n> 8\n9. 2%\n> 0\n.8 5\nLe ng\nth o\nf t he\nan\nsw er\n\u2264 15\n\u2264 0\n.3 21\n> 7\n3. 7%\nC oh .\u2714 > 2 3. 2% Th e an sw er is n ot cl ea r, c on ci se o r us es la ng ua ge th at is n ot e as y to un de rs ta nd o r pr ec is e > 8 4. 4%\nA : 1\n5 00\n1 0\n00 2\n70 0-\n4 10\n0\nQ : M\nrs . M\nar ia\ng oe\ns to\nth e\nsu pe\nrm ar\nke t\nan d\nbu ys\n1 b\nox o\nf c er\nea l f\nor $\n15 00\n, 1\nlit er\no f m\nilk fo\nr $1\n00 0\nan d\n1 ki\nlo o f av oc ad os fo r $2 70 0. a k ilo o f a vo\nca do s fo r $2 70 0. H ow m uc h di d sh e sp en d in to ta l? E xp la in w ha t s te ps y ou pe rf or m ed a nd w ri te th e op er at io n to ar ri ve a t t he r es ul t. to a rr iv e at th e re su lt .\nA : 3\n5\nQ : P\ned ro\nh as\n2 0\nre d\nca rs\na nd\nh is\nm\not he\nr gi\nve s\nhi m\n1 5\ngr ee\nn ca\nrs . H\now\nm an\ny ca\nrs d\noe s\nPe dr\no ha\nve in\nto ta\nl?\nLe ng\nth o\nf t he\nan\nsw er\nTh e\nan sw\ner d\noe s\nno t c\non si\nde r\nal l\npo ss\nib le\no pt\nio ns\no r\npr ov\nid e\na ra\nti on\nal e\nfo r\ndi sc\nar di\nng\nin co\nrr ec\nt o pt\nio ns\nM ax\nim um\nn um\nbe r\nof c\non so\nna nt s re pe at ed in w or\nds\nof th\ne an\nsw er\n> 15\n\u2264 1\n3. 3%\n> 4\nC oh .\u2714 > 6 5. 1%\nA : 2\n1 m\nul tp\nlie d\n7x 3\nan d\nI go\nt 2 1\nQ : I\nf t he\ns id\ne A\nB o\nf a tr\nia ng\nle m\nea su\nre s\n7 cm\n, h ow\nlo ng\nis th\ne pe\nri m\net er\no f t\nhe\nfig ur\ne co\nns id\ner in\ng th\nat it\ns th\nre e\nsi de s ha ve e qu al m ea su re s? th re e si de s ha\nve\neq ua\nl m ea\nsu re\n?\nLe ng\nth o\nf t he\nan\nsw er\n\u2264 15\n\u2264 0\n.3 21\nIn c.\n\u2714\nQ : P\ned ro\nh as\ns av\ned $\n2 5\n00 . H\nis\nm ot\nhe r\npu t m\nor e\nm on\ney in\nth e\npi gg y ba nk . P ed ro ta ke s ou t t he m on ey a nd no w h as a to ta l o f $ 3 5 00 . H ow m uc h m on ey d id h is m ot he r gi ve h im ? St at e th e eq ua ti on . A : 2 .5 00 + X Is th e an sw er in co he re nt o r co\nhe re\nnt\nto th\ne op\nen -e\nnd ed\nq u\nes ti\non ?\nO ne\nM ai\nn C\nom pl\nex T\nas k\nB in\nar y\nSu bt\nas k\nQ ue\nst io\nns\nE xp\ner t F\nea tu\nre s\nD ec\nis io\nn Tr\nee P\nat h\nIn te\nrp re\nta bl e M L m od el\n\u2264 \u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264\nE xp\nla na\nti on\nF ea\ntu re\nE xt\nra ct\nio n\nN LL\nF E\nF\n\u2026\n\u2264 \u2264\n\u2264 \u2264\nTh e\nan sw\ner d\noe s\nno t c\non si\nde r\nal l\npo ss\nib le\no pt\nio ns\no r\npr ov\nid e\na ra\nti on\nal e\nfo r\ndi sc\nar di\nng\nin co\nrr ec\nt o pt\nio ns\nN um\nbe r\nof b\nin ar y w or ds in th e an sw er\nTh e\nan sw\ner d\noe s\nno t s\nho w\nun\nde rs\nta nd\nin g\nof\nth e\nq u\nes ti\non o r ev id en ce o f a ny at te m pt to s ol ve\nth e\nm at\nhe m\nat ic\nal\npr ob\nle m\nN um\nbe r\nof\npr on\nou ns\nth at\na re\nin\nth e\nan sw\ner a\nnd\nin th\ne q\nu es\nti on\nTh e\nan sw\ner d\noe s\nno t c\non si\nde r\nal l\npo ss\nib le\no pt\nio ns\no r\npr ov\nid e\na ra\nti on\nal e\nfo r\ndi sc\nar di\nng\nin co\nrr ec\nt o pt\nio ns\nPr op\nor ti\non o\nf d ig\nit s\nin th\ne an\nsw er\nTh e\nan sw\ner is\nne\nit he\nr hu\nm or\nou s\nno r\nin ap\npr op\nri at e to th e q u es ti on\nPr op\nor ti\non o\nf v ow\nel\nle tt\ner s\nin th e an sw er\nTh e\nan sw\ner is\ncl\nea r\nan d\nus es\nla\nng ua\nge a\nnd\nsp el\nlin g\nap pr\nop ri\nat e\nto th e q u es ti on p os ed\nTh e\nan sw\ner is\nn ot\ncl\nea r,\nc on\nci se\no r\nus es\nla ng\nua ge\nth at\nis\nn ot\ne as\ny to\nun\nde rs\nta nd\no r\npr ec\nis e\nPr op\nor ti\non o\nf v ow\nel\nle tt\ner s\nin th e an sw er\n> 7\n3. 7%\n\u2264 2\n3. 2%\nTh e\nan sw\ner d\noe s\nno t d\nem on\nst ra\nte\nad eq\nua te\nkn\now le\ndg e\nan d\nun de\nrs ta\nnd in\ng of\nth\ne su\nbj ec\nt m at\nte r\nof th\ne q\nu es\nti on\n\u2264 9\n2. 3%\nTh e\nan sw\ner is\ncl\nea r\nan d\nus es\nla\nng ua\nge a\nnd\nsp el\nlin g\nap pr\nop ri\nat e\nto th e q u es ti on p os ed\nTh e\nan sw\ner is\nn ot\ncl\nea r,\nc on\nci se\no r\nus es\nla ng\nua ge\nth at\nis\nn ot\ne as\ny to\nun\nde rs\nta nd\no r\npr ec\nis e\nFi gu\nre 13\n: E\nxp la\nna tio\nn of\nth e\nde ci\nsi on\ntr ee\ns re\nsu lts\nus in\ng th\ne fe\nat ur\nes N\nL L\nF+ E\nF fo\nr th\ne IA\nD ta\nsk .\nIn c.\nis in\nco he\nre nt\n,a nd\nC oh\n. is\nco he\nre nt\n. T\nhe ch\nec k\nm ar\nk m\nea ns\nth at\nth e\npr ed\nic tio\nn is\nco rr\nec t.\nTr an\nsl at\ned fr\nom Sp\nan is\nh.\nFi na\nl L ab\nel\nA rt\nic le\nA bs\ntr ac\nt\nG re\nen h\nou se\ng as\nm it\nig at\nio n\na n\nd\nof fs\net o\np ti\non s\nfo r\nsu ck\nle r\nco w\nfa\nrm s:\na n\ne co\nn om\nic c\nom p\nar is\non\nfo r\nth e\nS w\nis s\nca se\nW e\nas se\nss ed\nt he\ne co\nno m\nic s\nui ta\nbi lit\ny of\n4\ngr ee\nnh ou\nse\nga s\n(G H\nG )\nm it\nig at\nio n\nop ti\non s\nan d\non e\nG H\nG o\nff se\nt op\nti on\nf or\nan\nim pr\nov em\nen t\nof t\nhe G\nH G\nb al\nan ce\no f\na re\npr es\nen ta\nti ve\nS w\nis s\nsu ck\nle r\nco w\nfa rm\nho\nus in\ng 35\nLi\nve st\noc k\nun it\ns an d cu lt iv at in g 25 ha gr as sl an d. G H G em is si on s pe r ki lo gr am m ea t in th e ec on om ic o pt im um d iff er b et w ee n th e pr od uc ti on s ys te m s an d ra ng e fr om 1 8 to 2 1. 9 kg C O 2- eq ./ kg m ea t. O nl y G H G of fs et b y ag ro fo re st ry s ys te m s sh ow ed th e po te nt ia l to si gn ifi ca nt ly re du ce th es e em is si on s. D ep en di ng on th e pr od uc ti on sy st em ag ro fo re st ry sy st em s co ul d re du ce ne t G H G em is si on s by 6 6% to 7 .3 k g \u2026 (m or e)\nTh e\nar ti\ncl e\nex pl\nor e\nth e\nim pa\nct o f ag ro ec ol og\nic al\npr\nac ti\nce s\non\ncl im\nat e\nch an\nge\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nso il\nap pe\nar s\nin th\ne te\nxt\nTh e\nar ti\ncl e\nex am\nin e\nth e\nim pa\nct\nof a\ngr oe\nco lo\ngy o n ni tr og en d yn am\nic s\n> 9\n5. 9%\n> 3\n\u2264 1\n> 6\n2. 9%\n> 0\n.2 %\nI\n\u2714\nB in\nar y\nSu bt\nas k\nQ ue\nst io\nns\nLi ng\nui st\nic F\nea tu\nre s\nD ec\nis io\nn Tr\nee P\nat h\nIn te\nrp re\nta bl\ne M\nL m\nod el\nE xp\nla na\nti on\nF ea\ntu re\nE xt\nra ct\nio n\nN LL\nF E\nF\n\u2026\n\u2264 \u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264 \u2264\n\u2264\nC on\nse rv\nat io\nn a\ngr ic\nu lt\nu re\na n\nd\nec os\nys te\nm s\ner vi\nce s:\nA n\no ve\nrv ie w C on se rv at io n ag ri cu lt ur e (C A ) ch an ge s so il pr op er ti es a nd p ro ce ss es c om pa re d to co nv en ti on al ag ri cu lt ur e. Th es e ch an ge s ca n, in t ur n, a ff ec t th e de liv er y of e co sy st em s er vi ce s, in cl ud in g cl im at e re gu la ti on th ro ug h ca rb on se qu es tr at io n an d gr ee nh ou se ga s em is si on s, an d re gu la ti on an d pr ov is io n of w at er th ro ug h so il ph ys ic al , ch em ic al an d bi ol og ic al pr op er ti es . C on se rv at io n ag ri cu lt ur e ca n al so af fe ct th e un de rl yi ng bi od iv er si ty th at su pp or ts m an y ec os ys te m s er vi ce s. I n th is o ve rv ie w , w e su m m ar iz e th e cu rr en t st at us o f th e sc ie nc e, th e ga ps in u nd er st an di ng , a nd hi gh lig ht s om e re se ar ch p ri or it ie s fo r ec os ys te m se rv ic es in co ns er va ti on al ag ri cu lt ur e. T he r ev ie w is \u2026 (m or e)\nTh e\nar ti\ncl e\nex pl\nor e\nth e\nim pa\nct o f ag ro ec ol og\nic al\npr\nac ti\nce s\non\ncl im\nat e\nch an\nge\nTh e\nar ti\ncl e\nco ve r cl im at e ch an ge m it ig at io n\nTh e\nar ti\ncl e\ndo es\nno\nt d is\ncu ss\nag\nro ec\nol og\nic al\npr\nac ti\nce s\n> 9\n5. 9%\n\u2264 9\n7. 2%\n\u2264 0\n\u2264 8\n.3 %\n> 0\n.4 %\nE\n\u2714\nIs th\nis a\nn ab\nst ra\nct o\nf a p\nap er\na ss\nes si\nng th\ne im\npa ct\no f a\ngr oe\nco lo\ngi ca\nl p ra\nct ic\nes o n cl im at e ch an ge m it ig at io n or c lim at e ch an ge a da pt at io n? O ne M ai n C om pl ex T as kT he a rt ic le c ov er ag ro ec ol og ic al pr ac ti ce s\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nsy st\nem s\nap pe\nar s\nin th\ne te\nxt\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix c on ve nt\nap\npe ar\ns in\nth e\nte xt\nTh e\nar ti\ncl e\nev al\nua te\nag\nro ec\nol og\ny' s\nim pa\nct o\nn ni\ntr og\nen\ndy na\nm ic\ns\nFi gu\nre 14\n:E xp\nla na\ntio n\nof th\ne de\nci si\non tre\nes re\nsu lts\nus in\ng th\ne fe\nat ur\nes N\nLL F+\nEF fo\nrt he\nSA C\nta sk\n.I is\nfo rI\nnc lu\nde ,a\nnd E\nfo rE\nxc lu\nde .T\nhe ch\nec k\nm ar\nk m\nea ns\nth at\nth e\npr ed\nic tio\nn is\nco rr\nec t.\n12 52\n9/ 19\n28\nLe ng\nth o\nf t he\nan\nsw er\n\u2264 1\n5 >\n15\n78 5/\n12 67\nPr op\nor ti\non o f vo w el le tt er s\nin th e an sw er\n\u2264 0\n.3 21\n> o\n.3 21\n11 74\n4/ 66\n1\nTh e\nan sw\ner d\noe s\nno t c\non si\nde r\nal l\npo ss\nib le\no pt\nio ns\no r\npr ov\nid e\na ra\nti on\nal e\nfo r\ndi sc\nar di\nng\nin co\nrr ec\nt o pt\nio ns\n\u2264 1\n3. 3%\n23 9/\n81 6\nTh e\nan sw\ner is\nn ot\ncl\nea r,\nc on\nci se\no r\nus es\nla ng\nua ge\nth at\nis\nn ot\ne as\ny to\nun\nde rs\nta nd\no r\npr ec\nis e\n> 1\n3. 3%\n\u2264 7\n3. 7%\n> 7\n3. 7%\n54 6/\n45 1\nTh e\nan sw\ner h\nas a\nnu\nm be\nr\n= 0\n> 0\n8/ 37\n8\n\u2264 4\n8. 6%\n> 4\n8. 6%\n7/ 37\n8\nTh e\nan sw\ner\nsh ow\ns th e ca lc ul at io\nns o r pr oc es se s to a rr iv e at a n um er ic al va lu e\n\u2264 9\n2. 7%\n> 9\n2. 7%\n6/ 37 8 1/ 0\nC 0h\n. In\nc.\n76 /7\n8/ 29 In c.\nC 0h\n.\n42 /2 3 12 /2 58 In c. C 0h\n.\n85 /6\n13 /6\n6 In c.\nC 0h\n.\n23 /2\n0/ 45 In c.\nC 0h\n.\n83 68\n/2 33\nC 0h\n.\n1/ 4 In c.\n96 /3\n9\nTh e\nan sw\ner is\nn ot\ncl\nea r,\nc on\nci se\no r\nus es\nla ng\nua ge\nth at\nis\nn ot\ne as\ny to\nun\nde rs\nta nd\no r\npr ec\nis e\n\u2264 8\n4. 4%\n> 8\n4. 4%\n23 1/\n43 8\nTh e\nan sw\ner is\ncl\nea r\nan d\nus es\nla\nng ua\nge a\nnd\nsp el\nlin g\nap pr\nop ri\nat e\nto th e q u es ti on p os ed\n\u2264 2\n3. 2%\n> 2\n3. 2%\n54 /2\n81\nN um\nbe r\nof\nre le\nva nt\ns ub\nje ct s th at a re in th e\nan sw\ner a\nnd in\nth e\nq u\nes ti\non\n= 0\n> 0\n98 /7\n2\nN um\nbe r\nof ti\nm es\nth\ne q\nu es\nti on\na sk s w he th er s om eo ne is c or re ct o r no t.\n= 0\n> 0\n15 2/\n35 3\nN um\nbe r\nof b\nin ar y w or ds in th e an sw er\n= 0\n> 0\n23 /4\n7\nN um\nbe r\nof b\nin ar y w or ds in th e an sw er\n= 0\n> 0\n39 4/\n98\nN um\nbe r\nof ti\nm es\nth\ne q\nu es\nti on\na sk s w he th er s om eo ne is r ig ht /c or re ct o r w ro ng /i nc or re ct\n= 0\n> 0\n12 /1\nTh e\nan sw\ner d\noe s\nno t c\non ta\nin\nsp el\nlin g\nor\ngr am\nm at\nic al\ne rr\nor s\n\u2264 1\n7. 3%\n> 1\n7. 3%\n50 /1\nM ax\nim um\nn um\nbe r\nof c\non so\nna nt s re pe at ed in w or\nds\nof th\ne an\nsw er\n\u2264 9\n> 9\n38 /8\n9\nTh e\nan sw\ner d\noe s\nno t s\nho w\nun\nde rs\nta nd\nin g\nof\nth e\nq u\nes ti\non o r ev id en ce o f a ny at te m pt to s ol ve\nth e\nm at\nhe m\nat ic\nal\npr ob\nle m\n\u2264 8\n3. 2%\n> 8\n3. 2%\n51 /4\nN um\nbe r\nof\nem ot\nic on\ns in\nth e\nan sw\ner\n\u2264 2\n> 2\n89 /9\n3\nN um\nbe r\nof b\nin ar y w or ds in th e an sw er\n= 0\n> 0\n83 69\n/2 37\nPr op\nor ti\non o f di gi ts in th e an sw er\n\u2264 0\n.8 5\n> 0\n.8 5\n11 65\n1/ 55\n0\n\u2264 8\n9. 2%\n> 8\n9. 2%\n11 65\n5/ 56\n8\n\u2264 4\n> 4\n0. 0\nG in\ni v al\nue\nC 0C 1\n0. 5\n0. 0\nG in\ni v al\nue\nC oh\n.\nIn c.\n0. 5\nTh e\nan sw\ner d\noe s\nno t p\nro vi\nde a\nju\nst ifi\nca ti\non fo\nr th e ch oi ce o f t he\nco rr\nec t c\nha ra\nct er\nM ax\nim um\nn um\nbe r\nof c\non so\nna nt s re pe at ed in w or\nds\nof th\ne an\nsw er\nTh e\nan sw\ner is\nne\nit he\nr hu\nm or\nou s\nno r\nin ap\npr op\nri at e to th e q u es ti on\n1/ 0 C 0h\n.\n37 /5\n98 /3\n94 In c.\nC 0h\n.\n13 5/\n39 9\nTh e\nan sw\ner d\noe s\nno t d\nem on\nst ra\nte\nad eq\nua te\nkn\now le\ndg e\nan d\nun de\nrs ta\nnd in\ng of\nth\ne su\nbj ec\nt m at\nte r\nof th\ne q\nu es\nti on\n\u2264 9\n2. 3%\n> 9\n2. 3%\n37 1/\n51\nC 0h\n.\n26 /8\n8\nN um\nbe r\nof\npr on\nou ns\nth at\na re\nin\nth e\nan sw\ner a\nnd\nin th\ne q\nu es\nti on 7/0\n19 /8\n8 In c.\nC 0h\n.\n\u2264 1\n> 1\n0/ 1 In c.\n12 /0 C 0h\n.\n50 /0 C 0h\n.\n0/ 1 In c.\n1/ 0 C 0h\n.\n0/ 3 In c.\n1/ 3\n\u2264 4\n5. 4%\n> 4\n5. 4%\nTh e\nan sw\ner\ndi sc\nar ds\nin co\nrr ec t op ti on s w it h ju st ifi ca ti on\n32 82\n/3 13\nC 0h\n.\n4/ 18\nTh e\nan sw\ner d\noe s\nno t i\nnd ic\nat e\nw he\nth er\na\nch ar\nac te\nr ha\ns m\nor e\nor le\nss m\nat h\nkn ow\nle dg\ne th\nan\not he\nr ch\nar ac\nte rs\nin\nth e\nq u\nes ti\non\n\u2264 6\n5. 1%\n> 6\n5. 1%\n4/ 0\n0/ 18 In c.\nC 0h\n.\nFi gu\nre 15\n:D ec\nis io\nn tr\nee w\nith N\nL L\nF an\nd E\nF fo\nrt he\nIA D\nta sk\n.T ra\nns la\nte d\nfr om\nSp an\nis h.\nIn c.\nis in\nco he\nre nt\n,a nd\nC oh\n.i s\nco he\nre nt\n.T he\nN L\nL F\nar e\nin w\nhi te\nan d\nth e\nE F\nar e\nin gr\ney .\nT he\nnu m\nbe rs\ndo w\nn th\ne bo\nxe s\nar e\nth e\nth re\nsh ol\nd of\nth e\nde ci\nsi on\n.T he\nG in\niv al\nue of\na tr\nee no\nde co\nrr es\npo nd\ns to\nth e\nra tio\nof th\ne m\nin or\nita ri\nan cl\nas s\nel em\nen ts\nov er\nth e\ndo m\nin an\nto ne\ns in\nth e\nsu bt\nre e.\n1/ 6\n40 /1\n2 E I 41 /1 8 > 9\n9. 5%\n\u2264 9\n9. 5%\n0/ 8\n1/ 0 E\nI\n1/ 8\nTh e\nar ti\ncl e\nex am\nin e\nth e\nim pa\nct o f ag ro ec ol og\ny on\nni\ntr og\nen d\nyn am\nic s\n\u2264 0\n.2 %\n> 0\n.2 %\n20 /1 0 0/ 5 I E 20 /1 5\n\u2264 9\n.5 %\n> 9\n.5 %\n0/ 3\n8/ 0 E\nI\n3/ 0\n9/ 34 I\nE\n8/ 3\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nag ri\ncu ltu\nre\nap pe\nar s\nin th\ne te\nxt\n\u2264 1\n> 1\n12 /3\n4\nTh e\nar ti\ncl e\npr ov\nid e a co m pr eh en\nsi ve\nco\nve ra\nge o\nf c lim\nat e\nch an\nge a\nnd\nen vi\nro nm\nen ta l as pe ct s\n\u2264 9\n9. 6%\n> 9\n9. 6%\n10 /4 3 15 5/ 14 1 E I 16 5/ 18 4\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix a gr\no ap pe ar s in th e\nte xt\n\u2264 1\n> 1\n2/ 0\n1/ 9 I\nE\n3/ 9\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix b io a pp ea rs in th e te xt\n\u2264 2\n> 2\n23 3/\n34\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nsy st\nem s\nap pe\nar s\nin th\ne te\nxt > 1\n\u2264 1\n15 8/\n68\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nsy st\nem s\nap pe\nar s\nin th\ne te\nxt\n\u2264 3\n> 3\n40 /1\n6\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix in cr ea s\nap pe\nar s\nin th\ne te\nxt\n= 0\n> 0\n20 /3\n7\nTh e\nar ti\ncl e\ndi sc\nus s\nth e\nro le\no f\nag ro\nec ol\nog y\nin\nen ha\nnc in\ng cl\nim at e re si lie nc e in ag ri cu lt ur al sy st em s\n\u2264 0\n.5 %\n> 0\n.5 %\n24 8/\n38 6\n\u2264 0\n.8 %\n> 0\n.8 %\n65 /3\n2\nTh e\nar ti\ncl e\ndo es\nno\nt d is\ncu ss\nag\nro ec\nol og\nic al\npr\nac ti\nce s\n\u2264 0\n.4 %\n> 0\n.4 %\n39 1/\n10 2\nTh e\nar ti\ncl e\nco ve r ag ro ec ol og ic al pr ac ti ce s\n\u2264 6\n2. 9%\n> 6\n2. 9%\n60 /5\n3\n= 0\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nsy st\nem s\nap pe\nar s\nin th\ne te\nxt\n> 0\n31 3/\n41 8\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nm od\nel\nap pe\nar s\nin th\ne te\nxt\n= 0\n> 0\n52 /2\n11\n\u2264 8\n.3 %\n> 8\n.3 %\n81 6/\n78 4\n\u2264 9\n5. 9%\n45 1/\n15 5\n> 9\n5. 9%\n\u2264 1\n> 1\n36 5/\n62 9\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix c on ve nt\nap\npe ar\ns in\nth e\nte xt\n= 0\n> 0\n0. 0\nG in\ni v al\nue 0.\n5\nE I\nTh e\nar ti\ncl e\nex pl\nor e\nth e\nim pa\nct\nof a\ngr oe\nco lo\ngi ca l pr ac ti ce s on\ncl im\nat e\nch an\nge\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nso il\nap pe\nar s\nin th\ne te\nxt\nTh e\nar ti\ncl e\ndo es\nno\nt c ov\ner th e ef fe ct s of m et\nha ne\n(C\nH 4)\ne m\nis si\non s\nTh e\nar ti\ncl e\ndo es\nno\nt e xa\nm in e w he th er\nag ro\nec ol\nog ic\nal\npr ac\nti ce\ns af\nfe ct\ncl\nim at\ne ch\nan ge\nTh e\nar ti\ncl e\ndo es\nno\nt i nv\nes ti\nga te\nth e\nre la\nti on\nsh ip\nbe\ntw ee n ag ro ec ol og\nic al\npr\nac ti\nce s\nan d\ncl im\nat e\nch an\nge\nTh e\nar ti\ncl e\nev al\nua te\nag\nro ec\nol og\ny' s\nim pa\nct o\nn ni\ntr og\nen\ndy na\nm ic\ns\n19 0/ 11 2/ 5 I E 19 2/ 16 > 0\n.5 %\n\u2264 0\n.5 %\nTh e\nar ti\ncl e\ndo es\nno\nt a dd\nre ss\ncl\nim at\ne ch\nan ge\na nd\nen\nvi ro\nnm en\nta l\nis su\nes in\nd ep\nth\n13 /1 9 14 4/ 41 E I 15 7/ 60\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix c on ve nt\nap\npe ar\ns in\nth e\nte xt\n= 0\n> 0\n0/ 1\n20 /0 E\nI\n20 /1\nTh e\nar ti\ncl e\ndo es\nno\nt e xa\nm in e w he th er\nag ro\nec ol\nog ic\nal\npr ac\nti ce\ns af\nfe ct\ncl\nim at\ne ch\nan ge\n\u2264 9\n7. 7%\n> 9\n7. 7%\n83 /2\n02 I\n0/ 5\n62 /1\n8 E I 62 /2 3\n\u2264 1\n4. 4%\n> 1\n4. 4%\nTh e\nar ti\ncl e\ndo es\nno\nt a na\nly ze\nth e\nim pa\nct o f ag ro ec ol og\nic al\npr\nac ti\nce s\non\ncl im\nat e\nch an\nge\n7/ 2\n20 /7\n3 I E 27 /7 5\n= 0\n> 0\n1/ 0\n7/ 12\n6 I E\n0/ 3\n4/ 0 E\nI\n8/ 12\n6\nTh e\nar ti\ncl e\nex pl\nor e\nth e\nim pa\nct\nof a\ngr oe\nco lo\ngi ca l pr ac ti ce s on\ncl im\nat e\nch an\nge\n\u2264 9\n9. 9%\n> 9\n9. 9%\n4/ 3\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nfa rm\nin g\nap pe\nar s\nin th\ne te\nxt\n\u2264 1\n> 1\n40 /8\n2\nTh e\nar ti\ncl e\nco ve r cl im at e ch an ge m it ig at io n\n\u2264 9\n7. 2%\n> 9\n7. 2%\n12 /1\n29\nN um\nbe r\nof ti\nm es\nth\ne w\nor ds\nw it\nh th e pr ef ix r ed uc\nap pe\nar s\nin th\ne te\nxt\n\u2264 7\n> 7\nN um\nbe r\nof ti\nm es\nth\ne w\nor d\nsc en\nar io\nap\npe ar\ns in\nth e\nte xt\n13 /3\n0/ 4 I\nE\n13 /7\nTh e\nar ti\ncl e\ndo es\nno\nt d is\ncu ss\nag\nro ec\nol og\nic al\npr\nac ti\nce s\n\u2264 0\n.4 %\n> 0\n.4 %\nFi gu\nre 16\n:D ec\nis io\nn tre\ne w\nith N\nLL F\nan d\nLF fo\nrt he\nSA C\nta sk\n.I is\nfo rI\nnc lu\nde ,a\nnd E\nfo rE\nxc lu\nde .T\nhe N\nLL F\nar e\nin w\nhi te\nan d\nth e\nEF ar\ne in\ngr ey\n.T he\nnu m\nbe rs\ndo w\nn th\ne bo\nxe s\nar e\nth e\nth re\nsh ol\nd of\nth e\nde ci\nsi on\n.T he\nG in\niv al\nue of\na tr\nee no\nde co\nrr es\npo nd\ns to\nth e\nra tio\nof th\ne m\nin or\nita ri\nan cl\nas s\nel em\nen ts\nov er\nth e\ndo m\nin an\nto ne\ns in\nth e\nsu bt\nre e."
        }
    ],
    "title": "Deep Natural Language Feature Learning for Interpretable Prediction",
    "year": 2023
}