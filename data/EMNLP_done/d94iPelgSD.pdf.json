{
    "abstractText": "Event argument extraction is critical to various natural language processing tasks for providing structured information. Existing works usually extract the event arguments one by one, and mostly neglect to build dependency information among event argument roles, especially from the perspective of event structure. Such an approach hinders the model from learning the interactions between different roles. In this paper, we raise our research question: How to adequately model dependencies between different roles for better performance? To this end, we propose an intra-event and inter-event dependency-aware graph network, which uses the event structure as the fundamental unit to construct dependencies between roles. Specifically, we first utilize the dense intra-event graph to construct role dependencies within events, and then construct dependencies between events by retrieving similar events of the current event through the retrieval module. To further optimize dependency information and event representation, we propose a dependency interaction module and two auxiliary tasks to improve the extraction ability of the model in different scenarios. Experimental results on the ACE05, RAMS, and WikiEvents datasets show the great advantages of our proposed approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Li"
        },
        {
            "affiliations": [],
            "name": "Yanan Cao"
        },
        {
            "affiliations": [],
            "name": "Yubing Ren"
        },
        {
            "affiliations": [],
            "name": "Fang Fang"
        },
        {
            "affiliations": [],
            "name": "Lanxue Zhang"
        },
        {
            "affiliations": [],
            "name": "Yingjie Li"
        },
        {
            "affiliations": [],
            "name": "Shi Wang"
        }
    ],
    "id": "SP:a4f96f70fa9b9db11cf9a73464c13dff56c13449",
    "references": [
        {
            "authors": [
                "Yubo Chen",
                "Liheng Xu",
                "Kang Liu",
                "Daojian Zeng",
                "Jun Zhao."
            ],
            "title": "Event extraction via dynamic multipooling convolutional neural networks",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
            "year": 2015
        },
        {
            "authors": [
                "Fan RK Chung."
            ],
            "title": "Spectral graph theory, volume 92",
            "venue": "American Mathematical Soc.",
            "year": 1997
        },
        {
            "authors": [
                "Lu Dai",
                "Bang Wang",
                "Wei Xiang",
                "Yijun Mo."
            ],
            "title": "Bi-directional iterative prompt-tuning for event argument extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6251\u20136263, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Weischedel."
            ],
            "title": "The automatic content extraction (ACE) program \u2013 tasks, data, and evaluation",
            "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC\u201904), Lisbon, Portugal. European Language Resources As-",
            "year": 2004
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Event extraction by answering (almost) natural questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 671\u2013683, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Xinya Du",
                "Sha Li",
                "Heng Ji."
            ],
            "title": "Dynamic global memory for document-level argument extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5264\u20135275, Dublin, Ireland. As-",
            "year": 2022
        },
        {
            "authors": [
                "Seth Ebner",
                "Patrick Xia",
                "Ryan Culkin",
                "Kyle Rawlins",
                "Benjamin Van Durme."
            ],
            "title": "Multi-sentence argument linking",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057\u20138077, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "I-Hung Hsu",
                "Kuan-Hao Huang",
                "Elizabeth Boschee",
                "Scott Miller",
                "Prem Natarajan",
                "Kai-Wei Chang",
                "Nanyun Peng."
            ],
            "title": "DEGREE: A data-efficient generation-based event extraction model",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Kung-Hsiang Huang",
                "Nanyun Peng."
            ],
            "title": "Document-level event extraction with efficient end-to-end learning of cross-event dependencies",
            "venue": "Proceedings of the Third Workshop on Narrative Understanding, pages 36\u201347, Virtual. Association",
            "year": 2021
        },
        {
            "authors": [
                "Yusheng Huang",
                "Weijia Jia."
            ],
            "title": "Exploring sentence community for document-level event extraction",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 340\u2013351, Punta Cana, Dominican Republic. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Fayuan Li",
                "Weihua Peng",
                "Yuguang Chen",
                "Quan Wang",
                "Lu Pan",
                "Yajuan Lyu",
                "Yong Zhu."
            ],
            "title": "Event extraction as multi-turn question answering",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 829\u2013838, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji",
                "Liang Huang."
            ],
            "title": "Joint event extraction via structured prediction with global features",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73\u201382, Sofia, Bulgaria.",
            "year": 2013
        },
        {
            "authors": [
                "Sha Li",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Document-level event argument extraction by conditional generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Jian Liu",
                "Yubo Chen",
                "Kang Liu",
                "Wei Bi",
                "Xiaojiang Liu."
            ],
            "title": "Event extraction as machine reading comprehension",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1641\u20131651, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jian Liu",
                "Yufeng Chen",
                "Jinan Xu."
            ],
            "title": "Machine reading comprehension as data augmentation: A case study on implicit event argument extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2716\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Heyan Huang",
                "Ge Shi",
                "Bo Wang."
            ],
            "title": "Dynamic prefix-tuning for generative template-based event extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5216\u20135228,",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Zhunchen Luo",
                "Heyan Huang."
            ],
            "title": "Jointly multiple events extraction via attention-based graph information aggregation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1247\u20131256, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Yaojie Lu",
                "Hongyu Lin",
                "Jin Xu",
                "Xianpei Han",
                "Jialong Tang",
                "Annan Li",
                "Le Sun",
                "Meng Liao",
                "Shaoyi Chen"
            ],
            "title": "Text2Event: Controllable sequence-tostructure generation for end-to-end event extraction",
            "year": 2021
        },
        {
            "authors": [
                "Yubo Ma",
                "Zehao Wang",
                "Yixin Cao",
                "Mukai Li",
                "Meiqi Chen",
                "Kun Wang",
                "Jing Shao."
            ],
            "title": "Prompt for extraction? PAIE: Prompting argument interaction for event argument extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Thien Huu Nguyen",
                "Kyunghyun Cho",
                "Ralph Grishman."
            ],
            "title": "Joint event extraction via recurrent neural networks",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2016
        },
        {
            "authors": [
                "Thien Huu Nguyen",
                "Ralph Grishman."
            ],
            "title": "Event detection and domain adaptation with convolutional neural networks",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
            "year": 2015
        },
        {
            "authors": [
                "Yubing Ren",
                "Yanan Cao",
                "Fang Fang",
                "Ping Guo",
                "Zheng Lin",
                "Wei Ma",
                "Yi Liu."
            ],
            "title": "CLIO: Roleinteractive multi-event head attention network for document-level event extraction",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Yubing Ren",
                "Yanan Cao",
                "Ping Guo",
                "Fang Fang",
                "Wei Ma",
                "Zheng Lin."
            ],
            "title": "Retrieve-and-sample: Document-level event argument extraction via hybrid retrieval augmentation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Compu-",
            "year": 2023
        },
        {
            "authors": [
                "Lei Sha",
                "Feng Qian",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "Jointly extracting event triggers and arguments by dependency-bridge RNN and tensor-based argument interaction",
            "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "David Wadden",
                "Ulme Wennberg",
                "Yi Luan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Entity, relation, and event extraction with contextualized span representations",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Ziqi Wang",
                "Xiaozhi Wang",
                "Xu Han",
                "Yankai Lin",
                "Lei Hou",
                "Zhiyuan Liu",
                "Peng Li",
                "Juanzi Li",
                "Jie Zhou."
            ],
            "title": "CLEVE: Contrastive Pre-training for Event Extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Xian Sun",
                "Zequn Zhang",
                "Jingyuan Zhang",
                "Guo Zhi",
                "Li Jin."
            ],
            "title": "Trigger is not sufficient: Exploiting frame-aware knowledge for implicit event argument extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Runxin Xu",
                "Tianyu Liu",
                "Lei Li",
                "Baobao Chang."
            ],
            "title": "Document-level event extraction via heterogeneous graph-based interaction model with a tracker",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Runxin Xu",
                "Peiyi Wang",
                "Tianyu Liu",
                "Shuang Zeng",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "A two-stream AMR-enhanced model for document-level event argument extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Sen Yang",
                "Dawei Feng",
                "Linbo Qiao",
                "Zhigang Kan",
                "Dongsheng Li."
            ],
            "title": "Exploring pre-trained language models for event extraction and generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5284\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Zhisong Zhang",
                "Xiang Kong",
                "Zhengzhong Liu",
                "Xuezhe Ma",
                "Eduard Hovy."
            ],
            "title": "A two-step approach for implicit event argument detection",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7479\u20137485, Online.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Event Argument Extraction (EAE) aims to identify the arguments of a given event and recognize the specific roles they play, which is one of the important subtasks of Event Extraction (EE). As illustrated by the example in Figure 1, given a disperseseparate event triggered by shipment, an event argument extractor is expected to identify Russia, arms, Armenia as the event arguments and predict their roles as (Transporter, Origin), Passenger, and Destination, where the Russia argument plays the roles of Transporter and Origin respectively.\n*Corresponding Author.\nTypical efforts in EAE can be roughly divided into classification-based models (Zhang et al., 2020; Huang and Jia, 2021; Xu et al., 2022; Ma et al., 2022; Ren et al., 2022), Question Answering (QA)-based models (Du and Cardie, 2020; Liu et al., 2020; Li et al., 2020; Wei et al., 2021; Liu et al., 2021), and generation-based models (Li et al., 2021; Lu et al., 2021; Hsu et al., 2022; Ren et al., 2023). In these works, arguments are usually extracted one by one, which mostly neglects to build dependency information among argument roles, especially from the perspective of event structure. For example, classification-based methods typically identify candidate spans and classify their roles independently; QA-based models recognize the boundaries of arguments with role-specific questions; Generation-based methods need to construct role-specific templates for argument filling. Although the state-of-the-art (SOTA) method (Ma et al., 2022) extracts all arguments of an event endto-end, it still does not explicitly capture dependency among roles.\nHowever, the roles of different arguments within the same event or between similar events clearly\ndepend on each other. Firstly, a complete event usually consists of multiple closely related roles, and establishing dependencies between them can provide intra-event core cues for each other. Furthermore, we count that 11.36% of arguments in RAMS (Ebner et al., 2020) have multiple roles, in which case modeling dependencies is more vital. As shown in Figure 1, the Russia plays the roles of Transporter and Origin, respectively. It is intuitive that capturing the dependencies between Transporter and Origin and jointly learning their semantic information can result in mutual gain. Secondly, similar events may have similar event structures and share partial roles, so modeling dependencies between roles in similar events can provide inter-event role correlation clues. According to our statistics, 76.98% and 44.00% event types in the ontologies of RAMS and WikiEvents (Li et al., 2021) datasets share more than three roles, respectively. The disperseseparate event and smuggleextract event in Figure 1 both describe the event of arms shipments between countries, and they share the roles of Transporter, Origin, and Destination, where utilizing the correlation between these two events can be mutually beneficial. Therefore, capturing role dependencies within the same event is essential, and modeling role dependencies between similar events is also crucial.\nBuilt on these motivations, we raise our research question: How to adequately model dependencies between different roles for better performance? To this end, we propose an intra-event and interEvent Dependency-aware Graph network for EAE (EDGE), which uses the event structure as the fundamental unit to build dependencies. Specifically, we first utilize the dense intra-event graph to capture the role dependencies within events. Furthermore, we propose a retrieval module to store and retrieve similar events of the current event, using the retrieved intra-event graph to assist in constructing an inter-event graph for capturing dependencies between events. To propagate the intra-event and inter-event dependency and filter redundant information, we propose a dependency interaction module to fully model the dependency information at different granularities. Finally, to further optimize the event representation, we propose two auxiliary tasks to improve the argument extraction ability of the model in different scenarios.\nWe summarize our contributions as follows:\n\u2022 We propose an intra-event and inter-event\ndependency-aware graph network for EAE, which can help learn rich role semantic information without using manual label templates.\n\u2022 When constructing the inter-event graph, we further introduce a retrieval module to retrieve the most similar event to the current event from the memory unit.\n\u2022 We conduct experiments on three widely used EAE datasets. Through experiments and visual analysis, our model achieves new SOTA due to the dependency modeling, demonstrating its effectiveness."
        },
        {
            "heading": "2 Related Works",
            "text": "Sentence-Level Event Extraction SEE extracts the event trigger and its arguments from a single sentence. It has achieved great progress in previous studies. Li et al. (2013) employ various handdesigned features to extract events. With the popularity of deep learning, researchers use various neural networks to extract events, such as convolutional neural network (CNN)-based models (Chen et al., 2015; Nguyen and Grishman, 2015), recurrent neural network (RNN)-based models (Nguyen et al., 2016; Sha et al., 2018) and graph neural networks (GNN)-based models (Liu et al., 2018). As pre-trained language models (PLM) have been proven to be powerful in language understanding and generation (Devlin et al., 2019), Yang et al. (2019); Wadden et al. (2019); Du and Cardie (2020); Wang et al. (2021); Lu et al. (2021); Hsu et al. (2022); Liu et al. (2022); Dai et al. (2022) use PLM-based methods to extract events.\nDocument-Level Event Extraction Considering that real-world events are often distributed across sentences, DEE has attracted more attention recently. Different from SEE, DEE extracts the event trigger and its arguments from the whole document. On the task level, most of these works fall into three categories: (1) classification-based models (2) QA-based models (3) generation-based models. Specifically, Zhang et al. (2020); Xu et al. (2021); Huang and Jia (2021); Huang and Peng (2021); Xu et al. (2022); Ma et al. (2022); Ren et al. (2022) employ traditional classification paradigms to identify arguments from text and classify the roles they play in events; Wei et al. (2021); Liu et al. (2021) treat DEE as extractive question answering task, where the extracted arguments are based on carefully constructed natural questions; With the help\nof the pre-trained Encoder-Decoder Transformer architecture (Lewis et al., 2020), Li et al. (2021); Du et al. (2022); Ren et al. (2023) convert the extraction task to a sequence-to-sequence generation task, and get the arguments and corresponding roles in the event in a generated way."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first formulate the EAE task, then introduce the overall framework of our model, as illustrated in Figure 2, which contains four core components: Role-Aware Encoding, BiGranularity Dependency Construction, Intra2Inter Dependency Interaction, and Dependency-Aware Argument Extraction.\nTask Definition We first describe the task formalization of EAE. Formally, given a context X = {x1, ..., xn} of n words, the event trigger word xt \u2208 X , the event type t \u2208 T , where T = {t1, ..., tu} is the set of all u different event types and the set of event-specific role types Rt. The EAE task aims to extract all the arguments {a1, ..., ad} related to xt in X , and assign a role r \u2208 Rt to each extracted argument ai, where ai is a text span in X ."
        },
        {
            "heading": "3.1 Role-Aware Encoding",
            "text": "Given context X and the set of event-specific role types Rt, this module generates event-oriented context representation and context-oriented role representation. We use pre-trained BART (Lewis et al., 2020) as our encoding model, a standard\nTransformer-based pre-trained language model consisting of both an Encoder and a Decoder.\nFollowing previous works (Ma et al., 2022), we feed the context into BART-Encoder and the role into BART-Decoder separately. Specifically, given the current trigger word xt, we first define the trigger markers \u2329t\u232a and \u2329/t\u232a as special tokens and insert them into context X before and after the trigger word xt to create an input context X\u0303 . In the next step, the BART-Encoder is employed to encode the input context X\u0303 to obtain the sequence representation Hencx . Finally, benefiting from mutual interaction information at the cross-attention layers in the decoder module, the BART-Decoder is applied to learn richer representations for the context and roles, returning Hx = BART-Decoder(Hencx ;H enc x ) and Hr = BART-Decoder(R\u0303t;Hencx ) for the context and roles, where R\u0303t denotes connecting different roles ri with learnable, role-specific pseudo tokens, Hx denotes the event-oriented context representation, Hr = {hr1 , ..., hrd} denotes context-oriented role representation, and d denotes the maximum number of roles for this event1."
        },
        {
            "heading": "3.2 Bi-Granularity Dependency Construction",
            "text": "The dependency construction module constructs two granularities of dependencies: intra-event dependency and inter-event dependency.\nIntra-Event Dependency Considering the oneto-many form of trigger words and arguments in the\n1The number of each role is determined by the maximum number of arguments for this role in the training set.\nevent structure, the roles participating in the same event are closely related, so the intra-event graph representing the dependency within the event is a fully connected graph. For each event type t, the nodes vi \u2208 Vl in the event are context-oriented role representation hri \u2208 Hr, using the co-occurrence information of roles of event type t in the training set as dependency weight:\n\u03b1tij = count(t, ri, rj)\ncount(t, ri) (1)\nFinally, after normalized symmetric (Chung, 1997) A\u0303 = D\u2212 1 2AD\u2212 1 2 , where D denotes degree matrix. The intra-event graph with weight \u03b1\u0303tij is used to enhance the representations for hri via a Graph Convolutional Network (GCN) (Kipf and Welling, 2017) with K layers:\nzki = ReLU( \u2211 vj\u2208Vl \u03b1\u0303tijz k\u22121 j w k + bk) (2)\nwhere wk and bk are learnable parameters, and zki represents the role representation for vi at the k-th layer of GCN (z0i = hri).\nInter-Event Dependency Different from intraevent dependencies, inter-event dependencies are represented by the inter-event graph to construct role dependency information between different events. In order to model the role dependency information between different events, we propose a memory unit M to store all the intra-event graphs in the form of tensors, which are dynamically updated according to the corresponding indexes at each training epoch. The intra-event graph obtained by the retrieval module will be used to construct the inter-event graph, and the dependencies between different events will be constructed. Specifically, we first use cosine similarity in the retrieval module to retrieve the intra-event graph of the event most similar to the current event:\nS(mi|mc) = f(Hri) \u00b7 f(Hrc)\n||f(Hri)|| \u00d7 ||f(Hrc)|| ms = argmax\nmi\u2208M (S(mi|mc)) (3)\nwhere f represents the mean pooling operation, mc and ms denote the current event and the most similar event, respectively. mi \u2208 M denotes the event in the memory unit, and the corresponding Hrc and Hri represent the context-oriented role representation of the current event and the event in the memory unit respectively.\nAfterward, we use the intra-event graphs of the current event mc and the most similar event ms to construct the inter-event graph. Specifically, the nodes Vg of the inter-event graph are the roles of multiple event types, which are obtained by mapping the corresponding nodes in the intra-event graphs of the current event and the most similar event. The dependency weights of the inter-event graph are constructed from the training set based on the co-occurrence information of all roles:\n\u03b2ij = count(ri, rj)\ncount(ri) (4)\nDifferent from the intra-event graph, the interevent graph captures dependencies between events, so its nodes and edges correspond to multiple event types and roles, not just a single event type. Finally, the representation of the inter-event graph is learned using a GCN similar to Equation 2."
        },
        {
            "heading": "3.3 Intra2Inter Dependency Interaction",
            "text": "We use two different GCNs to learn the representation of the intra-event graph and the inter-event graph. To propagate dependency information at different granularities, we align nodes representing the same roles in the intra-event and inter-event graph at each layer of the GCN: Zkl = Z k g = (Zkl +Z k g )/2, where Z k l and Z k g represent the node representation of the intra-event and inter-event graph at the k-th layer of the GCN, respectively. The dependency representations are obtained by the above two K-layer GCNs and K times of dependency interaction, and finally the dependency information is assigned to the context representations by concatenating: H \u2032r = FFN(concat(Hr, Z K l )), where FFN is a feed-forward network.\nEvent Structure Constraint To learn and model the event structure information, we add two additional event structure constraints: the role constraint, which uses the nodes of the dependency graph to predict the role type, and the event constraint, which uses the entire dependency graph to predict the event type:\nLr = \u2211 X\u2208D \u2211 h\u2032ri\u2208H \u2032 r \u2212 log(Softmax(h\u2032riw r))\nLe = \u2211 X\u2208D \u2212 log(Softmax(f(H \u2032r)we)) (5) where wr and we are learnable parameters, Lr and Le represent role constraint loss and event constraint loss respectively, and D ranges over all context in the dataset."
        },
        {
            "heading": "3.4 Dependency-Aware Argument Extraction",
            "text": "Given the context representation Hx and the set of dependency-aware role representation H \u2032r, each h\u2032rk aims to extract the corresponding argument span (sk, ek) from the context X , where sk and ek are the start and end word indices of the argument. The span (sk, ek) will be set to (0, 0) if h\u2032rk has no corresponding argument (the current event has no argument about the role, or the argument-slot number of this role exceeds the actual argument number). For each h\u2032rk , we calculate the distribution of each token being selected as the start and end position of the argument (Du and Cardie, 2020; Ma et al., 2022):\nlogitstartk = (h \u2032 rk \u2299 wstart)Hx \u2208 Rn\nlogitendk = (h \u2032 rk \u2299 wend)Hx \u2208 Rn (6)\nwhere wstart and wend are learnable parameters, and \u2299 represents the element-wise multiplication.\nWe calculate probabilities where the start and end positions are located as follows:\npstartk = Softmax(logit start k ) \u2208 Rn\npendk = Softmax(logit end k ) \u2208 Rn (7)\nThe argument extraction loss is defined as:\nLk(X) = \u2212(log pstartk (sk) + log pendk (ek))\nLa = \u2211 X\u2208D d\u2211 k=1 Lk(X) (8)\nThe overall loss function is divided into three parts: argument extraction loss La, role constraint loss Lr, and event constraint loss Le:\nL = La + \u03bb1Lr + \u03bb2Le (9)\nand \u03bb1, \u03bb2 are hyper-parameters."
        },
        {
            "heading": "3.5 Inference",
            "text": "During inference, we consider all candidate spans of the argument as C = {(i, j)|(i, j) \u2208 n2, 0 < j \u2212 i \u2264 \u03b4} \u222a {(0, 0)}, ensuring that the length of all spans does not exceed the threshold \u03b4 and (0, 0) means there is no argument. The argument of each h\u2032rk is extracted by enumerating and scoring all candidate spans as:\nscorek(i, j) = logitstartk (i) + logit end k (j) (10)\nThe span with the highest score will be selected as the prediction result. Specifically, the prediction span of h\u2032rk is computed:\n(s\u0302k, e\u0302k) = argmax (i,j)\u2208C scorek(i, j) (11)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets We conduct experiments on three commonly used EAE datasets: RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021) and ACE05 (Doddington et al., 2004). RAMS and WikiEvents are widely used document-level EAE datasets, and ACE05 is another classic dataset for sentence-level EAE tasks. Detailed statistics are listed in Appendix A.\nEvaluation Metrics We measure the performance with three evaluation metrics: (1) Argument Identification (Arg-I): an event argument is correctly identified if its offsets and event type match those of any of the argument mentions. (2) Argument Classification (Arg-C): an event argument is correctly classified if its role type is also correct. (3) For WikiEvents dataset, we follow previous work (Li et al., 2021), additionally evaluate Argument Head Classification (Head-C), which only concerns the matching of the headword of an argument. The F-measure (F1) score is used to evaluate the performance of the model.\nImplementation Details We initialize our models with the pre-trained BART base models (Lewis et al., 2020). For important hyper-parameters and details, please refer to Appendix B.\nBaselines We compare our model to several representative and competitive baselines:\n\u2022 EEQA (Du and Cardie, 2020) treats sentencelevel EAE as a QA task, and obtains the start and end offsets of the argument spans through question answering.\n\u2022 FEAE (Wei et al., 2021) is a QA-based method extended to EAE by considering argument interactions via knowledge distillation.\n\u2022 DocMRC (Liu et al., 2021) is another QAbased method, assisted by two data augmentation regimes.\n\u2022 BART-Gen (Li et al., 2021) defines EAE as a sequence-to-sequence task and generates corresponding arguments in a predefined format.\n\u2022 PAIE (Ma et al., 2022) is a prompt-based method. It extracts argument spans by constructing role templates and is the current SOTA model. For a fair comparison with our model, we use its soft prompt setting."
        },
        {
            "heading": "4.2 Overall Performance",
            "text": "Table 1 compares our method with all baselines on the ACE05, RAMS, and WikiEvents datasets. We have several observations and discussions:\n(1) Our method consistently outperforms the latest baselines and achieves the SOTA performance. Specifically, our method achieves 75.3 Arg-I F1 and 70.6 Arg-C F1 on ACE05. On the RAMS dataset, Our method improves 1.3~8.8 ArgI F1 and 1.7~5.7 Arg-C F1 over previous methods. On the WikiEvents dataset, our method improves 2.0~20.7 Arg-I F1, 2.7~21.1 Arg-C F1 and 2.1~21.7 Head-C F1. These results demonstrate that our method is superior to previous methods in both argument identification and classification.\n(2) We find that extracting all arguments from an event end-to-end is more efficient than extracting arguments one at a time. Compared with EEQA, the end-to-end methods (our method and PAIE) achieve different degrees of performance improvement, which demonstrates the existence of rich implicit dependency information between roles. Independent role-specific QA methods can only extract one argument at a time, ignore this dependency information, and still struggle with the task of argument identification and classification.\n(3) Compared to the current SOTA model PAIE,\nour model not only achieves better results on all evaluation metrics but also achieves higher performance gains on the more difficult Arg-C metric of document-level EAE. This result shows that simply implicitly constructing dependencies between argument roles is not sufficient for EAE. By explicitly constructing dependencies between roles, our model can more accurately identify and extract the corresponding arguments."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "We conduct ablation studies to investigate the effectiveness of each component of our approach and the results are shown in Table 2. Specifically, \u201cw/o inter-event dependency\u201d means removing inter-event dependencies to quantify its effectiveness, \u201cw/o retrieval module\u201d means removing the retrieval module and using only current events for graph representation, \u201cw/o role constraint\u201d and \u201cw/o event constraint\u201d mean that the two auxiliary constraints are not considered respectively.\nWe can find that each module can help boost the performance of EAE, removing any module will lead to different degrees of performance degrada-\ntion. To be specific, (1) Without inter-event dependency, the performance drops dramatically by about 1.9 points in terms of Arg-C F1. This justifies that it is necessary to build dependencies between events and establish connections between different events. (2) Removing the retrieval module has a performance drop of 1.2 Arg-C F1, which indicates that our retrieval module can improve task performance by efficiently retrieving similar events. (3) Similar observations can be found in other modules, removing the role constraint and event constraint results in different degrees of performance degradation, indicating that the event structure is helpful for argument identification and classification."
        },
        {
            "heading": "5 Analysis on Real Scenario",
            "text": ""
        },
        {
            "heading": "5.1 Same-Argument Role Assignment",
            "text": "In the EAE task, a complete event usually includes multiple arguments and corresponding roles. Furthermore, an argument may play more than one role in an event, and extracting these arguments is more important for event understanding, but also more difficult. In order to explore the performance of our\nmodel in this scenario, we split the data according to the number of roles corresponding to the argument. Figure 3(a) shows the experimental results in scenarios with different numbers of roles. Compared with the baseline, our method achieves better results in all scenarios and exhibits better robustness. It is worth noting that our method achieves greater performance improvement as the number of roles corresponding to arguments increases, which indicates the necessity of building dependencies among the roles in such scenarios."
        },
        {
            "heading": "5.2 Same-Role Argument Assignment",
            "text": "In addition to one argument playing multiple roles, there may be multiple arguments playing the same role in a complex event. These arguments are indispensable in the event, so it is important to extract them to understand the complex event. In order to explore the performance of our method in this scenario, we split the data according to the number of arguments corresponding to the role, and the experimental results are shown in Figure 3(b). Our method achieves better performance than the baseline in all scenarios, especially in the most complex scenarios where the number of arguments is no less than three, achieving 8.0 Arg-C F1 gains. This demonstrates that our method can alleviate the argument and role assignment and more accurately extract multiple arguments for the same role by building intra-event and inter-event dependencies to learn semantic information between roles."
        },
        {
            "heading": "5.3 Role Performance Analysis",
            "text": "To examine the performance on the role of the longtail distribution (Ebner et al., 2020), we calculate the Arg-C F1 of the method on each role. The experimental results are shown in Figure 4. Compared with the PAIE, our method achieves the same\nor better results on 71.43% (20/28) roles. Specifically, our method achieves a performance improvement of more than 10 Arg-C F1 on multiple roles, such as instrument (15.79), origin (13.17), and employee (11.11). All these observations verify that our method significantly improves the extraction performance of different argument roles by constructing intra-event and inter-event role dependencies, which helps alleviate the long-tail problem."
        },
        {
            "heading": "5.4 Visualization Analysis",
            "text": "In order to explore whether our method fully models the semantic information of argument roles, we map all roles on the RAMS development set to the 2-dimensional vector space through the tDistributed Stochastic Neighbor Embedding (tSNE) (Van der Maaten and Hinton, 2008) method for visual analysis. The visualization results are shown in Figure 5, where different roles are displayed in different colors. Compared with w/o Dep, our method can cluster the roles of similar semantics together according to the semantic information of the context, showing better intra-role and inter-role distance under different events. This indicates that by constructing dependencies between argument roles and event structure constraints, our method can better learn the semantic information of roles in different events."
        },
        {
            "heading": "5.5 Case Study",
            "text": "In this subsection, we show an example from RAMS dataset to illustrate the capability of our method. The example is presented in Figure 6, including the input document, event argument extraction results, and the most similar event retrieved\nfrom EDGE. Specifically, EDGE can accurately extract all the arguments, while w/o Dep fails. At the same time, the input documents and retrieved events are armed attacks that cause citizens to be injured, indicating that our retrieval module can accurately retrieve similar events and construct the inter-event role dependencies between these events. The results show that our method can retrieve similar events with similar semantics and event structure as the input document, and all arguments in events can be accurately extracted by constructing role dependencies between events."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a novel method based on role dependencies for EAE. Specifically, we first utilize the intra-event graph and the inter-event graph retrieved from the intra-event graph to automatically construct intra-event and inter-event dependencies, and then fully learn and model the dependencies of roles in different scenarios through two different GCNs. To further optimize dependency information and event representation, we propose a dependency interaction module and two auxiliary tasks to improve the argument extraction ability of the model in different scenarios. We conduct extensive experiments on three EAE datasets and compare our model with several strong baselines. The results show that our model achieves SOTA performance and outperforms all baselines in different scenarios. In further work, we are interested in constructing dependency information for other information extraction tasks and would like to apply our model to these tasks, such as named entity recognition and relation extraction.\nLimitations\nWe discuss the limitations of our research as follows:\n\u2022 Our method uses additional graph convolutional networks to construct the dependency of argument roles, resulting in an increase in training parameters, and one training process occupies nearly one NVIDIA V100 32GB GPU.\n\u2022 Our method requires event ontology information to construct intra-event and inter-event graphs, and is not suitable for scenarios where event ontology is unknown."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Key Research and Development Program of China (NO.2022YFB3102200) and Strategic Priority Research Program of the Chinese Academy of Sciences with No. XDC02030400."
        },
        {
            "heading": "A Dataset statistics",
            "text": "RAMS focuses on a document-level EAE task, including 139 event types and 65 argument roles, with more than 9k events. WikiEvents is another widely used document-level EAE dataset, which contains 50 event types and 59 argument roles, containing more than 3.9k events. ACE05 is a sentencelevel extraction dataset, including 33 event types and 35 argument roles, with more than 5k events. We follow the official data split of each dataset. For ACE05, we follow the pre-processing procedure of DyGIE++ (Wadden et al., 2019). The specific statistics of the datasets are listed in Table 3 and 4.\nB Implementation Details\nIn our implementations, we use the HuggingFace\u2019s Transformers library to implement the BART base model. The model is optimized with the AdamW optimizer (Loshchilov and Hutter, 2019). Other important hyper-parameters are shown in Table 5."
        }
    ],
    "title": "Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction",
    "year": 2023
}