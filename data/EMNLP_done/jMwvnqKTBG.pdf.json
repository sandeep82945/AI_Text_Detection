{
    "abstractText": "Writing strong arguments can be challenging for learners. It requires to select and arrange multiple argumentative discourse units (ADUs) in a logical and coherent way as well as to decide which ADUs to leave implicit, so called enthymemes. However, when important ADUs are missing, readers might not be able to follow the reasoning or understand the argument\u2019s main point. This paper introduces two new tasks for learner arguments: to identify gaps in arguments (enthymeme detection) and to fill such gaps (enthymeme reconstruction). Approaches to both tasks may help learners improve their argument quality. We study how corpora for these tasks can be created automatically by deleting ADUs from an argumentative text that are central to the argument and its quality, while maintaining the text\u2019s naturalness. Based on the ICLEv3 corpus of argumentative learner essays, we create 40,089 argument instances for enthymeme detection and reconstruction. Through manual studies, we provide evidence that the proposed corpus creation process leads to the desired quality reduction, and results in arguments that are similarly natural to those written by learners. Finally, first baseline approaches to enthymeme detection and reconstruction demonstrate the corpus\u2019 usefulness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maja Stahl"
        },
        {
            "affiliations": [],
            "name": "Nick D\u00fcsterhus"
        },
        {
            "affiliations": [],
            "name": "Mei-Hua Chen"
        },
        {
            "affiliations": [],
            "name": "Henning Wachsmuth"
        }
    ],
    "id": "SP:982dba85b49780c6fed03afe9e9fcb0ca4e9eed7",
    "references": [
        {
            "authors": [
                "Tazin Afrin",
                "Diane Litman."
            ],
            "title": "Annotation and classification of sentence-level revision improvement",
            "venue": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 240\u2013246, New Orleans, Louisiana.",
            "year": 2018
        },
        {
            "authors": [
                "Khalid Al Khatib",
                "Henning Wachsmuth",
                "Johannes Kiesel",
                "Matthias Hagen",
                "Benno Stein."
            ],
            "title": "A news editorial corpus for mining argumentation strategies",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Lin-",
            "year": 2016
        },
        {
            "authors": [
                "Milad Alshomary",
                "Nick D\u00fcsterhus",
                "Henning Wachsmuth."
            ],
            "title": "Extractive snippet generation for arguments",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201920,",
            "year": 2020
        },
        {
            "authors": [
                "Milad Alshomary",
                "Shahbaz Syed",
                "Martin Potthast",
                "Henning Wachsmuth."
            ],
            "title": "Target inference in argument conclusion generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4334\u20134345, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Aristotle. ca"
            ],
            "title": "On Rhetoric: A Theory of Civic Discourse",
            "year": 2007
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Filip Boltu\u017ei\u0107",
                "Jan \u0160najder."
            ],
            "title": "Fill the gap! Analyzing implicit premises between claims from online debates",
            "venue": "Proceedings of the Third Workshop on Argument Mining (ArgMining2016), pages 124\u2013 133, Berlin, Germany. Association for Computational",
            "year": 2016
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Aadit Trivedi",
                "Smaranda Muresan."
            ],
            "title": "Implicit premise generation with discourse-aware commonsense knowledge models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Fan Chen",
                "Mei-Hua Chen",
                "Garima Mudgal",
                "Henning Wachsmuth."
            ],
            "title": "Analyzing culturespecific argument structures in learner essays",
            "venue": "Proceedings of the 9th Workshop on Argument Mining, pages 51\u201361, Online and in Gyeongju, Republic",
            "year": 2022
        },
        {
            "authors": [
                "Thi Hanh Dang",
                "Thanh Hai Chau",
                "To Quyen Tra."
            ],
            "title": "A study on the difficulties in writing argumentative essays of english-majored sophomores at tay do university, vietnam",
            "venue": "European Journal of English Language Teaching, 6(1).",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "G\u00fcnes Erkan",
                "Dragomir R. Radev."
            ],
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "venue": "J. Artif. Int. Res., 22(1):457\u2013479.",
            "year": 2004
        },
        {
            "authors": [
                "Vanessa Wei Feng",
                "Graeme Hirst."
            ],
            "title": "Classifying arguments by scheme",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 987\u2013996, Portland, Oregon, USA. Association for",
            "year": 2011
        },
        {
            "authors": [
                "Sylviane Granger",
                "Estelle Dagneaux",
                "Fanny Meunier",
                "Magali Paquot."
            ],
            "title": "The International Corpus of Learner English",
            "venue": "Presses universitaires de Louvain, Louvain-la-Neuve.",
            "year": 2009
        },
        {
            "authors": [
                "Sylviane Granger",
                "Ma\u00eft\u00e9 Dupont",
                "Fanny Meunier",
                "Hubert Naets",
                "Magali Paquot."
            ],
            "title": "The International Corpus of Learner English",
            "venue": "Version 3. Presses universitaires de Louvain, Louvain-la-Neuve.",
            "year": 2020
        },
        {
            "authors": [
                "Timon Gurcke",
                "Milad Alshomary",
                "Henning Wachsmuth."
            ],
            "title": "Assessing the sufficiency of arguments through conclusion generation",
            "venue": "Proceedings of the 8th Workshop on Argument Mining, pages 67\u201377, Punta Cana, Dominican Republic. Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Ivan Habernal",
                "Henning Wachsmuth",
                "Iryna Gurevych",
                "Benno Stein."
            ],
            "title": "The argument reasoning comprehension task: Identification and reconstruction of implicit warrants",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTa: Decodingenhanced BERT with disentangled attention",
            "venue": "CoRR, abs/2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani",
                "Sofie Van Landeghem",
                "Adriane Boyd"
            ],
            "title": "spacy: Industrialstrength natural language processing in python",
            "year": 2020
        },
        {
            "authors": [
                "Dirk Hovy",
                "Taylor Berg-Kirkpatrick",
                "Ashish Vaswani",
                "Eduard Hovy."
            ],
            "title": "Learning whom to trust with MACE",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2013
        },
        {
            "authors": [
                "Ken Hyland."
            ],
            "title": "Metadiscourse: Exploring Interaction in Writing",
            "venue": "Bloomsbury Classics in Linguistics. Bloomsbury Academic, London, England.",
            "year": 2018
        },
        {
            "authors": [
                "Ralph H. Johnson",
                "J. Anthony Blair"
            ],
            "title": "Logical self-defense",
            "year": 2006
        },
        {
            "authors": [
                "Maleerat Ka-kan-dee",
                "Sarjit Kaur."
            ],
            "title": "Argumentative writing difficulties of thai english major students",
            "venue": "The 2014 WEI International Academic Conference Proceedings, pages 193\u2013207.",
            "year": 2014
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "Tie-Yan Liu."
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing",
            "year": 2017
        },
        {
            "authors": [
                "Joseph J Lee",
                "Lydia Deakin."
            ],
            "title": "Interactions in l1 and l2 undergraduate student writing: Interactional metadiscourse in successful and less-successful argumentative essays",
            "venue": "Journal of second language writing, 33:21\u201334.",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Richard Liaw",
                "Eric Liang",
                "Robert Nishihara",
                "Philipp Moritz",
                "Joseph E Gonzalez",
                "Ion Stoica."
            ],
            "title": "Tune: A research platform for distributed model selection and training",
            "venue": "arXiv preprint arXiv:1807.05118.",
            "year": 2018
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Zhexiong Liu",
                "Diane Litman",
                "Elaine Wang",
                "Lindsay Matsumura",
                "Richard Correnti."
            ],
            "title": "Predicting the quality of revisions in argumentative writing",
            "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA",
            "year": 2023
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "TextRank: Bringing order into text",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404\u2013411, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Oluwatoyin Enikuomehin",
                "T. Odunowo Adebisi",
                "S. Mustapha Oluwatoyin"
            ],
            "title": "An algorithm for the reconstruction of enthymemes for effective machine translation",
            "venue": "International Journal of Applied Information Systems, 12(31):1\u20137.",
            "year": 2020
        },
        {
            "authors": [
                "Burhan Ozfidan",
                "Connie Mitchell."
            ],
            "title": "Detected difficulties in argumentative writing: The case of culturally and linguistically saudi backgrounded students",
            "venue": "Journal of Ethnic and Cultural Studies, 7(2):pp. 15\u201329.",
            "year": 2020
        },
        {
            "authors": [
                "Lawrence Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd."
            ],
            "title": "The pagerank citation ranking: Bringing order to the web",
            "venue": "Technical Report 1999-66, Stanford InfoLab. Previous number = SIDLWP-1999-0120.",
            "year": 1999
        },
        {
            "authors": [
                "Isaac Persing",
                "Alan Davis",
                "Vincent Ng."
            ],
            "title": "Modeling organization in student essays",
            "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229\u2013239, Cambridge, MA. Association for Computational Linguis-",
            "year": 2010
        },
        {
            "authors": [
                "Isaac Persing",
                "Vincent Ng."
            ],
            "title": "Modeling thesis clarity in student essays",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 260\u2013 269, Sofia, Bulgaria. Association for Computational",
            "year": 2013
        },
        {
            "authors": [
                "Isaac Persing",
                "Vincent Ng."
            ],
            "title": "Modeling argument strength in student essays",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol-",
            "year": 2015
        },
        {
            "authors": [
                "Zahra Rahimi",
                "Diane J. Litman",
                "Richard Correnti",
                "Lindsay Clare Matsumura",
                "Elaine Wang",
                "Zahid Kisa."
            ],
            "title": "Automatic scoring of an analytical responseto-text assessment",
            "venue": "Intelligent Tutoring Systems, pages 601\u2013610, Cham. Springer International Pub-",
            "year": 2014
        },
        {
            "authors": [
                "Pavithra Rajendran",
                "Danushka Bollegala",
                "Simon Parsons."
            ],
            "title": "Contextual stance classification of opinions: A step towards enthymeme reconstruction in online reviews",
            "venue": "Proceedings of the Third Workshop on Argument Mining (ArgMining2016), pages",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Ameer Saadat-Yazdi",
                "Jeff Z. Pan",
                "Nadin Kokciyan."
            ],
            "title": "Uncovering implicit inferences for improved relational argument mining",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2484\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Agustinus Bambang Balinga Saputra",
                "Jumariati",
                "Emma Rosana Febriyanti."
            ],
            "title": "Efl students\u2019 problems in writing argumentative essays",
            "venue": "Proceedings of the 2nd International Conference on Education, Language, Literature, and Arts (ICELLA 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Gabriella Skitalinskaya",
                "Maximilian Splieth\u00c3\u00b6ver",
                "Henning Wachsmuth"
            ],
            "title": "Claim optimization in computational argumentation",
            "venue": "In Proceedings of the 16th International Natural Language Generation Conference,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriella Skitalinskaya",
                "Henning Wachsmuth."
            ],
            "title": "To revise or not to revise: Learning to detect improvable claims for argumentative writing support",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2023
        },
        {
            "authors": [
                "Regina L. Smalley",
                "Mary K. Ruetten",
                "Joann Rishel Kozyrev."
            ],
            "title": "Refining Composition Skills: Rhetoric and grammar",
            "venue": "Heinle and Heinle Boston.",
            "year": 2001
        },
        {
            "authors": [
                "Christian Stab",
                "Iryna Gurevych."
            ],
            "title": "Annotating argument components and relations in persuasive essays",
            "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1501\u20131510, Dublin,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Stab",
                "Iryna Gurevych."
            ],
            "title": "Parsing argumentation structures in persuasive essays",
            "venue": "Computational Linguistics, 43(3):619\u2013659.",
            "year": 2017
        },
        {
            "authors": [
                "Christian Stab",
                "Iryna Gurevych."
            ],
            "title": "Recognizing insufficiently supported arguments in argumentative essays",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages",
            "year": 2017
        },
        {
            "authors": [
                "Henning Wachsmuth",
                "Khalid Al-Khatib",
                "Benno Stein."
            ],
            "title": "Using argument mining to assess the argumentation quality of essays",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages",
            "year": 2016
        },
        {
            "authors": [
                "Henning Wachsmuth",
                "Johannes Kiesel",
                "Benno Stein."
            ],
            "title": "Sentiment flow \u2014 A general model of web review argumentation",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 601\u2013611, Lisbon, Portugal.",
            "year": 2015
        },
        {
            "authors": [
                "Henning Wachsmuth",
                "Nona Naderi",
                "Yufang Hou",
                "Yonatan Bilu",
                "Vinodkumar Prabhakaran",
                "Tim Alberdingk Thijm",
                "Graeme Hirst",
                "Benno Stein."
            ],
            "title": "Computational argumentation quality assessment in natural language",
            "venue": "Proceedings of the 15th Con-",
            "year": 2017
        },
        {
            "authors": [
                "Douglas Walton",
                "Christopher Reed",
                "Fabrizio Macagno."
            ],
            "title": "Argumentation Schemes",
            "venue": "Cambridge University Press.",
            "year": 2008
        },
        {
            "authors": [
                "Thiemo Wambsganss",
                "Christina Niklaus",
                "Matthias Cetto",
                "Matthias S\u00f6llner",
                "Siegfried Handschuh",
                "Jan Marco Leimeister."
            ],
            "title": "AL: An adaptive learning support system for argumentation skills",
            "venue": "Proceedings of the 2020 CHI Conference on Human",
            "year": 2020
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Gurcke"
            ],
            "title": "2021)\u2019s approach, and (iii) the ADU generated by the approach BARTaugmented. D.1 Definitions and Scales Clarity The argument has a high clarity if \u201cit",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Argumentative writing is an essential skill that can be challenging to acquire (Ozfidan and Mitchell, 2020; Dang et al., 2020; Saputra et al., 2021). To write a logically strong or cogent argument, a writer has to present sufficient evidence (i.e., the argument\u2019s premises) that justifies the argument\u2019s claim (i.e., its conclusion). In addition, arguments draw on premises that are left implicit, so called enthymemes (Feng and Hirst, 2011), which may be key to match the expectations of the audience (Habernal et al., 2018). However, if a relevant premise is missing, the argument may be insufficient to accept its conclusion (Johnson and Blair,\n2006). Similarly, the quality of an argument will also often be limited if the conclusion is missing, as this easily renders unclear what the argument claims (Alshomary et al., 2020b).\nStudies show that learners often fail to provide solid evidence for their claims (Ka-kan-dee and Kaur, 2014) as well as to compose their arguments in a coherent way (Dang et al., 2020). NLP may help learners improve the quality of their arguments by providing feedback on the arguments\u2019 logical quality. For example, the feedback could point to enthymematic gaps in arguments and make suggestions on how to fill these gaps. Until now, research on assessing logical quality dimensions of learner argumentation has focused on the amount of evidence an essay provides (Rahimi et al., 2014), and the sufficiency of its arguments, i.e., whether the premises together give enough reason to accept the conclusion (Stab and Gurevych, 2017b; Gurcke et al., 2021). To the best of our knowledge, no approach explicitly studies the adequacy of enthymemes in learner arguments yet.\nTo initiate research in this direction, we introduce two new tasks on learner arguments: enthymeme detection and enthymeme reconstruction. We define an enthymeme broadly here as any miss-\ning argumentative discourse unit (ADU) that would complete the logic of a written argument. The goal of the first task is to assess whether there is a possibly problematic enthymematic gap at a specified position of an argument. Given an argument with such a gap, the goal of the second task is then to generate a new ADU that fills the gap. Approaches to both tasks may help learners improve the quality of their arguments by pointing to locations of enthymemes and making suggestions on how to reconstruct the enthymemes if desired.\nData for studying enthymemes in learner arguments is neither available so far, nor is it straightforward to write respective arguments in a natural way. Therefore, we study how to automatically create a corpus for the two tasks from existing argument corpora. In particular, we propose a self-supervised process that carefully removes ADUs from highquality arguments while maintaining the text\u2019s naturalness. We rank candidate ADUs for removal by both their contribution to an argument\u2019s quality and their centrality within an argument, combining essay scoring methods (Wachsmuth et al., 2016) with a PageRank approach (Page et al., 1999). Furthermore, we ensure the naturalness of the remaining argument (with an enthymematic gap) by utilizing BERT\u2019s capabilities to predict whether a sentence follows another sentence (Devlin et al., 2019). We create enthymemes only that cannot be detected by a BERT model. Table 1 shows an example of an enthymeme created by our approach.\nBased on the third version of the ICLE corpus of argumentative learner essays (Granger et al., 2020), we automatically create 40,089 argument instances for enthymeme detection as well as 17,762 argument instances for enthymeme reconstruction. An evaluation of the created corpus suggests that our enthymeme creation approach leads to the desired quality reduction in arguments. In addition, manual annotators of the argument\u2019s naturalness deemed our modified arguments similarly natural to the original ones hand-written by the learners. Finally, we develop baseline approaches to enthymeme detection and enthymeme reconstruction based on DeBERTa (He et al., 2020) and BART (Lewis et al., 2020), respectively. Experimental results provide evidence for the usefulness of the corpus for studying the two presented tasks.\nAltogether, this paper\u2019s main contributions are:1\n1The code to reproduce the data and results can be found under: https://github.com/webis-de/EMNLP-23\n\u2022 An automated approach for creating corpora of enthymematic arguments in learner essays\n\u2022 A corpus for studying two new NLP tasks on learner arguments: enthymeme detection and enthymeme reconstruction\n\u2022 Baseline approaches to detecting and reconstructing enthymemes computationally"
        },
        {
            "heading": "2 Related Work",
            "text": "Originally, Aristotle (ca. 350 BCE / translated 2007) referred as enthymeme to a specific presumptive argument scheme in which certain premises are left unstated intentionally. Nowadays, the term is used both for arguments with one or more missing or implicit premises (Walton et al., 2008) and for the implicit premises themselves (Feng and Hirst, 2011). We adopt the latter use here and also cover implicit conclusions, which are often left unstated for rhetorical reasons (Al Khatib et al., 2016).\nThe detection of enthymemes has been studied in empirical analyses (Boltu\u017eic\u0301 and \u0160najder, 2016) and with computational methods (Rajendran et al., 2016; Saadat-Yazdi et al., 2023). Also, their reconstruction with generation methods has come into focus recently (Alshomary et al., 2020b; Oluwatoyin et al., 2020; Chakrabarty et al., 2021). For a specific type of enthymemes, namely warrants which explain the connection between premises and conclusions, Habernal et al. (2018) created a dataset with 1,970 pairs of correct and incorrect instances to be used for comprehending debate portal arguments. In contrast, we target learner arguments in this work, particularly to enable research on the question how to identify inadequate enthymemes, that is, those that should not be left implicit.\nArgumentative writing poses many challenges for learners, particularly in the organization and development of arguments (Dang et al., 2020; Ozfidan and Mitchell, 2020). Learners commonly struggle with unclear, irrelevant, or missing thesis statements (Ozfidan and Mitchell, 2020) and face difficulties providing substantial evidence as argument premises to support their opinions (Ka-kandee and Kaur, 2014; Ozfidan and Mitchell, 2020). In some cases, learners address the claims but fail to adequately support them by omitting certain reasons (Ozfidan and Mitchell, 2020). Formulating clear conclusions, also known as the claims of arguments, is another area where learners often falter (Lee and Deakin, 2016; Skitalinskaya and Wachsmuth, 2023). These findings indicate that\ninadequate enthymemes are a problem in learner arguments, which, so far, has not been extensively explored using computational methods.\nArgument mining, the automated process of extracting argumentative discourse units (ADUs) and their relationships, has demonstrated its effectiveness in analyzing argumentative essays written by learners (Stab and Gurevych, 2014, 2017a). We do not study argument mining, but we leverage it to filter candidate enthymemes in our corpus creation approach. In line with Chen et al. (2022), we consider the ADU types premise, claim, major claim, and non-argumentative.\nMoreover, we employ argument quality assessment in the corpus creation process (Wachsmuth et al., 2016). Essay argumentation has been evaluated across various quality dimensions such as an essay\u2019s organization (Persing et al., 2010), the clarity of its thesis (Persing and Ng, 2013), the strength of its arguments altogether (Persing and Ng, 2015), and the sufficiency of each individual argument (Stab and Gurevych, 2017b). This has also facilitated the development of approaches to generate missing argumentative ADUs, specifically argument conclusions (Gurcke et al., 2021). In a related line of research, approaches have been proposed to suggest specific revisions of argumentative essays (Afrin and Litman, 2018), to assess the quality of revisions (Liu et al., 2023), and to perform argument revisions computationally (Skitalinskaya et al., 2023). Our proposed enthymeme-related tasks complement these attempts, having the same ultimate goal, namely to learn support systems for argumentative writing (Wambsganss et al., 2020).\nIn conclusion, the computational study of enthymemes in learner arguments is a fairly unexplored research area, and the automatic detection and reconstruction of inadequate enthymemes in this domain have not been addressed as of now. Developing corpora specifically designed to analyze inadequate enthymemes in learner arguments may provide valuable insights into common pitfalls and enable the development of effective methods for helping learners to improve their argumentative writing skills in this regard."
        },
        {
            "heading": "3 Automatic Corpus Creation",
            "text": "This section presents our approach to automatic corpus creation. Given a base corpus of argumentative learner texts as input, we propose to create enthymematic arguments automatically by remov-\ning argumentative discourse units (ADUs) from high-quality arguments in these texts. In the following, we detail the three parts that constitute our approach: (a) finding candidate arguments, (b) finding candidate enthymemes in these arguments, and (c) ranking the candidates to remove the most suitable one per argument. Figure 1 illustrates the corpus creation process."
        },
        {
            "heading": "3.1 Finding Candidate Arguments",
            "text": "We consider only arguments (i) from essays of high quality that (ii) have a specified minimum length. With these filtering steps, we aim to minimize the risk of already existing, unknown enthymemes within an argument.\nEssay Quality Filtering We model three essaylevel quality dimensions that have been well studied in previous work: organization, thesis clarity, and argument strength (Persing et al., 2010; Persing and Ng, 2013, 2015; Wachsmuth et al., 2016; Chen et al., 2022). To assess these dimensions for the base essays, we train one LightGBM regression model (Ke et al., 2017) per quality dimension, using a combination of new features and reimplemented features from Wachsmuth et al. (2016):2\n\u2022 ADU n-grams, with n \u2208 {1, 2, 3} \u2022 Sentiment flow (Wachsmuth et al., 2015)\n\u2022 Discourse function flow (Persing et al., 2010)\n\u2022 Prompt similarity flow\n\u2022 Token and POS n-grams\n\u2022 Length statistics\n\u2022 Frequency of linguistic errors (LanguageTool)\n\u2022 Distribution of named entity types\n\u2022 Distribution of metadiscourse type markers (Hyland, 2018)\nWe performed ablation tests to determine the best feature combination per quality dimension in terms of mean squared error (mse). Details on all features and the best combinations can be found in Appendix A. Table 2 shows the performance of our essay scoring models with the best-performing feature combinations. Our models are on par with the above-mentioned state of the art approaches, even being best in predicting argument strength.\nFor filtering, we average the three scores per essay to an overall score between 1.0 (very bad) to 4.0 (very good) (Persing and Ng, 2015) and keep\n2Initial experiments on predicting the essay quality dimensions using Longformer (Beltagy et al., 2020) led to lower performance compared to the feature-based approaches, prompting us to stop further investigations in this direction.\nonly the essays with an overall score above 3.0, as they should be of decent quality.\nArgument Length Filtering As a simple heuristic, we expect essay paragraphs with at least four sentences to be arguments, since we observed that shorter paragraphs are unlikely to be substantial enough to contain a full argument. Furthermore, removing one ADU from an argument with three or fewer sentences may likely corrupt the argument beyond creating an inadequate enthymeme.3"
        },
        {
            "heading": "3.2 Finding Candidate Enthymemes",
            "text": "From the candidate arguments, we select ADUs as candidate enthymemes in two ways: (i) We ensure that removing the ADU does not destroy the argument\u2019s naturalness, that is, the arguments still read like they were written by a human; and (ii) we require an ADU to be argumentative in order to be selected as a candidate enthymeme. Otherwise, it might not reduce the logical quality of an argument.\nNaturalness ADU Filtering We aim to create enthymemes that are likely to appear in humanwritten arguments. To prevent the remaining argument from being unnatural, we check whether a candidate enthymeme can be detected by BERT\u2019s next sentence prediction (Devlin et al., 2019): We retain only those candidates for which a BERT model predicts that the sentences before and after it could be neighbors, that is, the enthymeme does not disrupt the text\u2019s naturalness. The text naturalness is evaluated manually in Section 5.2.\nArgumentativeness ADU Filtering Removing a non-argumentative ADU from an argument does not necessarily create an inadequate enthymeme. Thus, we predict the ADU type of argument sentences adopting the approach of Chen et al. (2022), which distinguishes between premise, claim, major claim, and non-argumentative. Additionally, we consider only ADUs with more than five tokens significant enough to an argument to create an inadequate enthymeme based on manual inspection."
        },
        {
            "heading": "3.3 Ranking Candidate Enthymemes",
            "text": "To find the most suitable candidate enthymemes, we use a PageRank-based approach (Page et al., 1999) that ranks ADUs according to two criteria:\n3We also only create enthymematic arguments of at most 500 tokens so that together with added special tokens, they fit the maximum input length of most common language models.\n(i) their centrality within an essay and (ii) their contribution to the essay\u2019s quality. We hypothesize that this enables us to choose high-quality ADUs for removal that are critical to an argument and therefore leave a quality-reducing, inadequate enthymeme. We take the whole essay into consideration to have as much context as possible for estimating the centrality and quality contribution of the ADUs. This way, we can also reuse our essay scoring models. After ranking, we determine the highest-ranked candidate enthymeme on argument level.\nEnthymeme Centrality Ranking Previous work has modeled a document as a graph where each node corresponds to a sentence, and the weights are derived from sentence similarity. This allows estimating the centrality of the sentences by calculating the sentence rank (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Alshomary et al., 2020a). We adapt this idea by constructing a graph that represents an essay\u2019s sentences and title by encoding each sentence si as a vector si using sBERT embeddings (Reimers and Gurevych, 2019). From this, we construct a similarity matrix A\u2032 with\na\u2032ij := cos(si, sj) = si \u00b7 sj\n||si|| \u00b7 ||sj || . (1)\nSince cosine similarity lies in the interval [\u22121, 1], we apply a softmax on each row A\u2032i to obtain a stochastic matrix A, with Ai := softmax(A\u2032i).\nQuality Reduction Ranking We aim to remove sentences that contribute to the quality of an argument in the sense that the created enthymemes harm the quality. Since there is, to our knowledge, no research on the quality contributions of individual sentences in the context of an essay, we estimate it by removing the sentences and observing the change in essay scores. In more detail, given the essay score q{1,...,n} of an essay with n sentences, we estimate the quality contribution ci of the i\u2212th sentence to be:\nci := q{1,...,n} \u2212 q{1,...,i\u22121,i+1,...,n} (2)\nWe use our essay scoring models to determine the quality scores for each candidate enthymeme. For the other sentences, we use the average quality score of all sentences in the essay.\nAs for the sentence centrality, we construct a quality graph, represented as matrix B\u2032, where each sentence si is linked to each other sentence sj with the weight being the quality contribution of sj , i.e.\nb\u2032ij := cj . To align the value range of the quality scores with the centrality scores, we use min-max scaling before applying the softmax function:\nBi := softmax( B\u2032i \u2212min(B\u2032i)\nmax(B\u2032i)\u2212min(B\u2032i) ) (3)\nPageRank-based Ranking To rank the sentences by both criteria, their centrality and their quality contribution, we combine the matrixes A and B into one transition matrix M :\nM := 0.5 \u00b7A+ 0.5 \u00b7B (4)\nThe final ranking is obtained by applying the power iteration method. Accordingly, the sentences with the highest rank are expected to be those that balance high centrality within the essay and great contribution to the quality of the essay."
        },
        {
            "heading": "4 Corpus",
            "text": "This section summarizes the corpora that we use for model training and for automatic corpus creation. Then, we give details about the composition and statistics of the resulting created corpus."
        },
        {
            "heading": "4.1 Base Corpora",
            "text": "To train essay scoring models, we needed essay quality data. For this, we reused the 1,003 argumentative learner essays rated for organization by (Persing et al., 2010), 830 essays rated for thesis clarity (Persing and Ng, 2013), and 1,000 essays rated for argument strength (Persing and Ng, 2015). These essays come from the second version of the International Corpus of Learner English, ICLEv2 (Granger et al., 2009).\nThe basis for our automatically-created corpus is the third version of the same corpus, ICLEv3 (Granger et al., 2020). It contains 8,965 argumentative essays written by learners with 25 different language backgrounds."
        },
        {
            "heading": "4.2 Corpus Composition and Statistics",
            "text": "Each filtering step described above resulted in the number of essays, arguments, and candidate enthymemes shown in Table 3. The steps applied filters to essays, arguments, or ADUs directly, which mostly resulted in indirect effects on other levels.\nWith a probability of 80%, we removed the highest-ranked ADU of an argument with at least one candidate enthymeme to create two instances: (a) One positive example with a separator token marking the correct enthymeme position and (b)"
        },
        {
            "heading": "Data # Essays # Arguments # Candidates",
            "text": "one negative example with the separator token at a random, incorrect position. In the remaining cases, the original argument without deletion and with a separator token at a random position is added as a negative example to improve model robustness. This results in 17,762 positive and 22,327 negative examples, with 40,089 in total. Within the positive examples, 63.28% of the enthymemes are premise removals, 30.97% claim removals, and 5.75% major claim removals, according to the ADU type classifiers based on Chen et al. (2022)\u2019s work. We randomly split the corpus into training, validation, and test set using a ratio of 7:1:2.\nTo provide insights into the behavior of our approach, Figure 2 visualizes the enthymeme positions in our corpus compared to a randomlygenerated corpus created by removing random argumentative ADUs with more than five tokens (argumentativeness ADU filtering only). The distribution of enthymeme positions shows a slight preference for the first ADU, which often introduces the topic and stance and is central to the argument. The positional bias can largely be attributed to differences in argument length."
        },
        {
            "heading": "5 Corpus Analysis",
            "text": "This section reports on our evaluation of the effectiveness of our automatic approach for creating inadequate enthymemes that are harmful to the quality of arguments while not making their texts seem unnatural. To this end, we manually assessed (a) the induced reduction in logical argument quality in terms of sufficiency and (b) the naturalness of the enthymematic arguments in comparison to the original arguments written by learners."
        },
        {
            "heading": "5.1 Manual Sufficiency Evaluation",
            "text": "In argumentation theory, sufficiency captures whether an argument\u2019s premises together make it\nrationally worthy of drawing its conclusion (Johnson and Blair, 2006), which captures our intended goal of having only adequate enthymemes well. We relax the notion of sufficiency here to also account for missing argument conclusions, and we deem an argument sufficient if (1) its premises together give enough reason to accept the conclusion and (2) its conclusion is clear. Given this definition, we evaluate the change in sufficiency resulting from the created enthymemes, that is, whether it is more, equally, or less sufficient than the original argument. Recall that the goal of our approach is to create insufficient arguments.\nFor evaluation, we randomly chose 200 original arguments from the given corpus and their modified versions created with four different methods:\n(a) Our full approach\n(b) Our approach without centrality ranking (i.e., M := B instead of Equation 4)\n(c) Our approach without quality reduction ranking (i.e., M := A)\n(d) A baseline that randomly removes any argumentative ADU with at least five tokens4\nWe hired five native or bilingual English speakers via Upwork for this task and paid 13$ per hour of work. The annotation guidelines can be found in Appendix B. To model each annotator\u2019s reliability, we use MACE for combining the non-expert annotators\u2019 labels into final labels (Hovy et al., 2013).\nTable 4 shows the distribution of MACE labels. The results suggest that all compared approaches\n4The baseline equals the modified arguments from the randomly created corpus, (Random Corpus).\nreduce the arguments\u2019 sufficiency in most cases, which is expected from the definition of sufficiency. However, our full approach works best in decreasing the original argument\u2019s sufficiency, outperforming its two variations and the random baseline. Additionally, our full approach to enthymeme creation is least likely to cause an increase in sufficiency. The average pairwise inter-annotator agreement in terms of Kendall\u2019s \u03c4 is 0.25."
        },
        {
            "heading": "5.2 Manual Naturalness Evaluation",
            "text": "To assess the impact of our naturalness ADU filtering (Section 3.2), we manually evaluated the naturalness of the arguments with enthymematic gaps in comparison to the original learner arguments. For this, three authors of this paper manually inspect a random sample of 100 arguments: 50 original learner arguments and 50 arguments modified by our full approach. We scored naturalness on a 3-point scale:\n1. Artificial. A part of the argument is unrelated to the preceding or succeeding text. For example, a reference cannot be resolved (cohesion), the text abruptly changes its theme or does not have any continuous theme (coherence), or some definitely required information is missing (clarity).\n2. Partly Natural. The text is mainly coherent, its use of cohesive devices does not affect continuity or comprehensibility, and the information provided suffices to get the author\u2019s point.\n3. Natural. The text is coherent, its use of cohesive devices is correct, and the information provided allows to clearly understand the author\u2019s point.\nThe results of the manual naturalness evaluation are shown in Table 5. Both original and modified arguments achieve a mean score slightly above partial naturalness. This indicates that the arguments written by learners already have some naturalness issues. Additionally, creating enthymematic gaps using our approach only slightly reduces the arguments\u2019 naturalness. The average pairwise interannotator agreement in terms of Kendall\u2019s \u03c4 is 0.27."
        },
        {
            "heading": "6 Experiments",
            "text": "Our automatically-created corpus is meant to enable studying two new tasks on learner arguments: Enthymeme detection and enthymeme reconstruction. This section presents baselines for the tasks in order to demonstrate the usefulness of the corpus."
        },
        {
            "heading": "6.1 Enthymeme Detection",
            "text": "We treat enthymeme detection as a binary classification task: Given a learner argument and a position within the argument, predict whether there is an inadequate enthymeme (say, a logical gap) at the given position. We mark the positions in question using a separator token, [SEP]. Furthermore, we use the token type IDs to differentiate between the sequences before and after the separator token.\nModels For classification, we fine-tune the language model DeBERTa (He et al., 2020) with 134M parameters, 12 layers, and a hidden size of 768 (microsoft/deberta-base) provided by Huggingface (Wolf et al., 2020). To quantify its learning success, we also report the results of a random classifier that makes random predictions and a majority classifier that always predicts the majority training class.\nExperimental Setup We tune the training hyperparameters of DeBERTa in a grid-search manner using Raytune (Liaw et al., 2018). We explored numbers of epochs in {8, 16, 24} with learning rates in {10\u22126, 5 \u00b7 10\u22126, 10\u22125} and batch size 16. The\nbest F1-score on the validation set was achieved by training for 24 epochs with a learning rate of 10\u22125.\nResults Table 6 shows the classification results. The learning success of the DeBERTa model suggests the possibility of automating the task of enthymeme detection in learner arguments on our corpus. However, further improvements using more advanced approaches are expected and encouraged."
        },
        {
            "heading": "6.2 Enthymeme Reconstruction",
            "text": "We treat enthymeme reconstruction as a maskinfilling task: Given a learner argument and a known enthymeme position within the argument, generate the enthymeme. We mark the enthymeme position with a special token, [MASK]. For this task, corpus instances without enthymemes are excluded since they lack target text.\nModels For mask-infilling, we fine-tune BART models (Lewis et al., 2020) with 400M parameters, 24 layers, and a hidden size of 1024 (facebook/bartlarge) provided by Huggingface (Wolf et al., 2020). We compare two different approaches: First, we adapt the approach of Gurcke et al. (2021) to argument conclusion generation to our task by providing the model with the argument and the mask token at the enthymeme position as input. To evaluate the importance of contextual information, the second approach, BART-augmented, extends the approach of Gurcke et al. (2021) by prepending the essay topic and the predicted ADU type of the enthymeme to the input.5\nExperimental Setup Using Huggingface\u2019s (Wolf et al., 2020) default sequence-to-sequence training parameters and learning rate 2 \u00b7 10\u22125, we fine-tune the BART models for five epochs. For this task we did not perform extensive hyperparameter optimization since the models\u2019 performance on the validation set was already comparable to related work (Gurcke et al., 2021) and sample experiments\n5ADU types are predicted with the classifiers that we trained based on the work by Chen et al. (2022)."
        },
        {
            "heading": "Approach BERTSc. ROU.-1 ROU.-2 ROU.-L",
            "text": "with other hyperparameter values did not lead to further improvements.\nResults Table 7 reports the automatic evaluation results for enthymeme reconstruction. We only include test instances for which both models generated text. In line with Gurcke et al. (2021), we report the recall-oriented ROUGE scores (Lin, 2004), since we see it is worse to omit parts of the target text than to generate text that goes beyond the ground-truth text for this task. The results suggest that the additional information on the topic and/or target ADU type can help with enthymeme\nreconstruction, although the gain is small on average. Moreover, a manual investigation of 30 arguments, as detailed in Appendix D, suggested that language models can learn to automatically reconstruct enthymemes in learner arguments using our corpus. We expect more advanced approaches can and should further increase effectiveness.\nTo provide further insights into how our models work on full arguments we applied our enthymeme detection model to each potential position in an example learner argument from the ICLEv3 (Granger et al., 2020). On positions that were classified as enthymematic, we applied our enthymeme reconstruction model to generate the missing component. The results are displayed in Table 8. Our models identified two enthymeme positions and generated corresponding texts, that make the connection between credit card usage and depts and the implicit claim of the argument towards the topic explicit."
        },
        {
            "heading": "7 Conclusion",
            "text": "Until now, data for studying enthymemes in learner arguments is neither available nor is it straightforward to write respective arguments in a natural way. Therefore, this paper has studied how to automatically create corpora of enthymematic learner arguments using a self-supervised process that carefully removes ADUs from argumentative texts of highquality while maintaining the texts\u2019 naturalness. To be able to point to possibly problematic gaps in learner arguments and make suggestions on how to fill these gaps, we present the tasks of enthymeme detection and reconstruction for learner arguments that can be studied using the created corpus.\nOur analyses provided evidence that our corpus creation approach leads to the desired reduction in logical argument quality while resulting in arguments that are similarly natural to those written by learners. Hence, we conclude that, mostly, inadequate enthymemes are created in the sense of missing ADUs that would complete the logic of a written argument. Moreover, first baselines demonstrate that large language models can learn to detect and fill these enthymematic gaps when trained on our corpus. Such models may help learners improve the logical quality of their written arguments."
        },
        {
            "heading": "8 Limitations",
            "text": "Aside from the still-improvable performance of the presented baseline models and the limitations of the models that are used as part of our approach,\nwe see three notable limitations of the research presented in this paper. We discuss each of them in the following.\nFirst, due to the lack of learner argument corpora with annotated inadequate enthymemes, we cannot compare the automatically-created enthymemes to human-created ones. Creating such enthymemes intentionally in arguments is not a straightforward task for humans, which is why we aimed to do this automatically in the first place. However, it should be noted that we cannot guarantee that our created enthymemes consistently align with the traditional understanding of enthymemes as studied in argumentation theory.\nSecond, despite applying quality filters to the original learner essays, there is a possibility that the arguments contain unknown inadequate enthymemes. This can harm the effectiveness of the corpus annotations for both presented tasks.\nLastly, we evaluate our corpus creation approach only on a single corpus of argumentative learner essays. This particular corpus may not represent the whole diversity of argumentative texts or learner proficiency levels. Consequently, the generalizability of our findings and the applicability of the proposed approaches to a wider array of argumentative writing remains to be explored."
        },
        {
            "heading": "9 Ethical Considerations",
            "text": "We do not see any apparent risks of the approach we developed being misused for ethically doubtful purposes. However, it is important to acknowledge that automatically-created artificial data could be mistaken for real or authentic data, leading to false understandings, biased conclusions, or misguided decision-making. This is not our intended use case for the created corpus, which is meant for method development. If texts from the corpus are directly displayed to learners, we suggest to mark that they have been created automatically."
        },
        {
            "heading": "Acknowledgments",
            "text": "This project has been partially funded by the German Research Foundation (DFG) within the project ArgSchool, project number 453073654. We would like to thank the participants of our study and the anonymous reviewers for the feedback and their time."
        },
        {
            "heading": "A Essay Scoring Models",
            "text": "This section gives additional details on the features and the model performance using different combinations of features used as input for the essay scoring models predicting organization, thesis clarity, and argument strength. We also report the feature combinations that led to the lowest mean squared error (mse) for each quality dimension."
        },
        {
            "heading": "A.1 Details on Features",
            "text": "\u2022 ADU n-grams: ADU n-grams, with n \u2208 {1, 2, 3}; frequencies of ADU type combinations and sequences of ADU types within paragraphs.\n\u2022 Sentiment flow: Sequence of the paragraph\u2019s sentiments, using Barbieri et al. (2020)\u2019s sentiment classifier.\n\u2022 Discourse function flow: Sequences of discourse functions: introduction, body, and conclusion (Persing et al., 2010).\n\u2022 Prompt similarity flow: Max., min. and mean cosine similarity of the paragraph embeddings to the prompt; Similarity flow, i.e., sequence of the paragraph\u2019s similarities to prompt using sBERT embeddings (Reimers and Gurevych, 2019).\n\u2022 Token and POS n-grams: Token n-grams (n \u2208 {1, 2, 3}) that occur in more tham 1% of the training data; POS m-grams (m \u2208 {1, .., 5}) that occur in more than 5% of the training data.\n\u2022 Length statistics: Number of paragraphs, sentences, tokens; maximum, minimum and average number of sentences per paragraph.\n\u2022 Frequency of linguistic errors: Number of linguistic errors given by LanguageTool.\n\u2022 Distribution of named entity types: Occurrences per named entity type using spaCy (Honnibal et al., 2020).\n\u2022 Distribution of metadiscourse type markers: Occurrences per metadiscourse marker category, as proposed by Hyland (2018)."
        },
        {
            "heading": "A.2 Comparison of Feature Combinations",
            "text": "Table 9 compares the model performances for different combinations of input features: all features, all feature combinations in which one feature is left out and the feature combination that performed best in terms of mse for each of the three quality dimensions.\nThe combinations of features that led to the best model performance it terms of mse, as reported in Table 9, are:\n\u2022 Organization. Linguistic errors, token and pos n-grams, length statistics, named entity types, prompt similarity flow.\n\u2022 Thesis Clarity. Linguistic errors, sentiment flow, discourse function flow, named entity types, prompt similarity flow.\n\u2022 Argument Strength. ADU n-grams, token and pos n-grams, prompt similarity flow."
        },
        {
            "heading": "B Annotation Guidelines: Sufficiency",
            "text": ""
        },
        {
            "heading": "B.1 Task Description",
            "text": "In this task, you will be presented with arguments from argumentative learner essays (original arguments) and different variations of them (modified arguments). We want to evaluate the argument quality of the variations in relation to the original argument. The task consists of reading 200 arguments and up to four argument variations each. You are asked to assess for each modified argument whether the quality of the modified argument is higher, equal, or lower than the quality of the original argument. We measure argument quality in terms of sufficiency as defined below. The estimated time for this task is 3-5 minutes to evaluate all variations of one original argument, approximately 15 hours of work."
        },
        {
            "heading": "B.2 Data",
            "text": "You are asked to assess the sufficiency of the modified arguments in relation to the corresponding original argument. The modified arguments are variations of the original argument in which one of the original sentences is removed. We replaced the removed sentence with \"<mask>\" to indicate the deletion position. When a row has no original argument, the previous original argument is the corresponding original argument. The same holds for the topic."
        },
        {
            "heading": "B.3 Sufficiency Definition",
            "text": "An argument is sufficient when (1) the premises together give enough reason to accept the conclusion, (2) and it is clear what the argument\u2019s conclusion is."
        },
        {
            "heading": "B.4 Annotation Scale",
            "text": "\u2022 Sufficiency increased: +1\n\u2022 Sufficiency unchanged: 0\n\u2022 Sufficiency decreased: +1"
        },
        {
            "heading": "C Annotation Guidelines: Naturalness",
            "text": ""
        },
        {
            "heading": "C.1 Task Description",
            "text": "For this task, you will be asked to evaluate what we defined as \u201cnaturalness\u201d of a paragraph. Therefore you will rate the provided paragraphs on an ordinal scale from 1-3. A rating of 3 points represents the highest grade and 1 the lowest. To come to a conclusion, we would like you to pay attention to the three main criteria of coherence, cohesion,\nand clarity. The paragraph should be awarded 3 points if the topic is not abruptly changed, cohesive devices can be resolved and the paragraph does not lack the information we do not expect elsewhere. We award 2 points if we see stronger flaws in cohesion or the use of cohesive devices or clarity, which are not out of the ordinary for student essays. If we observe a strong discontinuity between parts of the paragraph, we award 1 point. We do not expect the essay written by English learners to be perfectly coherent, cohesive and clear. However, if you notice strong incoherences or cohesive ties that cannot be resolved, the paragraph would be seen as artificial. Intuitively, read the paragraph sentence per sentence, and examine the continuity/transition between the sequence of sentences. If you consider a sentence not dependent on the preceding text, the text can be seen as unnatural and artificial. The evaluation scheme and scoring criteria are presented below. Concrete representations of natural or artificial are difficult to precisely define, the criteria should help the annotators to judge the \u201dnaturalness\u201d based on their feeling for the English language and knowledge about the level of student\u2019s writing."
        },
        {
            "heading": "C.2 Annotation Scale",
            "text": "1. Artificial: A part of the paragraph (e.g. a sen-\ntence) seems unrelated to the preceding or succeeding text. For example, a reference cannot be resolved. (cohesion). Or the text abruptly changes the overall thematic scheme or does not have an underlying theme. (coherence). Or a substantial amount of information that\nwe would have expected based on the given text cannot be found (clarity)\n2. Partially Natural: The text is largely coherent (no strong incoherences), the use of cohesive devices does not damage the continuity/comprehensibility of the text, and the provided information is enough to get his point across.\n3. Natural: The text is at least not incoherent, cohesive devices are correctly used, and the provided information allows one to understand his argument clearly."
        },
        {
            "heading": "D Manual Investigation: Enthymeme Reconstruction",
            "text": "As a manual investigation of the generated reconstructed enthymemes, we asked three annotators to evaluate 30 arguments along the three dimensions clarity, argument strength and coherence on 5-point scales as defined below, with 5 being the best score. The annotators were given arguments with a [MASK] token at the enthymeme position as context and three different ADUs for the masked position: (i) the original ADU, (ii) the ADU generated by Gurcke et al. (2021)\u2019s approach, and (iii) the ADU generated by the approach BARTaugmented."
        },
        {
            "heading": "D.1 Definitions and Scales",
            "text": "Clarity The argument has a high clarity if \u201cit uses correct and widely unambiguous language as well as if it avoids unnecessary complexity and deviation from the issue\u201d (Wachsmuth et al., 2017).\n1. The addition of the ADU makes the argument less clear. The ADU itself uses incorrect language and does not align with the topic of the argument.\n2. The addition of the ADU makes the argument less clear. The ADU itself is not fully understandable and only fits partly to the topic of the argument.\n3. The addition of the ADU does not improve the clarity of the argument. The ADU itself is in terms of language in line with the rest of the argument.\n4. The addition of the ADU slightly improves the clarity of the argument. The ADU itself uses mostly correct, unambiguous language and fits the topic.\n5. The addition of the ADU improves the clarity of the argument. The ADU itself uses correct, unambiguous language and fits the topic.\nArgument Strength Following (Persing and Ng, 2015), we define the strength of an argument by its effectiveness in convincing a majority of a (reasonable) audience.\n1. The addition of the ADU does not fit the argument, it contradicts or makes the main standpoint unclear.\n2. The addition of the ADU weakens the effectiveness of the argument.\n3. The addition of the ADU leads to neither an improvement in strength nor does it make the argument less convincing.\n4. The addition of the ADU has a positive effect on the effectiveness of the argument.\n5. The addition of the ADU strengthens the argument, it becomes more convincing.\nCoherence In line with (Smalley et al., 2001), we define that a coherent paragraph or argument should contain sentences that are arranged logically and have a smooth flow.\n1. The addition of the ADU results in an illogical argument, which is hard to follow. The topic of the ADU does not align with the rest of the argument.\n2. The addition of the ADU results in an argument that is less logical than before and the argument is more difficult to follow. The topic of the ADU does not fully fit into the rest of the argument.\n3. The addition of the ADU is in line with the flow and logical arrangement of the argument. The ADU fits the underlying topic.\n4. The addition of the ADU improves the coherence of the argument a little. The ADU adds to the logical arrangement and makes it easier to follow.\n5. The addition of the ADU improves the coherence of the argument a lot. The ADU adds to the logical arrangement and makes it much easier to follow."
        },
        {
            "heading": "D.2 Results",
            "text": "Table 10 shows the results of the manual enthymeme reconstruction evaluation. As expected, the original ADUs lead to the best average score in all three dimensions, followed by the extended approach BERT-augmented. The generated ADUs reach medium quality in terms of clarity, argument strength and coherence in most cases. Overall, the quality of the generated ADUs seems to be very comparable with the original ADUs written by learners, which suggests that language models can learn to automatically reconstruct enthymemes in learner arguments using our corpus."
        }
    ],
    "title": "Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments",
    "year": 2023
}