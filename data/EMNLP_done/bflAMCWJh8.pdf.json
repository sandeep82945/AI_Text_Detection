{
    "abstractText": "With the rapid development of neural network applications in NLP, model robustness problem is gaining more attention. Different from computer vision, the discrete nature of texts makes it more challenging to explore robustness in NLP. Therefore, in this paper, we aim to connect discrete perturbations with continuous perturbations, therefore we can use such connections as a bridge to help understand discrete perturbations in NLP models. Specifically, we first explore how to connect and measure the correlation between discrete perturbations and continuous perturbations. Then we design a regression task as a PerturbScore to learn the correlation automatically. Through experimental results, we find that we can build a connection between discrete and continuous perturbations and use the proposed PerturbScore to learn such correlation, surpassing previous methods used in discrete perturbation measuring. Further, the proposed PerturbScore can be well generalized to different datasets, perturbation methods, indicating that we can use it as a powerful tool to study model robustness in NLP. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Linyang Li"
        },
        {
            "affiliations": [],
            "name": "Ke Ren"
        },
        {
            "affiliations": [],
            "name": "Yunfan Shao"
        },
        {
            "affiliations": [],
            "name": "Pengyu Wang"
        },
        {
            "affiliations": [],
            "name": "Xipeng Qiu"
        }
    ],
    "id": "SP:91e23b41ffaf65fefe2f882b5539deb265da372a",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473.",
            "year": 2014
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David A. Wagner."
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "CoRR, abs/1608.04644.",
            "year": 2016
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou."
            ],
            "title": "Hotflip: White-box adversarial examples for text classification",
            "venue": "arXiv preprint arXiv:1712.06751.",
            "year": 2017
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy."
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572.",
            "year": 2014
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich."
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "arXiv preprint arXiv:1903.12261.",
            "year": 2019
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Adversarial examples for evaluating reading comprehension systems",
            "venue": "arXiv preprint arXiv:1707.07328.",
            "year": 2017
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is BERT really robust? natural language attack on text classification and entailment",
            "venue": "CoRR, abs/1907.11932.",
            "year": 2019
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "Bert-attack: Adversarial attack against bert using bert",
            "venue": "arXiv preprint arXiv:2004.09984.",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Jianhan Xu",
                "Jiehang Zeng",
                "Linyang Li",
                "Xiaoqing Zheng",
                "Qi Zhang",
                "Kai-Wei Chang",
                "Cho-Jui Hsieh."
            ],
            "title": "Searching for an effective defender: Benchmarking defense against adversarial word substitution",
            "venue": "arXiv preprint arXiv:2108.12777.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "year": 2019
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Diarmuid O S\u00e9aghdha",
                "Blaise Thomson",
                "Milica Ga\u0161i\u0107",
                "Lina Rojas-Barahona",
                "Pei-Hao Su",
                "David Vandyke",
                "Tsung-Hsien Wen",
                "Steve Young."
            ],
            "title": "Counter-fitting word vectors to linguistic constraints",
            "venue": "arXiv preprint arXiv:1603.00892.",
            "year": 2016
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Rafael Pinot",
                "Laurent Meunier",
                "Alexandre Araujo",
                "Hisashi Kashima",
                "Florian Yger",
                "C\u00e9dric GouyPailler",
                "Jamal Atif."
            ],
            "title": "Theoretical evidence for adversarial robustness through randomization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang"
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Yao-Yuan Yang",
                "Cyrus Rashtchian",
                "Hongyang Zhang",
                "Russ R Salakhutdinov",
                "Kamalika Chaudhuri."
            ],
            "title": "A closer look at accuracy vs",
            "venue": "robustness. Advances in neural information processing systems, 33:8588\u20138601.",
            "year": 2020
        },
        {
            "authors": [
                "Mingyang Yi",
                "Lu Hou",
                "Jiacheng Sun",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Zhiming Ma."
            ],
            "title": "Improved ood generalization via adversarial training and pretraing",
            "venue": "International Conference on Machine Learning, pages 11987\u201311997. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Zang",
                "Fanchao Qi",
                "Chenghao Yang",
                "Zhiyuan Liu",
                "Meng Zhang",
                "Qun Liu",
                "Maosong Sun."
            ],
            "title": "Word-level textual adversarial attacking as combinatorial optimization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric Xing",
                "Laurent El Ghaoui",
                "Michael Jordan."
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "International conference on machine learning, pages 7472\u20137482. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "ArXiv, abs/1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Chen Zhu",
                "Yu Cheng",
                "Zhe Gan",
                "Siqi Sun",
                "Thomas Goldstein",
                "Jingjing Liu."
            ],
            "title": "Freelb: Enhanced adversarial training for language understanding",
            "venue": "arXiv preprint arXiv:1909.11764.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Natural language processing (NLP) applications based on neural networks are developing rapidly, exemplified by applications based on pre-trained models (Devlin et al., 2018) such as ChatGPT 2 (Brown et al., 2020), machine translation systems (Bahdanau et al., 2014), question-answering systems (Rajpurkar et al., 2016). While they are growing at an incredible speed, it is of great concern how we can trust these neural networks. Therefore, exploring model robustness in NLP is essential for future NLP developments. Model\n\u2217Equal Contribution. \u2020Corresponding author.\n1We will release our code and generated datasets at https://github.com/renke999/PerturbScore\n2https://openai.com/blog/chatgpt/\nrobustness problems mostly focus on exploring the model behavior when the inputs are perturbed. However, unlike the computer vision field, the discrete nature of natural language makes it more challenging to define, construct and measure the perturbations added to the texts.\nPrevious works usually separate two lines of work concerning discrete perturbations and continuous perturbations: In the computer vision (CV) field, continuous perturbations are widely explored (Goodfellow et al., 2014) since studying perturbations can help improve model robustness (Madry et al., 2019) and generalization abilities (Hendrycks and Dietterich, 2019; Hendrycks et al., 2021). As for perturbations in NLP, the difference and challenge in discrete perturbations constrain the development of model robustness in NLP. Jin et al. (2019); Zang et al. (2020); Li et al. (2020) craft adversarial examples with synonyms as word-level perturbations, which is hard to measure whether the perturbations are imperceptible. Also, crafting these perturbations would face a combinatorial explosion problem. As studying discrete perturbations is more challenging than continuous perturbations, it is intuitive to wonder: can we find the correlations between discrete and continuous perturbations in NLP and study continuous perturbations instead?\nIn this paper, we aim to explore connections between discrete and continuous perturbations in NLP models hoping that such connections can help studies of model robustness in NLP. We first give definitions and measuring standards of perturbations in discrete and continuous space and align the form and notations for studying their correlations. Then we make several assumptions to provide the possibility of connecting discrete and continuous perturbations. Specifically, we assume that discrete perturbations and continuous perturbations have similar effects on neural models when being added to the input to perturb the\nmodel. Therefore, we are able to search for a continuous perturbation that can be considered as a substitution for the discrete perturbation. We then introduce a method to quantify the correlations between discrete and continuous perturbations and design a regression task to automatically learn the correlation. Specifically, we use the gradientprojection descent method to search for a minimum continuous perturbation that has similar effects on the model behavior with the discrete perturbation. After quantifying the correlations between discrete and continuous perturbations, we use an additional neural network named PerturbScorer to learn such a correlation. That is, given the original input and a discrete perturbation, we use a neural network to predict its continuous perturbation range.\nWe construct experiments on IMDB and AG\u2019s News datasets, which are widely used datasets in NLP robustness studies. We first explore the correlations between discrete and continuous perturbations when they are used to perturb fine-tuned models such as BERT; then we test the performances of the learned network and empirically verify that the correlations between perturbations can be learned through a neural network, providing evidence for researchers to study continuous perturbations in NLP as a substitute for discrete perturbations.\nFurther, we design extensive analytical experiments and through the experimental results, we make several non-trivial conclusions: (1) we can build a connection between discrete and continuous perturbations; (2) such a connection can be generalized and help improve model robustness and generalization abilities; (3) continuous-space adversarial training is effective because it narrows the gap between discrete and continuous space.\nTo summarize, in this paper, we explore the correlations between discrete and continuous perturbation, a fundamental challenge in robustness studies in NLP. We provide detailed notations and make assumptions to explore the correlations between perturbations and propose a method to connect these perturbations; further, we design a PerturbScorer to learn such correlation; through experimental results, we show that we can connect discrete perturbations with continuous perturbations. We are hoping that the concept of studying discrete perturbations in NLP through building the connections between continuous perturbations can provide hints for future studies."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Model Robustness and Perturbations",
            "text": "Robustness problems are widely explored in the deep learning field: Goodfellow et al. (2014); Carlini and Wagner (2016) discussed the possibility of crafting gradient-based perturbations as adversarial examples to mislead neural models. Madry et al. (2019) introduces the projected gradient descent method to construct perturbations. Hendrycks and Dietterich (2019); Hendrycks et al. (2021) discussed more general perturbations such as Gaussian noise, blurs, etc. in the distribution shift scenarios. When the perturbations are continuous, studies focus on exploring connections between model robustness and model accuracy (Zhang et al., 2019a; Yang et al., 2020) and plenty of analytical works dive deep into the model robustness studies (Pinot et al., 2019). These robustness studies assume that the perturbations are continuous, therefore, they are not suitable for discrete perturbations and NLP robustness studies."
        },
        {
            "heading": "2.2 Perturbations in NLP",
            "text": "In the NLP field, the robustness problem becomes more challenging due to the discrete nature of texts. Ebrahimi et al. (2017) explores crafting character-level and word-level perturbations as adversarial examples to attack NLP models. Follow-up works such as Jin et al. (2019); Zang et al. (2020); Li et al. (2020) aim to find better methods to craft semantic-preserving adversaries. As for more general perturbations, Jia and Liang (2017) explores how adding random sentences can mislead question-answering systems; Yi et al. (2021) explores how adversarial training improves out-of-distribution model generalization problems. Unlike the continuous perturbations explored in the computer vision field, the NLP field rarely discusses how the model behaves in robustness against adversaries and generalization abilities. Zhu et al. (2019) introduces embedding-space gradient-based adversarial training and discovers that continuous space adversaries can help improve NLP model generalization abilities without further explanation. Li et al. (2021) founds that gradient-based adversarial training can be used in defense against word-substitution attacks. In general, robustness studies in NLP rarely focus on finding the correlation between discrete and continuous perturbations which separate works in vision and language fields."
        },
        {
            "heading": "3 Connecting Perturbations",
            "text": ""
        },
        {
            "heading": "3.1 Defining Perturbations",
            "text": "We first define perturbations in deep neural networks for NLP applications:\nGiven an input text S = [w0, w1, \u00b7 \u00b7 \u00b7 , wn, \u00b7 \u00b7 \u00b7 ], the corresponding embedding of S is X = [x\u20d70, x\u20d71, \u00b7 \u00b7 \u00b7 , x\u20d7n, \u00b7 \u00b7 \u00b7 ]. The prediction of the input S is denoted as f(X). Here, we use the embedding output X as the model input since we aim to connect the discrete perturbations with continuous perturbations in the embedding space. When the input text is maliciously attacked or perturbed by random noise, the input text becomes S \u2032 . We useP(S) to denote the perturbation process therefore the perturbed text S \u2032\n= S + P(S). The perturbation function P(S) can be various methods including adversarial attacks and random perturbations. Representative adversarial attack methods are word-substitution adversarial attacks such as HotFlip (Ebrahimi et al., 2017), Textfooler (Jin et al., 2019) and BERT-Attack (Li et al., 2020). Unlike random perturbations such as random deleting or replacing words/characters, adversarial attack methods aim to find the minimum amount of character- or word-level substitutions that can mislead target models.\nUnlike in the computer vision field where continuous perturbations can be directly added to the input, the continuous perturbations can only be added to the embedding output X in the language field. For embedding output X \u2208 Rl\u2217d with sequence length l and hidden size d, we have perturbed output X \u2032 = X + \u03b4. The continuous perturbation \u03b4 \u2208 Rl\u2217d can be random noise (Hendrycks and Dietterich, 2019) such as Gaussian noise, blurs, pixelate, or adversarial perturbations. A representative method to generate adversarial perturbation \u03b4 is the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014). Given a target model f\u03b8(\u00b7), the perturbation of sample S is generated based on the gradients: \u03b4 = \u03b1 \u00b7 sgn(\u2207Xf\u03b8(X, y)). Here, \u03b1 is a hyper-parameter controlling the perturbation range."
        },
        {
            "heading": "3.2 Measuring Perturbations",
            "text": "After defining perturbations, it is important to measure how perturbations affect neural models.\nMeasuring the severity of the perturbation is a challenge in discrete text perturbations. The similarity between the perturbed and original texts\ncannot be easily measured. We use A(P(S)) to measure the perturbation intensity of perturbation P(S) added to the original text S, which could be edit-distance, semantic shift, grammar change, etc. For instance, when we use edit-distance as A(P(S)), we assume that the fewer tokens the original text is replaced, the less the text is perturbed. Besides edit-distance, Jin et al. (2019) introduces USE (Cer et al., 2018) to measure the perturbation intensity. We assume that the less semantic information is changed, the less the text is perturbed. In general, finding an accurate measure strategy A(\u00b7) is challenging since the standard can be diversified and subjective when measuring discrete perturbations.\nOn the other hand, continuous perturbations can be measured by constraining the \u2113p-norm ||\u03b4||p of the perturbations \u03b4. The most common constraint is the \u21132-norm. Compared with measuring discrete perturbation, it is easy and straightforward to measure the continuous perturbation range."
        },
        {
            "heading": "3.3 Connecting Perturbations",
            "text": "As illustrated, it is challenging to measure discrete perturbations, which makes it more difficult to explore how perturbations affect the model behavior. Meanwhile, another challenge is that constructing discrete perturbations is also challenging, for instance, replacing discrete tokens in a multi-token text with multiple candidates for each token is a combinatorial explosion problem. Therefore, since measuring and studying continuous perturbations is more convenient, instead of searching for methods to evaluate the shift caused by discrete perturbations, we aim to build a connection between discrete perturbations and continuous perturbations and explore how the continuous perturbation affects model behaviors instead. We hope that by connecting discrete perturbations to continuous perturbations, we can introduce new perspectives to NLP field model robustness problems.\nTo build the connection between discrete perturbations and continuous perturbations, we make several assumptions:\nAssumption 1. Continuous perturbations \u03b4 added to X have similar effects on target model f\u03b8 compared to discrete perturbations added to S. That is, both types of perturbations can cause a model prediction shift, and stronger perturbations would cause more damage to the model.\nAssumption 2. Target model f\u03b8 follows Lipschitz constraint: when ||\u03b4||2 < \u03f5, ||f\u03b8(X + \u03b4) \u2212 f\u03b8(X)||2 < K \u00b7 \u03f5. Here, we assume that in NLP models, such as a fine-tuned BERT, small perturbations in the embedding space do not cause severe damage to model outputs. Otherwise, the behavior change caused by input perturbations is hard to predict, and finding correlations between perturbations is more challenging.\nAssumption 3. For a discrete perturbation P(S), there exist a continuous perturbation \u03b4 that satisfies: \u03f5\u2212 \u03b5 < ||\u03b4||2 < \u03f5, here, \u03b5 is a small interval. And such \u03b4 satisfies:\n\u2223\u2223\u2223\u2223 ||f\u03b8(S + P(S)) \u2212 f\u03b8(S)||2||f\u03b8(S + P(S)||2 \u00b7 ||f\u03b8(S)||2 \u2212 ||f\u03b8(X + \u03b4) \u2212 f\u03b8(X)||2||f\u03b8(X + \u03b4)||2 \u00b7 ||f\u03b8(X)||2 \u2223\u2223\u2223\u2223 < \u03d5\n, here, \u03d5 is a hyper-parameter, and for simplification, f\u03b8(\u00b7) takes both discrete tokens S and embedding output X of the discrete tokens S as input, skipping the embedding process.\nWe assume that the continuous perturbation has a similar effect on model f\u03b8(\u00b7), therefore, when the absolute value of the gap between model shift caused by discrete and continuous perturbations is small, we consider they are equal in perturbing neural models. Therefore, we can use continuous perturbations as an approximation of discrete perturbations by building connections between them."
        },
        {
            "heading": "3.4 Quantify Connections",
            "text": "After assuming that we can build connections between discrete and continuous perturbations, we aim to quantify such connections. For a discrete perturbation P(S), we find the minimum continuous perturbation \u03b4 under Assumption 3. Specifically, we aim to find the proper norm-bound\n\u03f5 that under such a norm-bound, there exists a perturbation \u03b4 satisfies Assumption 3 mentioned above. Therefore, when S and P(S) is fixed, the goal is to find a norm-bound \u03f5:\nargmin \u03f5 max ||\u03b4||2<\u03f5 ( ||f\u03b8(X + \u03b4)\u2212 f\u03b8(X)||2 ||f\u03b8(X + \u03b4)||2 \u00b7 ||f\u03b8(X)||2 ) (1)\nTherefore, we would obtain a data tuple [S,P(S), \u03f5], which is the correlation of discrete and continuous perturbations. We are hoping that we can empirically verify that the data tuple can be connected, and verify the assumptions made above.\nAlgorithm 1 Obtaining norm-bound \u03f5 Require: Inputs X,S,P(S), label y, search step\nTa, norm range interval \u03b5 1: \u0393\u2190 ||f\u03b8(S+P(S))\u2212f\u03b8(S)||2||f\u03b8(S+P(S)||2\u00b7||f\u03b8(S)||2 2: for \u03f5 = 0, \u03b5, 2\u03b5, ... do 3: \u03b40 \u2190 0 4: for t = 0, 1, ...Ta do 5: g\u03b4 \u2190\u25bd\u03b4L(f\u03b8(X + \u03b4t), y) 6: // Get Gradients 7: \u03b4t \u2190 \u220f ||\u03b4||2<\u03f5 (\u03b4t + \u03b1 \u00b7 g\u03b4 ||g\u03b4||2 ) 8: // Get Perturbation 9: if \u2223\u2223 ||f\u03b8(X+\u03b4t)\u2212f\u03b8(X)||2 ||f\u03b8(X+\u03b4t)||2\u00b7||f\u03b8(X)||2\u2212\u0393\n\u2223\u2223 < \u03d5 then 10: return tuple [S,P(S), \u03f5] 11: else 12: discard tuple\nIn practice, to obtain \u03f5, we use a standard projected-gradient-descent (PGD) (Madry et al., 2019) method. As seen in Algorithm 1, we use multi-step gradient-descent to generate perturbations within the range \u03f5, which is the perturbation generation used in gradient-based\nadversarial training. Specifically, the notation x used in line 7 is to constrain the perturbations within the norm bound \u03f5. In line 9, we pick the \u03f5 that satisfies the assumption that there exists a continuous perturbation that has a similar effect on neural models compared with discrete perturbations. Therefore, once the perturbation \u03b4 obtains ideal effects on the neural model (bigger than the effects caused by discrete perturbations), we consider the \u03f5 found is the proper one. A special case is that if the continuous perturbation effect ||f\u03b8(X+\u03b4)\u2212f\u03b8(X)||2||f\u03b8(X+\u03b4)||2\u00b7||f\u03b8(X)||2 is way bigger than \u0393, we simply drop the sample."
        },
        {
            "heading": "3.5 PerturbScorer",
            "text": "After constructing the quantification of correlations between discrete and continuous perturbations, we design a PerturbScorer to score the correlations.\nThe perturbation \u03b4 is a continuous variant, therefore, we formulate a regression task to learn the range of \u03f5 given S and P(S). We train the task as the PerturbScorerM([S,P(S)], \u03f5) to measure the correlation between discrete and continuous perturbations in the target model f\u03b8(\u00b7).\nConsidering that the perturbation P(S) should be a small perturbation, we use a simple strategy that concatenates original texts and perturbations in P(S) as the input of the regression task to learn the correlation. We simply concat the perturbations behind the original token, (e.g.: . . . , it would recall [reminds] . . . ). Such patterns help the model understand the perturbation of the original texts. Then we use the crafted inputs to predict the norm bounds of the correlated continuous perturbations."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset Construction",
            "text": "To explore the correlations between discrete and continuous perturbations, we use several datasets widely used in exploring model robustness in NLP. We use the IMDB dataset (Maas et al., 2011) and the AG\u2019s News dataset (Zhang et al., 2015) which are text classification tasks with an average text length of 220 and 47 accordingly.\nWe use two widely used perturbation methods P(S), Textfooler (Jin et al., 2019) and randomperturb. In the Textfooler method, we follow the standard generation process and save perturbations in multiple queries regardless of the attack result, which is different from its original usage that keeps finding perturbations until the attack is successful.\n||f\u03b8(S+P(S)||2\u00b7||f\u03b8(S)||2 and edit-distance, containing\ncurves of neural model f\u03b8 including FreeLB-trained model and BERT fine-tuned model and perturbation method P(S) including Textfooler and Random perturbation tested on the IMDB dataset; (c) is the curve of ||f\u03b8(X+\u03b4t)\u2212f\u03b8(X)||2||f\u03b8(X+\u03b4t)||2\u00b7||f\u03b8(X)||2 and norm-ball range, containing curves of FreeLB-trained model and BERTfine-tuned model tested on the IMDB dataset; (b) and (d) are the corresponding results of the AG\u2019s News dataset.\nFor each input text S, we have multiple P(S) with different edit-distance differences.\nIn the random perturb method, we randomly replace a token using a random word from a general vocabulary, which is the vocabulary used to obtain synonyms in the Textfooler method (Mrk\u0161ic\u0301 et al., 2016; Jin et al., 2019). Similar to the Textfooler perturbation method, we also collect multiple perturbations per text including different numbers, places of substitutes, and different substitutes.\nIn generating continuous perturbations, we use the PGD method to find the minimum continuous perturbation that has a similar model prediction shift compared to the discrete perturbation. Specifically, we set the adversarial step Ta = 15 and the adversarial learning rate \u03b1 = 1e \u2212 1, the norm-ball search interval \u03b5 of \u03f5 is set to 0.01 and the discard parameter \u03d5 is set to 0.005.\nIn Figure 1, we draw curves exploring the connection between model output shifts and different levels of perturbations. As shown, we can observe that the average model output shifts caused by discrete and continuous perturbations show consistency with edit distance and norm-ball range. We observe that when the perturbations grow\nlarger, both discrete and continuous perturbations will cause more damage to neural models. Plus, the models trained by the FreeLB method show better resistance against both discrete and continuous perturbations. These results verify Assumption 1 and show that discrete and continuous perturbation can be correlated.\nIn Table 2, we count the tuple number of different \u03f5 ranges in the constructed data tuple of multiple datasets to show the connection between discrete and continuous perturbations. We observe that we only discard a small proportion of collected data tuples, proving that we can successfully find norm ball \u03f5 that satisfies Assumption 3 that such a continuous perturbation \u03b4 is equivalent to the discrete perturbations P(S) in interfering neural models f\u03b8(\u00b7). We also observe that as the discrete perturbation P(S) is uniformly distributed in the edit-distance range from 1 to 30 in the IMDb Dataset and 1 to 15 in the AG\u2019s News Dataset, the continuous perturbations mostly fall in the range that \u03f5 < 2e-1, indicating that most discrete perturbations with different edit-distances (indicating different numbers of substitutions) only compares to a minimum continuous perturbation. Therefore, learning the correlation between these discrete and continuous perturbations can help understand the discrete perturbations. Further, compared with the random perturbation, the Textfooler method generates more discrete perturbations that have more damage to model predictions and the corresponding continuous perturbations require larger norm balls, indicating that stronger discrete perturbations equal to larger continuous perturbations, providing the possibility to connect discrete perturbations with continuous perturbations.\nFor the collected data, we select 80% data tuples as the training set and 20 % as the test set in training and testing the PerturbScorer."
        },
        {
            "heading": "4.2 Evaluating Quantification of Correlation",
            "text": "After constructing the discrete perturbations and finding the corresponding \u03f5 of these discrete perturbations, we are able to explore whether the discrete and continuous perturbations can be connected and show similar effects on neural networks. To evaluate the quantification process of correlations illustrated in Sec. 3.4, we use Kendall and Pearson correlation coefficient index to measure whether the discrete perturbations and the continuous perturbations can be connected.\nThe goal is to measure the correlation coefficient index such as Kendall and Pearson index between A(P(S)) and the selected norm-bound \u03f5. If the correlation between A(P(S)) and \u03f5 is large, we can verify Assumption 3 that assumes there exists a continuous perturbation that equals to the discrete perturbation in interfering neural models. We use several simple A(\u00b7) including edit-distances, BERTScore (Zhang et al., 2019b) and USE (Cer et al., 2018). Here, BERTScore and USE measure the similarity between two sentences, which is reversed compared with edit-distance and perturbation scorer, therefore, we use the opposite number of the BERTScore and USE score as A(\u00b7) to measure the correlation coefficient index.\nFurther, we can directly measure the correlation coefficient index between the predicted and the found \u03f5, exploring whether the PerturbScorer can learn the connection between perturbations, which supports the assumptions we made above in Sec. 3.3 and provides a powerful tool to quantify the discrete perturbations for robustness studies in NLP."
        },
        {
            "heading": "4.3 PerturbScorer Training",
            "text": "The training process of the PerturbScorer follows the standard fine-tuning process used in fine-tuning regression tasks such as the STS-B (Cer et al., 2017) dataset using huggingface Transformers\n(Wolf et al., 2019). We set the learning rate to 5e-5 with batch size set to 64 and 128 for IMDB and AG\u2019s News datasets and use 4xNvidia 3090 GPUs to run the PerturbScorer training process."
        },
        {
            "heading": "4.4 Correlation Quantification Results",
            "text": "In Table 3, we list the correlation coefficient index of different measuring methods of perturbations and the PerturbScorer learned correlation of the perturbations:\nWe can observe that when we use scorersA(\u00b7) to measure the discrete perturbation, the correlation quantification results between the A(\u00b7) and the obtained continuous perturbation bound is not significant. In different setups including different datasets and perturbation methods, the Kendall correlation is smaller than 0.6 and the Spearman correlation is smaller than 0.8, indicating that the discrete perturbation measuring methods do not have close correlations with the continuous perturbations that have a similar effect to neural models, further proving that these measuring methods cannot properly measure the damage to neural models.\nOn the other hand, we can observe that when we use the PerturbScorer M(\u00b7) to predict the corresponding continuous perturbations, the correlation scores are significant enough to prove that the PerturbScorer can learn the connection between the discrete perturbations and the continuous perturbations. Such results show that we can use our proposed PerturbScorer as a powerful tool to build a connection between the discrete and continuous perturbations."
        },
        {
            "heading": "4.5 Analysis",
            "text": "As we first make assumptions about the correlations between discrete and continuous perturbations, we construct the data tuples and\ndesign a PerturbScorer to explore whether the correlation can be learned and generalized by neural networks. By exploring the correlations, we can obtain non-trivial observations that can be helpful in model robustness in NLP:"
        },
        {
            "heading": "4.5.1 Lipschitz Constraint Tightness",
            "text": "As we observe in Figure 1, the model shift is in direct proportion to the perturbation range, indicating that the target model f\u03b8 follows a Lipschitz constraint on a general scale. Further, as seen in Table 3, compared with the model trained by the FreeLB method, it is more challenging to study the correlation of the normal fine-tuned BERT as f\u03b8(\u00b7), indicating that gradient-based adversarial training helps build a tighter connection between discrete and continuous perturbations, providing a perspective to explain why gradientbased adversarial training helps in improving robustness performances and generalization performances in NLP tasks with discrete inputs (Zhu et al., 2019; Li et al., 2021)."
        },
        {
            "heading": "4.5.2 PerturbScorer Generalization",
            "text": "In Table 3, we show that the correlation between discrete perturbation P(S) and continuous perturbations range \u03f5 of model f\u03b8(\u00b7) can be learned by a PerturbScorer M(\u00b7), further, we aim to explore whether building such a correlation can be applied to various scenarios in robustness studies in NLP. That is, we explore the generalization ability of PerturbScorer M(\u00b7). We explore whether the learned PerturbScorer M(\u00b7) based on target model f\u03b8(\u00b7) can be generalized to cross-dataset, cross-perturbation method P(S), cross-model f\u03b8(\u00b7), therefore, the application of the PerturbScorer and the concept of learning the correlation between discrete and continuous perturbations can be used in various scenarios. We list thorough results in the Appendix.\nWe can explore how PerturbScorer M(\u00b7) performs on different datasets or faces different types of perturbations:\nCross-Perturbation PerturbScorer In crossperturbation tests, we observe that when we train the PerturbScorer with random perturbations as P(S), the PerturbScorer can learn perturbations generated by textfooler, while textfooler-generated perturbations cannot be well generalized. Such results show that we can collect multiple types of perturbations to train a PerturbScorer that can be generalized to recognize various discrete perturbations as a powerful tool to quantify how the discrete perturbations affect neural models.\nCross-Dataset PerturbScorer In cross-dataset tests, we observe that when we test the AG\u2019s News data tuples using the PerturbScorer trained with the IMDB dataset, the correlation is weakened but still stronger than correlations with edit distance, indicating that the PerturbScorer we train can be generalized to different datasets, showing that the connection between discrete and continuous perturbations is strong in general NLP systems, which provides possibilities of using such correlations in various NLP robustness scenarios.\nCross-Model PerturbScorer In general, the correlation between discrete and continuous perturbations is dependent on the neural model f\u03b8(\u00b7) since the perturbation range \u03f5 is calculated based on a certain model f\u03b8(\u00b7). However, when we test the generalization ability between different neural models f\u03b8(\u00b7), we observe that the correlation is still close. Therefore, it is possible to build a more general PerturbScorer as a general metric to score the discrete perturbations.\nCombined Scorer We further build a combined PerturbScorer that is trained by a mixture of data\ntuples including different datasets, perturbations methods and neural models to explore a more generalized scenario.\nAs seen in Table 4, when we train a model using mixed data collected, we can build a general PerturbScorer that can successfully predict the correlations between discrete perturbations and the corresponding continuous perturbation ranges. Such a result shows that it is possible to build a general PerturbScorer that can be used in solving different datasets and perturbation types, showing that we can use continuous perturbations as a proxy for discrete perturbations when studying NLP robustness problems."
        },
        {
            "heading": "5 Conclusion and Future Directions",
            "text": "In this paper, we focus on a fundamental problem in robustness studies in NLP, which is the discrete nature of texts. The discrete nature isolates NLP robustness studies from well-studied machine learning fields, therefore, we introduce the concept of building connections between discrete and continuous perturbations as a new perspective to explore NLP robustness. We build a PerturbScorer to learn the correlation between discrete and continuous perturbations and find that such a PerturbScorer can learn the connection between perturbations, allowing us to use continuous perturbation ranges as a proxy constraint of discrete perturbations, which avoids the challenge that discrete perturbations are hard to measure. Further, we find that our proposed PerturbScorer can be generalized to different datasets and perturbation methods, indicating that such a process can be further applied in the future in NLP robustness studies. For future directions, we aim to explore more effective methods to build a stronger PerturbScorer and to explore more broad scenarios to utilize the proposed PerturbScorer.\nLimitations\nIn this work, we explore the discrete perturbation in robustness studies in NLP. We aim to find correlations between discrete perturbations and continuous perturbations since continuous perturbations are easily measured and well-studies in the computer vision field. Our work makes assumptions that discrete perturbations show similar effect to neural networks compared with continuous perturbations, therefore, one limitation of such assumptions is that similar effect does strictly make two types of perturbations equal in nature. We find one perspective to connect the discrete perturbations and continuous perturbations, which is not the only solution. Future works can explore more strict constraints and find stronger connections between discrete and continuous perturbations.\nAlso, better PerturbScorer designing and the applications based on correlations between discrete and perturbations and PerturbScorers can be further explored. We focus on defining and building the connection between discrete and continuous perturbations, and we do not explore further applications based on these connections and our proposed PerturbScorer. For instance, as the PerturbScorer can be used in scoring the discrete perturbations, it can be used in recognizing differences between sentences or measuring distribution shifts. Also, previous works explore robustness and generalization trade-offs and explainable robustness theories on continuous space, mostly in the computer vision field, our works reveal the potential to explore these problems in NLP, which can be explored in future works.\nFurther, as large language models (LLMs) are drawing much attention in the NLP community, how strong LLMs behave in connecting discrete and continuous space perturbations remains unexplored, especially when GPT-4 (OpenAI, 2023) is known to support images and texts. As these models are not open-source to the public, we leave exploring the perturbation in LLMs in future works."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027). We would like to extend our gratitude to the anonymous reviewers for their\nvaluable comments. Additionally, we sincerely thank Qipeng Guo for his valuable discussions and insightful suggestions on this study."
        }
    ],
    "title": "PerturbScore: Connecting Discrete and Continuous Perturbations in NLP",
    "year": 2023
}