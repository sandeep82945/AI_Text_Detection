{
    "abstractText": "Most current Event Extraction (EE) methods focus on the high-resource scenario, which requires a large amount of annotated data and can hardly be applied to low-resource domains. To address EE more effectively with limited resources, we propose the Demonstrationenhanced Schema-guided Generation (DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we propose the demonstration-based learning paradigm for EE to fully use the annotated data, which transforms them into demonstrations to illustrate the extraction process and help the model learn effectively. Secondly, we formulate EE as a natural language generation task guided by schemabased prompts, thereby leveraging label semantics and promoting knowledge transfer in lowresource scenarios. We conduct extensive experiments under in-domain and domain adaptation low-resource settings on three datasets, and study the robustness of DemoSG. The results show that DemoSG significantly outperforms current methods in low-resource scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gang Zhao"
        },
        {
            "affiliations": [],
            "name": "Xiaocheng Gong"
        },
        {
            "affiliations": [],
            "name": "Xinjie Yang"
        },
        {
            "affiliations": [],
            "name": "Guanting Dong"
        },
        {
            "affiliations": [],
            "name": "Shudong Lu"
        }
    ],
    "id": "SP:a6388f87cf20cb484c4ef9a853bc82cf0ab3ed52",
    "references": [
        {
            "authors": [
                "David Ahn."
            ],
            "title": "The stages of event extraction",
            "venue": "Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1\u20138, Sydney, Australia. Association for Computational Linguistics.",
            "year": 2006
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Shiyao Cui",
                "Bowen Yu",
                "Tingwen Liu",
                "Zhenyu Zhang",
                "Xuebin Wang",
                "Jinqiao Shi."
            ],
            "title": "Edge-enhanced graph convolution networks for event detection with syntactic relation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "George Doddington",
                "Alexis Mitchell",
                "Mark Przybocki",
                "Lance Ramshaw",
                "Stephanie Strassel",
                "Ralph Weischedel."
            ],
            "title": "The automatic content extraction (ACE) program \u2013 tasks, data, and evaluation",
            "venue": "Proceedings of the Fourth International Conference",
            "year": 2004
        },
        {
            "authors": [
                "George Doddington",
                "Alexis Mitchell",
                "Mark Przybocki",
                "Lance Ramshaw",
                "Stephanie Strassel",
                "Ralph Weischedel."
            ],
            "title": "The automatic content extraction (ACE) program \u2013 tasks, data, and evaluation",
            "venue": "Proceedings of the Fourth International Conference",
            "year": 2004
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Event extraction by answering (almost) natural questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 671\u2013683, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Event extraction by answering (almost) natural questions",
            "venue": "arXiv preprint arXiv:2004.13625.",
            "year": 2020
        },
        {
            "authors": [
                "Rujun Han",
                "I-Hung Hsu",
                "Jiao Sun",
                "Julia Baylon",
                "Qiang Ning",
                "Dan Roth",
                "Nanyun Peng."
            ],
            "title": "ESTER: A machine reading comprehension dataset for reasoning about event semantic relations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
            "year": 2021
        },
        {
            "authors": [
                "I-Hung Hsu",
                "Kuan-Hao Huang",
                "Elizabeth Boschee",
                "Scott Miller",
                "Prem Natarajan",
                "Kai-Wei Chang",
                "Nanyun Peng."
            ],
            "title": "DEGREE: A data-efficient generation-based event extraction model",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Kung-Hsiang Huang",
                "Nanyun Peng."
            ],
            "title": "Document-level event extraction with efficient end-to-end learning of cross-event dependencies",
            "venue": "Proceedings of the Third Workshop on Narrative Understanding, pages 36\u201347, Virtual. Association",
            "year": 2021
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Kangmin Tan",
                "Mahak Agarwal",
                "Xinyu Feng",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "Good examples make a faster learner: Simple demonstration-based learning for low-resource NER",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Dingcheng Li",
                "Zheng Chen",
                "Eunah Cho",
                "Jie Hao",
                "Xiaohu Liu",
                "Fan Xing",
                "Chenlei Guo",
                "Yang Liu."
            ],
            "title": "Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Fayuan Li",
                "Weihua Peng",
                "Yuguang Chen",
                "Quan Wang",
                "Lu Pan",
                "Yajuan Lyu",
                "Yong Zhu."
            ],
            "title": "Event extraction as multi-turn question answering",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 829\u2013838.",
            "year": 2020
        },
        {
            "authors": [
                "Ying Lin",
                "Heng Ji",
                "Fei Huang",
                "Lingfei Wu."
            ],
            "title": "A joint neural model for information extraction with global features",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7999\u20138009, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Jie Lou",
                "Yaojie Lu",
                "Dai Dai",
                "Wei Jia",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Universal information extraction as unified semantic matching",
            "venue": "arXiv preprint arXiv:2301.03282.",
            "year": 2023
        },
        {
            "authors": [
                "Yaojie Lu",
                "Hongyu Lin",
                "Jin Xu",
                "Xianpei Han",
                "Jialong Tang",
                "Annan Li",
                "Le Sun",
                "Meng Liao",
                "Shaoyi Chen."
            ],
            "title": "Text2Event: Controllable sequence-tostructure generation for end-to-end event extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Thien Huu Nguyen",
                "Kyunghyun Cho",
                "Ralph Grishman."
            ],
            "title": "Joint event extraction via recurrent neural networks",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2016
        },
        {
            "authors": [
                "Trung Minh Nguyen",
                "Thien Huu Nguyen."
            ],
            "title": "One for all: Neural joint modeling of entities and events",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6851\u20136858.",
            "year": 2019
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Giovanni Paolini",
                "Ben Athiwaratkun",
                "Jason Krone",
                "Jie Ma",
                "Alessandro Achille",
                "Rishita Anubhai",
                "Cicero Nogueira dos Santos",
                "Bing Xiang",
                "Stefano Soatto."
            ],
            "title": "Structured prediction as translation between augmented natural languages",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Alan Ramponi",
                "Rob van der Goot",
                "Rosario Lombardo",
                "Barbara Plank."
            ],
            "title": "Biomedical event extraction as sequence labeling",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5357\u20135367,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Florian Schmidt."
            ],
            "title": "Generalization in generation: A closer look at exposure bias",
            "venue": "arXiv preprint arXiv:1910.00292.",
            "year": 2019
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern."
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596\u20134604.",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "David Wadden",
                "Ulme Wennberg",
                "Yi Luan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Entity, relation, and event extraction with contextualized span representations",
            "venue": "arXiv preprint arXiv:1909.03546.",
            "year": 2019
        },
        {
            "authors": [
                "Haoran Yan",
                "Xiaolong Jin",
                "Xiangbin Meng",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Event detection with multiorder graph convolution and aggregated attention",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Hongming Zhang",
                "Xin Liu",
                "Haojie Pan",
                "Yangqiu Song",
                "Cane Wing-Ki Leung."
            ],
            "title": "Aser: A largescale eventuality knowledge graph",
            "venue": "WWW \u201920, page 201\u2013211, New York, NY, USA. Association for Computing Machinery.",
            "year": 2020
        },
        {
            "authors": [
                "Hongxin Zhang",
                "Yanzhe Zhang",
                "Ruiyi Zhang",
                "Diyi Yang"
            ],
            "title": "Robustness of demonstration-based learning under limited data scenario",
            "year": 2022
        },
        {
            "authors": [
                "Tongtao Zhang",
                "Heng Ji",
                "Avirup Sil."
            ],
            "title": "Joint entity and event extraction with generative adversarial imitation learning",
            "venue": "Data Intelligence, 1(2):99\u2013120.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Event Extraction (EE) aims to extract event records from unstructured texts, which typically include a trigger indicating the occurrence of the event and multiple arguments of pre-defined roles (Doddington et al., 2004a; Ahn, 2006). For instance, the text shown in Figure 1 describes two records corresponding to the Transport and Meet events, respectively. The Transport record is triggered by the word \"arrived\" and consists of 3 arguments:\"Kelly\", \"Beijing\", and \"Seoul\". Similarly, the Meet record is triggered by \"brief \" and has two arguments: \"Kelly\" and \"Yoon\". EE plays a crucial role in natural language processing as it provides valuable information for various downstream tasks, including knowledge graph construction (Zhang et al., 2020) and question answering (Han et al., 2021).\n\u2217Corresponding author.\nMost studies on EE primarily focus on the highresource scenario (Nguyen et al., 2016; Yan et al., 2019; Cui et al., 2020; Du and Cardie, 2020a; Ramponi et al., 2020; Huang and Peng, 2021), which requires a large amount of annotated training data to attain satisfactory performance. However, event annotation is a costly and labor-intensive process, rendering these methods challenging to apply in domains with limited annotated data. Thus, there is a growing need to explore EE in low-resource scenarios characterized by a scarcity of training examples, which has garnered recent attention.\nLu et al. (2021) models EE as a unified sequenceto-structure generation, facilitating knowledge sharing between the Event Detection and Argument Extraction subtasks. Building upon Lu et al. (2021), Lu et al. (2022) introduces various pre-training strategies on large-scale datasets to enhance structure generation and improve low-resource EE performance. Hsu et al. (2022) incorporates supplementary information by manually designing a prompt for each event type, encompassing event descriptions and role relations. However, despite their effectiveness, existing methods exhibit cer-\ntain inadequacies: 1) Sequence-to-structure Generation ignores the gap between the downstream structure generation and pre-training natural language generation. 2) Large-scale pre-training needs a lot of computational resources and excess corpus. 3) Manual design of delicate prompts still demands considerable human effort, and the performance of EE is sensitive to prompt design.\nIn this paper, to tackle EE more effectively with limited resources, we propose the Demonstrationenhanced Schema-guided Generation (DemoSG) model, benefiting from two aspects: 1) Improving the efficiency of using annotated data. 2) Enhancing the knowledge transfer across different events.\nTo make full use of annotated training data, we consider them not only as signals for supervising model learning, but also as task demonstrations to illustrate the extraction process to the model. Specifically, as shown in Figure 1, DemoSG selects a suitable training example for each event, transforms it into a demonstration in the natural language style, and incorporates the demonstration along with the input text to enhance the extraction. We refer to this paradigm as demonstration-based learning for EE, which draws inspiration from the in-context learning of the GPT series (Ouyang et al., 2022). To identify appropriate instances for demonstrating, we devise several selecting strategies that can be categorized into two groups: 1) Demooriented selection, which aims to choose examples with the best demonstrating features. 2) Instanceoriented retrieval, which aims to find the examples that are most semantically similar to the input sentence. With the enhancement of demonstrations, DemoSG can better understand the EE process and learn effectively with only a few training examples.\nTo further enhance the knowledge transfer capability and improve the effectiveness of demonstrations, we leverage label semantic information by formulating EE to a seq2seq generation task guided by schema-based prompts. In detail, DemoSG creates a prompt formally similar to the demonstrations for each event, which incorporates the event type and roles specified in the event schema. Then, the prompt and demo-enhanced sentence are fed together into the pre-trained language model (PLM) to generate a natural sentence describing the event records. Finally, DemoSG employs a rule-based decoding algorithm to decode records from the generated sentences. The schema-guided sequence generation of DemoSG promotes knowledge transfer in\nmultiple ways: Firstly, leveraging label semantics facilitates the extraction and knowledge transfer across different events. For instance, the semantic of the \"Destination\" role hints that its argument is typically a place, and the similarity between \"Attacker\" and \"Adjudicator\" makes it easy to transfer argument extraction knowledge among them, for they both indicate to humans. Secondly, natural language generation is consistent with the pre-training task of PLM, eliminating the need for excessive pre-training of structural generation methods (Lu et al., 2021, 2022). Furthermore, unlike classification methods constrained by predefined categories, DemoSG is more flexible and can readily adapt to new events through their demonstrations without further fine-tuning, which is referred to as the parameter-agnostic domain adaptation capability.\nOur contributions can be summarized as follows: 1) We propose a demonstration-based learning paradigm for EE, which automatically creates demonstrations to help understand EE process and learn effectively with only a few training examples.\n2) We formulate EE to a schema-guided natural language generation, which leverages the semantic information of event labels and promotes knowledge transfer in low-resource scenarios.\n3) We conduct extensive experiments under various low-resource settings on three datasets and study the robustness of DemoSG. The results show that DemoSG significantly outperforms previous methods in low-resource scenarios."
        },
        {
            "heading": "2 Methodology",
            "text": "In this section, we first elucidate some preliminary concepts of low-resource EE in Section 2.1. Next, we present the proposed Demonstration-enhanced Schema-guided Generation model in Section 2.2. Lastly, we provide details regarding the training and inference processes in Section 2.3."
        },
        {
            "heading": "2.1 Low-resource Event Extraction",
            "text": "Given a tokenized input sentence X = {xi}|X|i=1, the event extraction task aims to extract a set of event records R = {Rj}|R|j=1, where Rj contains a trigger Tj and several arguments Aj = {Ajk} |Aj | k=1 . Each Tj or Ajk corresponds to an event type Ej or event role Ojk predefined in the schema S . This paper considers two types of low-resource EE scenarios:\nIn-domain low-resource scenario focuses on the challenge that the amount of training examples is quite limited. Considering the complete train-\ning dataset Dh = {(Xi,Ri)} |Dh| i=1 with the schema Sh and a subset Dl with Sl \u2282 Sh, the objective is to fully use the limited data and train a highperformance model on Dl when |Dl| \u226a |Dh|. The in-domain low-resource scenario poses challenges to the data utilization efficiency of EE models.\nDomain adaptation scenario pertains to the situation where the target domain lacks examples but a large number of source domain examples are available. Given two subsets Dsrc and Dtgt, where Ssrc \u2229 Stgt = \u2298 and |Dtgt| \u226a |Dsrc| , our objective is to initially pre-train a source-domain model on Dsrc, then achieve high performance on target domain subset Dtgt. Domain adaptation enables the low-resource domain to leverage the knowledge acquired from the well-studied domain, requiring a strong knowledge transfer capability of EE models."
        },
        {
            "heading": "2.2 Demonstration-enhanced Schema-guided Generation for Event Extraction",
            "text": "We propose an end-to-end model DemoSG, which leverages demonstration-based learning and schema-guided generation to address the aforementioned low-resource scenarios. As Figure 2 shows, DemoSG initially constructs the task demonstration and prompt for each event type, and then employs a sequence-to-sequence network to generate natural sentences that describe the event records."
        },
        {
            "heading": "2.2.1 Unified Sequence Representation of",
            "text": "Event Record\nTo model EE as a sequence-to-sequence generation task, we design a unified representation template to transform event records to unambiguous natural\nsentences that include event triggers, arguments, and their corresponding event roles.\nIn detail, given a record R with a trigger T and several arguments A = {Ai}|A|i=1, where each Ai corresponds to the role Oi, DemoSG transfers R to a sequence Y = {yj}|Y |j=1 that is designed as \"Event trigger is T . O1 is A1 . O2 is A2...\". Those roles without any arguments are followed by the padding token \"None\". For example, the Transport record in Figure 2 is represented as \"Event trigger is arrived. Artifact is Kelly. Origin is Beijing. Destination is Seoul. Vehicle is None...\". We also consider the following special situations: Multiple records of the same event type can be expressed by concatenating respective sequence representations. Multiple arguments corresponding to the same role will be merged together via \"&\", such as \"Artifact is Kelly & Yoon\". Since DemoSG extracts each type of events separately, event type is no longer required for record representations, which relieves the pressure of model prediction."
        },
        {
            "heading": "2.2.2 Event Demonstration Construction",
            "text": "To effectively use the limited training examples, we not only treat them as traditional supervised learning signal, but also transform them to event demonstrations which can bring additional information and help understand the extraction process.\nThe demonstration Di of event type Ei is a natural sentence containing a context part and an annotation part, which is constructed by the following steps: Firstly, DemoSG selects or retrieves an example (Xi,Ri) from the training set Dtrain, which\ncontains records of Ei. Next, DemoSG transforms records associated with Ei to an annotation sentence Yi following the record representation template in Section 2.2.1. Note that we employ the unified record template for both demonstrations and model predictions, promoting the cohesive interaction between them. Finally, the demonstration Di is constructed by concatenating the context part Xi and the annotation part Yi. Given the significance of selecting appropriate examples for constructing demonstrations, we propose several selection strategies that can be categorized into two groups:\nDemonstration-oriented selection aims to pick the examples with the best demonstrating characteristics. Specifically, the training sample associated with more event roles tends to contain more information of extracting such type of events. And an example with longer text may offer more contextual information for extracting the same event. Based on these considerations, we propose two selection strategies: 1) rich-role strategy selects the example with the highest number of associated roles for each event. 2) rich-context strategy chooses the example with the longest context for each event.\nInstance-oriented retrieval focuses on retrieving examples that are most semantically similar to the input sentence, as the semantic similarity may enhance the effectiveness of demonstrations. Concretely, similar strategy involves encoding the input sentence X and each example sentence Xi using SBERT (Reimers and Gurevych, 2019), followed by calculating the cosine similarity between their [CLS] embeddings to rank Xi. Finally, similar strategy retrieves the top-ranked example for each event type to construct its demonstration."
        },
        {
            "heading": "2.2.3 Schema-based Prompt Construction",
            "text": "We design a prompt template for DemoSG to exploit the semantic information of event types and roles base on the event schema. Given the event schema S = {Ei,Oi}NEi=1, where Ei is the event type and Oi = {Oij}|Oi|j=1 are event roles, the prompt of Ei is designed as: \"Event type is Ei. Event trigger is <Mask>. Oi1 is <Mask>. Oi2 is <Mask>...\", where <Mask> represent the mask token of the PLM. For example, the prompt of aformentioned Transport event can be constructed as \"Event type is Transport. Event trigger is <Mask>. Artifact is <Mask>. Origin is <Mask>...\". Leveraging the label semantic information not only helps query relevant triggers and arguments, but also facilitates knowledge transfer across different events."
        },
        {
            "heading": "2.2.4 Enhanced Sequence-to-sequence Generation for Event Extraction",
            "text": "As Figure 2 shows, DemoSG generates the record sequence of each event type individually via a common encoder-decoder architecture enhanced by respective event demonstration and prompt. Given an input sentence X = {xj}|X|j=1, DemoSG first constructs event demonstration Di = {dij}|Di|j=1 and schema-based prompt Pi = {pij}|Pi|j=1 for the event type Ei. Then, DemoSG concatenates Di, X and Pi via the \"<SEP>\" token, and uses a Transformer Encoder (Vaswani et al., 2017) to obtain the demonstration-enhanced hidden representation: Hi = Encoder([Di;X;Pi]) (1) Subsequently, DemoSG decodes the enhanced representation Hi and generates event record sequence Yi = {yij}|Yi|j=1 token by token:\nyij ,hij = Decoder([Hi;hi1, ...,hi,j\u22121]) (2) where Decoder(\u00b7) represents the transformer decoder and hij is the decoder state at the jth step. By iterating above generation process for all event types {Ei}NEi=1, DemoSG finally obtains the complete record sequence set Y = {Yi}NEi=1."
        },
        {
            "heading": "2.3 Training and Inference",
            "text": "Since DemoSG generates records for each event type individually, we consider the sentences generated for annotated event types as positive examples, while the sentences generated for unannotated event types serve as negative examples. At the training stage, we sample negative examples m times the number of positive samples, where m is a hyperparameter. The following negative log-likelihood loss function is optimized when training:\nL = \u2212 \u2211\nDP\u222aDN\nlog p(Y |X,Dtrain,S, \u03b8) (3)\np(Y |X,Dtrain,S) = |Y |\u220f i=1 p(yi|y<i,Dtrain,S)\n(4) where \u03b8 represents the model parameters, Dtrain is the training set, S is the event schema, DP and DN are the positive and sampled negative sets.\nAt the inference phase, DemoSG decodes event records from the generated Y = {Yi}NEi=1 via a rulebased deterministic algorithm, and employs string matching to obtain the offsets of event triggers and arguments. Following Lu et al. (2021), when the predicted string appears multiple times in the sentence, we choose all matched offsets for trigger prediction, and the matched offset that is closest to the predicted trigger for argument extraction."
        },
        {
            "heading": "3 Experiments",
            "text": "To assess the effectiveness of DemoSG, we conduct comprehensive experiments under in-domain lowresource, domain adaptation, and high-resource settings. An ablation study is applied to explore the impact of each module and the improvement achieved. Furthermore, we also investigate the robustness of applying demonstrations of DemoSG."
        },
        {
            "heading": "3.1 Experimental Settings",
            "text": "Datasets. We evaluate our method on two widely used event extraction benchmarks: ACE05EN (Wadden et al., 2019) and ACE05-EN+ (Lin et al., 2020). Both of them contain 33 event types as well as 22 event roles, and derive from ACE2005 (Doddington et al., 2004b)1, a dataset that provides rich annotations of entities, relations and events in English. The full data splits and preprocessing steps for both benchmarks are consistent with previous works (Wadden et al., 2019; Lin et al., 2020). Additionally, we utilize the same data sampling strategy as UIE (Lu et al., 2022) for the low-resource settings. Detail statistics of the datasets are shown in Table 1.\nEvaluation Metrics. We utilize the same evaluation metrics as previous event extraction studies (Wadden et al., 2019; Lin et al., 2020; Lu et al., 2021, 2022): 1) Trigger Classification Micro F1score (Trig-C): a trigger is correctly classified if its offset and event type align with the gold labels. 2) Argument Classification Micro F1-score (Arg-C): an argument is correctly classified if its event type, offset, and event role all match the golden ones.\nBaselines. We compare our method with the following baselines in low-resource scenarios: 1) OneIE (Lin et al., 2020), the current SOTA high-resource method, which extracts globally optimal event records using global features. 2) Text2Event (Lu et al., 2021), which integrates\n1https://catalog.ldc.upenn.edu/LDC2006T06\nevent detection and argument extraction into a unified structure generation task. 3) UIE (Lu et al., 2022), which improves the low-resource performance through various pre-training strategies based on Text2Event. 4) DEGREE (Hsu et al., 2022), which incorporates additional information by manually designing a prompt for each event type.\nFor the high-resource experiments, we also compare with the following methods: 1) DYGIE++ (Wadden et al., 2019), which is a BERTbased classification model that utilizes the span graph propagation. 2) Joint3EE (Nguyen and Nguyen, 2019), which jointly extracts entities, triggers, and arguments based on the shared representations. 3) GAIL (Zhang et al., 2019), which is a joint entity and event extraction model based on inverse reinforcement learning. 4) EEQA (Du and Cardie, 2020b) and MQAEE (Li et al., 2020), which formulate event extraction as a question answering problem using machine reading comprehension models. 5) TANL (Paolini et al., 2021), which formulates event extraction as a translation task between augmented natural languages.\nImplementations. We employ the BART-large pre-trained by Lewis et al. (2020) as our seq2seq network, and optimize our model via the Adafactor (Shazeer and Stern, 2018) optimizer with a learning rate of 4e-5 and a warmup rate of 0.1. The training batch size is set to 16, and the epoch number for the in-domain low-resource experiments is set to 90, while 45 for the domain adaptation and high-resource experiments of DemoSG. Since ONEIE is not specifically designed for lowresource scenarios, we train it for 90 epochs as well in the in-domain setting. The negative example sampling rate m is set to 11 after the search from {5,7,9,11,13,15} using dev set on the highresource setting, and we find that m can balance Recall and Precision scores in practice. We conduct experiments on the single Nvidia Tesla-V100 GPU. All experiments of baseline methods are conducted based on the released code of their orig-\ninal papers. Considering the influence of sampling training examples in low-resource experiments, we run 5 times with different sampling seeds and report the average results as well as standard deviations. Our code will be available at https://github.com/GangZhao98/DemoSG."
        },
        {
            "heading": "3.2 In-domain Low-resource Scenario",
            "text": "To verify the effectiveness of DemoSG in indomain low-resource scenarios, we conduct extensive experiments in several few-shot and datalimited settings, following Lu et al. (2022). For the few-shot experiments, we randomly sample 2/5/10 examples per event type from the original training set, while keeping the dev and test sets unchanged. For the data-limited experiments, we directly sample 2%/5%/10% from the original training set for model training. Compared with the few-shot setting, the data-limited setting has unbalanced data distributions and zero-shot situations, posing challenges to the generalization ability of EE methods.\nTable 2 presents the results of in-domain lowresource experiments. We can observe that:\n1) DemoSG shows remarkable superiority over other baselines in the in-domain low-resource scenarios. For argument extraction, DemoSG consistently achieves improvements on few-shot and data-limited settings compared to baseline methods. For instance, in the 2/5/10-shot settings of ACE05EN and ACE05-EN+, DemoSG outperforms the highest baseline methods by +7.4%/+10.0%/+7.5% and +9.4%/+7.7%/+11.4% in terms of Arg-C F1, respectively. Regarding event detection, DemoSG also surpasses the highest baselines by +1.0% on AVE-R of ACE05-EN+, and +5.1%/+7.5% on\nAVE-S of two datasets. These results provide compelling evidence for the effectiveness of our method in low-resource event extraction.\n2) It is noteworthy that DemoSG exhibits greater improvement in few-shot settings compared to data-limited settings, and the extent of improvement in data-limited settings increases with the growth of available data. Specifically, for argument extraction, DemoSG achieves a +4.8%/+6.2% improvement in AVE-R, whereas it achieves a higher improvement of +7.4%/+9.0% in AVE-S on the two benchmarks. Furthermore, in the data-limited settings of the two benchmarks, DemoSG demonstrates a +1.1%/+6.5%/+8.2% and +3.9%/+9.7%/+6.5% increase in Arg-C F1, respectively. These observations suggest that our proposed demonstration-based learning may be more effective when there is a more balanced data distribution or greater availability of demonstrations.\n3) All of the proposed demonstration selection strategies achieve strong performance and possess distinct characteristics when compared to each other. The similar retrieving strategy excels in tackling low-resource argument extraction, with a 2.7%/2.0% higher AVE-S of Arg-C than richrole on the two benchmarks. And the rich-context strategy tends to have better low-resource event detection ability, with a 1.7%/0.8% higher AVE-R of Trig-C than similar retriving on both benchmarks."
        },
        {
            "heading": "3.3 Domain Adaptation Scenario",
            "text": "To investigate the cross-domain knowledge transfer capability of DemoSG, we perform experiments in both the parameter-adaptive and parameteragnostic domain adaptation settings.\nParameter-adaptive domain adaptation. Following Lu et al. (2021), in the parameter-adaptive setting, we divide each dataset into a source domain subset src and a target domain subset tgt. The src keeps the examples associated with the top 10 frequent event types, and the tgt keeps the sentences associated with the remaining 23 event types. For both src and tgt, we sample 80% examples for model training and use the other 20% for evaluation. After the sampling, we first pre-train a source domain model on the src, and then fine-tune the model parameters and evaluate on the tgt set. As shown in Table 3, DemoSG outperforms the baseline methods in argument extraction for the target domain, exhibiting a +5.8%/+2.0% improvement in Arg-C F1 on ACE05-EN/ACE05-EN+ compared to the best-performing baselines. For event detection, DemoSG also achieves the highest performance on ACE05-EN+ and performs competitively with the SOTA method ONEIE on ACE05-EN. The results suggest that DemoSG possesses strong domain adaptation capabilities by leveraging the label semantic information of the event schema.\nParameter-agnostic domain adaptation. Unlike previous event extraction methods, we enhance\nthe generation paradigm via demonstration-based learning, enabling DemoSG to adapt to new domains and event types without further finetuning model parameters on the target domain. Specifically, DemoSG can directly comprehend the extraction process of new event types via demonstrations of the target domain at the inference phase. We regard this ability as the parameter-agnostic domain adaptation, which can avoid the catastrophic forgetting (Li et al., 2022) and the extra computing cost brought by the finetuning process. For the parameter-agnostic setting, we first train a source domain model on the src, and then directly evaluate on the tgt set without parameter finetuning. As Table 3 shows, DemoSG gains a significant improvement in both event detection and argument extraction on both datasets. Regarding event detection, DemoSG surpasses the highest baseline UIE by +10.7%/+3.1% on Trig-C F1, benefiting from its ability to comprehend target domain extraction through demonstrations. In terms of argument extraction, although DEGREE showcases strong performance in the parameter-agnostic setting by incorporating role relation information of new event types via manually designed prompts, DemoSG still outperforms DEGREE on both datasets by +18.8%/+8.6% on Arg-C F1. This outcome not only validates the effectiveness of demonstrations for DemoSG but also suggests that demonstrationbased learning is a more effective approach to help comprehend the task than intricate prompts."
        },
        {
            "heading": "3.4 High-resource Scenario",
            "text": "To gain insights into our framework, we also evaluate our method in the high-resource scenario, where each type of training example is abundant. For the high-resource experiments, we train all models on the full training sets and evaluate their performance on the original development and test sets.\nAccording to Table 4, although designed for low-\nresource event extraction, our DemoSG also outperforms baselines by a certain margin in the argument extraction (Arg-C) in the high-resource setting on ACE05-EN+. In terms of the event detection (TrigC), DemoSG also achieves competitive results on both benchmarks (73.4% and 71.2%). The above results prove that DemoSG has good generalization ability in both low-resource and high-resource scenarios. Furthermore, we observe that generative methods perform better than classification-based methods in many cases under both the low-resource and high-resource scenarios, which illustrates the correctness and great potential of our choice to adopt the generative event extraction framework."
        },
        {
            "heading": "3.5 Ablation Study",
            "text": "To examine the impact of each module of DemoSG and the resulting improvement, we perform ablation experiments on three variants of DemoSG in the few-shot, parameter-adaptive, and high-resource settings: 1) w/o Demo, which eliminates demonstrations and generates records solely based on concatenated input text and schema-based prompts. 2) w/o Schema, which excludes the utilization of schema semantics by replacing all labels with irrelevant tokens during prompt construction. 3) w/o Demo&Schema, which removes both demonstrations and schema semantics. From Table 5 we can observe that:\n1) The performance of DemoSG experiences significant drops when removing demonstrations in three different scenarios, particularly -5.5%/- 3.2% for Arg-C F1 in the 5-shot setting on ACE05-EN/ACE05-EN+. This result indicates that demonstration-based learning plays a crucial role in our framework, especially for in-domain lowresource event extraction.\n2) Incorporating label semantic information of the event schema is essential for DemoSG, as it significantly influences all settings. For instance, removing the schema semantic information results in a noteworthy -5.4/-1.9 decrease in Arg-C F1 for the parameter-adaptive domain adaptation setting.\n3) We observe that removing both demonstra-\ntions and the schema semantic information leads to a substantial performance degradation compared to removing only one of them, particularly -26.5%/- 21.5% for Arg-C F1 in the 5-shot setting."
        },
        {
            "heading": "4 Effectiveness and Robustness of Demonstration-based Learning",
            "text": "Since demonstrations can influence the lowresource EE performance, we design two variants for DemoSG with different kinds of damage to the demonstrations to explore the effectiveness and robustness of demonstration-based learning:\n1) Demonstration Perturbation. To analyze the influence of incorrect demonstrations, we sample 40% of the demonstrations and replace the golden triggers and arguments with random spans within the context. Test Perturbation applies perturbation during the inference phase only, while Train-test Perturbation applies perturbation during both the training and inference phases.\n2) Demonstration Dropping. To investigate the robustness of demonstration-based learning to missing demonstrations, we randomly drop 40% of the demonstrations during the training or inference phases. Test Drop performs dropping during the inference phase only, while Train-test Drop applies dropping during both training and inference.\nWe conduct the above experiments based on DemoSG with rich-role strategy under the 5-shot setting. From Figure 3, we can observe that:\n1) Incorrect demonstrations exert a detrimental influence on the model performance. Specifically, perturbing the demonstrations during the inference phase leads to a reduction of -1.3%/-2.2% on ArgC F1 for both benchmarks. The reduction further increases to -1.8%/-3.1% when the perturbation is applied during both the training and inference phases. Despite experiencing a slight performance drop in the presence of incorrect demonstrations, DemoSG consistently outperforms the robust baseline UIE. The results underscore the effectiveness of demonstration-based learning, highlighting the influence of accurate demonstrations in helping model understand the extraction process.\n2) The absence of demonstrations has a more significant impact than incorrect demonstrations. Specifically, when dropping demonstrations only during the inference phase, there is a reduction of -8.5%/-6.6% in Arg-C F1, resulting in DemoSG performing worse than UIE on ACE05-EN+. We consider that this phenomenon can be attributed to the Exposure Bias (Schmidt, 2019) between the training and inference phases, which also explains why DemoSG exhibits fewer improvements in datalimited settings. It is noteworthy that dropping demonstrations during both the training and inference phases leads to a recovery of +3.1%/+3.6% in Arg-C performance compared to Test Drop. These results suggest that reducing exposure bias could be an effective approach to enhance the robustness of demonstration-based learning. We leave further investigation of this topic for future studies."
        },
        {
            "heading": "5 Related Work",
            "text": "Low-resource Event Extraction. Most of the previous EE methods focus on the high-resource scenario (Nguyen et al., 2016; Yan et al., 2019; Cui et al., 2020; Du and Cardie, 2020a; Ramponi et al., 2020; Huang and Peng, 2021), which makes them hardly applied to new domains where the annotated data is limited. Thus, low-resource EE starts to attract attention recently. Lu et al. (2021) models EE as a unified sequence-to-structure generation, which shares the knowledge between the Event Detection and Argument Extraction subtasks. However, the gap between the downstream structure generation and pre-training natural language generation has not been considered. Based on Lu et al. (2021), Lu et al. (2022) proposes several pretraining strategies on large-scale datasets, which boosts the structure generation and improves the low-resource EE performance. However, largescale pre-training needs a lot of computing resources and excess corpus. Hsu et al. (2022) incorporates additional information via manually designing a prompt for each event type, which contains the description of the event and the relation of roles. However, designing such delicate prompts still requires quite a few human efforts, and the EE performance is sensitive to the design. Recently, Lou et al. (2023) proposes unified token linking operations to improve the knowledge transfer ability. We leave the comparison to future studies since their code and data sampling details are not available.\nDemonstration-based Learning. The idea is originally motivated by the in-context learning ability of the GPT3 (Brown et al., 2020), where the model learns by only conditioning on the demonstration without tuning. However, in-context learning relies on the enormous size of PLMs. To make the demonstration effective for small PLMs, Lee et al. (2022) proposes a demonstrationbased learning framework for NER, which treats demonstrations as additional contextual information for the sequence tagging model during training. Zhang et al. (2022) further studies the robustness of demonstration-based learning in the sequence tagging paradigm for NER. Our method differs from these studies from: 1) Our demonstrationenhaced paradigm is designed according to the specific characteristics of event extraction. 2) Our generative framework enhances the flexibility of demonstration-based learning, providing it with the parameter-agnostic domain-adaptation capability."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose the Demonstrationenhanced Schema-guided Generation (DemoSG) model for low-resource event extraction, which benefits from two aspects: Firstly, we propose the demonstration-based learning paradigm for EE to fully use the annotated data, which transforms them into demonstrations to illustrate the extraction process and help the model learn effectively. Secondly, we formulate EE as a natural language generation task guided by schema-based prompts, thereby leveraging label semantics and promoting knowledge transfer in low-resource scenarios. Extensive experiments and analyses show that DemoSG significantly outperforms current methods in various low-resource and domain adaptation scenarios, and demonstrate the effectiveness of our method.\nLimitations\nIn this paper, we propose the DemoSG model to facilitate low-resource event extraction. To exploit the additional information of demonstrations and prompts, DemoSG generates records of each event type individually. Though achieving significant improvement in low-resource scenarios, the individual generation makes DemoEE predict relatively slower than methods that generate all records at once (Lu et al., 2021, 2022).\nEthics Statement\nThe contributions of this paper are purely methodological, specifically introducing the Demonstration-enhanced Schema-guided Generation model (DemoSG) to enhance the performance of low-resource Event Extraction. Thus, this paper does not have any direct negative social impact."
        },
        {
            "heading": "Acknowledgements",
            "text": "We sincerely thank all anonymous reviewers for their valuable comments and suggestions. This work was supported by Program for Youth Innovative Research Team of BUPT No. 2023QNTD02."
        },
        {
            "heading": "A Comparison of Model Parameters",
            "text": "We put the comparison of the PLMs and the amount of model parameters of DemoSG with other lowresource baselines in Table 6."
        }
    ],
    "title": "DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction",
    "year": 2023
}