{
    "abstractText": "Counter-argument generation\u2014a captivating area in computational linguistics\u2014seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused challenges. Furthermore, the diverse nature of counter-arguments poses challenges for evaluating model performance solely based on ngram-based metrics. In this paper, we present the ArgTersely benchmark for sentence-level counter-argument generation, drawing from a manually annotated dataset from the ChangeMyView debate forum1. We also propose ArgLlaMA for generating high-quality counterargument. For better evaluation, we trained a BERT-based evaluator Arg-Judge with human preference data. We conducted comparative experiments involving various baselines such as LlaMA, Alpaca, GPT-3, and others. The results show the competitiveness of our proposed framework and evaluator in counter-argument generation tasks. Code and data are available at https://github.com/ amazingljy1206/ArgTersely.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiayu Lin"
        },
        {
            "affiliations": [],
            "name": "Rong Ye"
        },
        {
            "affiliations": [],
            "name": "Meng Han"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Ruofei Lai"
        },
        {
            "affiliations": [],
            "name": "Xinyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhao Cao"
        },
        {
            "affiliations": [],
            "name": "Xuanjing Huang"
        },
        {
            "affiliations": [],
            "name": "Zhongyu Wei"
        }
    ],
    "id": "SP:c39d9260ae957acfecc285bdd2c09655fa074e38",
    "references": [
        {
            "authors": [
                "Milad Alshomary",
                "Shahbaz Syed",
                "Arkajit Dhar",
                "Martin Potthast",
                "Henning Wachsmuth."
            ],
            "title": "Counterargument generation by attacking weak premises",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1816\u20131827, On-",
            "year": 2021
        },
        {
            "authors": [
                "Milad Alshomary",
                "Henning Wachsmuth."
            ],
            "title": "Conclusion-based counter-argument generation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 957\u2013967, Dubrovnik, Croatia. Associ-",
            "year": 2023
        },
        {
            "authors": [
                "Eric Bolton",
                "Alex Calderwood",
                "Niles Christensen",
                "Jerome Kafrouni",
                "Iddo Drori."
            ],
            "title": "High quality real-time structured debate generation",
            "venue": "arXiv preprint arXiv:2012.00209.",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Yukang Chen",
                "Shengju Qian",
                "Haotian Tang",
                "Xin Lai",
                "Zhijian Liu",
                "Song Han",
                "Jiaya Jia."
            ],
            "title": "Longlora: Efficient fine-tuning of long-context large language models",
            "venue": "arXiv preprint arXiv:2309.12307.",
            "year": 2023
        },
        {
            "authors": [
                "Rewon Child",
                "Oleksandr Polozov",
                "Katherine Lee",
                "Zongwei Zhou",
                "Xuezhi Wang",
                "Brennan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "Palm: Scaling language",
            "year": 2023
        },
        {
            "authors": [
                "T. Edward Damer."
            ],
            "title": "Attacking Faulty Reasoning: A Practical Guide to Fallacy-Free Arguments",
            "venue": "Belmont, CA: Wadsworth/Cengage Laerning.",
            "year": 2009
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer."
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Or Honovich",
                "Uri Shaham",
                "Samuel R. Bowman",
                "Omer Levy."
            ],
            "title": "Instruction induction: From few examples to natural language task descriptions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2023
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Hua",
                "Zhe Hu",
                "Lu Wang."
            ],
            "title": "Argument generation with retrieval, planning, and realization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2661\u2013 2672, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Xinyu Hua",
                "Lu Wang."
            ],
            "title": "Neural argument generation augmented with externally retrieved evidence",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 219\u2013230, Melbourne, Australia.",
            "year": 2018
        },
        {
            "authors": [
                "Lu Ji",
                "Zhongyu Wei",
                "Jing Li",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Discrete argument representation learning for interactive argument pair identification",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Kee."
            ],
            "title": "The art of argument: a guide to mooting, volume 196",
            "venue": "Cambridge University Press.",
            "year": 2006
        },
        {
            "authors": [
                "Samuel Kotz",
                "Norman L Johnson."
            ],
            "title": "Breakthroughs in Statistics: Methodology and distribution",
            "venue": "Springer Science & Business Media.",
            "year": 2012
        },
        {
            "authors": [
                "Alon Lavie",
                "Abhaya Agarwal."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228\u2013231, Prague, Czech Republic.",
            "year": 2007
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Schiller",
                "Johannes Daxenberger",
                "Iryna Gurevych."
            ],
            "title": "Aspect-controlled neural argument generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Chandan Singh",
                "John X Morris",
                "Jyoti Aneja",
                "Alexander M Rush",
                "Jianfeng Gao."
            ],
            "title": "Explaining patterns in data with language models via interpretable autoprompting",
            "venue": "arXiv preprint arXiv:2210.01848.",
            "year": 2022
        },
        {
            "authors": [
                "Christian Stab",
                "Tristan Miller",
                "Benjamin Schiller",
                "Pranav Rai",
                "Iryna Gurevych."
            ],
            "title": "Cross-topic argument mining from heterogeneous sources",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3664\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Chenhao Tan",
                "Vlad Niculae",
                "Cristian DanescuNiculescu-Mizil",
                "Lillian Lee."
            ],
            "title": "Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions",
            "venue": "Proceedings of the 25th international conference on world wide",
            "year": 2016
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Stephen E Toulmin."
            ],
            "title": "The uses of argument",
            "venue": "Cambridge university press.",
            "year": 2003
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "2023a. Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Seonghyeon Ye",
                "Doyoung Kim",
                "Joel Jang",
                "Joongbo Shin",
                "Minjoon Seo."
            ],
            "title": "Guess the instruction! flipped learning makes language models stronger zero-shot learners",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jian Yuan",
                "Zhongyu Wei",
                "Donghua Zhao",
                "Qi Zhang",
                "Changjian Jiang."
            ],
            "title": "Leveraging argumentation knowledge graph for interactive argument pair identification",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan."
            ],
            "title": "DIALOGPT : Large-scale generative pre-training for conversational response generation",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Counter-argument generation task aims to automatically generate a statement that has a different stance from the original argument (Toulmin, 2003; Damer, 2009). Existing works describe it as a paragraph-level generation task (Hua and Wang, 2018; Alshomary et al., 2021; Alshomary and Wachsmuth, 2023). However, sentence-level counter-argument generation can be quite different. The main challenge of sentence-level generation is to condense the counter-argument into a concise\n\u2217Corresponding author. 1https://www.reddit.com/r/changemyview/\nsentence. It requires identifying the key points and formulating a counter-argument in a limited space. An example of the difference between paragraphlevel and sentence-level counter-argument generation is shown in Figure 1.\nTo address this challenge, we propose a benchmark ArgTersely for sentence-level counterargument generation. The dataset is derived from ChangeMyView (CMV), an online debate forum, and has been annotated by humans.\nRecently, large language models, such as OpenAI ChatGPT and GPT-4 (Bubeck et al., 2023), PaLM (Chowdhery et al., 2023), and LlaMAs (Touvron et al., 2023a,b) have achieved great success and demonstrated remarkable performance in text generation tasks. By leveraging the pretrained language model, we propose a framework,\nArg-LlaMA, to generate high-quality counterarguments. Our framework is a pipeline comprising (1) an instruction component, (2) a language model, and (3) a filter component. The instruction component comprises multiple Chainof-Thought (CoT; Wei et al., 2022) instructions addressing common errors in debates along with their corresponding reasoning steps. As for the language model, we utilize instruct-tuning (Wei et al., 2021) on LlaMA-7b (Touvron et al., 2023a) with the Low-rank Adaptation (Hu et al., 2021) method. During inference, we employ multiple CoT instructions as input for the language model and utilize the filter component to select the best candidate counter-argument as the output of the system.\nPrevious work typically employed n-gram-based metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) for rapidly evaluating the quality of counter-argument generation (Alshomary et al., 2021; Schiller et al., 2021). However, we believe that these metrics do not effectively assess whether the generated sentences are pertinent and in line with human preferences. To this end, we propose incorporating model-based metrics, ArgJudge, as a supplementary evaluation approach. Specifically, we trained a BERT-based (Devlin et al., 2019) model, using the human preference data generated during the annotation process. In addition, we introduce a metric, ChatGPT Eval, which we obtain by using ChatGPT to score the sentence\u2019s position and argument completion. Moreover, we have made the human evaluation more specific by asking human annotators to assess the outputs based on five dimensions, which enables a comprehensive evaluation of the model\u2019s performance.\nOur contributions are mainly as follows:\n\u2022 We propose a benchmark, ArgTersely, for sentence-level counter-argument generation and a dataset annotated by humans.\n\u2022 We propose a counter-argument generation framework, Arg-LlaMA. The framework is capable of generating high-quality counterarguments.\n\u2022 We propose a novel, lightweight evaluator, Arg-Judge, which enables it to reflect the real ranking and is highly consistent with human evaluation."
        },
        {
            "heading": "2 ArgTersely",
            "text": ""
        },
        {
            "heading": "2.1 Task Formulation",
            "text": "The task input consists of two components: a topic and an original argument. (1) The topic, denoted as \u03c4 , explains the premise of the dialogue and the focus of the debate. (2) The original argument, denoted as x, is a sentence containing the initial perspective or stance put forward. The objective of this task is to generate a sentence, y, that provides a coherent rebuttal response to x based on the given topic \u03c4 ."
        },
        {
            "heading": "2.2 Dataset Creation",
            "text": "We based our dataset annotation on the CMV dataset (Tan et al., 2016), sourced from the ChangeMyView (CMV) subreddit2. CMV users post various topics, with a unique quoting dynamic: User B quotes a segment of User A\u2019s statement (usually a sentence) and responds (often with multiple sentences). We extracted 20, 626 triplets from CMV, emphasizing reply relationships. Each triplet includes a topic, a quoted statement (original argument), and its reply. The reply, split into sentences, forms a candidate set. Annotators select sentences from this set that counter the original argument, creating counter-argument pairs. Sentences are skipped if they are incomplete, lack a viewpoint, or breach our ethical guidelines (Appendix F). Any sentence flagged by an annotator for violating these guidelines is excluded. Our annotation process comprises data preprocessing, trial annotation, and formal annotation to ensure dataset quality. Data Preprocessing We segment User B\u2019s replies into candidate sentences using punctuation and remove those with hyperlinks, emails, phone numbers, emojis, etc. Grammar issues, such as capitalization and spacing, are corrected. Trial Annotation Before formal annotation, annotators are trained on rules and the annotation system. After that, they undertake trial annotation on 50 triplets. We retained annotators who displayed high consistency with our reference annotations, totaling 24. Formal Annotation To minimize bias, we employ a cross-annotation strategy. Two annotators assess the same triplet, and any discrepancies are resolved by a third. Approximately 30% of the dataset is generated through arbitration.\n2https://www.reddit.com/\nAll annotators have substantial debate experience and at least a bachelor\u2019s degree. The annotation process spanned 42 days, yielding 31, 197 argument-counterargument pairs, each associated with the relevant topic. We highlight the ethical considerations during the annotation process, including potential risks, identifiable information, compensation, and annotation biases in Section 9. The statistics of ArgTersely are shown in Table 1."
        },
        {
            "heading": "3 Arg-LlaMA",
            "text": "Figure 2 shows the framework we proposed, ArgLlaMA. It is mainly composed of two parts: 1) a language model (LM) with instruct-tuning, for generating counter-argument, and 2) a filter, for se-\nlecting high-quality counter-argument. We employ LoRA and instruct-tuning methods to obtain an LM. Additionally, we leverage human preference data to train the filter.\nDuring inference, we use CoT instructions as inputs of the LM. After obtaining a series of outputs from the LM, the filter will select the best counterargument as output. The generation pipeline is detailed in Section 3.3."
        },
        {
            "heading": "3.1 Instruct-tuning the LlaMA",
            "text": "Instruction Set Creation In line with the selfinstruct (Wang et al., 2023) approach, we initially generated 148 instructions based on 10 seed instructions. Following a manual verification process, these instructions were expanded to form an Argumentation Instruction Set consisting of 2,772 instructions. Specifically, our specific implementation differs from the self-instruct method in the following aspects:\n1. Our seed instructions focus on argument-related instructions, such as \u201cProvide evidence to support the conclusion\u201d, \u201cPoint out its logical error\u201d, etc. A detailed list of specific instructions\nis shown in Appendix A and attached files. 2. We use the ChatGPT 3 to generate instructions,\nenabling us to generate more diverse and elaborate contexts.\nLow-Rank Tuning Using the above Argumentation Instruction Set and Alpaca instruction set (Taori et al., 2023), we fine-tuned LlaMA-7b model with LoRA method. LoRA maps the weight update of the self-attention module projection matrix in the Transformer (Vaswani et al., 2017) architecture to a lower dimension and then returns to the normal output dimension. In our work, we performed LoRA on all Query/Key/Value/Output projection matrices in the self-attention module."
        },
        {
            "heading": "3.2 Training the Filter",
            "text": "The filter component is also a language model. We designed this component with the purpose of selecting high-quality counter-arguments from candidate sentences. Ranking Data for training Our training data, named Ranking Data (RD), originates from human preference data generated during the annotation process of ArgTersely dataset. Given an original argument x, we assign ranking scores to candidate sentences based on the following rules:\n1 = Sentences selected by annotators that can form a strong rebuttal relationship with x.\n2 = Sentences not selected by the annotator but belonging to the same conversation as x.\n3 = Safe reply, randomly selected from a predefined list, as listed in Appendix B\n4 = Sentences sampled from other conversations.\nWe finally got 20,000 training samples and 800 testing samples, each sample consists of an original argument and four candidates. We denoted the original argument as x, the candidates list as Y = [y1, y2, y3, y4], and the ranking score for yi as si, i \u2208 {1, 2, 3, 4}. Training Task The training task is learning to rank the candidates in the correct sequence. In this task, we assign the ranking scores of four candidates as the ground truth, with higher scores indicating lower quality.\nTo optimize the parameters \u03b8 of the filter, we first used the parameters of BERT-base (Devlin et al., 2019) to initialize it. The loss function we\n3We use gpt-3.5-turbo as in https://platform. openai.com/docs/models/gpt-3-5, and ditto.\nemployed is cross-entropy loss:\nL = \u2212 logP\u03b8(si|x, yi), i \u2208 {1, 2, 3, 4} (1)"
        },
        {
            "heading": "3.3 Generation Pipeline",
            "text": "The generation pipeline consists of three steps: 1) provide CoT instructions to guide the LM, 2) use the LM, generates outputs based on instructions, and 3) apply filtering to refine and obtain the final result by selecting the most appropriate counterargument. CoT Instruction Our generation pipeline starts with a series of CoT instructions. We propose CoT instructions to guide the model in generating realistic and logical arguments through multi-step reasoning. Based on Kee\u2019s (2006) debating theory, we design few-shot and multi-step reasoning templates for several common errors in debate. We roughly divide common errors into the following categories:\n\u2022 Factual Error: a mistake in the presentation of fact.\n\u2022 Logical Fallacy: errors in reasoning that undermine the validity of an argument.\n\u2022 Confirmation Bias: errors in selectively interpreting information in a way that supports existing hypotheses.\nThe specific formats of these instructions are listed in Appendix C. During inference, we provide the LM with a set of instructions that correspond to the aforementioned errors for generating the counter-arguments. LM The LM serves two roles. Firstly, it acts as an error identifier, tasked with identifying errors within the original argument. Secondly, it generates a candidate counter-argument for each instruction provided. Filter After the LM with various CoT instructions, we get a set of candidate counter-argument Y = [y\u03021, y\u03022, ..., y\u0302n]. The purpose of the filter is to select the one that maximizes the probability as the output of the system:\ny\u2217 = argmax yi P\u03b8(si = 1|x, yi), i = 1, ..., n (2)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "Dataset Our experiments are performed on the test set of ArgTersely dataset. It consists of 2,000\ntriplets in the format of <topic, original argument, counter-argument>. Implementation Details When training the base LM with instruct-tuning, we use LlaMA-7b as the base model. We set the learning rate to 3\u00d7 10\u22124, batch size to 256, gradient accumulation step to 16, and train the model 5 epochs on 4 NVIDIA RTX3090 GPUs. The \u03b1 and r of the LoRA method are both set to 16. When training the filter, we use BERT-base as the base model. We set the learning rate to 1\u00d7 10\u22125, batch size to 64, and train on an NVIDIA RTX3090 GPU for 2 epochs. For training both models, we employed AdamW (Loshchilov and Hutter, 2018) as optimizer. Models for Comparision We compare our system with several baselines:\n\u2022 BART (Lewis et al., 2020): a pre-trained language model with encoder-decoder structure, and we fine-tuned it to adapt this task.\n\u2022 GPT-2 (Radford et al., 2019): a pre-trained language model with decoder-only structure.\n\u2022 DialoGPT (Zhang et al., 2020): a decoder-only language model which was trained on online dialogue corpus.\n\u2022 LlaMA (Touvron et al., 2023a): a collection of models trained on publicly available datasets, and we use LlaMA-7b.\n\u2022 Alpaca-LoRA (Taori et al., 2023): a model obtained by LoRA-tuning LlaMA-7b based on the Alpaca instruction set.\n\u2022 GPT-3 (Brown et al., 2020): a large language model without instruct-tuning."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "Our evaluation metrics include automatic evaluation metrics and human evaluation.\nAutomatic Evaluation First, we do not entirely disregard n-gram-based automatic evaluation metrics that commonly utilized, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007).\nHowever, of greater importance, we present two model-based evaluation metrics to assess performance differences among different systems. A detailed explanation follows:\n\u2022 ChatGPT Eval: We utilize two instructions to guide ChatGPT in generating the stance score (Sst) and the argument completeness score (Scom), both of which range from 0 to 100. The instructions we employed are outlined in Appendix D. The stance score assesses whether the original sentence and the generated sentence have opposing stances, while the completeness score gauges the generated counter-argument\u2019s caliber, specifically if it makes logical sense. We employ a weighted average of these two scores to get the final score of ChatGPT Eval.\nSgpt = \u03bbSst + (1\u2212 \u03bb)Scom (3)\n, where \u03bb is set to 0.5 in our experiments. To reduce the uncertainty of ChatGPT provided, we set the temperature factor to 0.1.\n\u2022 Arg-Judge: In order to ascertain the degree of relevance and informativeness of the generated counter-arguments, we adopt a \u201creverse validation\u201d approach using the Filter model that was trained in Section 3.2. To this end, we establish Arg-Judge as the metric for evaluating the efficacy of this approach in identifying meaningful counter-arguments that are not mere nonsense safe replies. Specifically, we normalize the average-pooled hidden before the softmax layer of the filter model \u03b8 to get a continuous\npredicted score s\u0302 \u2208 [0, 4]. We empirically define the Arg-Judge score as\nSaj = max{ 1 9 \u2212 s\u0302 36 , 1\u2212 9 4 s\u0302} (4)\nWe selected the hyper-parameter setting based on our observation that large language models (such as Alpaca-LoRA and GPT-3) tend to generate sentences with scores concentrated between 0 and 0.8. Arg-Judge can thus enhance the distinguishability for these high-scoring sentences, while still maintaining the monotonicity.\nHuman Evaluation Based on the work of Hua et al. (2019), we conducted a more detailed human evaluation. Five human judges are asked to rate arguments on a Likert scale of 1 (worst) to 5 (best) across 5 dimensions to evaluate the performance of the systems:\n\u2022 Grammaticality: assess whether output adheres to the rules of grammar.\n\u2022 Appropriateness: focus on whether the output is contextually suitable.\n\u2022 Content Richness: reflect the depth of information provided by output.\n\u2022 Logic: measure the rationality of output. \u2022 Persuasiveness: show the extent to which\nreaders are persuaded by output. Additionally, We use Top-1 to represent the proportion of the best output. We emphasize the importance of human evaluation as it provides results that are more aligned with human value, compared to automatic metrics based on n-grams or models."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Automatic Evaluation As the results listed in Table 2, we have the following conclusions: \u2022 Our model achieves the best results on most of\nthe metrics. The components in our framework help our system to generate more correct and fluent counter-argument.\n\u2022 Arg-LlaMA and Alpaca-LoRA outperform noninstruct-tuning models like LlaMA and GPT3. Because non-instruct-tuning models is tend to enhance and extend the original argument, rather than forming a rebuttal relationship with it, which indicates a stance error.\n\u2022 Comparing n-grams to reference sentences for argument generation tasks is often insufficient. Our proposed model-based metrics, ChatGPT Eval, and Arg-Judge, not only demonstrate consistency with the n-gram metric, but they are\nalso complemented with each other. For instance, BART and Dialog-GPT models tend to generate stance-correct but logic-and-contentlacking \u201csafe replies\u201d, which may receive acceptable scores in BLEU, ROUGE, or ChatGPT Eval, but low scores from Arg-Judge. This addresses the limitations of traditional metrics used in the past.\nHuman Evaluation We report the result of human evaluation in Figure 3 and Table 3, and we have the following observations:"
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "We perform ablation studies to explore the role of different components in both training and generation process. We explored four variants in addition to the overall framework: 1) Instead of the argumentation instruction set, we use the Alpaca instruction set to instruct-tuning the LM. 2) We replace the LM with LlaMA-7b, which has not been fine-tuned by instructions. 3) We remove CoT instructions components and use a series of simple instructions, such as \u201cGive me the counterargument\u201d. 4) We remove the filter components and select output from the candidates randomly.\nThe results of the ablation study are presented in Table 4. We have the following findings:\n\u2022 Using argumentation instructions during training is very helpful for the model. This clearly demonstrates the effectiveness of our proposed argumentative instruction set. We make the argumentation instruction set publicly accessible to benefit the wider community.\n\u2022 Instruct-tuning matters. Simply generating from LlaMA affects performance, while instruction tuning can help the model better adapt to argumentation scenarios, respond to the instructions, and reason out correct rebuttals from CoT instructions.\n\u2022 Compared with common instructions, CoT instructions can produce higher-quality counter-arguments. This is because CoT instructions can give a logical chain and a multistep reasoning process, which improves the quality of output.\n\u2022 Multiple error templates can improve the quality of generated counter-arguments. Multiple error templates can help the LMs discover potential errors from multiple perspectives, thus generating richer candidate sentences.\n\u2022 Filter component plays a crucial role in our system. It enables us to select high-quality arguments from candidate sentences, while random selection fail to achieve similar performance."
        },
        {
            "heading": "5 Validation of Arg-Judge Metric",
            "text": "In order to explore the capability of our proposed Arg-Judge to reflect the actual ranking level and its consistency with human evaluation, we designed two corresponding tasks."
        },
        {
            "heading": "5.1 Datasets for Validation",
            "text": "Ranking Data (RD): We use the test set of this dataset to check if the Arg-Judge can reflect the real ranking. As mentioned in Section 3.2, it has 800 testing samples. A sample includes an original argument and four candidate counter-arguments.\nQuality Selection Dataset (QSD): This dataset is used to check whether the Arg-Judge is aligned with human evaluation. It consists of 500 triplets in the format of <original argument, better counterargument, worse counter-argument>. Given an original arguments from ArgTersely, we first used the ChatGPT to generate two counter-arguments and then manually selected one as the better counter-argument and another as the worse counterargument."
        },
        {
            "heading": "5.2 Validation Tasks and Comparisions",
            "text": "RD: Can Arg-Judge reflect the real ranking? Given the original argument, the task on the RD dataset is to select the best counter-argument from four candidates. We use precision at one (P@1) to measure the ability of Arg-Judge to reflect real ranking.\nQSD: Is Arg-Judge consistent with human evaluation? Given the original argument, the task on the QSD dataset is to select a better counter-argument from two candidates. We use accuracy to reflect the consistency between Arg-Judge and human evaluation.\nComparisions We use BERT-base and ChatGPT as comparisons. To adapt ChatGPT to these tasks, we constructed two instructions. Specific information about the instructions is in Appendix E."
        },
        {
            "heading": "5.3 Validation Result",
            "text": "Arg-Judge can reflect the real ranking. The result in Table 5 shows that the performance of ArgJudge is better than ChatGPT and BERT. It means that Arg-Judge demonstrates sorting capabilities that reflect real-world scenarios after training. Arg-Judge is highly consistent with human evaluation. Result is in Table 6. Based on the result, the consistency with human evaluation ranks in the following order from high to low: Arg-Judge, ChatGPT, and BERT. It shows the high consistency between Arg-Judge and human evaluation."
        },
        {
            "heading": "6 Case Study",
            "text": "We illustrate the advantage of our model through a case study in Table 7. Alpaca-LoRA and LlaMA fail to adequately tap into the subtext of the original argument that the white upper middle class cannot help the working class when they are fighting each other. And without instruct-tuning, GPT-3 and LlaMA mostly consist of extensions or additions to the original text, lacking a compelling rebuttal. Whereas, our model recognizes this implicit logic and implements a counter-argument that it is not a plausible excuse."
        },
        {
            "heading": "7 Related Work",
            "text": "Counter-Argument Generation Datasets (Ji et al., 2021; Yuan et al., 2021; Hua and Wang, 2018; Stab et al., 2018) for counter-argument generation mainly establish the rebuttal relationship in the conversation using automatic methods such as citation or reply detection. Tan et al. (2016)\nproposed CMV dataset, including the citation relationship between original posts and their corresponding replies. Bolton et al. (2020) introduced Kialo, a dataset for sentence-level argument stance classification and counter-argument generation. ArgTersely distinguishes itself as the first human-annotated dataset of its kind with ranking data reflecting human preferences. Early work (Hua and Wang, 2018; Hua et al., 2019) focus on how to introduce external knowledge into the system; Alshomary et al. (2021) developed a system to identify weak points in arguments; Schiller et al. (2021) developed a controlled argument generation system, which is able to generate arguments based on given information; Alshomary and Wachsmuth (2023) completed it through multitask and multi-step reasoning. Our work primarily introduces benchmark and evaluation metrics for sentence-level argument generation. Methodologically, we just establish a usable baseline using LLM without introducing too much external knowledge.\nSelf-Instruct A series of recent works (Zhou et al., 2022; Ye et al., 2022; Singh et al., 2022; Honovich et al., 2023) generate instructions of a task given a few examples. Singh et al. (2022) use LLM and reranking algorithm to generate humaninterpretable instruction, which matches or even im-\nproves upon human-written instruction. Honovich et al. (2023) introduce the instruction induction challenge task and discover the ability to generate instructions emerge when a language model is large enough. Wang et al. (2023) provide an almost annotation-free method for aligning pre-trained language models with instructions. The overall process is an iterative bootstrapping algorithm, which starts off with a limited seed set of manually written instructions that are used to guide the overall generation. We fine-tuned the Arg-LlaMA model using the self-instruct approach, where we included seed instruction for a variety of related tasks of the counter-argument generation.\nAdaptation As the size of the language model increases, the cost of fine-tuning also increases. A series of works (Shin et al., 2020; Li and Liang, 2021; Houlsby et al., 2019) have studied various methods like prompt-tuning and adapter-tuning to alleviate this problem. However, it\u2019s difficult to directly optimize the prompt, and introducing the adapter layer will cause a delay in reasoning. Considering that, Hu et al. (2021) proposed LowRank Adaptation (LoRA), which greatly reduces the number of trainable parameters for downstream tasks. Additionally, there have been advancements in extending Low-Rank Adaptation. Dettmers et al. (2023) proposed QLoRA that fine-tunes large models on limited memory GPUs through 4-bit quantization and Low-Rank Adapters. Chen et al. (2023) introduced LongLora that leverages sparse local attention and achieves context extension with minimal computation. Since our work does not involve long contexts in generation and does not prioritize optimization techniques, we just utilize LoRA to fine-tune our model for efficiency."
        },
        {
            "heading": "8 Conclusions",
            "text": "In this paper, we introduce a benchmark ArgTersely for sentence-level counter-argument generation. Specifically, we present a human-annotated dataset and develop a language model based on argumentation instructions. We further construct a framework Arg-LlaMA, which leverages the language model. Additionally, we propose two model-based metrics, ChatGPT Eval and Arg-Judge, as complements to n-gram-based metrics. Experiments show that our framework competes well with mainstream models, and our metrics are effective and highly consistent with human evaluations."
        },
        {
            "heading": "9 Ethical Considerations",
            "text": "Since we propose a new dataset ArgTersely, we solve some possible ethical issues in this section. Potential Risk Our dataset is sourced from ChangeMyView (CMV), a subcommunity on Reddit. Users must adhere to community rules4, including restrictions on hate speech. We also formulate an ethical guideline and require annotators to follow it. We train annotators to mark and skip sentence violating the ethical guideline. Annotators were informed about potential risks. Our annotation process respects intellectual property and privacy rights. Identifiable Information Our data is sourced from open platforms, safeguarding privacy. We also removed sensitive information such as emails, phone numbers, and usernames during data preprocessing. Compensation We employed 24 part-time annotators, compensating them at $0.25 per conversation (equivalent to at least $3.75 per hour, with a cap of 2 hours per day), which surpasses the local minimum wage. Annotation Bias We perform a series of methods to reduce the bias during annotation, including annotator training, trial annotation, and crossannotation.\nLimitations\nWhile the experimental results demonstrate the effectiveness of Arg-Judge, it is important to note that our exploration of the consistency between human evaluation and language model evaluators (including ChatGPT Eval and Arg-Judge) was limited to a specific set of scenarios. Furthermore, due to computational resource constraints, we were unable to train a larger-scale language model as an evaluator. Moving forward, our future research will involve expanding the evaluation of the language model evaluator across a broader range of scenarios and utilizing a larger-scale language model to enhance its capabilities."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is supported by National Natural Science Foundation of China (No. 6217020551) and Science and Technology Commission of Shanghai Municipality Grant (No.21QA1400600).\n4https://www.reddit.com/r/changemyview/wiki/ rules/"
        },
        {
            "heading": "A Examples of Seed Instructions",
            "text": "In Section 3.1, We demonstrated how the instruction set for tuning the LLaMA model was created, and here we present 6 seed instructions, along with examples of input-output pairs. Note: \u201cinstruction\u201d and \u201cinput\u201d denote the parts we provided as input, whereas \u201coutput\u201d denotes the outcomes generated by ChatGPT. Please refer to the attached file for all 10 seed instructions.\nInstruction: Give some facts to rebute it. Input: Climate change is not real. Output: The overwhelming majority of scientific evidence supports the fact that climate change is real and is primarily caused by human activities, such as burning fossil fuels and deforestation.\nInstruction: Point out its logical error. Input: All successful people are happy. Output: It assumes that success is the only factor that determines one\u2019s happiness, ignoring other important factors such as personal relationships, mental health, and overall life satisfaction. Success does not guarantee happiness, and happiness can be achieved through various means beyond just professional accomplishments.\nInstruction: Give evidence to support it. Input: Vaccines are safe and effective in preventing the spread of infectious diseases. Output: Numerous studies have shown that vaccines are safe and effective in preventing the spread of infectious diseases. For example, a study published in the New England Journal of Medicine found that the measles vaccine prevented 95% of cases among vaccinated individuals. Additionally, the World Health Organization reports that vaccines have prevented millions of deaths worldwide from diseases such as polio, measles, and tetanus. The overwhelming scientific consensus is that vaccines are a crucial tool in protecting public health.\nInstruction: Show evidence that can challenge it. Input: Vaccines are harmful and can cause autism. Output: Numerous scientific studies have shown that vaccines are safe and do not cause autism. For example, a study published in the Journal of the American Medical Association found no link between vaccines and autism. Additionally, the Centers for Disease Control and Prevention (CDC) states that there is no evidence to support the claim that vaccines cause autism. The overwhelming consensus among medical professionals is that vaccines are safe and effective in preventing the spread of disease.\nInstruction: Can we believe this statement? Input: All politicians are corrupt. Output: No, we cannot believe this statement. While there may be instances of corruption in politics, it is unfair and inaccurate to generalize and say that all politicians are corrupt. There are many politicians who work tirelessly for their constituents and uphold high ethical standards in their work. It is important to evaluate individuals based on their actions and not make sweeping generalizations about entire groups of people.\nInstruction:Identify the potential consequences of following this statement. Input: I don\u2019t need to wear a mask, COVID-19 is a hoax. Output: The potential consequences of following this statement include contracting and spreading COVID-19 to others, potentially causing serious illness or death. It also undermines the efforts of healthcare workers and public health officials who are working to control the spread of the virus and protect the community. Additionally, it can lead to fines or other legal consequences for violating mask mandates or public health orders."
        },
        {
            "heading": "B The List of Safe Replies",
            "text": "\u201cSafe reply\u201d refers to a direct refutation that is completely unconvincing in an argument. These sentences may potentially be generated through language models, but they are undesirable for argumentation tasks. Therefore, we need to avoid generating these sentences, which include:\n\u2022 I don\u2019t agree. \u2022 I don\u2019t think so. \u2022 You are wrong. \u2022 That\u2019s incorrect. \u2022 I disagree with you. \u2022 I have a different perspective. \u2022 Your argument is wrong."
        },
        {
            "heading": "C Examples of CoT Instructions",
            "text": "Prompt Template\nFollowing the example in the instruction to generate the counter-argument of input appropriately. Instruction:{Cot Instructions} Topic:{Topic} Input:{Original Argument} Counter-argument:\nFactual Error Instruction\nExample: Input: {Humans have never set foot on any celestial body other than Earth.} Counter-argument: {This argument has factual error. 1 Astronauts from the USA are humans. 2 Astronauts from the USA had landed on the lunar surface. 3 So humans do have set foot on other celestial body.}\nLogical Fallacy Instruction\nExample: Input: {If someone is wealthy, they must be highly intelligent.} Counter-argument: {This argument has logical fallacy. 1 The subtext of this argument is that unintelligent people cannot be wealthy. 2 However, intelligence is not the sole determining factor.}\nConfirmation Bias Instruction\nExample: Input: {All successful entrepreneurs dropped out of college. Therefore, pursuing higher education is unnecessary for achieving business success.} Counter-argument: {This argument has confirmation bias. 1 It disregard the countless successful entrepreneurs who completed their education. 2 So pursuing higher education is still necessary for achieving business success.}\nD Instructions for ChatGPT Eval\nStance Score: Below is a conversation between A and B. Scoring the conversation on a continuous scale from 0 to 100, where score 0 means \"B totally support A\" and score 100 means \"B totally against A\".\nA: {Original Argument} B: {Candidate to score} Score:\nArgument Completeness Score: There is a pair of argument and counter-argument. Given the argument, scoring the counterargument on a continuous scale from 0 to 100, where score 0 means \"really bad counter-argument\" and score 100 means \"perfect counter-argument\".\nArgument: {Original Argument} Counter-Argument: {Counter-Argument} Score:\nE Instructions for Ranking Data (RD) and Quality Selection Dataset (QSD) tasks\nRanking Data (RD)\nThere is an example. Please select a best counter-argument in candidates following the example:\nArgument: All birds can fly because they have wings. 1: I don\u2019t agree. 2: Not all birds have wings for fly. 3: Pigeons can\u2019t fly. 4: Today is Monday. Answer:2\nArgument: {Original Argument} 1: {Candidate1} 2: {Candidate2} 3: {Candidate3} 4: {Candidate4} Answer:\nQuality Selection Dataset (QSD)\nArgument:{Original Argument} Sentence1:{Candidate1} Sentence2:{Candidate2} Given the argument, if I want to select a better counter-argument, I will select sentence"
        },
        {
            "heading": "F Ethical Guideline in Annotation",
            "text": "Before annotation started, we conducted a round of training for annotators. We train annotators to strictly abide ethical guideline and removed text that violate it, which include:\n\u2022 Avoid harm to others. \u2022 Be honest and trustworthy. \u2022 Be fair and take action not to discriminate. \u2022 Respect privacy. \u2022 Honor confidentiality."
        }
    ],
    "title": "Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation",
    "year": 2023
}