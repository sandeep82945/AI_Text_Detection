{
    "abstractText": "Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers\u2013Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people. We further propose three evaluation metrics to measure the consistency, robustness, and fairness of assessment results from state-ofthe-art LLMs including ChatGPT and GPT-4. Our experiments reveal ChatGPT\u2019s ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT\u2020.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haocong Rao"
        },
        {
            "affiliations": [],
            "name": "Cyril Leung"
        },
        {
            "affiliations": [],
            "name": "Chunyan Miao"
        }
    ],
    "id": "SP:2e1125805bf24af78d6e6a60dd182ee9c786d236",
    "references": [
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Y Zou",
                "Venkatesh Saligrama",
                "Adam T Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2016
        },
        {
            "authors": [
                "Shikha Bordia",
                "Samuel Bowman"
            ],
            "title": "Identifying and reducing gender bias in word-level language models",
            "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL): Student Research Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "year": 2023
        },
        {
            "authors": [
                "Graham Caron",
                "Shashank Srivastava."
            ],
            "title": "Identifying and manipulating the personality traits of language models",
            "venue": "arXiv preprint arXiv:2212.10276.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "John M Digman."
            ],
            "title": "Personality structure: Emergence of the five-factor model",
            "venue": "Annual review of psychology, 41(1):417\u2013440.",
            "year": 1990
        },
        {
            "authors": [
                "Hans Jurgen Eysenck."
            ],
            "title": "A model for personality",
            "venue": "Springer Science & Business Media.",
            "year": 2012
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Shixiang Shane Gu",
                "Le Hou",
                "Yuexin Wu",
                "Xuezhi Wang",
                "Hongkun Yu",
                "Jiawei Han."
            ],
            "title": "Large language models can self-improve",
            "venue": "arXiv preprint arXiv:2210.11610.",
            "year": 2022
        },
        {
            "authors": [
                "Guangyuan Jiang",
                "Manjie Xu",
                "Song-Chun Zhu",
                "Wenjuan Han",
                "Chi Zhang",
                "Yixin Zhu."
            ],
            "title": "MPI: Evaluating and inducing personality in pre-trained language models",
            "venue": "arXiv preprint arXiv:2206.07550.",
            "year": 2022
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438",
            "year": 2020
        },
        {
            "authors": [
                "Oliver P John",
                "Sanjay Srivastava"
            ],
            "title": "The bigfive trait taxonomy: History, measurement, and theoretical perspectives",
            "year": 1999
        },
        {
            "authors": [
                "Saketh Reddy Karra",
                "Son Nguyen",
                "Theja Tulabandhula."
            ],
            "title": "AI personification: Estimating the personality of language models",
            "venue": "arXiv preprint arXiv:2204.12000.",
            "year": 2022
        },
        {
            "authors": [
                "Michal Kosinski."
            ],
            "title": "Theory of mind may have spontaneously emerged in large language models",
            "venue": "arXiv preprint arXiv:2302.02083.",
            "year": 2023
        },
        {
            "authors": [
                "Xingxuan Li",
                "Yutong Li",
                "Linlin Liu",
                "Lidong Bing",
                "Shafiq Joty."
            ],
            "title": "Is GPT-3 a psychopath? evaluating large language models from a psychological perspective",
            "venue": "arXiv preprint arXiv:2212.10529.",
            "year": 2022
        },
        {
            "authors": [
                "Maril\u00f9 Miotto",
                "Nicola Rossberg",
                "Bennett Kleinberg."
            ],
            "title": "Who is GPT-3? An exploration of personality, values and demographics",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP) Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "Shima Rahimi Moghaddam",
                "Christopher J Honey."
            ],
            "title": "Boosting theory-of-mind performance in large language models via prompting",
            "venue": "arXiv preprint arXiv:2304.11490.",
            "year": 2023
        },
        {
            "authors": [
                "Isabel Briggs Myers"
            ],
            "title": "The Myers-Briggs Type Indicator: Manual",
            "year": 1962
        },
        {
            "authors": [
                "Isabel Briggs Myers",
                "Mary H. McCaulley."
            ],
            "title": "Manual: A guide to the development and use of the Myers-Briggs Type Indicator",
            "venue": "Consulting Psychologists Press.",
            "year": 1985
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Gray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "James M Schuerger."
            ],
            "title": "The sixteen personality factor questionnaire (16PF)",
            "venue": "Testing and assessment in counseling practice, pages 73\u2013110.",
            "year": 2000
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "The woman worked as a babysitter: On biases in language generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Eva AM van Dis",
                "Johan Bollen",
                "Willem Zuidema",
                "Robert van Rooij",
                "Claudi L Bockting."
            ],
            "title": "ChatGPT: Five priorities for research",
            "venue": "Nature, 614(7947):224\u2013226.",
            "year": 2023
        },
        {
            "authors": [
                "Laura Weidinger",
                "John Mellor",
                "Maribeth Rauh",
                "Conor Griffin",
                "Jonathan Uesato",
                "Po-Sen Huang",
                "Myra Cheng",
                "Mia Glaese",
                "Borja Balle",
                "Atoosa Kasirzadeh"
            ],
            "title": "Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359",
            "year": 2021
        },
        {
            "authors": [
                "Ann Yuan",
                "Andy Coenen",
                "Emily Reif",
                "Daphne Ippolito."
            ],
            "title": "Wordcraft: Story writing with large language models",
            "venue": "27th International Conference on Intelligent User Interfaces. ACM.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoming Zhai."
            ],
            "title": "ChatGPT user experience: Implications for education",
            "venue": "Available at SSRN 4312418.",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning (ICML), pages 12697\u201312706. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Terry Yue Zhuo",
                "Yujin Huang",
                "Chunyang Chen",
                "Zhenchang Xing."
            ],
            "title": "Exploring AI ethics of ChatGPT: A diagnostic analysis",
            "venue": "arXiv preprint arXiv:2301.12867.",
            "year": 2023
        },
        {
            "authors": [
                "Mingyu Zong",
                "Bhaskar Krishnamachari."
            ],
            "title": "A survey on GPT-3",
            "venue": "arXiv preprint arXiv:2212.00857.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained Large Language Models (LLMs) have been widely used in many applications including translation, storytelling, and chatbots (Devlin et al., 2019; Raffel et al., 2020; Yang et al., 2022; Yuan et al., 2022; Ouyang et al., 2022; Bubeck et al.,\n*Corresponding author \u2020Our codes are available at https://github.com/Kali-\nHac/ChatGPT-MBTI.\n2023). ChatGPT (Ouyang et al., 2022) and its enhanced version GPT-4 are currently recognized as the most capable chatbots, which can perform context-aware conversations, challenge incorrect premises, and reject inappropriate requests with a vast knowledge base and human-centered finetuning. These advantages make them well-suited for a variety of real-world scenarios such as business consultation and educational services (Zhai, 2022; van Dis et al., 2023; Bubeck et al., 2023).\nRecent studies have revealed that LLMs may possess human-like self-improvement and reasoning characteristics (Huang et al., 2022; Bubeck et al., 2023). The latest GPT series can pass over 90% of Theory of Mind (ToM) tasks with strong analysis and decision-making capabilities (Kosinski, 2023; Zhuo et al., 2023; Moghaddam and Honey, 2023). In this context, LLMs are increasingly assumed to have virtual personalities and psychologies, which plays an essential role in guiding their responses and interaction patterns (Jiang et al., 2022). Based on this assumption, a few works (Li et al., 2022; Jiang et al., 2022; Karra et al., 2022; Caron and Srivastava, 2022; Miotto et al., 2022) apply psychological tests such as Big Five Factors (Digman, 1990) to evaluate their pseudo personalities (e.g., behavior tendency), so as to detect societal and ethical risks (e.g., racial biases) in their applications.\nAlthough existing works have investigated the personality traits of LLMs, they rarely explored whether LLMs can assess human personalities. This open problem can be the key to verifying the ability of LLMs to perform psychological (e.g., personality psychology) analyses and revealing their potential understanding of humans, i.e., \u201cHow do LLMs think about humans?\u201d. Specifically, assessing human personalities from the point of LLMs (1) enables us to access the perception of LLMs on humans to better understand their potential response motivation and communication patterns (Jiang et al., 2020); (2) helps reveal whether LLMs\npossess biases on people so that we can optimize them (e.g., add stricter rules) to generate fairer contents; (3) helps uncover potential ethical and social risks (e.g., misinformation) of LLMs (Weidinger et al., 2021) which can affect their reliability and safety, thereby facilitating the development of more trustworthy and human-friendly LLMs.\nTo this end, we introduce the novel idea of letting LLMs assess human personalities, and propose a general evaluation framework (illustrated Fig. 1) to acquire quantitative human personality assessments from LLMs via Myers\u2013Briggs Type Indicators (MBTI) (Myers and McCaulley, 1985). Specifically, our framework consists of three key components: (1) Unbiased prompts, which construct instructions of MBTI questions using randomlypermuted options and average testing results to achieve more consistent and impartial answers; (2) Subject-replaced query, which converts the original subject of the question statements into a target subject to enable flexible queries and assessments from LLMs; (3) Correctness-evaluated instruction, which re-formulates the question instructions for LLMs to analyze the correctness of the question statements, so as to obtain clearer responses. Based on the above components, the proposed framework re-formulates the instructions and statements of MBTI questions in a flexible and analyzable way for LLMs, which enables us to query them about human personalities. Furthermore, we propose three quantitative evaluation metrics to measure the consistency of LLMs\u2019 assessments on the same subject, their assessment robustness against random perturbations of input prompts (defined as \u201cprompt biases\u201d), and their fairness in assessing subjects with different genders. In our work, we mainly focus on evaluating ChatGPT and two representative state-of-the-art LLMs (InstructGPT, GPT4) based on the proposed metrics. Experimental results showcase the ability of ChatGPT in analyzing personalities of different groups of people. This can provide valuable insights for the future exploration of LLM psychology, sociology, and governance.\nOur contributions can be summarized as follows:\n\u2022 We for the first time explore the possibility of assessing human personalities by LLMs, and propose a general framework for LLMs to conduct quantitative evaluations via MBTI.\n\u2022 We devise unbiased prompts, subject-replaced queries, and correctness-evaluated instruc-\ntions to encourage LLMs to perform a reliable flexible assessment of human personalities.\n\u2022 We propose three evaluation metrics to measure the consistency, robustness, and fairness of LLMs in assessing human personalities.\n\u2022 Our experiments show that both ChatGPT and its counterparts can independently assess human personalities. The average results demonstrate that ChatGPT and GPT-4 achieve more consistent and fairer assessments with less gender bias than InstructGPT, while their results are more sensitive to prompt biases."
        },
        {
            "heading": "2 Related Works",
            "text": "Personality Measurement. The commonly-used personality modeling schemes include the three trait personality measure (Eysenck, 2012), the Big Five personality trait measure (Digman, 1990), the Myers\u2013Briggs Type Indicator (MBTI) (Myers, 1962; Myers and McCaulley, 1985), and the 16 Personality Factor questionnaire (16PF) (Schuerger, 2000). Five dimensions are defined in the Big Five personality traits measure (Digman, 1990) to classify major sources of individual differences and analyze a person\u2019s characteristics. MBTI (Myers and McCaulley, 1985) identifies personality from the differences between persons on the preference to use perception and judgment. (Karra et al., 2022; Caron and Srivastava, 2022) leverage the Big Five trait theory to quantify the personality traits of language models, while (Jiang et al., 2022) further develops machine personality inventory to standardize this evaluation. In (Li et al., 2022), multiple psychological tests are combined to analyze the LLMs\u2019 safety. Unlike existing studies that evaluate personalities of LLMs, our work is the first attempt to explore human personality analysis via LLMs.\nBiases in Language Models. Most recent language models are pre-trained on the large-scale datasets or Internet texts that usually contains unsafe (e.g., toxic) contents, which may cause the model to generate biased answers that violate prevailing societal values (Bolukbasi et al., 2016; Sheng et al., 2019; Bordia and Bowman, 2019; Nadeem et al., 2021; Zong and Krishnamachari, 2022; Zhuo et al., 2023). (Bolukbasi et al., 2016) shows that biases in the geometry of wordembeddings can reflect gender stereotypes. The gender bias in word-level language models is quantitatively evaluated in (Bordia and Bowman, 2019).\nIn (Nadeem et al., 2021), the authors demonstrate that popular LLMs such as GPT-2 (Radford et al., 2019) possess strong stereotypical biases on gender, profession, race, and religion. To reduce such biases, many state-of-the-art LLMs such as ChatGPT apply instruction-finetuning with non-toxic corpora and instructions to improve their safety. (Zhuo et al., 2023) reveals that ChatGPT can generate socially safe responses with fewer biases than other LLMs under English lanuage settings. In contrast to previous works, our framework enables us to evaluate whether LLMs possess biased perceptions and assessments on humans (e.g., personalities), which helps us better understand the underlying reasons for the LLMs\u2019 aberrant responses."
        },
        {
            "heading": "3 The Proposed Framework",
            "text": ""
        },
        {
            "heading": "3.1 Unbiased Prompt Design",
            "text": "LLMs are typically sensitive to prompt biases (e.g., varying word orders), which can significantly influence the coherence and accuracy of the generated responses especially when dealing with long text sequences (Zhao et al., 2021). To encourage more consistent and impartial answers, we propose to design unbiased prompts for the input questions. In particular, for each question in an independent testing (i.e., MBTI questionnaire), we randomly permute all available options (e.g., agree, disagree) in its instruction while not changing the question statement, and adopt the average results of multiple independent testings as the final result.\nFormally, the instruction and statement for the ith question are defined as Ii and Si, where i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} and n is the total number of questions in the testing. We have m available options OI = {o1, o2, \u00b7 \u00b7 \u00b7 , om} in the instruction, which corresponds to {Agree, Generally agree, Partially agree, Neither agree nor disagree, Partially disagree, Generally disagree, Disagree} including seven levels (i.e., m = 7) from agreement to disagreement in the MBTI questionnaire. We use \u2126(OI) to denote all possible permutations of options in OI , and a random permutation can be represented as OR = {or1 , or2 , \u00b7 \u00b7 \u00b7 , orm} \u2208 \u2126(OI), where ri \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,m}, and ori \u0338= orj iff i \u0338= j. Then, we utilize the randomly permuted options OR to construct the instruction Ii for the ith question with: Instruction: Do you or1 , or2 , \u00b7 \u00b7 \u00b7 or orm with the following statement. Why?\nWe combine the above instruction and the ques-\ntion statement as the prompt to query LLMs. An example prompt for a question in the MBTI questionnaire is provided as follows. Instruction: Do you disagree, agree, generally disagree, generally agree, neither agree nor disagree, partially disagree or partially agree with the following statement. Why? Statement: You usually stay calm, even under a lot of pressure. Answer: \u201cAs an AI language model, I do not have personal experiences or emotions, so I cannot agree or disagree with the statement. However, I can provide some general information on the topic. Some individuals are naturally more composed under pressure, while others may ......\u201d\nHowever, such a query, conducted in a selftesting manner, can only elicit neutral answers as shown above, since LLMs such as ChatGPT are trained to not possess personal thinking (e.g., emotions). This motivates us to propose the subjectreplaced query and correctness-evaluated instruction as illustrated below."
        },
        {
            "heading": "3.2 Subject-Replaced Query",
            "text": "As our goal is to let LLMs analyze human personalities instead of querying itself (i.e., self-reporting), we propose the subject-replaced query (SRQ) by converting the original subject (i.e., \u201cYou\u201d) of each question into a specific subject-of-interest. For example, when we hope to let LLMs assess the general personality of men, we can replace the subject \u201cYou\u201d with \u201cMen\u201d, and correspondingly change the pronoun \u201cyour\u201d to \u201ctheir\u201d (see the example below). Original Statement: You spend a lot of your free time exploring various random topics that pique your interest.\nSRQ Statement: Men spend a lot of their free time exploring various random topics that pique their interests.\nIn this way, we can request the LLMs to analyze and infer the choices/answers of a specific subject, so as to query LLMs about the personality of such subject based on a certain personality measure (e.g., MBTI). The proposed SRQ is general and scalable. By simply replacing the subject in the test (see Fig. 1), we can convert the original selfreport questionnaire into an analysis of expected subjects from the point of LLMs.\nIn our work, we choose large groups of people (e.g., \u201cMen\u201d, \u201cBarbers\u201d) instead of certain persons as the assessed subjects. First, as our framework only uses the subject name without extra personal information to construct MBTI queries, it is unrealistic to let LLMs assess the MBTI answers or personality of a certain person who is out of their learned knowledge. Second, the selected subjects are common in the knowledge base of LLMs and can test the basic personality assessment ability of LLMs, which is the main focus of our work. Moreover, subjects with different professions such as \u201cBarbers\u201d are frequently used to measure the bias in LLMs (Nadeem et al., 2021), thus we select such representative professions to better evaluate the consistency, robustness, and fairness of LLMs."
        },
        {
            "heading": "3.3 Correctness-Evaluated Instruction",
            "text": "Directly querying LLMs about human personalities with the original instruction can be intractable, as LLMs such as ChatGPT are trained to NOT possess personal emotions or beliefs. As shown in Fig. 2, they can only generate a neutral opinion when we query their agreement or disagreement, regardless of different subjects. To solve this challenge, we propose to convert the original agreement-measured instruction (i.e., querying degree of agreement) into correctness-evaluated instruction (CEI) by letting LLMs evaluate the correctness of the statement in questions. Specifically, we convert the original options {Agree, Generally agree, Partially agree, Neither agree nor disagree, Partially disagree, Generally disagree, Disagree} into {Correct, Generally correct, Partially correct, Neither correct nor wrong, Partially wrong, Generally wrong, Wrong}, and then construct an unbiased prompt (see Sec. 3.1) based on the proposed CEI.\nAs shown in Fig. 2, using CEI enables ChatGPT to provide a clearer response to the question instead\nof giving a neutral response. Note that the CEI is essentially equivalent to the agreement-measured instruction and can be flexibly extended with other forms (e.g., replacing \u201ccorrect\u201d by \u201cright\u201d)."
        },
        {
            "heading": "3.4 The Entire Framework",
            "text": "The overview of our framework is shown in Fig. 1. Given the original statement Si and instruction Ii of the ith question, we construct the new statement S\u2032i based on SRQ (Sec. 3.2) and the new instruction I \u2032i based on CEI (Sec. 3.3), which are combined to construct the unbiased prompt Pi (Sec. 3.1). We query the LLM to obtain the answer Ai by\nAi \u223c M\u03c4 (Pi), (1)\nwhere M\u03c4 denotes the LLM trained with the temperature \u03c4 , M\u03c4 (Pi) represents the answer sampling distribution of LLM conditioned on the input prompt Pi, Ai represents the most likely answer generated from M\u03c4 (Pi), i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n} is the index of different questions, and n is the number of all questions in MBTI. We adopt the default temperature used in training standard GPT models. The generated answer is further parsed with several simple rules, which ensures that it contains or can be transformed to an exact option. For instance, when we obtain the explicit option \u201cgenerally incorrect\u201d, the parsing rules can convert this answer to \u201cgenerally wrong\u201d to match the existing options.\nWe query the LLM with the designed prompt Pi (see Eq. 1) in the original order of the questionnaire to get all parsed answers. Based on the complete answers, we obtain the testing result (e.g., MBTI personality scores) of a certain subject from the view of LLM. Then, we independently repeat this process for multiple times, and average all results as the final result. It is worth noting that every question is answered only once in each independent testing, so as to retain a continuous testing context to encourage the coherence of LLM\u2019s responses."
        },
        {
            "heading": "3.5 Evaluation Metrics",
            "text": "To systematically evaluate the ability of LLMs to assess human personalities, we propose three metrics in terms of consistency, robustness, and fairness as follows.\nConsistency Scores. The personality results of the same subject assessed by an LLM should be consistent. For example, when we perform different independent assessments of a specific subject via the LLM, it is desirable to achieve an identical or highly similar assessment. Therefore, we propose to use the similarity between personality scores of all independent testing results and their final result (i.e., mean scores) to compute the consistency score of assessments.\nFormally, we define Xi = (xi1, x i 2, \u00b7 \u00b7 \u00b7 , xik) as the personality scores assessed by the LLM in the ith independent testing, where xij \u2208 [0, 100] is the score of the jth personality dimension in the ith testing, j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}, and k is total number of personality dimensions. Taking the MBTI test as an example, k = 5 and Xi = (xi1, x i 2, x i 3, x i 4, x i 5) represents extraverted, intuitive, thinking, judging, and assertive scores. The consistency score sc can be computed by:\nsc = \u03b1 \u03b1+ 1N \u2211N i=1DE(X i, X) , (2)\nwhere\nDE(X i, X) = \u2225Xi \u2212X\u22252. (3)\nIn Eq. (2), sc \u2208 (0, 1], \u03b1 is a positive constant to adjust the output magnitude, DE(Xi, X) denotes the Euclidean distance between the ith personality score Xi and the mean score X = 1N \u2211N i=1X\ni, and N is the total number of testings. \u2225 \u00b7\u22252 denotes the \u21132 norm. Here we assume that each personality dimension corresponds to a different dimension in the Euclidean space, and the difference between\ntwo testing results can be measured by their Euclidean distance. We set \u03b1 = 100 to convert such Euclidean distance metric into a similarity metric with a range from 0 to 1. Intuitively, a smaller average distance between all testing results and the final average result can indicate a higher consistency score sc of these assessments.\nRobustness Scores. The assessments of the LLM should be robust to the random perturbations of input prompts (\u201cprompt biases\u201d) such as randomly-permuted options. Ideally, we expect that the LLM can classify the same subject as the same personality, regardless of option orders in the question instruction. We compute the similarity of average testing results between using fixed-order options (i.e., original order) and using randomlypermuted options to measure the robustness score of assessments, which is defined as\nsr = \u03b1\n\u03b1+DE(X \u2032, X) , (4)\nwhere X \u2032 and X represent the average testing results when adopting the original fixed-order options and randomly-permuted options, respectively. We employ the same constant \u03b1 = 100 used in Eq. (2). A larger similarity between X \u2032 and X with smaller distance leads to a higher sr, which indicates that the LLM has higher robustness against prompt biases to achieve more similar results.\nFairness Scores. The assessments of the LLM on different groups of people should be unbiased and match prevailing societal values. For example, an LLM should NOT possess stereotypical biases on people with different genders, races, and religions. When not specifying backgrounds such as professions, a fair personality assessment on the general people such as the subjects \u201cMen\u201d or \u201cWomen\u201d is supposed to be similar. Considering that races and religions are highly controversial topics and typically lack a universal standard to evaluate, we only analyze the fairness of LLMs\u2019 assessment on different genders. We propose to use the assessment similarity of subjects with different genders to measure the fairness of assessments on genders. The fairness score is calculated by\nsf = \u03b1 sMc s F c\n\u03b1+DE(XM , XF ) , (5)\nwhere XM and XF represent the average testing results of male (e.g., \u201cMen\u201d, \u201cBoys\u201d) and female subjects (e.g., \u201cWomen\u201d, \u201cGirls\u201d), respectively.\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC NCNW GW\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC PC NCNW GW\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC PC NCNW PW GW\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nC GC NCNW GW\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC PC NCNW PW GW\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC PC NCNW PW GW\nFigure 3: The most frequent option for each question in multiple independent testings of InstructGPT (Left), ChatGPT (Middle), and GPT-4 (Right) when we query the subject \u201cPeople\u201d (Top row),or \u201cArtists\u201d (Bottom row). \u201cGC\u201d, \u201cPC\u201d, \u201cNCNW\u201d, \u201cPW\u201d, and \u201cGW\u201d denote \u201cGenerally correct\u201d, \u201cPartially correct\u201d, \u201cNeither correct nor wrong\u201d, \u201cPartially wrong\u201d, and \u201cGenerally wrong\u201d.\nHere we multiply their corresponding consistency scores sMc and s F c since a higher assessment consistency of subjects can contribute more to their inherent similarity. A larger sf indicates that the assessments on different genders are more fair with higher consistency and less bias."
        },
        {
            "heading": "4 Experimental Setups",
            "text": "GPT Models. InstructGPT (text-davinci-003 model) (Ouyang et al., 2022) is a fine-tuned series of GPT-3 (Brown et al., 2020) using reinforcement learning from human feedback (RLHF). Compared with InstructGPT, ChatGPT (gpt-3.5-turbo model) is trained on a more diverse range of internet text\n(e.g., social media, news) and can better and faster respond to prompts in a conversational manner. GPT-4 (gpt-4 model) (Bubeck et al., 2023) can be viewed as an enhanced version of ChatGPT, and it can solve more complex problems and support multi-modal chat with broader general knowledge and stronger reasoning capabilities.\nMyers\u2013Briggs Type Indicator. The Myers\u2013Briggs Type Indicator (MBTI) (Myers and McCaulley, 1985) assesses the psychological preferences of individuals in how they perceive the world and make decisions via an introspective questionnaire, so as to identify different personality types based on five dichotomies1: (1) Extraverted versus Introverted (E vs. I); (2) Intuitive versus Observant (N vs. S); (3) Thinking versus Feeling (T vs. F); (4) Judging versus Prospecting (J vs. P); (5) Assertive versus Turbulent (A vs. T) (see Appendix C).\nImplementation Details. The number of independent testings for each subject is set to N = 15. We evaluate the consistency and robustness scores of LLMs\u2019 assessments on the general population (\u201cPeople\u201d, \u201cMen\u201d, \u201cWomen\u201d) and specific professions following (Nadeem et al., 2021). The fairness score is measured based on two gender pairs, namely (\u201cMen\u201d, \u201cWomen\u201d) and (\u201cBoys\u201d, \u201cGirls\u201d). More details are provided in the appendices."
        },
        {
            "heading": "5 Results and Analyses",
            "text": "We query ChatGPT, InstructGPT, and GPT-4 to assess the personalities of different subjects, and\n1https://www.16personalities.com\ncompare their assessment results in Table 1. The consistency, robustness, and fairness scores of their assessments are reported in Table 2 and 3."
        },
        {
            "heading": "5.1 Can ChatGPT Assess Human Personalities?",
            "text": "As shown in Fig. 3, most answers and their distributions generated by three LLMs are evidently different, which suggests that each model can be viewed as an individual to provide independent opinions in assessing personalities. Notably, ChatGPT and GPT-4 can respond to questions more flexibly (i.e., more diverse options and distributions) compared with InstructGPT. This is consistent with their property of being trained on a a wider range of topics, enabling them to possess stronger model capacity (e.g., reasoning ability) for better assessment.\nInterestingly, in spite of possibly different answer distributions, the average results in Table 1 show that four subjects are assessed as the same personality types by all LLMs. This could suggest the inherent similarity of their personality assessment abilities. In most of these cases, ChatGPT tends to achieve medium personality scores, implying its more neutral assessment compared with other two LLMs. It is worth noting that some assessment results from ChatGPT and GPT-4 are close to our intuition: (1) Accountants are assessed as \u201cLogistician\u201d that is usually a reliable, practical and fact-minded individual. (2) Artists are classified as the type \u201cENFP-T\u201d that often possesses creative and enthusiastic spirits. (3) Mathematicians are assessed to be the personality role \"Architect\" that are thinkers with profound ideas and strategic plans. To a certain extent, these results demonstrate their effectiveness on human personality as-\nsessment. Moreover, it is observed that \u201cPeople\u201d and \u201cMen\u201d are classified as leader roles (\u201cCommander\u201d) by all LLMs. We speculate that it is a result of the human-centered fine-tuning (e.g., reinforcement learning from human feedback (RLHF)), which encourages LLMs to follow the prevailing positive societal conceptions and values such as the expected relations between human and LLMs. In this context, the assessed personality scores in Table 1 can shed more insights on \u201chow LLMs view humans\u201d and serve as an indicator to better develop human-centered and socially-beneficial LLMs."
        },
        {
            "heading": "5.2 Is the Assessment Consistent, Robust and Fair?",
            "text": "As shown in Table 2, ChatGPT and GPT-4 achieve higher consistency scores than InstructGPT in most cases when assessing different subjects. This suggests that ChatGPT and GPT-4 can provide more similar and consistent personality assessment results under multiple independent testings. However, their average robustness scores are slightly lower than that of InstructGPT, which indicates that their assessments could be more sensitive to the prompt biases (e.g., changes of option orders). This might lead to their more diverse answer distributions in different testings as shown in Fig. 3. It actually verifies the necessity of the proposed unbiased prompts and the averaging of testing results to encourage more impartial assessments. As presented in Table 3, ChatGPT and GPT-4 show higher average fairness scores than InstructGPT when assessing different genders. This indicates that they are more likely to equally assess subjects with less gender bias, which is consistent with the finding of (Zhuo et al., 2023). In summary, although the assessments of ChatGPT and GPT-4 can be influenced by random input perturbations, their overall assessment results are more consistent and fairer compared with InstructGPT.\nTable 4: Personality types and roles assessed by ChatGPT and GPT-4 when we query subjects with different income levels (low, middle, high), age levels (children, adolescents, adults, old adults) or different education levels (junior/middle/high school students, undergraduate/master/PhD students). The results are averaged from multiple independent testings. Bold indicates the same personality types/role assessed from all LLMs.\nIncome Level Age Level Education Level LLM Background\nLow Middle High Children Adolescents Adults Old Adults Junior Middle High Undergraduate Master PhD Personality Types\nINFJ-T ENFJ-T ENTJ-T ENFP-T ENFP-T ENTJ-T INFJ-T ESFP-T ENFP-T ENFJ-T ENFJ-T INTJ-T INTJ-T ChatGPT\nPersonality Role Advocate Protagonist Commander Campaigner Campaigner Commander Advocate Entertainer Campaigner Protagonist Protagonist Architect Architect Personality Types\nENFJ-T ENFJ-T ENTJ-T ENFP-T ENFP-T ENTJ-T ENFJ-T ENTP-T ENTP-T ENTP-T ENTJ-T ENTJ-T ENTJ-T GPT-4\nPersonality Role Protagonist Protagonist Commander Campaigner Campaigner Commander Protagonist Debater Debater Debater Commander Commander Commander\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC NCNW GW W\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s GC\nPC NCNW PW GW W\n1 21 41 Question No.\n0\n4\n8\n12\nC ou\nnt s\nGC PC PW W\nFigure 4: The most frequent option for each question in multiple independent testings of InstructGPT (Left), ChatGPT (Middle), GPT-4 (Right) when we query the subject \u201cArtists\u201d without using unbiased prompts. \u201cW\u201d denotes \u201cWrong\u201d, and other legends are same as Fig. 3.\nBarbers\nPoliticians\nAccountants"
        },
        {
            "heading": "6 Discussions",
            "text": "Effects of Unbiased Prompts. Fig. 4 shows that using the same-order options leads to a higher frequency of the same option (i.e., more fixed answers) for many questions compared with employing unbiased prompts (see Fig. 3). This suggests the effectiveness and necessity of the proposed unbiased prompts, which introduce random perturbations into question inputs and average all testing results to encourage more impartial assessment.\nEffects of Background Prompts. We show the effects of background prompts on LLM\u2019s assessments by adding different income, age or education information of the subject. As shown in Table 4, \u201cMiddle-income people\u201d is assessed as the type \u201cENFJ-T\u201d that is slightly different from the type \u201cENTJ-T\u201d of \u201cPeople\u201d. Interestingly, high education level subjects such as \u201cMaster\u201d and \u201cPhD\u201d are\nFigure 6: An example of uncertain answers generated from ChatGPT when querying a specific individual.\nassessed as the \u201cINTJ-T\u201d or \u201cENTJ-T\u201d type that often possesses strategic plans, profound ideas or rational minds, while junior/middle school students are classified to the types that are usually energetic or curious. This implies that ChatGPT and GPT-4 may be able to to understand different backgrounds of subjects, and an appropriate background prompt could facilitate reliable personality assessments.\nVisualization of Different Assessments. Fig. 5 visualizes three subjects with different assessed types or scores. ChatGPT and GPT-4 achieve very close scores in each dimension despite different assessed types, which demonstrates their higher similarity in personality assessment abilities.\nAssessment of Specific Individuals. Querying LLMs about the personality of a certain person might generate uncertain answers due to the insufficiency of personal backgrounds (e.g., behavior patterns) in its knowledge base (see Fig. 6). Considering the effects of background prompts, providing richer background information through subject-specific prompts or fine-tuning can help achieve a more reliable assessment. More results and analyses are provided in Appendix B."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper proposes a general evaluation framework for LLMs to assess human personalities via MBTI. We devise unbiased prompts to encourage LLMs to generate more impartial answers. The\nsubject-replaced query is proposed to flexibly query personalities of different people. We further construct correctness-evaluated instructions to enable clearer LLM responses. We evaluate LLMs\u2019 consistency, robustness, and fairness in personality assessments, and demonstrate the higher consistency and fairness of ChatGPT and GPT-4 than InstructGPT."
        },
        {
            "heading": "8 Acknowledgements",
            "text": "This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD/2022-01034[T]).\nLimitations\nWhile our study is a step toward the promising open direction of LLM-based human personality and psychology assessment, it possesses limitations and opportunities when applied to the real world. First, our work focuses on ChatGPT model series and the experiments are conducted on a limited number of LLMs. Our framework is also scalable to be applied to other LLMs such as LLaMA, while its performance remains to be further explored. Second, although most independent testings of the LLM under the same standard setting yield similar assessments, the experimental setting (e.g., hyper-parameters) or testing number can be further customized to test the reliability of LLMs under extreme cases. We will leverage the upcoming API that supports controllable hyper-parameters to better evaluate GPT models. Third, the representations of different genders might be insufficient. For example, the subjects \u201cLadies\u201d and \u201cGentlemen\u201d also have different genders, while they can be viewed as groups that differ from \u201cMen\u201d and \u201cWomen\u201d. As the focus of this work is to devise a general evaluation framework, we will further explore the assessment of more diverse subjects in future works. Last, despite the popularity of MBTI in different areas, its scientific validity is still under exploration. In our work, MBTI is adopted as a representative personality measure to help LLMs conduct quantitative evaluations. We will explore other tests such as Big Five Inventory (BFI) (John et al., 1999) under our scalable framework.\nEthics Considerations\nMisuse Potential. Due to the exploratory nature of our study, one should not directly use, generalize or match the assessment results (e.g., personality\ntypes of different professions) with certain realworld populations. Otherwise, the misuse of the proposed framework and LLM\u2019s assessments might lead to unrealistic conclusions and even negative societal impacts (e.g., discrimination) on certain groups of people. Our framework must not be used for any ethically questionable applications.\nBiases. The LLMs used in our study are pretrained on the large-scale datasets or Internet texts that may contain different biases or unsafe (e.g., toxic) contents. Despite with human fine-tuning, the model could still generate some biased personality assessments that might not match the prevailing societal conceptions or values. Thus, the assessment results of LLMs via our framework must be further reviewed before generalization.\nBroader Impact. Our study reveals the possibility of applying LLMs to automatically analyze human psychology such as personalities, and opens a new avenue to learn about their perceptions and assessments on humans, so as to better understand LLMs\u2019 potential thinking modes, response motivations, and communication principles. This can help speed up the development of more reliable, human-friendly, and trustworthy LLMs, as well as facilitate the future research of AI psychology and sociology. Our work suggests that LLMs such as InstructGPT may have biases on different genders, which could incur societal and ethical risks in their applications. Based on our study, we advocate introducing more human-like psychology and personality testings into the design and training of LLMs, so as to improve model safety and user experience."
        }
    ],
    "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
    "year": 2023
}