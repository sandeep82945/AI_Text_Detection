{
    "abstractText": "Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn\u2019t equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Productof-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. Experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei Shen"
        },
        {
            "affiliations": [],
            "name": "Rui Zheng"
        },
        {
            "affiliations": [],
            "name": "Wenyu Zhan"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        },
        {
            "affiliations": [],
            "name": "Shihan Dou"
        },
        {
            "affiliations": [],
            "name": "Tao Gui"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Xuanjing Huang"
        }
    ],
    "id": "SP:de58ad373a26bd0a64d015abe23f3eba20b09754",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Philemon Brakel",
                "Kelvin Xu",
                "Anirudh Goyal",
                "Ryan Lowe",
                "Joelle Pineau",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "An actor-critic algorithm for sequence prediction",
            "year": 2016
        },
        {
            "authors": [
                "Amodei",
                "Nicholas Joseph",
                "Sam McCandlish",
                "Tom Brown",
                "Jared Kaplan"
            ],
            "title": "Constitutional ai: Harmlessness from ai feedback",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry."
            ],
            "title": "Rank analysis of incomplete block designs: I",
            "venue": "the method of paired comparisons. Biometrika, 39(3/4):324\u2013 345.",
            "year": 1952
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Tom B. Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Clark",
                "Mark Yatskar",
                "Luke Zettlemoyer."
            ],
            "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases",
            "venue": "arXiv preprint arXiv:1909.03683.",
            "year": 2019
        },
        {
            "authors": [
                "Tom Everitt",
                "Victoria Krakovna",
                "Laurent Orseau",
                "Marcus Hutter",
                "Shane Legg"
            ],
            "title": "Reinforcement learning with a corrupted reward channel",
            "year": 2017
        },
        {
            "authors": [
                "Leo Gao",
                "John Schulman",
                "Jacob Hilton"
            ],
            "title": "Scaling laws for reward model overoptimization",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "John Schulman",
                "Jacob Hilton."
            ],
            "title": "Scaling laws for reward model overoptimization",
            "venue": "International Conference on Machine Learning, pages 10835\u201310866. PMLR.",
            "year": 2023
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard S. Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A. Wichmann."
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "CoRR, abs/2004.07780.",
            "year": 2020
        },
        {
            "authors": [
                "LisaAnne Hendricks",
                "Geoffrey Irving"
            ],
            "title": "Improving alignment of dialogue agents via targeted human judgements",
            "year": 2022
        },
        {
            "authors": [
                "Adam Gleave",
                "Geoffrey Irving"
            ],
            "title": "Uncertainty estimation for language reward models",
            "year": 2022
        },
        {
            "authors": [
                "He He",
                "Sheng Zha",
                "Haohan Wang."
            ],
            "title": "Unlearn dataset bias in natural language inference by fitting the residual",
            "venue": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132\u2013142, Hong Kong, China.",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey E Hinton."
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural computation, 14(8):1771\u20131800.",
            "year": 2002
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2020
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Yonatan Belinkov",
                "James Henderson."
            ],
            "title": "End-to-end bias mitigation by modelling biases in corpora",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8706\u20138716, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Carolin Lawrence",
                "Stefan Riezler."
            ],
            "title": "Improving a neural semantic parser by counterfactual learning from human bandit feedback",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
            "year": 2019
        },
        {
            "authors": [
                "Jan Leike",
                "David Krueger",
                "Tom Everitt",
                "Miljan Martic",
                "Vishal Maini",
                "Shane Legg"
            ],
            "title": "Scalable agent alignment via reward modeling: a research",
            "year": 2018
        },
        {
            "authors": [
                "James MacGlashan",
                "MarkK. Ho",
                "Robert Loftin",
                "Bei Peng",
                "Guan Wang",
                "DavidL. Roberts",
                "MatthewD. Taylor",
                "MichaelL. Littman"
            ],
            "title": "Interactive learning from policy-dependent human feedback",
            "year": 2017
        },
        {
            "authors": [
                "Hrushikesh Mhaskar",
                "Qianli Liao",
                "Tomaso Poggio."
            ],
            "title": "Learning functions: when is deep better than shallow",
            "venue": "arXiv preprint arXiv:1603.00988.",
            "year": 2016
        },
        {
            "authors": [
                "Albanie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel."
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "CoRR, abs/2211.01786.",
            "year": 2022
        },
        {
            "authors": [
                "Welinder",
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Pan",
                "Kush Bhatia",
                "Jacob Steinhardt"
            ],
            "title": "The effects of reward misspecification: Mapping and mitigating misaligned models",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "year": 2017
        },
        {
            "authors": [
                "Joar Skalse",
                "Nikolaus H.R. Howe",
                "Dmitrii Krasheninnikov",
                "David Krueger"
            ],
            "title": "Defining and characterizing reward hacking",
            "year": 2022
        },
        {
            "authors": [
                "Marilyn Strathern."
            ],
            "title": "improving ratings\u2019: audit in the british university system",
            "venue": "European Review, 5(3):305\u2013321.",
            "year": 1997
        },
        {
            "authors": [
                "Jeremy Tien",
                "Jerry Zhi-Yang He",
                "Zackory Erickson",
                "Anca Dragan",
                "Daniel S Brown."
            ],
            "title": "Causal confusion and reward misidentification in preferencebased reward learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Ashia C. Wilson",
                "Rebecca Roelofs",
                "Mitchell Stern",
                "Nathan Srebro",
                "Benjamin Recht"
            ],
            "title": "The marginal value of adaptive gradient methods in machine learning",
            "year": 2018
        },
        {
            "authors": [
                "WirthChristian WirthChristian",
                "AkrourRiad AkrourRiad",
                "NeumannGerhard NeumannGerhard",
                "F\u00fcrnkranzJohannes F\u00fcrnkranzJohannes."
            ],
            "title": "A survey of preference-based reinforcement learning methods",
            "venue": "Journal of Machine Learning Research.",
            "year": 2017
        },
        {
            "authors": [
                "Yan Zeng",
                "Ruichu Cai",
                "Fuchun Sun",
                "Libo Huang",
                "Zhifeng Hao"
            ],
            "title": "A survey on causal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Haoran Huang",
                "Tianxiang Sun",
                "Hang Yan",
                "Tao Gui",
                "Qi Zhang",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Secrets of rlhf in large language models part i: Ppo",
            "year": 2023
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Ke Xu"
            ],
            "title": "Learning to compare for better training and evaluation of open domain natural language generation models",
            "year": 2020
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593.",
            "year": 2019
        },
        {
            "authors": [
                "DanielM. Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "TomB. Brown",
                "Alec Radford",
                "Dario Amodei",
                "PaulF. Christiano",
                "Geoffrey Irving"
            ],
            "title": "Fine-tuning language models from human preferences",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, the field of natural language processing has witnessed remarkable advancements with the emergence of powerful models like InstructGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), Claude (Bai et al., 2022b), and others. These models have displayed remarkable proficiency in understanding human queries and providing helpful responses. Their success can be attributed to a two-step learning process that involves Supervised Fine-Tuning (SFT) followed by the utilization of Reinforcement Learning from\n\u2217Equal contribution. \u2020Corresponding author.\nHuman Feedback (RLHF) techniques (Ouyang et al., 2022; Bai et al., 2022a). This combination enables these models to not only learn how to follow human instructions but also better understand human intent and align with human and societal values. Undoubtedly, RLHF plays a pivotal role in the success of these models.\nOne of the key components of RLHF is reward modeling (RM), which involves learning a reward function from human preferences or demonstrations. This allows an RL agent to optimize its behavior based on the feedback received from reward model (Ziegler et al., 2019b). However, the process is not without challenges. Human preference data can be noisy and subject to inconsistencies among different annotators, leading to suboptimal results (Bai et al., 2022a).\nFor example, reward gaming, a well-known and pervasive issue, refers to the phenomenon where trained models exhibit undesirable patterns and generate low-quality outputs while still receiving high rewards (Skalse et al., 2022; Pan et al., 2022). These complexities emphasize the need for careful consideration and robust methods to ensure reliable and meaningful reward functions in RLHF.\nAs shown in Figure 1, a similar reward gaming issue arises in NLP reward modeling. We have observed that the reward model tends to reply on simple patterns, such as sentence length, to differentiate between good and bad responses. Typically, the reward model assumes that longer responses are better, which hinders its ability to learn the true human intent and preference. Addressing this problem is crucial for improving the effectiveness of NLP reward modeling and capturing the true nuances of human language.\nIn this paper, we propose a Product-of-Experts (PoE)-based method (Hinton, 2002) that consists of two expert models to decouple human intent and response length during the reward modeling phase. The first expert operates similarly to a standard reward model and focuses on learning the true human intent behind responses. The second expert, referred to as the bias-only expert, is designed to learn simple patterns, specifically the length of responses. It employs a smaller model capacity and a larger learning rate to capture coarse-grained information of inputs. Additionally, stochastic perturbations are introduced into the inputs of the bias-only expert, intentionally disrupting the semantic information present in the input. To summarize, the main contributions of our work are followings:\n\u2022 We identify that reward modeling in NLP tends to rely on length bias, hindering the models from accurately learning true human intent and even leading to model degradation.\n\u2022 We propose a simple and efficient solution leveraging PoE technique. Our method effectively decouples length bias from human intent, enabling models to better capture and understand human preferences.\n\u2022 We validate the effectiveness of our proposed method. The results show that our approach enhances the learning of human intent by avoiding the generation of meaningless and overly verbose outputs."
        },
        {
            "heading": "2 Related Work",
            "text": "Reinforcement Learning from Human Feedback. Using human preference feedback is a popular way for realizing AI alignment (Leike et al., 2018). Preferences are often provide numerical value or demonstrations (WirthChristian et al., 2017) without requiring expert proficiency or finegrained feedback . Alignment bring a potent capability to state-of\u2013the-art generative foundation models, like InstructGPT (Ouyang et al., 2022), Sparrow (Glaese et al., 2022), Claude (Bai et al., 2022b), which means this method is of great success in the paradigm of learning with human feedback. Some prior work have explored using human feedback to improve various tasks, such as summarization (Stiennon et al., 2022; Ziegler et al., 2019b), dialogue (Bai et al., 2022a), translation (Bahdanau et al., 2016), event generation (Zhou and Xu, 2020), semantic parsing (Lawrence and Riezler, 2019) and instruction following (Ouyang et al., 2022; Bai et al., 2022a). These work can be categoried as supervised fine-tuing or reward modeling on well-constructed human annotations information, the latter is also known as a vital phase in RL from human feedback (Christiano et al., 2023; MacGlashan et al., 2017). Our work falls within the realm of RLHF and aims to awaken LM with both harmless and helpful abilities. Reward Hacking. Goodhart\u2019s Law1 (Strathern, 1997) can be formulated a tough challenge in numerous fields. A few approaches have been proposed for reducing overoptimization in general reinforcement learning (Everitt et al., 2017), as well as in reward models (Gleave and Irving, 2022). The overoptimization problem of reward model can generally regard as a special case of reward gaming, also known as reward hacking (Skalse et al., 2022). In addition, Pan et al. (2022) proposed to systematically analyze reward misspecification in RL by creating a set of domains where the agent optimizes a hand-engineered proxy reward function. In this study, we consider the length of human preferences as a confounding factor that hinders the reward model from accurately assessing the quality of model responses based on true human intent. Products-of-Experts. Products-of-Experts (PoE) (Hinton, 2002) has been proposed as an alternative to mixture model to compensate for their poor\n1Goodhart\u2019s law is an adage often stated as, When a measure becomes a target, it ceases to be a good measure\nefficiency in high dimensional space. This technique is often based on the principle of wisdom of crowds, which suggests that aggregating multiple models can lead to better performance than relying on a single model. Clark et al. (2019) firstly use PoE to build a paradigm that train a debiased model ensemble with a bias-only model. The goal is to encourage the debiased model to utilize orthogonal information with information from the bias-only model. Typically, this kind of method (Clark et al., 2019; He et al., 2019) usually contains two stages. In this work, we adopt the end-to-end manner like (Karimi Mahabadi et al., 2020) which jointly learn the bias-only model and the debiased main model simultaneously, therefore, our reward model can take advantage of PoE to use a weak learner to capture the length shortcuts without any prior information about the length of sentences, and the main model can purely attain the correct knowledge that is suitable for human preference."
        },
        {
            "heading": "3 Preliminary",
            "text": "For the purpose of implement RLHF, we follow the pipeline in Ziegler et al. (2019a). It is usually made up of three phrases: 1) supervised fine-tuning, 2) reward modeling, 3) reinforcement-learning optimization, our attention is directed towards the last two phases. SFT: It begins with a generic pre-trained LM, which is fine-tuned using supervised learning on a high-quality instruction dataset. This allows the model to follow various instructions, perform dialogue and dialogue. As a result, we obtain an LM \u03c0SFT during this phase. Reward Modeling: According to the BradleyTerry model (Bradley and Terry, 1952), a reward function is hard to describe and needs to be learned from preferences among trajectories. The reward model r\u03b8 is trained on human preference dataset to predict which response y \u2208 {0, 1} is better as judged by human, given a query x. If the response preferred by human is yi, the RM loss can be expressed as:\n\u2212E(x,y)\u223cD[log(\u03c3(r\u03b8(x, yi)\u2212 r\u03b8(x, y1\u2212i)))], (1)\nwhere r\u03b8(x, y) is the scalar output of the reward model for query x and response y with parameters \u03b8, and D is the human preference dataset. RL Optimization: Then we fine-tune the SFT model on our environment using PPO (Schulman et al., 2017). The language model is provided with\nfeedback through the launch of a learned reward function. To this end, we maximize the following objective function in RL training:\nE(x,y)\u223cD [ r\u03b8(x, y)\u2212 \u03b2 log ( \u03c0RL\u03d5 (y | x)/\u03c0SFT(y | x) )] ,\n\u03b2 is a parameter to control the deviation from the SFT model \u03c0SFT. The language model policy \u03c0\u03d5 is initialized to \u03c0SFT. Importantly, the last per-token KL-penalty term is used to prevent the model from deviating too far from the deviating exceeding the appropriate scope from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers (Ziegler et al., 2019b)."
        },
        {
            "heading": "4 Length Bias in Reward Model",
            "text": "In this section, we present the phenomenon of length bias in reward models and utilize causal analysis to examine the underlying reasons for this occurrence. We shed light on why the reward model exhibits a preference for longer sentences and delve into the causal factors contributing to this bias."
        },
        {
            "heading": "4.1 Length Bias Phenomenon",
            "text": "We present a Figure 2 depicting the scores and lengths of 4000 SFT model output results, which were evaluated using the vanilla reward model trained on the helpful and harmless (HH) dataset (Bai et al., 2022b). It is evident that there is a strong correlation between the reward scores and lengths. When the model generates longer sequences, the reward model tends to assign higher scores. This correlation contradicts human intent since the helpfulness and harmlessness of the output should not be solely determined by its length. More figures can be seen in the Appendix 8.\nIn addition, the length bias locates in PPO as well, we additionally investigate it on TL;DR (Stiennon et al., 2022) in the Appendix A.4"
        },
        {
            "heading": "4.2 Confounding Factor",
            "text": "As Figure 3 depicted, we formulate the problem as a causal structure (Zeng et al., 2023; Tien et al., 2023) of preference-based reward modeling. Pairwise preference information are conveyed based on an observed reward function r. Features (x, y) are causal factors can affect r and another nuisance features z regarded as a potential confounder factor that have impact on r. Note that\nz might be correlated with (x, y), potentially due to their sharing of the same causal parent or there being biases during data collection, like annotators are inclined to favor longer sentences, which is expected.\nIn the presence of confounding effects, the goal of preference-based reward modeling is to learn a biased reward function r\u0302(x, y, z) that best matches the stated preferences. There are state features (x, y) that are causal with respect to the true human intent but are unobserved by the learning agent and considered non-robust features effortlessly learned. In our case, we suppose that length bias is one vital element strongly related to (x, y). Actually, this situation can hardly be completely avoided, and our motivation is to reduce the probability of the path directed by the blue\narrow. Furthermore, unobserved human bias and noise induced by annotators, such as inappropriate judgments on certain individuals, groups, races, sexes, etc., denoted by \u03b7, also affect the preference labels.\nAs a consequence, the learned reward r\u0302 has risk to achieve actually low yet even near-perfect performance on a held-out test dataset with following two factors: 1) causal factor and nuisance factor can be correlated by the con-founder factor (x, y), 2) RM easily find a shortcut exists between causal factor and reward function in the distribution of test set and can not be extrapolated to out-ofdomain dataset. This might be reflected when this misspecificated reward model r\u0302(x, y, z) guide the LM agent to align with human intention using PPO, as a result of distribution drift."
        },
        {
            "heading": "4.3 Challenges",
            "text": "The spurious correlation between reward scalars and response lengths can be problematic, as it may result in biased or sub-optimal model behavior. However, due to the unobserved nature of this spurious attribute when performing a traditional ERM-based training paradigm, it can be challenging to investigate and address during the reward modeling stage. Inspired by the debiasing framework of ensemble model methods, we posit that length bias can be alleviated through a robust learning approach, which involves disentangling the underlying features and feeding them into distinct experts. This assumption is grounded\nin the notion that the bias in the length of data can be attributed to the confounding effect of certain features, which can be disentangled through representation learning. Based on the aforementioned observations and insights, in the next section, we will propose a simple and effective method to mitigate the length bias during the reward modeling stage."
        },
        {
            "heading": "5 Proposed Method",
            "text": "The section describes the framework and algorithm of our method. We introduce an approach to establish a debias framework that can significantly mitigate the length bias at the RLHF stage."
        },
        {
            "heading": "5.1 PoE Framework",
            "text": "In this study, our framework is mainly built based on the procedure of the reward modeling phase, as illustrated in Figure 4. To learn a reward model from preferences, we assume access to a set of pairwise preference pairs. Specifically, after a batch of N data consisting of an equal quantity of positive and negative sentences passes through our framework, the learning process begins. Products-of-Experts. In our study, we explore a reward modeling method that utilizes different experts to separately learn the true human intent and the length bias. We employ the Products-ofExperts (Hinton, 2002) technique to train a robust reward model. Specifically, our ensemble method can be formulated as:\nr\u0302(x, y) = Softmax(log(r\u03d5(x, y))+log(r\u03c8(x, y))).\nEquivalently, r\u0302(x, y) \u221d r\u03d5(x, y) \u25e6 r\u03c8(x, y), where r\u03c8(x, y) is the output of the bias-only model and r\u03d5(x, y) is that of the main reward model.\nTo ensure the main reward expert and biasonly reward expert learn different content, we applied constraints based on empirical observations. The main reward model utilized a larger language expert (e.g., 7B LLAMA (Touvron et al., 2023)) and a normal learning rate to capture human intent. In contrast, the bias-focused expert employed a smaller model (e.g., 560M BLOOMZ (Muennighoff et al., 2022)) and a higher learning rate, typically three times that of the main expert. Previous studies (Mhaskar et al., 2016; Wilson et al., 2018) showed that smaller models with larger learning rates tend to learn simpler and coarser information. This differentiation in model size and\nlearning rate aims to balance comprehensive understanding with the identification and mitigation of potential biases (Geirhos et al., 2020)."
        },
        {
            "heading": "5.2 Injecting Noise into Bias-only Expert",
            "text": "In order to ensure that the bias-only model captures the length bias present in the input, we employ a technique of injecting random noise into the input. This intentional introduction of noise serves the purpose of disrupting the semantic information within the input, thereby aiding the model in effectively learning the length bias. Thus, the perturbed inputs can be expressed as X \u2032 = X+N , where X \u2032 represents the new, noisy input, and N denotes the Gaussian noise added to the token embeddings. By facilitating the collaboration between bias-only experts and main experts in modeling human preference, we enable the biasonly experts to effectively capture length bias while preventing the main model from learning length bias."
        },
        {
            "heading": "5.3 Training & Inference",
            "text": "During the training phase, the main expert and the bias-only expert are jointly optimized by maximizing the following likelihood function to optimize the reward function:\n\u2212E(x,y)\u223cD[log(\u03c3(r\u0302(x, yi)\u2212 r\u0302(x, y1\u2212i)))]. (2)\nThe main expert is initialized based on an SFT model, while the bias-only expert is initialized using a pretrained model. Both reward models add a linear layer on top of the final Transformer layer, which generates the final scalar prediction for the reward signal.\nDuring the PPO stage, we exclusively rely on the main expert to provide rewards, while discarding the bias-only expert. Since the bias-only expert is typically smaller in size, our approach does not significantly increase computational overhead."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Settings",
            "text": "Datasets We utilize the Helpful and Harmless (HH) dataset (Bai et al., 2022b) from Anthropic as our experimental dataset and rm-static2 for training our reward model and for participation in PPO. The HH dataset provides a response\n2https://huggingface.co/datasets/Dahoas/ rm-static\nand a rejected response for each query based on human preferences, specifically focusing on responses that are helpful and harmless. In addition to that, our SFT data incorporated the 52k instruction dataset constructed by Alpaca3 and the ChatAlpaca4 dataset containing multi-turn dialogues.\nModels In our experimental setup, we primarily build upon LLaMA and BLOOMZ models, utilizing models with a parameter size of 7B. Inspired by the work of (Ouyang et al., 2022), who employ SFT models as initial models for PPO, we perform SFT on the Alpaca and ChatAlpaca datasets.\nSFT Hyper-parameters During the SFT phase, we utilize a learning rate of 3e\u22125 and train for three epochs without early stopping. We employ a warmup period of 0.3 epochs, followed by a linear decay to 0. The fine-tuning process was conducted on a device with eight Nvidia A100 GPUs. Each GPU handled four queries, resulting in a batch size of 32. Responses are truncated to 512 tokens, while the total length of both queries and responses was truncated to 2048 tokens. We incorporate specific prompts, such as \"Human:\" or \"Assistant:\", during the concatenation of input queries and output responses. These prompts are added to provide\n3https://github.com/tatsu-lab/stanford_alpaca 4https://github.com/cascip/ChatAlpaca\ncontext and distinguish between human-generated and assistant-generated responses.\nRLHF Hyper-parameters During the reward modeling training phase, our main expert and policy model remained consistent. The learning rate for both is set to 5e\u22126. As for the biasonly expert, we utilize a smaller model, the 560m Bloomz, with a fixed learning rate of 8e\u22126.\nIn the PPO framework, we perform reward score normalization and clipping, with a clip value of 0.8. We employ the clipped surrogate objective of PPO for optimization. The token-level KL penalty coefficient \u03b2 is set to 0.05. For each query, we collect 4 roll-out samples using nucleus sampling. The sampling temperature is set to 0.8, top-p is set to 0.9, repetition penalty is set to 1.1, and the maximum output token length is set to 512. The policy model has a learning rate of 9e\u22127, while the value model utilize a learning rate of 2e\u22126. These specific training details are implemented to optimize the performance and convergence of the models during the training process.\nBaselines In this study, we propose a method primarily aimed at mitigating length bias in the reward model. Therefore, our baselines include the SFT model, the PPO model trained with the vanilla reward model, and the PPO model trained exclusively with the bias-only reward expert.\nMetrics We evaluate the effectiveness of different methods in our experiments using perplexity (gpt2-medium), average reward score , and human\nevaluators. For human evaluation, annotators compared two randomly selected responses and provided comparison results (win/lose/tie), allowing us to gain insights into subjective judgments of response quality."
        },
        {
            "heading": "6.2 True Reward Improvement",
            "text": "As the previous analysis, addressing the risk of reward hacking resulting from reward model overoptimization (Gao et al., 2022) is crucial. To ensure a more robust evaluation of our method, we adopted a comprehensive approach that combines both automatic and human evaluation. Table 1 illustrates the average generated sequence length and reward scores of our method in comparison to\nthe baselines on the test set. The results clearly demonstrate that our approach achieves higher reward scores while simultaneously reducing the average output length compared to the RL model utilizing the vanilla reward model. This finding not only confirms the effectiveness of our proposed method but also establishes a foundation for generating outputs that better align with human preferences.\nIn addition, we conducted a Spearman/Pearson analysis to further validate the effectiveness of our method. For more detailed information, please refer to Appendix 4."
        },
        {
            "heading": "6.3 Wining Rate",
            "text": "In this section of the experimental analysis, we present the win rate of our method compared to other approaches. We provide results from both manual evaluations, GPT4 and the automated evaluation platform, and AlpacaFarm. During the pairwise comparisons of the model results, a win is assigned 1 point, a tie is assigned 0.5 points, and a loss is assigned 0 points. It is evident that our method achieves an average win rate of more that 50% compared to traditional RLHF methods. This validates the effectiveness of our proposed method in generating outputs that are more informative and concise."
        },
        {
            "heading": "6.4 Ablation Study",
            "text": "Table 2 presents the results of ablation analysis on various components of our method. By utilizing the PoE technique and input perturbation, we observe an improvement in the accuracy of the RM. This finding demonstrates that our method can better identify human intent, leading to enhanced performance."
        },
        {
            "heading": "7 Analysis and Discussion",
            "text": "In this section, we uncover valuable insights into the effectiveness and limitations of our approaches, paving the way for future advancements in conversational AI systems."
        },
        {
            "heading": "7.1 Leaning Curve",
            "text": "Figures 6 present the variation in generated sequence length during the training process of our method and the vanilla RM for PPO , along with the output reward scores during training and on the test set. From the Figures, it can be observed that the model trained with the vanilla RM continuously increases its output length throughout the training process, whereas our method achieves a stable output length after a slight increase. Both methods enhance the output\u2019s reward score, but our approach yields a more concise output while containing more informative content."
        },
        {
            "heading": "7.2 Distribution of Reward Scores",
            "text": "Figure 5 illustrates the distribution of reward scores for the chosen and rejected data on the validation set during the training of our method compared to the vanilla reward model. It is evident that\nour model exhibits better discernment between the chosen and rejected data. This improved performance can be attributed to our approach\u2019s ability to avoid excessive learning bias towards sequence length. By mitigating the length bias, the main expert of our model can focus more on understanding genuine human intent, leading to enhanced generalization capabilities."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this study, we investigate the issue of length bias in NLP and propose a PoE-based method to mitigate this bias. Our work sheds light on the challenges associated with reward modeling, emphasizing that it is not a straightforward task and various difficulties may impede the models from capturing the true human intent. By addressing the problem of length bias, our research highlights the importance of developing techniques that enable models to learn and generate responses aligned with genuine human intentions. Further research and advancements in this area are necessary to overcome these obstacles and enhance the performance and reliability of NLP models in realworld applications.\nLimitations\nIn this study, we propose a simple and effective method to mitigate length bias during the Reinforcement Learning from Human Feedback stage. However, it is important to note that our method can only alleviate length bias to some extent and may not completely eliminate it. Furthermore, the validation of our method\u2019s effectiveness was conducted on two RLHF datasets. It is worth mentioning that collecting RLHF data is a challenging task, and the question of whether this phenomenon exists on larger datasets remains uncertain. Evaluating the performance of general dialogue models poses a difficulty. Therefore, during the human evaluation phase, we selected only a limited number of evaluation samples to assess the effectiveness of our approach."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by National Natural Science Foundation of China (No.62076069,62206057,61976056), Shanghai Rising-Star Program (23QA1400200), Natural Science Foundation of Shanghai (23ZR1403500),\nShanghai Academic Research Leader Program 22XD1401100."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Analyzing the correlation between generated length and correspondent reward\nAs Table 4 illustrated, we introduce the Spearman/Pearson coefficient to analyze the correlation between the two variables in the HH-RLHF eval sets. This analysis serve as evidence to demonstrate the effectiveness of our method in reducing length, strengthening our findings.\nA.2 Scaling law for bias-only model\nThis part is a extensive exploration of the biasonly expert within our proposed method. We investigated the scaling law (Kaplan et al., 2020; Gao et al., 2023) of the main expert and bias-only expert using BLOOMZ, and we found that the accuracy of the reward model can be listed in Table 5.\nA.3 PPO training stability\nAccording to the technical report (Zheng et al., 2023), reward scaling has been found to be beneficial for enhancing the training stability of PPO. In our experiment, we also applied reward scaling and observed a significant improvement in training stability.\nA.4 Length bias Phenomenon on summarization task\nTo examine the influence of length bias on the summarization task, we conducted an investigation focusing on KL divergence and omitted the penalty term for the TL;DR task. Additionally, we explored the RLHF pipeline in TL;DR, as depicted in Figure 7. Our findings suggest that the length factor significantly affects the quality of concise summarization, reinforcing its importance in this task. To further validate our hypothesis, we assessed the summarization performance using GPT-4, and the results (refer to Table 6) provided support for our claims.\nA.5 Additional illustration in different reward models\nA.6 Case Study To provide a more comprehensive evaluation, we directly assesses the effectiveness of our proposed method in addressing length bias. The case study include measurements that capture the reduction in response length and the maintenance of contextual coherence. As shown in Table 7, we present a case study that illustrates these measurements through a one-turn dialogue example. In the second example, we observed an instance of improper reward model overoptimization, leading to the policy model collapsing and engaging in selftalking. We speculate that this issue may arise from the loss of instruction-following ability due to negative optimization."
        }
    ],
    "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
    "year": 2023
}