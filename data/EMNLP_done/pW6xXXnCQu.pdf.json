{
    "abstractText": "Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual\u2019s personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of wellstructured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tao Yang"
        },
        {
            "affiliations": [],
            "name": "Tianyuan Shi"
        },
        {
            "affiliations": [],
            "name": "Fanqi Wan"
        },
        {
            "affiliations": [],
            "name": "Xiaojun Quan"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        },
        {
            "affiliations": [],
            "name": "Bingzhe Wu"
        },
        {
            "affiliations": [],
            "name": "Jiaxiang Wu"
        }
    ],
    "id": "SP:4499af869af9eefbe944f2fd1c33fa6e706283bc",
    "references": [
        {
            "authors": [
                "Mohammad Hossein Amirhosseini",
                "Hassan Kazemian."
            ],
            "title": "Machine learning approach to personality type prediction based on the myers\u2013briggs type indicator\u00ae",
            "venue": "Multimodal Technologies and Interaction, 4(1):9.",
            "year": 2020
        },
        {
            "authors": [
                "Sean Andrist",
                "Bilge Mutlu",
                "Adriana Tapus."
            ],
            "title": "Look like me: matching robot personality via gaze to increase motivation",
            "venue": "Proceedings of the 33rd annual ACM conference on human factors in computing systems, pages 3603\u20133612.",
            "year": 2015
        },
        {
            "authors": [
                "R Michael Bagby",
                "Tara M Gralnick",
                "Nadia Al-Dajani",
                "Amanda A Uliaszek."
            ],
            "title": "The role of the fivefactor model in personality assessment and treatment planning",
            "venue": "Clinical Psychology: Science and Practice, 23(4):365.",
            "year": 2016
        },
        {
            "authors": [
                "Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901",
            "year": 2020
        },
        {
            "authors": [
                "Yahui Chen."
            ],
            "title": "Convolutional neural network for sentence classification",
            "venue": "Master\u2019s thesis, University of Waterloo.",
            "year": 2015
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168",
            "year": 2021
        },
        {
            "authors": [
                "Philip J Corr",
                "Gerald Ed Matthews."
            ],
            "title": "The Cambridge handbook of personality psychology",
            "venue": "Cambridge University Press.",
            "year": 2009
        },
        {
            "authors": [
                "Brandon Cui",
                "Calvin Qi."
            ],
            "title": "Survey analysis of machine learning methods for natural language processing for mbti personality type prediction",
            "venue": "Final Report Stanford University.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "John M Digman."
            ],
            "title": "Personality structure: Emergence of the five-factor model",
            "venue": "Annual review of psychology, 41(1):417\u2013440.",
            "year": 1990
        },
        {
            "authors": [
                "M Brent Donnellan",
                "Frederick L Oswald",
                "Brendan M Baird",
                "Richard E Lucas."
            ],
            "title": "The mini-ipip scales: tiny-yet-effective measures of the big five factors of personality",
            "venue": "Psychological assessment, 18(2):192.",
            "year": 2006
        },
        {
            "authors": [
                "Adithya V Ganesan",
                "Yash Kumar Lal",
                "August H\u00e5kan Nilsson",
                "H Andrew Schwartz."
            ],
            "title": "Systematic evaluation of gpt-3 for zero-shot personality estimation",
            "venue": "arXiv preprint arXiv:2306.01183.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Lewis R Goldberg."
            ],
            "title": "An alternative\" description of personality\": the big-five factor structure",
            "venue": "Journal of personality and social psychology, 59(6):1216.",
            "year": 1990
        },
        {
            "authors": [
                "Samuel D Gosling",
                "Peter J Rentfrow",
                "William B Swann Jr."
            ],
            "title": "A very brief measure of the bigfive personality domains",
            "venue": "Journal of Research in personality, 37(6):504\u2013528.",
            "year": 2003
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla."
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Louis Hickman",
                "Nigel Bosch",
                "Vincent Ng",
                "Rachel Saef",
                "Louis Tay",
                "Sang Eun Woo."
            ],
            "title": "Automated video interview personality assessments: Reliability, validity, and generalizability investigations",
            "venue": "Journal of Applied Psychology, 107(8):1323.",
            "year": 2022
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Oliver P John",
                "Eileen M Donahue",
                "Robert L Kentle."
            ],
            "title": "Big five inventory",
            "venue": "Journal of Personality and Social Psychology.",
            "year": 1991
        },
        {
            "authors": [
                "Oliver P John",
                "Laura P Naumann",
                "Christopher J Soto"
            ],
            "title": "Paradigm shift to the integrative big five trait taxonomy: History, measurement, and conceptual issues",
            "year": 2008
        },
        {
            "authors": [
                "Amir A Khan",
                "Kristen C Jacobson",
                "Charles O Gardner",
                "Carol A Prescott",
                "Kenneth S Kendler."
            ],
            "title": "Personality and comorbidity of common psychiatric disorders",
            "venue": "The British Journal of Psychiatry, 186(3):190\u2013196.",
            "year": 2005
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "ICML 2022 Workshop on Knowledge Retrieval and Language Models.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Veronica Lynn",
                "Niranjan Balasubramanian",
                "H Andrew Schwartz."
            ],
            "title": "Hierarchical modeling for user personality prediction: The role of message-level attention",
            "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Sandra C Matz",
                "Michal Kosinski",
                "Gideon Nave",
                "David J Stillwell."
            ],
            "title": "Psychological targeting as an effective approach to digital mass persuasion",
            "venue": "Proceedings of the national academy of sciences, 114(48):12714\u201312719.",
            "year": 2017
        },
        {
            "authors": [
                "Yash Mehta",
                "Navonil Majumder",
                "Alexander Gelbukh",
                "Erik Cambria."
            ],
            "title": "Recent trends in deep learning based personality detection",
            "venue": "Artificial Intelligence Review, pages 1\u201327.",
            "year": 2019
        },
        {
            "authors": [
                "Isabel Myers-Briggs."
            ],
            "title": "Introduction to type: A description of the theory and applications of the myersbriggs indicator",
            "venue": "Consulting Psychologists: Palo Alto.",
            "year": 1991
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Gregory Park",
                "H Andrew Schwartz",
                "Johannes C Eichstaedt",
                "Margaret L Kern",
                "Michal Kosinski",
                "David J Stillwell",
                "Lyle H Ungar",
                "Martin EP Seligman."
            ],
            "title": "Automatic personality assessment through social media language",
            "venue": "Journal of personality and",
            "year": 2015
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are nlp models really able to solve simple math word problems",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "James W Pennebaker",
                "Martha E Francis",
                "Roger J Booth."
            ],
            "title": "Linguistic inquiry and word count: Liwc 2001",
            "venue": "Mahway: Lawrence Erlbaum Associates, 71(2001):2001.",
            "year": 2001
        },
        {
            "authors": [
                "James W Pennebaker",
                "Laura A King."
            ],
            "title": "Linguistic styles: language use as an individual difference",
            "venue": "Journal of personality and social psychology, 77(6):1296.",
            "year": 1999
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Md Abdur Rahman",
                "Asif Al Faisal",
                "Tayeba Khanam",
                "Mahfida Amjad",
                "Md Saeed Siddik."
            ],
            "title": "Personality detection from text using convolutional neural network",
            "venue": "2019 1st international conference on advances in science, engineering and robotics tech-",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Philip Sedgwick."
            ],
            "title": "Spearman\u2019s rank correlation coefficient",
            "venue": "Bmj, 349.",
            "year": 2014
        },
        {
            "authors": [
                "Zhihong Shao",
                "Yeyun Gong",
                "Yelong Shen",
                "Minlie Huang",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Synthetic prompting: Generating chain-of-thought demonstrations for large language models",
            "venue": "arXiv preprint arXiv:2302.00618.",
            "year": 2023
        },
        {
            "authors": [
                "Xiangguo Sun",
                "Bo Liu",
                "Jiuxin Cao",
                "Junzhou Luo",
                "Xiaojun Shen."
            ],
            "title": "Who am i? personality detection based on deep learning for texts",
            "venue": "2018 IEEE international conference on communications (ICC), pages 1\u20136. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Edward P Tighe",
                "Jennifer C Ureta",
                "Bernard Andrei L Pollo",
                "Charibeth K Cheng",
                "Remedios de Dios Bulos."
            ],
            "title": "Personality trait classification of essays with the application of feature reduction",
            "venue": "SAAIP@ IJCAI, pages 22\u201328.",
            "year": 2016
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Jindong Wang",
                "HU Xixu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Wei Ye",
                "Haojun Huang",
                "Xiubo Geng"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-ofdistribution perspective",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022b. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Feifan Yang",
                "Xiaojun Quan",
                "Yunyi Yang",
                "Jianxing Yu."
            ],
            "title": "Multi-document transformer for personality detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14221\u2013 14229.",
            "year": 2021
        },
        {
            "authors": [
                "Tao Yang",
                "Jinghao Deng",
                "Xiaojun Quan",
                "Qifan Wang."
            ],
            "title": "Orders are unwanted: Dynamic deep graph convolutional network for personality detection",
            "venue": "Proceedings of AAAI 2023.",
            "year": 2022
        },
        {
            "authors": [
                "Tao Yang",
                "Feifan Yang",
                "Haolan Ouyang",
                "Xiaojun Quan."
            ],
            "title": "Psycholinguistic tripartite graph network for personality detection",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "2022a. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "arXiv preprint arXiv:2210.03493.",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "NeurIPS 2022 Foundation Models for Decision Making Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "Yangfu Zhu",
                "Linmei Hu",
                "Xinkai Ge",
                "Wanrong Peng",
                "Bin Wu."
            ],
            "title": "Contrastive graph transformer network for personality detection",
            "venue": "Proc. IJCAI Conf.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Personality, as an important psychological construct, refers to individual differences in patterns of thinking, feeling, and behaving (Corr and Matthews, 2009). Consequently, detecting one\u2019s personality from their generated textual data has garnered considerable interest from researchers due to its wide-ranging applications (Khan et al., 2005;\n\u2217Corresponding authors.\nBagby et al., 2016; Andrist et al., 2015; Hickman et al., 2022; Matz et al., 2017). For instance, personality aids clinical psychologists in gaining a better understanding of psychiatric disorders (Khan et al., 2005) and developing personalized treatment modalities (Bagby et al., 2016); it improves the human-robot interaction, particularly for socially assistive robots (Andrist et al., 2015).\nPrevious studies (Mehta et al., 2019; Yang et al., 2021b, 2022) on text-based personality detection have focused on training or fine-tuning specific models. However, their performance is significantly limited by the quality and quantity of training data. The emergence of large language models (LLMs), such as GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), and LLaMA (Touvron et al., 2023), has recently demonstrated impressive incontext learning (ICL) ability, in which LLMs\nmake predictions solely based on designed prompts or instructions without any parameter modifications, leading to a new paradigm in the NLP community. Building upon these strengths, this study aims to investigate the ability of LLMs to perceive an author\u2019s personality from text, an aspect that has not been extensively explored previously.\nInspired by the process of human to complete self-assessment personality tests, we approach personality detection as a multi-step reasoning task since psychologists often use a series of assessments to measure an individual\u2019s personality. To perform complex reasoning, a recently technique named chain-of-thought (CoT) has been developed for solving math word problems (Cobbe et al., 2021; Patel et al., 2021) by generating intermediate reasoning steps (Wei et al., 2022b; Kojima et al., 2022; Zhang et al., 2022b). In our scenario, we argue that the items in psychological questionnaires can be considered as a set of rigorous reasoning steps. Consequently, we propose a novel personality reasoning framework, PsyCoT, which utilizes psychological questionnaires as CoT. Figure 1 illustrates the framework of PsyCoT, which mimics a human to complete personality test through a multiturn dialogue. Specifically, we prompt the LLM (e.g., GPT-3.51) to act as an AI assistant whose task is to rate2 a given item based on the author\u2019s text. At each turn, we sample an item from the psychological questionnaire and present it to the assistant. The AI assistant then returns a specific score for the item. Once all the items have been rated, the AI assistant selects a definitive personality trait based on the historical rating results. Additionally, these scores can be aggregated using rules defined by questionnaires to yield an overall score, which severs as a double check and provides confidence for the chosen personality trait.\nTo evaluate the effectiveness of PsyCoT, we conduct extensive experiments on two benchmark datasets (i.e., Essays and Kaggle) that employ different personality taxonomies (i.e., Big Five (Goldberg, 1990) and MBTI (Myers-Briggs, 1991)). Experimental results show that PsyCoT significantly increases the zero-shot performance of GPT-3.5 in personality detection. For instance, PsyCoT outperforms the standard prompting by 4.23/10.63 points in average F1 on the two datasets. Moreover, in the Essays dataset, PsyCoT surpasses fine-tuned meth-\n1https://platform.openai.com/docs/models/gpt-3-5 2We follow the original rating rules defined by psychologi-\ncal questionnaires.\nods on most personality traits, demonstrating its competitiveness within the fine-tuning paradigm. Furthermore, ablation studies and analysis indicate that the form of multi-turn dialogue helps to achieve a more accurate reasoning than the singleturn dialogue, and PsyCoT exhibits strong robustness when faced perturbation in option order.\nOur work is the first to explore the ability of LLMs in detecting personality. The proposed PsyCoT incorporates a well-designed questionnaire as CoT steps to facilitate the reasoning of LLM. We highlight that GPT-3.5 yields comparable performance to some fine-tuning methods by equipped with PsyCoT. Besides, PsyCoT offers a fresh perspective of using well-designed questionnaires to design prompts in the era of LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": "Personality Detection In early stage, Pennebaker et al. (2001) developed Linguistic Inquiry and Word Count (LIWC) to extract psycholinguistic features from text, which has been used for feature engineering in machine learning models (Cui and Qi, 2017; Amirhosseini and Kazemian, 2020). However, these feature engineering-based methods have limitations in extracting implicit semantic features. Recently, several studies have proposed unbiased user representations for personality detection. For instance, Yang et al. (2021a, 2022) addressed the post-order problem in encoding posts by introducing a multi-document Transformer and a unordered dynamic deep graph network, respectively. Zhu et al. (2022) constructed a fully connected post graph for each user and developed CGTN to consider correlations among traits. Another body of research incorporates additional knowledge into the model. For example, Yang et al. (2021b) proposed TrigNet, which constructs a heterogeneous tripartite graph using LIWC and utilizes flow-GAT to operate on this graph. Yang et al. (2021b) introduced PQ-Net, which incorporates psychological questionnaires as additional guidance to capture item-relevant information from contextual representations. Despite their successes, these methods are rely on a data-driven approach to train the model to capture implicit personality cues, which are differ from our study as we specifically explore the zero-shot performance of the LLM in personality detection.\nLLMs and Prompting Methods Recent years have witnessed an increasing interest in LLMs,\nsuch as GPT-3 (Brown et al., 2020), FLAN (Wei et al., 2022a), OPT (Zhang et al., 2022a), PaLM (Chowdhery et al., 2022), and LLaMA (Touvron et al., 2023), due to their impressive zero-shot generalization across various tasks. With techniques like instruction tuning (Wei et al., 2022a) and reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022), ChatGPT3 demonstrates remarkable alignment capabilities when following human instructions. Consequently, to leverage the potential of LLMs in downstream tasks, several works have focused on carefully designing prompts manually (Hendy et al., 2023) or automatically (Gao et al., 2021; Zhou et al., 2022b). Another approach that has gained attention is the Chain-ofThought (CoT), which explicitly guide LLMs in generating reasoning steps. Wei et al. (2022b) initially introduced the few-shot-CoT, which utilized reasoning steps as demonstrations by crafting fewshot exemplars, resulting in significant improvements in complex math tasks. Building on this work, other studies have proposed various variants of CoT, including Zero-shot CoT (Kojima et al., 2022), Auto-CoT (Zhang et al., 2022b), Least-tomost prompting (Zhou et al., 2022a), and Synthetic prompting (Shao et al., 2023). Unlike most of these works that focus on how to select few-shot exemplars, we aim to use psychological questionnaire as a explicitly CoT to facilitate reasoning."
        },
        {
            "heading": "3 Psychological Questionnaire as CoT",
            "text": "For the current generative LLM, personality detection can be formulated as a reading comprehensive task. Formally, given a set of texts X= {x1, x2, . . . , xn} written by an individual, the goal is to determine the correct personality option Y= { yt }T t=1\nabout this individual based on X . We can achieve the goal by designing the appropriate prompt P = {D, I}, in which D represents the task description prompt that informs the LLM about the task\u2019s definition, and I represents the inference prompt that pushes the LLM to select the desired personality trait in a specified format."
        },
        {
            "heading": "3.1 Standard Prompting",
            "text": "We first describe the standard prompting, which infers the results directly from the given input text X under prompted by D and I:\ny\u0302i = LLM(D,X, I) (1)\n3https://chat.openai.com/\nAlgorithm 1: PsyCoT Prompting. Input: LLM: LLM(\u00b7); Author\u2019s text: X;\nTask description prompt: D; Inference prompt: I; K Reasoning prompts: R= {rk}Kk=1\nOutput: the personality trait: y\u0302i\n1: Initialize a dialogue history H = [null] 2: for k = 1 to K do 3: Get k-th rating result ak under rk: ak = LLM(D,X,H, rk) 4: Append the rk and ak into H: H = [r1, a1, . . . , rk, ak] 5: end for 6: Infer the personality trait:\nyi = LLM(D,X,H, I) 7: return y\u0302i\nwhere y\u0302i is the inferred personality trait. We show an example of the standard prompting at the top of Figure 2. The initial paragraph introduces a task description prompt D, where we specify the role of LLM as an AI assistant specializing in text analysis. Its objective is to predict an individual\u2019s personality trait based on their written text. The subsequent paragraph contains the text written by the author, which will be analyzed by the LLM. Lastly, the final paragraph depicts the inference prompt I , which includes the provided options and instructions for the LLM to generate the choice using a fixed format. The standard prompting relies solely on the LLM to comprehend the concept of personality and determine how to evaluate it."
        },
        {
            "heading": "3.2 PsyCoT Prompting",
            "text": "While standard prompting techniques have demonstrated effectiveness across various tasks (Sanh et al., 2022; Wang et al., 2023), they fall short when confronted with the complex task of personality detection. This limitation arises from the fact that texts authored by individuals are often topicagnostic, lacking explicit references to their personality traits. Motivated by self-assessment personality tests (Digman, 1990; Myers-Briggs, 1991), we introduce PsyCoT prompting, which utilizes items from questionnaires as a chain-of-thought (CoT) framework to enhance reasoning capabilities.\nThe example of PsyCoT utilizing the 44-item Big Five Inventory (John et al., 2008) is depicted at the bottom of Figure 2. In comparison to standard prompting, PsyCoT mimics the process of\nself-assessment personality tests and enhances standard prompting in the following key aspects: (1) We modify the task description D by instructing the LLM to rate each statement (item in the questionnaire). (2) We include in D a description of the rating rules for the questionnaire, encompassing the scoring system (e.g., \u201c1=disagree strongly, 2=disagree a little, 3=neutral, 4=agree a little, and 5=agree strongly\u201d) and highlighting the significance of reversed statements. (3) Most importantly, we introduce K reasoning steps R= {rk}Kk=1 prior to accessing the personality trait y\u0302i via multidialogue, guiding the LLM in inferring the personality with a more reasonable manner. The overall progress of PsyCoT is described in Algorithm 1, including step by step reasoning in Line 2-4. We provide complete dialogue records in Appendix B.\nA by-produce of PsyCoT Prompting is the rating results [a1, a2, . . . , aK ], which can be aggregated into an overall score like the self-assessment personality tests. For example, the overall score si in 44-item Big Five Inventory is computed as:\nsi = 1\nK K\u2211 k=1 sk (2)\nwhere sk is the transition score, which defined as:\nsk = { 6\u2212 ak rk \u2208 Rs ak otherwise\n(3)\nwhere Rs is the set of reversed statements. Since positively correlated with yi, the overall score si can serve as a double check and provide confidence for the inferred y\u0302i.\nDiscussion PsyCoT employs a multi-turn approach to rating questionnaire items, as opposed to providing simultaneous ratings for all items within a single turn. There are two main reasons. Firstly, by rating each item individually during each turn, we can maintain a higher level of focus for the LLM, resulting in more accurate outcomes. Secondly, there are inherent correlations between the items. For instance, statements such as \u201cThe author\nis reserved.\u201d and \u201cThe author tends to be quite.\u201d exhibit a high level of correlation. Consequently, incorporating the historical rating results when evaluating the current item within a multi-turn structure will enhance the consistency of the obtained results. We provide more investigations in Section 4.5."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate PsyCoT on two public datasets: Essays (Pennebaker and King, 1999) and Kaggle4. The Essays comprises 2468 anonymous texts written by volunteers in a strict environment. These volunteers were instructed to write whatever came to their mind within a short time-frame. Each essay is associated with the author\u2019s Big Five personality traits, namely Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness. The authors assessed their own traits using a 44-item Big Five Inventory (John et al., 1991), which is the same questionnaire we employ in PsyCoT. To ensure fair comparisons with fine-tuned methods, we randomly divided the dataset into training, validation, and testing with proportions of 80-10-10, respectively. For prompt-based methods, we evaluate their zero-shot performance on the test set.\nThe Kaggle dataset is collected from PersonalityCafe5, a forum where people share their personality traits and engage in discussions about personalityrelated topics. The Kaggle comprises a total of 8675 user\u2019s data, each consisting of 45-50 posts. The used personality taxonomy is MBTI (MyersBriggs, 1991), which divides personality into four dimensions: Introversion or Extroversion (I/E), Sensing or iNtuition (S/N), Thinking or Feeling (T/F), and Perception or Judging (P/J). Limited by API resources, we randomly selected 200 samples form the test set to evaluate PsyCoT and baselines.\nFor the Essay dataset, the average token count per sample is 762.38, whereas for Kaggle, it stands at 1632.38. More details of the two datasets and the used questionnaires are provided in Appendix C and Appendix D, respectively."
        },
        {
            "heading": "4.2 Baselines",
            "text": "In our experiments, we adopt the following previous methods as baselines. LIWC+SVM (Tighe et al., 2016): This shallow method firstly extracts psycholinguistic features\n4https://www.kaggle.com/datasnaek/mbti-type 5http://personalitycafe.com/forum\nusing LIWC (Pennebaker et al., 2001), and then applies SVM as the classifier. TF-IDF+SVM (Cui and Qi, 2017): This method is similar to LIWC+SVM, but it extracts features using TF-IDF. W2V+CNN (Rahman et al., 2019) and Glove+LSTM (Sun et al., 2018): These two methods are non-pretrained models. The former encodes the context using the word2vec algorithm and then applies CNN (Chen, 2015) to obtain the context representation. The latter uses Glove (Pennington et al., 2014) for word embeddings and then utilizes LSTM (Hochreiter and Schmidhuber, 1997) to encode the context. Regression (Park et al., 2015): This method trains a regression model with two regression scores 0 and 1. The same quantile discretization method from Ganesan et al. (2023) is then used to convert the test set scores into categorical labels. BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019): These two models are fine-tuned and utilize \u201cbert-base-cased\u201d and \u201croberta-base\u201d as backbones, respectively. For Essays, they encode the context directly, while for Kaggle, they encode each post and then aggregate post representations via mean pooling. SN+Attn (Lynn et al., 2020): This method adopts a hierarchical attention network to generate the user representation. Following Yang et al. (2021b), the pre-trained BERT is utilized as a post-encoder to ensure fair comparisons. TrigNet (Yang et al., 2021b): TrigNet constructs a tripartite graph with psycholinguistic knowledge in LIWC to fuse posts. DDGCN (Yang et al., 2022): DDGCN is the latest SOTA method in the Kaggle dataset, which firstly encodes each post using a domain-adapted BERT, and then aggregates the posts in a disorderly manner by a dynamic deep graph network.\nFor prompt-based methods, in addition to standard prompting, we adopt Zero-shot-CoT (Kojima et al., 2022), which inserts a reasoning step with \u201cLet\u2019s think step by step.\u201d before accessing the final personality via the inference prompt."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "In this study, we simplify multi-label personality detection into multiple binary classification tasks.6 For the prompt-based methods, we request the GPT3.5 API (gpt-3.5-turbo-0301) to obtain results,\n6More details are provided in Appendix A\nwhich is currently the most popular and forms the foundation of ChatGPT. We set the temperature as 0 to ensure deterministic outputs. In the case of Essays, we limited the maximum tokens for the author\u2019s text to 3200, while for Kaggle, we restricted it to 80 tokens per post. For the fine-tuning based methods, we set the learning rate to 2e-5, and report their test performance (using same data as prompt-based methods) by averaging the results of five runs. The evaluation metrics employed in our study include Accuracy and Macro-F1 score."
        },
        {
            "heading": "4.4 Overall Results",
            "text": "The overall results of PsyCoT and several baselines on Essays are listed in Table 1. Data-driven baselines comprise three types of methods: shallow model (LIWC+SVM and Regression), nonpretrained model (W2V+CNN), and fine-tuned models (BERT and RoBERTa). The prompt-based baselines include Standard prompting and Zeroshot-CoT.\nThere several key observations from these re-\nsults. First, PsyCoT outperforms the baselines on most personality traits, even surpassing the fine-tuned models. Specifically, PsyCoT enhances standard prompting with an average increase of 0.94/4.23 points in accuracy and macro-F1. Second, PsyCoT performs worse than standard prompting on the Neuroticism trait. Further analysis reveal that this discrepancy may be attributed to dataset bias. The Essays contains a significant amount negative emotions expressed by authors, which misleads the LLM into assigning high scores for statements such as \u201cThe author worries a lot.\u201d, \u201cThe author is depressed, blue.\u201d, and \u201cThe author can be moody.\u201d. Third, although includes a reasoning step with \u201cLet\u2019s think step by step.\u201d, Zero-shotCoT does not consistently improve the performance of standard prompting. Our investigation shows that Zero-shot-CoT often fails to guide the reasoning process correctly, resulting in meaningless responses like \u201cSure, what would you like to start with?\u201d. In contrast, PsyCoT directly constructs reasoning steps with the help of questionnaires.\nTable 2 presents additional results on the Kaggle dataset. The obeservations are threefold. First, PsyCoT achieves the highest performance among prompt-based methods with 18.37/10.63 points improvement in accuracy and macro-F1 over standard prompting, and 1.50/4.08 points improvement over Zero-shot-CoT. Second, compared to fine-tuned models, PsyCoT achieves a performance very close to that of BERT, while still falls behind the current SOTA method DDGCN by a non-negligible margin. To explore the underlying reasons, we analyze the textual input and find that users in the Kaggle dataset directly discuss the MBTI and frequently employ abbreviations such as \"Fi,\" \"Fe,\" \"Ni,\" \"Te,\" and \"IxxP\" to describe personality traits. These abbreviations hold potential as strong features for data-driven methods, but for zero-shot methods, they may be insufficient for capturing the association between these abbreviations and personalities. Third, despite being weaker than the fine-tuned methods on the Kaggle, prompt-based methods have the advantages of not requiring the collection of training data and retraining of models to accommodate changes in personality types and application scenarios (e.g., Essays and Kaggle should be quite different in terms of the text style: freely-written text vs. discussion posts.)"
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "Table 3 presents several ablation results of PsyCoT to facilitate more comprehensive investigations.\nSingle-turn PsyCoT adopts a multi-turn dialogue structure to facilitate reasoning. This structure allows the LLM to concentrate on a particular item, enhancing the accuracy of its reasoning process. To verify its effectiveness, we test another alternative\napproach, which involves presenting all the items within a single-turn dialogue and instructing the LLM to assign ratings to all of them simultaneously. From Table 3, we can observe that reasoning with a single-turn dialogue deteriorates the performance, demonstrating that PsyCoT with multi-turn for reasoning is preferable.\nOther Questionnaire We utilizes the 44-item Big Five Inventory (John et al., 1991) for operating PsyCoT in Essays. However, alternative questionnaires also can be applied in PsyCoT. Therefore, we have included other widely used brief measures, such as TIPI (10-item) (Gosling et al., 2003) and Mini-IPIP (20-item) (Donnellan et al., 2006), to examine the impact of different questionnaires. Appendix D presents a comparison between these three inventories. It is evident that the 44-item inventory offers a broader range of aspects for evaluating an individual\u2019s personality, making it a potentially more effective CoT for guiding reasoning. The results in Table 3 further confirm that the TIPI is insufficient in eliciting the desired level of reasoning, as it leads to a significant decline in performance. While the 20-item scale is better than 10-item scale, but still inferior to the 44-item scale. This experiment underscores the crucial of\nselecting appropriate questionnaires for PsyCoT."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Correlation Analysis",
            "text": "In this study, we explicitly introduce the psychological questionnaire as CoT, which guides the LLM in assessing the author\u2019s personality by rating each item. Consequently, as defined by Eq.(2), these ratings can be computed into an overall score (we refer to trait score) that reflects the strength of a specific personality trait. To investigate the correlation between the trait score and the chosen personality trait, we firstly visualize the distributions of trait scores across five different dimensions in the Essays dataset, as shown in Figure 3.\nThe observations yield two main findings. First, in general, there is a significant difference in the distribution between low and high traits, and the trait scores exhibit a strong positive correlation with the personality types, particularly in Agreeableness, Extraversion, and Openness. Second, the distribution of scores for Extraversion and Openness can be distinguished by the dashed line, illustrating that the LLM\u2019s ratings for these traits align with the criteria of the questionnaire. On the other three traits, however, the distributions intersect the dashed line and shift to one side, suggesting potential bias in the LLM\u2019s ratings of the questionnaires. We further apply Spearman\u2019s Coefficient (Sedgwick, 2014) to quantitatively measure the correlations. From Figure 3, the trait scores for Agreeableness, Extraversion, and Openness have a higher correlation (> 0.70) with the chosen personality type, which validates our observations."
        },
        {
            "heading": "5.2 Statistical Tests",
            "text": "To test the statistical significance of our method, we conduct a comparison between the Standard prompt and PsyCot using the Essays dataset, and the F1 results are presented in the Table 4. Each point represents the average of 5 runs. The T-test analysis indicates that our improvements are statistically significant, with p-values of less than 0.05 for AGR, less than 0.001 for CON, EXT, and OPN."
        },
        {
            "heading": "5.3 Robustness Testing",
            "text": "A notorious drawback of LLMs is its vulnerability to minor variations in prompts, such as changes in the order of options within choice questions. Intuitively, PsyCoT incorporates questionnaire to facilitate reasoning, potentially improving the robustness of LLMs. To verify this hypothesis, we conduce an experiment where we swap the option orders in both the task description prompt D and inference prompt I (e.g., A: \u201cHigh Agreeableness\u201d or B: \u201cLow Agreeableness\u201d is exchanged to A: \u201cLow Agreeableness\u201d or B: \u201cHigh Agreeableness\u201d), and re-test the prompt-based methods. We measure the unchanged rate of y\u0302i across 100 samples. The results from the Essays dataset are presented in Figure 4. PsyCoT achieves the highest unchanged rate and significantly superiors other methods, demonstrating that the inclusion of the questionnaire enhances its robustness."
        },
        {
            "heading": "5.4 Impact of Post Order",
            "text": "The Kaggle dataset contains a set of posts for each user. These posts are concatenated in sequence to create a long document, which serves as the input X . However, Yang et al. (2021a, 2022) have demonstrated that encoding posts sequentially is order-sensitive for the fine-tuned models. To investigate whether the prompt-based methods are also influenced by post order, we randomly disrupt the post orders and re-test the prompt-based methods using 100 samples. Similar to Section 5.3, the unchanged rate is used for evaluation. As shown in Figure 5, shuffling post orders leads to noticeable changes in the predictions across several personal-\nity traits, including PsyCoT. This observation highlights that the order of posts remains an important issue for the LLM."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a novel method, PsyCoT, for zero-shot personality detection. PsyCoT prompts the LLM to engage in reasonable personality reasoning. Unlike the standard prompting approach that directly requests the LLM to infer the answer, PsyCoT draws inspiration from selfassessment personality tests that evaluate an individual\u2019s personality through well-designed questionnaires. PsyCoT introduces items from the questionnaire as a set of rigorous CoT. It guides the LLM to evaluate each item based on the author\u2019s text and then infer the final personality trait. Extensive experiments on two benchmark datasets demonstrate that PsyCoT outperforms Standard prompting and Zero-shot-CoT approaches by a large margin. It also achieves comparable performance to some fine-tuned models. This work represents the first effort to leverage psychological questionnaires to elicit the LLM\u2019s reasoning abilities, providing a new perspective for exploring the use of questionnaires in the era of LLMs."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62176270), the Guangdong Basic and Applied Basic Research Foundation (No. 2023A1515012832), and 2023 Tencent Rhino-Bird Research Elite Program.\nLimitations\nThe potential limitations of PsyCoT are discussed below. Firstly, PsyCoT utilizes a multi-turn dialogue approach to rate items from the questionnaire. Although this approach surpasses the single-turn method (as reported in Section 4.5), it does result in more API requests. Secondly, the performance of PsyCoT is heavily influenced by the selected questionnaire, necessitating additional effort in finding the most suitable one. Thirdly, PsyCoT has only been applied to one language, and there is a need to explore its application in a broader range of languages.\nEthics Statement\nText-based personality detection is a long history research area within psycholinguistics. We clarify that the purpose of this study is to explore and improve the ability of the LLM in this particular task, rather than creating a tool that invades privacy. The Essays and Kaggle datasets used in our study are public available and all user information has been anonymized. We have strictly adhered to the data usage policy throughout our research. However, it is crucial to acknowledge that the misuse of this technology may pose ethical risks. We state that any research or application stemming from this study is solely permitted for research purposes, and any attempt to exploit this technology to covertly infer individuals\u2019 personality traits or other sensitive characteristics is strictly prohibited."
        },
        {
            "heading": "A Appendix: Form of the Task",
            "text": "Although the original tasks of the Big 5 are in the regression form, the Essays dataset is further collected and processed into the binary label format (Y/N), without releasing the original scores from the raters. Consequently, we have framed this task as a classification problem, designating \u2019High\u2019 as \u2019Y\u2019 and \u2019Low\u2019 as \u2019N.\u2019 Besides, our initial study indicates that simplifying multi-label personality detection into multiple binary classification tasks leads to performance improvements (the average results on standard prompt are reported in Table 5). Hence, this approach was chosen for our experiments."
        },
        {
            "heading": "B Appendix: Dialogue Record",
            "text": "To better understand our method, we provide two complete records in Figure 6 and Figure 7, respectively. These records demonstrate how PsyCoT predicts Big Five and MBTI personality traits."
        },
        {
            "heading": "C Appendix: Details of Datasets",
            "text": "We provide the statistics of Essays and Kaggle in Table 6."
        },
        {
            "heading": "D Appendix: Details of Questionnaires",
            "text": "In this study, the 44-item Big Five Inventory and Myers-Briggs Type Indicator are used for our methods. The details of items are listed in Table 7 and Table 10. Note that the original items in MyersBriggs Type Indicator do not include the option \"Not sure whether A or B\". We include this option to ensure that the LLM has the ability to indicate uncertainty in cases where the provided information is insufficient for making a definitive choice. Besides, in Section 4.5, we utilize TIPI and Mini-PIPI for comparison. The details of these two scales are listed in Table 8 and Table 9, respectively."
        }
    ],
    "title": "PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection",
    "year": 2023
}