{
    "abstractText": "In a practical dialogue system, users may input out-of-domain (OOD) queries. The Generalized Intent Discovery (GID) task aims to discover OOD intents from OOD queries and extend them to the in-domain (IND) classifier. However, GID only considers one stage of OOD learning, and needs to utilize the data in all previous stages for joint training, which limits its wide application in reality. In this paper, we introduce a new task, Continual Generalized Intent Discovery (CGID), which aims to continuously and automatically discover OOD intents from dynamic OOD data streams and then incrementally add them to the classifier with almost no previous data, thus moving towards dynamic intent recognition in an open world. Next, we propose a method called Prototypeguided Learning with Replay and Distillation (PLRD) for CGID, which bootstraps new intent discovery through class prototypes and balances new and old intents through data replay and feature distillation. Finally, we conduct detailed experiments and analysis to verify the effectiveness of PLRD and understand the key challenges of CGID for future research.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaoshuai Song"
        },
        {
            "affiliations": [],
            "name": "Yutao Mou"
        },
        {
            "affiliations": [],
            "name": "Keqing He"
        },
        {
            "affiliations": [],
            "name": "Yueyan"
        },
        {
            "affiliations": [],
            "name": "Qiu"
        },
        {
            "affiliations": [],
            "name": "Pei Wang"
        },
        {
            "affiliations": [],
            "name": "Weiran Xu"
        }
    ],
    "id": "SP:061def53ce38923162a1e47edf991ca7ebdc3719",
    "references": [
        {
            "authors": [
                "Magdalena Biesialska",
                "Katarzyna Biesialska",
                "Marta R Costa-Jussa."
            ],
            "title": "Continual lifelong learning in natural language processing: A survey",
            "venue": "arXiv preprint arXiv:2012.09823.",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin."
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in neural information processing systems, 33:9912\u20139924.",
            "year": 2020
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Tem\u010dinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38\u201345, On-",
            "year": 2020
        },
        {
            "authors": [
                "Qian Chen",
                "Zhu Zhuo",
                "Wen Wang."
            ],
            "title": "Bert for joint intent classification and slot filling",
            "venue": "arXiv preprint arXiv:1902.10909.",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Marco Cuturi."
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Advances in neural information processing systems, 26.",
            "year": 2013
        },
        {
            "authors": [
                "Guanting Dong",
                "Zechen Wang",
                "Liwen Wang",
                "Daichi Guo",
                "Dayuan Fu",
                "Yuxiang Wu",
                "Chen Zeng",
                "Xuefeng Li",
                "Tingfeng Hui",
                "Keqing He",
                "Xinyue Cui",
                "Qixiang Gao",
                "Weiran Xu"
            ],
            "title": "2023a. A prototypical semantic decoupling method via joint contrastive learning",
            "year": 2023
        },
        {
            "authors": [
                "Guanting Dong",
                "Zechen Wang",
                "Jinxu Zhao",
                "Gang Zhao",
                "Daichi Guo",
                "Dayuan Fu",
                "Tingfeng Hui",
                "Chen Zeng",
                "Keqing He",
                "Xuefeng Li",
                "Liwen Wang",
                "Xinyue Cui",
                "Weiran Xu"
            ],
            "title": "2023b. A multi-task semantic decomposition framework with task-specific pre-training",
            "year": 2023
        },
        {
            "authors": [
                "Binzong Geng",
                "Min Yang",
                "Fajie Yuan",
                "Shupeng Wang",
                "Xiang Ao",
                "Ruifeng Xu."
            ],
            "title": "Iterative network pruning with uncertainty regularization for lifelong sentiment classification",
            "venue": "Proceedings of the 44th International ACM SIGIR conference on Research",
            "year": 2021
        },
        {
            "authors": [
                "Ashraful Islam",
                "Chun-Fu Richard Chen",
                "Rameswar Panda",
                "Leonid Karlinsky",
                "Richard Radke",
                "Rogerio Feris."
            ],
            "title": "A broad study on the transferability of visual representations with contrastive learning",
            "venue": "Proceedings of the IEEE/CVF International Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Zixuan Ke",
                "Hu Xu",
                "Bing Liu."
            ],
            "title": "Adapting BERT for continual learning of a sequence of aspect sentiment classification tasks",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Stefan Larson",
                "Anish Mahendran",
                "Joseph J. Peper",
                "Christopher Clarke",
                "Andrew Lee",
                "Parker Hill",
                "Jonathan K. Kummerfeld",
                "Kevin Leach",
                "Michael A. Laurenzano",
                "Lingjia Tang",
                "Jason Mars"
            ],
            "title": "An evaluation dataset for intent classification and out-of",
            "year": 2019
        },
        {
            "authors": [
                "Guodun Li",
                "Yuchen Zhai",
                "Qianglong Chen",
                "Xing Gao",
                "Ji Zhang",
                "Yin Zhang."
            ],
            "title": "Continual few-shot intent detection",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 333\u2013343, Gyeongju, Republic of Korea. Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Pan Zhou",
                "Caiming Xiong",
                "Steven CH Hoi."
            ],
            "title": "Prototypical contrastive learning of unsupervised representations",
            "venue": "arXiv preprint arXiv:2005.04966.",
            "year": 2020
        },
        {
            "authors": [
                "Ting-En Lin",
                "Hua Xu."
            ],
            "title": "Deep unknown intent detection with margin loss",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5491\u20135496, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Ting-En Lin",
                "Hua Xu",
                "Hanlei Zhang."
            ],
            "title": "Discovering new intents via constrained deep adaptive clustering with cluster refinement",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8360\u20138367.",
            "year": 2020
        },
        {
            "authors": [
                "Tingting Ma",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Tiejun Zhao",
                "Chin-Yew Lin."
            ],
            "title": "Decomposed metalearning for few-shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1584\u20131596, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "J MacQueen."
            ],
            "title": "Some methods for classification and analysis of multivariate observations",
            "venue": "Proc. 5th Berkeley Symposium on Math., Stat., and Prob, page 281.",
            "year": 1965
        },
        {
            "authors": [
                "Marc Masana",
                "Xialei Liu",
                "Bart\u0142omiej Twardowski",
                "Mikel Menta",
                "Andrew D Bagdanov",
                "Joost van de Weijer."
            ],
            "title": "Class-incremental learning: survey and performance evaluation on image classification",
            "venue": "IEEE Transactions on Pattern Analysis and Machine",
            "year": 2022
        },
        {
            "authors": [
                "Sanket Vaibhav Mehta",
                "Darshan Patil",
                "Sarath Chandar",
                "Emma Strubell."
            ],
            "title": "An empirical investigation of the role of pre-training in lifelong learning",
            "venue": "arXiv preprint arXiv:2112.09153.",
            "year": 2021
        },
        {
            "authors": [
                "Yutao Mou",
                "Keqing He",
                "Pei Wang",
                "Yanan Wu",
                "Jingang Wang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Watch the neighbors: A unified k-nearest neighbor contrastive learning framework for OOD intent discovery",
            "venue": "Proceedings of the 2022 Conference on Empirical",
            "year": 2022
        },
        {
            "authors": [
                "Yutao Mou",
                "Keqing He",
                "Yanan Wu",
                "Pei Wang",
                "Jingang Wang",
                "Wei Wu",
                "Yi Huang",
                "Junlan Feng",
                "Weiran Xu."
            ],
            "title": "Generalized intent discovery: Learning from open world dialogue system",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Yutao Mou",
                "Keqing He",
                "Yanan Wu",
                "Zhiyuan Zeng",
                "Hong Xu",
                "Huixing Jiang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Disentangled knowledge transfer for OOD intent discovery with unified contrastive learning",
            "venue": "Proceedings of the 60th Annual Meeting of the As-",
            "year": 2022
        },
        {
            "authors": [
                "John Platt"
            ],
            "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
            "venue": "Advances in large margin classifiers, 10(3):61\u201374.",
            "year": 1999
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Alexander Kolesnikov",
                "Georg Sperl",
                "Christoph H Lampert."
            ],
            "title": "icarl: Incremental classifier and representation learning",
            "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001\u20132010.",
            "year": 2017
        },
        {
            "authors": [
                "Haobo Wang",
                "Ruixuan Xiao",
                "Yixuan Li",
                "Lei Feng",
                "Gang Niu",
                "Gang Chen",
                "Junbo Zhao."
            ],
            "title": "Pico: Contrastive label disambiguation for partial label learning",
            "venue": "arXiv preprint arXiv:2201.08984.",
            "year": 2022
        },
        {
            "authors": [
                "Yanan Wu",
                "Zhiyuan Zeng",
                "Keqing He",
                "Yutao Mou",
                "Pei Wang",
                "Weiran Xu."
            ],
            "title": "Distribution calibration for out-of-domain detection with Bayesian approximation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Zhi-Fan Wu",
                "Tong Wei",
                "Jianwen Jiang",
                "Chaojie Mao",
                "Mingqian Tang",
                "Yu-Feng Li."
            ],
            "title": "Ngc: A unified framework for learning with open-world noisy data",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 62\u201371.",
            "year": 2021
        },
        {
            "authors": [
                "Jingkang Yang",
                "Kaiyang Zhou",
                "Yixuan Li",
                "Ziwei Liu."
            ],
            "title": "Generalized out-of-distribution detection: A survey",
            "venue": "arXiv preprint arXiv:2110.11334.",
            "year": 2021
        },
        {
            "authors": [
                "Lu Yu",
                "Bartlomiej Twardowski",
                "Xialei Liu",
                "Luis Herranz",
                "Kai Wang",
                "Yongmei Cheng",
                "Shangling Jui",
                "Joost van de Weijer."
            ],
            "title": "Semantic drift compensation for class-incremental learning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and",
            "year": 2020
        },
        {
            "authors": [
                "Weihao Zeng",
                "Keqing He",
                "Zechen Wang",
                "Dayuan Fu",
                "Guanting Dong",
                "Ruotong Geng",
                "Pei Wang",
                "Jingang Wang",
                "Chaobo Sun",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Semi-supervised knowledge-grounded pre-training for task-oriented dialog systems",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyuan Zeng",
                "Keqing He",
                "Yuanmeng Yan",
                "Zijun Liu",
                "Yanan Wu",
                "Hong Xu",
                "Huixing Jiang",
                "Weiran Xu."
            ],
            "title": "Modeling discriminative representations for out-of-domain detection with supervised contrastive learning",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Mou"
            ],
            "title": "2022b), we use SGD with momentum as the optimizer with linear warm-up and cosine annealing (warm-up ratio of 10%, momentum of 0.9, maximum learning rate of 0.1, and weight decay of 1.5e-4)",
            "venue": "For E2E,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The traditional intent classification (IC) in a taskoriented dialogue system (TOD) is based on a closed set assumption (Chen et al., 2019; Yang et al., 2021; Zeng et al., 2022) and can only handle queries within a limited scope of in-domain (IND) intents. However, users may input out-of-domain (OOD) queries in the real open world. Recently, the research community has paid more attention to OOD problems. OOD detection (Lin and Xu, 2019; Zeng et al., 2021; Wu et al., 2022; Mou et al., 2022d) aims to identify whether a user\u2019s query is outside the range of the predefined intent set to\n\u2217The first three authors contribute equally. Weiran Xu is the corresponding author.\n1We release our code at https://github.com/ songxiaoshuai/CGID\navoid wrong operations. It can safely reject OOD intents, but it also ignores OOD concepts that are valuable for future development. OOD intent discovery (Lin et al., 2020; Zhang et al., 2021; Mou et al., 2022c,a) helps determine potential development directions by grouping unlabelled OOD data into different clusters, but still cannot incrementally expand the recognition scope of existing IND classifiers. Generalized Intent Discovery (GID) (Mou et al., 2022b) further trains a network that can classify a set of labelled IND intent classes and simultaneously discover new classes from an unlabelled OOD set and incrementally add them to the classifier.\nAlthough GID realizes the incremental expansion of the recognition scope of the intent classifier without any new intents labels, two major problems limit the widespread application of GID in reality as shown in Fig 1: (1) GID only considers singlestage of OOD discovery and classifier expansion. In real scenarios, OOD data is gradually collected over time. Even if the current intent classifier is incrementally expanded, new OOD queries and intents will continue to emerge. Besides, the timeliness of OOD discovery needs to be considered:\ntimely discovery of new intents and expansion to the system can help improve the subsequent user experience. (2) GID require data in all previous stages for joint training to maintain the classification ability for known intents. Since OOD samples are collected from users\u2019 real queries, storing past data may bring serious privacy issues. In addition, unlike Class Incremental Learning (CIL) that require new classes with real labels, it is hard to obtain a large amount of dynamic labeled data in reality, and the label set for OOD queries is not predefined and needs to be mined from query logs.\nInspired by the above problems, in this paper, we introduce a new task, Continual Generalized Intent Discovery (CGID), which aims to continually and automatically discover OOD intents from OOD data streams and expand them to the existing IND classifier. In addition, CGID requires the system to maintain the ability to classify known intents with almost no need to store previous data, which makes existing GID methods fails to be applied to CGID. Through CGID, the IC system can continually enhance the ability of intent recognition from unlabeled OOD data streams, thus moving towards dynamic intent recognition in an open world. We show the difference between CGID and GID, as well as the CIL task in Fig 2 and then leave the definition and evaluation protocol in Section 2.\nAs CGID needs to continuously learn from unlabeled OOD data, it is foreseeable that the system will inevitably suffer from the catastrophic forgetting (Biesialska et al., 2020; Masana et al., 2022) of\nknown knowledge as well as the interference and propagation of OOD noise (Wu et al., 2021). To address the issues, we propose the Prototype-guided Learning with Replay and Distillation (PLRD) for CGID. Specifically, PLRD consists of a main module composed of an encoder and a joint classifier, as well as three sub-modules: (1) class prototype guides pseudo-labels for new OOD samples and alleviate the OOD noise; (2) feature distillation reduces catastrophic forgetting; (3) a memory balances new class learning and old class classification by replaying old class samples (Section 3) 2. Furthermore, to verify the effectiveness of PLRD, we construct two public datasets and three baseline methods for CGID. Extensive experiments prove that PLRD has significant performance improvement and the least forgetting compared to the baselines, and achieves a good balance among old classes classification, new class discovery and incremental learning (Section 4). To further shed light on the unique challenges faced by the CGID task, we conduct detailed qualitative analysis (Section 5). We find that the main challenges of CGID are conflicts between different sub-tasks, OOD noise propagation, fine-grained OOD classes and strategies for replayed samples (Section 6), which provide profound guidance for future work.\nOur contributions are three-fold: (1) We introduce a new task, Continual Generalized Intent Discovery (CGID), to achieve the dynamic and openworld intent recognition and then construct datasets and baselines for evaluating CGID. (2) We propose a practical method PLRD for CGID, which guides new samples through class prototypes and balances new and old tasks through data replay and feature distillation. (3) We conduct comprehensive experiments and in-depth analysis to verify the effectiveness of PLRD and understand the key challenges of CGID for future work."
        },
        {
            "heading": "2 Problem Definition",
            "text": "In this section, we first briefly introduce the Generalized Intent Discovery(GID) task, then delve into the details of the Continual Generalized Intent Discovery (CGID) task we proposed.\n2PLRD\u2019s memory mechanism stores only a tiny fraction of samples, offering a significant privacy advantage compared to GID, which stores all past data. PLRD serves not only to provide a privacy-conscious mode of learning but also takes into account the long-term stability of task performance. As for the methods that completely eliminate the need for prior data storage, we leave them for further exploration."
        },
        {
            "heading": "2.1 GID",
            "text": "Given a set of labeled in-domain data DIND = {(xINDi , yINDi )}ni=1 and unlabeled OOD data DOOD = {(xOODi )}mi=1, where yINDi \u2208 Y IND, Y IND = {1, 2, . . . , N}, GID aims to train a joint classifier to classify an input query into the total label set Y = {1, . . . , N,N + 1, . . . . . . , N + M}, where the first N elements represent the labeled IND classes and the last M elements represent newly discovered unlabeled OOD classes."
        },
        {
            "heading": "2.2 CGID",
            "text": "In contrast, CGID provides data and expands the classifier in a sequential manner, which is more in line with real scenarios.\nFirst, we define t \u2208 [0, T ], which denotes the current learning stage of CGID and T denotes the maximum number of learning stages of CGID. In the IND learning stage (t = 0), given a labeled in-domain dataset DIND = {(xINDi , yINDi )}, the model needs to classify IND classes to a predefined set Y0 = {1, 2, . . . , N} and learn representations that are also helpful for subsequent stages.\nThen, a series of unlabeled out-of-domain datasets {DOODt }Tt=1 are given in sequence, where DOODt = {x OODt i }. In the OOD learning stage t \u2208 [1, T ], the model is expected to discover new OOD classes Yt3 from DOODt and incrementally extend Yt to the classifier while maintaining the ability to classify known classes {Yi\u2264t}. The ultimate goal of CGID is to obtain a joint classifier that can classify queries into the total label set Y allT = Y0 \u222a Y1 \u222a . . . \u222a YT .\n3Estimating |Yt| is out of the scope of this paper. In the following experiment, we assume that |Yt| is ground-truth and provide an analysis in Section 5.5."
        },
        {
            "heading": "2.3 Evaluation Protocol",
            "text": "For CGID, we mainly focus on the classification performance along the training phase. Following (Mehta et al., 2021), we let at,i4 denote the accuracy on class set Yi after training on stage t. When t > 0, we calculate the accuracy At as follows:\nAINDt = at,0 A OOD t =\n1\n|{Y1\u2264i\u2264t}| t\u2211 i=1 |Yi|at,i\nAALLt = 1\n|Y allt | t\u2211 i=0 |Yi|at,i\n(1) Moreover, to measure catastrophic forgetting in CGID, we introduce the forgetting Ft as follows:\nF INDt = a0,0 \u2212 at,0\nFOODt = 1\n|{Y1\u2264i\u2264t}| t\u2211 i=1 |Yi|(ai,i \u2212 at,i)\nFALLt = 1\n|Y allt | t\u2211 i=0 |Yi|(ai,i \u2212 at,i)\n(2)\nOn the whole, AINDt and F IND t measure the extent of maintaining the IND knowledge, while AOODt and FOODt denote the ability to learn the OOD classes. AALLt and F ALL t are comprehensive metrics for CGID."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overall Architecture",
            "text": "As shown in Fig 3, our proposed PLRD framework consists of a main module which composed\n4Following (Zhang et al., 2021), we use the Hungarian algorithm (Kuhn, 1955) to obtain the mapping between the predicted OOD classes and ground-truth classes in the test set.\nof an encoder and a joint classifier and three submodules: (1) Memory module is responsible for replaying known class samples to balance the learning of new classes and maintain known classes; (2) Class prototype module is responsible for generating pseudo-labels for new OOD samples; (3) Feature distillation is responsible for alleviating catastrophic forgetting of old classes. The joint classifier h consists of an old class classification head hold and a new class classification head hnew, outputting logit l = [lold; lnew]. After stage t ends, lnew will be merged into lold, i.e., lold \u2190 [lold; lnew]. Then, when stage t+ 1 starts, a new head lnew with the dimension |Yt+1| will be created."
        },
        {
            "heading": "3.2 Memory for Data Replay",
            "text": "We equip a memory module M for PLRD. After each learning stage, M stores a very small number of training samples and replays old class samples in the next learning stage to prevent catastrophic forgetting and encourage positive transfer. Specifically, in the IND learning stage, we randomly select n samples for each IND class according to the ground-truth labels; in each OOD learning stage, since the ground-truth labels are unknown, we randomly select n samples5 for each new class according to the pseudo-labels and store them in M together with the pseudo-labels. In the new learning stage, for each batch, we randomly select old class samples {xold} with the same number as new class samples {xnew} from M and input them into the BERT encoder f(\u00b7) together with new class samples xnew, i.e., |{xnew}| = |{xold}|, {x} = {xnew} \u222a {xold}."
        },
        {
            "heading": "3.3 Prototype-guide Learning",
            "text": "Previous semantic learning studies (Yu et al., 2020; Wang et al., 2022; Ma et al., 2022; Dong et al., 2023a,b) have shown that learned representations can help to disambiguate the noisy sample labels and mitigate forgetting. Therefore, we build prototypes through a linear projection layer g(\u00b7) after the encoder. In stage t > 0, we first randomly initialize new class prototype \u00b5j , j \u2208 Yt.6 For sample xi \u2208 {x}, we use an |Y allt |-dimensional vector qi representing the probabilities of xi being assigned\n5In the following experiment, we set n=5 and analyze the effects of different n in the Section 5.3.\n6When t = 1, we additionally initialize the prototypes of IND classes.\nto all prototypes:\nqi = { onehot(yoldi ) xi \u2208 {xold}[ 0|Y allt\u22121| ; lnewi ] xi \u2208 {xnew}\n(3)\nwhere yoldi is the ground-truth or pseudo label of xi in M and 0|Y allt\u22121| is a |Y all t\u22121|-dimensional zero vector. Then we introduce prototypical contrastive learning (PCL) (Li et al., 2020) as follows:\nLpcl = \u2212 \u2211 i,j qji log exp(sim(zi, \u00b5j)/\u03c4\u2211 r exp(sim(zi, \u00b5r))/\u03c4 (4)\nwhere \u03c4 denotes temperature, qji is the j-th element of qi and zi = g(f(xi)). By pulling similar samples into the same prototype, PCL can learn clear intent representations for new classes and maintain representations for old classes. To further improve the generalization of representation, we also introduce the instance-level contrastive loss (Chen et al., 2020) to xi as follows:\nLins = \u2212 \u2211 i log exp(sim(zi, z\u0302i)/\u03c4)\u2211 j 1[i \u0338=j] exp(sim(zi, zj)/\u03c4)\n(5) where z\u0302i denotes the dropout-augmented view of zi. Next, we update all new and old prototypes in a sample-wise moving average manner to reduce the computational complexity following (Wang et al., 2022). For sample xi, prototype \u00b5j is updated as follows:\n\u00b5j = \u03b3\u00b5j + (1\u2212 \u03b3)zi (6)\nwhere the moving average coefficient \u03b3 is an adjustable hyperparameter and j is the index of the maximum element in qi.\nFinally, for the new sample xi \u2208 {xnew}, its pseudo label is assigned as the index of the nearest new class prototype to its representation zi. We optimize the joint classifier using cross-entropy Lce over both the new and replayed samples."
        },
        {
            "heading": "3.4 Feature Distillation",
            "text": "It can be expected that the encoder features may change significantly when updating the network parameters in the new learning stage. This means that the network tends to forget the knowledge learned from the old classes before and suffers from catastrophic forgetting. To further remember the knowledge in the non-forgotten features, we integrate the feature distillation into PLRD. Specifically, at the beginning of stage t, we copy and freeze the encoder, denoted as f init(\u00b7). Then given replayed samples xi \u2208 {xold} in a batch, we constrain the feature output f(xi) of the current encoder with the feature f init(xi). Formally, the feature distillation loss is as follows:\nLfd = |{xold}|\u2211 i=1 (f(xi)\u2212 f init(xi))2 (7)"
        },
        {
            "heading": "3.5 Overall Training",
            "text": "The total loss is defined as follows:\nL = Lce + Lpcl + Lins + Lfd (8)"
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We construct the CGID datasets based on two widely used intent classification datasets, Banking (Casanueva et al., 2020) and CLINC (Larson et al., 2019). Banking covers only a single domain, containing 13,083 user queries and 77 intents, while CLINC contains 22,500 queries covering 150 intents across 10 domains. For the CLINC and Banking datasets, we randomly select a specified proportion of all intent classes (about 40%, 60%, and 80%\nrespectively) as OOD types, with the rest being IND types. Furthermore, we assign the maximum stage T=3, so we divide the OOD data into three equal parts for each OOD training stage. We show the number of classes at each stage in Table 1 and leave the detailed statistics in Appendix A."
        },
        {
            "heading": "4.2 Baselines",
            "text": "Since this is the first study on CGID, there are no existing methods that solve exactly the same task. We adopt three prevalent methods in OOD discovery and GID, and extend them to the CGID setting to develop the following competitive baselines7. \u2022 K-means is a pipeline baseline which first use the\nclustering algorithm K-means (MacQueen, 1965) to cluster the new samples to obtain pseudo labels and then combine these samples and replayed samples in the memory to train the joint classifier at each OOD training stage.\n\u2022 DeepAligned is another pipeline baseline that leverages the iterative clustering algorithm DeepAligned (Zhang et al., 2021). At each OOD training phase, DeepAligned iteratively clusters the new data and then utilizes them along with the replayed samples for classification training.\n\u2022 E2E is an end-to-end baseline. At each OOD training stage, E2E (Mou et al., 2022b) amalgamates the new instances and replayed samples and then obtain the logits through the encoder and joint classifier. The model is optimized with a unified classification loss, where the new OOD pseudo-labels are obtained by swapping predictions (Caron et al., 2020).\n7We leave the detailed implementation of these baselines in Appendix B."
        },
        {
            "heading": "4.3 Main Results",
            "text": "We conduct experiments on Banking and CLINC with three different OOD ratios, as shown in Tables 2 and 3. In general, our proposed PLRD consistently outperform all the baselines with a large margin. Next, we analyze the results from three aspects:\n(1) Comparison of different methods We observe that DeepAligned roughly achieves the best IND performance while E2E has the best OOD performance among all baselines. However, our proposed PLRD consistently outperforms all baselines significantly in both IND and OOD, achieving best performance and new-old task balance. Specifically, under the average of three ratios, PLRD is better than the optimal baseline by 7.57% (AINDT ), 1.09% (AOODT ), and 4.06% (A ALL T ) on Banking, and by 3.35% (AINDT ), 0.04% (A OOD T ), and 2.13% (AALLT ) on CLINC. As for forgetting, E2E experiences a substantial performance drop on old classes when learning new classes, while PLRD is lower than the optimal baseline by 3.72%, 1.69% (FALLT ) on Banking and CLINC respectively. This indicates PLRD does not sacrifice too much performance on old classes when learning new classes and has the least forgetting among all methods.\n(2) Comparison of different datasets We validate the effectiveness of our method on different datasets, where CLINC is multi-domain and coarsegrained while Banking contains more fine-grained intent types within a single domain. We see that the performance of all methods on CLINC is significantly better than that on Banking. For example, PLRD is 11.72% (AALLT ) higher on CLINC than on Banking at an OOD ratio of 60%. In addition, at the same OOD ratio, PLRD shows an average increase of 7.53% (F INDT ), 6.45% (F OOD T ), and 6.57% (FALLT ) on Banking over CLINC. We believe this could be because fine-grained new and old classes\nare easily confused, which leads to serious new-old task conflicts and high forgetting. However, PLRD achieves larger improvements than baselines on Banking, indicating that PLRD can better cope with challenges in fine-grained intent scenarios.\n(3) Effect of different OOD ratios We observe that as the OOD ratio increases, the forgetting of IND classes increases and accuracy of OOD classes decreases significantly for all methods. For PLRD, when the OOD ratio increases from 40% to 80% on Banking, F INDT rises from 9.64% to 19.80%, and AOODT drops from 76.70% to 63.19%. Intuitively, more OOD classes make it challenging to distinguish samples from different distributions, leading to noisy pseudo-labels. Moreover, in the incremental process, more OOD classes will update the model to a greater extent, resulting in more IND knowledge forgetting."
        },
        {
            "heading": "5 Qualitative Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Representation Learning",
            "text": "In order to better understand the evolution of CGID along different training stages, we visualize the intent representations after each training stage for PLRD in Fig 4. It can be seen that in the IND learn-\ning stage, the IND classes form compact clusters, while the OOD samples are scattered in space. As the stage progresses, the gray points are gradually colored and move from dispersion to aggregation, indicating that new OOD classes continue to be discovered and learned good representations. In addition, the already aggregated clusters are gradually dispersed (see \"red\" points), indicating that the representations of old classes are deteriorating.\nNext, to quantitatively measure the quality of representations, we calculate the intra-class and inter-class distances and use the ratio of inter-class distance to intra-class distance as the compactness following (Islam et al., 2021). We report the compactness and accuracy in Fig 5. It can see that the compactness of OOD classes is much lower than that of IND classes, indicating that the representation learning with labeled IND samples outperforms that with unlabeled OOD samples. As the stage t increases, the compactness of the IND classes gradually decreases. And the compactness of the Yi(i > 0) classes increases significantly when t equals i, and then gradually decreases. This demonstrates the learning and forgetting effects in CGID from a representation perspective. Furthermore, we observe that the maximal compactness of Yi decreases as i increases, showing that the learning ability of new classes gradually declines. We attribute this to the noise in the OOD pseudolabeled data and the greater need to suppress forgetting of more old classes. Finally, the trend of accuracy and compactness remains consistent, suggesting that representation is closely related to the classification performance of CGID."
        },
        {
            "heading": "5.2 Loss and Gain of CGID",
            "text": "During the CGID process, the performance of the classifier on IND classes gradually declined, while\nthe number of supported OOD classes continually expanded. In order to quantify the change of the classifier, we define the Loss and the Gain in stage t for CGID as follows:\nLosst = \u2212F INDt AIND0 Gaint = |Y allt |AALLt |Y0|AIND0 \u2212 1 (9) We illustrate the variations in Loss and Gain of all methods over stages in Fig 6. The results show that as the training progresses, the Loss of all methods decreases overall and the Gain increases continuously. After finishing the training, although the Loss decrease by 20% roughly, the increase in Gain of PLRD is greater, reaching over 200%. This indicates that the Gain generated by CGID is much higher than the Loss and brings positive effects to the classifier as a whole. Compared with other methods, PLRD has the lowest Loss and highest Gain at each stage, and its advantage continuous amplifies over stages. These further consolidate the conclusion that PLRD outperforms the baselines."
        },
        {
            "heading": "5.3 Effect of Replaying Samples in Memory",
            "text": "In this section, we explore the effect of replayed samples from both selection strategies and quantity.\nSelection Strategy In the CIL task, since the\nsamples are labeled, we only need to consider the diversity of replayed samples. However, in CGID, we need to take the quality of pseudo-labels into account additionally. We explore three selection strategies for replaying samples: random (randomly sampling from training set), icarl (selecting these closest to their prototypes, following Rebuffi et al. (2017)), and icarl_contrary (select the samples farthest from their prototypes). As shown in Table 4, we report the pseudo-label accuracy (Acc) and average feature variance (Var) of replayed samples, as well as the final classifier accuracy of PLRD. We can see that icarl has the highest pseudo-label accuracy while icarl_contrary has the largest sample variance and is inclined to diversity. However, PLRD under the random strategy has the highest OOD and ALL accuracy. This demonstrates that neither accuracy nor diversity alone leads to better performance. CGID needs to strike a balance between diversity and accuracy of the replayed samples.\nQuantity of Replayed Samples Fig 7 illustrates the effect of replaying different numbers of previous examples. It is evident that replaying more previous examples leads to higher accuracy. Compared with replaying no examples (n=0), storing just one example for each old class (n=1) significantly improves accuracy, demonstrating that replaying old samples is crucial. In addition, PLRD outperforms the baselines significantly when n \u2264 10, proving PLRD\u2019s effectiveness with few-shot samples replay. However, when all previous examples are replayed (n=ALL), PLRD performs slightly worse than E2E. We believe this is because PLRD\u2019s anti-forgetting mechanism limits learning new classes lightly, and replaying all previous examples deviates from the setting of CGID."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "As reported in Table 5, we perform ablation study to investigate the effect of each learning objective\non the PLRD framework. When removing Lfd, the performance declines significantly in both IND and OOD classes. This suggests that forgetting is one of the main challenges faced by CGID, and mitigating forgetting can bring positive effects to continual OOD learning stage by retaining prior knowledge. In addition, removing Lins and Lpcl respectively leads to a certain degree of performance decline, indicating that prototype and instance-level contrastive learning are helpful for OOD discovery and relieving OOD noise. Finally, only retaining the Lce of PLRD will result in the largest accuracy decline, proving the importance of multiple optimization objectives in PLRD."
        },
        {
            "heading": "5.5 Estimate the Number of OOD intents",
            "text": "In the previous experiments, we assumed that the number of new OOD classes at each stage is predefined and is ground-truth. However, in practical applications, the number of new classes usually needs to be estimated automatically. We adopt the same estimation algorithm as Zhang et al. (2021); Mou et al. (2022b). 8. Since the estimation algorithm is based on sample features, we use the model itself as the feature extractor at the beginning of each OOD learning stage. As shown in Table 6, when the estimated number of classes is inaccurate, the performance of all methods declines to some extent. However, PLRD can estimate the number most accurately and achieve the best performance. Then, in order to align different methods, we consistently use the frozen model after finishing the IND training stage as the feature extractor for subsequent stages. With the same estimation quality, PLRD still significantly outperforms each baseline, demonstrating that PLRD is robust.\n8We leave the details of the algorithm in Appendix C."
        },
        {
            "heading": "6 Challenges",
            "text": "Based on the above experiments and analysis, we comprehensively summarize the unique challenges faced by CGID :\nConflicts between different sub-tasks In CGID, the discovery and classification of new OOD classes tend towards different features, and learning new OOD classes interfere with existing knowledge about old classes inevitably. However, preventing forgetting will lead to model rigidity and is not conducive to the learning of new classes.\nOOD noise accumulation and propagation In the continual OOD learning stage, using pseudolabeled OOD samples with noise to fine-tune the model as well as replaying samples with noise will cause the noise to accumulate and spread to the learning of new OOD samples in future stages. This will potentially affect the model\u2019s ability to learn effectively from new OOD samples in subsequent stages of learning.\nFine-grained OOD classes Section 4.3 indicate that fine-grained data leads to high forgetting and poor performance. We believe this is because finegrained new classes and old classes are easily confused, which brings serious conflicts between new and old tasks.\nStrategy for replayed samples The experiment in Section 5.3 shows that CGID needs to consider the trade-off between replay sample diversity and accuracy as well as the trade-off between quantity of replayed samples and user privacy.\nContinual quantity estimation of new classes Section 5.5 shows that even minor estimation errors for each stage can accumulate over stages, leading to severely biased estimation and deteriorated performance."
        },
        {
            "heading": "7 Related Work",
            "text": "OOD Intent Discovery OOD Intent Discovery aims to discover new intent concepts from unlabeled OOD data. Unlike simple text clustering tasks, it considers how to leverage IND prior to enhance the discovery of unknown OOD intents. Lin et al. (2020) use OOD representations to compute similarities as weak supervision signals. Zhang et al. (2021) propose an iterative method, DeepAligned, that performs representation learning and clustering assignment iteratively while Mou et al. (2022c) perform contrastive clustering to jointly learn representations and clustering assignments. Nevertheless, it\u2019s essential to note\nthat OOD intent discovery primarily focus on unveiling new intents, overlooking the integration of these newfound, unknown intents with the existing, well-defined intent categories.\nGeneralized Intent Discovery To overcome the limitation of OOD intent discovery that cannot expand the existing classifier, Mou et al. (2022b) proposed the Generalized Intent Discovery (GID) task. GID takes both labeled IND data and unlabeled OOD data as input and performs joint classification over IND and OOD intents. As such, GID needs to discover semantic concepts from unlabeled OOD data and learn joint classification. However, GID can only perform one-off OOD learning stage and requires full data of known classes, severely limiting its practical use. Therefore, we introduce Continual Generalized Intent Discovery (CGID) to address the challenges of dynamic and continual open-world intent classification.\nClass Incremental Learning The primary goal of class-incremental learning (CIL) is to acquire knowledge about new classes while preserving the information related to the previously learned ones, thereby constructing a unified classifier. Earlier studies (Ke et al., 2021; Geng et al., 2021; Li et al., 2022) mainly focused on preventing catastrophic forgetting and efficient replay in CIL. However, these studies assumed labeled data streams, whereas in reality large amounts of continuously annotated data are hard to obtain and the label space is undefined. Unlike CIL, CGID charts a distinct course by continuously identifying and assimilating new classes from unlabeled OOD data streams. This task presents a set of formidable challenges compared to conventional CIL."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we introduce a more challenging yet practical task as Continuous Generalized Intent Discovery (CGID), which aims at continuously and automatically discovering new intents from OOD data streams and incrementally extending the classifier, thereby enabling dynamic intent recognition in an open-world setting. To address this task, we propose a new method called Prototype-guided Learning with Replay and Distillation (PLRD). Extensive experiments and qualitative analyses validate the effectiveness of PLRD and provide insights into the key challenges of CGID.\nLimitations\nThis paper proposes a new task as Continual General Intent Discovery (CGID) aimed at continually and automatically discovering new intents from unlabeled out-of-domain (OOD) data and incrementally expand them to the existing classifier. Furthermore, a practical method Prototype-guided Learning with Replay and Distillation (PLRD) is proposed for CGID. However, there are still several directions to be improved: (1) Although PLRD achieves better performance than each baseline, the performance still has a large gap to improve compared with the theoretical upper bound of a model without forgetting previous knowledge. (2) In addition, all baselines and PLRD use a small number of previous samples for replay. The CGID method without utilizing any previous samples is not explored in this paper and can be a direction for future work. (3) Although PLRD does not generate additional overhead during inference, it requires maintaining prototypes and a frozen copy of the encoder during training, resulting in additional resource occupancy. This can be further optimized in future work."
        },
        {
            "heading": "A Datasets",
            "text": "We present detailed statistics for the original dataset Banking and CLINC in Table 7. Then, we show statistics of the CGID datasets that are constructed based on Banking and CLINC in Table 8. Since we conduct three random partitions of classes under each OOD ratio and Banking is class-imbalanced, we report the the average number of samples.\nB Implementation Details\nTo ensure a fair comparison for PLRD and all baselines, we consistently use the pre-trained BERT model (BERT-base uncased 9, with 12 layer transformer) as the network backbone and add a pooling layer to obtain the intent representation (dimension = 768). In addition, we freeze all but the last transformer layer parameters to achieve better performance with BERT backbone, and speed up the training process as suggested in Lin et al. (2020). We use the same IND training phase and Memory mechanism for all methods. In the IND training stage, following Zhang et al. (2021), we adopt Adam with linear warm-up as the optimizer, with a batch size of 128 and a learning rate of 5e5, and select the best checkpoint according to the accuracy of validation set.\nDuring the continual OOD training stages, for the CGID baseline, we follow the hyperparameter settings of the original method as much as possible. Specifically, for DeepAligned and K-means, following Zhang et al. (2021), we adopt Adam with linear warm-up as the optimizer and set the training batch size to 128, the learning rate to 5e-4. In addition, we set the weight coefficient \u03bb =3 of the crossentropy loss corresponding to the replay samples, which is obtained by grid search \u03bb \u2208 {1, 2, 3, 4, 5}. For E2E, following Mou et al. (2022b), we use SGD with momentum as the optimizer with linear warm-up and cosine annealing (warm-up ratio of 10%, momentum of 0.9, maximum learning rate of 0.1, and weight decay of 1.5e-4). In addition, the temperature coefficient of cross-entropy is 0.1\n9https://github.com/google-research/bert\nand multi-head clustering (number of heads is 4) is used to improve performance for E2E.\nFor all experiments of PLRD, we also use SGD with momentum as the optimizer (warm-up ratio is 10%, momentum is 0.9, maximum learning rate is 0.01, and weight decay of 1.5e-4). Class prototype embedding (dimension=128) is obtained by a linear projection layer through the output feature (dimension=768) of the encoder. We set the temperature \u03c4 of prototype and instance-level contrastive learning to 0.5. For the construction of the augmented examples, we set the dropout value is 0.5. Following (Mou et al., 2022b), we also calibrate the logit lnewi by the SK algorithm (Cuturi, 2013) when assigning prototypes. We set the corresponding hyperparameters such as number of iteration is 3 and \u03f5=0.05 as same as E2E. When OOD ratio=40% or 60%, the moving average coefficient \u03b3=0.7, and when OOD ratio=80%, \u03b3=0.9. We believe that high OOD ratio will lead to high forgetting, while larger moving average coefficient can mitigate this by helping the model remember learned prototypes.\nFor all methods, we train 200 epochs for each OOD learning stage to achieve sufficient convergence. The trainable model parameters of PLRD are almost consistent with the baseline (approximately 9.1M). However, PLRD adopts prototype learning and a frozen BERT for knowledge distillation, , which leads to additional memory occupation and training computation. It should be noted that PLRD only needs the classifier branch in the inference stage, so there is no additional computational and spatial overhead. All experiments use a single Nvidia RTX 3090 GPU (24 GB memory)."
        },
        {
            "heading": "C The algorithm of estimating the number of new classes",
            "text": "We follow the estimation algorithm (Zhang et al., 2021) to estimate the number of new intents for each OOD stage. Specifically, We assign a big K \u2032 as the number of clusters (In this paper, it is twice the number of ground-truth classes) at the beginning of each OOD stage. As a good feature initialization is helpful for partition-based methods (e.g., K-means) (Platt et al., 1999), we use the encoder f(\u00b7) to extract intent features of all new training examples. Then, we perform K-means with the extracted features. We suppose that real clusters tend to be dense even with K \u2032 , and the size of more confident clusters is larger than some threshold t. Therefore, we drop the low confidence\ncluster which size smaller than t, and calculate K with:\nK = K \u2032\u2211 i=1 \u03b4(|Si| \u2265 t) (10)\nwhere |Si| is the size of the ith produced cluster, and \u03b4(\u00b7) is an indicator function. It outputs 1 if the condition is satisfied, and outputs 0 if not. Notably, we assign the threshold t as the expected cluster mean size N\nK\u2032 in this formula."
        }
    ],
    "title": "Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition",
    "year": 2023
}