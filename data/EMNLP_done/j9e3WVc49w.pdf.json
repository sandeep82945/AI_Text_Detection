{
    "abstractText": "Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest argument of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.",
    "authors": [
        {
            "affiliations": [],
            "name": "Md Arafat Sultan"
        }
    ],
    "id": "SP:a0551c740b4a0eb7ad8e58e3f260a464b5acabb9",
    "references": [
        {
            "authors": [
                "Defang Chen",
                "Jian-Ping Mei",
                "Yuan Zhang",
                "Can Wang",
                "Zhe Wang",
                "Chun Chen"
            ],
            "title": "Cross-Layer Distillation with Semantic Calibration",
            "year": 2021
        },
        {
            "authors": [
                "Tianlong Chen",
                "Zhenyu Zhang",
                "Sijia Liu",
                "Shiyu Chang",
                "Zhangyang Wang."
            ],
            "title": "Robust Overfitting may be Mitigated by Properly Learned Smoothening",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer."
            ],
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "venue": "arXiv preprint arXiv:2305.14314.",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "William B Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically Constructing a Corpus of Sentential Paraphrases",
            "venue": "Proceedings of the International Workshop on Paraphrasing.",
            "year": 2005
        },
        {
            "authors": [
                "Bin Dong",
                "Jikai Hou",
                "Yiping Lu",
                "Zhihua Zhang."
            ],
            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
            "venue": "arXiv prePrintt arXiv:1910.01255.",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the Knowledge in a Neural Network",
            "venue": "NeurIPS Deep Learning Worksop.",
            "year": 2014
        },
        {
            "authors": [
                "Dongkyu Lee",
                "Ka Chun Cheung",
                "Nevin L.Zhang."
            ],
            "title": "Adaptive Label Smoothing with SelfKnowledge in Natural Language Generation",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Kevin J Liang",
                "Weituo Hao",
                "Dinghan Shen",
                "Yufan Zhou",
                "Weizhu Chen",
                "Changyou Chen",
                "Lawrence Carin."
            ],
            "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Aditya K Menon",
                "Ankit Singh Rawat",
                "Sashank Reddi",
                "Seungyeon Kim",
                "Sanjiv Kumar."
            ],
            "title": "A Statistical Perspective on Distillation",
            "venue": "ICML.",
            "year": 2021
        },
        {
            "authors": [
                "Hossein Mobahi",
                "Mehrdad Farajtabar",
                "Peter L. Bartlett."
            ],
            "title": "Self-Distillation Amplifies Regularization in Hilbert Space",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev abd",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
            "venue": "ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter",
            "venue": "EMC\u22272 at NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqiang Shen",
                "Zechun Liu",
                "Dejia Xu",
                "Zitian Chen",
                "Kwang-Ting Cheng",
                "Marios Savvides."
            ],
            "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank",
            "venue": "EMNLP.",
            "year": 2013
        },
        {
            "authors": [
                "Md Arafat Sultan",
                "Avirup Sil",
                "Radu Florian."
            ],
            "title": "Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
            "venue": "NAACL.",
            "year": 2018
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-Art Natural Language Processing",
            "venue": "EMNLP (System Demonstrations).",
            "year": 2020
        },
        {
            "authors": [
                "Li Yuan",
                "Francis EH Tay",
                "Guilin Li",
                "Tao Wang",
                "Jiashi Feng."
            ],
            "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Zhilu Zhang",
                "Mert R. Sabuncu."
            ],
            "title": "SelfDistillation as Instance-Specific Label Smoothing",
            "venue": "NeurIPS.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge distillation (KD) was originally proposed as a mechanism for a small and lightweight student model to learn to perform a task, e.g., classification, from a higher-capacity teacher model (Hinton et al., 2014). In recent years, however, this view of KD as a knowledge transfer process has come into question, with a number of studies finding that it has features characteristic of a regularizer (Yuan et al., 2020; Zhang and R. Sabuncu, 2020; Dong et al., 2019). Perhaps the most direct argument for this new position comes from Yuan et al. (2020), who show that KD has interesting similarities with label smoothing (LS) (Szegedy et al., 2016). One of their main conclusions, that \u201cKD is a type of learned LS regularization,\u201d has subsequently been reiterated by many (Menon et al., 2021; Chen et al., 2021a; Shen et al., 2021; Chen et al., 2021b) and remains an important part of our understanding of KD to this day.\nThis is a rather curious suggestion, however, given the starkly different purposes for which the two methods were originally developed: LS was\ndesigned \u201cfor encouraging the model to be less confident\u201d (Szegedy et al., 2016) by imposing a uniform prior on the target distribution; the resulting new distribution is intended to simply prevent overfitting without revealing any new details about the task. KD, on the other hand, was designed to explicitly teach the student better inter-class relationships for classification, so that \u201ca BMW, for example, may only have a very small chance of being mistaken for a garbage truck, but that mistake is still many times more probable than mistaking it for a carrot\u201d (Hinton et al., 2014).\nSuch different motivations of the two methods prompt us to re-examine the claim of their equivalence in this paper, where we find the underlying argument to be essentially flawed. With a growing dependence in key areas like NLP on increasingly large models, KD and other model compression methods are becoming more relevant than ever before (Wang et al., 2023; Taori et al., 2023; Dettmers et al., 2023). It is therefore crucial that we also interpret such methods correctly to help steer their future research in the right direction, which is a primary goal of this work.\nKD and LS do indeed share the common property that they both substitute one-hot training labels with soft target distributions. The key question for us, however, is whether the soft teacher estimates in KD simply regularize training like the smoothed labels in LS do, or if they present a detailed and finergrained view of the task, as originally intended. Yuan et al. (2020) argue in favor of the former based on observations such as (a) similarities in the cost functions of KD and LS, and (b) strong generalization of distilled image classifiers independent of the level of expertise of their teachers.\nHere we first analyze their arguments and point out why they may be inadequate for a \u201cKD \u2248 LS\u201d assertion (\u00a72, 3). Then we present empirical evidence on four text classification tasks from GLUE (Wang et al., 2019) that KD lacks the defining prop-\nerty of LS in most settings. Concretely, relative to standard empirical risk minimization (ERM) over one-hot expert labels, LS by design increases the uncertainty (entropy) in the trained model\u2019s posterior over the associated categories. In other words, it always reduces the model\u2019s confidence in its predictions, as expected. The effect of KD on the student\u2019s posterior, on the other hand, is a function of the relative capacities of the teacher and the student: In the more common setting where a larger teacher is distilled into a smaller student and also in self-distillation, KD almost always reduces entropy over ERM, while also improving accuracy. It is only when the student is larger than its teacher that we see a small increase in entropy\u2014still much less than in LS\u2014along with a smaller gain in accuracy. These results clearly show that unlike LS, KD does not operate by merely making the trained model less confident in its predictions.\nThe following is a summary of our contributions: \u2022 We study the inner workings of KD in the rel-\natively under-explored modality of language, specifically on text classification tasks.\n\u2022 We show that KD lacks the defining property of LS regularization, rebutting the claim that the former is a form of the latter.\n\u2022 We demonstrate how the specific selection of the teacher model determines student confidence in KD, providing strong support for its classical knowledge transfer view."
        },
        {
            "heading": "2 The Argument for KD \u2248 LS",
            "text": "Let us first take a look at the main arguments, primarily from Yuan et al. (2020), for why KD should be considered a form of LS. Given a training instance x, let q(k|x) be the corresponding one-hot ground truth distribution over the different classes k \u2208 {1, ...,K}. LS replaces q(k|x) with the following smoothed mixture as the target for training:\nqls(k|x) = (1\u2212 \u03b1)q(k|x) + \u03b1u(k)\nwhere u(k) = 1/K is the uniform distribution over the K classes. Yuan et al. (2020) show that optimizing for qls is equivalent to minimizing the following cost function:\nLLS = (1\u2212 \u03b1)H(q, p) + \u03b1DKL(u, p) (1)\nwhere p(k|x) is the model output for x, and H and DKL are the cross-entropy and the KullbackLeibler divergence between the two argument distributions, respectively. Note that the H(q, p) term is\nthe same cross-entopy loss LERM that is typically minimized in standard empirical risk minimization (ERM) training of classifiers.\nKD, on the other hand, is defined directly as minimizing the following cost function:\nLKD = (1\u2212 \u03b1)H(q, p) + \u03b1DKL(pt, p) (2)\nwhere pt(k|x) is the posterior computed by a pretrained teacher t given x. The similarity in the forms of Eqs. 1 and 2 is one of the main reasons why Yuan et al. (2020) interpret KD \u201cas a special case of LS regularization.\u201d\nA second argument stems from their observation on a number of image classification datasets that a \u201cteacher-free KD\u201d method, which employs a smoothed target distribution as a virtual teacher, has similar performance as standard KD. Finally, some of their empirical observations, where KD improves student accuracy even when the teacher is a weak or poorly trained model, provide support for the more general view of KD as a form of regularization. Investigation of this general question can be found in others\u2019 work as well, e.g., in the context of multi-generational self-KD (Zhang and R. Sabuncu, 2020; Mobahi et al., 2020) and early stopping (Dong et al., 2019).\nFinally, a related line of work view KD as \u201cinstance-specific label smoothing\u201d, where the amount of smoothing is not uniform across categories, but is a function of the input approximated using a pre-trained network (Zhang and R. Sabuncu, 2020; Lee et al., 2022). We note that this is a fundamentally different assertion than KD being a form of LS with a uniform prior, since such methods do in fact bring new task-specific knowledge into the training process, with the pre-trained network essentially serving as a teacher model."
        },
        {
            "heading": "3 A Closer Look at the Arguments",
            "text": "As already stated, the focus of this paper is on the specific claim by Yuan et al. (2020) that KD is a form of LS with a uniform prior (Szegedy et al., 2016), a primary argument for which is that their cost functions in Eqs. 1 and 2 have a similar form. A careful examination of the two functions, however, reveals a key difference.\nTo elaborate, let H(.) represent the entropy of a probability distribution; it then follows that \u2200p \u2208 \u2206n\\{u}H(u) > H(p), where \u2206n is a probability simplex, u is the uniform distribution and p a\ndifferent distribution on \u2206n. Accordingly, minimizing DKL(u, p) in Eq. 1 unconditionally increases H(p), training higher-entropy models than ERM, a fact that we also confirm empirically in \u00a74.\nThe same is not true for DKL(pt, p) in Eq. 2, however, since pt is dependent on the specific selection of the teacher model t, and H(pt) can take on any value between 0 and H(u). Crucially, this means that H(pt) can be both higher or lower than H(p) if the two respective models were trained separately to minimize LERM . Unlike in LS, it therefore cannot be guaranteed in KD that minimizing DKL(pt, p) would increase H(p). In \u00a74, we show how the selection of the teacher model affects the predictive uncertainty of the student, for different combinations of their sizes. Based on the same results, we also comment on the teacher-free KD framework of Yuan et al. (2020)."
        },
        {
            "heading": "4 Experimental Results",
            "text": "Setup. We use four text classification tasks from GLUE (Wang et al., 2019) in our experiments: \u2022 MRPC (Dolan and Brockett, 2005) asks for a\nyes/no decision on if two given sentences are paraphrases of each other.\n\u2022 QNLI provides a question and a sentence from SQuAD (Rajpurkar et al., 2016) and asks if the latter answers the former (yes/no). \u2022 SST-2 (Socher et al., 2013) is a sentiment detection task involving positive/negative classification of movie reviews.\n\u2022 MNLI (Williams et al., 2018) is a 3-way (entailment/contradiction/neutral) classification task asking for whether a premise sentence entails a hypothesis sentence.\nFor each task, we use a train-validation-test split for training, model selection, and evaluation, respectively. See Appendix A.1 for more details.\nTo examine the effect of KD in depth, like Yuan et al. (2020), we run experiments under three different conditions: Standard KD distills from a finetuned BERT-base (Devlin et al., 2019) (BERT henceforth) classifier into a DistilBERT-base (Sanh et al., 2019) (DistilBERT henceforth) student; Self-KD distills from a DistilBERT teacher into a DistilBERT student; Reverse KD distills from a DistilBERT teacher into a BERT student. In all cases, the teacher is trained using ERM with a cross-entropy loss.\nFor every model evaluated under each condition, we perform an exhaustive search over the hyperparameter grid of Table 1 (when applicable).\nBatch size is kept fixed at 32. We train with mixed precision on a single NVIDIA A100 GPU. An AdamW optimizer with no warm-up is used. In Appendix A.2, we provide the hyperparameter configuration that is optimal on validation and is subsequently evaluated on test for each model and task. We use the Transformers library of Wolf et al. (2020) for implementation.\nThe experiments reported in this section use a KD temperature \u03c4 = 1, effectively disallowing any artificial smoothening of the target distribution. In Appendix A.3, we also show that treating \u03c4 as a hyperparameter during model selection does not qualitatively change these results.\nResults. Table 2 compares the average entropy of posteriors over classes predicted by LS models with those by standard and self-KD students, measured on the four test sets. As expected, LS increases model uncertainty over ERM, in fact quite significantly (\u223c3\u20136\u00d7). Both KD variants, on the other hand, bring entropy down; the only exception to this is self-KD on MNLI, where we see a very slight increase. These results have direct implications for the central research question of this paper, as they reveal that the typical effect of KD is a reduction in predictive uncertainty over ERM, which is the exact opposite of what LS does by design.\nFigure 1 illustrates how each training method regulates model uncertainty at real time during training. Both KD variants cause a rapid drop in entropy early in training, and an entropy lower than that of both ERM and LS is maintained throughout. The comparisons are also very similar between the training and the test set, indicating that the smoothing effects (or lack thereof) of each of these methods generalize well from training to inference.\nIn Table 3 we report the test set accuracy of each method on the four tasks. Both KD variants do generally better than LS, as the latter also lags behind the ERM baseline in three out of four evaluations. Interestingly, self-KD outperforms standard KD in most cases. While Yuan et al. (2020) explain similar successes of weaker teacher models in image classification with the equivalence of KD and LS, an alternative explanation is that these results are simply an outcome of the standard-KD student not adequately learning to mimic its higher-capacity teacher due to limited amounts of training data. Others (Liang et al., 2021; Sultan et al., 2022) have shown that such teacher-student knowledge gaps can be reduced by using additional synthetic data during distillation.\nNext we turn our attention to reverse KD from DistilBERT into BERT. In Table 4, LS unconditionally increases posterior entropy as before. The effect of reverse KD, on the other hand, is qualitatively different than standard and self-KD, as it also increases entropy in most cases over ERM. We\nalso observe a similar difference in runtime training entropy, as depicted in Figure 2.\nThe contrast in the results of Tables 2 and 4 paints a clear and important picture: the predictive uncertainty of the student in KD is a direct function of that of the teacher, and not of the KD process. To elaborate, we first point the reader to row 1 of both tables, which show that when trained using ERM, BERT classifiers have a lower posterior entropy than their smaller DistilBERT counterparts. Consequently, in Table 2, standard KD yields lowerentropy students than self-KD. For the same reason, reverse KD in Table 4 also generally trains higherentropy models than ERM.\nThe above results have key implications for the general question concerning the true nature of KD, namely, is it knowledge transfer or regularization? As a student learns to mimic its teacher in KD, we see in the above results that it also inherits its confidence\u2014an integral aspect of the data-driven knowledge of both models. This is clearly unlike methods such as LS or teacher-free KD in (Yuan\net al., 2020), which unconditionally reduce confidence in a data-agnostic way to limit overfitting.\nWith a weaker teacher model, reverse KD yields a smaller overall gain in accuracy than before over ERM and LS (see Table 5). It is interesting, nevertheless, that a lower inductive bias along with strong language modeling capabilities enables the reverse-KD student to outperform its teacher."
        },
        {
            "heading": "5 Conclusion",
            "text": "Knowledge distillation is a powerful framework for training lightweight and fast yet high-accuracy models. While still a topic of active interest, any misinterpretation, including false equivalences with other methods, can hinder progress for the study of this promising approach going forward. In this paper, we present evidence against its interpretation as label smoothing, and more generally, regularization. We hope that our work will inspire future efforts to understand distillation better and further improve its utility."
        },
        {
            "heading": "Limitations",
            "text": "To manage the complexity of our already largescale experiments involving (a) four different classification tasks, and (b) hyperparameter grids containing up to 1,960 search points, we ran experiments with a single random seed. While this is sufficient for exploring our main research question involving the posterior entropy of different models\u2014for which we only need to show one counterexample, i.e., one where entropy decreases with knowledge distillation\u2014the exact accuracy figures would be more reliable if averaged over multiple seeds."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Datasets Used in Our Experiments",
            "text": "We collect all datasets and their splits from the Hugging Face Datasets library1. Among the four tasks, only MRPC has a labeled test set. For the other three datasets, we split the train set into two parts. In QNLI and MNLI experiments, the two parts are used for training and validation, and the original validation set is used as a blind test set. For SST-2, since the validation set is small, we also use it for model selection; the train set is therefore split into train and test sets. For MNLI, we evaluate only on the test-matched set, which consists of unseen in-domain examples. Table 6 shows the number of examples in each dataset."
        },
        {
            "heading": "A.2 Model Selection",
            "text": "Table 7 contains the optimal configurations for the models evaluated in \u00a74 as observed on the respective validations sets."
        },
        {
            "heading": "A.3 Distillation at Higher Temperatures",
            "text": "We present results for the same experiments as in \u00a74 here, with two main differences: First, for KD, we include the temperature \u03c4 in the hyperparameter search and re-define the domain for \u03b1 as follows:\nHyperparameter Values \u03c4 {1, 2, 4, 8, 16, 32, 64}\n\u03b1 (Eq. 2) {.25, .5, .75, 1} Learning rate {1, 2, ..., 7} \u00d7 10\u22125 # of epochs {1, 2, ..., 10}\nyielding a 4D grid consisting of 1,960 points. Note that for this more general experiment, we allow \u03b1 = 1, which only uses the teacher\u2019s soft targets for training. Second, to keep the grid size the same (1,960) for LS, we re-define the domain of \u03b1 for LS to contain 28 equally-distanced points ranging from .1 to .9991.\nIn Table 8 and Figure 3, we report posterior entropies of DistilBERT models measured at inference and during training, respectively. Similar to the experimental results in \u00a74, both KD variants generally train models with lower entropy than not\n1https://huggingface.co/docs/datasets/index\nonly LS, but also ERM. In Table 9, KD again has better overall test set accuracy than LS. Across all these results, the comparisons between standard and self-KD also remain generally unchanged.\nThe reverse KD results\u2014for both predictive uncertainty in Table 10 and Figure 4, and test set performance in Table 11\u2014also show very similar trends as those described in \u00a74. Altogether, the results presented in this section with KD in three different settings demonstrate that even at high temperatures, our original findings and conclusions hold true.\nThe optimal hyperparameter combinations for the above experiments are provided in Table 12."
        }
    ],
    "title": "Knowledge Distillation \u2248 Label Smoothing: Fact or Fallacy?",
    "year": 2023
}