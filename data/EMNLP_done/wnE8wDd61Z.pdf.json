{
    "abstractText": "Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on more salient and relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge.1",
    "authors": [
        {
            "affiliations": [],
            "name": "EunJeong Hwang"
        },
        {
            "affiliations": [],
            "name": "Veronika Thost"
        },
        {
            "affiliations": [],
            "name": "Vered Shwartz"
        },
        {
            "affiliations": [],
            "name": "Tengfei Ma"
        }
    ],
    "id": "SP:27fdedecf292b515811175761288a6336ed01d23",
    "references": [
        {
            "authors": [
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Chaitanya Malaviya",
                "Keisuke Sakaguchi",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Doug Downey",
                "Wen-tau Yih",
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "8th International Conference on Learning Represen-",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDuran",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems, volume 26. Curran Associates,",
            "year": 2013
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi."
            ],
            "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Faeze Brahman",
                "Vered Shwartz",
                "Rachel Rudinger",
                "Yejin Choi."
            ],
            "title": "Learning to rationalize for nonmonotonic reasoning with distant supervision",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35:12592\u201312601.",
            "year": 2021
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90% chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Jaemin Cho",
                "Minjoon Seo",
                "Hannaneh Hajishirzi"
            ],
            "title": "Mixture content selection for diverse sequence generation",
            "year": 2019
        },
        {
            "authors": [
                "Marco Cuturi."
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Advances in neural information processing systems, 26.",
            "year": 2013
        },
        {
            "authors": [
                "A.P. Dempster",
                "N.M. Laird",
                "D.B. Rubin."
            ],
            "title": "Maximum likelihood from incomplete data via the EM algorithm",
            "venue": "Journal of the Royal Statistical Society: Series B, 39:1\u201338.",
            "year": 1977
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Yanlin Feng",
                "Xinyue Chen",
                "Bill Yuchen Lin",
                "Peifeng Wang",
                "Jun Yan",
                "Xiang Ren."
            ],
            "title": "Scalable multihop relational reasoning for knowledge-aware question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher Manning",
                "Percy Liang."
            ],
            "title": "Truncation sampling as language model desmoothing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3414\u2013 3427, Abu Dhabi, United Arab Emirates. Association",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Junhyun Lee",
                "Inyeop Lee",
                "Jaewoo Kang."
            ],
            "title": "Self-attention graph pooling",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Xinyue Chen",
                "Jamin Chen",
                "Xiang Ren."
            ],
            "title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Tengfei Ma",
                "Jie Chen"
            ],
            "title": "Unsupervised learning of graph hierarchical abstractions with differentiable coarsening and optimal transport",
            "year": 2020
        },
        {
            "authors": [
                "Ninareh Mehrabi",
                "Pei Zhou",
                "Fred Morstatter",
                "Jay Pujara",
                "Xiang Ren",
                "Aram Galstyan."
            ],
            "title": "Lawyers are dishonest? quantifying representational harms in commonsense knowledge resources",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi."
            ],
            "title": "Computational optimal transport",
            "venue": "Foundations and Trends in Machine Learning, 11 (5-6):355\u2013602.",
            "year": 2019
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Explain yourself! leveraging language models for commonsense",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
            "venue": "Proceedings of the AAAI con-",
            "year": 2019
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N Kipf",
                "Peter Bloem",
                "Rianne Van Den Berg",
                "Ivan Titov",
                "Max Welling."
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "venue": "The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete,",
            "year": 2018
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Myle Ott",
                "Michael Auli",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Mixture models for diverse machine translation: Tricks of the trade",
            "venue": "International Conference on Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Quan Tu",
                "Yanran Li",
                "Jianwei Cui",
                "Bin Wang",
                "Ji-Rong Wen",
                "Rui Yan."
            ],
            "title": "MISC: A mixed strategyaware model integrating COMET for emotional support conversation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Shuailong Liang",
                "Yili Jin",
                "Yilong Wang",
                "Xiaodan Zhu",
                "Yue Zhang."
            ],
            "title": "SemEval2020 task 4: Commonsense validation and explanation",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 307\u2013321, Barcelona",
            "year": 2020
        },
        {
            "authors": [
                "Ruize Wang",
                "Duyu Tang",
                "Nan Duan",
                "Zhongyu Wei",
                "Xuanjing Huang",
                "Jianshu Ji",
                "Guihong Cao",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
            "venue": "Findings of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Weiqi Wang",
                "Tianqing Fang",
                "Baixuan Xu",
                "Chun Yi Louis Bo",
                "Yangqiu Song",
                "Lei Chen."
            ],
            "title": "Cat: A contextualized conceptualization and instantiation framework for commonsense reasoning",
            "venue": "Proceedings of the 61st Annual Meeting of the Asso-",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Wang",
                "Jianwen Zhang",
                "Jianlin Feng",
                "Zheng Chen."
            ],
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 28.",
            "year": 2014
        },
        {
            "authors": [
                "Siwei Wu",
                "Xiangqing Shen",
                "Rui Xia"
            ],
            "title": "Commonsense knowledge graph completion via contrastive pretraining and node clustering",
            "year": 2023
        },
        {
            "authors": [
                "Zonghan Wu",
                "Shirui Pan",
                "Fengwen Chen",
                "Guodong Long",
                "Chengqi Zhang",
                "Philip S. Yu."
            ],
            "title": "A comprehensive survey on graph neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 32(1):4\u201324.",
            "year": 2021
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Hongyu Ren",
                "Antoine Bosselut",
                "Percy Liang",
                "Jure Leskovec."
            ],
            "title": "QA-GNN: Reasoning with language models and knowledge graphs for question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter",
            "year": 2021
        },
        {
            "authors": [
                "Zhitao Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "Will Hamilton",
                "Jure Leskovec."
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Wenhao Yu",
                "Chenguang Zhu",
                "Lianhui Qin",
                "Zhihan Zhang",
                "Tong Zhao",
                "Meng Jiang."
            ],
            "title": "Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Michel Galley",
                "Jianfeng Gao",
                "Zhe Gan",
                "Xiujun Li",
                "Chris Brockett",
                "Bill Dolan."
            ],
            "title": "Generating informative and diverse conversational responses via adversarial information maximization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yaoming Zhu",
                "Sidi Lu",
                "Lei Zheng",
                "Jiaxian Guo",
                "Weinan Zhang",
                "Jun Wang",
                "Yong Yu"
            ],
            "title": "Texygen: A benchmarking platform for text generation models",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Commonsense knowledge graphs (CSKGs) have been used to improve the performance of downstream applications such as question answering (Yasunaga et al., 2021) and dialogue (Tu et al., 2022), as well as for enhancing neural models for commonsense reasoning tasks (Lin et al., 2019; Yu et al., 2022). Typically, these methods extract keywords from the input and construct a subgraph around them using the KG knowledge, which is then incorporated into the model.\nRecent popular CSKGs such as ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019) represent nodes in natural language, which allows flexibility but also adds redundancy and noise (Wu\n1Code is available at: https://github.com/eujhwang/KG-Compression\net al., 2023). Moreover, the retrieved subgraphs around a task\u2019s concepts potentially include information that is not relevant to the context. For example, in Figure 1, the goal is to generate a reason why the input sentence (\u201cA shark interviews a fish\u201d) defies commonsense. The concepts tank and business are semantically irrelevant to either the input or the reference output sentences. Including irrelevant information introduces noise that can deteriorate the model\u2019s performance. Recent work has addressed this by pruning noisy paths based on low edge confidence scores in knowledge base embeddings (Lin et al., 2019) or by using language models (LMs) (Yasunaga et al., 2021). Yet, the relevance of paths is not determined in relation to the given task.\nIn this paper, we propose to use differentiable graph compression that enables the model to learn how to select the crucial concepts that are actually related to the task. Our method contains two main components: using self-attention scores to select relevant concept nodes in the retrieved subgraph,\nand employing optimal transport loss to ensure the chosen concepts preserve the most crucial information of the original graph. In this way, the irrelevant or redundant concepts can be automatically eliminated in the subgraph.\nWe demonstrate the usefulness of our method on two commonsense generation tasks: commonsense explanation generation and abductive commonsense reasoning. Our method outperforms a range of baselines that use KGs in terms of both diversity and quality of the generations. We further conduct a comprehensive analysis, exploring a different setup, such as the scenario of incorporating new knowledge into the subgraph. Different from the baselines, our method enables the model to maintain performance, even in the presence of potentially increased noisy data. Finally, we show that our approach demonstrates better quality-diversity tradeoff than the large language model vicuna-13b, which has 100 times more parameters."
        },
        {
            "heading": "2 Background",
            "text": "KG-Enhanced Neural Methods. KGs have been used to enhance models for question answering (Lin et al., 2019; Feng et al., 2020; Yasunaga et al., 2021), relation classification (Wang et al., 2021), textual entailment (Kapanipathi et al., 2020), and more. Typically, such methods extract a subgraph of knowledge related to keywords in the input, which is then either embedded or represented in natural language before being incorporated into the model. For example, both Wang et al. (2023) and Wang, Fang, et al. (2023) used CSKGs to enhance a commonsense inference and a QA model by including the abstraction of concepts in the input (e.g. vacation \u2192 relaxing event). However, some knowledge may be irrelevant in the context of the particular question.\nTo reduce such noise, prior methods have proposed to score and prune the paths. Lin et al. (2019) used TransE (Wang et al., 2014) to score each edge in the path, while Yasunaga et al. (2021) scores nodes based on the likelihood of a pre-trained LM to generate it after the input. In both methods, the scores are not trained to represent a node\u2019s importance in relation to the task.\nGenerating Commonsense Explanations. This paper focuses on the task of generating commonsense explanations, in particular focusing on the following datasets. In ComVE (Wang et al., 2020) the goal is to generate explanations for why a given\nsentence, such as \u201cA shark interviews a fish\u201d, does not make sense. \u03b1-NLG (Bhagavatula et al., 2020) presents models with a past observation, such as \u201cMike spends a lot of his time on the internet\u201d and a future observation such as \u201cNow other people love the internet because of Mike\u2019s website\u201d. The goal is to generate a plausible explanation for what might have happened in-between, such as \u201cMike created a website that helps people search\u201d. In a related line of work, researchers collected or generated commonsense explanations for existing tasks (e.g., Camburu et al., 2018; Rajani et al., 2019; Brahman et al., 2021).\nDiverse Sentence Generation. One of the desired aspects of generating commonsense explanations is the diversity of the outputs. Popular LM decoding methods such as top-k (Fan et al., 2018), top-p (Holtzman et al., 2020), and truncated sampling (Hewitt et al., 2022) generate diverse outputs by pruning the probability distribution over the vocabulary for the next token and then sampling a token from the pruned distribution. An alternative approach is to use a mixture of experts (MoE) to produce diverse outputs (Shen et al., 2019; Cho et al., 2019). Our approach extends MoKGE Yu et al. (2022), a model for commonsense explanation generation. MoKGE uses a combination of KGs to diversify the outputs of a MoE model. However, the knowledge that MoKGE retrieves from the KG is not filtered, hence may contain loosely related, redundant and irrelevant information, which can negatively impact the model\u2019s performance in generating high-quality diverse outputs. In our approach, we employ knowledge graph compression to prioritize more important information."
        },
        {
            "heading": "3 Method",
            "text": "Our goal is to generate diverse sentences, {y1, y2, ..., yk} that explain a given instance x (see Sec 2 for the specific task descriptions). The objective is to maximize the probability of generating each yi: P (yi|x), as well as to diversify them. Previous KG-enhanced approaches usually add an external graph Gx to make the generation also conditioned on the graph: P (yi|x,Gx). However, as we discussed in Sec 1, Gx often contains redundancy or noise. For example, given a target concept A, there is a semantically similar concept (e.g. a synonym) A\u2032 and a noisy concept B in the graph Gx). Obviously, A\u2032 will negatively impact the diversity of generations because the model may select both\nA and A\u2032 for generation and the semantics of the generations are similar; concept B will hurt the generation quality since it is irrelevant to the context. So, a natural idea to solve the problem is to eliminate these concepts by compressing the graph.\nOur method extends MoKGE (Yu et al., 2022) by compressing the retrieved external knowledge graph. The framework is illustrated in Figure 2 and described in detail subsequently. In a nutshell, it aims to identify the concepts within the KG that provide the most relevant knowledge for a particular instance. We first extract a subgraph from the KG based on the given input sentence, and encode it into a vector representation (Sec 3.1). Then, we learn a compressed graph that maintains only the most relevant concepts for the given instance (Sec 3.2). We train the model with the corresponding losses (Sec 3.3) and finally apply MoE to generate diverse outputs (Sec 3.4)."
        },
        {
            "heading": "3.1 KG Subgraph Extraction and Encoding",
            "text": "The subgraph extraction and encoding follows MoKGE (Yu et al., 2022).\nSubgraph Extraction. We first associate each input sentence with the set of concepts from the KG that match its tokens. For example, given the sentence q =\u201cA shark interviews a fish\u201d (the \u201cquery\u201d), we extract the concepts Cq = {fish, shark, interview} from ConceptNet.2 Second, we fix a radius h and extract a subgraph Gq with node set Vq \u2287 Cq from the KG such that it contains all KG nodes and edges that are up to h = 2 hops around the concepts in Cq (e.g. shark\n2In what follows, our notation refers to KG concepts and their corresponding KG nodes interchangeably.\n\u2192 swim \u2192 fish).\nGraph Encoding. To obtain embeddings for the concept nodes, we apply an off-the-shelf graph encoder over the extracted subgraph (Wu et al., 2021). In our implementation, we follow Yu et al. (2022) and use the relational graph convolutional network (R-GCN; Schlichtkrull et al., 2018). R-GCN computes node representations by iteratively aggregating neighboring node representations and thereby taking the relation types into account. In this way, the final embeddings capture the structural patterns of the subgraph."
        },
        {
            "heading": "3.2 Differentiable Graph Compression",
            "text": "As we discussed before, the extracted subgraphs often contain redundancy and noise, and we aim to compress the graph and remove the irrelevant information. This introduces two challenges: (1) how to make the graph compression differentiable so that it can be trained in the context of downstream tasks; and (2) how to maintain the most important and relevant information in the compressed graph.\nSelf-Attention for Concept Scoring. Since we want to select concepts for the generation step (Sec 3.4), we can\u2019t apply differentiable pooling methods (Ying et al., 2018; Ma and Chen, 2020) and instead choose to construct a semantically meaningful subgraph containing the relevant nodes and edges. To do so, we apply self-attention and hence essentially use the features computed in the previous step as main criterion to determine the concepts\u2019 importance. Specifically, we compute self-attention scores Z \u2208 RC\u00d71 as proposed by Lee et al. (2019) using graph convolution (Kipf\nand Welling, 2017):\nZ = \u03c3(D\u0303\u2212 1 2 A\u0303D\u0303\u2212 1 2X\u0398att)\nwhere \u03c3 is the non-linear activation function tanh; C := |Vq| is the number of concept nodes in the subgraph; A\u0303 \u2208 RC\u00d7C is the adjacency matrix extended by self-connections; D\u0303 is the degree matrix of A\u0303, which is used for normalization; X \u2208 RC\u00d7F is the matrix of concept embeddings obtained in the previous step, with embedding dimension F ; and \u0398att \u2208 RF\u00d71 is the parameter matrix for the self-attention scores. Given the concept scores Z, we consider a pre-set assignment ratio s \u2208 (0, 1], and form the compressed graph, G\u2032, by selecting s% of concept nodes. We denote S as the number of concept nodes selected. In the example in Figure 2, the compressed (third) graph contains 80% of the nodes in the original subgraph.\nOptimal Transport for Regularization. The self-attention based concept selection make the graph compressed in an differentiable way, however the attention parameters can only be trained from downstream generation tasks which cannot gurantee the compression quality as well as generalizability. Consider the case with concept A and its synonym A\u2032 in the retrieved graph Gq, if A is selected by the attention scores, it is highly possible A\u2032 also has a high score to be selected, so the redundancy cannot be removed.\nFor this reason, we additionally apply optimal transport (OT; Peyr\u00e9 and Cuturi, 2019), a method commonly used for measuring the distance between two probability measures. Here, we regard a graph as a discrete distribution, similarly to Ma and Chen (2020), and minimize the OT distance between the original graph and its compressed version. To this end, we define an optimal transport loss between graphs. Given a m-node graph and a n-node graph, we assume they have discrete distributions \u00b5 = \u2211m i=1 ai\u03c3xi and \u03bd = \u2211n j=1 bj\u03c3xj , where xi and xj indicate the nodes, \u03c3 is a delta function, a = (a1, ..., am) and b = (b1, ..., bn) are weights of nodes (generally uniform). If we define a cost matrix M whose element Mij indicates the transport cost from node xi to node xj , then the optimal transport distance is:\nW (\u00b5, \u03bd) = min T < T,M > (1)\nT \u2208 Rm\u2217n is called a transportation plan, whose element Tij denotes the transportation probability\nfrom xi to xj , and it meets the requirements that T1n = a, and T T 1m = b.\nOnce the optimal transport distance is minimized, the compressed graph is expected to keep as much information of the original graph. Thus redundant concepts will be largely removed, since involving them in the compressed graph will lead to less information kept. Take a simple example, given an original graph with nodes {A,A\u2032, C}, the subgraph with node {A,C} should be more informative than the one with {A,A\u2032}, and its optimal transport distance between the original graph should be smaller.\nSince solving an OT problem is computationally expensive, we add an entropy regularization term E(T ) = \u2211 ij Tij(log Tij \u2212 1), to allow for solving it approximately using Sinkhorn\u2019s algorithm (Cuturi, 2013) in practice, following prior work. With a hyperparameter \u03b3 > 0, the entropy-regularized loss becomes:\nW\u03b3(\u00b5, \u03bd) = min T\n< T,M > \u2212\u03b3E(T ) (2)"
        },
        {
            "heading": "3.3 Loss Functions for Training",
            "text": "Following Yu et al. (2022), we train BART-base (Lewis et al., 2020) in a seq2seq architecture on the commonsense explanation generation task, with a generation loss, and apply a KG concept loss in addition. We also include an optimal transport loss.\nGeneration Loss. For sentence generation, we maximize the conditional probability of the target sequence y given the input sequence x concatenated with the selected KG concepts c1, c2, ...cS . We utilize the standard auto-regressive crossentropy loss as follows:\nLg = \u2212 |y|\u2211 t=1 logP (yt|x, c1, c2, ..., cS , y<t)\nwhere t is the timestep of the actual output. In the generation step, the model auto-regressively generates the output y with input x and S selected concepts.\nKG Concept Loss. The effectiveness of the concept selection can be measured in terms of which of the chosen concepts appear in the output sentence a (the reference answer). More specifically, we consider a regular binary cross entropy loss with targets yc = I(c \u2208 Vq \u2229Ca) for each c \u2208 Vq. Here, I(\u00b7) represents the indicator function. and Ca is\nthe set of concepts that are present in the output. To obtain a probability for each of the S concepts in the compressed graph, we apply an MLP. The resulting loss is as follows:\nLc = \u2212 (\u2211 c\u2208Vq\u2229Ca yc logP (c) + \u2211 c\u2208Vq\u2212Ca(1\u2212 yc) log 1\u2212 P (c) )\nOptimal Transport Loss. To make the optimal transport distance differentiable, we solve Eq. 2 using the Sinkhorn\u2019s algorithm (Cuturi, 2013):\nStarting with any positive vector v0, we iteratively update u and v as follows:\nui+1 = a\u2298Kvi; vi+1 = b\u2298KTui+1 (3)\nwhere \u2298 is the element-wise division and K is an intermediate variable derived from the cost matrix M : K = exp(\u2212M/\u03b3).\nAfter k steps, we arrive at the k-step result P k = diag(uk)K diag(vk) as an approximated optimal transportation plan, hence the optimal transport loss is approximated by\nLt = W k\u03b3 (G,Gc) =< P k,M > \u2212\u03b3E(P k)\nAltogether, our model is trained with three loss functions:\nL = Lg + \u03b1Lc + \u03b2Lt (4)\nwhere \u03b1 and \u03b2 are hyperparameters that control the relative importance of the individual loss functions. In our experimental setup, we set both \u03b1 and \u03b2 to a value of 0.3."
        },
        {
            "heading": "3.4 Diverse Generation based on MoE",
            "text": "To encourage more diverse outputs, we follow previous work (Shen et al., 2019; Cho et al., 2019; Yu et al., 2022) and use mixture of experts (MoE).\nWe use K experts, where each expert is responsible for generating a unique set of KG concepts. The model is trained using hard-EM algorithm (Dempster et al., 1977). Since it is similar to (Yu et al., 2022)), we put the details in Appendix E. In Figure 2, the nodes in the 4th graph highlighted in green, red, and blue colors indicate the K = 3 respective experts assigned to handle different concepts. The utilization of our compressed graph version helps the model better prioritize the crucial concepts during output generation, as we demonstrate in our experiments."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "ComVE (Wang et al., 2020) was part of the SemEval 2020 commonsense validation task. Given a nonsensical sentence, the task is to generate explanations for why it doesn\u2019t make sense. The dataset contains 10k training examples and roughly 1000 examples each for test and validation. Each example comes with 3 reference output sentences. The other dataset, \u03b1-NLG (Bhagavatula et al., 2020), addresses the abductive commonsense reasoning task. Given a past observation and a future observation, the goal is to generate plausible explanations for what might have happened in-between. The dataset consists of 50k training examples, 1,779 validation and 3,560 test examples. Each example in the dataset includes up to 5 reference outputs."
        },
        {
            "heading": "4.2 Baselines",
            "text": "MoE-based Methods. MoE-embed (Cho et al., 2019) and MoE-prompt (Shen et al., 2019) produce diverse sentences by sampling different mixture components. While MoE-embed employs independent latent variables when generating diverse outputs, MoE-prompt shares the latent variable between the experts. MoKGE (Yu et al., 2022) is the approach that we extend by adding graph compression. It generates outputs by incorporating KG concepts on top of MoE-based methods.\nOther Methods to Improve Diversity. To show that our method yields a sophisticated concept selection beyond regular filtering, we compare it to a simple synonym filtering on top of MoKGE, applied during the inference step, that yields a set of unique KG concepts for generating outputs. This baseline prevents the model from selecting similar concepts when generating the outputs. Second, we consider the common pruning approach, which removes irrelevant paths from the potentially noisy subgraph, following KagNet (Lin et al., 2019). To measure the quality of the path, the path is decomposed into a set of triples. Each triple is scored based on the scoring function of the knowledge graph embedding technique, TransE (Bordes et al., 2013) and the score for each path is the product of its triple scores. The threshold for pruning is a hyperparameter and set to 0.15 following Lin et al. (2019).\nLarge Language Model (LLM). Lastly, we compare to Vicuna-13b (Chiang et al., 2023). This\nlarge LM was built upon LLaMA-13b (Touvron et al., 2023), a transformer-based LM trained on trillions of tokens exclusively sourced from publicly available data. Vicuna-13b performs on par with ChatGPT (Chiang et al., 2023). We employ Vicuna-13b in a 2-shot setup (see Appendix A for the prompts)."
        },
        {
            "heading": "4.3 Metrics",
            "text": "Following the same evaluation setting in previous works, we assess the performance of the generated sentences in terms of both diversity and quality.\nPairwise Diversity. Self-BLEU (Zhu et al., 2018) is used to evaluate how each sentence is similar to the other generated sentences based on n-gram overlap. Self-BLEU-3/4 are diversity scores based on 3/4-gram overlap. The metrics compute the average of sentence-level self-BLEU scores between all pairwise combinations of generated outputs. Hence, lower self-BLEU scores indicate a greater variety between the sentences in the set generated for each input sample.\nCorpus Diversity. Distinct-k (Li et al., 2016) is calculated by counting the number of unique kgrams in the generated sentence and dividing it by the total number of generated tokens, to prevent preference towards longer sentences. Additionally, we report entropy-k (Zhang et al., 2018), which evaluates the evenness of the empirical n-gram dis-\ntribution within the generated sentence.\nQuality. We use standard metrics for automatic evaluation of generative tasks: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which are based on n-gram overlap between the generated sentences and human-written reference outputs. They assess the highest accuracy by comparing the best generated sentences to the target sentences."
        },
        {
            "heading": "5 Results and Discussion",
            "text": "Comparison to Baselines, Table 1. We observe similar trends for the two datasets and across the two model series, based on embedding and prompts. Overall, the differences are strongest for self-BLEU and Distinct-2, two aspects that are particularly important for diverse generation. This suggests that our model is able to reason about different possible contexts. On both datasets, our method, MoKGE+SAG+OT, outperforms the mixtures of experts by large margins in terms of diversity and, at the same time, achieves comparable or better performance in terms of quality. Note that, on ComVE, the quality differences between the best and our, second-best model are within standard deviation.\nThe effectiveness of our approach is especially evident from the comparison to the filtering and pruning baselines. Recall that these approaches similarly aim at better exploiting the KG by im-\nproving diversity and removing noise, respectively. However, we observe a considerable decrease in diversity and nearly always also slightly in quality. This shows that simple solutions, unrelated to the task at hand, are seemingly not able to identify the most relevant knowledge. More specifically, for the filtering baseline, we observed that the model is unable to learn what concepts to choose for unseen data. As a result, its ability to generalize to unseen data is limited, resulting in lower diversity scores on the test data. Altogether, this demonstrates that our approach, based on the compressed graph, is effective in suppressing redundant information present in the KG and promoting other knowledge that is more relevant in the given context.\nWe additionally confirm that our optimal transport loss helps the model to keep the KG subgraph more coherently; see especially the \u03b1-NLG results.\nGeneration Examples, Figure 4. Observe that MoKGE tends to generate sentences with simpler structure and fewer concepts, whereas our model employs a broader range of KG concepts. This makes the generations effectively more similar to the human-written ones, where each of the three sentences addresses a different context. We show more examples in Appendix B.\nTesting Robustness with Potentially more Redundancy and Noise, Table 2. We created a more challenging scenario by extending the extracted subgraphs with additional, related knowledge potentially including more of both relevant and redundant information. This was done by applying COMET (Bosselut et al., 2019), a transformer that was trained to generate KG triples (i.e., entity-relation-entity tuples) based on given entityrelation pairs. The original MoKGE model seems to struggle in this scenario: its performance decreases without exception in terms of all metrics. In contrast, our approach, applied on top of MoKGE, is successful in both retaining the performance of MoKGE alone and even the improvements of MoKGE+SAG+OT.\nComparison with Large Language Model, Table 3 & Figure 4. We compare our method to Vicuna-13b. Most interestingly, our proposal outperforms the LLM on Distinct-2 and Entropy-4. Note that even MoKGE alone is slightly better than the LLM in these aspects, yet our method is effective in extending the gap by better exploiting the external knowledge. Figure 4 gives example\noutputs and shows that the LLM is still prone to generating sentences with similar structure (e.g. \u201cI wore a wig to ...\u201d), as it can be seen with \u03b1-NLG. Furthermore, while the generated sentence \u201cI wore a wig to a party and felt great\u201d explains the first observation \u201cI always wondered why I loved wearing wigs\u201d, it fails to explain the second observation \u201cI got beat up and reminded of why I shouldn\u2019t\u201d. In the ComVE task, the generated sentences are diverse in terms of both sentence structure and word usage, but the model sometimes generates sentences that are less logical, such as \u201cWriting in a paper with an eraser is not permanent\u201d. In contrast, our approach enables MoKGE to generate a wider range of sentences that incorporate relevant concepts and enhance the interpretability of the generation process."
        },
        {
            "heading": "6 Analysis",
            "text": "Compression Ratios, Figure 3. This hyperparameter determines the amount of concept nodes to be kept in the compressed subgraph. Maintaining 65% of the nodes in the subgraph yields the optimal performance in terms of both diversity and quality on both datasets (see Appendix C for ComVE dataset). Interestingly, we do not observe a great negative impact on performance, even up to a ratio of 30%. This shows that ConceptNet apparently contains much information that is not necessarily beneficial for diverse generations in the context of a particular task and hence justifies our proposal.\nUnique Concepts in the Output, Appendix D. The comparison of MoKGE and MoKGE+SAG+OT shows that MoKGE tends to generate more sentences containing 0\nor 1 concepts only. This indicates that the lower diversity scores of MoKGE may be due to the selection of irrelevant concepts during output generation, showing the model\u2019s inability to effectively utilize them. The table shows that our method increases the numbers of KG knowledge\nactually used by the model and thus its success in injecting external bias into LMs.\nHuman Evaluation, Table 4. We conducted human evaluation on the outputs produced by our model MoKGE+SAG+OT and the baseline MoKGE for the \u03b1-NLG task. We randomly se-\nlected 30 generations from each model. The annotation was performed by 3 researchers in the lab. We instructed the annotators to score the diversity and correctness (quality) of each generation on a scale of 0 to 3. Table 4 shows a consistent performance improvement across both diversity and quality when comparing our model to the baseline."
        },
        {
            "heading": "7 Conclusion",
            "text": "We present a differentiable graph compression algorithm that enables the model to focus on crucial information. Through experiments on two commonsense explanation generation tasks, we show that our approach not only improves the diversity but also maintains the quality of outputs. Moreover, our graph compression helps the model regain performance when new and potentially noisy information is added to graphs. Our work opens up future research in effectively selecting and incorporating symbolic knowledge into NLP models.\nLimitations\nUse of Single Word Concept. Since ConceptNet contains mostly single words, we limit additional KG concepts to single words only. However, it can easily be extended into phrases and we leave it to future work to investigate how to effectively utilize longer phrases.\nUse of Relations. When additional KG concepts are added to the model, we focus more on the concept nodes, not the edges. However, relation edges may provide additional insight. We leave the exploration of this for future work.\nEthics Statement\nData The datasets used in our work, SemEval2020 Task 4 Commonsense Validation and Explantation (ComVE; Wang et al., 2020) and Abductive Commonsense Reasoning (\u03b1-NLG; Bhagavatula et al., 2020), are publicly available. The two datasets aim to produce commonsense explanations and do not include any offensive, hateful, or sexual content. The commonsense knowledge graph,\nConceptNet, was collected through crowdsourcing and may also introduce bias to our model (Mehrabi et al., 2021). However, we only use single word nodes from ConceptNet, which limits the impact of such bias.\nModels The generative models presented in the paper are trained on a large-scale publicly available web corpus and may also bring some bias when generating sentences."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs program, an NSERC discovery grant, and a research gift from AI2."
        },
        {
            "heading": "A Prompt used with Vicuna-13b",
            "text": "We present the prompts that we used for Vicuna13b for ComVE (Figure 5) and \u03b1-NLG (Figure 6)."
        },
        {
            "heading": "B Additional Generation Examples",
            "text": "We show additional sentences generated by MoKGE and MoKGE+SAG+OT for ComVE (Figure 7) and \u03b1-NLG (Figure 8)."
        },
        {
            "heading": "C Assignment Ratio for ComVE",
            "text": "We show the performance on ComVE with varying node assignment ratios in Figure 9."
        },
        {
            "heading": "D Concept Inclusiveness",
            "text": "We analyze how well the model incorporates KG concepts in output generation in Table 5."
        },
        {
            "heading": "E Mixture of Experts",
            "text": "Given input sentence q and target sentence y, MoE employs a multinomial latent variable \u03b4 \u2208 {1, 2, ...,K} and decomposes the marginal likelihood as:\nP (y|x, gx) = K\u2211 \u03b4=1 P (\u03b4|x,G\u2032x)P (y|\u03b4, x,G\u2032x)\nEach \u03b4 represents an expert, which is responsible for explaining (x,G\u2032x, y) observation.\nWith the above decomposition, the model minimizes the loss function (Eq.(4))\n\u2207 logP (y|x,G\u2032x) = \u2211K \u03b4=1 P (\u03b4|x, y,G\u2032x) \u00b7 \u2207 logP (y, \u03b4|x,G\u2032x)\nand is trained using hard-EM algorithm (Dempster et al., 1977) as follows:\n\u2022 E-step: choose expert \u03b4best with minimal loss.\n\u03b4best = argmin \u03b4 \u2212 logP (y, \u03b4|x,G\u2032x)\n\u2022 M-step: update the parameters of the chosen expert \u03b4best from E-step."
        },
        {
            "heading": "F Hyper-parameters",
            "text": "We used BART-base (Lewis et al., 2020), which is built on the Transformer architecture with a 6 layer encoder-decoder. For model training, we used Adam optimizer with a batch size of 60 and a learning rate of 3e-5. For the ComVE dataset, the warmup steps are set to 5000. For the \u03b1-NLG dataset, the weight decay is set to 0.01. For optimal transport, \u03b3 is set to 1.0. As to the weights in the discrete distributions, {ai} are set evenly as 1/m, and {bj} are all set as 1/n, where m and n are number of nodes in the graphs."
        }
    ],
    "title": "Knowledge Graph Compression Enhances Diverse Commonsense Generation",
    "year": 2023
}