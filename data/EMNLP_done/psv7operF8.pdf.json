{
    "abstractText": "The label noise in real-world scenarios is unpredictable and can even be a mixture of different types of noise. To meet this challenge, we develop an adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage followed by a hybrid training stage. Specifically, an early stopping method, relying solely on the training set, is designed to dynamically terminate the warm-up process based on the model\u2019s fit level to different noise scenarios. The hybrid training stage incorporates several generalization strategies to gradually correct mislabeled instances, thereby making better use of noisy data. Experiments on multiple datasets demonstrate that our approach performs on-par with or even better than the state-of-the-art methods in various noise scenarios, including scenarios with the mixture of multiple types of noise.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shaohuan Cheng"
        },
        {
            "affiliations": [],
            "name": "Wenyu Chen"
        },
        {
            "affiliations": [],
            "name": "Mingsheng Fu"
        },
        {
            "affiliations": [],
            "name": "Xuanting Xie"
        },
        {
            "affiliations": [],
            "name": "Hong Qu"
        }
    ],
    "id": "SP:0158999c586b6b213340d218f7077192852824b9",
    "references": [
        {
            "authors": [
                "Ashkan Alinejad",
                "Hassan S. Shavarani",
                "Anoop Sarkar."
            ],
            "title": "Translation-based supervision for policy generation in simultaneous neural machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Devansh Arpit",
                "Stanis\u0142aw Jastrz\u0119bski",
                "Nicolas Ballas",
                "David Krueger",
                "Emmanuel Bengio",
                "Maxinder S. Kanwal",
                "Tegan Maharaj",
                "Asja Fischer",
                "Aaron Courville",
                "Yoshua Bengio",
                "Simon Lacoste-Julien."
            ],
            "title": "A closer look at memorization in deep networks",
            "venue": "In",
            "year": 2017
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel."
            ],
            "title": "Mixmatch: A holistic approach to semisupervised learning",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Pengfei Chen",
                "Junjie Ye",
                "Guangyong Chen",
                "Jingwei Zhao",
                "Pheng-Ann Heng"
            ],
            "title": "Beyond classconditional assumption: A primary attempt",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Siddhant Garg",
                "Goutham Ramakrishnan",
                "Varun Thumbe."
            ],
            "title": "Towards robustness to label noise in text classification via noise modeling",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 3024\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Xingrui Yu",
                "Gang Niu",
                "Miao Xu",
                "Weihua Hu",
                "Ivor Tsang",
                "Masashi Sugiyama."
            ],
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Advances in Neural Information Processing Systems, volume 31.",
            "year": 2018
        },
        {
            "authors": [
                "Lifeng Jin",
                "Linfeng Song",
                "Kun Xu",
                "Dong Yu."
            ],
            "title": "Instance-adaptive training with noise-robust losses against noisy labels",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5647\u20135663, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Ishan Jindal",
                "Daniel Pressel",
                "Brian Lester",
                "Matthew Nokleby."
            ],
            "title": "An effective label noise model for DNN text classification",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2019
        },
        {
            "authors": [
                "Junnan Li",
                "Richard Socher",
                "Steven CH Hoi."
            ],
            "title": "Dividemix: Learning with noisy labels as semi-supervised learning",
            "venue": "arXiv preprint arXiv:2002.07394.",
            "year": 2020
        },
        {
            "authors": [
                "xiaobo liang",
                "Lijun Wu",
                "Juntao Li",
                "Yue Wang",
                "Qi Meng",
                "Tao Qin",
                "Wei Chen",
                "Min Zhang",
                "Tie-Yan Liu"
            ],
            "title": "R-drop: Regularized dropout for neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xuefeng Liang",
                "Xingyu Liu",
                "Longshan Yao."
            ],
            "title": "Review\u2013a survey of learning from noisy labels",
            "venue": "ECS Sensors Plus, 1(2):021401.",
            "year": 2022
        },
        {
            "authors": [
                "Bo Liu",
                "Wandi Xu",
                "Yuejia Xiang",
                "Xiaojun Wu",
                "Lejian He",
                "Bowen Zhang",
                "Li Zhu."
            ],
            "title": "Noise learning for text classification: A benchmark",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 4557\u20134567, Gyeongju,",
            "year": 2022
        },
        {
            "authors": [
                "Sheng Liu",
                "Jonathan Niles-Weed",
                "Narges Razavian",
                "Carlos Fernandez-Granda."
            ],
            "title": "Early-learning regularization prevents memorization of noisy labels",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 20331\u201320342. Curran Associates,",
            "year": 2020
        },
        {
            "authors": [
                "Wanlong Liu",
                "Shaohuan Cheng",
                "Dingyi Zeng",
                "Qu Hong."
            ],
            "title": "Enhancing document-level event argument extraction with contextual clues and role relevance",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12908\u201312922.",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language",
            "year": 2011
        },
        {
            "authors": [
                "Haim Permuter",
                "Joseph Francos",
                "Ian Jermyn."
            ],
            "title": "A study of gaussian mixture models of color and texture features for image classification and segmentation",
            "venue": "Pattern recognition, 39(4):695\u2013706.",
            "year": 2006
        },
        {
            "authors": [
                "Samira Pouyanfar",
                "Saad Sadiq",
                "Yilin Yan",
                "Haiman Tian",
                "Yudong Tao",
                "Maria Presa Reyes",
                "Mei-Ling Shyu",
                "Shu-Ching Chen",
                "Sundaraja S Iyengar."
            ],
            "title": "A survey on deep learning: Algorithms, techniques, and applications",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "Dan Qiao",
                "Chenchen Dai",
                "Yuyang Ding",
                "Juntao Li",
                "Qiang Chen",
                "Wenliang Chen",
                "Min Zhang."
            ],
            "title": "SelfMix: Robust learning against textual label noise with self-mixup training",
            "venue": "Proceedings of the 29th International Conference on Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Songbo Tan",
                "Jin Zhang."
            ],
            "title": "An empirical study of sentiment analysis for chinese documents",
            "venue": "Expert Systems with applications, 34(4):2622\u20132629.",
            "year": 2008
        },
        {
            "authors": [
                "Michael T\u00e4nzer",
                "Sebastian Ruder",
                "Marek Rei."
            ],
            "title": "Memorisation versus generalisation in pre-trained language models",
            "venue": "arXiv preprint arXiv:2105.00828.",
            "year": 2021
        },
        {
            "authors": [
                "Ellen M Voorhees",
                "Dawn M Tice"
            ],
            "title": "The trec-8 question answering track evaluation",
            "venue": "In TREC,",
            "year": 1999
        },
        {
            "authors": [
                "Yisen Wang",
                "Xingjun Ma",
                "Zaiyi Chen",
                "Yuan Luo",
                "Jinfeng Yi",
                "James Bailey."
            ],
            "title": "Symmetric cross entropy for robust learning with noisy labels",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 322\u2013330.",
            "year": 2019
        },
        {
            "authors": [
                "Tingting Wu",
                "Xiao Ding",
                "Minji Tang",
                "Hao Zhang",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "STGN: an implicit regularization method for learning with noisy labels in natural language processing",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "arXiv preprint arXiv:1710.09412.",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Muhao Chen."
            ],
            "title": "Learning from noisy labels for entity-centric information extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5381\u20135392, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Dawei Zhu",
                "Michael A Hedderich",
                "Fangzhou Zhai",
                "David Ifeoluwa Adelani",
                "Dietrich Klakow."
            ],
            "title": "Is bert robust to label noise? a study on learning with noisy labels in text classification",
            "venue": "arXiv preprint arXiv:2204.09371.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, deep neural networks (DNNs) have been successfully applied in many fields (Pouyanfar et al., 2018; Alinejad et al., 2021; Liu et al., 2023) and the performance largely depends on well-labeled data. However, accessing large-scale datasets with expert annotation in the real world is difficult due to the significant time and labor costs involved. Instead, the noisy data obtained directly from the real world is often utilized in practical scenarios, even though it inevitably contains some incorrect labels. Thus, research on learning with noisy labels has gained attention in various fields such as natural language processing (NLP) (Jindal et al., 2019; Jin et al., 2021; Wu et al., 2022).\nThere are two main types of label noise in NLP: class-conditional noise (CCN) and instancedependent noise (IDN). CCN assumes that label noise is dependent on the true class, which can simulate the confusion between similar classes like\n\u2217Corresponding author\n\u201cGame\u201d and \u201cEntertainment\u201d. On the other hand, IDN assumes that label noise is dependent on the instance, which simulates the confusion caused by the characteristics of the instance. For example, a piece of news containing the phrase \u201cplayed on a pitch as slow as a bank queue\u201d may be misclassified as Economic news instead of Sports news due to the specific wording. However, most studies focus on a particular type of noise. For example, Jindal et al. (2019) introduces a non-linear processing layer to learn the noise transition matrix of CCN. Qiao et al. (2022) designs the class-regularization loss according to the characteristic of IDN. However, there is a premise for applying these methods, which is that the noise is known and of a single type.\nIn the real-world, the noise scenarios are more complex and involve a mixture of multiple noises arising from various factors, such as data ambiguity, collection errors, or annotator inexperience. Methods that specifically target one type of noise are less effective when dealing with other types of noise. This limitation hinders their applicability in real scenarios where the noise is unknown and variable.\nTo address the challenges posed by real noise scenarios, we develop an adaptive textual label noise learning framework based on pre-trained models. This framework can handle various noise scenarios well, including different types of noise and mixed noise types. Specifically, our approach begins with an adaptive warm-up stage, then divides the data into clean and noisy sets by the correctness statistic of samples, and utilizes different generalization strategies on them. In particular, there are three key designs in our approach. First, the warm-up stage is designed to automatically stop early based on the model\u2019s fit level to the noise scenario, which effectively prevents the model overfitting erroneous labels especially under IDN scenarios or with a high ratio of noise. No-\ntably, the adaptive warm-up method relies solely on the raw training set, rather than a clean validation set, making it more suitable for practical scenarios. Second, unlike previous works (Li et al., 2020; Qiao et al., 2022) that fit GMM (Permuter et al., 2006) on the training losses to separate data, the data is separated according to the correctness statistic of each sample. The correctness statistic is accumulated by assessing the consistency between the model\u2019s predictions and the given labels during the warm-up stage. This prolonged observation provides more accurate grounds for data partitioning. Third, a linear decay fusion strategy is designed to gradually correct the potential wrong labels to generate more accurate pseudo-labels by adjusting the fusion weights of the original labels and the model outputs.\nWe conduct extensive experiments on four classification datasets, considering different types of noise: class-conditional noise, instance-dependent noise, and a mixture of multiple noises. To the best of our knowledge, previous methods have not explored such mixed noise. The experimental results demonstrate that our method surpasses existing general methods and approaches the performance of methods specifically designed for particular noise types in different noise settings. Our contributions can be concluded as follows:\n\u2022 We design an early stopping method for finetuning the pre-trained models, which adapts to various noise scenarios and datasets and achieves near the best test accuracy by relying solely on training set.\n\u2022 We develop an adaptive noise learning framework based on pre-trained models, which can make good use of different types of noisy data while effectively preventing the model from overfitting erroneous labels.\n\u2022 Experimental results of various noise settings show that our approach performs comparably or even surpasses the state-of-the-art methods in various noise scenarios, which proves the superiority of our proposed method in practical scenarios."
        },
        {
            "heading": "2 Related work",
            "text": "Universal Label Noise Learning. Label noise learning methods can be divided into two groups: loss correction and sample selection methods\n(Liang et al., 2022). Loss correction tries to reduce the effect of noisy labels during training by adding regularization item in loss, designing robust network structure for noisy label and so on. For example, Wang et al. (2019) adds a reverse cross-entropy term to the traditional loss to reduce the disturbance brought by noise. ELR (Liu et al., 2020) adds a regularization term prevent the model from memorizing the noisy labels because it would not be fitted in the early training stage. Sample selection divides the data into clean and noisy subsets, and uses different methods for different subsets. For example, Co-Teaching (Han et al., 2018) maintains two networks. During training, the two networks respectively pick out some small-loss samples as clean data for each other to learn. DivideMix (Li et al., 2020) uses Gaussian mixture model to separate clean and noisy samples. The noisy samples are treated as unlabeled data, whose pseudo-labels are generated by the model. Finally, Mixmatch (Berthelot et al., 2019) method is adopted for mixed training on clean set and noisy set.\nLabels Noise Learning in NLP. The above works mainly focus on vision tasks, the researches on textual scenarios are relatively fewer. Jindal et al. (2019) and Garg et al. (2021) add additional noise modules based on lightweight models such as CNN and LSTM to learn the probability of noise transfer. (Liu et al., 2022; T\u00e4nzer et al., 2021; Zhu et al., 2022; Qiao et al., 2022) conduct research on pre-trained models and find that pre-trained models demonstrate superior performance in noise learning compared to trained-from-scratch models. However, most works focus on one certain type of noise such as CCN. Qiao et al. (2022) studies both CCN and IDN, but still conducts experiments in settings where the type of noise is known and designs specific regularization loss for IDN.\nFew works have focused on general methods of label noise in NLP. Zhou and Chen (2021) develops a general denoising framework for information retrieval tasks, which reduces the effect of noise by adding a regularization loss to samples whose model predictions are inconsistent with the given label. Jin et al. (2021) proposes an instanceadaptive training framework to address the problem of dataset-specific parameters and validates its versatility across multiple tasks. However, these methods rely on additional components, such as supplementary structures or auxiliary dataset, which limits their practicality. In contrast, our method relies\nsolely on a single network and the noisy training data, making it superior in terms of practicality."
        },
        {
            "heading": "3 Methodology",
            "text": "Problem Definition. Without loss of generalization, we take text classification as an example. Given a noisy training dataset D\u0303 = (X, Y\u0303 ) = {(xi, y\u0303i)}Ni=1, the one-hot label y\u0303i associated with sample xi is probably wrong. Our goal is to learn a classification model p(y|x; \u03b8, \u03b6) from the noisy dataset D\u0303, which generalizes well on clean test data. Specifically, the classification model p(y|x; \u03b8, \u03b6) consists of a pre-trained encoder and a classifier with parameters \u03b8 and \u03b6, respectively. Overview of the Proposed Method. To address various noise types, we propose a general learning method consisting of the warm-up training stage and the hybrid training stage, and the overall diagram is shown in Figure 1. The classification model p(y|x; \u03b8, \u03b6) is first trained by the raw data in the warm-up stage to form the initial classification ability. During the warm-up stage, we also maintain a correctness statistic to justify whether the model begins to overfit the noisy data. Once there is a sign of overfitting to noisy data, we will stop the warm-up, and move on to the subsequent hybrid training stage. During the hybrid training stage, the raw data is divided into the clean set and noisy set according to the correctness statistic, and then further train model p(y|x; \u03b8, \u03b6) by applying different training strategies to the clean set and the noisy set respectively."
        },
        {
            "heading": "3.1 Adaptive warm-up",
            "text": "The goal of this warm-up stage is to obtain a classification model that fits clean data well but not noisy data. As shown in Figure 2, however, the optimal warm-up time may vary significantly for different noise scenarios. To meet this challenge,\nan adaptive early stopping condition is involved to terminate the warm-up training before overfitting noisy data. The details of our warm-up stage are given as follows.\nTraining Data. We directly use the raw noisy dataset D\u0303 to warm up the classification model and determine when to stop early.\nLearning objective. The standard cross-entropy loss is used to warm up the model:\nLwarm = \u2212 N\u2211 i=1 y\u0303Ti log ( p(y\u0303i|xi; \u03b8, \u03b6) ) . (1)\nOverfitting regarding noisy data. Our stopping condition is based on the following Assumption 1 and Assumption 2.\nAssumption 1: As learning progresses, the clean data is fitted faster than the noisy data. We empirically demonstrate that the clean samples can be recalled earlier than the noisy samples during learning (see Figure 2), regardless of the noise types. This is mainly caused by the memorization effect (Arpit et al., 2017) which means DNNs tend to learn simple patterns before fitting noise. As a result, the whole learning process can be roughly divided into Clean Rising (CR) phase (where most clean samples are quickly learned) and Noisy Rising (NR) phase (where the noisy samples are fitted slowly), which are differentiated by backgrounds in Figure 2.\nAssumption 2: The prediction regarding noisy data tends to swing between the true label and the\nnoisy label. It originates from another memorization phenomenon regarding noisy samples(Chen et al., 2021). The results in Figure 3 also demonstrate that the prediction for the noisy samples exhibits a higher level of inconsistency with the given label due to the activation of the true label.\nCorrectness statistic. According to these two assumptions, we update the correctness statistic of training set at intervals to judge whether the model begins to overfit the noisy data. Specifically, for a given sample xi with label y\u0303i, its correctness coefficient mti is:\nmti = \u2211 t rti , (2)\nobtained by,\nrti =  1, if argmaxk\u2208K p k(xi; \u03b8, \u03b6) = argmax k\u2208K y\u0303ki\n\u22121, else. (3)\nwhere the correctness rti indicates whether the prediction of sample xi is consistent with the given label at moment t, and mti is the statistic of correctness in a given time range.\nThe higher the number of correct predictions compared to incorrect predictions for a sample, the higher the degree to which the model fits that sample. Intuitively, positive mti indicates xi has been fitted by the model at moment t.\nFurthermore, we have a proportion ratiot,\nratiot = 1\nN \u2211 i I(mti > 0), (4)\nwhich indicates the sample fitting level for the whole dataset with N samples.\nEarly stopping condition. Through observing the change of ratiot, we can determine whether the learning process has entered the NR phase. According to Assumption 1, in the CR phase, ratiot\nshould increase fast since the model fits clean samples quickly. In the NR phase, ratiot should stay within a certain range for an extended period because the noisy samples are difficult to fit (Assumption 2).\nAs a consequence, if ratiot stays within a range \u03b5 for \u03b7 times, we can assume that the learning process has entered the NR phase. To approach the optimal stopping point, we continue to warm up the model until a certain improvement in the training accuracy. The magnitude of improvement is set to be a fraction \u03c11 of the current remaining accuracy. The pseudo-code for adaptive warm-up process is shown in Appendix B. Note that this adaptive warm-up process is suitable for different noise scenarios due to Assumption 1 and Assumption 2 can be widely satisfied by different noise types."
        },
        {
            "heading": "3.2 Hybrid training",
            "text": "To further leverage the underlying values of noisy samples, we propose a hybrid training method applying different training strategies to clean samples and noisy samples respectively.\nData. Based on the correctness statistic M = {mt\u2032i }Ni=1 (assuming t\u2032 is the stopping time of the warm-up stage), the whole training set D\u0303 = {(xi, y\u0303i)}Ni=1 can be divided into the \u201cclean\u201d set D\u0303c and \u201cnoisy\u201d set D\u0303n as follows.\nD\u0303c = (Xc, Y\u0303c) = {(xi, y\u0303i)|if mt \u2032 i \u2265 l}, D\u0303n = (Xn, Y\u0303n) = {(xi, y\u0303i)|if mt \u2032 i < l},\n(5)\nwhere l is the N\u03c12-th largest correctness statistic value of M because a larger number indicates that the sample is fitted earlier and has a higher probability of being a clean sample. \u03c12 is a given percentage, which is set to 20%.\nPseudo-labeling. To minimize the side-effects of the noisy labels, we regenerate labels for both the clean set D\u0303c and noisy set D\u0303n through the pseudolabeling method combining the original labels and the prediction of the classification model. Note that there may inevitably be some noisy samples in set D\u0303c, albeit fewer than in D\u0303n.\nFor each sample (xi, y\u0303i) in clean set D\u0303c, the corresponding pseudo-labels y\u0302i is obtained by,\ny\u0302i = w t c \u00b7 y\u0303i + (1\u2212 wtc) \u00b7 p(xi; \u03b8, \u03b6), (6)\nwhere the weight wtc decays linearly with the training step t, which is calculated by,\nwtc = 1\u2212 (1\u2212 \u03b41) \u00b7 t\nT , (7)\nwhere T indicates the all training steps of this stage, wtc decays from 1 to \u03b41 due to relatively reliable labels in the clean set D\u0303c.\nLikewise, for each sample in noisy set D\u0303n, its pseudo-labels y\u0302i are obtained by,\nwtn = \u03b42 \u00b7 (1\u2212 t\nT ),\ny\u0302i = w t n \u00b7 y\u0303i + (1\u2212 wtn) \u00b7 p(xi; \u03b8, \u03b6),\n(8)\nwhere wtn decays from \u03b42 to 0 due to most labels in D\u0303n are incorrect.\nBy linearly decaying the weight of original labels, a good balance is achieved between leveraging the untapped label information and discarding noise. As the accuracy of the classification model\u2019s predictions continues to improve, the generated pseudo-labels will be closer to the true labels.\nFurthermore, for each pseudo-label y\u0302i , we adopt the sharpen function to encourage the model to generate low entropy predictions, i.e., y\u0302i = y\u0302 1/\u03c4 i / \u2225\u2225\u2225y\u03021/\u03c4i \u2225\u2225\u2225 1 , where \u03c4 is the temperature parameter and \u2225\u00b7\u22251 is l1-norm. Finally, the new whole set D\u0302 = {(xi, y\u0302i)}Ni=1 is reformed by clean and noisy sets.\nMixup. To enhance the generalization of the model, mixup technique (Zhang et al., 2017; Berthelot et al., 2019) is adopted to introduce diverse and novel examples during training. Different from mixing pictures directly in vision tasks, the text input cannot be directly mixed due to the discreteness of words. Like previous work (Berthelot et al., 2019; Qiao et al., 2022) in NLP, the sentence presentations encoded by pre-trained encoder are used to perform mixup:\n\u03bb = Beta(\u03b1, \u03b1) (9) \u03bb\u2032 = max(\u03bb, 1\u2212 \u03bb) (10) h\u2032 = \u03bb\u2032p(xi; \u03b8) + (1\u2212 \u03bb\u2032)p(xj ; \u03b8) (11) y\u2032 = \u03bb\u2032y\u0302i + (1\u2212 \u03bb\u2032)y\u0302j (12)\nwhere \u03b1 is the parameter of Beta distribution, p(x; \u03b8) is the sentence embedding which corresponds to \u201c[CLS]\u201d token. (xi, y\u0302i) and (xj , y\u0302j) are randomly sampled in the corrected set D\u0302.\nThe mixed hidden states {h\u2032i}Ni=1 and targets {y\u2032i}Ni=1 are used to train the classifier by applying entropy loss:\nLmix = \u2212 N\u2211 i=1 y\u2032 T i log ( p(h\u2032; \u03b6) ) (13)\nR-Drop. Since the prediction of the model is fused in pseudo-labeling, it needs to be as accurate as possible. Therefore, the R-Drop strategy (liang et al., 2021) is applied on noisy samples Xn to promote the model to have a consistent output distribution for the same input. R-Drop minimizes the Kullback-Leibler divergence between two distributions predicted by the model with dropout mechanism for the same sample :\nLdrop = M\u2211 i=1 1 2 ( DKL ( p1(xi; \u03b8, \u03b6)||p2(xi; \u03b8, \u03b6) ) +DKL ( p2(xi; \u03b8, \u03b6)||p1(xi; \u03b8, \u03b6) )) ,\n(14) where M is the number of noisy samples Xn. p1(xi; \u03b8) and p2(xi; \u03b8) are two predictions of xi.\nLearning objective. The total loss is:\nL = Lmix + \u03b2Ldrop, (15)\nwhere \u03b2 is the hyper-parameter of KL loss, which is set to 0.3 in experiments."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental settings",
            "text": "Datasets Experiments are conducted on four text classification datasets, including Trec (Voorhees et al., 1999), Agnews (Zhang et al., 2015), IMDB (Maas et al., 2011) and Chnsenticorp (Tan and Zhang, 2008), where Chnsenticorp is Chinese dataset and the rests are English datasets. The statistics of datasets are presented in Table 1, where the training set of Chnsenticorp is a merger of the original training and the validation set.\nNoise types The following types of noise are injected to standard datasets:\nClass-conditional noise: We choose typical symmetric (Sym) and asymmetric (Asym) noises in various class-conditional noise to conduct experiments. Symmetric noise flips a certain percentage of labels in a given category into other categories uniformly. Asymmetric noise flips labels between given similar class pairs. IMDB and Chnsenticorp\nare binary classification datasets, so their symmetric and asymmetric noises are the same.\nInstance-dependent noise: Follow (Qiao et al., 2022), a LSTM classifier is trained to determine which sample features are likely to be confused. The labels of the samples closest to the decision boundary are flipped to their opposite category based on the classifier prediction probability. All datasets except Trec are used because of its small sample size and uneven categories. Main experiments only show results for a single type of noise with a ratio of 40%, results for other ratios can be found in the Appendix C.\nMixture of multiple noises: We mix different types of noise to verify the ability of the algorithm to deal with complex and unknown scenarios. For datasets with only two types of noise, including Trec, IMDB and Chnsenticorp, we mix the two noises evenly. Specifically, Sym and Asym are evenly mixed on Trec, Asym and IDN are evenly mixed on IMDB and Chnsenticorp. For Agnews, we mix three noises unevenly for a larger challenge. The result of evenly mixing of two types of noise on Agnews is in Appendix C."
        },
        {
            "heading": "4.2 Baselines",
            "text": "BERT-FT (Devlin et al., 2018): the benchmark classification model without special methods, directly trained on noisy data. Co-Teaching (CT for short) (Han et al., 2018): a well-known noise learning method, which maintains two models simultaneously and lets each model select clean samples for the other to train. ELR (Liu et al., 2020): ELR designs a regularization term to prevent the model from memorizing noisy labels by increasing the magnitudes of the coefficients on clean samples to counteract the effect\nof wrong samples. SelfMix(Qiao et al., 2022): SelfMix first warms up the model and then uses GMM to separate data to perform semi-supervised self-training, and designs the normalization method according to the characteristics of IDN, resulting in a significant performance improvement compared to other methods lacking specific designs.\nFor fair comparison, the backbone model of each method is the same, including a pre-trained encoder BERT1 and a two-layer MLP. The training time of all methods except SelfMix is set to 6 epochs and their hyper-parameters are the same under different noise settings. For SelfMix, we follow the instructions of its paper to set different hyper-parameters for different datasets and noise types. Specially, the warm time for symmetric and asymmetric noise is 2 epochs, the warm time for instance-dependent noise is 1 epoch, and the semi-supervised selftraining time is 4 epochs. The average results are run five times on the code base provided by Qiao et al. (2022)."
        },
        {
            "heading": "4.3 Parameter settings",
            "text": "The same hyper-parameters as baselines remain unchanged, where the maximum sentence length is 256, the learning rate is 1e-5, the drop rate is 0.1, the size of middle layer of MLP is 768 and optimizer is set to Adam. Additionally, the temperature \u03c4 is 0.5, \u03b1 of Beta distribution is 0.75.\nThere are some specific hyper-parameters in our method. Specifically, in warm up stage, the interval s of monitoring training set is 1/10 epoch, the converge range \u03b5 is 0.01, the times \u03b7 is 3 and the\n1the weight of BERT for English task is initialized with bert-base-uncased, and the wight for Chinese task is initialized with chinese-bert-wwm-ext of HIT.\nfraction \u03c11 is 0.1, the batch size is 12. In hybrid training stage, the fraction \u03c12 to divide data is 0.2, the threshold \u03b41 is 0.9 and \u03b42 is 0.4, and the loss weight \u03b2 is 0.3. The batch size of clean set in hybrid training stage is 3 and the batch size of noisy set is 12 to match the ration \u03c12 of their numbers.\nThe duration of the warm up stage varies under different noise settings. The hybrid training stage lasts for 4 epochs to compare with SelfMix. For generality, the parameters used in all cases are same as above."
        },
        {
            "heading": "4.4 Main Results",
            "text": "Table 2 demonstrates the main results of our method and baselines.\nBaselines BERT-FT is highly affected by noise, with its performance and stability decreasing significantly as the complexity of noise increases. CT and ELR demonstrate strengths in handling simple noise scenarios, such as Trec with 40% symmetric noise and a mixture of symmetric and asymmetric noise. However, they perform poorly under more complex noise settings, such as IDN noise. In contrast, SelfMix excels in cases involving IDN due to its design of class-regularization loss and reduced warm-up time. Among all the methods, only SelfMix needs to adjust specific hyper-parameters based on the dataset and noise type.\nOur method In contrast to the baselines, our method consistently performs well across all noise scenarios. With simple CCN setting, our method exhibits further improvements in performance compared to well-performing baselines like ELR. With IDN setting, our method outperforms general baselines and even surpasses SelfMix in certain cases, such as on Agnews and Chnsenticorp. Moreover, when confronted with mixed noise, our method consistently achieves top or near-top results while maintaining stability. In conclusion, our approach demonstrates superior performance and adaptability in various noise scenarios, making it more practical for real-world scenarios."
        },
        {
            "heading": "5 Analysis",
            "text": "In this section, some experiments are designed to make a more comprehensive analysis of our proposed method. The main results are shown in Table 3 and the correspond analyses are as follows. More analyses can be found in Appendix D."
        },
        {
            "heading": "5.1 Ablation experiment",
            "text": "To verify that each component contributes to the overall approach, we remove each component separately and check the test accuracy. To remove linear decay fusion, the original labels of clean set are kept and the pseudo-labels of noisy set are generated by the model. For the design of correctness statistic, the relevant parts including early stopping and linear decay fusion are removed and replaced by normal training. The standard cross entropy loss are directly applied on all samples and the corresponding pseudo labels to remove mixup operation. For R-Drop, the KL-divergence term is eliminated. The first part of Table 3 shows each component is helpful to the final performance. Correctness statistic and R-Drop are more critical to complex setting (i.e. 40% IDN), because the model is greatly affected by this noise. And due to the introduction of linear decay fusion, R-Drop becomes more important in the aspect of keeping the model stable."
        },
        {
            "heading": "5.2 Analysis of early stopping",
            "text": "Stopping warm up properly is important for adapting to various noise cases and datasets. To verify this view, experiments with different warm-up times are performed. The numbers of war-up epoch are set to {1,2,3, 4} and the results are shown in the\nsecond part of Table 3. We can observe that no one fixed warm-up time is suitable for all situations. For simple cases such as asymmetric noise, warm up for 3 epochs is beneficial. For complex cases such as instance-dependent noise, warm up for 1 epoch is enough. But adaptive warm-up works better than fixed times on processing all cases. Therefore, finding appropriate stopping point according to different noisy training datasets may be a good strategy of noise learning based on pre-trained models.\nTo check whether our early stopping method finds the stopping point close to the maximum obtainable test accuracy (MOTA), we draw some finding instances compared with other two heuristic approaches, including stopping through noise ratio (Arpit et al., 2017) and clean validation set (we use test set for validation here). As shown in Figure 4, the estimated start and end points of early stopping (ES) are close to the MOTA under different noise settings. And in some cases ES is closer to the MOTA than stopping through noise ratio (Ratio) especially under IDN. Because IDN is easier fitted by deep models, which resulting in high training accuracy at early stage. Compared with the two methods relying on additional conditions, our method only relies on the original noisy training set, which is more adaptable to the real world."
        },
        {
            "heading": "5.3 The design of correctness statistic",
            "text": "During warm up stage, a correctness statistic is maintained to judge the time of early stopping. In addition, it is used to separate the training data into clean set and noisy set for training in hybrid stage. We directly select the top 20% samples of the high-\nest value in the correctness statistic as the clean set. It is different from previous noise-learning works (Li et al., 2020; Qiao et al., 2022; Garg et al., 2021) where the sets are separated by GMM or BMM. To make comparison, GMM is used in hybrid training stage and divide data at each epoch. In addition to the results, the ratios of correctly separation (i.e. how many samples in clean sets are truly clean) are also listed in Table 3, and highlighted with the gray background. As shown, the right ratio of our method is slightly higher than that of GMM, and the performance is better especially under the higher noise ratio settings."
        },
        {
            "heading": "5.4 Computational cost",
            "text": "Compared to previous noise learning approaches that include a normal warm-up phase (Han et al., 2018; Qiao et al., 2022), our method involves an additional cost incurred by inferring the entire training set at intervals during the adaptive warm-up phase. This cost is directly proportional to the number of training set samples and the frequency of inference. To strike a balance between monitoring and cost, we set the interval to 1/10 epoch in our experiments. Additionally, during the hybrid training stage, we reduce the cost of inferring the training set and fitting the GMM compared to SelfMix. Table 4 provides an example of the time costs of BERT-FT, SelfMix and our method under the same settings, with all methods trained on a single GeForce RTX 2080Ti."
        },
        {
            "heading": "6 Conclusion",
            "text": "Based on the unknown and complex nature of noise, we propose a noise learning framework based on pre-trained models that can adapt to various textual noise scenarios. This framework automatically stops the warm-up process based on the magnitude and complexity of the noise to prevent the model from overfitting noisy labels. To further leverage mislabeled data, the linear decay fusion strategy is combined with mixup and R-Drop to improve performance while maintaining stability. Experimen-\ntal results demonstrate that our method achieves performance comparable to state-of-the-art in all settings within common noise range.\nLimitations\nWe would like to introduce a versatile noise framework that can adapt to various noise scenarios and have conducted extensive experiments across different simulated scenarios to evaluate the performance. However, it is crucial to acknowledge that we didn\u2019t experiment on real textual noise scenarios. If the noise learning method can be verified in real industrial datasets, it will be more convincing. Furthermore, due to the necessity of monitoring the training set during the warm-up stage, the overall training time of our method tends to be longer compared to other approaches, especially when dealing with large datasets like Agnews. Resolving this issue or exploring alternative approaches to reduce training time is a direction that requires further investigation, which we leave to future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the Science and technology support program of Sichuan Province under Grant 2022YFG0313."
        },
        {
            "heading": "A Examples of Assumption",
            "text": "The more examples of two assumptions are shown in Figure 5 and 6."
        },
        {
            "heading": "B Pseudo-code of Adaptive Warm-up",
            "text": "We summarize the pseudo-code of adaptive warmup in Algorithm 1."
        },
        {
            "heading": "C Experiments",
            "text": "C.1 Detailed Results\nThe detailed results are given in this section. Table 5 shows the results of different ratios of CCN, Table 6 shows the results of different ratios of IDN, and the results of the mixture of two types of noise on Agnews are supplemented in Table 7.\nClass-conditional noise: With this simple assumption, our proposed method basically achieves the best scores across all noise cases in Table 5. All methods can obtain very good performance under a low noise ratio due to the deep model is robust to simple noise. However, there are still some gains to be made with our approach in some cases such as the results on Trec and Chnsenticorp with 20% asymmetric label noise. Under a high noise ration i.e. 40%, the baselines are all affected to a greater or lesser extent, but our method performs well on different datasets and outperforms all methods. Additionally, we list the results on clean datasets without noise. The performance gap between clean data and noisy data are getting smaller by applying our method.\nInstance-dependent noise: Table 6 shows that the complexity of IDN results in a substantial decrease in performance for general methods CT and ELR, indicating that they cannot cope well with this noise setting. In these experiments, SelfMix gets more best scores, which is related to its special designs based on noise type such as reducing the warm-up time, designing class-regularization loss and etc. In contrast, our method exhibits strong performance in all cases and achieves scores close to or even better than SelfMix without making assumptions about the type of noise. Particularly noteworthy are the cases where SelfMix underperforms, such as Trec and Chnsenticorp with 10% noise, whereas our method surpasses even the strongest baseline. This further highlights the versatility and effectiveness of our method across different scenarios.\nMixture of multiple noises: As shown in Table 7, mixed noise pose challenges, and various baselines exhibit distinct characteristics. BERT-FT is highly impacted by noise, particularly with a significant drop in the Last score. CT and ELR demonstrate their strengths in handling simple noise scenarios, such as a mixture of symmetric and asymmetric noise. On the other hand, SelfMix excels in cases involving IDN. In contrast, our method consistently achieves top or near-top performance while maintaining stability across all scenarios. It offers a more flexible and adaptable solution for unknown noise cases.\nIn conclusion, our method gets good enough scores under all noise conditions without knowing the noise type, noise ratio or the characteristics of dataset. Besides, the same hyper-parameters are used for all datasets and noise settings. These factors make our approach more practical in real scenarios."
        },
        {
            "heading": "D More Analysis",
            "text": "D.1 Analysis of linear decay fusion\nIn linear decay fusion, we set \u03b41 = 0.1 to let the weight of label of cleaner data decay from 1 to 0.1, and set \u03b42 = 0.6 to let that of noisier data from 0.6 to 0. To understand the role of two thresholds, Table 8 lists some results of fixing one of them and changing the other. The first three rows shows that \u03b41 has little effect on asymmetric noise but limits the performance of IDN. Because IDN is easily fitted by the model, some noisy samples are inevitably assigned to cleaner set. It is more appropriate to reduce the weight to a small value. As can be observed from the last two lines, small value of \u03b42 is also detrimental to IDN, because it introduces much noise from noisier set. But large value lets some valuable information to be lost, the trade-off value 0.6 is a good choice. The setting of the two thresholds allows the proposed method to handle complex noise cases and cover possible noise situations, thus making it well applicable to other datasets or the real world.\nD.2 Stability Analysis\nIn main experiments, the time of hybrid training is set to 4 epochs to fairly compare with SelfMix. The stability of noise learning methods is also an important aspect, and the training time is extended to verify the stability. Since the warm up time is adaptive, we set different hybrid training times.\nThere are two modes to extend because of the existence of linear decay fusion. The first is that the decay time is extended as same as the training time, which is expressed as a uniform number in the table. The second is that the decay time is fixed to 4 epochs and the total training time is extended, which is expressed in the form of adding numbers. For example, \u201c4+2\u201d in Table 9 means the weight of original labels decays to the wanted threshold in four epochs and remains unchanged during two more epochs. The results shows that as training time increases, the accuracy of the last epoch decreases but not by much. And interestingly, the Best scores get better in some cases because there are more monitored moments.\nD.3 The trend of correctness statistic Figure 7 displays the trend of the correctness statistic ratiot along with the curve representing the\nproportion of samples correctly partitioned into the clean set under different \u03c12 values during warm-up. Each point on the correctness statistic curve corresponds to a monitored situation. We observe the appearance of a turning point in various scenarios, and its occurrence time is determined by the speed at which the model fits the training samples. When the turning point appears, the frequency of occurrences within a fixed range quickly rises. Therefore, we set the value of \u03b7 to 3 and the corresponding range \u03f5 to 0.01. This is a moderate choice that can accommodate various noise scenarios. A larger \u03b7 is also acceptable but would result in a later early stop time.\nFor \u03c12, we compare two values, 0.5 and 0.2. Under simpler noise settings, such as with 20% symmetric noise, the two \u03c12 curves remain relatively flat, indicating that the cleanliness of the top 20% and top 50% sets is similar. However, under more complex settings, such as with 40% idn, \u03c12 = 0.2 exhibits some advantages. Additionally, a smaller \u03c12 value can accommodate a higher noise ratio.\nAlgorithm 1: Adaptive Warm-up Input: \u03b8 and \u03b6, training set (X, Y\u0303 ), monitoring interval s, converge range \u03b5, times \u03b7, accuracy\nfraction \u03c11 Initial: P = \u2205, enter = False ; while t < MaxStep do\nDraw a mini-batch {(xb, yb); b \u2208 (1, ..., B)} from (X, Y\u0303 ) ; Optimize \u03b8 and \u03b6 by Equation 1 ; // standard training /* monitoring the training process at intervals */ if t mod s == 0 then\nCompute rti by Equation 3, (xi, y\u0303i) \u2208 (X, Y\u0303 ) ; mti = \u2211 t r t i ; // update the correctness statistic\nratiot = 1N \u2211\ni I(mti > 0) ; // calculate the proportion of samples that have been fitted P\u2190 P \u222a {\u2308 ratiot\n\u03b5\n\u2309 } ; // determine the range that ratiot stays inside\nif not enter then if P has \u03b7 same items then\nenter = True ; // enter the NR phase Compute training accuracy acc ; stop_acc = acc+ (1\u2212 acc) \u00b7 \u03c11 ;\n// calculate the space available for further warming up\nend else\nCompute training accuracy acc ; if acc \u2265 stop_acc then\nBreak ; // stop warming up end\nend end\nend Output: \u03b8 and \u03b6, M = {mti}Ni=1 ; // output trained model and correctness statistic"
        }
    ],
    "title": "Adaptive Textual Label Noise Learning based on Pre-trained Models",
    "year": 2023
}