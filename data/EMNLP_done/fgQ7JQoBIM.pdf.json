{
    "abstractText": "Semantic textual similarity (STS), a cornerstone task in NLP, measures the degree of similarity between a pair of sentences, and has broad application in fields such as information retrieval and natural language understanding. However, sentence similarity can be inherently ambiguous, depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called Conditional STS (C-STS) which measures sentences\u2019 similarity conditioned on an feature described in natural language (hereon, condition). As an example, the similarity between the sentences \u201cThe NBA player shoots a three-pointer.\u201d and \u201cA man throws a tennis ball into the air to serve.\u201d is higher for the condition \u201cThe motion of the ball\u201d (both upward) and lower for \u201cThe size of the ball\u201d (one large and one small). C-STS\u2019s advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS and (2) enables fine-grained language model evaluation through diverse natural language conditions. We put several state-of-the-art models to the test, and even those performing well on STS (e.g. SimCSE, Flan-T5, and GPT-4) find CSTS challenging; all yielding Spearman correlation scores below 50. To encourage a more comprehensive evaluation of semantic similarity and natural language understanding, we make nearly 19K C-STS examples and code available for others to train and test their models. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Ameet Deshpande"
        },
        {
            "affiliations": [],
            "name": "Carlos E. Jimenez"
        },
        {
            "affiliations": [],
            "name": "Howard Chen"
        },
        {
            "affiliations": [],
            "name": "Vishvak Murahari"
        },
        {
            "affiliations": [],
            "name": "Victoria Graf"
        },
        {
            "affiliations": [],
            "name": "Tanmay Rajpurohit"
        },
        {
            "affiliations": [],
            "name": "Ashwin Kalyan"
        },
        {
            "affiliations": [],
            "name": "Danqi Chen"
        },
        {
            "affiliations": [],
            "name": "Karthik Narasimhan"
        }
    ],
    "id": "SP:daa9c7d2ab88bb260e0386803c2ca3dd63924fef",
    "references": [
        {
            "authors": [
                "Mohamed Abdalla",
                "Krishnapriya Vishnubhotla",
                "Saif M. Mohammad."
            ],
            "title": "What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study",
            "venue": "ArXiv:2110.04845 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2014 task 10: Multilingual semantic textual similarity",
            "venue": "Proceedings of the",
            "year": 2014
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",
            "venue": "Proceedings",
            "year": 2016
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings",
            "year": 2012
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor GonzalezAgirre",
                "Weiwei Guo."
            ],
            "title": "SEM 2013 shared task: Semantic textual similarity",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the",
            "year": 2013
        },
        {
            "authors": [
                "Akari Asai",
                "Timo Schick",
                "Patrick Lewis",
                "Xilun Chen",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Hannaneh Hajishirzi",
                "Wen-tau Yih."
            ],
            "title": "Task-aware retrieval with instructions",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 3650\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Cer",
                "Yinfei Yang",
                "Sheng-yi Kong",
                "Nan Hua",
                "Nicole Limtiaco",
                "Rhomni St. John",
                "Noah Constant",
                "Mario Guajardo-Cespedes",
                "Steve Yuan",
                "Chris Tar",
                "Brian Strope",
                "Ray Kurzweil."
            ],
            "title": "Universal sentence encoder for English",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Rumen Dangovski",
                "Hongyin Luo",
                "Yang Zhang",
                "Shiyu Chang",
                "Marin Soljacic",
                "ShangWen Li",
                "Scott Yih",
                "Yoon Kim",
                "James Glass."
            ],
            "title": "DiffCSE: Difference-based contrastive learning for sentence embeddings",
            "venue": "Proceedings of the 2022",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela",
                "Holger Schwenk",
                "Lo\u00efc Barrault",
                "Antoine Bordes"
            ],
            "title": "Supervised learning of universal sentence representations",
            "year": 2017
        },
        {
            "authors": [
                "Simon De Deyne",
                "Daniel J Navarro",
                "Amy Perfors",
                "Gert Storms."
            ],
            "title": "Structure at every scale: A semantic network account of the similarities between unrelated concepts",
            "venue": "Journal of Experimental Psychology: General, 145(9):1228.",
            "year": 2016
        },
        {
            "authors": [
                "Simon De Deyne",
                "Amy Perfors",
                "Daniel J Navarro."
            ],
            "title": "Predicting human similarity judgments with distributional models: The value of word associations",
            "venue": "Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Joseph L. Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological Bulletin, 76:378\u2013382.",
            "year": 1971
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Hua He",
                "Kevin Gimpel",
                "Jimmy Lin."
            ],
            "title": "MultiPerspective Sentence Similarity Modeling with Convolutional Neural Networks",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576\u20131586, Lisbon,",
            "year": 2015
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Humeau",
                "Kurt Shuster",
                "Marie-Anne Lachaux",
                "Jason Weston."
            ],
            "title": "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Ting Jiang",
                "Jian Jiao",
                "Shaohan Huang",
                "Zihan Zhang",
                "Deqing Wang",
                "Fuzhen Zhuang",
                "Furu Wei",
                "Haizhen Huang",
                "Denvy Deng",
                "Qi Zhang."
            ],
            "title": "PromptBERT: Improving BERT sentence embeddings with prompts",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR",
            "year": 2020
        },
        {
            "authors": [
                "Rensis Likert."
            ],
            "title": "A technique for the measurement of attitudes",
            "venue": "Archives of psychology.",
            "year": 1932
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "European Conference on Computer Vision.",
            "year": 2014
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "I. Lopez-Gazpio",
                "M. Maritxalar",
                "A. Gonzalez-Agirre",
                "G. Rigau",
                "L. Uria",
                "E. Agirre."
            ],
            "title": "Interpretable semantic textual similarity: Finding and explaining differences between sentences",
            "venue": "Knowledge-Based Systems, 119:186\u2013199.",
            "year": 2017
        },
        {
            "authors": [
                "George A Miller",
                "Walter G Charles."
            ],
            "title": "Contextual correlates of semantic similarity",
            "venue": "Language and Cognitive Processes.",
            "year": 1991
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing ChatGPT",
            "venue": "https:// openai.com/blog/chatgpt.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4",
            "venue": "Accessed: 2023-05-23.",
            "year": 2023
        },
        {
            "authors": [
                "Zarana Parekh",
                "Jason Baldridge",
                "Daniel Cer",
                "Austin Waters",
                "Yinfei Yang."
            ],
            "title": "Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO",
            "venue": "arXiv:2004.15020",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural lan",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Hongjin Su",
                "Weijia Shi",
                "Jungo Kasai",
                "Yizhong Wang",
                "Yushi Hu",
                "Mari Ostendorf",
                "Wen-tau Yih",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Tao Yu."
            ],
            "title": "One embedder, any task: Instruction-finetuned text embeddings",
            "venue": "Findings of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q. Tran",
                "Xavier Garcia",
                "Jason Wei",
                "Xuezhi Wang",
                "Hyung Won Chung",
                "Dara Bahri",
                "Tal Schuster",
                "Steven Zheng",
                "Denny Zhou",
                "Neil Houlsby",
                "Donald Metzler."
            ],
            "title": "UL2: Unifying language learning paradigms",
            "venue": "In",
            "year": 2023
        },
        {
            "authors": [
                "Amos Tversky."
            ],
            "title": "Features of similarity",
            "venue": "Psychological Review, 84:327\u2013352.",
            "year": 1977
        },
        {
            "authors": [
                "Andreas Veit",
                "Serge J. Belongie",
                "Theofanis Karaletsos."
            ],
            "title": "Conditional similarity networks",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1781\u20131789.",
            "year": 2016
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
            "venue": "arXiv preprint 1905.00537.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "EMNLP 2018, page 353.",
            "year": 2018
        },
        {
            "authors": [
                "Shailaja Keyur Sampat",
                "Siddhartha Mishra",
                "Sujan Reddy A",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen."
            ],
            "title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "CoRR,",
            "year": 2019
        },
        {
            "authors": [
                "Han-Jia Ye",
                "Yi Shi",
                "De-Chuan Zhan"
            ],
            "title": "Identifying ambiguous similarity conditions via",
            "year": 2022
        },
        {
            "authors": [
                "Han-Jia Ye",
                "Yi Shi",
                "De-Chuan Zhan."
            ],
            "title": "Identifying Ambiguous Similarity Conditions via Semantic Matching",
            "venue": "ArXiv:2204.04053 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "\u201cThe NBA player shoots a three-pointer.\u201d and \u201cA man throws a tennis ball into the air to serve.\u201d is higher for the condition \u201cThe motion of the ball\u201d (both upward) and lower for \u201cThe size of the ball\u201d (one large and one small). C-STS\u2019s advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS and (2) enables fine-grained language model evaluation through diverse natural language conditions. We put several state-of-the-art models to the test, and even those performing well on STS (e.g. SimCSE, Flan-T5, and GPT-4) find CSTS challenging; all yielding Spearman correlation scores below 50. To encourage a more comprehensive evaluation of semantic similarity and natural language understanding, we make nearly 19K C-STS examples and code available for others to train and test their models. 1"
        },
        {
            "heading": "1 Introduction",
            "text": "Over the years, natural language processing (NLP) has progressed through the co-evolution of model design (e.g. architectures, training methods) and evaluation methods for language tasks (Wang et al., 2018, 2019; Hendrycks et al., 2021). A common task used to evaluate NLP models has been Semantic Textual Similarity (STS) (Agirre et al., 2012), which evaluates the models\u2019 ability to predict the\n* Equal Contribution 1Code: www.github.com/princeton-nlp/c-sts\nsemantic similarity between two sentences. Several diverse STS datasets are popularly used, with prior work expanding the STS task to multiple domains and languages (Agirre et al., 2013, 2014, 2015, 2016; Cer et al., 2017; Abdalla et al., 2021). STS tasks have been a component of the popular GLUE natural language understanding benchmark (Wang et al., 2018) and are a key evaluation tool for sentence-representation learning specifically (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Gao et al., 2021, inter alia).\nDespite its popularity, STS may be inherently ill-defined. The general semantic similarity of two sentences can be highly subjective and vary wildly depending on which aspects one decides to focus on. As observed in several studies, ambiguity in similarity judgements of word or sentence pairs can be reduced with the help of context for both hu-\nmans (De Deyne et al., 2016a,b) and models (Veit et al., 2016; Ye et al., 2022a; Lopez-Gazpio et al., 2017; Camburu et al., 2018).\nConsidering the importance of STS tasks for evaluating sentence representations, we propose a new task called Conditional STS (C-STS), illustrated in Figure 1, which seeks to disambiguate the similarity of two sentences by measuring similarity within the context of a condition sentence.\nC-STS, uses free-form natural language conditions, enabling us to evaluate and probe natural language understanding for myriad fine-grained aspects. Figure 1 illustrates two conditions (\u201cThe base of the object\u201d and \u201cThe way the object is propelled\u201d) which probes language models\u2019 conception of similarity for different aspects concerning water sports and physical reasoning. Since conditions themselves are unconstrained sentences, they allow us to evaluate a precise, grounded, and multifaceted notion of sentence similarity.\nTo comprehensively test models on C-STS, we create the C-STS-2023 dataset which includes 18,908 instances containing sentence pairs, a condition, and a scalar similarity judgement on the Likert scale (Likert, 1932). We find that even stateof-the-art sentence encoders and large language models perform poorly on our task. Although SimCSE (Gao et al., 2021) and GPT-4 (OpenAI, 2023a) are among the best-performing systems, their relatively poor Spearman correlation of 47.5 and 43.6 respectively, points to significant room for improvement (SimCSE achieves a Spearman correlation of\n88.09 on STS-B validation splits for comparison). We believe that C-STS provides a testbed for potentially novel modeling settings and applications. Toward this end, we propose and evaluate a unique encoding setting (a tri-encoder) and objectives (a quadruplet contrastive loss with hard negatives) that take advantage of C-STS\u2019s threesentence inputs and paired high- and low-similarity instances.\nOur qualitative analysis shows that models find C-STS challenging when tested on different aspects of the same sentence pair rather than testing an unconditional and ambiguous notion of similarity. We hope that future work evaluates on C-STS in addition to STS tasks to comprehensively benchmark semantic similarity in language models."
        },
        {
            "heading": "2 Methodology",
            "text": "The C-STS task requires sentence pairs, conditions which probe different aspects of similarity, and the similarity label for a given sentence pair and condition. This section describes the technical details involved in creating the dataset."
        },
        {
            "heading": "2.1 Background: Semantic textual similarity",
            "text": "Semantic textual similarity (STS) (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017) is a task which requires machines to make similarity judgements between a pair of sentences ({s1, s2}). STS measures the unconditional semantic similarity between sentences because the annotator making the similarity assessment must infer which as-\npect(s) of the sentences are being referred to. Formally, consider conditions (ci \u2208 C) that refer to disjoint aspects of the sentences, then the similarity of the two sentences may be represented as:\n|C|\u2211\ni=1\nwi simci (s1, s2) s.t. \u2211\ni\nwi = 1\nHere, wi is the weight assigned by the annotator to the condition ci, and simci (s1, s2) is the similarity of the sentences with respect to the condition. These weights are latent to the task and each annotator has their own interpretation of them which helps marginalize similarity, thus introducing ambiguity in the task. C-STS seeks to disambiguate the STS task by measuring similarity conditioned by a single aspect specified in natural language."
        },
        {
            "heading": "2.2 Conditional semantic textual similarity",
            "text": "C-STS is a task comprised of quadruplets containing two sentences (a sentence pair), a natural language condition, and a similarity assessment ({s1, s2, c, y}). Crucially, we do not place any strict constraints on c, allowing it to be any relevant phrase. This allows us to probe potentially any possible aspect of similarity that may be considered between sentences."
        },
        {
            "heading": "2.2.1 Sentence Data Collection",
            "text": "The first stage of making the C-STS dataset is to acquire the sentence pairs that will later be used in eliciting conditioning statements from annotators.\nWe source sentence pairs {s1, s2} for our dataset from image-captioning datasets through a two-step process: (1) generate candidate text pairs through dense retrieval from the corresponding image representations and (2) filter out candidates that are irrelevant or ineffectual for our purposes.\nImage Retrieval Image-captioning datasets provide a compelling data source because image pair similarity and caption (text) pair similarity encode different semantics (Parekh et al., 2021). Imagerepresentations thus serve as an informative latent variable which can represent their captions in ways that are not captured by text retrievers.\nSince current sentence representation models overlook aspects of conditional similarity, we utilize both the image and text to retrieve sentence pairs which form the foundation of our dataset.\nWe aim to derive sentence pairs from an imagecaption dataset D to aid in creating conditioning statements. To do this, we first generate a store of\nimage pairs, or PI . Each pair, denoted by Ii, Ij , is such that Ij is amongst the top-k most similar images to Ii, determined by the cosine distance metric of their respective image representations obtained via an image encoder EI(\u00b7). After establishing PI , we convert it into a sentence pair store (PS) by replacing each image in a pair with its corresponding caption. When each image Ii \u2208 D is associated with a set of sentences {s}i we take all sentence pairs from the Cartesian product {s}i \u00d7 {s}j for each image pair Ii, Ij \u2208 PI .\nCandidate Filtering After acquiring initial sentence pairs through image retrieval, we perform additional filtering to eliminate sentence pairs which are ill-suited for our task.\nSpecifically, we aim to include only pairs of sentences for which the unconditional similarity is somewhat ambiguous, as this incentivizes models to rely on the condition when reasoning about the conditional similarity.\nTo this end, we avoid high similarity sentence pairs by filtering out those with a high bag-ofwords intersection over union and avoid low similarity sentence by choosing sentences with moderate or high cosine similarity of their SimCSE embeddings (Gao et al., 2021). See Appendix A.2 for a full description of all filtering criteria used.\nDataset sources For the construction of sentence pairs candidates, we use two image-caption datasets: the train split from the 2014 MS COCO dataset (Lin et al., 2014) containing \u223c 83,000 images, and Flickr30K (Young et al., 2014) containing \u223c 31,000 images. Each dataset is processed separately and we do not intermix them during the retrieval stage. We use CLIP-ViT (Radford et al., 2021) to encode images and include the specific filtering criteria in Table 6 of Appendix A.2."
        },
        {
            "heading": "2.2.2 Annotation Methodology",
            "text": "For each sentence pair in the store (PS), we wish to collect conditions and semantic similarity annotations for each sentence pair and condition triplet, {s1, s2, c}. As c is a free-form natural language sentence, the annotator is provided with a highlevel of control over which aspect to condition on. Human annotations are acquired through Mechanical Turk in a 3-stage process.\nStage 1: Choosing a high-quality worker pool In the first stage, we design a qualification test to select workers who excel at our task. Specifically, we test two skills: (1) The quality of conditions they\nwrite for a given sentence pair and (2) semantic similarity judgements for a triplet {s1, s2, c}. We choose a pool of 271 workers who perform well on both tasks and restrict subsequent stages to include only workers from this pool. See Appendices C.1 and C.2 for an example of these tests.\nStage 2: Condition annotation After sourcing sentence pairs {s1, s2} using the strategy discussed in the Section 2.2.1, we instruct workers to annotate each pair with one condition such that the similarity in its context is high (C-High) and one such that the similarity in its context is low (C-Low). Example: s1 : A large green ball was bouncing on the street s2 : I bought a small green avocado C-High : The color of the object C-Low : The size of the object\nWe do not place any constraints on the conditions other than that they should be semantically unambiguous phrases and relevant to the sentence pair (Appendix C.1).\nStage 3: Condition verification and similarity assessment The output of annotations from the previous stage are triplets {s1, s2, c} with a binary similarity assessment (high or low). In this stage we ask new annotators to assign a similarity on a Likert scale (Likert, 1932) (as an integer between 1 and 5) as is common with semantic textual similarity tasks (Agirre et al., 2012). In addition to assigning a similarity, we also use this stage to verify if the conditions from the previous stage are pertinent to the sentence pairs, filtering out potentially low quality examples. At the end of this stage, we have {s1, s2, c, y} quadruplets which have passed a layer of human verification (Appendix C.2)."
        },
        {
            "heading": "3 Dataset Analysis",
            "text": "Dataset statistics To ensure high-quality, faithful, and diverse annotations, we collect a total of 20,000 instances and perform quality assurance (Section 5.3) resulting in a total of 18,908 instances as part of the C-STS-2023 dataset. Following standard practice, we create train, validation, and test splits in a 60 : 15 : 25 ratio. We present the distribution of similarity scores, which are discrete numbers between [1, 5], in Figure 4. We also measure the inter-annotator agreement on a random sample of 100 examples with three independent annotations and find Fleiss\u2019 kappa score (Fleiss,\n1971) to be 0.61 which implies substantial interannotator agreement. Average length of sentences and conditions is 12.6 and 5.3 words.\nQualitative analysis C-STS allows us to evaluate the generally fuzzy notion of sentence similarity with greater fidelity. We illustrate this in Table 1, where precise and discriminative conditions allow a targeted, fine-grained, and grounded definition of sentence similarity. The following is a representative instance where the conditions tease out nuanced and hidden similarities and differences between the two lexically similar sentences on surfing: Consider s1: \u201cA windsurfer skims the water. . . \u201d and s2: \u201cThe surfer is riding a wave. . . \u201d). While the sentences are significantly dissimilar based on the condition \u201dthe way the object is propelled\u201c as they talk about windsurfing and surfing respectively (the former uses a sail whereas the latter depends on the wave), they are very similar in context of the condition \u201dthe base of the object\u201c as both windsurfing and surfing use a similar board.\nOur diverse set of conditions provides broad support over the distribution of conditions and enables a holistic and multi-faceted evaluation of sentence similarity. For example, the conditions for the sentences on Tennis in Table 1 test similarity both on the sport being played (which requires understanding lexical and knowledge artifacts) as well as the number of people (which requires reasoning and commonsense capabilities)."
        },
        {
            "heading": "4 Baselines",
            "text": "We evaluate our dataset on several baselines which can be categorized into (1) Fine-tuning baselines, which are pre-trained models finetuned on the CSTS training split, and (2) Large language models (LLMs) baselines, which are evaluated using instructions and in-context examples."
        },
        {
            "heading": "4.1 Fine-tuning baselines",
            "text": "We evaluate three sentence encoder models RoBERTa (Liu et al., 2019), supervised SimCSE (Gao et al., 2021) and unsupervised DiffCSE (Chuang et al., 2022). SimCSE and DiffCSE represent state-of-the-art sentence encoder models which are particularly strong on STS tasks. For both SimCSE and DiffCSE, we use the RoBERTa pre-trained varieties.\nEncoding configurations Encoder-only Transformer models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), initially performed\nregression finetuning for STS tasks by simply concatenating the sentences and encoding them together before generating a prediction; let us call this type of architecture a cross-encoder. Recent approaches instead opt to encode sentences separately and compare their similarity using a distance metric, such as the cosine distance Reimers and Gurevych (2019); which we will call a bi-encoder. While DiffCSE and SimCSE were designed with the bi-encoder setting in mind, we observe that they work well in the cross-encoder setting as well.\nFor our baselines, we evaluate each model in both settings. For the cross-encoder configuration, we encode the triplet containing the sentences and the condition ({s1, s2, c}), and the output is a scalar similarity score \u2013 f\u03b8(s1; s2; c). For the bi-encoder configuration (Reimers and Gurevych, 2019), the sentences of a pair are encoded independently along with the condition using a Siamese network and their cosine similarity is computed \u2013 cos(f\u03b8(s1; c), f\u03b8(s2; c)).\nIn addition to the bi- and cross-encoder models, we propose tri-encoder models which encode each sentence and condition separately. This conceptually resembles late-interaction contextualized retrieval approaches, such as Humeau et al. (2020) or Khattab and Zaharia (2020), but our approach is specific to C-STS. For this, we first encode all sentences of the triplet separately, with encoder f\u03b8(\u00b7) as si = f\u03b8(si), where si \u2208 Rd. We then perform an additional transformation h : R2d \u2192 Rd that operates on the condition and one each of the sentences. We finally compute the conditional similarity using\nthe cosine similarity as cos (h (c; s1) , h (c; s2)). We experiment with 2 functions for h, an MLP and the Hadamard product.\nObjectives In addition to the standard MSE loss for regression, we use a quadruplet contrastive margin loss which we denote Quad. Since each sentence pair in C-STS comes with two conditions (one with higher similarity and one with lower similarity) we represent the conditional encoding of each sentence in the higher-similarity pair as p1 and p2 and represent the conditional encoding of each sentence in the lower similarity pair as n1 and n2. The Quad loss is then defined as follows:\nQuad(p1, p2, n1, n2) =max(\u03bb+ cos(n1, n2)\n\u2212 cos(p1, p2), 0)\nwhere \u03bb is a margin hyperparameter. We train all of our tasks for regression using, alternatively, mean squared error (MSE), Quad, and a linear combination of the quadruplet loss and MSE (Quad + MSE). Since we require a separate conditional encoding fore each sentence, the Quad and (Quad + MSE) objectives apply only the the bi-encoder and tri-encoder configurations.\nHyperparameters We evaluate the baselines on the test split for C-STS. We perform a hyperparameter sweep to select the best performing configuration and test using models trained with 3 random seeds, with further details in Appendix A.3. As a comparison for our training setting, we perform a similar hyperparameter sweep for the STS-B (Cer et al., 2017) dataset, with the validation split results\nand best hyperparameters shown in Table 9, showing that our finetuned baselines achieve very strong performance on traditional STS tasks."
        },
        {
            "heading": "4.2 Large language models baselines",
            "text": "For the generative setting, we evaluate two types of models (1) instruction-finetuned encoderdecoder models, including Flan-T5 (Chung et al., 2022), Flan-UL2 (Tay et al., 2023), and TkINSTRUCT (Wang et al., 2022) and (2) proprietary autoregressive LLMs including ChatGPT3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2023a). For ChatGPT-3.5 and GPT-4, we use the OpenAI API with versions gpt-3.5-turbo-0301 and gpt-4-0314 respectively.\nWhen evaluating zero- or few-shot capabilities, each model input is composed of up to three parts: instruction (task definition), k in-context examples, and query. Models are evaluated with 0, 2, or 4 examples and using three different instruction prompts: no instruction, short instruction, which provides only a high-level description of the task, and long instruction, shown in Figure 6, which resembles the annotation guidelines and is similar to the instructions used for the STS-B classification task in Wang et al. (2022).\nFor few-shot evaluation, we additionally always group a sentence pairs\u2019 two conditional similarity examples together, so models will always see contrasting pairs in the examples, but won\u2019t see a paired example for the query. We provide examples of the formats used for the input and output for more settings in Appendix B. As we did for the finetuned models, we also evaluate these models on the STS-B validation split, shown in Table 12, with instruction finetuned models and ChatGPT achieving strong performance."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Evaluating sentence encoders on C-STS",
            "text": "Zero-shot bi-encoder performance As an initial comparison, we evaluate bi-encoder models without finetuning, on both C-STS and STS-B. As shown in Table 2, we see that strong performance on STS-B does not translate to good performance on C-STS, suggesting that these models fail entirely to incorporate the provided conditioning statement. These results suggest that current approaches to training sentence encoders may be too specialized to existing tasks for evaluation, such as STS-B.\nFine-tuning baselines We finetune our sentence encoder baselines on C-STS and show the test performance in Table 3. Again, the best models are SimCSE and DiffCSE in the bi-encoding setting. This is suggests that the sentence representations learned in their contrastive learning phase facilitate learning for C-STS substantially, but still struggle with all Spearman correlation below 50.\nPerformance on C-STS varies significantly depending on the encoding configurations, with the bi-encoder setting proving to be the most effective, especially for SimCSE and DiffCSE models. Performance of the tri-encoder model, introduced in Section 4.1 was generally poor, with all models performing well below their bi-encoding and cross-encoding counterparts."
        },
        {
            "heading": "5.2 Evaluating pre-trained LLMs",
            "text": "We show performance of generative models evaluated on C-STS in various prompting settings in Table 4, with some additional results for smaller Flan-T5 models in Table 11 in the Appendix. Notably, the state-of-the-art language model, GPT-4, performs substantially better than all competing models and systems (UL2, Flan-T5, ChatGPT-3.5) and is competitive with a finetuned SimCSELARGE model, the best performing sentence-encoder. For example, in most settings, GPT-4 outperforms ChatGPT-3.5 and Flan models by over 10 points. This suggests existing large language benchmarks may correlates with C-STS as GPT-4 has shown to be the most proficient in a wide variety of evaluation settings (OpenAI, 2023b).\nBetween suites of models of different sizes (viz. Flan-T5, Tk-Instruct), we observe a strong correlation between model scale and performance. We also find that providing instructions improves performance substantially for C-STS and that this performance is robust to different instructions lengths and the number of in-context examples."
        },
        {
            "heading": "5.3 Analysis",
            "text": "Scaling laws for C-STS We evaluate the effect of the quantity of C-STS data on sentence-embedding methods for SimCSELARGE (Figure 3). We notice that for all three encoding strategies, performance monotonically increases as we increase the size of the training dataset. For example, for the SimCSE bi-encoder, the Spearman correlation increases from 30 when using a train set of 1,000 examples to 45 for 7,000 examples.\nThere is almost a linear increase in the performance of the models, especially the bi-encoder as we increase the amount of data. This quantitatively enforces the quality of the dataset, but also retroactively makes that point that rather than relying on more data, we require better modeling strategies.\nQualitative analysis We present predictions from different models in Table 5 to illustrate systematic pitfalls. For instance, Flan-T5 makes incorrect predictions even for straightforward instances and falsely predicts that both sentences talk about the same dish, even though the sentences clearly talk about sandwiches and pizza respectively. Additionally, ChatGPT-3.5 incorrectly predicts that the two sentences are completely dissimilar when talking about the types of plants, even though both sentences mention flowering plants. Note that our annotation, unlike ChatGPT-3.5, captures the nuance that the first sentence talks about both shrubbery and flowers, while the second sentence talks only about flowers, and therefore assigns a conservative similarity score of 3. The most proficient model on C-STS, GPT-4, is much better at capturing these nuances and accurately predicts, for instance, that the height of the giraffe\u2019s head (refer to the fourth example), is high in one sentence and\nlow in another. GPT-4 is far from perfect though, and we outline a negative prediction (refer to the third example), where the model does not predict that the two sentences talk about the same game, even though they are very clearly about \u201cFootball\u201d.\nMore broadly, C-STS provides a lens into a model\u2019s ability to understand and reason over specific parts of each sentence and is well-suited to revealing systematic modeling issues."
        },
        {
            "heading": "6 Related Work",
            "text": "Historical perspectives of semantic similarities Measuring semantic similarities is a long-standing problem spanning cognitive science (Miller and Charles, 1991) to psychology (Tversky, 1977) where early attempts are made to quantify the subjective similarity judgements with information theoretical concepts. More recently, interest in semantic similarity has gained popularity in the context of machine learning, with works in computer vision recognizing that the notion of similarity between images varies with conditions (Veit et al., 2016) and can therefore be ambiguous (Ye et al., 2022b).\nTextual similarity tasks Capturing textual similarity is also considered a fundamental problem in natural language processing. Works such as Agirre et al. (2012, 2016) define the textual semantic similarity tasks (STS), which is widely used in common benchmarks such as GLUE (Wang et al., 2018). Extensions to the STS setting have been proposed such as making the task broader with multilinguality (Cer et al., 2017) or incorporating relatedness (Abdalla et al., 2021). However, the loose definition of similarity has not been acknowledged as an issue explicitly. In contrast, our work tackles the\nambiguity problem by collecting conditions and hence reduce subjectivity. To alleviate ambiguity, explanations play an important role in identifying the differences between the two sentences either in their syntactical structure (Lopez-Gazpio et al., 2017) or in natural language (Camburu et al., 2018), but the post-hoc nature of explanations prevents it from being used prior to the similarity judgement, rendering it a supplemental component as opposed to a paradigm change in the task setup. Beyond STS, works that leverage conditioning to enhance sentence representations obtain improved performance for retrieval (Asai et al., 2023) and embedding qualities (He et al., 2015; Su et al., 2023; Jiang et al., 2022), which corroborates the observation that conditioning as a form of disambiguation benefits similarity measures."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we propose conditional semantic textual similarity (C-STS), a novel semantic similarity assessment task that resolves the inherent ambiguity in STS. Given the importance of STS and its importance in sentence representation evaluation we believe that C-STS is a timely and necessary addition to the language model evaluation landscape. Rather than testing unconditional semantic similarity, the diversity of conditions in our dataset allows fine-grained evaluation. The same sentence pairs can be tested on a variety of different aspects represented by conditions, with similarities often varying significantly. C-STS poses a challenging hurdle to both encoder-only and state-of-the-art generative language models which struggle to capture the high-dimensional manifold of similarity.\nWe believe that a combination of improved modeling and fine-tuning strategies are required to push the boundaries on C-STS and we hope that C-STS can enable innovative future work in language understanding and representation learning.\nLimitations\nWe propose the novel task of conditional semantic textual similarity (C-STS). Given that this is a new task, we collect a dataset of over 19,000 instances, but one limitation that this size can be increased to ensure sentence embedding style models have additional data for fine-tuning. Further, we use two different sources to collect our sentence pairs, and future studies, motivated by STS follow-ups, can collect data from other sources."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Distribution of annotated similarity in the dataset\nThe distribution of similarities is equitably spread out over the Likert scale, as depicted in Figure 4.\nA.2 Sentence Pair Generation Details Here we include some further details about sourcing sentence pairs from image-caption datasets.\nAs discussed in Section 2, we use a variety of metrics to quantitatively characterize the sentence pairs, and then to filter with the goal of removing pairs with excessively high or low unconditional similarity. The general criteria we consider are defined as follows:\n\u2022 IOU - This is computed by taking the intersection over union of the bag of words for each sentence, after stopword removal. It represents the lexical similarity and overlap of a sentence pair.\n\u2022 dtext - The cosine distance of the pair\u2019s SimCSE embeddings. We chose SimCSE due to its ubiquity and effectiveness.\n\u2022 ratio - This is the ratio of the shorter sentence\u2019s word count to the longer sentence\u2019s word count in a given pair.\n\u2022 length - This is the character length of the shortest sentence in a pair.\nUsing these criteria, we filter the sentence pairs based upon thresholds (exact values shown in Table 6) where sentences are rejected if they violate\nany of these criteria. These thresholds were selected based primarily manual inspection of samples on their margins. Criteria such as ratio and length are used primarily to facilitate comparison. Sentences with very different lengths are more difficult to compare, as are sentences that are very short or contain few details.\nA.3 Evaluation Details\nImplementation Details All models, with the exception of the ChatGPT systems, are trained and or evaluated in PyTorch using the Huggingface Transformers library (Wolf et al., 2019) and pre-trained weights repository. We use the STS-B dataset as distributed on https://huggingface.co/docs/ datasets as part of the GLUE (Wang et al., 2018) evaluation benchmark.\nFinetuned Baselines For evaluation of the finetuned baselines on C-STS, we perform a hyperparameter sweep to select the best training settings for each model and encoding method before evaluating on the test split of C-STS. We show the hyperparameter values used in the sweep in Table 7, and the final hyperparameter values chosen in Table 8. We evaluate 3 random seeds using the best validation configuration to evaluate on the test data, with final results reported in Table 3.\nWe additionally perform an extensive evaluation of our models on STS-B. We perform a comparable validation sweep as shown in Table 7, reporting the best performing hyperparameters and their performance in Table 9.\nLastly, we perform a data ablation training a RoBERTaBASE model alternatively on only the condition and only the sentence pair. The model trained to predict similarity based on the condition statement alone recovers non-trivial performance, but falls well behind the full-input baseline.\nGenerative Baselines We report more details of results of the generative baselines for the validation sets of C-STS and STS-B.\nFor comparison to validation performance of other models, we include the validation performance for C-STS in Table 11, which largely mirrors performance on the test set. We notice, expectedly, that models frequently output non-numerical responses in settings where there are no instructions to do so, or no in-context examples to follow.\nOn STS-B validation performance, models generally perform much better than on C-STS, with some models performing comparably to finetuned models. Since STS-B is included as a task in Natural Instructions v2 (Wang et al., 2022), it is likely to be recognizable to Flan-T5 models, which counts Natural Instructions v2 in its training data. Likewise, STS-B is comprised of long-existing and popular datasets, which plausibly exist in the the corpora used to train ChatGPT models.\nProcessing Prompting Baseline Generations For parsing prompting model generations, we allow for a maximum of 20 generation tokens. The output is stripped of non-numeric characters and errant punctuation before being cast to a float. For example, the response \u201cThe Answer is 2.0.\u201d is processed as 2.0 and counts as a valid prediction. If the cast fails, we mark the answer invalid and replace the predictions by a number y \u223c U [1, 5]."
        },
        {
            "heading": "B Prompt Examples",
            "text": "All prompts for the prompting baselines may consist of instructions, examples, and a query, though we include evaluations for no instructions and no examples in our results. Figure 5 shows an prompt example for the short instructions and K = 2 and\nFigure 6 shows an example for long instructions and zero-shot setup."
        },
        {
            "heading": "C Crowdsourcing Guidelines",
            "text": "C.1 Condition Annotation We provide the complete condition annotation guidelines used for Mechanical Turk data collection in Figure 7.\nC.2 Condition Verification We provide the complete verification guidelines used for Mechanical Turk data collection in Figure 8.\nb. S2: A green avocado in the basket. c. C-High: The color is green.\nInstead, the same condition can correctly be written as: \u201cThe color of the fruit\u201d. 4. Avoid conditions which explicitly use words like \u201csentences\u201d. For example, instead\nof saying \u201cthe color in the sentence\u201d, just say \u201cThe color\u201d. 5. Avoid vague conditions which do not help narrow down a specific aspect of the\nsentence. For example, avoid conditions which simply say \u201cThe activity\u201d, which does not help narrow down the aspect. Instead use more informative words like \u201cthe sport\u201d or \u201cthe hobby\u201d as much as possible.\n6. Whenever possible, try to write conditions which refer to abstract similarity. Consider the following sentences:\na. Two women are celebrating a goal. b. A couple is eating a tasty meal.\nA condition which is more abstract is preferred: c. Abstract condition: The sentiment of the people. Although a more literal condition is valid, it is less preferred: d. Literal condition: The number of people.\nExamples\nWe provide good and bad examples of conditions for sentence pairs, along with the reasoning.\nGood examples\nAll the following conditions are valid because they follow our guidelines.\nSentence 1 Sentence 2 Condition Similarity Explanation\nThe moon looked incredible!\nThe car was completely covered in snow. The color of the object. High The color is white in both cases. This is a good condition because it references the color of the object without explicitly mentioning it.\nA group of people wearing helmets and riding on bikes. A group of bikers are gathered together and taking pictures. The speed of the cyclists. Low The group of cyclists is moving in the first sentence whereas they are not in the second. Hence their speeds are dissimilar.\nThree people are holding a ladder while another climbs it. Three people are listening to music in a car. The number of people. Low There are four people in the first sentence but only three in the second sentence.\nBad examples\nAll the following conditions are invalid because they ignore one or more of our guidelines.\nSentence 1 Sentence 2 Condition Reason for invalidity of condition\nEgyptians appeased gods with offerings and prayers. People in this era put faith in specific gods to protect their lives. The culture involved.\nIt violates guideline 2. The culture in the second sentence cannot be inferred and is missing information.\nAn adult elephant is playing in the river. A boulder is rolling down the hill.\nThe size of the object is large. It violates guideline 3. The condition should have been \u201cThe size of the object\u201d, without explicitly referring to it being \u201clarge\u201d.\nA guitarist is playing on a bench. A man in a green hat is playing the guitar on the road. The instrument in the sentence. It violates guideline 4. The condition would be good if \u201cin the sentence\u201d was removed so that it is just \u201cThe instrument\u201d.\nA middle-aged man is helping construct a grass hut. Three men work on a roof. The activity. This condition is too vague and does not reference a specific aspect. A better condition would be: \u201cThe type of construction\u201d.\nA man on top of a partially completed roof laying down more shingles.\nA man in a hard hat and safety gear stands in a construction site.\nThe number of people. While this condition is valid, it violates guideline 6, which says that an abstract condition should be considered wherever possible. A better condition would have been, \u201cThe occupation of the man\u201d, which is \u201cconstruction worker\u201d in both cases.\nS1: A man cooks in the kitchen. S2: A woman is riding a bike on the road. C: The gender (Man and woman are dissimilar with respect to gender)\n2. Score = 2: The two sentences are dissimilar, but are on a similar topic with respect to the condition. For example:\nS1: A man plays the guitar. S2: A little girl listens to the violin. C: The instrument (Both are string instruments, similar but different instruments)\n3. Score = 3: The two sentences are roughly equivalent, but some important information differs or is missing with respect to the condition. For example:\nS1: A small crowd gathered around the injured person. S2: A crowd jumps up and down to the tunes played by an artist. C: Number of people (While both are crowds, it is important and unclear how many people there are.)\n4. Score = 4: The two sentences are mostly equivalent, but some unimportant details differ with respect to the condition. For example:\nS1: The little girl plays the jazz guitar. S2: The guitar looked nice and shiny. C: The instrument (Guitar in both cases, but the exact type is different and unimportant)\n5. Score = 5: The two sentences are completely equivalent as they mean the same thing with respect to the condition. For example:\nS1: Three boys play on the playground. S2: There are 3 girls near the fountain. C: The number of people (3 and three are strictly equivalent)"
        }
    ],
    "title": "C-STS: Conditional Semantic Textual Similarity",
    "year": 2023
}