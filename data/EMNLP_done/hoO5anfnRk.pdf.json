{
    "abstractText": "Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce Entity-Driven Image Search (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects realworld web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges stateof-the-art methods with dense entities and the large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Siqi Liu"
        },
        {
            "affiliations": [],
            "name": "Weixi Feng"
        },
        {
            "affiliations": [],
            "name": "Tsu-jui Fu"
        },
        {
            "affiliations": [],
            "name": "Wenhu Chen"
        },
        {
            "affiliations": [],
            "name": "William Yang Wang"
        }
    ],
    "id": "SP:766539d02427a5877c14b748055a529d94722548",
    "references": [
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman."
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738.",
            "year": 2021
        },
        {
            "authors": [
                "Yingshan Chang",
                "Mridu Narang",
                "Hisami Suzuki",
                "Guihong Cao",
                "Jianfeng Gao",
                "Yonatan Bisk."
            ],
            "title": "Webqa: Multihop and multimodal qa",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16495\u201316504.",
            "year": 2022
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Jordi Pont-Tuset",
                "Vittorio Ferrari",
                "Radu Soricut."
            ],
            "title": "Telling the what while pointing to the where: Multimodal queries for image retrieval",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12136\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "European conference on computer vision, pages 104\u2013120. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Mengjun Cheng",
                "Yipeng Sun",
                "Longchao Wang",
                "Xiongwei Zhu",
                "Kun Yao",
                "Jie Chen",
                "Guoli Song",
                "Junyu Han",
                "Jingtuo Liu",
                "Errui Ding"
            ],
            "title": "Vista: Vision and scene text aggregation for cross-modal retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference",
            "year": 2022
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Linjie Li",
                "Zhe Gan",
                "Kevin Lin",
                "William Yang Wang",
                "Lijuan Wang",
                "Zicheng Liu."
            ],
            "title": "Violet: End-to-end video-language transformers with masked visual-token modeling",
            "venue": "arXiv preprint arXiv:2111.12681.",
            "year": 2021
        },
        {
            "authors": [
                "Xingyu Fu",
                "Ben Zhou",
                "Ishaan Chandratreya",
                "Carl Vondrick",
                "Dan Roth."
            ],
            "title": "There\u2019sa time and place for reasoning beyond the image",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Saadia Gabriel",
                "Skyler Hallinan",
                "Maarten Sap",
                "Pemi Nguyen",
                "Franziska Roesner",
                "Eunsol Choi",
                "Yejin Choi."
            ],
            "title": "Misinfo reaction frames: Reasoning about readers\u2019 reactions to news headlines",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Conghui Hu",
                "Gim Hee Lee."
            ],
            "title": "Feature representation learning for unsupervised cross-domain image retrieval",
            "venue": "European Conference on Computer Vision, pages 529\u2013544. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim."
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "International Conference on Machine Learning, pages 5583\u20135594. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Ming Gong",
                "Daxin Jiang."
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11336\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi."
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557.",
            "year": 2019
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "European conference on computer vision, pages 740\u2013755. Springer.",
            "year": 2014
        },
        {
            "authors": [
                "Fuxiao Liu",
                "Yinghan Wang",
                "Tianlu Wang",
                "Vicente Ordonez."
            ],
            "title": "Visual news: Benchmark and challenges in news image captioning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6761\u20136771.",
            "year": 2021
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Sheena Panthaplackel",
                "Adrian Benton",
                "Mark Dredze."
            ],
            "title": "Updated headline generation: Creating updated summaries for evolving news stories",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Zarana Parekh",
                "Jason Baldridge",
                "Daniel Cer",
                "Austin Waters",
                "Yinfei Yang."
            ],
            "title": "Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO",
            "venue": "Proceedings of the 16th Conference of the European Chapter of",
            "year": 2021
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik."
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "Proceedings of the IEEE",
            "year": 2015
        },
        {
            "authors": [
                "Jordi Pont-Tuset",
                "Jasper Uijlings",
                "Soravit Changpinyo",
                "Radu Soricut",
                "Vittorio Ferrari."
            ],
            "title": "Connecting vision and language with localized narratives",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Sai Chetan",
                "Zhenhailong Wang",
                "Yi R Fung",
                "Kathryn Conger",
                "Ahmed Elsayed",
                "Martha Palmer",
                "Preslav Nakov",
                "Eduard Hovy",
                "Kevin Small"
            ],
            "title": "Newsclaims: A new benchmark for claim detection from news with attribute knowledge",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Patsorn Sangkloy",
                "Wittawat Jitkrittum",
                "Diyi Yang",
                "James Hays."
            ],
            "title": "A sketch is worth a thousand words: Image retrieval with text and sketch",
            "venue": "European Conference on Computer Vision, pages 251\u2013 267. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Spangher",
                "Xiang Ren",
                "Jonathan May",
                "Nanyun Peng."
            ],
            "title": "Newsedits: A news article revision dataset and a novel document-level reasoning challenge",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Reuben Tan",
                "Bryan A Plummer",
                "Kate Saenko",
                "JP Lewis",
                "Avneesh Sud",
                "Thomas Leung."
            ],
            "title": "Newsstories: Illustrating articles with visual summaries",
            "venue": "European Conference on Computer Vision, pages 644\u2013661. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Alasdair Tran",
                "Alexander Mathews",
                "Lexing Xie."
            ],
            "title": "Transform and tell: Entity-aware news image captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13035\u201313045.",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Varab",
                "Natalie Schluter."
            ],
            "title": "Massivesumm: a very large-scale, very multilingual, news summarisation dataset",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10150\u201310161.",
            "year": 2021
        },
        {
            "authors": [
                "Yuxuan Wang",
                "Difei Gao",
                "Licheng Yu",
                "Weixian Lei",
                "Matt Feiszli",
                "Mike Zheng Shou."
            ],
            "title": "Geb+: A benchmark for generic event boundary captioning, grounding and retrieval",
            "venue": "European Conference on Computer Vision, pages 709\u2013725. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "feng Gao"
            ],
            "title": "Vinvl: Making visual representa",
            "year": 2021
        },
        {
            "authors": [
                "Query: Diane"
            ],
            "title": "Keaton wins her Oscar for 1977 s Annie Hall Figure 9: Additional EDIS dataset examples set",
            "year": 1977
        },
        {
            "authors": [
                "Query: Matthew"
            ],
            "title": "Centrowitz won the first US gold in men s 1500 since 1908 Text: Matt Centrowitz wins gold in men's 1,500; first for USA",
            "year": 1908
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Image search, also known as text-to-image retrieval, is to retrieve matching images from a candidate set given a text query. Despite the advancements in large-scale vision-and-language models (Wang et al., 2021; Zhang et al., 2021; Chen et al., 2020; Li et al., 2020b), accurately retrieving images from a large web-scale corpus remains a challenging problem. There remain several critical issues: 1) Lack of large-scale datasets: existing image retrieval datasets typically contain 30K-100K images\n\u2217 Equal contribution 1Code and data: https://github.com/emerisly/EDIS\n(Plummer et al., 2015; Lin et al., 2014), which is far less than the number of images that search engines must deal with in real applications. 2) Insufficient entity-specific content: existing datasets focus on generic objects without specific identities. Specific entities (\u201cStatue of Liberty\u201d) in web images and text may be recognized as general objects (\u201cbuilding\u201d). 3) Modality mismatch: existing image retrieval methods usually measure image-text similarity. However, for web image search, the surrounding text of an image also plays a crucial part in this fast and robust retrieval process.\nRecently, there has been a continuous interest in event-centric tasks and methods in the news domain (Reddy et al., 2021; Varab and Schluter,\n2021; Spangher et al., 2022). For instance, NYTimes800K (Tran et al., 2020), and Visual News (Liu et al., 2021) are large-scale entity-aware benchmarks for news image captioning. TARA (Fu et al., 2022) is proposed to address time and location reasoning over news images. NewsStories (Tan et al., 2022) aims at illustrating events from news articles using visual summarization. While many of these tasks require accurate web image search results as a premise, a large-scale image retrieval dataset is lacking to address the challenges of understanding entities and events.\nTherefore, to tackle the aforementioned three key challenges, we introduce a large-scale dataset named Entity-Driven Image Search (EDIS) in the news domain. As is shown in Fig.1, EDIS has a much larger candidate set and more entities in the image and text modalities. In addition to images, text segments surrounding an image are another important information source for retrieval. In news articles, headlines efficiently summarize the events and impress readers in the first place (Panthaplackel et al., 2022; Gabriel et al., 2022). Hence, to simulate web image search with multi-modal information, we pair each image with the news headline as a textual summarization of the event. As a result, EDIS requires models to retrieve over image-headline candidates, which is a novel setup over existing datasets.\nGiven a text query, existing models can only measure query-image or query-text similarity alone. BM25 (Robertson et al., 2009), and DPR (Karpukhin et al., 2020) fail to utilize the visual features, while vision-language models like VisualBert (Li et al., 2019) and Oscar (Li et al., 2020b) cannot be adopted directly for image-headline candidates and are infeasible for large-scale retrieval. Dual-stream encoder designs like CLIP (Radford et al., 2021) are efficient for large-scale retrieval and can compute a weighted sum of query-image and query-text similarities to utilize both modalities. However, as is shown later, such multi-modal fusion is sub-optimal for EDIS. In this work, we evaluate image retrieval models on EDIS and reveal that the information from images and headlines cannot be effectively utilized with score-level fusion. Therefore, we further proposed a featurelevel fusion method to utilize information from both images and headlines effectively. Our contribution is three-fold:\n\u2022 We collect and annotate EDIS for large-scale\nimage search, which characterizes singlemodality queries and multi-modal candidates. EDIS is curated to include images and text segments from open sources that depict a significant amount of entities and events.\n\u2022 We propose a feature-level fusion method for multi-modal inputs before measuring alignment with query features. We show that images and headlines are exclusively crucial sources for accurate retrieval results yet cannot be solved by naive reranking.\n\u2022 We evaluate existing approaches on EDIS and demonstrate that EDIS is more challenging than previous datasets due to its large scale and entity-rich characteristics."
        },
        {
            "heading": "2 Related Work",
            "text": "Cross-Modal Retrieval Datasets Given a query sample, cross-modal retrieval aims to retrieve matching candidates from another modality (Bain et al., 2021; Hu and Lee, 2022; Wang et al., 2022; Sangkloy et al., 2022). Several datasets have been proposed or repurposed for text-to-image retrieval. For instance, MSCOCO (Lin et al., 2014), and Flickr-30K (Plummer et al., 2015) are the two widely used datasets that consist of Flickr images of common objects. CxC (Parekh et al., 2021) extends MSCOCO image-caption pairs with continuous similarity scores for better retrieval evaluation. Changpinyo et al.(2021) repurposes ADE20K (Pont-Tuset et al., 2020) for image retrieval with local narratives and mouse trace. WebQA (Chang et al., 2022) has a similar scale to EDIS and defines source retrieval as a prerequisite step for answering questions. The source candidates are either text snippets or images paired with a short description. In contrast, EDIS is a large-scale entity-rich dataset with multi-modal candidates that aligns better with realistic image search scenarios.\nText-to-Image Retrieval Methods Text-toimage retrieval has become a standard task for vision-language (VL) understanding (Lu et al., 2019; Li et al., 2020a; Kim et al., 2021; Jia et al., 2021; Fu et al., 2021; Dou et al., 2022). Single-stream approaches like VisualBert (Li et al., 2019), and UNITER (Chen et al., 2020) rely on a unified transformer with concatenated image and text tokens as input. Dual-stream approaches like CLIP (Radford et al., 2021) or ALIGN (Jia\net al., 2021) have separate encoders for image and text modality and, thus, are much more efficient for retrieval. However, adopting these models for multi-modal candidates leads to architecture modification or suboptimal modality fusion. In contrast, ViSTA (Cheng et al., 2022) can aggregate scene text as an additional input to the candidate encoding branch. In this work, we propose a method named mBLIP to perform feature-level fusion, achieving more effective information fusion for multi-modal candidates."
        },
        {
            "heading": "3 Task Formation",
            "text": "EDIS contains a set of text queries Q = {q1, q2, . . .}, and a set of candidates cm, essentially image-headline pairs B = {c1 = (i1, h1), c2 = (i2, h2), . . .} where in denotes an image and hn denotes the associated headline. For a query qm, a retrieval model needs to rank top-k most relevant image-headline pairs from B. As is shown in Fig. 1, both images and headlines contain entities that are useful for matching with the query.\nWe evaluate approaches with both distractor and full candidate sets. The distractor setup is similar to conventional text-to-image retrieval using MSCOCO (Lin et al., 2014), or Flickr30K (Plummer et al., 2015), where images are retrieved from a limited set B\u0303 with \u223c25K (image, headline) pairs. The full setting requires the model to retrieve images from the entire candidate set B."
        },
        {
            "heading": "4 Entity-Driven Image Search (EDIS)",
            "text": "We select queries and candidates from humanwritten news articles and scraped web pages with different stages of filtering. Then we employ human annotators to label relevance scores. Fig. 2\nillustrates the overall dataset collection pipeline."
        },
        {
            "heading": "4.1 Query Collection",
            "text": "We extract queries and ground truth images from the VisualNews (Liu et al., 2021), and TARA (Fu et al., 2022) datasets. These datasets contain news articles that have a headline, an image, and an image caption. We adopt captions as text queries and use image-headline pairs as the retrieval candidates.\nWe design a series of four filters to select highquality, entity-rich queries. 1) Query complexity: we first evaluate the complexity of queries and remove simple ones with less than ten tokens. 2) Query entity count: we use spaCy to estimate average entity counts in the remaining queries and remove 20% queries with the lowest entity counts. The resulting query set has an average entity count above 4.0. 3) Query-image similarity: to ensure a strong correlation between queries and the corresponding ground truth image, we calculate the similarity score between query-image using CLIP (Radford et al., 2021) and remove 15% samples with the lowest scores. 4) Query-text similarity: we calculate the query-text similarity using SentenceBERT (Reimers and Gurevych, 2019) and remove the top 10% most similar data to force the retrieval model to rely on visual representations.\nTo avoid repeating queries, we compare each query to all other queries using BM25 (Robertson et al., 2009). We remove queries with high similarity scores as they potentially describe the same news event and lead to the same retrieved images. As shown in Table 1, we end up with 32,493 queries split into 26K/3.2K/3.2K for train/validation/test."
        },
        {
            "heading": "4.2 Candidate Collection",
            "text": "In web image search experience, multiple relevant images exist for a single query. Therefore, we expand the candidate pool so that each query corresponds to multiple image-headline pairs. Additional candidates are collected from Google Image Search and the rest of the VisualNews dataset. For each query from VisualNews, we select seven image-headline pairs from Google search. For each query from TARA, we select five image-headline pairs from Google search. and two image-headline pairs from VisualNews. Then, we ask annotators to label the relevance score for each candidate on a three-point Likert scale. Score 1 means \u201cnot relevant\u201d while 3 means \u201chighly relevant\u201d. Formally, denote E(\u00b7) as the entity set and E(\u00b7) as the event of a query qm or a candidate cn = (in, hn), we define the ground truth relevance scores as:\nrel(m,n) =  3 if E(qm) \u2286 E(cn) and E(qm) = E(cn), 2 if E(cn) \u2229 E(qm) \u0338= \u2205 and E(qm) = E(cn),\n1 if E(qm) \u2229 E(in) = \u2205 or E(qm) \u0338= E(cn).\n(1)\nEach candidate is annotated by at least three workers, and it is selected only when all workers\nreach a consensus. Controversial candidates that workers cannot agree upon after two rounds of annotations are discarded from the candidate pool. Additionally, one negative candidate is added to each annotation task to verify workers\u2019 attention. The final conformity rate among all annotations is over 91.5%.\nHard Negatives Mining We discover that EDIS queries can be challenging to Google Image Search in some cases. Among the 200K images from Google Search, 29K (\u223c15%) are annotated with a score of 1, and 124K (\u223c62%) are annotated with a score of 2. These candidates are hard negatives that require retrieval models to understand and ground visual entities in the images. As for candidates from VisualNews, there are 9.7K (\u223c41%) with a score of 1 and 9.5K (\u223c40%) candidates with a score of 2. We refer to these samples as in-domain hard negatives as their headlines share some entities with the query but refer to different events with discrepant visual representations.\nSoft Negative Mining Lastly, we utilize the rest of the image-headline pairs from VisualNews and TARA to augment the candidate pool. These candidates are naturally negative candidates with a relevance score of 1 because of the unique article contents and extensive diversity in topics. Therefore, our dataset consists of 1,040,919 image-headline candidates in total."
        },
        {
            "heading": "4.3 Dataset Statistics",
            "text": "We demonstrate the major advantage of EDIS over existing datasets in Table 1. EDIS has the largest candidate set with a consistent candidate modality. Our images are not restricted to a specific source as a result of collecting images from a real search engine. Queries from EDIS are entity-rich compared to datasets with general objects (e.g. MSCOCO).\nIn Fig. 3 (left), we show the score distribution of\nhuman annotations. Candidates mined from Visual News are mostly in-domain hard negatives, while the images represent missing entities or different events. These candidates are mostly annotated with scores of 1 or 2. As for Google search candidates, many images depict the same event but with missing entities. Therefore, the annotations concentrate on score 2. In Fig. 3 (right), we show that most of the queries have at least one hard negative, usually more score 2 negatives than score 1 negatives. About half of the queries have more than one positive candidate (score 3). We show more examples of EDIS candidates in Fig. 8-11."
        },
        {
            "heading": "5 Multi-Modal Retrieval Method",
            "text": "Given a text query q, a model should be able to encode both images in and headlines hn to match the query encoding. Therefore, the model should include a multi-modal candidate encoder fC and a query encoder fQ. Within fC , there is a branch for image input fI and a branch for headline fH . We formalize the matching process between a query qm and a candidate cn = (in, hn) as:\nsm,n = fQ(qm) T fC(in, hn) (2)\nwhere sm,n is the similarity score between qm and cn. Based on the design of fC , we categorize methods into score-level fusion and feature-level fusion.\nScore-level Fusion These methods encode image and headline independently and compute a weighted sum of the features, i.e., fC(in, hn) = w1fI(in) + w2fH(hn). Therefore, sm,n is equivalent to a weighted sum of query-image similarity sik,k and query-headline similarity s h k,k:\nsm,n = fQ(qm) T (w1fI(in) + w2fH(hn)) (3)\n= w1s I m,n + w2s H m,n (4)\nSpecifically, CLIP (Radford et al., 2021), BLIP (Li et al., 2022), and a combination of models like CLIP and BM25 (Robertson et al., 2009) belong to this category.\nFeature-level Fusion In Sec. 6, we show that score-level fusion is a compromised choice for encoding multi-modal candidates. Therefore, we propose a modified version of BLIP (mBLIP) to fuse features throughout the encoding process. The overall fusion process can be abstracted as follows:\nfC(in, hn) = fH(hn, fI(in)) (5)\nsm,n = fQ(qm) T fH(hn, fI(in)). (6)\nAs is shown in Fig. 4, we first extract image embeddings fI(\u00b7) using the image encoder and then feed fI(\u00b7) into the cross-attention layers of fH . The output from fH is a feature vector vi,h that fuses the information from both image and text modalities. We separately obtain the query feature vq = fQ(qm) where fQ shares the same architecture and weights with fH , except that the cross-attention layers are not utilized. We adopt the Image-Text Contrastive (ITC) loss (Li et al., 2021) between vi,h and vq to align the fused features with query features."
        },
        {
            "heading": "6 Experiment Setup",
            "text": ""
        },
        {
            "heading": "6.1 Baselines",
            "text": "For score-level fusion mentioned in Sec. 5, we consider CLIP, BLIP, fine-tuned BLIP, and BM25+CLIP reranking to utilize both modalities of the candidates. In addition, we benchmark existing text-to-image retrieval methods, and text document retrieval methods, including VinVL (Zhang et al., 2021), ALBEF (Li et al., 2021), and BM25 (Robertson et al., 2009). Although they are not designed for multi-modal candidates, benchmarking these methods facilitates our understanding of the importance of single modality in the retrieval process. We do not consider single-stream approaches like UNITER (Chen et al., 2020) as they are not efficient for large-scale retrieval and result in extremely long execution time (see Appendix A)."
        },
        {
            "heading": "6.2 Evaluation Metrics",
            "text": "We evaluate retrieval models with the standard metric Recall@k (R@k) that computes the recall rate of the top-k retrieved candidates. k is set to 1, 5, 10. We report mean Average Precision (mAP) to reflect the retrieval precision considering the ranking\nposition of all relevant documents. Formally,\nRecall@k = 1\n|Q| |Q|\u2211 m=1 \u2211k n=1 rel(m,n)\u2211 n rel(m,n)\n(7)\nmAP = 1\n|Q| |Q|\u2211 m=1 \u2211 n P (m,n)rel(m,n)\u2211 n rel(m,n) (8)\nrel(m,n) = { 1 if rel(m,n) = 3 0 otherwise\n(9)\nwhere P (m,n) is the Precision@n of a query qm. For R@k and mAP, candidates with relevant score 3 are positive candidates, while candidates with scores 2 or 1 are (hard) negative samples. These two metrics reflect the model\u2019s ability to retrieve the most relevant candidates, which aligns with the definition in Fig. 2.\nTo give merits to candidates with a score of 2, we also report Normalized Discounted Cumulative Gain (NDCG). NDCG assigns importance weights proportional to the relevance score so that ranking score 2 candidates before score 1 candidate will lead to a higher metric value.\nDCG(m) = \u2211 n rel(m,n)\u2212 1 log2(1 + n)\n(10)\nNDCG = 1\n|Q| |Q|\u2211 m=1 DCG(m) IDCG(m) , (11)\nwhere IDCG(m) is the DCG value of qm with the ideal candidate ranking."
        },
        {
            "heading": "6.3 Implementation Details",
            "text": "For BLIP fine-tuning, we adopt the same loss and hyperparameters as reported in the original implementation. We increase the learning rate to 1e-5 for optimal validation results. We directly rank the candidates by computing the cosine similarity of query features and candidate features and do not use any linear regression heads for reranking. Therefore, we abandon the image-text matching (ITM) loss in mBLIP fine-tuning and increase the learning rate to 5e-5 for optimal performance. More details can be found in Appendix A."
        },
        {
            "heading": "7 Experimental Results",
            "text": ""
        },
        {
            "heading": "7.1 BLIP-based fusion methods",
            "text": "We first investigate the performance difference between score-level and feature-level fusion as mentioned in Sec. 5. We implement these two ap-\nproaches on BLIP (Li et al., 2022). Table 2 compares the result under two different setups where \u201cBLIP\u201d denotes the score-level fusion using the original BLIP architecture, and \u201cmBLIP\u201d denotes our proposed feature-level fusion. For score-level fusion, we obtain the weights from a grid search on the validation set under the distractor setup.\nDistractor Set Pre-trained BLIP achieves 18.4 R@1 and 46.6 R@5, which means that almost onethird of the queries have a positive candidate in top-1 results, and around half of the positive candidates are retrieved in top-5 results. After finetuning, BLIP doubles R@1 to 32.6 and achieves significant gain in other metrics. The improvement shows that entities in EDIS are out-of-domain concepts for zero-shot BLIP, and EDIS training split is useful for models to adapt to the news domain. mBLIP outperforms BLIP in all metrics except R@1. The overall improvement entails that featurelevel fusion is superior to score-level fusion by utilizing headlines more effectively. The degradation in R@1 can be attributed to the fact that the image-query alignment is accurate enough for a small number of queries. Therefore, utilizing headlines slightly harms the results as they only provide high-level summarization.\nFull Set Retrieving from the full candidate set significantly degrades the performance by over 50% in all metrics. Though the distractor setup was widely adopted in previous work, we show that a larger candidate set imposes remarkable challenges to the SOTA models. We can observe similar trends by comparing the three variants of BLIP. mBLIP achieves over 17% relative improvement across all metrics except R@1, even more significant than 4- 12% relative improvement under the distractor set. The degradation in R@1 is also much less severe. Therefore, feature-level fusion is a more effective\nway to encode multi-modal candidates, considering that users usually receive more than one searched image in reality."
        },
        {
            "heading": "7.2 Additional Baselines",
            "text": "In Table 3, the defective recall rates of BM25 and CLIP text encoder imply that headlines solely are insufficient for accurate retrieval. However, textbased retrieval achieves promising NDCG values, indicating that headlines are useful for ranking score 2 candidates to higher positions.\nScore-level Fusion \u201cBM25+CLIP\u201d first ranks candidates using BM25 and then reranks the top 50 or 200 candidates with CLIP to utilize the images. Despite the improvement compared to text-based methods, it underperforms zero-shot CLIP or BLIP. This implies that ranking with query-headline similarity imposes a bottleneck on the reranking process. CLIP achieves the best performance in terms of R@1/5/10 and mAP compared to other methods. We hypothesize that the \u201cCLIP filtering\u201d step in Sec. 4.1 eliminates hard negative query-image pairs for CLIP and thus introduces performance bias towards CLIP. Fine-tuned CLIP does not show apparent improvement and thus is not shown in Table 3. Therefore, EDIS is still challenging for SOTA retrieval models.\nFeature-level Fusion mBLIP consistently outperforms other approaches in NDCG regardless of the candidate set scale, achieving 75.3/47.9 NDCG under distractor/full set. mBLIP fuses headline features with visual features more effectively and thus proves that headlines are critical for ranking\nscore 2 candidates higher. We conjecture that many score 2 images have insufficient entities, resulting in lower query-image similarity scores. Hence, models must rely on headlines to simultaneously recognize entities from multiple modalities."
        },
        {
            "heading": "7.3 Ablation Study",
            "text": "Component Analysis Table 4 shows the performance of two fusion approaches without either image or headline branch. BLIP achieves much lower performance when relying solely on query-headline alignment (6.6 R@1, 29.7 mAP) compared to utilizing images only (33.9 R@1, 54.0 mAP). BLIP only achieves comparable and slightly degraded performance when using images and headlines for score fusion. Therefore, score-level fusion cannot easily tackle multi-modal candidates in EDIS.\nIn contrast, mBLIP shows improved performance with the headline encoder while decreased performance with the image encoder only. This is intuitive as the BLIP fine-tuning process only utilizes images without headlines, yet mBLIP utilizes both images and headlines. More interestingly,\nwhen using both image and headline encoders, mBLIP demonstrates over 20% relative increase in all metrics. The results imply that feature-level fusion is a more effective method to combine candidate features from multiple modalities."
        },
        {
            "heading": "7.4 Case Study",
            "text": "Success Case We show one success case and one failure case of mBLIP in Fig. 5. In the success case (top), mBLIP manages to retrieve all four relevant images while BLIP retrieves five false positives. Since all ten images contain a \u201ccruise\u201d, we conjecture that entities in headlines (e.g., \u201cCambodia\u201d, \u201cSihanoukville\u201d) play a critical role for mBLIP to outperform BLIP in this case. The case shows feature-level fusion is much more effective in utilizing headline features than score-level fusion.\nFailure Case As for the failure case in Fig. 5 (bottom), BLIP and mBLIP fail to retrieve the positive candidates in the top-5 results. Both methods fail to recognize \u201cJohn Podesta\u201d and align the text with the visual representation. For example, the top-2 candidates retrieved by mBLIP depict a different person from a different event. \u201cHillary Clinton\u201d becomes a distracting entity in the query, and the model must understand the event instead of just matching entities to achieve accurate retrieval\nresults. The third candidate of mBLIP shows the image with the correct person but from a different event. It further proves that the EDIS is a challenging dataset that requires specific knowledge of entities, cross-modal entity matching, and event understanding."
        },
        {
            "heading": "8 Conclusion",
            "text": "Training and evaluating large-scale image retrieval datasets is an inevitable step toward real image search applications. To mitigate the gap between existing datasets and real-world image search challenges, we propose a large-scale dataset EDIS with a novel retrieval setting and one million candidates. EDIS queries and candidates are collected from the news domain describing abundant entities and events. EDIS candidates are image-headline pairs since realistic image search utilizes the surrounding text of an image to facilitate accurate searching results. As a primary step towards handling multimodal candidates in EDIS, we review two primary fusion approaches and propose a feature-level fusion method to utilize the information from both images and headlines effectively. Our experimental results show ample space for improvement on EDIS. Future work should consider more principled solutions involving knowledge graphs, entity\nlinking, and training algorithm design."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "The work was funded by the National Science Foundation award #2048122. The writers\u2019 opinions and conclusions in this publication are their own and should not be construed as representing the sponsors\u2019 official policy, expressed or inferred."
        },
        {
            "heading": "10 Limitations",
            "text": "In this study, we only cover image retrieval datasets with English instructions. Queries and headlines in other languages may characterize different types of ambiguity or underspecification. Thus, expanding the datasets to multi-lingual image retrieval based on our dataset is important. Secondly, we only consider the news domain to collect entityrich queries and images. We plan to expand our dataset to open-domain where other entities like iconic spots will be included. In addition, we only consider the headlines as the text information to utilize in the retrieval process. However, in real image search scenarios, search engines usually utilize multiple paragraphs of the surrounding text to determine the relevance of the image. In the future, we will expand the text of the multimodal candidates with news articles or segments of the articles. Our dataset and models trained on it could be biased if the model is not accurate enough. The model may return completely incorrect candidates and cause users to confuse persons or objects with incorrect identities. We will provide all ground truth annotations with visualization code to help users learn about the ground truth candidates. Last but not least, we do not consider the phenomenon of underspecification in the image search experience. Users search with phrases or incomplete sentences to save typing efforts. Therefore, more realistic queries can be underspecified and grammatically incorrect. However, this is a problem universal to all existing image retrieval datasets, as collecting real human search results could be challenging. We plan to make our dataset more realistic in the future by utilizing powerful tools such as large language models to generate underspecified, near-realistic queries."
        },
        {
            "heading": "11 Ethics Consideration",
            "text": "We will release our dataset EDIS for academic purposes only and should not be used outside of research. We strictly follow any licenses stated in\nthe datasets that we have newly annotated. As introduced in Sec. 4.2, we annotated the data with crowd-workers through Amazon Mechanical Turk. The data annotation part of the project is classified as exempt by our Human Subject Committee via IRB protocols. We required the workers to be in English-speaking regions (Australia, Canada, New Zealand, the United Kingdom, and the United States). We keep the identity of workers anonymized throughout the collection and postprocessing stages. We also require the workers to have a HIT approval rating of \u2265 96% or higher. We pay each completed HIT $0.2, and each HIT takes around 30-40 seconds to complete on average. Therefore, this resulted in an hourly wage of $18- $24, as determined by the estimation of completing time for each annotation task. Example screenshots of our annotation interface can be found in Fig. 6-7 under Appendix A."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Additional Implementation Details Hyperparameters We fine-tuned BLIP and mBLIP on 4 40GB NVIDIA A100. It takes 5 hours for BLIP fine-tuning and 3 hours for mBLIP. For both BLIP and mBLIP, we train the model for 6 epochs with batch size 16 per GPU. The model checkpoint with the best recall rate over the validation set is selected for final evaluation. We apply grid search for score-level fusion using BLIP or CLIP to find the optimal w1. We first search over ten intermediate numbers between 0 and 1 and then narrow the range to search for 100 intermediate numbers. Finally, we found training and validation results stable without much randomness for all implemented methods. Therefore, we evaluate every model once and report the metric values of one-time evaluation.\nBLIP Training For BLIP fine-tuning, we follow the original implementation and adopt the original image-text contrastive (ITC) loss and image-text matching (ITM) loss. We only utilize images with scores of 3 and text queries for training. As for mBLIP, the headline encoder and the query encoder share the same weight. We utilize images with scores of 3, associated headlines, and text queries for training. The output from the image encoder is fed into the transformer layers of the headline encoder through cross-attention layers. Then the output of the headline encoders can be treated as the fused feature of image-headline pairs. We compute ITC loss based on the headline encoder outputs and the query encoder outputs.\nSingle Stream Models We do not evaluate any single stream models or modules due to time complexity. Consider m queries and n candidates. The complexity for a dual encoder model to obtain all features is O(m+n). The computation cost of computing cosine similarity is trivial compared to the forward process of a model and can be neglected. However, for a single stream model, it takes O(mn)\nto obtain similarity scores for all query-candidate pairs. Since it takes around 3.5 minutes for BLIP to evaluate over 3.2k queries with 25K candidates, it is taking more than 5 days for a single encoder model to complete retrieval under the distractor setting. It takes more than a year to complete retrieval under the full setting.\nQuery: A rally in front of the New Jersey State House on Thursday, when the authorization for the state\u2019s transportation trust fund expired"
        }
    ],
    "title": "EDIS: Entity-Driven Image Search over Multimodal Web Content",
    "year": 2023
}