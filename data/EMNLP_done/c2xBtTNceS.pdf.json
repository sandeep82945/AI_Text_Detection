{
    "abstractText": "We introduce inverse reinforcement learning (IRL) as an effective paradigm for training abstractive summarization models, imitating human summarization behaviors. Our IRL model estimates the reward function using a suite of important sub-rewards for summarization and concurrently optimizes the policy network. Experimental results across datasets in different domains (CNN/DailyMail and WikiHow) and various model sizes (BART-base and BARTlarge) demonstrate the superiority of our proposed IRL model for summarization over MLE and RL baselines. The resulting summaries exhibit greater similarity to human-crafted gold references, outperforming MLE and RL baselines on metrics such as ROUGE, coverage, novelty, compression ratio, factuality, and human evaluations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yu Fu"
        },
        {
            "affiliations": [],
            "name": "Deyi Xiong"
        },
        {
            "affiliations": [],
            "name": "Yue Dong"
        }
    ],
    "id": "SP:9cab3b97d3d394bf475fc11b4dd92d506acf7bb9",
    "references": [
        {
            "authors": [
                "Saurabh Arora",
                "Prashant Doshi."
            ],
            "title": "A survey of inverse reinforcement learning: Challenges, methods and progress",
            "venue": "Artificial Intelligence, 297:103500.",
            "year": 2021
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Felix Hill",
                "Jan Leike",
                "Edward Hughes",
                "Seyed Arian Hosseini",
                "Pushmeet Kohli",
                "Edward Grefenstette."
            ],
            "title": "Learning to understand goal specifications by modelling reward",
            "venue": "7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Junyi Bian",
                "Xiaodi Huang",
                "Hong Zhou",
                "Shanfeng Zhu."
            ],
            "title": "Gosum: Extractive summarization of long documents by reinforcement learning and graph organized discourse state",
            "venue": "arXiv preprint arXiv:2211.10247.",
            "year": 2022
        },
        {
            "authors": [
                "Florian B\u00f6hm",
                "Yang Gao",
                "Christian M. Meyer",
                "Ori Shapira",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Better rewards yield better summaries: Learning to summarise without references",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Shashank Srivastava"
            ],
            "title": "How helpful is inverse",
            "year": 2021
        },
        {
            "authors": [
                "Yongchang Hao",
                "Yuxin Liu",
                "Lili Mou."
            ],
            "title": "Teacher forcing recovers reward functions for text generation",
            "venue": "arXiv preprint arXiv:2210.08708.",
            "year": 2022
        },
        {
            "authors": [
                "Mahnaz Koupaee",
                "William Yang Wang."
            ],
            "title": "Wikihow: A large scale text summarization dataset",
            "venue": "arXiv preprint arXiv:1810.09305.",
            "year": 2018
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "arXiv preprint arXiv:1910.12840.",
            "year": 2019
        },
        {
            "authors": [
                "Faisal Ladhak",
                "Esin Durmus",
                "He He",
                "Claire Cardie",
                "Kathleen McKeown."
            ],
            "title": "Faithful or extractive? on mitigating the faithfulness-abstractiveness tradeoff in abstractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Siyao Li",
                "Deren Lei",
                "Pengda Qin",
                "William Yang Wang."
            ],
            "title": "Deep reinforcement learning with distributional semantic rewards for abstractive summarization",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "arXiv preprint arXiv:2005.00661.",
            "year": 2020
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "Thirty-first AAAI conference on artificial intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Ranking sentences for extractive summarization with reinforcement learning",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Shashi Narayan",
                "Yao Zhao",
                "Joshua Maynez",
                "Gon\u00e7alo Sim\u00f5es",
                "Vitaly Nikolaev",
                "Ryan McDonald."
            ],
            "title": "Planning with learned entity prompts for abstractive summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1475\u20131492.",
            "year": 2021
        },
        {
            "authors": [
                "Ramakanth Pasunuru",
                "Mohit Bansal."
            ],
            "title": "Multireward reinforced summarization with saliency and entailment",
            "venue": "arXiv preprint arXiv:1804.06451.",
            "year": 2018
        },
        {
            "authors": [
                "Romain Paulus",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "A deep reinforced model for abstractive summarization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Steven J Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel."
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7008\u20137024.",
            "year": 2017
        },
        {
            "authors": [
                "Alexander M. Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389, Lisbon, Portugal.",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Zhan Shi",
                "Xinchi Chen",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Toward diverse text generation with inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1804.11258.",
            "year": 2018
        },
        {
            "authors": [
                "David Wan",
                "Mohit Bansal."
            ],
            "title": "Factpegasus: Factuality-aware pre-training and fine-tuning for abstractive summarization",
            "venue": "arXiv preprint arXiv:2205.07830.",
            "year": 2022
        },
        {
            "authors": [
                "Chaojun Wang",
                "Rico Sennrich."
            ],
            "title": "On exposure bias, hallucination and domain shift in neural machine translation",
            "venue": "arXiv preprint arXiv:2005.03642.",
            "year": 2020
        },
        {
            "authors": [
                "Xin Wang",
                "Wenhu Chen",
                "Yuan-Fang Wang",
                "William Yang Wang."
            ],
            "title": "No metrics are perfect: Adversarial reward learning for visual storytelling",
            "venue": "arXiv preprint arXiv:1804.09160.",
            "year": 2018
        },
        {
            "authors": [
                "Ronald J Williams",
                "David Zipser."
            ],
            "title": "A learning algorithm for continually running fully recurrent neural networks",
            "venue": "Neural computation, 1(2):270\u2013280.",
            "year": 1989
        },
        {
            "authors": [
                "Shweta Yadav",
                "Deepak Gupta",
                "Asma Ben Abacha",
                "Dina Demner-Fushman."
            ],
            "title": "Reinforcement learning for abstractive question summarization with question-aware semantic rewards",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang."
            ],
            "title": "Hegel: Hypergraph transformer for long document summarization",
            "venue": "arXiv preprint arXiv:2210.04126.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Guanjie Zheng",
                "Hanyang Liu",
                "Kai Xu",
                "Zhenhui Li."
            ],
            "title": "Objective-aware traffic simulation via inverse reinforcement learning",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 3771\u20133777. Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Ming Zhong",
                "Pengfei Liu",
                "Yiran Chen",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Extractive summarization as text matching",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197\u20136208, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Brian D. Ziebart",
                "Andrew L. Maas",
                "J. Andrew Bagnell",
                "Anind K. Dey."
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July",
            "year": 2008
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Most fine-tuned abstractive summarization systems (Rush et al., 2015; Dou et al., 2021) are trained using maximum likelihood estimation (MLE) and the negative log-likelihood (NLL) loss. Previous research has demonstrated that MLE training possesses certain disadvantages: (1) object mismatch (Ding and Soricut, 2017), where the NLL loss concentrates on word-level matches, neglecting token rearrangement and paraphrases; (2) exposure bias (Ranzato et al., 2016), the discrepancy between training and inference regarding reference tokens.\nTo address these issues, reinforcement learning (RL), which optimizes policy networks by directly maximizing the discrete reward, has emerged as an alternative training paradigm for summarization (Paulus et al., 2018; Yadav et al., 2021). Typically, RL-trained summarization models require a predefined reward function and a common practice (Paulus et al., 2018) is to use ROUGE (Lin, 2004). ROUGE-base reward does not, however, consider\n\u2217Corresponding author.\nother quality aspects like fluency, coherence, or paraphrasing. Li et al. (2019) and Pasunuru and Bansal (2018) later integrated other types of rewards such as BERTScore (Zhang et al., 2019) or multiple rewards into the RL training process. However, as reward components increase, their weights must be set manually, relying heavily on the author\u2019s experience and making generalization into new domains difficult.\nIn contrast to RL, we argue that inverse reinforcement learning (IRL) may be more suitable for text summarization. IRL focuses on estimating an agent\u2019s reward function based on their observed behavior, rather than predefining it (Arora and Doshi, 2021). Consequently, IRL can be advantageous in situations where the reward function is not explicitly known (Ghosh et al., 2021) or challenging to define through interactions (Bahdanau et al., 2019). Our experimental results suggest that by using IRL to automatically learn weights over combined summarization subrewards and imitate human generations/expert demonstration, we can jointly optimize the reward function and policy network, yielding superior summaries as measured by both automatic and human evaluations.\nMore specifically, inspired by Shi et al. (2018); Ghosh et al. (2021), we integrate IRL into text summarization, which estimates the reward function for summarization and optimizes the model accordingly. By employing IRL, we gain the ability to dynamically learn the weights of various sub-reward components crucial to the summarization task based on the training data. Once the subrewards are defined, the training process with IRL consists of two alternating phases: the reward update phase, which focuses on learning the reward function, and the policy update phase, which aims to optimize the model to maximize the reward. Figure 1 presents an overview of our approach.\nCompared to the models trained with MLE or RL, our empirical results on multiple summariza-\ntion datasets and different model sizes suggest that the IRL-trained agent produces summaries that are significantly more similar to the human-written reference summaries across multiple evaluation metrics, including ROUGE, coverage, novelty, and compression ratio. In addition, although we did not explicitly incorporate faithfulness as a sub-reward during training, our empirical results indicate that our models exhibit a higher level of abstraction, with a notably lower decline rate in hallucination. This particular characteristic closely aligns with the behavior observed in human-generated summaries.\nOur contributions can be summarized as follows:\n\u2022 We introduce inverse reinforcement learning into text summarization and define a suite of rewards that are important for summarization optimization, which, to the best of our knowledge, is the first attempt at this task.\n\u2022 We demonstrate that IRL method is effective at learning the weights for combining different sub-rewards for summarization, allowing us to train policies based on the training datasets instead of manually determining these weights.\n\u2022 By simultaneously estimating the reward function and optimizing the summarization agent with expert demonstrations, we show that the model trained with IRL produces summaries that closely follow human behaviors, in terms of better ROUGE, coverage, novelty, compression ratio and factuality when compared to the baselines trained with MLE and RL."
        },
        {
            "heading": "2 Related Work",
            "text": "Summarization Extensive research has been conducted on fine-tuned deep learning approaches\nfor both abstractive summarization (Rush et al., 2015; See et al., 2017; Dou et al., 2021) and extractive summarization (Nallapati et al., 2017; Zhong et al., 2020). In spite of the differences in generation paradigms, these models trained with the NLL loss have similar limitations, including exposure bias and objective mismatch (Paulus et al., 2018). To address these challenges, reinforcement learning has emerged as a popular alternative for training abstractive (Paulus et al., 2018; Yadav et al., 2021; Dou et al., 2021) and extractive summarization systems (Zhong et al., 2020; Bian et al., 2022; Zhang et al., 2022). RL-based approaches are designed to optimize a discrete reward, often chosen heuristically, such as ROUGE. For instance, Yadav et al. (2021) propose a reward function specifically tailored for consumer health question (CHQ) datasets, while Fabbri et al. (2022) construct a reward function based on textual entailment and coverage of summaries in semantic spaces. However, manually designing reward functions can limit their generalizability beyond specific domains. PA notable work similar to ours is the model introduced by B\u00f6hm et al. (2019), which focuses on learning reward functions rather than manually selecting them for policy optimization. In contrast to our approach, they train neural networks to learn directly from human ratings instead of expert demonstrations, which may result in lower interpretability.\nHallucination The increasing flexibility of text generation models has given rise to a concerning phenomenon known as hallucination (Narayan et al., 2021; Dong et al., 2022; Cao et al., 2022), where models generate text unsupported by source documents (Maynez et al., 2020). Moreover, it has been observed that conventional evaluation met-\nrics, such as ROUGE, do not effectively capture factuality (Maynez et al., 2020). To tackle issues related to faithfulness and factuality in generated summaries, numerous methods have been proposed to enhance the preservation of entity-level information. For example, Narayan et al. (2021) employ entity chains as an intermediate planning stage, while Dong et al. (2022) incorporate entity-level external knowledge into summarization. Instead of designing additional components to explicitly address hallucination, we adopt an approach similar to Wang and Sennrich (2020) and leverage IRL that naturally mitigates hallucinations by addressing the exposure bias. This perspective offers a novel direction to enhancing the factuality of summarization systems, complementing existing strategies that focus on entity-level information preservation.\nInverse Reinforcement Learning The backbone of our approach is inverse reinforcement learning, which has been widely used in a diverse range of areas, including computer vision (Zheng et al., 2021) and NLP (Shi et al., 2018; Wang et al., 2018; Ghosh et al., 2021; Ghosh and Srivastava, 2021; Hao et al., 2022). In NLP, Wang et al. (2018) explore expert demonstrations to train neural networks with learned reward functions for visual story generation. Hao et al. (2022) employ the trained teacherforcing model as the reward function, generating step-wise rewards for text generation. However, these approaches implicitly provide the reward function for policy optimization. Of the closely related works, both Ghosh et al. (2021) and Ghosh and Srivastava (2021) incorporate IRL into their proposed methods. These studies focus on tableto-text generation and program generation from natural language instructions, respectively, while our research centers on the summarization task. Given the task differences, we propose distinct subreward components tailored specifically for text summarization. Furthermore, we provide a comprehensive analysis highlighting the advantages of IRL in summarization, particularly concerning ngrams, entities, and hallucinations."
        },
        {
            "heading": "3 Method",
            "text": "This section presents an overview and formulation of summarization, along with a discussion of reinforcement learning (RL) and our approach to training summarization models using inverse reinforcement learning (IRL).\nProblem Formulation The task of abstractive summarization can be viewed as a conditional language generation task. Given a source document x = {x1, x2, . . . , xn} with n tokens, the summarization task involves learning a conditional probabilistic model p\u03b8(y|x) that produces a summary y = (y1, ..., y|y|), where yi is chosen from a predefined vocabulary V and \u03b8 denotes the parameters of the summarization model. p\u03b8(y|x) can be further decomposed into the product of conditional probabilities for each token based on the previously generated context:\np\u03b8(y|x) = |y|\u220f t=1 p\u03b8(yt | y<t,x). (1)\nGenerally, p\u03b8 in Eqn. (1) is trained using the maximum likelihood estimation (MLE) objective with teacher forcing (Williams and Zipser, 1989), which aims to maximize the likelihood of the human-written reference summary y\u2217 = (y\u22171, ..., y \u2217 |y|):\nLMLE = \u2212 |y\u2217|\u2211 t=1 log p(yt|y\u22171, . . . , y\u2217t\u22121) (2)\nwhere |y\u2217| denotes the length of the target sequence.\nReinforcement Learning Due to exposure bias (Ranzato et al., 2016) in MLE with teacher forcing, RL has emerged as an alternative for training neural summarization models (Paulus et al., 2018). It offers the advantage of directly optimizing discrete metrics such as ROUGE (Lin, 2004), which considers a certain degree of flexibility for token rearrangement and paraphrasing.\nIn RL training, models typically require predefining the reward R. The reward function R(y) is established by comparing the output sequence y with the ground truth sequence y\u2217 using evaluation metrics such as the average of ROUGE -1, -2, -L, and F1 scores with respect to the gold references (Narayan et al., 2018; Dong et al., 2018). The objective of RL is to learn a policy that maximizes the predefined discrete metric:\nLRL = R(ys) m\u2032\u2211 t=1 log p(yst |ys1, . . . , yst\u22121) (3)\nwhere ys is obtained by sampling from the probability distribution in the current policy p at each\ndecoding time step, with m\u2032 representing the length of ys.\nTo stabilize training and reduce variance in text generation, the commonly used self-critical policy gradient training algorithm (Rennie et al., 2017) is employed. In this algorithm, two separate output sequences, ys and yb, are generated during each training iteration. These sequences represent the sampled output and the baseline output, respectively. The baseline output is obtained by maximizing the output probability distribution at each time step, which essentially involves performing a greedy search. With this baseline formulation, the reward R can be calculated as R = R(ys)\u2212R(yb)."
        },
        {
            "heading": "3.1 Training with Inverse Reinforcement Learning",
            "text": "We apply the Maximum Entropy IRL algorithm (Ghosh et al., 2021) to train our summarization agent. The objective is to establish an effective reward function derived from expert demonstrations, which, in our context, take the form of humanauthored reference summaries. We identify crucial reward components for text summarization, including salience, novelty/paraphrasing, compression ratio, and content coverage. It\u2019s worth noting that we do not claim optimality for the defined sub-rewards, and exploration to better match human preferences is left for future work. Instead, we demonstrate improvements in the summarization agent across various critical measures by defining a set of subrewards and training the IRL agent to optimize a linear combination of these sub-rewards.\nTraining an agent with IRL involves two phases that are performed alternatively: (1) the reward update phase that focuses on learning the reward function and (2) the policy update phase that focuses on finding the optimal agent. During the reward update phase, we utilize the fixed, learned policy to generate a summary. We then update the weights of different sub-reward components by considering the reference summary in the training pair. In the policy update phase, we fix the reward function and employ it to update the policy gradients, refining the agent\u2019s performance.\nThe base of our IRL method consists of subreward components C = {C1, C2, . . . , Ck}, as elaborated in Section 4.2. The IRL reward function is a weighted sum of these components:\nR\u03d5(y) = \u03d5 TC (4)\nwhere \u03d5 = {\u03d51, \u03d52, . . . , \u03d5k} is the weight vector\nfor the reward components. For IRL, we assume that the summary is sampled from a distribution p\u03d5(y), which is defined as:\np\u03d5(y) = 1\nZ exp(R\u03d5(y)). (5)\nR is defined in Eqn. (4), and Z = \u222b y exp(R\u03d5(y)) is the partition function. The training objective, denoted by J (\u03d5), is to update the weights of subrewards in order to maximize the log-likelihood of the probability defined in Eqn. (5), computed as:\nJ (\u03d5) = 1 N N\u2211 n=1 log p\u03d5(y n). (6)\nFor a reward component Cj , the gradient can be calculated as follows (Ziebart et al., 2008):\n\u2207\u03d5jJ (\u03d5) =Ey\u223cpdata\u2207\u03d5jR\u03d5(y) \u2212 Ey\u223cp\u03d5(y)\u2207\u03d5jR\u03d5(y).\n(7)\nTo estimate Eqn. (7), which involves expectations over all possible summaries, we employ importance sampling. Specifically, we estimate it by sampling N summaries from the expert demonstrations distribution pdata, and M summaries from the policy distribution p\u03b8(y):\n\u2207\u03d5jJ (\u03d5)) = 1\nN N\u2211 n=1 \u2207\u03d5jR\u03d5(y n) \u2212 1\u2211 m \u03b2m M\u2211 m=1 \u03b2m\u2207\u03d5jR\u03d5(y m)\n(8) where\n\u03b2m \u221d expR\u03d5(y\nm)\np\u03b8(ym) .\nyn and ym are drawn from pdata and the p\u03b8(y) respectively. The full training procedure is illustrated in Algorithm 1. More mathematical details can be found in Shi et al. (2018) and Ghosh et al. (2021).\nNote that we employed mixed training (see Appendix A for hyperparameter details) for both RL and IRL training to expedite convergence while maintaining a consistent setting for comparison. While setting the reward function is always a challenge in RL, IRL can simply learn a paradigm from expert demonstration, which in turn reduces the inductive bias brought by humans. Our results and analyses in section 4 demonstrate the improvements from multiple perspectives, including salience, coverage, and faithfulness.\nAlgorithm 1 IRL Training for Summarization Input: Pretrained policy (summarization) model p\u03b8(y). Initital reward model R\u03d5(y). Labelled samples {xi,yi}ni=1. Policy learning rate \u03b1, reward learning rate \u03b2. Training epoch H , reward update frequency F . Output: Optimal policy model and reward model.\n1: for h\u2190 1 to H do 2: if h%F = 0 then 3: for \u03d5j \u2208 {\u03d51, \u03d52, . . . , \u03d5k} do 4: Get \u2207\u03d5jJ (\u03d5)) according to Eqn.\n(8) and update the reward model: 5: \u03d5j = \u03d5j + \u03b2\u2207\u03d5jJ (\u03d5)) 6: end for 7: fix reward model R\u03d5(y). 8: end if 9: for mini-batch B from {xi,yi}ni=1 do\n10: Use reward model R\u03d5(y) and get LRL according to Eqn. (3) to update thepolicy model: 11: \u03b8 = \u03b8 \u2212 \u03b1\u2207\u03b8LRL 12: end for 13: end for"
        },
        {
            "heading": "4 Experiments and Results",
            "text": "We conducted extensive experiments to examine the effectiveness of the proposed IRL-based summarization approach against traditional summarization methods. This section will provide details about the datasets, baselines, reward components, experiment settings and results."
        },
        {
            "heading": "4.1 Datasets and Baselines",
            "text": "BART-base and BART-large (Lewis et al., 2020) were used as the backbone model for the experiments with the Hugging Face MLE implementation.1 RL (Equal) means using equal weight for every sub-reward components defined in section 4.2 as the final training reward. We carried out experiments on both CNN/DailyMail (See et al., 2017) and WikiHow (Koupaee and Wang, 2018) datasets. Further training and evaluation details are presented in Appendix A.\n1https://github.com/huggingface/transformers/ tree/v4.9.2/examples/pytorch/summarization"
        },
        {
            "heading": "4.2 Sub-Reward Components",
            "text": "This part provides a detailed definition of subreward components used in the IRL in Eqn. (4). The sub-reward components encourage the agent to generate summaries that closely align with humanwritten reference summaries in terms of salience, novelty, coverage, and compression ratio.\n\u2022 ROUGE (Lin, 2004): Encourages generations to match the references in terms of salience, with a focus on ROUGE-L as the sub-reward.\n\u2022 Novelty (Chen et al., 2020): Ensures a similar level of novelty in the generation to reference, measured by novel n-grams in the summary.\n\u2022 Coverage (Grusky et al., 2018): Ensures that the generated summaries cover a similar amount of content as the reference, calculated using the word overlap rate between the summary and the original article.\n\u2022 Compression Ratio (Grusky et al., 2018): Maintains a balanced word count ratio between the generated/reference summary and the original article."
        },
        {
            "heading": "4.3 Main Results",
            "text": "The results demonstrate consistent superiority of the IRL models over both RL and MLE models\nacross datasets and model sizes in the majority of measures. Particularly, the performance improvement of the BART-large model is notably significant for the CNN/DM dataset. On the other hand, it is observed that the BART-large model on the WikiHow dataset adopted a distinct strategy, leading to a notable improvement in ROUGE-1 at the expense of a decline in ROUGE-2 and BERTScore 2. Nevertheless, our IRL model consistently outperforms models trained with MLE and RL across most metrics by effectively balancing and utilizing different sub-reward components across various datasets and models."
        },
        {
            "heading": "4.4 Component-wise Results",
            "text": "The objective of MaxEnt IRL is to learn a reward function that aligns with human behaviors and train the policy to generate summaries similar to expert demonstrations. To assess the effectiveness of IRL in optimizing each dimension of summarization, as identified in prior work as characteristic of effective summarization, we conducted experiments to evaluate the fine-grained performance of BART-base using various training strategies.\nWe present the results for each sub-reward component in Table 2, which demonstrate that our IRL model closely aligns with the references in each component, indicating the successful fulfillment of the IRL training objective. Additionally, the coverage results in Table 2 suggest that models trained with MLE and RL tend to prefer directly copying words from the source article. In contrast, our\n2https://github.com/huggingface/datasets/tree/ 1.15.1/metrics/bertscore\nIRL model generates more abstractive summaries while maintaining a similar coverage to the reference. The only exception is the RL model, which achieves better compression results on the WikiHow dataset. As we train both RL and IRL models concurrently with the MLE model, we consider the MLE result as a reference point for both models. RL models consistently achieve higher compression results than MLE models, as they primarily optimize for the final ROUGE score. However, our IRL model allows us to adjust the MLE result towards the reference (CNN/DM: 15.22-13.23, 13.81; WikiHow: 12.70-15.88, 18.07)."
        },
        {
            "heading": "4.5 Human Evaluation",
            "text": "In addition, we conducted a human evaluation comparing BART-base trained with IRL versus RL on the CNN/DM dataset. The human judges 3 were presented with reference summaries and generations from different summarization systems in a random and anonymized order. The judges were asked to evaluate which system\u2019s summary was more similar to the reference overall. They were instructed to read the source article only when they were unable to decide or needed additional information. 4\nTable 3 presents the human evaluation results. With a confidence level of 95% and one-sided A/B tests, IRL exhibits significantly higher similarity to human-generated reference summaries (p = 0.0246). Furthermore, the preference for IRL (55.67%) surpasses that of RL (47.67%) by a notable margin of 16.78%. Additionally, pairwise inter-annotator agreement was measured, yielding agreement percentages of 55%, 60%, and 57% for the respective evaluations. These findings provide strong support for the IRL method, highlighting its\n3All judges are native English speakers with a minimum of a bachelor\u2019s degree and were compensated at a rate of $19.5/h.\n4We made the decision to make reading the source article optional for the judges in order to prevent creating a significant cognitive burden and to encourage them to take shortcuts.\ncapacity to enhance the quality of summarization outputs."
        },
        {
            "heading": "5 Analysis",
            "text": "This section provides in-depth analyses of the proposed framework from different perspectives, including faithfulness, coverage, entity-level analysis and weight analysis."
        },
        {
            "heading": "5.1 Results on Hallucination Reduction",
            "text": "Following Wang and Sennrich (2020)\u2019s findings correlating exposure bias with hallucination in NMT, we investigated if IRL can alleviate this problem in summarization. We utilized the popular FactCC (Krys\u0301cin\u0301ski et al., 2019) for measuring faithfulness and hallucination (Dong et al., 2022;\nKL-divergence CNNDM MLE: 0.1371 RL: 0.0500 IRL: 0.0078WikiHow MLE: 0.0126 RL: 0.0165 IRL: 0.0097\nCao et al., 2022; Wan and Bansal, 2022). Without considering the abstractive level of the generations, it seems that models trained with MLE have the highest faithfulness scores according to Table 4. However, we also notice that reference summaries have lower FactCC scores. The decrease in FactCC might be rooted in reference summaries being more abstract, indicated by the novel n-gram measures (left column). Still, IRL models tend to generate summaries more closely aligned with the reference in terms of novelty and FactCC.\nTo further measure the abstractiveness vs. faithfulness trade-off, we plot the trade-off curve similar to Ladhak et al. (2022) by using the REF as the anchor. As Figure 2 shows, the curve for our IRL model is significantly above both the MLE and RL model, which demonstrates that our IRL model tends to be \"abstractive\" with the slowest decline of faithfulness."
        },
        {
            "heading": "5.2 Coverage Analysis",
            "text": "One limitation of the Coverage metric from Grusky et al. (2018) is its disregard for the position of copied text in the original source. We believe that the position of these copied fragments is also crucial because a different copy position may increase the Coverage score but widen the gap between the generated summary and the reference summary.\nThe position/location distribution can be seen in Figure 3. We limited the maximum location to 200, as the remaining locations make up a small percentage. It is clear that the IRL model is more closely aligned with the REF, particularly on the CNN/DM\ndataset. To further gauge the similarity of the distributions, we computed the KL-divergence between the models and REF and present the results in Figure 3. A lower score indicates a more similar distribution to REF. On both CNN/DM and WikiHow datasets, the IRL model had the lowest KL score, indicating that it learned a policy that closely mimics the expert policy used to generate REF, which aligns with its intended purpose."
        },
        {
            "heading": "5.3 Entity-level Analysis",
            "text": "Summarization aims to condense the main ideas of an article into a brief summary. We conducted additional analyses to evaluate the ability of the models to preserve important information at the entity level, as presented in Table 5. By comparing the entities extracted from the generated summaries, reference summaries, and original articles, we obtained insightful results. Notably, the IRL model achieved the highest F1 score, indicating its proficiency in retaining important information. Additionally, the IRL model produced shorter summaries with a higher concentration of information on the CNN/DM dataset, making it an effective approach for summarization."
        },
        {
            "heading": "5.4 Weight Analysis",
            "text": "To gain insights into how the IRL model effectively manages diverse sub-reward components, we plot the weight curves associated with these components during the IRL training phase, depicted in Figure 4. The figure reveals compelling observations, illustrating that the weight curves for CNN/DM and WikiHow exhibit similar trends that align with the primary objective of summarization, while also showcasing distinct patterns that correspond to the unique characteristics of each dataset.\nMethod Precision Recall F1 Length\nCNN/DM\nOn both datasets, the ROUGE and Novelty components maintain a consistently positive weight throughout the IRL training process. In contrast, the Coverage component gradually diminishes in importance over time. Notably, the IRL models have acquired distinct compression strategies specific to each dataset. In comparison to the MLE results, the IRL models generate shorter summaries for the CNN/DM dataset, while producing longer summaries for WikiHow. Importantly, these revised summaries closely match the reference summaries, indicating improved performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduce inverse reinforcement learning into text summarization and demonstrate the efficiency of this method. Using IRL, we can train a policy to better match human behaviors by learning from expert demonstrations. Our experimental results in-\ndicate that IRL can improve the summary quality in a variety of measures, including ROUGE, novelty, Coverage, compression ratios, and factuality. Thus, our empirical results suggest that IRL can better fit into the goal of summarization, in addition to providing more interpretable training."
        },
        {
            "heading": "7 Limitations",
            "text": "We only considered four sub-rewards to fit into the summarization task for interpretable results. However, IRL allows for the use of more sub-rewards during training, and as such, there is potential for further exploration in this area. Secondly, we use self-critical policy gradient training as the backbone RL algorithm, but other advanced algorithms such as PPO (Schulman et al., 2017) could be incorporated into IRL and summarization in the future. IRL does not restrict the choice of the backbone RL algorithm during the reward update phase."
        },
        {
            "heading": "8 Acknowledgments",
            "text": "The present research was partially supported by the Natural Science Foundation of Xinjiang Uygur Autonomous Region (No. 2022D01D43) and Zhejiang Lab (No. 2022KH0AB01). We would like to thank the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Mixed Training For abstractive summarization, ROUGE metric is widely used to evaluate the performance of the summarization model. For reinforcement learning, if we simply use ROUGE as the reward with only the RL loss, it may cause too many repetitions in the final output. Following Paulus et al. (2018), we use the MLE loss together with the RL loss for training, as:\nLMix = (1\u2212 \u03b3)LRL + \u03b3LMLE where \u03b3 is a hyper-parameter. In other words, the LRL used in the paper is actually LMix.We set \u03b3=0.0016 for the CNN/DailyMial dataset as in (Paulus et al., 2018). Similarly, we also set \u03b3=0.0016 for the WikiHow dataset.\nA.2 Training and Evaluation Details We used the BART-base6 model as our backbone model. It has around 140M parameters. We performed the MLE, RL, and IRL training on four GeForce RTX 2080Ti GPUs with 11 GB of memory each. For the MLE training, we followed the scripts provided by the transformers7 package. For the RL training, as using the full dataset requires too much time, following (Pasunuru and Bansal, 2018), we used the first 10K examples in the dataset to train the model. The training epoch was set to 20, and the policy learning rate \u03b1 was set to 1e-6 for both RL and IRL. Additionally, for IRL training, we set N = M = 100 in Eqn. (8) to update the reward. The update frequency used in Algorithm 1 was set to 1. For all of the training methods, we chose the best model based on the ROUGE-L score on the validation set.\nFor evaluation, we used the Hugging Face dataset package8 to get both ROUGE and BERTScore.\n6https://huggingface.co/facebook/bart-base 7https://github.com/huggingface/transformers/\ntree/v4.9.2/examples/pytorch/summarization 8https://github.com/huggingface/datasets/tree/ 1.15.1/datasets"
        }
    ],
    "title": "Inverse Reinforcement Learning for Text Summarization",
    "year": 2023
}