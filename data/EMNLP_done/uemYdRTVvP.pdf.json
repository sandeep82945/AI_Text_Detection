{
    "abstractText": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose Sequence X (Check) GPT, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like waves in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Pengyu Wang"
        },
        {
            "affiliations": [],
            "name": "Linyang Li"
        },
        {
            "affiliations": [],
            "name": "Ke Ren"
        },
        {
            "affiliations": [],
            "name": "Botian Jiang"
        },
        {
            "affiliations": [],
            "name": "Dong Zhang"
        },
        {
            "affiliations": [],
            "name": "Xipeng Qiu"
        }
    ],
    "id": "SP:75888e3482fb9071c4bc3b89ade8c21f83e0f8eb",
    "references": [
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460",
            "year": 2020
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human",
            "year": 2022
        },
        {
            "authors": [
                "Anton Bakhtin",
                "Sam Gross",
                "Myle Ott",
                "Yuntian Deng",
                "Marc\u2019Aurelio Ranzato",
                "Arthur D. Szlam"
            ],
            "title": "Real or fake? learning to discriminate machine from human generated text. ArXiv, abs/1906.03351",
            "year": 2019
        },
        {
            "authors": [
                "Sid Black",
                "Stella Biderman",
                "Eric Hallahan",
                "Quentin Anthony",
                "Leo Gao",
                "Laurence Golding",
                "Horace He",
                "Connor Leahy",
                "Kyle McDonell",
                "Jason Phang"
            ],
            "title": "Gpt-neox-20b: An open-source autoregressive language model",
            "venue": "arXiv preprint arXiv:2204.06745",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Arman Cohan",
                "Franck Dernoncourt",
                "Doo Soon Kim",
                "Trung Bui",
                "Seokhwan Kim",
                "Walter Chang",
                "Nazli Goharian."
            ],
            "title": "A discourse-aware attention model for abstractive summarization of long documents",
            "venue": "Proceedings of the 2018",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Hendrik Strobelt",
                "Alexander Rush."
            ],
            "title": "GLTR: Statistical detection and visualization of generated text",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiheng Huang",
                "Wei Xu",
                "Kai Yu."
            ],
            "title": "Bidirectional lstm-crf models for sequence tagging",
            "venue": "arXiv preprint arXiv:1508.01991.",
            "year": 2015
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Daniel Duckworth",
                "Chris CallisonBurch",
                "Douglas Eck."
            ],
            "title": "Automatic detection of generated text is easiest when humans are fooled",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is BERT really robust? natural language attack on text classification and entailment",
            "venue": "CoRR, abs/1907.11932.",
            "year": 2019
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S Weld",
                "Luke Zettlemoyer."
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "arXiv preprint arXiv:1705.03551.",
            "year": 2017
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein."
            ],
            "title": "A watermark for large language models",
            "venue": "arXiv preprint arXiv:2301.10226.",
            "year": 2023
        },
        {
            "authors": [
                "Keita Kurita",
                "Paul Michel",
                "Graham Neubig."
            ],
            "title": "Weight poisoning attacks on pretrained models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2793\u20132806, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2019
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "Bert-attack: Adversarial attack against bert using bert",
            "venue": "arXiv preprint arXiv:2004.09984.",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Demin Song",
                "Xiaonan Li",
                "Jiehang Zeng",
                "Ruotian Ma",
                "Xipeng Qiu."
            ],
            "title": "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Linyang Li",
                "Pengyu Wang",
                "Ke Ren",
                "Tianxiang Sun",
                "Xipeng Qiu."
            ],
            "title": "Origin tracing and detecting of llms",
            "venue": "arXiv preprint arXiv:2304.14072.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D. Manning",
                "Chelsea Finn."
            ],
            "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "venue": "ArXiv, abs/2301.11305.",
            "year": 2023
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural",
            "year": 2018
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "http://web.archive.org/ web/20230109000707/https://openai.com/ blog/chatgpt/. Accessed: 2023-01-10.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "The Journal of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Irene Solaiman",
                "Miles Brundage",
                "Jack Clark",
                "Amanda Askell",
                "Ariel Herbert-Voss",
                "Jeff Wu",
                "Alec Radford",
                "Gretchen Krueger",
                "Jong Wook Kim",
                "Sarah Kreps"
            ],
            "title": "Release strategies and the social",
            "year": 2019
        },
        {
            "authors": [
                "Irene Solaiman",
                "Miles Brundage",
                "Jack Clark",
                "Amanda Askell",
                "Ariel Herbert-Voss",
                "Jeff Wu",
                "Alec Radford",
                "Jasmine Wang."
            ],
            "title": "Release strategies and the social impacts of language models",
            "venue": "ArXiv, abs/1908.09203.",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Adaku Uchendu",
                "Thai Le",
                "Kai Shu",
                "Dongwon Lee."
            ],
            "title": "Authorship attribution for neural text generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8384\u20138395,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Pengyu Wang",
                "Zhichen Ren."
            ],
            "title": "The uncertaintybased retrieval framework for ancient chinese cws and pos",
            "venue": "Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 164\u2013168.",
            "year": 2022
        },
        {
            "authors": [
                "Dong Zhang",
                "Rong Ye",
                "Tom Ko",
                "Mingxuan Wang",
                "Yaqian Zhou."
            ],
            "title": "Dub: Discrete unit backtranslation for speech translation",
            "venue": "Findings of ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the rapid growth of Large Language Models (LLMs) exemplified by PaLM, ChatGPT and GPT-4 (Chowdhery et al., 2022; OpenAI, 2022, 2023), which are derived from pre-trained models (PTMs) (Devlin et al., 2018; Radford et al., 2019; Lewis et al., 2019; Raffel et al., 2020), AI-generated texts (AIGT) can be seen almost everywhere. Therefore, avoiding the abuse of AIGT is a significant challenge that require great effort from NLP researchers. One effective\n\u2217Advisor. \u2020Corresponding author.\n1Our code and generated datasets are available at https: //github.com/Jihuai-wpy/SeqXGPT\nsolution is AIGT detection, which discriminates whether a given text is written or modified by an AI system (Mitchell et al., 2023; Li et al., 2023).\nCurrent AIGT detection strategies, such as supervised-learned discriminator 2, perplexitybased methods (Mitchell et al., 2023; Li et al., 2023), etc., focus on discriminating whether a whole document is generated by an AI. However, users often modify partial texts with LLMs rather than put full trust in LLMs to generate a whole document. Therefore, it is important to explore fine-grained (e.g. sentence-level) AIGT detection.\nBuilding methods that solve the sentence-level AIGT detection challenge is not an incremental modification over document-level AIGT detection. On the one hand, model-wise methods like DetectGPT and Sniffer require a rather long\n2https://github.com/openai/ gpt-2-output-dataset\ndocument as input (over 100 tokens), making them less effective at recognizing short sentences. On the other hand, supervised methods such as RoBERTa fine-tuning, are prone to overfitting on the training data, as highlighted by Mitchell et al. (2023); Li et al. (2023). Therefore, it is necessary to build sentence-level detectors considering the limitation of current AIGT methods.\nIn this paper, we study the sentence-level AIGT detection challenge:\nWe first build a sentence-level AIGT detection dataset that contains documents with both humanwritten sentences and AI-generated sentences, which are more likely to occur in real-world AIassisted writings. Further, we introduce SeqXGPT, a strong approach to solve the proposed challenge. Since previous works (Solaiman et al., 2019a; Mitchell et al., 2023; Li et al., 2023) show that perplexity is a significant feature to be used for AIGT detection, we extract word-wise log probability lists from white-box models as our foundational features, as seen in Figure 1. These temporal features are composed like waves in speech processing where convolution networks are often used (Baevski et al., 2020; Zhang et al., 2023). Therefore, we also use convolution network, followed by self-attention (Vaswani et al., 2017) layers, to process the wave-like features. We employ a sequence labeling approach to train SeqXGPT, and select the most frequent word-wise category as the sentence category. In this way, we are able to analyze the fine-grained sentence-level text provenance.\nWe conduct experiments based on the synthesized dataset and test modified existing methods as well as our proposed SeqXGPT accordingly. Experimental results show that existing methods, such as DetectGPT and Sniffer, fail to solve sentence-level AIGT detection. Our proposed SeqXGPT not only obtains promising results in both sentence and document-level AIGT detection challenges, but also exhibits excellent generalization on out-of-distribution datasets."
        },
        {
            "heading": "2 Background",
            "text": "The proliferation of Large Language Models (LLMs) such as GPT-4 has given rise to increased concerns about the potential misuse of AIgenerated texts (AIGT) (Brown et al., 2020; Zhang et al., 2022; Scao et al., 2022). This emphasizes the necessity for robust detection mechanisms\nto ensure security and trustworthiness of LLM applications."
        },
        {
            "heading": "2.1 Principal Approaches to AIGT Detection",
            "text": "Broadly, AIGT detection methodologies fall into two primary categories:\n1. Supervised Learning: This involves training models on labeled datasets to distinguish between human and AI-generated texts. Notable efforts in this domain include the studies by Bakhtin et al. (2019); Solaiman et al. (2019b); Uchendu et al. (2020), where supervised models were trained on the top of neural representations. OpenAI releases GPT-2 outputs along with a supervisedlearning baseline based on RoBERTa, which claims that GPT-2 outputs are complicated for pre-trained models to discriminate.\n2. Utilizing Model-wise Features: Rather than relying solely on labeled datasets, this approach focuses on intrinsic features of texts, such as token log probabilities, token ranks, and predictive entropy, as proposed by Gehrmann et al. (2019); Solaiman et al. (2019b); Ippolito et al. (2020). The original version of GPTZero 3 attempts to use the text perplexities as white-box features, while DetectGPT (Mitchell et al., 2023) innovated a perturbation-based method leveraging average per-token log probabilities. The recent work of Li et al. (2023) extends this further by studying multi-model detection through text perplexities, aiming to trace the precise LLM origin of texts."
        },
        {
            "heading": "2.2 Detection Task Formulations",
            "text": "The AIGT detection tasks can be formulated in different setups:\n1. Particular-Model Binary AIGT Detection: In this setup, the objective is to discriminate whether a text was produced by a specific known AI model or by a human. Both GPTZero and DetectGPT fall into this category.\n2. Mixed-Model Binary AIGT Detection: Here, the detectors are designed to identify AI-generated content without the need to pinpoint the exact model of origin.\n3https://gptzero.me/\n3. Mixed-Model Multiclass AIGT Detection: This is a more difficult task, aiming not only to detect AIGT but also to identify the exact AI model behind it. Sniffer (Li et al., 2023) is a prominent model in this category."
        },
        {
            "heading": "2.3 Related Fields of Study",
            "text": "It\u2019s worth noting that AIGT detection has parallels with other security-centric studies such as watermarking (Kurita et al., 2020; Li et al., 2021; Kirchenbauer et al., 2023), harmful AI (Bai et al., 2022), and adversarial attacks (Jin et al., 2019; Li et al., 2020). These studies, together with AIGT detection, are indispensable for utilizing and safeguarding these advanced technologies and becoming more important in the era of LLMs."
        },
        {
            "heading": "3 Sentence-Level AIGT Detection",
            "text": ""
        },
        {
            "heading": "3.1 Sentence-Level Detection",
            "text": "The document-level detection methods struggle to accurately evaluate documents containing a mix of AI-generated and human-authored content or consisting of few sentences, which can potentially lead to a higher rate of false negatives or false positives.\nIn response to this limitation, we aim to study sentence-level AIGT detection. Sentence-level AIGT detection offers a fine-grained text analysis compared to document-level AIGT detection, as it explores each sentence within the entire text. By addressing this challenge, we could significantly reduce the risk of misidentification and achieve both higher detection accuracy and finer detection granularity than document-level detection.\nSimilarly, we study sentence-level detection challenges as follows:\n1. Particular-Model Binary AIGT Detection: Discriminate whether each sentence within a candidate document is generated by a specific model or by a human.\n2. Mixed-Model Binary AIGT Detection: Discriminate whether each sentence is generated by an AI model or a human, regardless of which AI model it is.\n3. Mixed-Model Multiclass AIGT Detection: Discriminate whether each sentence is generated by an AI model or a human and identify which AI model generates it."
        },
        {
            "heading": "3.2 Dataset Construction",
            "text": "Current AIGT detection datasets are mainly designed for document-level detection. Therefore, we synthesize a sentence-level detection dataset for the study of fine-grained AIGT detection.\nSince the human-authored documents from SnifferBench cover various domains including news articles from XSum (Narayan et al., 2018), social media posts from IMDB (Maas et al., 2011), web texts (Radford et al., 2019), scientific articles from PubMed and Arxiv(Cohan et al., 2018), and technical documentation from SQuAD (Rajpurkar et al., 2016), we utilize these documents to construct content that include both human-authored and AI-generated sentences. To ensure diversity while preserving coherence, we randomly select the first to third sentences from the human-generated documents as the prompt for subsequent AI sentence generation.\nSpecifically, when generating the AI component of a document using language models such as GPT2, we utilize the prompt obtained via the above process to get the corresponding AI-generated sentences. In the case of instruction-tuned models like GPT-3.5-turbo, we provide specific instruction to assure the generation. We show instruction details in Appendix D.\nRegarding open-source models, we gather the AI-generated portions of a document from GPT-2 (Radford et al., 2019), GPT-J (Wang and Komatsuzaki, 2021), GPT-Neo (Black et al., 2022), and LLaMA (Touvron et al., 2023). For models which provide only partial access, we collect data from GPT-3.5-turbo, an instruction-tuned model. With a collection of 6,000 human-written documents from SnifferBench, we similarly assemble 6,000 documents containing both human-written and AI-generated sentences from each of the models. This results in a total of 30,000 documents. Accordingly, we have:\nParticular-Model Binary Detection Dataset To contrast with DetectGPT and the log p(x) method, we construct the dataset using documents that consist of sentences generated by a particular model or by humans. For each document, sentences generated by the particular model are labeled as \"AI\", while sentences created by humans are labeled as \"human\". Using the collected data, we can construct five binary detection datasets in total.\nMixed-Model Binary Detection Dataset We utilize all the collected data to construct a dataset\nintended for multi-model binary detection research. For each document, we label sentences generated by any model as \"AI\", and sentences created by humans as \"human\".\nMixed-Model Multiclass Detection Dataset For multiclass detection research, we again use the entire data. Unlike the binary detection dataset that merely differentiates between human-created and AI-generated sentences, we use more specific labels such as \"GPT2\", \"GPTNeo\", and so forth, instead of the generic label \"AI\". This allows us to further distinguish among a variety of AI models.\nFor each dataset, We divide the documents into a train/test split with a 90%/10% partition. We name the collected datasets SeqXGPT-Bench, which can be further used for fine-grained AIGT detection."
        },
        {
            "heading": "4 SeqXGPT",
            "text": "To solve the sentence-level detection challenge, we introduce a strong method SeqXGPT:"
        },
        {
            "heading": "4.1 Strategies for Sentence-Level Detection",
            "text": "A sentence-level detection task is a specific form of the sentence classification task. Therefore, there are mainly two approaches:\nSentence Classification This method treats each sentence in the document as an input text and independently determines whether each sentence is generated by AI, as depicted in Figure 2(b).\nSequence Labeling This method treats the entire document as input, creating a sequence labeling task to decide the label for each word. For each sentence, we count the number of times each label appears and select the most frequent label as the category of the sentence, as shown in Figure 2(c). It\u2019s worth noting that each dataset uses a crosslabel set based on {B, I,O}, similar to the usage in\nsequence labeling tasks (Huang et al., 2015; Wang and Ren, 2022). We use B-AI, B-HUMAN, etc. as word-wise labels."
        },
        {
            "heading": "4.2 Baseline Approaches",
            "text": "In the sentence-level AIGT detection challenge, we explore how to modify baselines used in documentlevel detection:\nlog p(x): We consider sentence-level detection as a sentence classification task to implement the log p(x) method. The log p(x) method discriminates AIGT through perplexity. Specifically, we utilize a particular model such as GPT-2 to compute the perplexity of each sentence in the document and draw a histogram (Figure 4) to show the distribution of perplexity. Then we select a threshold manually as the discrimination boundary to determine whether each sentence is generated by a human or an AI.\nDetectGPT (Mitchell et al., 2023): Similarly, we regard sentence-level detection as a sentence classification task to implement DetectGPT. DetectGPT discriminates AIGT using their proposed z-score. For each sentence, we add multiple perturbations (we try 40 perturbations for each sentence). We then compute the z-scores for each sentence based on a particular model and generate a histogram (Figure 4) showing the score distribution. Similarly, we manually select a threshold to discriminate AI-generated sentences.\nSniffer (Li et al., 2023): Sniffer is a powerful model capable of detecting and tracing the origins of AI-generated texts. In order to conduct sentencelevel AIGT detection, we train a sentence-level Sniffer following the structure and training process of the original Sniffer, except that our input is a single sentence instead of an entire document.\nRoBERTa: RoBERTa (Liu et al., 2019) is based on the Transformer encoder that can perform sentence classification tasks as well as sequence labeling tasks. Based on RoBERTa-base, we train two RoBERTa baselines for sentence-level AIGT detection. As mentioned in Sec. 4.1, one model employs a sequence labeling approach, while the other uses a sentence classification method. They are referred to as Seq-RoBERTa and SentRoBERTa, respectively."
        },
        {
            "heading": "4.3 SeqXGPT Framework",
            "text": "In this section, we will provide a detailed introduction to SeqXGPT. The SeqXGPT employs a sequence labeling approach for sentencelevel AIGT detection, which consists of the following three parts: (1) Perplexity Extraction and Alignment; (2) Feature Encoder; (3) Linear Classification Layer.\nPerplexity Extraction and Alignment Previous works demonstrate that both the average per-token log probability (DetectGPT) and contrastive features derived from token-wise probabilities (Sniffer) can contribute to AIGT detection. Therefore, in this work, we choose to extract token-wise log probability lists from several public open-source models, serving as the original features of SeqXGPT.\nSpecifically, given a candidate text S, a known model \u03b8n and the corresponding tokenizer Tn, we obtain the encoded tokens x = [x1, ..., xi, ...] and a list of token-wise log probabilities ll\u03b8n(x), where the log probability of the ith token xi is the loglikelihood ll\u03b8n(xi) = logp\u03b8n(xi|x<i). Given a list of known models \u03b81, ..., \u03b8N , we can obtain N tokenwise log probability lists of the same text S.\nConsidering that the tokens are commonly subwords of different granularities and that different tokenizers have different tokenization methods (e.g. byte-level BPE by GPT2 (Radford et al.,\n2019), SentencePiece by LLaMA (Touvron et al., 2023)), they do not align perfectly. As a result, we align tokens to words, which addresses potential discrepancies in the tokenization process across different LLMs. We use a general word-wise tokenization of S: w = [w1, . . . , wt] and align calculated log probabilities of tokens in x to the words in w. Therefore, we can obtain a list of word-wise log probabilities ll\u03b8n(w) from tokenwise log probability list ll\u03b8n(x). We concatenate these word-wise log probability lists together to form the foundational features L = [l1, . . . , lt], where the feature vector of the ith word wi is li = [ll\u03b81(wi), . . . , ll\u03b8N (wi)].\nWe select GPT2, GPT-Neo, GPT-J, and LLaMA as white-box models to obtain the foundational features. In the various implementations of alignment algorithms, this paper utilizes the alignment method proposed by Li et al. (2023).\nFeature Encoder The foundational features are lists of word-wise log probabilities, which prevents us from utilizing any existing pre-trained models. Consequently, we need to design a new, efficient model structure to perform sequence labeling for AIGT detection based on these foundational features.\nThe word-wise log probability lists for the same sentence of different LLMs may differ due to different parameter scales, training data, and some other factors. Here, we propose treating the word-wise log probability list as a kind of feature that reflects a model\u2019s understanding of different semantic and syntactic structures. As an example, more complex models are able to learn more complex language patterns and syntactic structures, which can result in higher probability scores. Moreover, the actual log probability list might be subject to some uncertainty due to sampling randomness and other factors. Due to these properties, it is possible to view these temporal lists as waves in speech signals. Inspired by speech processing studies, we choose to employ convolutional networks to handle these foundational features. Using convolutional networks, local features can be extracted from input sequences and mapped into a hidden feature space. Subsequently, the output features are then fed into a context network based on self-attention layers, which can capture long-range dependencies to obtain contextualized features. In this way, our model can better understand and process the\nwave-like foundational features. As shown in Figure 3, the foundational features are first fed into a five-layer convolutional network f : L \u2192 Z to obtain the latent representations [z1, . . . , zt]. Then, we apply a context network g : Z \u2192 C to the output of the convolutional network to build contextualized features [c1, . . . , ct] capturing information from the entire sequence. The convolutional network has kernel sizes (5, 3, 3, 3, 3) and strides (1, 1, 1, 1, 1), while the context network has two Transformer layers with the simple fixed positional embedding. More implementation details can be seen in Appendix A.\nLinear Classification Layer After extracting the contextualized features, we train a simple linear classifier F (\u00b7) to project the features of each word to different labels. Finally, we count the number of times each word label appears and select the most frequent label as the final category of each sentence."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Implementation Details",
            "text": "We conduct sentence-level AIGT detection based on SeqXGPT-Bench. As part of our experiments, we test modified existing methods as described in Sec. 4.2, as well as our proposed SeqXGPT detailed in Sec. 4.3.\nWe use four open-source (L)LMs to construct our SeqXGPT-Bench: GPT2-xl (1.5B), GPT-Neo (2.7B), GPT-J (6B) and LLaMA (7B). These models are also utilized to extract perplexity lists, which are used to construct the original features of our SeqXGPT and the contrastive features for Sniffer. For each model, we set up an inference server on NVIDIA4090 GPUs specifically for extracting perplexity lists. The maximum sequence length is set to 1024 for all models, which is the maximum context length supported by all the pre-trained models we used. We align all texts with a white-space tokenizer to obtain uniform tokenizations for our SeqXGPT.\nIn the evaluation process, we utilize Precision (P.), Recall (R.), and Macro-F1 Score as our metrics. Precision and Recall respectively reflect the \"accuracy\" and \"coverage\" of each category, while the Macro-F1 Score effectively combines these two indicators, allowing us to consider the overall performance."
        },
        {
            "heading": "5.2 Sentence-Level Detection Results",
            "text": "In this section, we present three groups of experiments:\nParticular-Model Binary AIGT Detection We test zero-shot methods log p(x) and DetectGPT, as well as Sent-RoBERTa and our SeqXGPT on two datasets shown in Table 1. The results for the other two datasets can be found in the appendix in Table 6.\nAs seen in Figure 4, there is a large overlap between the peaks of AI-generated and humancreated sentences when using log p(x) and DetectGPT, making it difficult to distinguish sentences from different categories. The peaks almost completely overlap for DetectGPT in particular. This situation can be attributed to two main factors. Firstly, the perplexity of a sentence tends to be more sensitive than a document. Secondly, given the brevity of sentences, perturbations often fail to achieve the same effect as in DetectGPT. That is, the perturbed sentences either remain nearly identical to the original, or vary significantly. These findings indicate that these two zero-shot methods are unsuitable for sentence-level AIGT detection.\nOverall, SeqXGPT exhibits great performance on all datasets, including the two presented in\nTable 1 and the other two in Table 6. Despite Sent-RoBERTa performs better than two zero-shot methods, it is still far inferior to our method.\nMixed-Model Binary AIGT Detection We implement Sniffer and Sent-RoBERTa based on sentence classification tasks, as well as our SeqXGPT and Seq-RoBERTa based on sequence labeling tasks. From the results in Table 3, it is apparent that our SeqXGPT shows the best performance among these four methods, significantly demonstrating the effectiveness of our method. In contrast, the performance of Sniffer is noticeably inferior, which emphasizes that document-level AIGT detection methods cannot be effectively modified for sentencelevel AIGT detection. Interestingly, we find that the performance of both RoBERTa-based methods is slightly inferior to SeqXGPT in overall performance. This suggests that the semantic features of RoBERTa might be helpful to discriminate human-created sentences.\nMixed-Model Multiclass AIGT Detection As illustrated in Table 2, SeqXGPT can accurately discriminate sentences generated by various models and those authored by humans, demonstrating its strength in multi-class detection. It is noteworthy that RoBERTa-based methods perform significantly worse than binary AIGT detection. They are better at discriminating\nsentences generated by humans or more humanlike LLMs, such as GPT-3.5-turbo, while having difficulty detecting sentences generated by GPT-2/GPT-J/Neo. The reason for this may be that sentences generated by stronger models contain more semantic features. This is consistent with the findings of Li et al. (2023). In contrast, the Seq-RoBERTa outperforms Sent-RoBERTa, indicating that sequence labeling methods, with their ability to obtain the context information of the entire document, are more suitable for sentence-level AIGT detection. Sniffer, however, performs poorly, which may be due to its primary design focusing on document-level detection."
        },
        {
            "heading": "5.3 Document-Level Detection Results",
            "text": "We conduct experiments on a document-level detection dataset (more details in Appendix C.1). For sentence classification-based models, we classify each sentence and choose the category\nwith the highest frequency as the document\u2019s category. For sequence labeling-based models, we label each word and select the category with the greatest number of appearances as the document\u2019s category.\nTable 4 indicates that sentence-level detection methods can be transformed and directly applied to document-level detection, and the performance is positively correlated with their performance on sentence-level detection. Despite their great performance in detecting human-generated sentences, both Seq-RoBERTa and SeqXGPT perform slightly worse in discriminating humancreated documents. This is mainly because our training set for human-created data only comes from the first 1-3 sentences, leading to a shortage of learning samples. Therefore, appropriately increasing human-generated data can further enhance the performance. We can see that Sniffer performs much better in document-level detection, indicating that Sniffer is more suitable for this task. Overall, SeqXGPT exhibits excellent performance in document-level detection."
        },
        {
            "heading": "5.4 Out-of-Distribution Results",
            "text": "Previous methods often show limitations on out-ofdistribution (OOD) datasets (Mitchell et al., 2023). Therefore, we intend to test the performance of each method on the OOD sentence-level detection dataset (more details in Appendix C.2).\nTable 5 demonstrates the great performance of SeqXGPT on OOD data, reflecting its\nstrong generalization capabilities. Conversely, Sent-RoBERTa shows a significant decline in performance, especially when discriminating sentences generated by GPT-3.5-turbo and humans. This observation aligns with the findings from prior studies (Mitchell et al., 2023; Li et al., 2023), suggesting that semantically-based methods are prone to overfitting on the training dataset. However, Seq-RoBERTa performs relatively better on OOD data. We believe that this is because sequence labeling methods can capture contextualized information, helping to mitigate overfitting during learning. Despite this, its performance still falls short of SeqXGPT."
        },
        {
            "heading": "5.5 Ablations and Analysis",
            "text": "In this section, we perform ablation experiments on the structure of SeqXGPT to verify the effectiveness of different structures in the model.\nFirst, we train a model using only transformer layers. As shown in Table 2, this model struggles with AIGT detection, incorrectly classifying all sentences as human-written. Although there are differences between a set of input feature waves (foundational features), they are highly correlated, often having the same rise and fall trend (Figure 3). Further, these features only have 4 dimensions. We think it\u2019s the high correlation and sparsity of the foundational features make it difficult for the Transformer layers to effectively learn AIGT detection. In contrast, CNNs are very good at handling temporal features. When combined\nwith CNNs, it\u2019s easy to extract more effective, concentrated feature vectors from these feature waves, thereby providing higher-quality features for subsequent Transformer layers. Moreover, the parameter-sharing mechanism of CNNs can reduce the parameters, helping us to learn effective features from limited samples. Thus, CNNs play an important role in SeqXGPT.\nHowever, when only using CNNs (without Transformer layers), the model performs poorly on sentences generated by strong models such as GPT-3.5-turbo. The reason for this may be that CNNs primarily extract local features. In dealing with sentences generated by these strong models, it is not sufficient to rely solely on local features, and we must also consider the contextualized features provided by the Transformer layers."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we first introduce the challenge of sentence-level AIGT detection and propose three tasks based on existing research in AIGT detection. Further, we introduce a strong approach, SeqXGPT, as well as a benchmark to solve this challenge. Through extensive experiments, our proposed SeqXGPT can obtain promising results in both sentence and document-level AIGT detection. On the OOD testset, SeqXGPT also exhibits strong generalization. We hope that SeqXGPT will inspire future research in AIGT detection, and may also provide insightful references for the detection of content generated by models in other fields.\nLimitations\nDespite SeqXGPT exhibits excellent performance in both sentence and document-level AIGT detection challenge and exhibits strong generalization, it still present certain limitations:\n(1) We did not incorporate semantic features, which could potentially assist our model further in the sentence recognition process, particularly in cases involving human-like sentence generation. We leave this exploration for future work.\n(2) During the construction of GPT-3.5-turbo data, we did not extensively investigate the impact of more diversified instructions. Future research will dive into exploring the influence of instructions on AIGT detection.\n(3) Due to limitations imposed by the model\u2019s context length and generation patterns, our samples only consist of two distinct sources of sentences.\nIn future studies, we aim to explore more complex scenarios where a document contains sentences from multiple sources."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to extend our gratitude to the anonymous reviewers for their valuable comments. Additionally, we sincerely thank Qipeng Guo for his valuable discussions and insightful suggestions on this study. This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027)."
        },
        {
            "heading": "B More Results",
            "text": "We provided results of Particular-Model Binary AIGT Detection on the other two datasets in Table 6. In fact, we have five binary detection datasets in total. Since Sniffer and DetectGPT can only perform white-box detection, we did not conduct experiment on dataset with AI-generated sentences are from GPT-3.5-turbo."
        },
        {
            "heading": "C Construction of Supplementary Datasets",
            "text": "In addition to the SeqXGPT-Bench designed specifically for sentence-level AIGT detection, we have also developed two additional datasets. These two datasets are respectively constructed for document-level AIGT detection and Outof-Distribution (OOD) sentence-level AIGT detection. In the following, we will elaborate on the construction process of these two datasets.\nC.1 Document-Level Detection Dataset In order to evaluate the performance in documentlevel AIGT detection, we construct a documentlevel detection dataset. To this end, we randomly sampled 200 human-created documents from the test set of SeqXGPT-Bench, which are used to construct AI-generated documents. Similar to Mitchell et al. (2023) and Li et al. (2023), we use the first sentence as a prompt to generate the entire document. This process is essentially the same as the construction process described in Sec. 3.2. The significant difference is that we set the maximum sequence length as large as possible and remove the prompt part after generation. We mix documents created by humans with those generated by AI to form our document-level detection dataset.\nC.2 OOD Sentence-Level Detection Dataset We construct an OOD sentence-level detection dataset to evaluate the performance of various methods on OOD data. Our aim in creating this dataset is to ensure both topic-wise and format-wise diversity to best represent the possible challenges faced by AI-generated text detectors. With this goal in mind, we utilize the triviaQA dataset (Joshi et al., 2017) as the foundational data for our OOD dataset. This choice was made after careful consideration and deliberation.\nTriviaQA covers a vast array of topics, drawing from a diverse collection of sources including blog articles, news reports, encyclopedia entries, and Wikipedia pages. This diversity ensures a comprehensive coverage of topics and introduces a rich variety of text styles and formats. Moreover, a significant advantage of utilizing triviaQA is its publication timeline. Published prior to the widespread emergence of AI text-generation technologies, the dataset is less likely to have been influenced by AI-generated texts, ensuring the authenticity of its human-derived content.\nFor the OOD dataset construction, we randomly\nsample 200 evidence documents from triviaQA. These documents, with their wide-ranging domains and inherent diversity, form an excellent basis for testing OOD performance. The methodology adopted for constructing this dataset mirrors that detailed in Sec. 3.2, where we randomly select the first to third sentences from the human-generated documents as the prompt for subsequent AI generation. We then employ this newly constructed dataset as a testbed for evaluating the OOD performance of the models trained in Sec. 5.2.\nD Instruction Details\nThe specific instruction used in GPT-3.5-turbo is: Please provide a continuation for the following content to make it coherent: {prompt} We consider simple instruction since we focus on studying the sentence-level AIGT detection without further analyzing the working mechanisms and better usage of instructions, which we leave to future works."
        }
    ],
    "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
    "year": 2023
}