{
    "abstractText": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher\u2019s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Hailin Chen"
        },
        {
            "affiliations": [],
            "name": "Amrita Saha"
        },
        {
            "affiliations": [],
            "name": "Steven HOI"
        },
        {
            "affiliations": [],
            "name": "Shafiq Joty"
        }
    ],
    "id": "SP:d5a00b06b5b0f5363200b05ad328943caecb314d",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell I. Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie J. Cai",
                "Michael Terry",
                "Quoc V. Le",
                "Charles Sutton."
            ],
            "title": "Program synthesis with large language models",
            "venue": "CoRR, abs/2108.07732.",
            "year": 2021
        },
        {
            "authors": [
                "Sahil Chaudhary."
            ],
            "title": "Code alpaca: An instruction-following llama model for code generation",
            "venue": "https://github.com/sahil280114/ codealpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Angelica Chen",
                "J\u00e9r\u00e9my Scheurer",
                "Tomasz Korbak",
                "Jon Ander Campos",
                "Jun Shern Chan",
                "Samuel R. Bowman",
                "Kyunghyun Cho",
                "Ethan Perez"
            ],
            "title": "Improving code generation by training with natural language feedback",
            "year": 2023
        },
        {
            "authors": [
                "Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "CoRR, abs/2107.03374.",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "CoRR, abs/2304.05128.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Yuxin Jiang",
                "Chunkit Chan",
                "Mingyang Chen",
                "Wei Wang."
            ],
            "title": "Lion: Adversarial distillation of closed-source large language model",
            "venue": "CoRR, abs/2305.12870.",
            "year": 2023
        },
        {
            "authors": [
                "nite",
                "Carlos Mu\u00f1oz Ferrandis",
                "Sean Hughes",
                "Thomas Wolf",
                "Arjun Guha",
                "Leandro von Werra",
                "Harm de Vries"
            ],
            "title": "2023a. Starcoder: may the source be with you! CoRR, abs/2305.06161",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Li",
                "Zhuoran Yang",
                "Mengdi Wang."
            ],
            "title": "Reinforcement learning with human feedback: Learning dynamic choices via pessimism",
            "venue": "CoRR, abs/2305.18438.",
            "year": 2023
        },
        {
            "authors": [
                "Hao Liu",
                "Carmelo Sferrazza",
                "Pieter Abbeel."
            ],
            "title": "Chain of hindsight aligns language models with feedback",
            "venue": "CoRR, abs/2302.02676.",
            "year": 2023
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang."
            ],
            "title": "Wizardcoder: Empowering code large language models with evolinstruct",
            "venue": "CoRR, abs/2306.08568.",
            "year": 2023
        },
        {
            "authors": [
                "Sean Welleck",
                "Bodhisattwa Prasad Majumder",
                "Shashank Gupta",
                "Amir Yazdanbakhsh",
                "Peter Clark."
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "CoRR, abs/2303.17651.",
            "year": 2023
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong."
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "der",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Rafael Rafailov",
                "Archit Sharma",
                "Eric Mitchell",
                "Stefano Ermon",
                "Christopher D. Manning",
                "Chelsea Finn."
            ],
            "title": "Direct preference optimization: Your language model is secretly a reward model",
            "venue": "CoRR, abs/2305.18290.",
            "year": 2023
        },
        {
            "authors": [
                "Samyam Rajbhandari",
                "Jeff Rasley",
                "Olatunji Ruwase",
                "Yuxiong He."
            ],
            "title": "Zero: memory optimizations toward training trillion parameter models",
            "venue": "SC, page 20. IEEE/ACM.",
            "year": 2020
        },
        {
            "authors": [
                "Heather Roberts-Mahoney",
                "Alexander J. Means",
                "Mark J. Garrison."
            ],
            "title": "Netflixing human capital development: personalized learning technology and the corporatization of k-12 education",
            "venue": "Journal of Education Policy, 31(4):405\u2013420.",
            "year": 2016
        },
        {
            "authors": [
                "Atikah Shemshack",
                "Jonathan Michael Spector."
            ],
            "title": "A systematic literature review of personalized learning terms",
            "venue": "Smart Learning Environments, 7(1):1\u201320.",
            "year": 2020
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Beck Labash",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "year": 2022
        },
        {
            "authors": [
                "Sean Welleck",
                "Ximing Lu",
                "Peter West",
                "Faeze Brahman",
                "Tianxiao Shen",
                "Daniel Khashabi",
                "Yejin Choi."
            ],
            "title": "Generating sequences by learning to self-correct",
            "venue": "CoRR, abs/2211.00053.",
            "year": 2022
        },
        {
            "authors": [
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang."
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "CoRR, abs/2304.12244.",
            "year": 2023
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley."
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "venue": "arXiv preprint arXiv:2304.01196.",
            "year": 2023
        },
        {
            "authors": [
                "Kechi Zhang",
                "Zhuo Li",
                "Jia Li",
                "Ge Li",
                "Zhi Jin."
            ],
            "title": "Self-edit: Fault-aware code editor for code generation",
            "venue": "CoRR, abs/2305.04087.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, powerful close-sourced large langauge models (LLMs) including ChatGPT, GPT-4 have become predominant, accumulating over 170 million users within 5 month of its launch. Such closesourced LLMs demonstrate strong performance in a wide range of tasks, from improving writing proficiency to code generation. However, due to their closed-source nature, concerns have been raised regarding factors such as the availability of these services, high associated costs, concerns on ethics and safety, and potential data privacy implications,\n*These authors contributed equally to this work 1Our codes will be available at https://github.\ncom/salesforce/PersDistill\nall of which limit their seamless integration into real-world applications. In light of these concerns, a natural question arises: Can we distill the remarkable abilities exhibited by closed-source LLMs into smaller open-source LLMs?\nResearchers have explored such distillation idea (Taori et al., 2023; Wang et al., 2022; Xu et al., 2023b), by querying ChatGPT to generate task instruction and solution pairs, and using the collected data to finetune a student model. However, this standard distillation approach fits different student models to the same data distribution (teacher\u2019s prior), disregarding their unique abilities and capacity. In education domain, personalised learning which provides customized learning experience that adapts to student\u2019s learning progress and capacity, has proven highly effective and widely adopted (Roberts-Mahoney et al., 2016; Shemshack and Spector, 2020). Inspired by such finding, we hypothesize that personalised learning is also beneficial for model distillation.\nIn this work, we propose personalised distillation and empirically evaluate its effectiveness in the domain of code generation. Similar to standard distillation, we first employ ChatGPT to generate task instructions accompanied by unit test cases. Then we follow three steps for personalized distillation as shown in Figure 1. First, we let the student model attempt to solve the task. Then, we evaluate the student\u2019s attempt with unit test cases and get execution feedback. If the execution feedback contains errors, in the final step we prompt the teacher model (ChatGPT) to refine the student\u2019s attempt.\nSuch data collection process makes the learning experience both interactive \u2014 as the student participates to make attempts, and personalised \u2014 both the input (tasks) and output (refinement data) are customised to the student. Essentially, personalised labeled data help the student to refine its own policy, rather than adopting a new prior of the teacher.\nWith the personalized code data as target out-\nFigure 1: Overview of our framework. Left: standard distillation. 1 Teacher generates standard answer to a given problem for the student to learn Right: personalised distillation. 1 Student first generates its own attempt to solve the task. 2 Executor evaluates generated code with unit test cases. 3 Teacher provides adaptive refinement given student\u2019s attempt and its execution feedback.\nput, we construct three variants of finetuning data (i) PERsD data which formats it as a typical textto-code generation task, (ii) PERsD-refine which treats it as a code-refinement task, given a task instruction, incorrect code and execution error feedback (ii) PERsD-combine which simply combines PERsD and PERsD-refine finetuning data, i.e. code generation and refinement tasks.\nWe collect 10K standard distillation examples and around 2.5-3K personalised examples for pretraining. Through zero-shot evaluation on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), we observe that all PERsD variants consistently outperform their counterparts which use standard distillation. This compelling result strongly validates our hypothesis regarding the advantages of personalized distillation. Ablation studies further reinforce our hypothesis, uncovering intriguing properties such as the benefits of multi-round personalized distillation and the ability of our models to leverage execution feedback for self-correction. Notably, personalised distillation boosts the state-of-the-art open-sourced pretrain model StarCoder (Li et al., 2023a) significantly \u2014 by 12.2% to achieve 45.8 in pass@1 and 82.3 in pass@100 on HumanEval."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Distillation from ChatGPT",
            "text": "Previous works have explored distillation from ChatGPT including Alpaca(Taori et al., 2023), Vicuna(Chiang et al., 2023) and Baize(Xu et al., 2023b). However, these works can all be considered as standard distillation as they do not consider the conditions and capacity of student model. Wiz-\nardLM(Xu et al., 2023a) and WizardCoder(Luo et al., 2023) iteratively prompts teacher model to generate more complex instructions. Their approach can be seen as an orthogonal advancement that can potentially be combined with personalised distillation.\nLion (Jiang et al., 2023) proposes to incorporate student model\u2019s answer and sample more hard tasks for which the student failed to solve. Thus, Lion can be considered as input personalised distillation as only the input tasks are customised for different student. Our approach differs as we provide customization both on input and output, and we empirically show that personalising labels is critically beneficial."
        },
        {
            "heading": "2.2 Code Generation with Feedback",
            "text": "Recently, there has been an increasing amount of research on exploring on how to use feedback for an iterative and improved code generation through code-refinement. Self-refine(Madaan et al., 2023), Self-debug(Chen et al., 2023b) and Reflexion (Shinn et al., 2023) are inference-time methods which use powerful close-sourced LLMs to generate better code from internal or external feedback. Although they show high performance,\nthese methods are limited as they require access to close-sourced LLMs. Self-edit (Zhang et al., 2023) trains a separate code editor to rectify generated code from a base LLM. The training label is from original gold answer, thus not label-personalised. Similarly, Self-correct (Welleck et al., 2022) trains a separate corrector model to rectify the output from a fixed generator model. However, the training label is from self-exploration of the corrector model: sampling multiple refinements and choosing the one leading to higher reward. Finally, ILF (Chen et al., 2023a) collects human-annotated code refinement data to train a separate refinement model on it. Fhe refinement model is used to generate text-to-code data for finetuning the code-generation LLM. Unlike ILF, our approach is more scalable as we do not require human annotation and our personalized data proves significantly more effective than ILF as we empirically investigate in \u00a75."
        },
        {
            "heading": "2.3 Reinforcement Learning from (Human) Feedback",
            "text": "After the launch of ChatGPT, aligning LLMs to human preference has drawn tremendous attention to research communities. As one of the most influential approaches in this direction, reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Li et al., 2023b), adopts an actor-critic framework, where the student model is optimized to generate responses to receive higher reward from the critic model. In InstructGPT (Ouyang et al., 2022), the critic (reward model) is trained from human annotation. Direct Preference Optimization (DPO) (Rafailov et al., 2023) drops the need of training a reward model, by using a reference LLM and offline trajectories to estimate the reward. Chain-of-Hindsight (Liu et al., 2023) converts human preference annotations into simple natural language feedback, and thus turns RL optimization to conditional generation. In above methods, the\nassumption is that there are no ground truth targets and thus they try to improve the LLM based on the assessment (critic) of multiple generated outputs. However, such RL-style training will be less effective and efficient to supervised finetuning, especially for challenging tasks with sparse rewards \u2013 e.g. sovling math puzzles or coding tasks. Unlike these methods, our approach can acquire \"ground truth\" outputs from a personalised teacher, thus supervised finetuning can be applied which makes the learning effective and efficient, even for challenging tasks like solving coding problems."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Standard Distillation",
            "text": "Assume a dataset of code generation tasks D = {(t, u)} where each problem (or task) consists of a task instruction t and a unit test collection u. During training, we have access to a teacher model \u03c0\u03d5 and a student model \u03c0\u03b8. The objective is to distill how the teacher solves code generation tasks to the student model, in the context of D. For each task (t, u), we first query the teacher \u03c0\u03d5(t) with the task instruction, to get a direct generated code snippet c\u03d5. Then, we execute the generated code c\u03d5 against unit test cases u and get its execution feedback f \u2190 EXEC(c\u03d5, u), where the EXEC function returns passed if the code passes all the unit tests, otherwise it returns an error message from the executor. By filtering out the tasks where c\u03d5 do not pass all the unit tests (i.e., f \u0338= passed), we get a new clean dataset DSTAND = {(t, u, c)}, where each task consists a task instruction t, a suite of unit tests u and a correct solution code c.\nWe then finetune the student model \u03c0\u03b8 on {(u, c)} \u223c DSTAND, where the input is the task instruction u and the output is the corresponding code solution c. We name this approach STAND."
        },
        {
            "heading": "3.2 Personalised Distillation",
            "text": "The STAND approach simply samples training examples (instructions and labels) from the prior distribution of the teacher model and feeds it to the student without considering the conditions of the student model. Inspired by modern education principles which advocates interactive and personalised learning experience, we propose personalised distillation: adapting teaching materials to student\u2019s current knowledge\nAlgorithm 1 personalised distillation for code generation (PERsD-combined).\n1: Input: Dataset DSTAND, student LLM \u03c0\u03b8 , unit test executor EXEC, refinement template Trefine, teacher LLM \u03c0\u03d5 2: Drefine \u2190 {} \u25b7 refinement data for finetuning 3: Dcode \u2190 {} \u25b7 direct generation data 4: for (t, u, c) \u2208 DSTAND do 5: c\u03b8 \u2190 \u03c0\u03b8(t) \u25b7 student generates c\u03b8 6: f \u2190 EXEC(c\u03b8, u) \u25b7 exec. feedback for c\u03b8 7: if f \u0338= passed then 8: // personalised refinement from teacher 9: crefine \u2190 \u03c0\u03d5(t, c\u03b8, f)\n10: // create refinement task instruction 11: trefine \u2190 Trefine(t, c\u03b8, f) 12: if EXEC(crefine, u) = passed then 13: Drefine.insert({trefine, crefine}) 14: Dcode.insert({t, c}) 15: end if 16: end if 17: end for 18: \u03c0\u03b8\u2217 \u2190 FINETUNE(\u03c0\u03b8,Drefine +Dcode)\nand capacity. We propose three variants:\nPERSD-combined Algorithm 1 shows detailed steps for PERSD-combined. This method takes the standard distillation dataset DSTAND from \u00a73.1 and first lets the student generate solutions for each task. Then it filters out the tasks where the student model can already solve correctly. For the remaining tasks, it obtains the teacher\u2019s personalised refinement conditioned on the student\u2019s attempt and its execution error feedback, and only keeps the tasks where the teacher\u2019s refinement is valid (i.e., passes all the unit test cases). Figure 1 visualizes these three steps.\nFor this final task-set, we create two datasets: i) Dcode containing task instruction as input and teacher\u2019s direct answer as output, and ii) Drefine containing task refinement instruction as input and personalised refinement answer as output. The task refinement instruction (line 9 in Algorithm 1) is created by concatenating task instruction t, student\u2019s attempt c\u03b8 and its execution feedback f with a refinement template Trefine (More details in Appendix C). Such refinement instruction turns standard code generation into a code refinement task, teaching the student how to refine its own solution. PERSD-combined then finetunes the student model on Drefine combined with Dcode.\nPERSD-refine Similar to PERSD-combined, this variant follows line 1-15 of Algorithm 1 to collect refinement data Drefine. However, it differs from the above model as it only uses Drefine to finetune the student model.\nPERSD This variant takes the training data Drefine from PERSD-refine and replace the input of each data point from code refinement prompt to original task instruction. It thus trains the student model with personalised labels on code generation.\nTo illustrate the difference between personalised refinement and teacher\u2019s direct solution, we show a real example in Figure 2. The top shows the personalised refinement for the given task, while the bottom section shows the direct teacher\u2019s generation for the same task. Note how the teacher\u2019s direct generation is significantly different from the student model\u2019s attempt, while the teacher\u2019s refinement follows the student\u2019s attempt and improves upon it. We hypothesize that such adaptive refinement where the teacher aligns to student\u2019s generation, helps the student to learn more efficiently and effectively, similar to how humans benefit from personalised learning."
        },
        {
            "heading": "3.3 Iterative Inference",
            "text": "Let Dtest = {(t, u)} denote our test set for inference, where each data point (t, u) consists of a task instruction t and a suite of hidden unit test cases u. We also assume that the task instruction contains some simple unit test cases in its doc-string (as often seen in code generation instructions), which we can extract and format using rule-based heuristics to obtain a suite of seen unit test cases useen (More details in Appendix A).\nFor single-step inference, we use the standard approach to evaluate pass@k. Specifically, for each task t, we query the model n times with the task instruction: ci\u03b8 \u2190 \u03c0\u03b8(t) for i = 1 . . . n. Then, following (Chen et al., 2021), we estimate pass@k from the number of attempts that passed the hidden unit test cases: EXEC(ci\u03b8, u) = passed.\nMulti-step inference If the model \u03c0\u03b8 has been trained to rectify, following our approach in PERsD-refine or PERsD-combine, and if unit tests are available during inference, we can perform 2-step inference: for each generated attempt ci\u03b8 in 1-step, we first get execution feedback f iseen \u2190 EXEC(ci\u03b8, useen). If f iseen = passed, we reuse the original attempt as the 2-step attempt. Otherwise, we create a refinement instruction ti \u2190 Trefine(t, ci\u03b8, f iseen) following the approach in PERsD-refine or PERsD-combined, and query the same model with the refinement instruction for 2-step attempt: ci\u03b8,2-step \u2190 \u03c0\u03b8(ti). We then compute pass@k over the 2-step generations similar to 1-step inference."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Baselines",
            "text": "The first baseline is STAND, the standard distillation approach mentioned in \u00a73.1.\nTo measure the effectiveness of personalised labels quantitatively, we also compare with Input-personalised distillation baselines as well, where only the input tasks are selected in a manner customized to the student\u2019s abilities. However, the output labels are not personalised, as they are taken from teacher\u2019s direction generation c instead of personalised refinement crefine. We start with Dcode from PERSD-combined and have three variants:\nINPD We finetune the student model \u03c0\u03b8 on {(t, c)} \u223c Dcode, where the input is a task instruction and the output is a code solution. This variant is more customized than STAND as it filters out the tasks which the student can already solve correctly.\nINPD-refine Similar to PERsD-refine, InpD-refine trains the student model to rectify its wrong attempt. The difference is in InpD-refine, the refined code is from teacher\u2019s direct solution c, instead of personalised refinement crefine.\nINPD-combined Similar to PERSD-combined, InpD-combined trains the student on rectifying its\nanswers as well as directly solving the task. The difference is that in InpD-combined, the labels for both code refinement and code generation are taken from teacher\u2019s direct solution c."
        },
        {
            "heading": "4.2 Pretraining Data Construction",
            "text": "To construct our pretraining data, we adopted the data collection process in code-alpaca(Chaudhary, 2023) and used a set of 374 seed tasks from MBPP (task-ids 601-974) as in-context prompt to query ChatGPT for novel code generation tasks. This seed-set increases the likelihood of ChatGPT generating python codes.\nThrough this process, we obtained a corpus of 20K code generation tasks from ChatGPT each comprising a task instruction and the corresponding generated code, which is typically a single python function. Next we show each generated instance to ChatGPT again and prompt it to generate 5 unique test-case inputs (i.e. input argument values) for the python function. We then parse and format the generated test-case input and execute the generated code on it obtain an output. Thus, out of 20K, for 14880 instances we could successfully generate and parse 5 unit test case inputs and for 10172 instances we were able to successfully execute the generated code and obtain outputs on all 5 inputs. This final corpus of 10K code generation tasks, each comprising a task instruction and the corresponding generated code along with 5 unit test input and outputs forms our standard distillation dataset DSTAND.\nTo collect personalised distillation data, we follow \u00a73.2 to first ask the student model to generate 1 output code per task, setting sampling temperature to 0.3. We then evaluate the student\u2019s attempt and only keep the tasks with the wrong generations (i.e. the ones which failed any of the unit test-case). We use this to query ChatGPT for personalised refinements and only retain the valid refinements which passed all unit tests. Our prompt to ChatGPT contains the original task instruction and code from DSTAND along with the student model\u2019s generated code and execution feedback (compiler errors or unit test failures). Our instruction to ChatGPT is to generate a correct solution that rectifies the errors and is closest in semantics to the student\u2019s code (More details in Appendix B). Table 3 shows the statistics of personalised data construction process."
        },
        {
            "heading": "4.3 Model Evaluation",
            "text": "We evaluate our models on two datasets: HumanEval(Chen et al., 2021), which contains 164 Python problems, and the subset MBPP(Austin et al., 2021) sanitized set that has no overlap with our MBPP seed tasks for pretraining data collection. This corresponds to test+validation+prompt splits of MBPP-sanitized and consists of 306 Python problems. We use nucleus sampling with temperature 0.2 to generate 20 candidates per task for estimating pass@1, and with temperature 0.8, 100 candidates per task for estimating pass@5/10/20/50/100.\nFor multi-step inference, we first extract the \u201cseen\u201d unit test-cases from the doc-string of the task instruction (More details in Appendix A). Next, we generate output samples in the usual code-generation style forming the set of 1-step generations for each instance. Each of these candidate generations are then executed on the extracted \u201cseen\u201d unit test cases to obtain a refined code, thus forming the set of 2-step generations."
        },
        {
            "heading": "4.4 Pretraining Setup",
            "text": "For all experiments with CodeGen-mono-6B backbone, we use effective batch size of 1024 and pretrain for 20 epochs. For backbone as CodeGenmono-16B, we use effective batch size of 1024 and pretrain for 3 epochs, as the training converges much faster than CodeGen-mono-6B. For PERsDcombine with StarCoder model, we use effective batch size of 1024 and pretrain for 8 epochs, which results in similar training loss as CodeGen-mono16B. We implement using HuggingFace transformers(Wolf et al., 2020) and DeepSpeed Zero (Rajbhandari et al., 2020). All experiments are conducted on a cluster of 8 A100-40GB GPUs."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "We empirically test the hypothesis that personalised distillation helps student model learn more effectively, by comparing PERsD models with baseline distillation methods (InpD, StanD) in Table 4.\nPersonalised labeled-data is generally better than standard data Comparing PERsDcombine to InpD-combine, we find PERsDcombine outperforms InpD-combine in all settings, often with a significant margin (two backbones, two datasets, two inference steps, 4 pass@k metric). Similar observation holds true when comparing PERsD-refine to InpD-refine (except for 2/32 settings), and PERsD to InpD. Thus, we conclude that PERsD-variants are generally significantly better than their InpD counterparts, providing strong evidence that personalised labels are more effective for the student model to learn than standard labels.\nPERsD outperforms StanD with less than onethird of its data We observe that PERsD outperforms StanD for every pass@k on both 16B and 6B CodeGen-mono backbone across both HumanEval and MBPP, even though StanD has 10K data and PERsD has only 3.3K and 2.8K examples for CodeGen-mono-6B and 16B. The only exception is in the setting CodeGen-mono-16B, MBPP, pass@1, where StanD edges out PERsD by 1.2 points. Given that our pretraining data is constructed from seed tasks taken from MBPP, we hypothesize that StanD might enjoy an unfair advantage due to its having three times more data, making it more susceptible to data leakage. We verify such hypothesis further in \u00a75.2. In summary, with PERsD outperforming StanD in 15 out of 16 settings while having less than a third of the data, it\u2019s evident that personalized labeled data makes the learning more efficient.\nMulti-step inference consistently improves answer quality For PERsD-refine and PERsDcombine models, we find that 2 step inference consistently improves performance on HumanEval and MBPP. This shows the models successfully learn how to rectify its solution based on execution error feedback. Note that InpD-refine yields worse accuracy with 2 step inference on HumanEval pass@10/20, strengthening the advantage of personalised labeled data over standard labeled data."
        },
        {
            "heading": "5.2 Train-Test overlap analysis",
            "text": "As observed in Table 4, PersD-variants enjoy higher average improvements over their InpD counterparts, on HumanEvan than on MBPP. To delve deeper, we conduct a data overlap analysis. For each test task, we extract the most similar training task and use GPT-3.5-turbo to score their semantic similarity, with 0 indicating no relation and 1\nindicating complete semantic overlap (further details in Appendix D). Table 5 reveals more overlap in MBPP than HumanEval, and more overlap for StanD compared to PERsD. This overlap could be why StanD surpasses PERsD in the 1/16 setting (CodeGen-mono-16B, MBPP, pass@1), as StanD has an unfair advantage of having significantly more data leakage. In addition, if we test our methods on clean-MBPP where the leaked data points are removed, then PERsD becomes almost on-par with StanD in this specific setting while having larger margin over StanD on the rest 15/16 settings (from 4.8 points average margin to 5.9 points, more details at Appendix E). Altogether, this overlap analysis, coupled with results from cleaned MBPP, further underscores the advantages of personalized distillation."
        },
        {
            "heading": "5.3 Effect of mixing StanD and InpD data",
            "text": "Table 6 shows the ablation study on mixing standard distillation data to PERsD-refine and InpDrefine: while mixing standard data to InpD-refine improves its 1-step performance on MBPP and\nroughly maintains its performance on other settings, mixing StanD data to PERsD-refine significantly deteriorate its performance (except pass@1 inf-step=2 on HumanEval). We conjecture that as StanD has much larger data volume than PERsDrefine, it overwhelms the student training on standard distillation. However, combining with a balanced input-personalised data can be beneficial, as we observe from the good performance of PERsDcombined in Table 4 on CodeGen-mono-16B.\nSimilarly, in Table 7 we show another ablation: that mixing InpD data with PERsD roughly maintains the performance on HumanEval but degrades on MBPP. This shows personalised labels are of higher quality and mixing non personalised labels for the same task generally hurts performance."
        },
        {
            "heading": "5.4 Multi-round Distillation",
            "text": "After finetuning the student model with the personalised distillation data, can we perform another round of personalised distillation, on the new model? We show such an ablation study in Table 8. Encouragingly, we find PERsD-combined round-2 generally outperforms PERsD-combined round-1 by a modest margin. This improvement provides further evidence of the benefits of personalized learning, even when applied to models trained with personalized distillation. These findings suggest the intriguing possibility of an online or active version of personalized distillation, where data collection and model training occur simultaneously to ensure each batch is fully personalized and has higher sample efficiency. However, we will leave such intriguing exploration for future work."
        },
        {
            "heading": "5.5 Utilizing feedback for multi-step Inference",
            "text": "To better understand the role of execution feedback during training and multi-step inference, we show an ablation study in Table 9, where we compare PERsD-combine with a specific variant (PERsD-combine*) that excludes feedback during both training and inference. we observed that PERsD-combine* performs comparably to PERsDcombine on HumanEval and slightly better on MBPP for 1-step inference. However, for 2-step inference, PERsD-combine* consistently underper-\nforms PERsD-combine. This result aligns well with our expectations that code-rectification needs the execution feedback to guide the refinement."
        },
        {
            "heading": "5.6 Cross-Model Personalised Distillation",
            "text": "To investigate whether personalised distillation data of one model can be benefical to another, we conduct an ablation in Table 10 by using PERsD-combined data of CodeGen-mono-6B to train CodeGen-mono-16B. The results show that such cross-model persionalised data do not perform as well as real personalised data: leading to a consistent performance drop by a large margin. This finding reinforces our notion that learning data should be tailored to the specific student model, as personalized data suitable for one model may not necessarily benefit others."
        },
        {
            "heading": "5.7 Comparison with other Feedback-based Code Generation Models",
            "text": "Comparison with ILF (Chen et al., 2023a): In order to compare with ILF, one of our closest related work, we experiment on a separate setting:\nstarting with full MBPP dataset (974 tasks) and use Task-Ids 11-111 as test split and remaining 863 as training data. On the training set, our student model CodeGen-6B (same as ILF) generated wrong attempts on 562 tasks, which were shown to ChatGPT along with the task instruction and execution error feedback to eventually collect 288 valid personalized code rectification labels.\nThe original MBPP text-to-code data and this collected personalized code-refinement data for the 288 tasks\nMBPP Test Set\nMethod Cost Pass@1 Pass@10 ILF >4K$ 36 68 PERSD 0.65$ 46.8 67.4 -refine 0.65$ 41.8 66.8 -combined 0.65$ 47.8 64.8\nTable 11: Comparison with ILF\nrespectively form the finetuning data Dcode and Drefine on which we train models PERSD and PERSD-refine. We further combine Dcode and Drefine to train PERSD-\ncombined. Our experimental results in Table 11 show that all PERSD-variants significantly outperform ILF by 11.8% at pass@1 at a cost 1e-4 times lower than ILF, thus showcasing the lack of scalability of ILF-style models.\nComparison with Self-Edit: Since Self-Edit (Zhang et al., 2023) uses a trainable CodeGen350M code editor model and a frozen codegeneration model, our experimental setup is not directly comparable with theirs. However, our INPD-refine and INPD-combined models can actually be considered as very close counterparts to a version of Self-Edit with shared a code-generation and code-refinement model and CodeGen-6B backbone. The consistent performance improvement of the personalized distillation models over the inputdistilled ones across the board, alludes towards the prospect that PERSD-models are indeed more effective than Self-Edit style models."
        },
        {
            "heading": "5.8 Comparison with SOTA Models",
            "text": "Fianlly, we compare PERsD-combine models with open-source and close-sourced state-of-theart models on HumanEval in Table 12.We find that PERsD-combine methods can significantly improve the backbone model, with a performance gain of 6.2 points for CodeGen-mono 6B (8.4% error reduction), 5.9 points for CodeGen-mono 16B (8.3% error reduction) and 12.2 points for StarCoder (18.4% error reduction). Moreover, StarCoder with PERsD-combined, outperforms other open-sourced models except WizardCoder. Note\nthat our model ues 5K data examples while WizardCoder uses 78K. As mentioned in \u00a72.1, WizardCoder is an orthogonal approach that can be integrated into personalised distillation."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduced personalized distillation as a method for collecting customized labeled data that adapts to the capacity of student models, resulting in more effective learning. We demonstrated the advantages of personalized distillation over standard distillation in the field of code generation, achieving superior performance on both the HumanEval and MBPP datasets. Through comprehensive ablation studies, we confirmed that personalized distillation leads to higher data quality, benefits from multi-round distillation, and enables models to leverage execution feedback for selfrectification. We believe personalized distillation represents an exciting step towards better distillation of closed-source LLMs to open-source models.\nLimitations\nIn this section, we discuss some limitations of this paper and future directions to make it more valuable:\nOn Data Scale For a fair comparison, we have conducted all experiments based on the same 10K DSTAND data (introduced \u00a74.2) and the corresponding personalised data processed from DSTAND are of size 2-3K as shown in Table 3. However, as we have proven personalised distillation supports\nmore effective and efficient learning, it is intriguing to investigate how well does personalised distillation scale with the data size. For example, if we scale personalised distillation data to 50K, how much more performance gain will PERsD methods receive compared to InpD and StanD with the scaling of data size.\nOnline Personalised Distillation As discussed in \u00a75.4, conducting a second round personalised distillation continues to improve a student model that is already trained with PERsD-combine. Such observation suggests the potential of an online version of personalised distillation, which collects a batch of personalised data on-the-fly with the teacher model, after each optimization step during finetuning. As we have proven that true personalised data is more beneficial than standard data or cross-model personalised data (\u00a75.6), such online personalised distillation will in-principle maximally benefit from personalised distillation, as each batch of training data is fully tailored to the student model."
        },
        {
            "heading": "A Details in Multi-step Model Evaluation",
            "text": "As the docstrings are ill-formated in HumanEval, we write a simple rule-based parsing code snippet to extract its seen unit test cases. On average per task, there is 2 seen unit test cases and 4.2 unseen unit test cases. The overlap between seen and unseen tests is 11.33%. For MBPP, since conventionally the instruction prompt is constructed by taking the task description and example usages (from the unit test cases) as part of the doc-string, we consider all the unit test cases to be \"seen\" and use all of them for multi-step inference."
        },
        {
            "heading": "B ChatGPT Prompt Template for Personalised Distillation",
            "text": "In Figure 3, we show the prompt template we use to query ChatGPT for personalised refinement. For each task example, with task instruction t, unit test cases u and correct code c, we query ChatGPT API with two turn conversation history.\nFor the first turn, we use the template in Figure 3a and replace \u00abTASK\u00bb, \u00abHEADER\u00bb with the actual task instruction t and function header extracted. This is added to first turn\u2019s user input and the correct code c is included as first turn\u2019s assistant output. For the second turn, we use the template in Figure 3b and replace \u00abCODE\u00bb, \u00abERROR\u00bb with the student model\u2019s attempt and its execution feedback. This is added to second turn\u2019s user input and we query ChatGPT with the constructed converstaion history to get second turn\u2019s assistant output as personalised code refinement."
        },
        {
            "heading": "C Prompt Template for Code Refinement Finetuning",
            "text": "Figure 4 shows the refinement template Trefine introduced in \u00a73.2), which is used to construct input prompt for code refinement finetuning. we replace \u00abTASK\u00bb with task instruction, \u00abCODE\u00bb with the initial wrong attempt from student, \u00abERROR\u00bb with the execution feedback, and \u00abHEADER\u00bb with function header extracted from task instruciton."
        },
        {
            "heading": "D Details in Data Overlap Analysis",
            "text": "This section describes the detailed procedures to conduct train-test data overlap analysis. The objective is to assess the extent of data leakage in the test datasets originating from our self-constructed pretraining corpus.\nFirstly, we have performed exact string match and found no data leakage in any test data (HumanEval/MBPP).\nTo measure the semantic similarity between training/test tasks, we did the following:\n1. For each task in the test (MBPP/HumanEval) we retrieve two closest training tasks (based on cosine similarity of starcoder embedding & tf-idf vectors of task description).\n2. We use gpt-3.5-turbo-16k to identify whether there is a data leak between a train and test instance by classifying the pair into (\u201cleak\u201d, \u201csomewhat similar\u201d, \u201csomewhat not similar\u201d, \u201cnot related\u201d). We use a prompt with instructions and manually created few-shot examples and ask gpt-3.5 to generate the reasoning and categorization. We manually examined several examples per category to ensure the reasoning and judgment is done correctly and consistently.\n3. Map the similarity categories to 0-1 similarityscore (\u201cleak\u201d -> 1, \u201csomewhat similar\u201d -> 0.75, \u201csomewhat not similar\u201d -> 0.25, \u201cnot related\u201d -> 0) and show the mean score and % of cases classified as \u201cleak\u201d. Note that StanD & PERsD have 10K & 3K training data respectively so their scores are different."
        },
        {
            "heading": "E Results in MBPP-Cleaned",
            "text": "In Appendix D, we find 55 data instances that are potentially leaked (with similarity score = 1) in MBPP test data. In this section, we construct a new MBPP-Cleaned dataset, where the leaked data points are removed (originally 306 problems \u2192 251 problems after filtering). The results on this new MBPP-Cleaned dataset is shown in Table 13. From\nthe results, we can see for setting CodeGen-mono16B, pass@1, PERsD becomes almost on-par with StanD (from a gap of -1.21 to -0.17). For the rest of 15/16 settings on PERsD comparing with StanD, its average margin is increased from 4.8 points to 5.9 points. Besides, PERsD-refine on MBPP-Cleaned shows more consistent and sizable improvements over InpD-refine, with an average edge of +0.86 for 1 step inference, and +1.91 for two step inference. Overall, with overlapped test data removed, PERsD methods show even larger advantages compared to StanD or InpD methods."
        }
    ],
    "title": "Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
    "year": 2023
}