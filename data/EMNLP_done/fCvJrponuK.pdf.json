{
    "abstractText": "Deep neural networks have been widely applied in real-world scenarios, such as product restrictions on e-commerce and hate speech monitoring on social media, to ensure secure governance of various platforms. However, illegal merchants often deceive the detection models by adding large-scale perturbations to prohibited products, so as to earn illegal profits. Current adversarial attacks using imperceptible perturbations encounter challenges in simulating such adversarial behavior and evaluating the vulnerabilities of detection models to such perturbations. To address this issue, we propose a novel black-box multimodal attack, termed Sparse Multimodal Attack (SparseMA), which leverages sparse perturbations to simulate the adversarial behavior exhibited by illegal merchants in the black-box scenario. Moreover, SparseMA bridges the gap between images and texts by treating the separated image patches and text words uniformly in the discrete space. Extensive experiments demonstrate that SparseMA can identify the vulnerability of the model to different modalities, outperforming existing multimodal attacks and unimodal attacks. SparseMA, which is the first proposed method for black-box multimodal attacks to our knowledge, would be used as an effective tool for evaluating the robustness of multimodal models to different modalities. Code is available at https://github.com/JHL-HUST/SparseMA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhen Yu"
        },
        {
            "affiliations": [],
            "name": "Zhou Qin"
        },
        {
            "affiliations": [],
            "name": "Zhenhua Chen"
        },
        {
            "affiliations": [],
            "name": "Meihui Lian"
        },
        {
            "affiliations": [],
            "name": "Haojun Fu"
        },
        {
            "affiliations": [],
            "name": "Weigao Wen"
        },
        {
            "affiliations": [],
            "name": "Hui Xue"
        },
        {
            "affiliations": [],
            "name": "Kun He"
        }
    ],
    "id": "SP:cb8312e1c4966ee4382a8de376e91eb485490a2d",
    "references": [
        {
            "authors": [
                "Firoj Alam",
                "Ferda Ofli",
                "Muhammad Imran."
            ],
            "title": "Crisismmd: Multimodal twitter datasets from natural disasters",
            "venue": "AAAI conference on web and social media.",
            "year": 2018
        },
        {
            "authors": [
                "Moustafa Alzantot",
                "Yash Sharma",
                "Ahmed Elgohary",
                "Bo-Jhang Ho",
                "Mani Srivastava",
                "Kai-Wei Chang."
            ],
            "title": "Generating natural language adversarial examples",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2018
        },
        {
            "authors": [
                "Francesco Croce",
                "Maksym Andriushchenko",
                "Naman D Singh",
                "Nicolas Flammarion",
                "Matthias Hein."
            ],
            "title": "Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein."
            ],
            "title": "Sparse and imperceivable adversarial attacks",
            "venue": "IEEE/CVF International Conference on Computer Vision.",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Fei-Fei Li."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "Conference on Computer Vision and Pattern Recognition.",
            "year": 2009
        },
        {
            "authors": [
                "Yinpeng Dong",
                "Fangzhou Liao",
                "Tianyu Pang",
                "Hang Su",
                "Jun Zhu",
                "Xiaolin Hu",
                "Jianguo Li."
            ],
            "title": "Boosting adversarial attacks with momentum",
            "venue": "Conference on Computer Vision and Pattern Recognition.",
            "year": 2018
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou."
            ],
            "title": "HotFlip: White-box adversarial examples for text classification",
            "venue": "Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Ivan Evtimov",
                "Russel Howes",
                "Brian Dolhansky",
                "Hamed Firooz",
                "Cristian Canton Ferrer."
            ],
            "title": "Adversarial evaluation of multimodal models under realistic gray box assumption",
            "venue": "ArXiv preprint.",
            "year": 2020
        },
        {
            "authors": [
                "Siddhant Garg",
                "Goutham Ramakrishnan."
            ],
            "title": "BAE: BERT-based adversarial examples for text classification",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2020
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy."
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "International Conference on Learning Representations.",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Conference on Computer Vision and Pattern Recognition.",
            "year": 2016
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is BERT really robust? A strong baseline for natural language attack on text classification and entailment",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Douwe Kiela",
                "Hamed Firooz",
                "Aravind Mohan",
                "Vedanuj Goswami",
                "Amanpreet Singh",
                "Pratik Ringshia",
                "Davide Testuggine."
            ],
            "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "venue": "Advances in Neural Information Processing Sys-",
            "year": 2020
        },
        {
            "authors": [
                "Dianqi Li",
                "Yizhe Zhang",
                "Hao Peng",
                "Liqun Chen",
                "Chris Brockett",
                "Ming-Ting Sun",
                "Bill Dolan."
            ],
            "title": "Contextualized perturbation for textual adversarial attack",
            "venue": "North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "ArXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi."
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in Neural Information Processing Sys-",
            "year": 2021
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2020
        },
        {
            "authors": [
                "Bin Liang",
                "Hongcheng Li",
                "Miaoqiang Su",
                "Pan Bian",
                "Xirong Li",
                "Wenchang Shi."
            ],
            "title": "Deep text classification can be fooled",
            "venue": "International Joint Conference on Artificial Intelligence.",
            "year": 2018
        },
        {
            "authors": [
                "Jiadong Lin",
                "Chuanbiao Song",
                "Kun He",
                "Liwei Wang",
                "John E. Hopcroft."
            ],
            "title": "Nesterov accelerated gradient and scale invariance for adversarial attacks",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu."
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Rishabh Maheshwary",
                "Saket Maheshwary",
                "Vikram Pudi."
            ],
            "title": "Generating natural language attacks in a hard label black box setting",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Blaise Thomson",
                "Milica Ga\u0161i\u0107",
                "Lina M. Rojas-Barahona",
                "Pei-Hao Su",
                "David Vandyke",
                "Tsung-Hsien Wen",
                "Steve Young."
            ],
            "title": "Counter-fitting word vectors to linguistic constraints",
            "venue": "North American Chapter of the",
            "year": 2016
        },
        {
            "authors": [
                "Nina Narodytska",
                "Shiva Prasad Kasiviswanathan."
            ],
            "title": "Simple black-box adversarial attacks on deep neural networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops.",
            "year": 2017
        },
        {
            "authors": [
                "Teng Niu",
                "Shiai Zhu",
                "Lei Pang",
                "Abdulmotaleb El Saddik."
            ],
            "title": "Sentiment analysis on multi-view social data",
            "venue": "International Conference on Multimedia Modeling.",
            "year": 2016
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Somesh Jha",
                "Matt Fredrikson",
                "Z Berkay Celik",
                "Ananthram Swami."
            ],
            "title": "The limitations of deep learning in adversarial settings",
            "venue": "IEEE European Symposium on Security and Privacy (EuroS&P).",
            "year": 2016
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ananthram Swami",
                "Richard Harang."
            ],
            "title": "Crafting adversarial input sequences for recurrent neural networks",
            "venue": "MILCOM IEEE Military Communications Conference.",
            "year": 2016
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus."
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2014
        },
        {
            "authors": [
                "Yapeng Tian",
                "Chenliang Xu"
            ],
            "title": "Can audio-visual integration strengthen robustness under multimodal attacks",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Nishant Vishwamitra",
                "Hongxin Hu",
                "Ziming Zhao",
                "Long Cheng",
                "Feng Luo."
            ],
            "title": "Understanding and measuring robustness of multimodal learning",
            "venue": "ArXiv preprint.",
            "year": 2021
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli."
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing.",
            "year": 2004
        },
        {
            "authors": [
                "Karren Yang",
                "Wan-Yi Lin",
                "Manash Barman",
                "Filipe Condessa",
                "Zico Kolter."
            ],
            "title": "Defending multimodal fusion models against single-source adversaries",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
            "year": 2021
        },
        {
            "authors": [
                "Youngjoon Yu",
                "Hong Joo Lee",
                "Byeong Cheon Kim",
                "Jung Uk Kim",
                "Yong Man Ro."
            ],
            "title": "Investigating vulnerability to adversarial examples on multimodal data fusion in deep learning",
            "venue": "ArXiv preprint.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Yu",
                "Xiaosen Wang",
                "Wanxiang Che",
                "Kun He."
            ],
            "title": "Texthacker: Learning based hybrid local search algorithm for text hard-label adversarial attack",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 622\u2013637.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Zahavy",
                "Abhinandan Krishnan",
                "Alessandro Magnani",
                "Shie Mannor."
            ],
            "title": "Is a picture worth a thousand words? A deep multi-modal architecture for product classification in e-commerce",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2018
        },
        {
            "authors": [
                "Yuan Zang",
                "Fanchao Qi",
                "Chenghao Yang",
                "Zhiyuan Liu",
                "Meng Zhang",
                "Qun Liu",
                "Maosong Sun."
            ],
            "title": "Word-level textual adversarial attacking as combinatorial optimization",
            "venue": "Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Jiaming Zhang",
                "Qi Yi",
                "Jitao Sang."
            ],
            "title": "Towards adversarial attack on vision-language pre-training models",
            "venue": "ACM International Conference on Multimedia.",
            "year": 2022
        },
        {
            "authors": [
                "Pu Zhao",
                "Sijia Liu",
                "Pin-Yu Chen",
                "Nghia Hoang",
                "Kaidi Xu",
                "Bhavya Kailkhura",
                "Xue Lin."
            ],
            "title": "On the design of black-box adversarial examples by leveraging gradient-free optimization and operator splitting method",
            "venue": "IEEE/CVF International Conference on",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the rapid development of Deep Neural Networks (DNNs), vision-language multimodal classification has been applied in various real-world applications, such as product restrictions on ecommerce (Zahavy et al., 2018) and hate speech monitoring on social media (Kiela et al., 2020), to ensure secure governance of various platforms.\n\u2217The first three authors contributed equally. \u2020Corresponding author.\nHowever, there are always some illegal merchants who attempt to sell prohibited goods to make illegal profits, such as illegal drugs or smuggled goods, which violate the rules and terms of platforms. Due to the lack of professional knowledge in adversarial attacks (Szegedy et al., 2014; Papernot et al., 2016b), they often adopt large-scale and sparse perturbations, including smearing and mosaicing, to deceive detection models deployed by various companies, thereby posing a significant threat to the security of platforms, as shown in Figure 1.\nCurrent adversarial attacks (Goodfellow et al., 2015; Papernot et al., 2016b) typically mislead the victim model by adding imperceptible perturbations to benign samples, which can not accurately evaluate the vulnerability of detection models to the adversarial behavior exhibited by illegal merchants. In addition, prior research has mainly focused on unimodal adversarial attacks in Computer Vision (CV) (Goodfellow et al., 2015; Madry et al., 2018) and Natural Language Processing (NLP) (Papernot et al., 2016b; Liang et al., 2018), with little attention paid to the vulnerability of multimodal models (Yu et al., 2020; Yang et al., 2021) that are more challenging but widely used in the real world. This motivates us to develop a multimodal adversarial attack that simulates physical adversarial behaviors\nin the black-box scenario, which can help ensure secure governance of various platforms.\nExisting multimodal attacks (Evtimov et al., 2020; Zhang et al., 2022) typically process images and texts independently due to the vastly different data attributes of the two modalities, such as continuous images vs. discrete texts. When employing different attack strategies to perturb images and texts, these methods are unable to effectively combine features from different input modalities, leading to relatively low attack performance. And these attacks run in white-box attack settings, which are almost impossible to apply to the real-world.\nTo address the above issue and simulate the sparse perturbations commonly used by illegal merchants, we propose to map them into the same discrete space to bridge the gap between continuous images and discrete texts. And then we perturb both images and texts simultaneously using the same sparse attack strategy. Although these sparse perturbations are indeed visible, they generally do not alter the semantics.\nIn this work, we propose a novel black-box multimodal attack, named Sparse Multimodal Attack (SparseMA). SparseMA splits the input image into multiple patches, then evaluates the impact of each patch in the image and each word in the input text on the victim model based on the output logits. After sorting all patches and words according to their impacts, we sequentially replace the original data with suitable candidates until an adversarial example is found. Note that SparseMA only needs access to the model output, making it more feasible in the real-world than white-box attacks, which require full access to the model, including architecture, parameter, gradient, output, etc.\nTo validate the effectiveness of the proposed SparseMA, we do comparison with one image attack, two text attacks, and two multimodal attacks on the task of vision-language multimodal classification. Extensive experiments demonstrate that SparseMA could identify the vulnerability of the model to different modalities, achieving a higher attack success rate than almost all the baselines. Moreover, SparseMA could generate better sparse adversarial examples with fewer perturbations, making them more similar to benign samples and more applicable to real-world scenarios. Through analyzing the impact of perturbations on the model\u2019s output in each modality, SparseMA can reveal the relative importance of each modal-\nity in the model\u2019s decision-making process. This information is valuable for researchers to better understand the multimodal model\u2019s behavior and enhance its robustness against adversarial attacks."
        },
        {
            "heading": "2 Related Work",
            "text": "This section provides a brief overview of unimodal adversarial attacks on images or texts, as well as multimodal adversarial attacks."
        },
        {
            "heading": "2.1 Unimodal Adversarial Attack",
            "text": "Image adversarial attacks Szegedy et al. (2014) first show the existence of adversarial examples. Subsequently, numerous works based on the l\u221e or l2 norm, including FGSM (Goodfellow et al., 2015), PGD (Madry et al., 2018), MIM (Dong et al., 2018), NIM (Lin et al., 2020), etc., are proposed to enhance the attack performance. JSMA (Papernot et al., 2016a) is the first to generate sparse perturbations by minimizing the l0 norm. Croce and Hein (2019) introduce the l0 norm variant of PGD, known as PGD0, and a black-box sparse attack of CornerSearch that evaluates the saliency of each pixel based on the changes on logits. LocSearchAdv (Narodytska and Kasiviswanathan, 2017), ADMM (Zhao et al., 2019), and SparseRS (Croce et al., 2022) adopt local search, Bayes optimization, and random search algorithms, respectively, to search for the optimal sparse adversarial example in the black-box setting.\nDespite the prosperity of sparse black-box attacks, they generally search for the optimal solution or evaluate the importance for each pixel, which require more than 106 queries on images of size (224, 224). Even if ADMM and Sparse-RS optimize the query efficiency, tens of thousands of queries are still required, making it challenging to scale to large-scale datasets, such as ImageNet (Deng et al., 2009). In this work, SparseMA splits the image into multiple patches, and does not need to process each pixel. It allows us to achieve good attack performance with only a few hundred queries.\nText adversarial attacks Existing text adversarial attacks typically modify the character, word, or sentence of the benign text to maximize the loss on the ground-truth class, among which wordlevel attacks (Li et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2021a; Maheshwary et al., 2021; Yu et al., 2022) show excellent performance. PWWS (Ren et al., 2019) and TextFooler (Jin et al., 2020) greedily substitute important words with syn-\nonyms based on the output logits. GA (Alzantot et al., 2018) and PSO (Zang et al., 2020) adopt evolutionary algorithms to search for a near-optimal text adversarial example."
        },
        {
            "heading": "2.2 Multimodal Adversarial Attack",
            "text": "To the best of our knowledge, there are a few works focusing on the vulnerability of multimodal models. Yu et al. (2020), Tian and Xu (2021) and Yang et al. (2021) find that samples could be misclassified by perturbing only single modality using PGD, revealing the vulnerability of multimodal models. Evtimov et al. (2020) perturb images with PGD and texts with HotFlip (Ebrahimi et al., 2018) to generate hateful posts that could fool multimodal models. MUROAN (Vishwamitra et al., 2021) utilizes the fusion features to decouple the input modalities and generate adversarial examples by removing salient data points. Co-Attack (Zhang et al., 2022) first perturbs the text and then perturbs the image according to the perturbed fusion features to conduct a collaborative attack on the pre-trained multimodal model to fool all downstream tasks.\nIn this work, by splitting the input image and text into multiple sparse components, SparseMA establishes a more fine-grained connection between each element rather than between the entire image and text, allowing us to generate more well-designed multimodal perturbations efficiently. To the best of our knowledge, SparseMA is the first proposed method for black-box multimodal attacks."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first formalize the problem of multimodal adversarial attack and then provide a detailed description of our method."
        },
        {
            "heading": "3.1 Multimodal Adversarial Attack",
            "text": "Given an input space X containing all input images and texts, and an output space Y = {y1, . . . , yk}, we have a pre-trained multimodal classifier f : X \u2192 Y , which maps the input image xI and text xT = w1w2 . . . wn to its ground-truth label y \u2208 Y . The adversary adds an imperceptible perturbation on the classified sample x = (xI , xT ) to craft an adversarial example (x\u2217I , x \u2217 T ) that misleads the classifier f to output a wrong prediction:\nargmax yi\u2208Y\nf(yi|x\u2217I , x\u2217T ) \u0338= y.\nMeanwhile, we should also guarantee the imperceptibility of adversarial perturbations. Therefore, we propose evaluating the quality of generated adversarial examples using multimodal similarity. To achieve this, we input both images and texts into pre-trained BLIP-2 (Li et al., 2023) and extract the vector of the last hidden layer as the multimodal feature vector of the sample. Then, we calculate the cosine similarity between these two vectors as the multimodal similarity. This approach provides a more comprehensive evaluation of the generated adversarial examples, taking into account both visual and textual aspects of the data.\nIn this work, we propose a novel sparse black-box multimodal attack approach, named SparseMA, in which only the output score of the target model is needed."
        },
        {
            "heading": "3.2 The Proposed SparseMA Method",
            "text": "Given a benign sample that contains image xI and text xT , SparseMA first splits image xI into multiple patches and text xT into multiple words. As illustrated in Figure 2, SparseMA consists of two main stages: i.e., replacement order determination and candidate substitution. Replacement order determination determines the importance and replacement order of each patch and word. Candidate substitution sequentially substitutes each patch or word with a suitable candidate and then optimizes the redundant substitutions by random recovery to generate an optimal adversary."
        },
        {
            "heading": "3.2.1 Replacement Order Determination",
            "text": "In order to map the image and text into a discrete space, we split the image xI into m patches {p1, p2, . . . , pm} of size (s, s) and the text xT into n words {w1, w2, . . . , wn}. Intuitively, more important components with greater impact on the victim model should be replaced. To evaluate the importance of each patch pi or word wj , we set the patch pi to be all 0s, or set the word wj to be unknown, i.e., [UNK], to obtain a modified sample x\u0302 = (x\u0302I , xT ) or (xI , x\u0302T ). We feed it into the target model, and calculate the importance score of each patch pi by examining the change of the model output:\nI(pi) = f(y|xI , xT )\u2212 f(y|x\u0302I , xT )\nd(x\u0302I , xI) . (1)\nSimilarly, we could obtain the importance score I(wj) of each word wj . I(\u00b7) measures the impact of the unit perturbation on the victim model by taking the perturbation size d(\u00b7, \u00b7) into account. It allows us to balance the choice of perturbation between image patches and text words to achieve maximum impact on the model with minimal disruption. In the end, we sort all patches and words in descending order based on the importance score as the replacement order."
        },
        {
            "heading": "3.2.2 Candidate Substitution",
            "text": "To find the suitable candidate for substitution, we pre-define candidate sets for each patch and word to ensure that the replacement operation has little impact on the semantics and similarity of samples.\nAlgorithm 1: The SparseMA Algorithm Input: Input sample that contains the image\nxI and the text xT with lable y; Target classifier f ; Number of iterations N for random recovery\nOutput: Adversarial example /* Replacement Order Determination */\n1 Split the image into m patches 2 for each patch in xI and each word in xT\ndo 3 Compute the importance score via Eq. 1\n4 Sort all patches and words based on the importance score in descending order\n/* Candidate Substitution */\n5 Construct the candidate set for each patch and word 6 for each element in replacement order do 7 Replace it on the sample (xI , xT ) with\nits optimal candidate via Eq. 2 to craft a new sample (xI , xT )\n8 if argmaxyi\u2208Y f(yi|xI , xT ) \u0338= y then 9 break\n10 if argmaxyi\u2208Y f(yi|xI , xT ) = y then 11 return None ; // Attack fails\n12 for t = 1 to N do 13 Replace a randomly selected perturbed\nelement on (xI , xT ) with the original data to craft a new sample (x\u0302I , x\u0302T )\n14 if argmaxyi\u2208Y f(yi|x\u0302I , x\u0302T ) \u0338= y then 15 Update (xI , xT )\u2190 (x\u0302I , x\u0302T )\n16 return (xI , xT ) ; // Attack succeeds\nFor each image patch, we select all-black, all-white, and crossed black-and-white patches as its candidate set to simulate the perturbation behavior that illegal merchants are most likely to adopt in reality. For each word, we select its top s nearest synonyms in the counter-fitted embedding space (Mrk\u0161ic\u0301 et al., 2016) as its candidate set.\nFor the patch pi or word wj to be replaced on sample xt = (xtI , x t T ) at the t-th iteration along the obtained replacement order, we greedily substitute it with the optimal candidate that has the greatest impact \u2206P on the model from its candidate sets:\n\u2206P = f(y|xtI , xtT )\u2212 f(y|xt+1I , x t+1 T ), (2)\nwhere the sample xt+1 = (xt+1I , x t+1 T ) is obtained by replacing the patch pi or word wj with a candi-\ndate on sample xt. Then, we sequentially substitute each patch or word with its optimal candidate according to the replacement order until we find an adversarial example successfully.\nSince some perturbed elements may have little influence on the initial adversarial examples, i.e., there are redundant perturbations, greedy substitution would result in suboptimal adversarial examples. To further reduce the perturbation while keeping adversarial, we randomly change the perturbed patches or words back to the original data. We continue this recovery operation on the resulting sample if it is still adversarial to find an optimal adversarial example. The overall algorithm of SparseMA is summarized in Algorithm 1."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we conduct extensive experiments to validate the effectiveness of SparseMA on three datasets and three models that are widely used."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets We adopt three widely investigated datasets for vision-language multimodal classification, including MVSA-Single (Niu et al., 2016), MVSA-Multi (Niu et al., 2016) and CrisisMMD (Alam et al., 2018). MVSA-Single and MVSA-Multi are two sentiment classification datasets that contain three sentiments (positive, negative and neutral). CrisisMMD is a multimodal dataset consisting of tweets and associated images collected during seven disaster events.\nVictim Models We consider ALBEF (Li et al., 2021b) and CLIP (Radford et al., 2021) as the victim models in this work. As CLIP is an aligned model, we construct a 2-layer multilayer perceptron for predicting the target label. Additionally, we consider two different image encoders for CLIP: ViT-B/16 (Dosovitskiy et al., 2021) and ResNet-50 (He et al., 2016), denoted as CLIPViT and CLIPRes, respectively. More details of these datasets and the classification accuracy of each model are shown in Table 1.\nBaselines Since there are only a few multimodal attacks proposed recently, we select representative works from unimodal and multimodal black-box attacks as our baselines. For image adversarial attacks, we choose the state-of-the-art sparse black-box attack Sparse-RS (Croce et al., 2022). For text adversarial attacks, we choose the word-level black-box attacks PWWS (Ren et al., 2019) and PSO (Zang et al., 2020). For multimodal attacks, we combine Sparse-RS and PWWS to attack images and texts respectively to perform the multimodal attack, named Sp-RS&PWWS.\nExperimental Settings The side length s of image patches is set to 20, and the number of iterations N for random recovery is set to four times the number of perturbed elements in the initial adversarial example. To ensure high semantic similarity between adversarial texts and benign texts for SparseMA and all text baselines, we also set the number of synonyms to 4. All evaluations are conducted on 1,000 randomly sampled texts from the corresponding test set. For a fair comparison with our SparseMA, we increase the maximum number of pixels perturbed to 3,600 and decrease the number of iterations accordingly to 2,000 for Sparse-RS."
        },
        {
            "heading": "4.2 Evaluation on Attack Effectiveness",
            "text": "To validate the effectiveness of SparseMA, we conduct experiments for vision-language classification on three victim models using three datasets. The results, including attack success rate, multimodal similarity and the number of queries, are summarized in Table 2.\nWe could observe that SparseMA achieves the highest attack success rate on 7 of 9 cases compared to all unimodal and multimodal baselines and is only slightly weaker than Sp-RS&PWWS in the other two cases. And we always generate adversarial examples that are more similar to the original samples, compared to Sp-RS&PWWS, indicating that we generate better adversarial examples. Meanwhile, the number of queries of SparseMA\nis lower than that of all baselines except PWWS, being about 1/3 that of the multimodal attack SpRS&PWWS. Despite the lower query number and higher similarity of PWWS, our attack success rate is significantly higher than PWWS. Therefore, it cannot be concluded that PWWS implies a better effectiveness.\nIn conclusion, SparseMA achieves a higher attack success rate and similarity with a lower query number compared to the combined multimodal attack Sp-RS&PWWS, demonstrating the necessity of bridging the gap between different modalities for effective multimodal attack. These results validate the superiority of our proposed method. Moreover, we find that by perturbing only 2% to 3% of the top-level features, we can make the victim model output completely different predictions, thus confirming the vulnerability of the multimodal model. These discoveries suggest that the model relies heavily on these small subsets of high-level features to determine its output, but these features are highly susceptible to adversarial attacks. Therefore, improving the robustness of the model\u2019s high-level features against perturbations would be a crucial method to enhance the model\u2019s robustness.\nThen, we present two instances of adversarial examples in Figure 3. It can be seen that SparseMA adds patches to the image to generate adversarial examples, which have fewer perturbations and are easier to add for attackers in the real-world. Although the perturbations are indeed visible, they do not alter the semantics of adversarial images, which are still recognizable by humans. Additionally, SparseMA perturbs fewer words than Sp-RS&PWWS. These evaluations demonstrate the high quality and practicality in the physical world of the adversarial examples generated by SparseMA."
        },
        {
            "heading": "4.3 Vulnerability on Different Modalities",
            "text": "Multimodal classifier utilizes information from various modalities to predict the classification results, which is expected to be robust to all modalities and achieves better performance. However, we find that multimodal models tend to be robust to one modality while being vulnerable to another. When attacking the robust modality, the attack performance is usually poor. In contrast, attacking the vulnerable modality would result in good attack performance. For instance, PWWS for text\n(a) The sample with positive label perturbed by various attacks is misclassified as negative.\n(b) The sample with negative label perturbed by various attacks is misclassified as positive.\nBack aux(to) school stress: Here's how to cope. Tips from parenting expert @anndouglas. #HamOnt http://t.co/gq4iPoBVOv\nSp-RS&PWWS SparseMA\nBack aux(to) tuition(school) stress: Here's modes(how) aux(to) cope. Tips z(from) fatherhood(parenting) specialist(expert) @anndouglas. #HamOnt http://t.co/gq4iPoBVOv\nfewer perturbations\nfewer perturbations\neasier to add\nSp-RS&PWWS SparseMA\nfewer perturbations\nfewer perturbations\neasier to add\nHow does ec(this) happen? 3 cars, 3 different directions! 100Ave &amp; 116st #yeg @yegtraffic @1023nowradio @925FreshRadio How ai(does) ec(this) happen? 3 cars,"
        },
        {
            "heading": "3 diverse(different) directions! 100Ave &amp; 116st #yeg @yegtraffic @1023nowradio @925FreshRadio",
            "text": "attack shows poor attack performance when attacking CLIPViT model on MVSA-Multi dataset, as shown in Table 2. Conversely, Sparse-RS for image attack performs better. We believe this is due to the model over-relying on one modality while disregarding the other. To better evaluate the vulnerability or robustness of multimodal models to different modalities, we present the percentage of adversarial examples generated by SparseMA that perturb only on image or text, or both, in Table 3.\nWe can see that SparseMA prefers to perturb the more vulnerable modality. For example, SparseMA perturbs more images on the CLIPViT model using the MVSA-Multi dataset, where image attacks perform well. In addition, SparseMA tends to perturb both modalities simultaneously when the multimodal model is robust to both modalities, such as on the ALBEF model using the MVSA-Multi dataset, where both image attacks and text attacks perform poorly. This demonstrates that SparseMA has discovered the vulnerability of the model and generated corresponding perturbations for the attack. It also indicates that SparseMA may be a good metric to evaluate the vulnerability or robustness of the model to different modalities."
        },
        {
            "heading": "4.4 Evaluation in the Real-World",
            "text": "Adversarial images captured by digital devices are usually affected by physical factors, such as brightness, saturation, contrast or sharpness, etc. Thus, a good physical attack should generate perturbations that are easy to be added to images, and also be resistant to various physical factors. To evaluate the effectiveness of SparseMA in the real-world, we randomly adjust the brightness, contrast, saturation, and sharpness of the adversarial images generated by various attacks on CLIPViT model using MVSAMulti dataset to 0.5 to 1.5 times the original value. Then we evaluate their attack success rate as shown in Table 4. The results are averaged on five runs to eliminate randomness. We could observe that brightness typically has the largest effect, and sat-\nuration has the least effect on all attacks. Notably, SparseMA consistently achieves the highest attack success rate among all baselines, demonstrating the practicality of SparseMA in the real-world."
        },
        {
            "heading": "4.5 Parameter Study",
            "text": "To investigate the impact of the hyper-parameters in SparseMA, including the side length s of patches and the number of iterations N for random recovery, we conduct a series of experiments on CLIPViT model using MVSA-Single dataset. We additionally evaluate the similarity to the original samples using the Structural Similarity (SSIM) (Wang et al., 2004) for images and the Universal Sequence Encoder (USE) (Cer et al., 2018) for texts, denoted as SimI and SimT. And we adopt the perturbation rate to measure the percentage of pixels perturbed in an image or words perturbed in a text, denoted as PertI and PertT.\nOn the side length of patches The side length restricts the minimum perturbation unit of images. In Table 5, we evaluate the attack performance of SparseMA using various side lengths from 8 to 32 with an interval of 4. SparseMA performs well when s = 8, achieving a high attack success rate. It also has the lowest perturbation rate and the highest similarity, which prefers to perturb images individually. However, it has a high query cost for the victim model. As we increase the side length of the image patch, the attack success rate will change unpredictably. Also, increasing the patch size results in a decrease in the number of queries but an increase in the perturbation rate. Additionally, SparseMA gradually boosts its preference for text perturbations. To balance attack performance and query cost, we select an intermediate value of s = 20 for our experiments.\nOn the number of iterations for random recovery To perform random recovery in the initial\nadversarial example with different numbers of perturbed elements, we set the number of iterations N to be an integer time of the number of perturbed elements. We evaluate the final perturbation rate and multimodal similarity in final adversarial examples using various iterations from 2 to 8 times the number of perturbed elements with an interval of 2. We observe that they achieve similar perturbation rates (8.71% \u223c 8.78% in images and 6.73% \u223c 6.92% in texts) and similar multimodal similarity (96.91% \u223c 96.94%). More iterations will result in a lower perturbation rate but more queries, so we choose an intermediate value, i.e., N is four times the number of perturbed elements."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose a vision-language black-box adversarial attack method called Sparse Multimodal Attack (SparseMA). SparseMA maps the input image and text into discrete space by splitting the image into patches and the text into words. Then it evaluates the importance of each patch or word based on its affects to the model\u2019s output. By greedily substituting important patches or words with suitable candidates, SparseMA generates adversarial examples with fewer perturbations and higher quality, which achieves a higher attack success rate than the baselines. Additionally, SparseMA can reveal the vulnerability of multimodal models to different modalities and concentrate on perturbing the more vulnerable ones. It could be served as a good metric to measure the vulnerability of multimodal models to different modalities. And it can help us better understand the multimodal model\u2019s behavior, and enhance its robustness against adversarial attacks. Experiments show that the sparse perturbations generated by SparseMA are more practical in the physical world that are easier to be added\nfor attackers, and perform well against real-world influences. SparseMA would be a strong baseline for future works and may inspire more researches on multimodal attacks.\nLimitations\nSparseMA focuses on the two most typical modalities in the multimodal classification task, i.e., continuous images and discrete text. It does not take other types of modalities, such as audio and signal, into consideration. Actually, data from these modalities can also be processed by a similar sparse strategy, and then apply our method to generate adversarial examples. We will continue to investigate the potential of SparseMA in our future work."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported by National Natural Science Foundation (62076105, U22B2017)."
        }
    ],
    "title": "Sparse Black-Box Multimodal Attack for Vision-Language Adversary Generation",
    "year": 2023
}