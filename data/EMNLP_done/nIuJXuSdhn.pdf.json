{
    "abstractText": "Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Basel Mousi"
        },
        {
            "affiliations": [],
            "name": "Nadir Durrani"
        },
        {
            "affiliations": [],
            "name": "Fahim Dalvi"
        }
    ],
    "id": "SP:789e4937567ec5b5daa1c5b9ab8186d47f66f6da",
    "references": [
        {
            "authors": [
                "Frayling",
                "Firoj Alam"
            ],
            "title": "Benchmarking arabic ai with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Firoj Alam",
                "Fahim Dalvi",
                "Nadir Durrani",
                "Hassan Sajjad",
                "Khan",
                "Abdul Rafae",
                "Jia Xu."
            ],
            "title": "Conceptx: A framework for latent concept analysis",
            "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI, Poster presentation),",
            "year": 2023
        },
        {
            "authors": [
                "Omer Antverg",
                "Yonatan Belinkov."
            ],
            "title": "On the pitfalls of analyzing individual neurons in language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Bau",
                "Yonatan Belinkov",
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "James Glass."
            ],
            "title": "Identifying and controlling important neurons in neural machine translation",
            "venue": "Proceedings of the Seventh International Conference on Learning Representa-",
            "year": 2019
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James Glass"
            ],
            "title": "What do neural machine translation models learn about morphology",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James Glass."
            ],
            "title": "On the linguistic representational power of neural machine translation models",
            "venue": "Computational Linguistics, 45(1):1\u201357.",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Llu\u00eds M\u00e0rquez",
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "James Glass."
            ],
            "title": "Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks",
            "venue": "Proceedings of the Eighth International",
            "year": 2017
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "German Kruszewski",
                "Guillaume Lample",
                "Lo\u00efc Barrault",
                "Marco Baroni."
            ],
            "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
            "venue": "Proceedings of the 56th Annual Meeting of the As-",
            "year": 2018
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Nadir Durrani",
                "Hassan Sajjad."
            ],
            "title": "Neurox library for neuron analysis of deep nlp models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 75\u201383, Toronto, Canada.",
            "year": 2023
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Nadir Durrani",
                "Hassan Sajjad",
                "Yonatan Belinkov",
                "D. Anthony Bau",
                "James Glass."
            ],
            "title": "What is one grain of sand in the desert? analyzing individual neurons in deep nlp models",
            "venue": "Proceedings of the Thirty-Third AAAI Conference on Artificial",
            "year": 2019
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Maram Hasanain",
                "Sabri Boughorbel",
                "Basel Mousi",
                "Samir Abdaljalil",
                "Nizi Nazar",
                "Ahmed Abdelali",
                "Shammur Absar Chowdhury",
                "Hamdy Mubarak",
                "Ahmed Ali",
                "Majd Hawasly",
                "Nadir Durrani",
                "Firoj Alam"
            ],
            "title": "Llmebench: A flexible",
            "year": 2023
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Abdul Rafae Khan",
                "Firoj Alam",
                "Nadir Durrani",
                "Jia Xu",
                "Hassan Sajjad."
            ],
            "title": "Discovering latent concepts learned in BERT",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Bosheng Ding",
                "Chengwei Qin",
                "Linlin Liu",
                "Lidong Bing",
                "Shafiq Joty",
                "Boyang Li"
            ],
            "title": "Is gpt-3 a good data annotator",
            "year": 2022
        },
        {
            "authors": [
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "Yonatan Belinkov",
                "Preslav Nakov."
            ],
            "title": "One size does not fit all: Comparing NMT representations of different granularities",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Nadir Durrani",
                "Hassan Sajjad",
                "Fahim Dalvi",
                "Firoj Alam."
            ],
            "title": "On the transformation of latent space in fine-tuned nlp models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1495\u20131516, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Joseph L Fleiss",
                "Bruce Levin",
                "Myunghee Cho Paik"
            ],
            "title": "Statistical methods for rates and proportions",
            "year": 2013
        },
        {
            "authors": [
                "Yao Fu",
                "Mirella Lapata"
            ],
            "title": "Latent topology induction for understanding contextualized representations",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy."
            ],
            "title": "Transformer feed-forward layers are keyvalue memories",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli"
            ],
            "title": "Chatgpt outperforms crowd-workers for textannotation",
            "year": 2023
        },
        {
            "authors": [
                "K Chidananda Gowda",
                "G Krishna."
            ],
            "title": "Agglomerative clustering using the concept of mutual nearest neighbourhood",
            "venue": "Pattern recognition, 10(2):105\u2013112.",
            "year": 1978
        },
        {
            "authors": [
                "Biyang Guo",
                "Xin Zhang",
                "Ziyuan Wang",
                "Minqi Jiang",
                "Jinran Nie",
                "Yuxuan Ding",
                "Jianwei Yue",
                "Yupeng Wu"
            ],
            "title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "year": 2023
        },
        {
            "authors": [
                "Lucas Torroba Hennigen",
                "Adina Williams",
                "Ryan Cotterell."
            ],
            "title": "Intrinsic probing through dimension selection",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197\u2013216, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "John Hewitt",
                "Percy Liang."
            ],
            "title": "Designing and interpreting probes with control tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Katharina Jeblick",
                "Balthasar Schachtner",
                "Jakob Dexl",
                "Andreas Mittermeier",
                "Anna Theresa St\u00fcber",
                "Johanna Topalis",
                "Tobias Weber",
                "Philipp Wesp",
                "Bastian Sabel",
                "Jens Ricke",
                "Michael Ingrisch"
            ],
            "title": "Chatgpt makes medicine easy to swallow: An exploratory",
            "year": 2022
        },
        {
            "authors": [
                "Akos K\u00e1d\u00e1r",
                "Grzegorz Chrupa\u0142a",
                "Afra Alishahi."
            ],
            "title": "Representation of linguistic form and function in recurrent neural networks",
            "venue": "Computational Linguistics, 43(4):761\u2013780.",
            "year": 2017
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Justin Johnson",
                "Li Fei-Fei."
            ],
            "title": "Visualizing and understanding recurrent networks",
            "venue": "arXiv preprint arXiv:1506.02078.",
            "year": 2015
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Yair Lakretz",
                "German Kruszewski",
                "Theo Desbordes",
                "Dieuwke Hupkes",
                "Stanislas Dehaene",
                "Marco Baroni."
            ],
            "title": "The emergence of number and syntax units in LSTM language models",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "ALBERT: a lite BERT for selfsupervised learning of language representations",
            "venue": "ArXiv:1909.11942.",
            "year": 2019
        },
        {
            "authors": [
                "J Richard Landis",
                "Gary G Koch."
            ],
            "title": "The measurement of observer agreement for categorical data",
            "venue": "biometrics, pages 159\u2013174.",
            "year": 1977
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Tal Linzen",
                "Emmanuel Dupoux",
                "Yoav Goldberg."
            ],
            "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
            "venue": "Transactions of the Association for Computational Linguistics, 4:521\u2013 535.",
            "year": 2016
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Matt Gardner",
                "Yonatan Belinkov",
                "Matthew E. Peters",
                "Noah A. Smith."
            ],
            "title": "Linguistic knowledge and transferability of contextual representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
            "year": 2019
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "CoRR, abs/2107.13586.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "ArXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in gpt",
            "year": 2023
        },
        {
            "authors": [
                "Julian Michael",
                "Jan A. Botha",
                "Ian Tenney."
            ],
            "title": "Asking without telling: Exploring latent ontologies in contextual representations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201920, pages 6792\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "Proceedings of the ICLR Workshop, Scottsdale, AZ, USA.",
            "year": 2013
        },
        {
            "authors": [
                "Peng Qian",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Investigating Language Universal and Specific Properties in Word Embeddings",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL \u201916, pages 1478\u20131488,",
            "year": 2016
        },
        {
            "authors": [
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi."
            ],
            "title": "Neuron-level interpretation of deep NLP models: A survey",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1285\u20131303.",
            "year": 2022
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "BERT rediscovers the classical NLP pipeline",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u2013 4601, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Ekaterina Vylomova",
                "Trevor Cohn",
                "Xuanli He",
                "Gholamreza Haffari."
            ],
            "title": "Word representation models for morphologically rich languages in neural machine translation",
            "venue": "pages 103\u2013108.",
            "year": 2017
        },
        {
            "authors": [
                "Shuohang Wang",
                "Yang Liu",
                "Yichong Xu",
                "Chenguang Zhu",
                "Michael Zeng"
            ],
            "title": "Want to reduce labeling cost? gpt-3 can help",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Bowen Zhang",
                "Daijun Ding",
                "Liwen Jing"
            ],
            "title": "How would stance detection techniques evolve after the launch of chatgpt",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A large body of work done on interpreting pretrained language models answers the question: What knowledge is learned within these models? Researchers have investigated the concepts encoded in pre-trained language models by probing them against various linguistic properties, such as morphological (Vylomova et al., 2017; Belinkov et al., 2017a), syntactic (Linzen et al., 2016; Conneau et al., 2018; Durrani et al., 2019), and semantic (Qian et al., 2016; Belinkov et al., 2017b) tasks, among others. Much of the methodology used in these analyses heavily rely on either having access to an annotated corpus that pertains to the linguistic concept of interest (Tenney et al., 2019; Liu et al.,\n1https://neurox.qcri.org/projects/ transformers-concept-net/\n2019a; Belinkov et al., 2020), or involve human-inthe-loop (Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2017; Geva et al., 2021; Dalvi et al., 2022) to facilitate such an analysis. The use of pre-defined linguistic concepts restricts the scope of interpretation to only very general linguistic concepts, while human-inthe-loop methods are not scalable. We circumvent this bottleneck by using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis.\nGenerative Pre-trained Transformers (GPT) have been trained on an unprecedented amount of textual data, enabling them to develop a substantial understanding of natural language. As their capabilities continue to improve, researchers are finding creative ways to leverage their assistance for various applications, such as question-answering in financial and medical domains (Guo et al., 2023), simplifying medical reports (Jeblick et al., 2022), and detecting stance (Zhang et al., 2023). We carry out an investigation of whether GPT models, specifically ChatGPT, can aid in the interpretation of pre-trained language models (pLMs).\nA fascinating characteristic of neural language models is that words sharing any linguistic relationship cluster together in high-dimensional spaces (Mikolov et al., 2013). Recent research (Michael et al., 2020; Fu and Lapata, 2022; Dalvi et al., 2022) has built upon this idea by exploring representation analysis through latent spaces in pre-trained models. Building on the work of Dalvi et al. (2022) we aim to identify encoded concepts within pre-trained models using agglomerative hierarchical clustering (Gowda and Krishna, 1978) on contextualized representations. The underlying hypothesis is that these clusters represent latent concepts, capturing the language knowledge acquired by the model. Unlike previous approaches that rely on predefined concepts (Michael et al., 2020; Durrani et al., 2022) or human annotation (Alam et al., 2023) to label these concepts, we leverage the ChatGPT model.\nOur findings indicate that the annotations produced by ChatGPT are semantically richer and accurate compared to the human-annotated concepts (for instance BERT Concept NET). Notably, ChatGPT correctly labeled the majority of concepts deemed uninterpretable by human annotators. Using an LLM like ChatGPT improves scalability and accuracy. For instance, the work in Dalvi et al. (2022) was limited to 269 concepts in the final layer of the BERT-base-cased (Devlin et al., 2019) model, while human annotations in Geva et al. (2021) were confined to 100 keys per layer. Using ChatGPT, the exploration can be scaled to the entire latent space of the models and many more architectures. We used GPT to annotate 39K concepts across 5 pre-trained language models. Building upon this finding, we further demonstrate that GPT-based annotations empowers methodologies in interpretation analysis of which we show two: i) probing framework (Belinkov et al., 2017a), ii) neuron analysis (Antverg and Belinkov, 2022).\nProbing Framework We train probes from GPTannotated concept representations to explore concepts that go beyond conventional linguistic categories. For instance, instead of probing for named\nentities (e.g. NE:PER), we can investigate whether a model distinguishes between male and female names or probing for \u201cCities in the southeastern United States\u201d instead of NE:LOC.\nNeuron Analysis Another line of work that we illustrate to benefit from GPT-annotated latent concepts is the neuron analysis i.e. discovering neurons that capture a linguistic phenomenon. In contrast to the holistic view offered by representation analysis, neuron analysis highlights the role of individual neurons (or groups of them) within a neural network ((Sajjad et al., 2022). We obtain neuron rankings for GPT-annotated latent concepts using a neuron ranking method called Probeless (Antverg and Belinkov, 2022). Such fine-grained interpertation analyses of latent spaces enable us to see how neurons distribute in hierarchical ontologies. For instance, instead of simply identifying neurons associated with the POS:Adverbs, we can now uncover how neurons are distributed across sub-concepts such as adverbs of time (e.g., \u201ctomorrow\u201d) and adverbs of frequency (e.g., \u201cdaily\u201d). Or instead of discovering neurons for named entities (e.g. NE:PER), we can discover neurons that capture \u201cMuslim Names\u201d versus \u201cHindu Names\u201d.\nTo summarize, we make the following contributions in this work:\n\u2022 Our demonstration reveals that ChatGPT offers comprehensive and precise labels for latent concepts acquired within pLMs.\n\u2022 We showcased the GPT-based annotations of latent concepts empower methods in interpretation analysis by showing two applications: Probing Classifiers and Neuron Analysis.\n\u2022 We release Transformers Concept-Net, an extensive dataset containing 39K annotated concepts to facilitate the interpretation of pLMs."
        },
        {
            "heading": "2 Methodology",
            "text": "We discover latent concepts by applying clustering on feature vectors (\u00a72.1). They are then labeled using ChatGPT (\u00a72.2) and used for fine-grained interpretation analysis (\u00a72.3 and 2.4). A visual representation of this process is shown in Figure 1."
        },
        {
            "heading": "2.1 Concept Discovery",
            "text": "Contextualized word representations learned in pretrained language models, can identify meaningful groupings based on various linguistic phenomenon. These groups represent concepts encoded within pLMs. Our investigation expands upon the work done in discovering latent ontologies in contextualized representations (Michael et al., 2020; Dalvi et al., 2022). At a high level, feature vectors (contextualized representations) are first generated by performing a forward pass on the model. These representations are then clustered to discover the encoded concepts. Consider a pre-trained model M with L layers: l1, l2, . . . , lL. Using dataset D = w1, w2, ..., wN , we generate feature vectors D M\u2212\u2192 zl = zl1, . . . , zln.2 Agglomerative hierar-\n2zi denotes the contextualized representation for word wi\nchical clustering is employed to cluster the words. Initially, each word forms its own cluster. Clusters are then merged iteratively based on Ward\u2019s minimum variance criterion, using intra-cluster variance as dissimilarity measure. The squared Euclidean distance evaluates the similarity between vector representations. The algorithm stops when K clusters (encoded concepts) are formed, with K being a hyper-parameter."
        },
        {
            "heading": "2.2 Concept Annotation",
            "text": "Encoded concepts capture latent relationships among words within a cluster, encompassing various forms of similarity such as lexical, syntactic, semantic, or specific patterns relevant to the task or data. Figure 2 provides illustrative examples of concepts encoded in the BERT-base-cased model.\nThis work leverages the recent advancements in prompt-based approaches, which are enabled by large language models such as GPT-3 (Brown et al., 2020). Specifically, we utilize a zero-shot learning strategy, where the model is solely provided with a natural language instruction that describes the task of labeling the concept. We used ChatGPT with zero-shot prompt to annotate the latent concepts with the following settings:3\nAssistant is a large language model trained by OpenAI Instructions: Give a short and concise label that best describes the following list of words: [\u201cword 1\u201d, \u201cword 2\u201d, ..., \u201cword N\u201d]"
        },
        {
            "heading": "2.3 Concept Probing",
            "text": "Our large scale annotations of the concepts in pLMs enable training probes towards fine-grained\n3We experimented with several prompts, see Appendix A.1 for details.\nconcepts that lack pre-defined annotations. For example we can use probing to assess whether a model has learned concepts that involve biases related to gender, race, or religion. By tracing the input sentences that correspond to an encoded concept C in a pre-trained model, we create annotations for a particular concept. We perform finegrained concept probing by extracting feature vectors from annotated data through a forward pass on the model of interest. Then, we train a binary classifier to predict the concept and use the probe accuracy as a qualitative measure of how well the model represents the concept. Formally, given a set of tokens W = {w1, w2, ..., wN} \u2208 C, we generate feature vectors, a sequence of latent representations: W M\u2212\u2192 zl = {zl1, . . . , zln} for each word wi by doing a forward pass over si. We then train a binary classifier over the representations to predict the concept C minimizing the cross-entropy loss:\nL(\u03b8) = \u2212 \u2211 i logP\u03b8(ci|wi)\nwhere P\u03b8(ci|zi) = exp(\u03b8l\u00b7zi)\u2211 c\u2032 exp(\u03b8l\u2032 \u00b7zi)\nis the probability that word xi is assigned concept c. We learn the weights \u03b8 \u2208 RD\u00d7L using gradient descent. Here D is the dimensionality of the latent representations zi and L is the size of the concept set which is 2 for a binary classifier."
        },
        {
            "heading": "2.4 Concept Neurons",
            "text": "An alternative area of research in interpreting NLP models involves conducting representation analysis at a more fine-grained level, specifically focusing on individual neurons. Our demonstration showcases how the extensive annotations of latent concepts enhance the analysis of neurons towards more intricate concepts. We show this by using a neuron ranking method called Probeless (Antverg and Belinkov, 2022) over our concept representations. The method obtains neuron rankings using an accumulative strategy, where the score of a given neuron n towards a concept C is defined as follows:\nR(n, C) = \u00b5(C)\u2212 \u00b5(C\u0302)\nwhere \u00b5(C) is the average of all activations z(n,w), w \u2208 C, and \u00b5(C\u0302) is the average of activations over the random concept set. Note that the ranking for each neuron n is computed independently."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "Latent Concept Data We used a subset of the WMT News 2018 dataset, containing 250K randomly chosen sentences (\u22485M tokens). We set a word occurrence threshold of 10 and restricted each word type to a maximum of 10 occurrences. This selection was made to reduce computational and memory requirements when clustering highdimensional vectors. We preserved the original embedding space to avoid information loss through dimensionality reduction techniques like PCA. Consequently, our final dataset consisted of 25,000 word types, each represented by 10 contexts.\nConcept Discovery We apply agglomerative hierarchical clustering on contextualized feature vectors acquired through a forward pass on a pLM for the given data. The resulting representations in each layer are then clustered into 600 groups.4\nConcept Annotation We used ChatGPT available through Azure OpenAI service5 to carryout the annotations. We used a temperature of 0 and a top p value of 0.95. Setting the temperature to 0 controls the randomness in the output and produces deterministic responses.\nPre-trained Models Our study involved several 12-layered transformer models, including BERTcased (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2020).\nProbing and Neuron Analysis For each annotated concept, we extract feature vectors using the relevant data. We then train linear classifiers with a categorical cross-entropy loss function, optimized using Adam (Kingma and Ba, 2014). The training process involved shuffled mini-batches of size 512 and was concluded after 10 epochs. We used a data split of 60-20-20 for train, dev, test when training classifiers. We use the same representations to obtain neuron rankings. We use NeuroX toolkit (Dalvi et al., 2023a) to train our probes and run neuron analysis.\n4Dalvi et al. (2022) discovered that selecting K within the range of 600 \u2212 1000 struck a satisfactory balance between the pitfalls of excessive clustering and insufficient clustering. Their exploration of other methods ELbow and Silhouette did not yield reliable results.\n5https://azure.microsoft.com/en-us/products/ cognitive-services/openai-service"
        },
        {
            "heading": "4 Evaluation and Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Results",
            "text": "To validate ChatGPT\u2019s effectiveness as an annotator, we conducted a human evaluation. Evaluators were shown a concept through a word cloud, along with sample sentences representing the concept and the corresponding GPT annotation. They were then asked the following questions:\n\u2022 Q1: Is the label produced by ChatGPT Acceptable or Unacceptable? Unacceptable annotations include incorrect labels or those that ChatGPT was unable to annotate.\n\u2022 Q2: If a label is Acceptable, is it Precise or Imprecise? While a label may be deemed acceptable, it may not convey the relationship between the underlying words in the concept accurately. This question aims to measure the precision of the label itself.\n\u2022 Q3: Is the ChatGPT label Superior or Inferior to human annotation? BCN labels provided by Dalvi et al. (2022) are used as human annotations for this question.\nIn the first half of Table 1, the results indicate that 90.7% of the ChatGPT labels were considered Acceptable. Within the acceptable labels, 75.1% were deemed Precise, while 24.9% were found to be Imprecise (indicated by Q2 in Table 1). We also computed Fleiss\u2019 Kappa (Fleiss et al., 2013) to measure agreement among the 3 annotators. For Q1, the inter-annotator agreement was found to\nbe 0.71 which is considered substantial according to Landis and Koch (1977). However, for Q2, the agreement was 0.34 (indicating a fair level of agreement among annotators). This was expected due to the complexity and subjectivity of the task in Q2 for example annotators\u2019 knowledge and perspective on precise and imprecise labels."
        },
        {
            "heading": "ChatGPT Labels versus Human Annotations",
            "text": "Next we compare the quality of ChatGPT labels to the human annotations using BERT Concept Net, a human annotated collection of latent concepts learned within the representations of BERT. BCN, however, was annotated in the form of Concept Type:Concept Sub Type (e.g., SEM:entertainment:sport:ice_hockey) unlike GPT-based annotations that are natural language descriptions (e.g. Terms related to ice hockey). Despite their lack of natural language, these reference annotations prove valuable for drawing comparative analysis between humans and ChatGPT. For Q3, we presented humans with a word cloud and three options to choose from: whether the LLM annotations are better, equalivalent, or worse than the BCN annotations. We found that ChatGPT outperformed or achieved equal performance to BCN annotations in 75.5% of cases, as shown in Table 2. The inter-annotator agreement for Q3 was found to be 0.56 which is considered moderate."
        },
        {
            "heading": "4.2 Error Analysis",
            "text": "The annotators identified 58 concepts where human annotated BCN labels were deemed superior. We have conducted an error analysis of these instances and will now delve into the cases where GPT did not perform well.\nSensitive Content Models In 10 cases, the API calls triggered one of the content policy models and failed to provide a label. The content policy models aim to prevent the dissemination of harmful, abusive, or offensive content, including hate speech, misinformation, and illegal activities. Figure 3a shows an example of a sensitive concept that\nincludes words related to crime and assault. This problem can be mitigated by using a version of LLM where content policy models are not enabled.\nLinguistic Ontologies In 8 of the concepts, human annotations (BCN) were better because the concepts were composed of words that were related through a lexical, morphological, or syntactic relationship. The default prompt we used to label the concept tends to find semantic similarity between the words, which did not exist in these concepts. For example, Figure 3b shows a concept composed of 3rd person singular present-tense verbs, but ChatGPT incorrectly labels it as Actions/Events in News Articles. However, humans are robust and can fall back to consider various linguistic ontologies.\nThe BCN concepts are categorized into semantic, syntactic, morphological, and lexical groups (See Table 3). As observed, both humans and ChatGPT found semantic meaning to the concept in majority of the cases. However, humans were also able to identify other linguistic relations such as lexical (e.g. grouped by a lexical property like abbreviations), morphological (e.g. grouped by the same parts-of-speech), or syntactic (e.g. grouped by position in the sentence). Note however, that prompts can be modified to capture specific linguistic property. We encourage interested readers to see our experiments on this in Appendix A.2-A.3.\nInsufficient Context Sometimes context contextual information is important to correctly label a concept. While human annotators (of the BCN corpus) were provided with the sentences in which the underlying words appeared, we did not provide the same to ChatGPT to keep the prompt costeffective. However, providing context sentences in the prompt6 along with the concept to label resulted in improved labels for 11 of the remaining 40 error cases. Figure 3d shows one such example where providing contextual information made ChatGPT to correctly label the concept as Cricket Scores as opposed to Numerical Data the label that it gives without seeing contextual information. However, providing context information didn\u2019t consistently prove helpful. Figure 3c shows a concept, where providing contextual information did not result in the accurate label: Rock Bands and Artists in the US, as identified by the humans.\nUninterpretable Concepts Conversely, we also annotated concepts that were considered uninterpretable or non-meaningful by the human annotators in the BCN corpus and in 21 out 26 cases, ChatGPT accurately assigned labels to these concepts. The proficiency of ChatGPT in processing extensive textual data enables it to provide accurate labels for these concepts.\n6We gave 10 context sentences to ChatGPT."
        },
        {
            "heading": "5 Concept-based Interpretation Analysis",
            "text": "Now that we have established the capability of large language models like ChatGPT in providing rich semantic annotations, we will showcase how these annotations can facilitate extensive finegrained analysis on a large scale."
        },
        {
            "heading": "5.1 Probing Classifiers",
            "text": "Probing classifiers is among the earlier techniques used for interpretability, aimed at examining the knowledge encapsulated in learned representations. However, their application is constrained by the availability of supervised annotations, which often focus on conventional linguistic knowledge and are subject to inherent limitations (Hewitt and Liang, 2019). We demonstrate that using GPT-based annotation of latent concepts learned within these models enables a direct application towards finegrained probing analysis. By annotating the latent space of five renowned pre-trained language models (pLMs): BERT, ALBERT, XLM-R, XLNet, and RoBERTa \u2013 we developed a comprehensive Transformers Concept Net. This net encompasses 39,000 labeled concepts, facilitating cross-architectural comparisons among the models. Table 4 showcases a subset7 of results comparing ALBERT and XLNet through probing classifiers.\nWe can see that the model learns concepts that may not directly align with the pre-defined human onotology. For example, it learns a concept based on Spanish Male Names or Football team names and stadiums. Identifying how\n7For a larger sample of concepts and additional models, please refer to Appendix B.\nfine-grained concepts are encoded within the latent space of a model enable applications beyond interpretation analysis. For example it has direct application in model editing (Meng et al., 2023) which first trace where the model store any concept and then change the relevant parameters to modify its behavior. Moreover, identifying concepts that are associated with gender (e.g., Female names and titles), religion (e.g. Islamic Terminology), and ethnicity (e.g., Nordic names) can aid in elucidating the biases present in these models."
        },
        {
            "heading": "5.2 Neuron Analysis",
            "text": "Neuron analysis examines the individual neurons or groups of neurons within neural NLP models to gain insights into how the model represents linguistic knowledge. However, similar to general interpretability, previous studies in neuron analysis are also constrained by human-in-the-loop (Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2017) or pre-defined linguistic knowledge (Lakretz et al., 2019; Dalvi et al., 2019; Hennigen et al., 2020). Consequently, the resulting neuron explanations are subject to the same limitations we address in this study.\nOur work demonstrates that annotating the latent space enables neuron analysis of intricate linguistic hierarchies learned within these models. For example, Dalvi et al. (2019) and Hennigen et al. (2020) only carried out analysis using very coarse morphological categories (e.g. adverbs, nouns etc.) in parts-of-speech tags. We now showcase how our discovery and annotations of fine-grained latent concepts leads to a deeper neuron analysis of these models. In our analysis of BERT-based partof-speech tagging model, we discovered 17 finegrained concepts of adverb (in the final layer). It is evident that BERT learns a highly detailed semantic hierarchy, as maintains separate concepts for the adverbs of frequency (e.g., \u201crarely, sometimes\u201d) versus adverbs of manner (e.g., \u201cquickly, softly\u201d). We employed the Probeless method (Antverg and Belinkov, 2022) to search for neurons associated with specific kinds of adverbs. We also create a super adverb concept encompassing all types of adverbs, serving as the overarching and generic representation for this linguistic category and obtain neurons associated with the concept. We then compare the neuron ranking obtained from the super concept to the individual rankings from sub concepts. Interestingly, our findings revealed that the top-ranking neurons responsible for learning\nSuper Concept # Sub Concepts Alignment\nAdverbs 17 0.36 \u21aa\u2192 c155: Frequency and manner 0.30 \u21aa\u2192 c136: Degree/Intensity 0.30 \u21aa\u2192 c057: Frequency 0.40 Nouns 13 0.28 \u21aa\u2192 c231: Activities and Objects 0.60 \u21aa\u2192 c279: Industries/Sectors 0.60 \u21aa\u2192 c440: Professions 0.10 Adjectives 17 0.21 \u21aa\u2192 c299: Product Attributes 0.30 \u21aa\u2192 c053: Comparative Adjectives 0.30 \u21aa\u2192 c128: Quality/Appropriateness 0.40 Numbers 17 0.23 \u21aa\u2192 c549: Prices 0.50 \u21aa\u2192 c080: Quantities 0.10 \u21aa\u2192 c593: Monetary Values 0.10\nTable 5: Neuron Analysis on Super Concepts extracted from BERT-base-cased-POS model. The alignment column shows the intersection between the top 10 neurons in the Super concept and the Sub concepts. For detailed results please check Appendix C (See Table 11)\nFigure 4: Neuron overlap between an Adverb Super Concept and sub concepts. Sub concepts shown are Adverbs of frequency and manner (c155), Adverbs of degree/intensity (c136), Adverbs of Probability and Certainty (c265), Adverbs of Frequency (c57), Adverbs of manner and opinion (c332), Adverbs of preference/choice (c570), Adverbs indicating degree or extent (c244), Adverbs of Time (c222).\nthe super concept are often distributed among the top neurons associated with specialized concepts, as shown in Figure 4 for adverbial concepts. The results, presented in Table 5, include the number of discovered sub concepts in the column labeled # Sub Concepts and the Alignment column indicates the percentage of overlap in the top 10 neurons between the super and sub concepts for each specific adverb concept. The average alignment across all sub concepts is indicated next to the super concept. This observation held consistently across various properties (e.g. Nouns, Adjectives and Numbers) as shown in Table 5. For further details please refer to Appendix C).\nNote that previously, we couldn\u2019t identify neurons with such specific explanations, like distinguishing neurons for numbers related to currency values from those for years of birth or neurons differentiating between cricket and hockey-related terms. Our large scale concept annotation enables locating neurons that capture the fine-grained aspects of a concept. This enables applications such as manipulating network\u2019s behavior in relation to that concept. For instance, Bau et al. (2019) identified \u201ctense\u201d neurons within Neural Machine Translation (NMT) models and successfully changed the output from past to present tense by modifying the activation of these specific neurons. However, their study was restricted to very few coarse concepts for which annotations were available."
        },
        {
            "heading": "6 Related Work",
            "text": "With the ever-evolving capabilities of the LLMs, researchers are actively exploring innovative ways to harness their assistance. Prompt engineering, the process of crafting instructions to guide the behavior and extract relevant knowledge from these oracles, has emerged as a new area of research (Lester et al., 2021; Liu et al., 2021; Kojima et al., 2023; Abdelali et al., 2023; Dalvi et al., 2023b). Recent work has established LLMs as highly proficient annotators. Ding et al. (2022) carried out evaluation of GPT-3\u2019s performance as a data annotator for text classification and named entity recognition tasks, employing three primary methodologies to assess its effectiveness. Wang et al. (2021) showed that GPT-3 as an annotator can reduce cost from 50-96% compared to human annotations on 9 NLP tasks. They also showed that models trained using GPT-3 labeled data outperformed the GPT-3 fewshot learner. Similarly, Gilardi et al. (2023) showed that ChatGPT achieves higher zero-shot accuracy compared to crowd-source workers in various annotation tasks, encompassing relevance, stance, topics, and frames detection. Our work is different from previous work done using GPT as annotator. We annotate the latent concepts encoded within the embedding space of pre-trained language models. We demonstrate how such a large scale annotation enriches representation analysis via application in probing classifiers and neuron analysis."
        },
        {
            "heading": "7 Conclusion",
            "text": "The scope of previous studies in interpreting neural language models is limited to general ontologies or small-scale manually labeled concepts. In our research, we showcase the effectiveness of Large Language Models, specifically ChatGPT, as a valuable tool for annotating latent spaces in pre-trained language models. This large-scale annotation of latent concepts broadens the scope of interpretation from human-defined ontologies to encompass all concepts learned within the model, and eliminates the human-in-the-loop effort for annotating these concepts. We release a comprehensive GPTannotated Transformers Concept Net (TCN) consisting of 39,000 concepts, extracted from a wide range of transformer language models. TCN empowers the researchers to carry out large-scale interpretation studies of these models. To demonstrate this, we employ two widely used techniques in the field of interpretability: probing classifiers and neuron analysis. This novel dimension of analysis, previously absent in earlier studies, sheds light on intricate aspects of these models. By showcasing the superiority, adaptability, and diverse applications of ChatGPT annotations, we lay the groundwork for a more comprehensive understanding of NLP models."
        },
        {
            "heading": "Limitations",
            "text": "We list below limitations of our work:\n\u2022 While it has been demonstrated that LLMs significantly reduce the cost of annotations, the computational requirements and response latency can still become a significant challenge when dealing with extensive or high-throughput annotation pipeline like ours. In some cases it is important to provide contextual information along with the concept to obtain an accurate annotation, causing the cost go up. Nevertheless, this is a one time cost for any specific model, and there is optimism that future LLMs will become more cost-effective to run.\n\u2022 Existing LLMs are deployed with content policy filters aimed at preventing the dissemination of harmful, abusive, or offensive content. However, this limitation prevents the models from effectively labeling concepts that reveal sensitive information, such as cultural and racial biases learned within the model to be interpreted. For example, we were unable to extract a label for\nracial slurs in the hate speech detection task. This restricts our concept annotation approach to only tasks that are not sensitive to the content policy.\n\u2022 The information in the world is evolving, and LLMs will require continuous updates to reflect the accurate state of the world. This may pose a challenge for some problems (e.g. news summarization task) where the model needs to reflect an updated state of the world."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Prompts",
            "text": ""
        },
        {
            "heading": "A.1 Optimal Prompt",
            "text": "Initially, we used a simple prompt to ask the model to provide labels for a list of words keeping the system description unchanged: Assistant is a large language model\ntrained by OpenAI Prompt Body: Give the following list of words a short label: [\u201cword 1\u201d, \u201cword 2\u201d, ..., \u201cword N\u201d]\nThe output format from the first prompt was unclear as it included illustrations, which was not our intention. After multiple design iterations, we developed a prompt that returned the labels in the desired format. In this revised prompt, we modified the system description as follows: Assistant is a large language model\ntrained by OpenAI. Instructions: When asked for labels, only the labels and nothing else should be returned.\nWe also modified the prompt body to: Give a short and concise label that\nbest describes the following list of words: [\u201cword 1\u201d, \u201cword 2\u201d, ..., \u201cword N\u201d]\nFigure 5 shows some sample concepts learned in the last layer of BERT-base-cased along with their labels."
        },
        {
            "heading": "A.2 Prompts For Lexical Concepts",
            "text": "During the error analysis (Section 4.2), we discovered that GPT struggled to accurately label concepts composed of words sharing a lexical property, such as a common suffix. However, we were able to devise a solution to address this issue by curating the prompt to effectively label such concepts. We modified the prompt to identify concepts that contain common n-grams. Give a short and concise label\ndescribing the common ngrams between the words of the given list\nNote: Only one common ngram should be returned. If there is no common ngram reply with \u2018NA\u2019\nUsing this improved we were able to correct 100% of the labeling errors in the concepts having lexical coherence. See Figure 7a for example. With the default prompt it was labelled as Superlative and ordinal adjectives and with the modified prompt, it was labeled as Hyphenated, cased & -based suffix."
        },
        {
            "heading": "A.3 Prompts for POS Concepts",
            "text": "Similarly we were able to modify the prompt to correctly label concepts that were made from words having common parts-of-speech. From the prompts we tested, the best performing one is below:\nGive a short and concise label describing the common part of speech tag between the words of the given list Note: The part of speech tag should be\nchosen from the Penn Treebank. If there\u2019s no common part of speech tag reply with \u2018NA\u2019\nIn Figure 7b, we present an example of a concept labeled as Surnames with \u2018Mc\u2019 prefix. However, it is important to note that not all the names in this concept actually begin with the \u201cMc\u201d prefix. The appropriate label for this concept would be NNP: Proper Nouns or SEM: Irish Names. With the POS-based prompt, we are able to achieve the former."
        },
        {
            "heading": "A.4 Providing Context",
            "text": "Our analysis revealed that including contextual information is crucial for accurately labeling concepts in certain cases. As shown in Figure 8, concepts were incorrectly labeled as Numerical Data despite representing different entities. Incorporating context enables us to obtain more specific labels. However, we face limitations in the number of input tokens we can provide to the model, which impacts the quality of the labels. Using context of 10 sentences we were able to correct 9 of the 38 erroneous labels."
        },
        {
            "heading": "A.5 Other Details",
            "text": "Tokens Versus Types We observed that the quality of labels is influenced by the word frequency in the given list. Using tokens instead of types leads to more meaningful labels. However, when the latent concept includes hate speech words, passing a token list results in failed requests due to content policy violations. In such cases, we opted to pass the list of types instead. Although this mitigates the issue to a certain extent, it does not completely\nresolve it. Refer to Figure 6 for examples of failed requests with Albert.\nKeyword prompts We also explored prompts to return 3 keywords that describe the concept instead of returning a concise label in an effort to produce multiple labels like BCN. Instructions: When asked for keywords, only the keywords and nothing else should be returned."
        },
        {
            "heading": "If asked for 3 keywords, the keywords should be returned in the form of [keyword_1, keyword_2, keyword_3]",
            "text": "To ensure compliance with our desired output format, we introduced a second instruction since the model was not following the first instruction as intended. We also modified the prompt body to:\nGive 3 keywords that best describe the following list of words\nUnfortunately, this prompt did not provide accurate labels, as illustrated in Table 6."
        },
        {
            "heading": "B Probing Classifiers",
            "text": ""
        },
        {
            "heading": "B.1 Running Probes At Scale",
            "text": ""
        },
        {
            "heading": "Probing For Fine-grained Semantic Concepts",
            "text": "We used the NeuroX toolkit to train a linear probe for several concepts chosen from layers 3, 9 and 12 of BERT-base-cased. We used a train/val/test splits of 0.6, 0.2, 0.2 respectively. Tables 8 and 9 show the data statistics and the probe results respectively. Table 10 shows results of probes trained on concepts chosen from multiple layers of ALBERT. In Table 7 we carried out a cross architectural comparison across the models by training probes towards the same set of concepts."
        },
        {
            "heading": "C Neuron Analysis Results",
            "text": "Neurons Associated with POS concepts We performed an annotation process on the final layer of a fine-tuned version of BERT-base-cased, specifically focusing on the task of parts-of-speech tagging. Once we obtained the labels, we organized them into super concepts based on a shared characteristic among smaller concepts. For instance, we grouped together various concepts labeled as nouns, as well as concepts representing adjectives,\nadverbs, and numerical data. To assess the alignment between the sub concepts and the super concept, we calculated the occurrence percentage of the top 10 neurons from the sub concept within the top 10 neurons of the super concept. The outcomes of this analysis can be found in table 11, illustrating the average alignment between the sub concepts and the super concepts.\nNeurons Associated with the Names concepts We replicated the experiment using named entity concepts derived from the final layer of bert-basecased. The findings are presented in table 12."
        }
    ],
    "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
    "year": 2023
}