{
    "abstractText": "Automating radiology report generation can significantly alleviate radiologists\u2019 workloads. Previous research has primarily focused on realizing highly concise observations while neglecting the precise attributes that determine the severity of diseases (e.g., small pleural effusion). Since incorrect attributes will lead to imprecise radiology reports, strengthening the generation process with precise attribute modeling becomes necessary. Additionally, the temporal information contained in the historical records, which is crucial in evaluating a patient\u2019s current condition (e.g., heart size is unchanged), has also been largely disregarded. To address these issues, we propose RECAP, which generates precise and accurate radiology reports via dynamic disease progression reasoning. Specifically, RECAP first predicts the observations and progressions (i.e., spatiotemporal information) given two consecutive radiographs. It then combines the historical records, spatiotemporal information, and radiographs for report generation, where a disease progression graph and dynamic progression reasoning mechanism are devised to accurately select the attributes of each observation and progression. Extensive experiments on two publicly available datasets demonstrate the effectiveness of our model.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenjun Hou"
        },
        {
            "affiliations": [],
            "name": "Yi Cheng"
        },
        {
            "affiliations": [],
            "name": "Kaishuai Xu"
        },
        {
            "affiliations": [],
            "name": "Wenjie Li"
        },
        {
            "affiliations": [],
            "name": "Jiang Liu"
        }
    ],
    "id": "SP:3fd6cd81a4379011ba5d1aeb1bc888553d79f4a6",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "2018 IEEE Conference on Computer Vision and Pattern",
            "year": 2018
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Alvarez-Valle",
                "Ozan Oktay"
            ],
            "title": "Learning to exploit temporal structure for biomedical visionlanguage processing",
            "year": 2023
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yaling Shen",
                "Yan Song",
                "Xiang Wan."
            ],
            "title": "Cross-modal memory networks for radiology report generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yan Song",
                "Tsung-Hui Chang",
                "Xiang Wan."
            ],
            "title": "Generating radiology reports via memory-driven transformer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2020
        },
        {
            "authors": [
                "Kenneth Ward Church",
                "Patrick Hanks."
            ],
            "title": "Word association norms, mutual information, and lexicography",
            "venue": "Computational Linguistics, 16(1):22\u201329.",
            "year": 1990
        },
        {
            "authors": [
                "Jean-Benoit Delbrouck",
                "Pierre Chambon",
                "Christian Bluethgen",
                "Emily Tsai",
                "Omar Almusa",
                "Curtis Langlotz."
            ],
            "title": "Improving the factual correctness of radiology report generation with semantic rewards",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255.",
            "year": 2009
        },
        {
            "authors": [
                "Mark Endo",
                "Rayan Krishnan",
                "Viswesh Krishna",
                "Andrew Y. Ng",
                "Pranav Rajpurkar."
            ],
            "title": "Retrievalbased chest x-ray report generation using a pretrained contrastive language-image model",
            "venue": "Proceedings of Machine Learning for Health, volume",
            "year": 2021
        },
        {
            "authors": [
                "Wenjun Hou",
                "Kaishuai Xu",
                "Yi Cheng",
                "Wenjie Li",
                "Jiang Liu"
            ],
            "title": "Organ: Observation-guided radiology report generation via tree reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Luyang Huang",
                "Lingfei Wu",
                "Lu Wang."
            ],
            "title": "Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5094\u20135107, On-",
            "year": 2020
        },
        {
            "authors": [
                "Zhongzhen Huang",
                "Xiaofan Zhang",
                "Shaoting Zhang."
            ],
            "title": "Kiut: Knowledge-injected u-transformer for radiology report generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19809\u201319818.",
            "year": 2023
        },
        {
            "authors": [
                "son",
                "Curtis P. Langlotz",
                "Bhavik N. Patel",
                "Matthew P. Lungren",
                "Andrew Y. Ng"
            ],
            "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
            "venue": "In The Thirty-Third AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Haozhe Ji",
                "Pei Ke",
                "Shaohan Huang",
                "Furu Wei",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "Language generation with multi-hop reasoning on commonsense knowledge graph",
            "venue": "Proceedings of the 2020 Conference",
            "year": 2020
        },
        {
            "authors": [
                "Baoyu Jing",
                "Pengtao Xie",
                "Eric P. Xing."
            ],
            "title": "On the automatic generation of medical imaging reports",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1:",
            "year": 2018
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Nathaniel R Greenbaum",
                "Matthew P Lungren",
                "Chih-ying Deng",
                "Yifan Peng",
                "Zhiyong Lu",
                "Roger G Mark",
                "Seth J Berkowitz",
                "Steven Horng"
            ],
            "title": "Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs",
            "year": 2019
        },
        {
            "authors": [
                "Mingjie Li",
                "Bingqian Lin",
                "Zicong Chen",
                "Haokun Lin",
                "Xiaodan Liang",
                "Xiaojun Chang."
            ],
            "title": "Dynamic graph enhanced contrastive learning for chest x-ray report generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Li",
                "Xiaodan Liang",
                "Zhiting Hu",
                "Eric P. Xing."
            ],
            "title": "Hybrid retrieval-generation reinforced agent for medical image report generation",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing",
            "year": 2018
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Fenglin Liu",
                "Shen Ge",
                "Xian Wu."
            ],
            "title": "Competence-based multimodal curriculum learning for medical report generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Fenglin Liu",
                "Xian Wu",
                "Shen Ge",
                "Wei Fan",
                "Yuexian Zou."
            ],
            "title": "Exploring and distilling posterior and prior knowledge for radiology report generation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Fenglin Liu",
                "Changchang Yin",
                "Xian Wu",
                "Shen Ge",
                "Ping Zhang",
                "Xu Sun."
            ],
            "title": "Contrastive attention for automatic chest x-ray report generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6,",
            "year": 2021
        },
        {
            "authors": [
                "Guanxiong Liu",
                "Tzu-Ming Harry Hsu",
                "Matthew B.A. McDermott",
                "Willie Boag",
                "Wei-Hung Weng",
                "Peter Szolovits",
                "Marzyeh Ghassemi."
            ],
            "title": "Clinically accurate chest x-ray report generation",
            "venue": "CoRR, abs/1904.02633.",
            "year": 2019
        },
        {
            "authors": [
                "Zhibin Liu",
                "Zheng-Yu Niu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Knowledge aware conversation generation with explainable reasoning over augmented graphs",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Justin Lovelace",
                "Bobak Mortazavi."
            ],
            "title": "Learning to generate clinically coherent chest X-ray reports",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1235\u20131243, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Jiasen Lu",
                "Caiming Xiong",
                "Devi Parikh",
                "Richard Socher."
            ],
            "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "Yasuhide Miura",
                "Yuhao Zhang",
                "Emily Tsai",
                "Curtis Langlotz",
                "Dan Jurafsky."
            ],
            "title": "Improving factual completeness and consistency of image-to-text radiology report generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Feiteng Mu",
                "Wenjie Li."
            ],
            "title": "Enhancing text generation via multi-level knowledge aware reasoning",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 4310\u20134316. International Joint Conferences on",
            "year": 2022
        },
        {
            "authors": [
                "Jianmo Ni",
                "Chun-Nan Hsu",
                "Amilcare Gentili",
                "Julian McAuley."
            ],
            "title": "Learning visual-semantic embeddings for reporting abnormal findings on chest X-rays",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1954\u20131960, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Toru Nishino",
                "Yasuhide Miura",
                "Tomoki Taniguchi",
                "Tomoko Ohkuma",
                "Yuki Suzuki",
                "Shoji Kido",
                "Noriyuki Tomiyama."
            ],
            "title": "Factual accuracy is not enough: Planning consistent description order for radiology report generation",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Farhad Nooralahzadeh",
                "Nicolas Perez Gonzalez",
                "Thomas Frauenfelder",
                "Koji Fujimoto",
                "Michael Krauthammer."
            ],
            "title": "Progressive transformer-based generation of radiology reports",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Han Qin",
                "Yan Song."
            ],
            "title": "Reinforced cross-modal alignment for radiology report generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 448\u2013458. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Vignav Ramesh",
                "Nathan Andrew Chi",
                "Pranav Rajpurkar"
            ],
            "title": "Improving radiology report generation systems by removing hallucinated references to nonexistent priors",
            "year": 2022
        },
        {
            "authors": [
                "Steven J. Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel."
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N Kipf",
                "Peter Bloem",
                "Rianne van den Berg",
                "Ivan Titov",
                "Max Welling."
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "venue": "European semantic web conference, pages 593\u2013607. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Akshay Smit",
                "Saahil Jain",
                "Pranav Rajpurkar",
                "Anuj Pareek",
                "Andrew Ng",
                "Matthew Lungren."
            ],
            "title": "Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Song",
                "Xiaodan Zhang",
                "Junzhong Ji",
                "Ying Liu",
                "Pengxu Wei."
            ],
            "title": "Cross-modal contrastive attention model for medical report generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 2388\u20132397, Gyeongju,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Tanida",
                "Philip M\u00fcller",
                "Georgios Kaissis",
                "Daniel Rueckert."
            ],
            "title": "Interactive and explainable regionguided radiology report generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7433\u20137442.",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Lin Tuan",
                "Sajjad Beygi",
                "Maryam Fazel-Zarandi",
                "Qiaozi Gao",
                "Alessandra Cervone",
                "William Yang Wang."
            ],
            "title": "Towards large-scale interpretable knowledge graph reasoning for dialogue systems",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan."
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "CVPR, pages 3156\u20133164. IEEE Computer Society.",
            "year": 2015
        },
        {
            "authors": [
                "Zhanyu Wang",
                "Lingqiao Liu",
                "Lei Wang",
                "Luping Zhou."
            ],
            "title": "Metransformer: Radiology report generation by transformer with multiple learnable expert tokens",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Shuxin Yang",
                "Xian Wu",
                "Shen Ge",
                "Shaohua Kevin Zhou",
                "Li Xiao."
            ],
            "title": "Knowledge matters: Radiology report generation with general and specific knowledge",
            "venue": "CoRR, abs/2112.15009.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Radiology report generation (Rennie et al., 2017; Anderson et al., 2018; Chen et al., 2020), aiming to generate clinically coherent and factually accurate free-text reports, has received increasing attention\n\u2217Equal Contribution. \u2020Corresponding authors. 1Our code is available at https://github.com/wjhou/\nRecap.\nfrom the research community due to its large potential to alleviate radiologists\u2019 workloads.\nRecent research works (Nooralahzadeh et al., 2021; Nishino et al., 2022; Delbrouck et al., 2022; Bannur et al., 2023; Tanida et al., 2023; Hou et al., 2023) have made significant efforts in improving the clinical factuality of generated reports. Despite their progress, these methods still struggle to produce precise and accurate free-text reports. One significant problem within these methods is that although they successfully captured the semantic information of observations, their attributes still remain imprecise. They either ignored historical records (i.e., temporal information) that are required for assessing patients\u2019 current conditions or omitted the fine-grained attributes of observations (i.e., spatial information) that are crucial in quantifying the severity of diseases, which are far from adequate and often lead to imprecise reports. Both\ntemporal and spatial information are crucial for generating precise and accurate reports. For instance, as illustrated in Figure 1, the patient\u2019s conditions can change from time to time, and the observations become Better and Stable. Only if accessing the historical records, the overall conditions could be estimated. In addition, different attributes reflect the severity of an observation, such as normal and top-normal for Cardiomegaly. In order to produce precise and accurate free-text reports, we must consider both kinds of information and apply stronger reasoning to strengthen the generation process with precise attribute modeling.\nIn this paper, we propose RECAP, which captures both temporal and spatial information for radiology REport Generation via DynamiC DiseAse Progression Reasoning. Specifically, RECAP first predicts observations and progressions given two consecutive radiographs. It then combines them with the historical records and the current radiograph for report generation. To achieve precise attribute modeling, we construct a disease progression graph, which contains the prior and current observations, the progressions, and the precise attributes. We then devise a dynamic progression reasoning (PrR) mechanism that aggregates information in the graph to select observation-relevant attributes.\nIn conclusion, our contributions can be summarized as follows:\n\u2022 We propose RECAP, which can capture both spatial and temporal information for generating precise and accurate free-text reports.\n\u2022 To achieve precise attribute modeling, we construct a disease progression graph containing both observations and fine-grained attributes that quantify the severity of diseases. Then, we devise a dynamic disease progression reasoning (PrR) mechanism to select observation/progression-relevant attributes.\n\u2022 We conduct extensive experiments on two publicly available benchmarks, and experimental results demonstrate the effectiveness of our model in generating precise and accurate radiology reports."
        },
        {
            "heading": "2 Preliminary",
            "text": ""
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "Given a radiograph-report pair Dc = {Xc, Y c}, with its record of last visit being either Dp =\n{Xp, Y p} or Dp = \u2205 if the historical record is missing2, the task of radiology report generation aims to maximize p(Y c|Xc, Dp). To learn the spatiotemporal information, observations O (i.e., spatial information) (Irvin et al., 2019) and progressions P (i.e., temporal information) (Wu et al., 2021) are introduced. Then, the report generation process is divided into two stages in our framework, i.e., observation and progression prediction (i.e., Stage 1) and spatiotemporal-aware report generation (i.e., Stage 2). Specifically, the probability of observations and progressions are denoted as p(O|Xc) and p(P |Xc, Xp), respectively, and then the generation process is modeled as p(Y c|Xc, Dp, O, P ). Finally, our framework aims to maximize the following probability:\np(Y c|Xc, Dp) \u221d Stage 1\ufe37 \ufe38\ufe38 \ufe37\np(O|Xc) \u00b7 p(P |Xc, Xp) \u00b7p(Y c|Xc, Dp, O, P )\ufe38 \ufe37\ufe37 \ufe38\nStage 2\n."
        },
        {
            "heading": "2.2 Progression Graph Construction",
            "text": "Observation and Progression Extraction. For each report, we first label its observations O = {o1, . . . , o|o|} with CheXbert (Smit et al., 2020). Similar to Hou et al. (2023), each observation is further labeled with its status (i.e., Positive, Negative, Uncertain, and Blank). We convert Positive and Uncertain as POS, Negative as NEG, and remove Blank, as shown in Figure 1. Then, we extract progression information P of a patient with Chest ImaGenome (Wu et al., 2021) which provides progression labels (i.e., Better, Stable, or Worse) between two regions of interest (ROIs) in Xp and Xc, respectively. However, extracting ROIs could be difficult, and adopting such ROI-level labels may not generalize well across different datasets. Thus, we use image-level labels, which only indicate whether there are any progressions between Xp and Xc. As a result, a patient may have different progressions (e.g., both Better and Worse). The statistics of observations and progressions can be found in Appendix A.1. Spatial/Temporal Entity (Attribute) Collection.3 To model spatial and temporal information, we\n2There are two kinds of records (i.e., first-visit and followup-visit). If it is the first visit of a patient, the historical record does not exist.\n3Attributes are included in the entity set as provided by Jain et al. (2021). For simplicity, we use \"attribute\" and \"entity\" interchangeably in this paper.\ncollect a set of entities to represent it. For temporal entities, we adopt the entities provided by (Bannur et al., 2023), denoted as ET . For spatial entities ES , we adopt the entities with a relation modify or located_at in RadGraph (Jain et al., 2021), and we also filter out stopwords4 and temporal entities from them. Part of the temporal and spatial entities are listed in Appendix A.2. Progression Graph Construction. Our progression graph G =< V,R > is constructed based purely on the training corpus in an unsupervised manner. Specificially, V = {O,ET , ES} is the node-set, and R = {S,B,W,RS , RO} is the edge set, where S, B, and W denote three progressions Stable, Better, and Worse, connecting an observation with an temporal entity. In addition, Rs and Ro are additional relations connecting current observations with spatial entities and prior/current observations, respectively. To extract spatial/temporal triples automatically, we use the proven-efficient statistical tool, i.e., pointwise mutual information (PMI; Church and Hanks (1990)), where a higher PMI score implies two units with higher co-occurrence, similar to Hou et al. (2023):\nPMI(x\u0304, x\u0302) = log p(x\u0304, x\u0302)\np(x\u0304)p(x\u0302) = log p(x\u0302|x\u0304) p(x\u0302) ,\nSpecifically, we set x\u0304 to (oi, rj) where rj \u2208 R and 4https://www.nltk.org/\nset x\u0302 to e\u2217k where e \u2217 k \u2208 {ET , ES}. Then, we rank these triples using PMI((oi, rj), e\u2217k) and select topK of them as candidates for each (oi, rj). Finally, we use observations as the query to retrieve relevant triples. We consider edges in the graph: e\u2217i\nrj\u2212\u2192 opk RO\u2212\u2212\u2192 ocl rm\u2212\u2212\u2192 e\u2217n, as shown in the top-right of Figure 2, consistent with the progression direction."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Visual Encoding",
            "text": "Given an image Xc, an image processor is first to split it into N patches, and then a visual encoder (i.e., ViT (Dosovitskiy et al., 2021)) is adopted to extract visual representations Xc:\nXc = {[CLS]c,xc1, . . . ,xcN} = ViT(Xc), where [CLS]c \u2208 Rh is the representation of the class token [CLS] prepended in the patch sequence, xci \u2208 Rh is the i-th visual representation. Similarly, the visual representation of image Xp is extracted using the same ViT model and represented as Xp = {[CLS]p,xp1, . . . ,x p N}."
        },
        {
            "heading": "3.2 Stage 1: Observation and Progression Prediction",
            "text": "Observation Prediction. As observations can be measured from a single image solely, we only use the pooler output [CLS]c of Xc for observation\nprediction. Inspired by Tanida et al. (2023), we divide it into two steps, i.e., detection and then classification. Specifically, the detection probability pd(oi) of the i-th observation presented in a report and the probability of this observation pc(oi) being classified as abnormal are modeled as:\npd(oi) = \u03c3(Wdi [CLS] c + bdi), pc(oi) = \u03c3(Wci [CLS] c + bci),\nwhere \u03c3 is the Sigmoid function, Wdi ,Wci \u2208 Rh are the weight matrices and bdi , bci \u2208 R are the biases. Finally, the probability of the i-th observation is denoted as p(oi) = pd(oi) \u00b7 pc(oi). Note that for observation No Finding on is presented in every sample, i.e., pd(on) = 1 and p(on) = pc(on). Progression Prediction. Similar to observation prediction, the pooler outputs [CLS]p of Xp and [CLS]c of Xc are adopted for progression prediction, and the probability of the j-th progression p(pj) is modeled as:\n[CLS] = [[CLS]p; [CLS]c], p(pj) = \u03c3(Wj [CLS] + bj),\nwhere [; ] is the concatenation operation, Wj \u2208 R2h is the weight matrix, and bj \u2208 R are the bias. As we found that learning sparse signals from image-level progression labels is difficult and has side effects on the performance of observation prediction, we detach [CLS] from the computational graph while training. Training. We optimize these two prediction tasks by minimizing the binary cross-entropy loss. Specifically, the loss of observation detection Ld is denoted as:\nLd = \u2212 1 |O| \u2211 [\u03b1d \u00b7 ldi \u00b7 logpd(oi)\n+(1\u2212 ldi) \u00b7 log(1\u2212 pd(oi))], where \u03b1d is the weight to tackle the class imbalance issue, ldi denotes the label of i-th observation di. Similarly, the loss of observation classification Lc and progression prediction Lp can be calculated using the above equation. Note that Lc and Lp are unweighted loss. Finally, the overall loss of Stage 1 is LS1 = Ld + Lc + Lp."
        },
        {
            "heading": "3.3 Stage 2: SpatioTemporal-aware Report Generation",
            "text": "Observation-aware Visual Encoding. To learn the observation-aware visual representations, we jointly encode Xc and its observations Oc using a Transformer encoder (Vaswani et al., 2017). Additionally, a special token [FiV] for first-visit records\nor [FoV] for follow-up-visit records is appended to distinguish them, represented as [F*V]:\nhc = [hcX ;h c o] = Encodero([X c; [F*V];Oc]),\nwhere hcX ,h c o \u2208 Rh are the visual hidden representations and observation hidden representations of the current radiograph and observations. Progression-aware Information Encoding. We use another encoder to encode the progression information (i.e., temporal information). Specifically, given Xp and Y p, the hidden states of the prior record are represented as:\nhp = [hpX ;h p Y ] = Encoderp([X p;Y p]),\nwhere hpX ,h p Y \u2208 Rh are the visual hidden representations and textual hidden representations of prior records, respectively. Concise Report Decoding. Given hp and hc, a Transformer decoder is adopted for report generation. Since not every sample has a prior record and follow-up records may include new observations, controlling the progression information is necessary. Thus, we include a soft gate \u03b1 to fuse the observation-related and progression-related information, as shown in Figure 2:\nDecoder =  hst = Self-Attn(h w t ,h w <t,h w <t), h\u0303ct = Cross-Attno(h s t ,h c,hc), h\u0303pt = Cross-Attnp(h\u0303 c t ,h p,hp), \u03b1 = \u03c3(W\u03b1h\u0303 c t + b\u03b1),\nht = \u03b1 \u00b7 h\u0303pt + (1\u2212 \u03b1) \u00b7 h\u0303ct , pV(yt) = Softmax(WVht + bV),\nwhere Self-Attn is the self-attention module, CrossAttn is the cross-attention module, hst , h\u0303ct , h\u0303 p t ,ht \u2208 Rh are self-attended hidden state, observationrelated hidden state, progression-related hidden state, and spatiotemporal-aware hidden state, respectively, W\u03b1 \u2208 Rh,WV \u2208 R|V|\u00d7h are weight matrices and b\u03b1 \u2208 R, bV \u2208 R|V| are the biases. Disease Progression Encoding. As there are different relations between nodes, we adopt an L-layer Relational Graph Convolutional Network (R-GCN) (Schlichtkrull et al., 2018) to encode the disease progression graph, similar to Ji et al. (2020):\nhl+1vi = ReLU  1 ci rj\u2208R\u2211 vk\u2208V W lrjh l vk +W l0h l vi  , where ci is the number of neighbors connected to the i-th node, W lrj ,W l 0 \u2208 Rh\u00d7h are learnable weight metrics, and hlvi ,h l+1 vi ,h l vk\n\u2208 Rh are the hidden representations.\nPrecise Report Decoding via Progression Reasoning. Inspired by Ji et al. (2020) and Mu and Li (2022), we devise a dynamic disease progression reasoning (PrR) mechanism to select observationrelevant attributes from the progression graph. The reasoning path of PrR is oci\nrj\u2212\u2192 ek, where rj belongs to either three kinds of progression or Rs. Specifically, given t-th hidden representation ht, the observation representation hLoi , and the entity representation hLek of ek, the progression score p\u0302st(ek) of node ek is calculated as:\npst(ek) = 1 |Nek | \u2211\n(oi,rj)\u2208Nek\n\u03d5(hTtWri [h L oi ;h L ek ]),\np\u0302st(ek) = \u03b3 \u00b7 pst(ek)+\u03d5(htWshLek), where \u03d5 is the Tangent function, \u03b3 is the scale factor, Nek is the neighbor collection of ek, and Wri \u2208 Rh\u00d72h and Ws \u2208 Rh\u00d7h are weight matrices for learning relation ri and self-connection, respectively. In the PrR mechanism, the relevant scores (i.e., pst(ek)) of their connected observations are also included in p\u0302st(ek) since ht contains observation information, and higher relevant scores of these connected observations indicate a higher relevant score of ek. Then, the distribution over all entities in G is denoted as:\npG(yt) = Softmax(p\u0302st(ek)).\nFinally, a soft gate gt = \u03c3(Wght + bg) is adopted\nto combine pV(yt) and pG(yt) into p(yt):\np(yt) = gt \u00b7 pV(yt) + (1\u2212 gt) \u00b7 pG(yt),\nwhere Wg \u2208 Rh and bg \u2208 R are the weight matrix and bias, respectively. Training. The generation process is optimized using the negative log-likelihood loss, given each token\u2019s probability p(yt) and the probability of gt:\nLNLL = \u2212 T\u2211 t=1 logp(yt),\nLg = \u2212 T\u2211 t=1 [lgt loggt + (1\u2212 lgt)log(1\u2212 gt)],\nwhere lgt indicates t-th token appears in G. Finally, the loss of Stage 2 is LS2 = LNLL + \u03bbLg."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We use two benchmarks to evaluate our models, MIMIC-ABN5 (Ni et al., 2020) and MIMICCXR6 (Johnson et al., 2019). We provide other details of data preprocessing in Appendix A.3.\n\u2022 MIMIC-CXR consists of 377,110 chest Xray images and 227,827 reports from 63,478 patients. We adopt the settings of Chen et al. (2020).\n5https://github.com/zzxslp/WCL 6https://physionet.org/content/mimic-cxr-jpg/\n2.0.0/\n\u2022 MIMIC-ABN is a modified version of MIMIC-CXR and only contains abnormal sentences. The original train/validation/test split of Ni et al. (2020) is 26,946/3,801/7,804 samples, respectively. To collect patients\u2019 historical information and avoid information leakage, we recover the data-split used in MIMIC-CXR according to the subject_id7. Finally, the data-split used in our experiments is 71,786/546/806 for train/validation/test sets, respectively."
        },
        {
            "heading": "4.2 Evaluation Metrics and Baselines",
            "text": "NLG Metrics. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) are selected as the Natural Language Generation metrics (NLG Metrics), and we use the MS-COCO evaluation tool8 to compute the results. CE Metrics. For Clinical Efficacy (CE Metrics), CheXbert9 (Smit et al., 2020) is adopted to label the generated reports compared with disease labels of the references. Besides, we use the temporal entity matching scores (TEM), proposed by Bannur et al. (2023), to evaluate how well the models generate progression-related information. Baselines. For performance evaluation, we compare our model with the following state-of-the-art (SOTA) baselines: R2GEN (Chen et al., 2020), R2GENCMN (Chen et al., 2021), KNOWMAT (Yang et al., 2021), M2TR (Nooralahzadeh et al., 2021), CMM-RL (Qin and Song, 2022), CMCA (Song et al., 2022), CXR-RePaiR-Sel/2 (Endo et al., 2021), BioViL-T (Bannur et al., 2023), DCL (Li et al., 2023), METrans (Wang et al., 2023), KiUT (Huang et al., 2023), and ORGAN (Hou et al., 2023).\n7subject_id is the anonymized identifier of a patient. 8https://github.com/tylin/coco-caption 9https://github.com/stanfordmlgroup/CheXbert"
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "We use the ViT (Dosovitskiy et al., 2021), a vision transformer pretrained on ImageNet (Deng et al., 2009), as the visual encoder10. The maximum decoding step is set to 64/104 for MIMIC-ABN and MIMIC-CXR, respectively. \u03b3 is set to 2 and K is set to 30 for both datasets.\nFor model training, we adopt AdamW (Loshchilov and Hutter, 2019) as the optimizer. The layer number of the Transformer encoder and decoder are both set to 3, and the dimension of the hidden state is set to 768, which is the same as the one of ViT. The layer number L of the R-GCN is set to 3. The learning rate is set to 5e-5 and 1e-4 for the pretrained ViT and the rest of the parameters, respectively. The learning rate decreases from the initial learning rate to 0 with a linear scheduler. The dropout rate is set to 0.1, the batch size is set to 32, and \u03bb is set to 0.5. We select the best checkpoints based on the BLEU-4 on the validation set. Our model has 160.05M trainable parameters, and the implementations are based on the HuggingFace\u2019s Transformers (Wolf et al., 2020). We implement our models on an NVIDIA-3090 GTX GPU with mixed precision. Other details of implementation (e.g., Stage 1 training) can be found in Appendix A.3."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Quantitative Analysis",
            "text": "NLG Results. The NLG results of two datasets are listed on the left side of Table 1 and Table 2. As we can see from Table 1, RECAP achieves the best\n10The model card is \"google/vit-base-patch16-224-in21k.\"\nperformance compared with other SOTA models and outperforms other baselines substantially on both datasets. Clinical Efficacy Results. The clinical efficacy results are shown on the right side of Table 1. RECAP achieves SOTA performance on F1 score, leading to a 1.2% improvement over the best baseline (i.e., ORGAN) on the MIMIC-ABN dataset. Similarly, on the MIMIC-CXR dataset, our model achieves a score of 0.393, increasing by 0.8% compared with the second-best. This demonstrates that RECAP can generate better clinically accurate reports. Temporal-related Results. Since there are only 10% follow-up-visits records in the MIMIC-ABN dataset, we mainly focus on analyzing the MIMICCXR dataset, as shown in Table 3 and Table 6. RECAP achieves the best performance on BLEU4, TEM. In terms of the clinical F1, RECAP w/o PrR outperforms other baselines. This indicates that historical records are necessary for generating follow-up reports. Ablation Results. We perform ablation analysis, and the ablation results are listed in Table 4. We also list the ablation results on progression modeling in Table 6. There are four variants: (1) RECAP w/o OP (i.e., a standard Transformer model, removing spatiotemporal information), (2) RECAP w/o Obs (i.e., without observation), (3) RECAP w/o Pro (i.e., without progression), and (4) RECAP w/o PrR, which does not adopt the disease progression reasoning mechanism.\nAs we can see from Table 4, without the spatiotemporal information (i.e., variant 1), the performances drop significantly on both datasets, which indicates the necessity of spatiotemporal modeling in free-text report generation. In addition, com-\npared with variant 1, the performance of RECAP w/o Obs increases substantially on the MIMICCXR dataset, which demonstrates the importance of historical records in assessing the current conditions of patients. In terms of CE metrics, learning from the observation information boosts the performance of RECAP drastically, with an improvement of 12%. In addition, the performance of RECAP increases compared with variant w/o PrR. This indicates that PrR can help generate precise and accurate reports."
        },
        {
            "heading": "5.2 Qualitative Analysis",
            "text": "Case Study. We conduct a detailed case study on how RECAP generates precise and accurate attributes of a given radiograph in Figure 3. RECAP successfully generates six observations, including five abnormal observations. Regarding attribute modeling, our model can generate the precise description \"the lungs are clear without focal consolidation\", which also appears in the reference, while RECAP w/o OP can not generate relevant descriptions. This indicates that spatiotemporal information plays a vital role in the generation process. Additionally, RECAP can learn to compare with the historical records (e.g., mediastinal contours are stable and remarkable) so as to precisely measure the observations. Error Analysis. We depict error analysis to provide more insights, as shown in Figure 4. There are two major errors, which are false-positive observations (i.e., Positive Lung Opacity and Positive Pleural Effusion) and false-negative observations (i.e., Negative Cardiomegaly). Improving the performance of observation prediction could be an important direction in enhancing the quality of gener-\nPrior Radiograph\nCurrent Radiograph Progression: Stable\n1\u20dd Enlarged Card./NEG 2\u20dd Cardiomegaly/POS 3\u20dd Edema/NEG 4\u20dd Consolidation/NEG 5\u20dd Pneumothorax/NEG 6\u20dd Pleural Effusion/NEG\nPrior Report: low lung volumes are present. 2\u20dd this accentuates the size of the cardiac silhouette which is likely mildly enlarged. 1\u20dd mediastinal and hilar contours are likely within normal limits. a right brachiocephalic venous stent is re-demonstrated. there is crowding of the bronchovascular structures with probable 3\u20dd mild pulmonary vascular congestion. 6\u20dd no pleural effusion or 5\u20dd pneumothorax is identified.\nReference: ap and lateral views of the chest. the lungs are 4\u20dd clear of consolidation or 6\u20dd effusion. 2\u20dd the cardiac silhouette is enlarged but unchanged. no acute osseous abnormality is detected. right brachiocephalic venous stent is again noted.\nOurs: the lungs are clear without focal consolidation. no pleural effusion or pneumothorax is seen. the cardiac silhouette is top-normal to mildly enlarged. mediastinal contours are stable and unremarkable. there is no pulmonary edema.\nOurs w/o OP: ap upright and lateral views of the chest provided. lung volumes are low limiting assessment. allowing for this there is 4\u20dd no focal consolidation 6\u20dd effusion or 5\u20dd pneumothorax. 1\u20dd the cardiomediastinal silhouette is normal. imaged osseous structures are intact. no free air below the right hemidiaphragm is seen.\nFigure 1: Case study of a follow-up-visit sample, given its prior radiograph and prior report. Attributes of observations in reports are highlighted in boldface, and spans with colors in reports indicate mentions of observations.\nFigure 3: Case study of a follow-up-visit sample, given its prior radiograph and prior report. Attributes of observations in reports are highlighted in boldface, and spans with colors in reports indicate mentions of observations.\n1\u20dd Enlarged Card./NEG 2\u20dd Cardiomegaly/F.NEG 3\u20dd Lung Opacity/F.POS 4\u20dd Pneumothorax/NEG 5\u20dd Effusion/F.POS\nReference: there is no new consolidation. right lower lobe pneumonia that was present in prior exams has significantly improved. esophageal stent is in unchanged position. there is no pneumomediastinum or pneumothorax. there is no pleural effusion . mediastinal and cardiac contours are stable. Ours: 2\u20dd the heart size is normal. the hilar and mediastinal contours are within normal limits. there is no pneumothorax. again seen is a 5\u20dd small right pleural effusion. the visualized osseous structures are unremarkable. there has been interval improvement of the 3\u20dd right basilar opacity.\nFigure 1: Error case generated by RECAP. The span and\nthe spans denote false negative observation and false positive observation, respectively.\nFigure 4: Error case generated by RECAP. The span and\nthe spans denote false negative observation and false positive observation, respectively.\nated reports. In addition, although RECAP aims to model precise attributes of observations presented in the radiograph, it still can not cover all the cases. This might be alleviated by incorporating external knowledge."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Medical Report Generation",
            "text": "Medical report generation(Jing et al., 2018; Li et al., 2018), as one kind of image captioning (Vinyals et al., 2015; Rennie et al., 2017; Lu et al., 2017; Anderson et al., 2018), has received increasing attention from the research community. Some works focus on recording key information of the generation process via memory mechanism (Chen et al., 2020, 2021; Qin and Song, 2022; Wang et al., 2023). In addition, Liu et al. (2021c) proposed to utilize contrastive learning to distill information. Liu et al. (2021a) proposed to use curriculum learning to\nenhance the performance and Liu et al. (2021b) proposed to explore posterior and prior knowledge for report generation. Yang et al. (2021); Li et al. (2023); Huang et al. (2023) proposed to utilize the external knowledge graph (i.e., RadGraph (Jain et al., 2021)) for report generation.\nOther works focused on improving the clinical accuracy and faithfulness of the generated reports. Liu et al. (2019a); Lovelace and Mortazavi (2020); Miura et al. (2021); Nishino et al. (2022); Delbrouck et al. (2022) designed various kinds of rewards (e.g., entity matching score) to improve clinical accuracy via reinforcement learning. Tanida et al. (2023) proposed an explainable framework for report generation that could identify the abnormal areas of a given radiograph. Hou et al. (2023) proposed to combine both textual plans and radiographs to maintain clinical consistency. Additionally, Ramesh et al. (2022) and Bannur et al. (2023) focus on handling the temporal structure in radiology report generation, either removing the prior or learning from the historical records."
        },
        {
            "heading": "6.2 Graph Reasoning for Text Generation",
            "text": "Graph reasoning for text generation (Liu et al., 2019b; Tuan et al., 2022) tries to identify relevant knowledge from graphs and incorporate it into generated text sequences. Huang et al. (2020) proposed to construct a knowledge graph from the input document and utilize it to enhance the performance of abstractive summarization. Ji et al. (2020) proposed to incorporate commonsense knowledge for\nlanguage generation via multi-hop reasoning. Mu and Li (2022) proposed to combine both eventlevel and token-level from the knowledge graph to improve the performance."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose RECAP, which can capture both spatial and temporal information for generating precise and accurate radiology reports. To achieve precise attribute modeling in the generation process, we construct a disease progression graph containing both observations and fined-grained attributes which quantify the severity of diseases and devise a dynamic disease progression reasoning (PrR) mechanism to select observation-relevant attributes. Experimental results demonstrate the effectiveness of our proposed model in terms of generating precise and accurate radiology reports."
        },
        {
            "heading": "Limitations",
            "text": "Our proposed two-stage framework requires predefined observations and progressions for training, which may not be available for other types of radiographs. In addition, the outputs of Stage 1 are the prerequisite inputs of Stage 2, and thus, our framework may suffer from error propagation. Finally, although prior information is important in generating precise and accurate free-text reports, historical records are not always available, even in the two benchmark datasets. Our framework will still generate misleading free-text reports, conditioning on non-existent priors, as indicated in Ramesh et al. (2022). This might be mitigated through rule-based removal operations."
        },
        {
            "heading": "Ethics Statement",
            "text": "The MIMIC-ABN(Ni et al., 2020) and MIMICCXR (Johnson et al., 2019) datasets are publicly available benchmarks and have been automatically de-identified to protect patient privacy. Although our model improves the factual accuracy of generated reports, its performance still lags behind the practical deployment. The outputs of our model may contain false observations and diagnoses due to systematic biases. In this regard, we strongly urge the users to examine the generated output in real-world applications cautiously."
        },
        {
            "heading": "Acknolwedgments",
            "text": "This work was supported in part by General Program of National Natural Science Foundation of\nChina (Grant No. 82272086, 62076212), Guangdong Provincial Department of Education (Grant No. 2020ZDZX3043), Shenzhen Natural Science Fund (JCYJ20200109140820699 and the Stable Support Plan Program 20200925174052004), and the Research Grants Council of Hong Kong (15207920, 15207821, 15207122)."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Observation and Progression Statitics",
            "text": "There are 14 observations: No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Lesion, Lung Opacity, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, Pleural Other, Fracture, and Support Devices. Table\n5 lists the observation distributions annotated by CheXbert(Smit et al., 2020) in the train/valid/test split of two benchmarks and Table 7 shows the progression distributions in our experiments."
        },
        {
            "heading": "A.2 Spatial and Temporal Entity",
            "text": "Here are some of the spatial entities: healed, fractured, healing, nondisplaced, top, size, heart, normal, mediastinum, widening, contour, widened, consolidative, collapse, underlying, developing, fibrosis, thickening, biapical, blunting, indistinctness, asymmetrical, haziness, asymmetric, layering, subpulmonic, thoracentesis, trace, small, adjacent, tiny, atypical, developing, supervening, multifocal, correct, superimposed, patchy, and borderline. For temporal entities, we use the same settings of Bannur et al. (2023), which are: bigger, change, cleared, constant, decrease, decreased, decreasing, elevated, elevation, enlarged, enlargement, enlarging, expanded, greater, growing, improved, improvement, improving, increase, increased, increasing, larger, new, persistence, persistent, persisting, progression, progressive, reduced, removal, resolution, resolved, resolving, smaller, stability, stable, stably, unchanged, unfolded, worse, worsen, wors-\nened, worsening and unaltered."
        },
        {
            "heading": "A.3 Other Implementation Details",
            "text": "Data Preprocessing. We adopt the preprocessing setup used in Chen et al. (2020), and the minimum count of each token is set to 3/10 for MIMICABN/MIMIC-CXR, respectively. Other tokens are replaced with a special token [UNK]. Implementation Details of Stage 1 Training. Table 8 shows the hyperparameters used in Stage 1 training for two datasets. Note that ldi is the weight for observation detection, and the weights of observation classification and progression classification are both set to 1. In addition, two data augmentation methods are used during training. Specifically,\nwe first resize an input image to 256 \u00d7 256, and then the image is randomly cropped to 224\u00d7 224, and finally, we flip the image horizontally with a probability of 0.5. We select the best checkpoint based on the Macro-F1 of abnormal observations at this stage."
        },
        {
            "heading": "Hyperparameter MIMIC-ABN MIMIC-CXR",
            "text": ""
        },
        {
            "heading": "Implementation Details of Stage 2 Training. As",
            "text": "the variant w/o OP and the variant w/o Obs in Table 4 are not trained in Stage 1, they are trained with more epochs (i.e., 10 epochs)."
        },
        {
            "heading": "A.4 Other Experimental Results",
            "text": "We show experimental results of observation prediction and progression prediction during Stage 1\ntraining in Table 9 and Table 10, respectively."
        }
    ],
    "title": "RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning",
    "year": 2023
}