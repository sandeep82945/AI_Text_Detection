{
    "abstractText": "Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MUG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-theart unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MUG is released at https://github. com/lujiaying/MUG-Bench with the data, tutorials, and implemented baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaying Lu"
        },
        {
            "affiliations": [],
            "name": "Yongchen Qian"
        },
        {
            "affiliations": [],
            "name": "Shifan Zhao"
        }
    ],
    "id": "SP:f5e136344e6cde16b8189eea023efdb663a36875",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "ICCV.",
            "year": 2015
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "ELECTRA: pretraining text encoders as discriminators rather than generators",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Janez Dem\u0161ar."
            ],
            "title": "Statistical comparisons of classifiers over multiple data sets",
            "venue": "The Journal of Machine learning research.",
            "year": 2006
        },
        {
            "authors": [
                "Xiangjue Dong",
                "Jiaying Lu",
                "Jianling Wang",
                "James Caverlee."
            ],
            "title": "Closed-book question generation via contrastive learning",
            "venue": "EACL.",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers",
            "year": 2020
        },
        {
            "authors": [
                "Nick Erickson",
                "Jonas Mueller",
                "Alexander Shirkov",
                "Hang Zhang",
                "Pedro Larroy",
                "Mu Li",
                "Alexander Smola."
            ],
            "title": "Autogluon-tabular: Robust and accurate automl for structured data",
            "venue": "arXiv preprint arXiv:2003.06505.",
            "year": 2020
        },
        {
            "authors": [
                "Nick Erickson",
                "Xingjian Shi",
                "James Sharpnack",
                "Alexander Smola."
            ],
            "title": "Multimodal automl for image, text and tabular data",
            "venue": "KDD.",
            "year": 2022
        },
        {
            "authors": [
                "Costa Georgantas",
                "Jonas Richiardi."
            ],
            "title": "Multiview omics translation with multiplex graph neural networks",
            "venue": "WWW.",
            "year": 2022
        },
        {
            "authors": [
                "Xiawei Guo",
                "Yuhan Quan",
                "Huan Zhao",
                "Quanming Yao",
                "Yong Li",
                "Weiwei Tu."
            ],
            "title": "Tabgnn: Multiplex graph neural network for tabular data prediction",
            "venue": "dlp-kdd.",
            "year": 2021
        },
        {
            "authors": [
                "Yubraj Gupta",
                "Ji-In Kim",
                "Byeong Chae Kim",
                "GooRak Kwon"
            ],
            "title": "Classification and graphical analysis of alzheimer\u2019s disease and its prodromal stage using multimodal features from structural, diffusion, and functional neuroimaging data and the apoe geno",
            "year": 2020
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec."
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Huan He",
                "Shifan Zhao",
                "Yuanzhe Xi",
                "Joyce C Ho."
            ],
            "title": "Meddiff: Generating electronic health records using accelerated denoising diffusion model",
            "venue": "arXiv preprint arXiv:2302.04355.",
            "year": 2023
        },
        {
            "authors": [
                "Stefan Hegselmann",
                "Alejandro Buendia",
                "Hunter Lang",
                "Monica Agrawal",
                "Xiaoyi Jiang",
                "David Sontag."
            ],
            "title": "Tabllm: Few-shot classification of tabular data with large language models",
            "venue": "AISTATS. PMLR.",
            "year": 2023
        },
        {
            "authors": [
                "MD Zakir Hossain",
                "Ferdous Sohel",
                "Mohd Fairuz Shiratuddin",
                "Hamid Laga."
            ],
            "title": "A comprehensive survey of deep learning for image captioning",
            "venue": "ACM Computing Surveys.",
            "year": 2019
        },
        {
            "authors": [
                "Mahesh G Huddar",
                "Sanjeev S Sannakki",
                "Vijay S Rajpurohit."
            ],
            "title": "An ensemble approach to utterance level multimodal sentiment analysis",
            "venue": "CTEMS.",
            "year": 2018
        },
        {
            "authors": [
                "William C Sleeman Iv",
                "Rishabh Kapoor",
                "Preetam Ghosh."
            ],
            "title": "Multimodal classification: Current landscape, taxonomy and future directions",
            "venue": "CSUR.",
            "year": 2021
        },
        {
            "authors": [
                "A Kautzky",
                "T Vanicek",
                "C Philippe",
                "GS Kranz",
                "W Wadsak",
                "M Mitterhauser",
                "A Hartmann",
                "A Hahn",
                "M Hacker",
                "D Rujescu"
            ],
            "title": "Machine learning classification of adhd and hc by multimodal serotonergic data. Translational psychiatry",
            "year": 2020
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "Tie-Yan Liu."
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Mackenzie Leake",
                "Hijung Valentina Shin",
                "Joy O Kim",
                "Maneesh Agrawala."
            ],
            "title": "Generating audiovisual slideshows from text articles using word concreteness",
            "venue": "CHI.",
            "year": 2020
        },
        {
            "authors": [
                "Jeungchan Lee",
                "Ishtiaq Mawla",
                "Jieun Kim",
                "Marco L Loggia",
                "Ana Ortiz",
                "Changjin Jung",
                "Suk-Tak Chan",
                "Jessica Gerber",
                "Vincent J Schmithorst",
                "Robert R Edwards"
            ],
            "title": "Machine learning-based prediction of clinical pain using multimodal neuroimaging",
            "year": 2019
        },
        {
            "authors": [
                "Jiao Li",
                "Xing Xu",
                "Wei Yu",
                "Fumin Shen",
                "Zuo Cao",
                "Kai Zuo",
                "Heng Tao Shen."
            ],
            "title": "Hybrid fusion with intra-and cross-modality attention for image-recipe retrieval",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "ICML.",
            "year": 2023
        },
        {
            "authors": [
                "Mingzhe Li",
                "Xiuying Chen",
                "Shen Gao",
                "Zhangming Chan",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Vmsmo: Learning to generate multimodal summary for videobased news articles",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Yiwei Lyu",
                "Xiang Fan",
                "Zetian Wu",
                "Yun Cheng",
                "Jason Wu",
                "Leslie Yufan Chen",
                "Peter Wu",
                "Michelle A Lee",
                "Yuke Zhu"
            ],
            "title": "Multibench: Multiscale benchmarks for multimodal representation learning",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Richard Liaw",
                "Eric Liang",
                "Robert Nishihara",
                "Philipp Moritz",
                "Joseph E Gonzalez",
                "Ion Stoica."
            ],
            "title": "Tune: A research platform for distributed model selection and training",
            "venue": "arXiv preprint arXiv:1807.05118.",
            "year": 2018
        },
        {
            "authors": [
                "Weiming Lin",
                "Qinquan Gao",
                "Jiangnan Yuan",
                "Zhiying Chen",
                "Chenwei Feng",
                "Weisheng Chen",
                "Min Du",
                "Tong Tong"
            ],
            "title": "Predicting alzheimer\u2019s disease conversion from mild cognitive impairment using an extreme learning machine-based grading method with",
            "year": 2020
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo."
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "ICCV.",
            "year": 2021
        },
        {
            "authors": [
                "Jiaying Lu",
                "Jinmeng Rao",
                "Kezhen Chen",
                "Xiaoyuan Guo",
                "Yawen Zhang",
                "Baochen Sun",
                "Carl Yang",
                "Jie Yang."
            ],
            "title": "Evaluation and mitigation of agnosia in multimodal large language models",
            "venue": "arXiv preprint arXiv:2309.04041.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaying Lu",
                "Xin Ye",
                "Yi Ren",
                "Yezhou Yang."
            ],
            "title": "Good, better, best: Textual distractors generation for multiple-choice visual question answering via reinforcement learning",
            "venue": "CVPR Workshop on ODRUM.",
            "year": 2022
        },
        {
            "authors": [
                "Liqiang Nie",
                "Mengzhao Jia",
                "Xuemeng Song",
                "Ganglu Wu",
                "Harry Cheng",
                "Jian Gu."
            ],
            "title": "Multimodal activation: Awakening dialog robots without wake words",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "John Edison Arevalo Ovalle",
                "Thamar Solorio",
                "Manuel Montes-y-G\u00f3mez",
                "Fabio A. Gonz\u00e1lez."
            ],
            "title": "Gated multimodal units for information fusion",
            "venue": "ICLR.",
            "year": 2017
        },
        {
            "authors": [
                "Ankur P Parikh",
                "Xuezhi Wang",
                "Sebastian Gehrmann",
                "Manaal Faruqui",
                "Bhuwan Dhingra",
                "Diyi Yang",
                "Dipanjan Das."
            ],
            "title": "ToTTo: A controlled table-totext generation dataset",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning",
            "year": 2011
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wenhui Wang",
                "Li Dong",
                "Yaru Hao",
                "Shaohan Huang",
                "Shuming Ma",
                "Furu Wei."
            ],
            "title": "Kosmos-2: Grounding multimodal large language models to the world",
            "venue": "arXiv preprint arXiv:2306.14824.",
            "year": 2023
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall."
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988.",
            "year": 2022
        },
        {
            "authors": [
                "Fang Qingyun",
                "Han Dapeng",
                "Wang Zhaokui."
            ],
            "title": "Cross-modality fusion transformer for multispectral object detection",
            "venue": "arXiv preprint arXiv:2111.00273.",
            "year": 2021
        },
        {
            "authors": [
                "Yubin Qu",
                "Fang Li",
                "Long Li",
                "Xianzhen Dou",
                "Hongmei Wang"
            ],
            "title": "Can we predict student performance based on tabular and textual data",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever."
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "ICML.",
            "year": 2021
        },
        {
            "authors": [
                "Shailaja Keyur Sampat",
                "Maitreya Patel",
                "Subhasish Das",
                "Yezhou Yang",
                "Chitta Baral."
            ],
            "title": "Reasoning about actions over visual and linguistic modalities: A survey",
            "venue": "arXiv preprint arXiv:2207.07568.",
            "year": 2022
        },
        {
            "authors": [
                "Claude Elwood Shannon."
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell system technical journal.",
            "year": 1948
        },
        {
            "authors": [
                "Xingjian Shi",
                "Jonas Mueller",
                "Nick Erickson",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Multimodal automl on structured tables with text fields",
            "venue": "8th ICML Workshop on AutoML.",
            "year": 2021
        },
        {
            "authors": [
                "Luis R Soenksen",
                "Yu Ma",
                "Cynthia Zeng",
                "Leonard Boussioux",
                "Kimberly Villalobos Carballo",
                "Liangyuan Na",
                "Holly M Wiberg",
                "Michael L Li",
                "Ignacio Fuentes",
                "Dimitris Bertsimas"
            ],
            "title": "Integrated multimodal artificial intelligence framework for healthcare",
            "year": 2022
        },
        {
            "authors": [
                "Krishna Srinivasan",
                "Karthik Raman",
                "Jiecao Chen",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Baohua Sun",
                "Lin Yang",
                "Wenhan Zhang",
                "Michael Lin",
                "Patrick Dong",
                "Charles Young",
                "Jason Dong."
            ],
            "title": "Supertml: Two-dimensional word embedding for the precognition on structured tabular data",
            "venue": "CVPR Workshops.",
            "year": 2019
        },
        {
            "authors": [
                "Quan Sun",
                "Qiying Yu",
                "Yufeng Cui",
                "Fan Zhang",
                "Xiaosong Zhang",
                "Yueze Wang",
                "Hongcheng Gao",
                "Jingjing Liu",
                "Tiejun Huang",
                "Xinlong Wang"
            ],
            "title": "Generative pretraining in multimodality",
            "year": 2023
        },
        {
            "authors": [
                "Shardul Suryawanshi",
                "Bharathi Raja Chakravarthi",
                "Mihael Arcan",
                "Paul Buitelaar."
            ],
            "title": "Multimodal meme dataset (multioff) for identifying offensive content in image and text",
            "venue": "Proceedings of the second workshop on trolling, aggression and cyberbullying.",
            "year": 2020
        },
        {
            "authors": [
                "Golsa Tahmasebzadeh",
                "Endri Kacupaj",
                "Eric M\u00fcllerBudack",
                "Sherzod Hakimov",
                "Jens Lehmann",
                "Ralph Ewerth."
            ],
            "title": "Geowine: Geolocation based wiki, image, news and event retrieval",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Alon Talmor",
                "Ori Yoran",
                "Amnon Catav",
                "Dan Lahav",
                "Yizhong Wang",
                "Akari Asai",
                "Gabriel Ilharco",
                "Hannaneh Hajishirzi",
                "Jonathan Berant."
            ],
            "title": "Multimodalqa: complex question answering over text, tables and images",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Jialin Tian",
                "Kai Wang",
                "Xing Xu",
                "Zuo Cao",
                "Fumin Shen",
                "Heng Tao Shen."
            ],
            "title": "Multimodal disentanglement variational autoencoders for zero-shot crossmodal retrieval",
            "venue": "SIGIR.",
            "year": 2022
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "JMLR.",
            "year": 2008
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Hoa Trong Vu",
                "Claudio Greco",
                "Aliia Erofeeva",
                "Somayeh Jafaritazehjan",
                "Guido Linders",
                "Marc Tanti",
                "Alberto Testoni",
                "Raffaella Bernardi",
                "Albert Gatt."
            ],
            "title": "Grounded textual entailment",
            "venue": "COLING.",
            "year": 2018
        },
        {
            "authors": [
                "Minjie Yu Wang."
            ],
            "title": "Deep graph library: Towards efficient and scalable deep learning on graphs",
            "venue": "ICLR workshop on representation learning on graphs and manifolds.",
            "year": 2019
        },
        {
            "authors": [
                "Tongxin Wang",
                "Wei Shao",
                "Zhi Huang",
                "Haixu Tang",
                "Jie Zhang",
                "Zhengming Ding",
                "Kun Huang."
            ],
            "title": "Mogonet integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification",
            "venue": "Nature Communica-",
            "year": 2021
        },
        {
            "authors": [
                "Te-Lin Wu",
                "Shikhar Singh",
                "Sayan Paul",
                "Gully Burns",
                "Nanyun Peng."
            ],
            "title": "Melinda: A multimodal dataset for biomedical experiment method classification",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Xueqing Wu",
                "Jiacheng Zhang",
                "Hang Li."
            ],
            "title": "Textto-table: A new way of information extraction",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Xu",
                "Xiaofeng Hou",
                "Jiacheng Liu",
                "Chao Li",
                "Tianhao Huang",
                "Xiaozhi Zhu",
                "Mo Niu",
                "Lingyu Sun",
                "Peng Tang",
                "Tongqiao Xu"
            ],
            "title": "Mmbench: Benchmarking end-to-end multi-modal dnns and understanding their hardware-software implications",
            "year": 2022
        },
        {
            "authors": [
                "Keyang Xu",
                "Mike Lam",
                "Jingzhi Pang",
                "Xin Gao",
                "Charlotte Band",
                "Piyush Mathur",
                "Frank Papay",
                "Ashish K Khanna",
                "Jacek B Cywinski",
                "Kamal Maheshwari"
            ],
            "title": "Multimodal machine learning for automated icd coding. In Machine learning for healthcare",
            "year": 2019
        },
        {
            "authors": [
                "Carl Yang",
                "Jieyu Zhang",
                "Haonan Wang",
                "Sha Li",
                "Myungwan Kim",
                "Matt Walker",
                "Yiou Xiao",
                "Jiawei Han."
            ],
            "title": "Relation learning on social networks with multi-modal graph edge variational autoencoders",
            "venue": "WSDM.",
            "year": 2020
        },
        {
            "authors": [
                "Amir Zadeh",
                "Minghai Chen",
                "Soujanya Poria",
                "Erik Cambria",
                "Louis-Philippe Morency."
            ],
            "title": "Tensor fusion network for multimodal sentiment analysis",
            "venue": "EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Hanqing Zeng",
                "Hongkuan Zhou",
                "Ajitesh Srivastava",
                "Rajgopal Kannan",
                "Viktor Prasanna."
            ],
            "title": "Graphsaint: Graph sampling based inductive learning method",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Zhixiong Zeng",
                "Shuai Wang",
                "Nan Xu",
                "Wenji Mao."
            ],
            "title": "Pan: Prototype-based adaptive network for robust cross-modal retrieval",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Dong Zhang",
                "Xincheng Ju",
                "Junhui Li",
                "Shoushan Li",
                "Qiaoming Zhu",
                "Guodong Zhou."
            ],
            "title": "Multimodal multi-label emotion detection with modality and label dependence",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yi Zhang",
                "Mingyuan Chen",
                "Jundong Shen",
                "Chongjun Wang."
            ],
            "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Yitan Zhu",
                "Thomas Brettin",
                "Fangfang Xia",
                "Alexander Partin",
                "Maulik Shukla",
                "Hyunseung Yoo",
                "Yvonne A Evrard",
                "James H Doroshow",
                "Rick L Stevens."
            ],
            "title": "Converting tabular data into images for deep learning with convolutional neural networks",
            "venue": "Scien-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MUG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-theart unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MUG is released at https://github. com/lujiaying/MUG-Bench with the data, tutorials, and implemented baselines."
        },
        {
            "heading": "1 Introduction",
            "text": "The world surrounding us is multimodal. Realworld data is often stored in well-structured databases that contain tabular fields, with textual and visual fields co-occurring. Numerous automated classification systems have been deployed on these multimodal data to provide efficient and scalable services. For instance, medical decision support systems (Soenksen et al., 2022) utilize patients\u2019 electronic health record data that contains tabular inputs (e.g., ages, genders, races), textual inputs (e.g., notes, prescriptions, written reports), and visual inputs (e.g., x-rays, magnetic resonance imaging, ct-scans) to help precise disease prediction. Similarly, e-commerce product classification systems (Erickson et al., 2022) categorize products based on their categorical/numerical quanti-\n*These authors contributed equally to this work\nties, textual descriptions, and teasing pictures, thus enhancing user search experiences and recommendation outcomes. Therefore, accurate classification models for table-text-image input are desired.\nDeep neural networks have shown significant progress in multimodal learning tasks, such as CLIP (Radford et al., 2021) for image-text retrieval and Fuse-Transformer (Shi et al., 2021) for tabular-with-text classification. This progress has been made with large-scale datasets provided to train the data-eager models. So far, there exist many datasets (Ovalle et al., 2017; Wu et al., 2021; Shi et al., 2021; Qu et al., 2022; Lin et al., 2020; Lee et al., 2019; Kautzky et al., 2020; Srinivasan et al., 2021) that cover one or two modalities. However, the progress in tabular-text-image multimodal learning lags due to the lack of available resources. In this paper, we provide a multimodal benchmark, namely MUG, that contains eight datasets for researchers to examine their al-\ngorithms\u2019 multimodal perception ability. MUG contains data samples with tabular, textual, and visual fields that are collected from various genres of games. We have made necessary cleaning, transformations, and modifications to the original data to make MUG easy to use. We further conduct comprehensive data analysis to demonstrate the diverse and multimodal-dependent properties of MUG.\nMUG can enable future studies of many multimodal tasks, and we focus on the multimodal classification task in this paper. For the primary classification evaluation, we incorporate two stateof-the-art (SOTA) unimodal classifiers for each of the three input modalities, resulting in a total of six, along with two SOTA multimodal classifiers. We also propose a novel baseline model MUGNET based on the graph attention network (Velic\u030ckovic\u0301 et al., 2018). In addition to capturing the interactions among the three input modalities, our MUGNET takes the sample-wise similarity into account, yielding a compatible performance to existing multimodal classifiers. We further conduct efficiency evaluations to reflect the practical requirements of many machine learning systems."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Multimodal Classification Datasets with Tabular, Textual, and Visual Fields",
            "text": "Machine learning models in real-world applications need to deal with multimodal data that contains both tabular, textual, and visual fields. Due to privacy or license issues, there exist very few datasets that cover these three modalities. To the best of our knowledge, PetFinder1 is one of the few publicly available datasets. HAIM-MIMIC-MM (Soenksen et al., 2022) is a multimodal healthcare dataset containing tabular, textual, image, and time-series fields. However, only credentialed users can access HAIM-MIMIC-MM. On the other hand, there exist many datasets that cover two modalities (out of table, text, and image modalities). The most common datasets are the ones with both textual and visual features (MM-IMDB (Ovalle et al., 2017), V-SNLI (Vu et al., 2018), MultiOFF (Suryawanshi et al., 2020), WIT (Srinivasan et al., 2021), MELINDA (Wu et al., 2021), etc.). Meanwhile, (Shi et al., 2021; Xu et al., 2019; Qu et al., 2022) provide benchmark datasets for table and text modalities, For the combination of table and image\n1PetFinder: https://www.kaggle.com/competitions/ petfinder-adoption-prediction/overview\nmodalities, there are a bunch of datasets from the medical domain (Lin et al., 2020; Lee et al., 2019; Kautzky et al., 2020; Gupta et al., 2020). Other than the mentioned table, text, and image modalities, multimodal learning has also been conducted in time-series, speech, and video modalities (Zhang et al., 2022, 2020; Li et al., 2020)."
        },
        {
            "heading": "2.2 Multimodal Classifiers for Tabular, Textual, and Visual Fields",
            "text": "Fusion is the core technology for multimodal classification problems, which integrates data from each input modality and utilizes fused representations for downstream tasks (classification, regression, retrieval, etc.). Based on the stage of fusion (Iv et al., 2021), existing methods can be divided into early, late, or hybrid fusion. Early fusion models (Sun et al., 2019; Shi et al., 2021; Zhu et al., 2021) usually fuse raw data or extracted features before they are fed into the learnable classifier, while late fusion models (Erickson et al., 2020; Soenksen et al., 2022; Lu et al., 2022) employ separate learnable encoders for all input modalities and then fuse these learned representations into the learnable classifier. Hybrid fusion models are more flexible, allowing for modality fusion to occur at different stages simultaneously (Qingyun et al., 2021; Li et al., 2021). Although existing works have demonstrated remarkable capability in modeling feature interactions, they ignore signals of sample proximity, such as the tendency for within a group to exhibit similar behavior or share common interests. In response, we propose our approach, MUGNET, which dynamically constructs graphs based on sample similarity and effectively combines graphical representation learning with multimodal fusion. Our approach draws inspiration from pioneering graph neural networks (Guo et al., 2021; Wang et al., 2021; Georgantas and Richiardi, 2022), which have achieved success in various classification tasks."
        },
        {
            "heading": "3 MUG: the benchmark",
            "text": "We create and release MUG with eight datasets for multimodal classification with tabular, text, and image fields to the community for future studies. Raw data and examples of how to appropriately load the data are provided in https://github. com/lujiaying/MUG-Bench. MUG is under the\n\"CC BY-NC-SA 4.0\" license2, and is designated to use for research purposes."
        },
        {
            "heading": "3.1 Data Sources",
            "text": "To collect multiple and large-scale datasets that support multimodal automated machine learning, we collected data from four games: Pok\u00e9mon, Hearthstone, League of Legends, and CounterStrike: Global Offensive. We deliberately chose these video games as they have distinct video game genres (e.g., role-playing, card, multiplayer online battle arena, and shooting). All of these datasets were gathered from publicly accessible web content by October 2022, and there are no licensing issues associated with them. They do not contain any user-specific private information. In particular,\n\u2022 Pok\u00e9mon is a video game centered around fictional creatures called \"Pocket Monsters\" that trainers capture and train to battle each other. Pok\u00e9mon is owned by Nintendo Co., Ltd., Creatures Inc., and Game Freak Inc. Pok\u00e9mon data is collected from https://bulbapedia. bulbagarden.net/wiki under the \u201cCC BY-NCSA 2.5\u201d license. \u2022 HearthStone is an online collectible card game developed by Blizzard Entertainment, Inc., featuring strategic gameplay where players build decks and compete against each other using a variety of spells, minions, and abilities. Hearthstone data is collected from https://hearthstonejson. com/ under the \u201cCC0\u201d license. \u2022 League of Legends (LoL) is a multiplayer online battle arena (MOBA) video game developed by Riot Games, Inc. where teams of players compete in fast-paced matches, utilizing unique champions with distinct abilities to achieve victory. LoL data is collected from https://lolskinshop. com/product-category/lol-skins/. \u2022 Counter-Strike: Global Offensive (CS:GO) is a multiplayer first-person shooter video game developed by Valve Corporation and Hidden Path Entertainment, Inc., where players join teams to compete in objective-based matches involving tactical gameplay and precise shooting. CS:GO data is collected from https://www. csgodatabase.com/.\n2CC BY-NC-SA 4.0: https://creativecommons.org/ licenses/by-nc-sa/4.0/"
        },
        {
            "heading": "3.2 Creation Process",
            "text": "To create MUG, we first identify the categorical columns that can serve as the prediction targets. The reasons for choosing these targets are elaborated in Appendix B.1. We obtain a total of eight datasets from the four games, including pkm_t1 and pkm_t2 from Pok\u00e9mon; hs_ac, hs_as, hs_mr, and hs_ss; lol_sc from LoL; csg_sq from CS:GO.\nThen, we conduct necessary data cleaning and verification to ensure the quality of MUG. To alleviate the class imbalance issue in some datasets (e.g., one class may contain less than 10 samples), we re-group sparse classes into one new class that contains enough samples for training and evaluation. For missing values of target categorical columns, we manually assign a special None_Type as one new class of the dataset. For missing values of input columns, we keep them blank to allow classification models to decide the best imputation strategies. Moreover, we also anonymize columns that cause data leakage (e.g., the id column in hs_as is transformed to anonymous_id column).\nAfter the abovementioned preprocessing, we split the dataset into training, validation, and testing sets with an 80/5/15 ratio. Each dataset compromises between 1K and 10K samples, associated with tabular, textual, and visual features. An overview of these datasets is shown in Table 1. This broad range of sample sizes and diverse data types ensures the representation of a wide variety of instances, allowing for robust model training and evaluation across different data modalities."
        },
        {
            "heading": "3.3 Benchmark Analysis",
            "text": "The MUG benchmark is curated to meet the following list of criteria: (i) Publicly available data and baseline models can facilitate reproducible experiments and accelerate the development of advanced models. (ii) Diversity should be preserved in the benchmark. We do not want the benchmark to have a bias toward certain data or class distribution. The benchmark with a high variety of datasets aids the research community in examining the robustness of models. (iii) Multimodal-dependent classification is expected for each dataset. Datasets that are too easy to be classified by a single modality are not suitable, since they would hide the gap between the multimodal perceptron abilities of models.\nWe conduct a rich set of analyses to verify\nthat MUG indeed satisfied the diversity requirement. Figure 2 shows the properties of datasets in multi-aspect. For the classification task properties(Figure 2a), we adopt the Shannon equitability index (Shannon, 1948) (definition in Appendix B.2) to measure the class balance ratio. The index ranges from 0 to 1, and the larger the Shannon equitability index, the more balanced the dataset is. For the feature properties, we include percentages of missing features (Figure 2b), means and standard deviations of numerical features (Figure 2c), category counts of categorical features (Figure 2d), distributions of word counts per sample (Figure 2e), and distributions of image mean RGB pixel values (Figure 2f). In these figures, we merged duplicated results from some datasets into one group to make the presentation clean and neat (i.e., pkm_t1, pkm_t2 are grouped into pkm; hs_ac, hs_as, hs_mr, hs_ss are grouped into hs). As shown in the figures, the eight datasets reflect real-world problems that are diverse and challenging. We further study the correlation between category labels and input modalities in MUG. Referring to the t-SNE projection of multimodal embeddings in Figure 7, it is evident that MUG exhibits a strong multimodal dependency. In this case, the use of unimodal in-\nformation alone is inadequate to differentiate between samples belonging to different classes. For a more comprehensive analysis, we encourage interested readers to refer to the details provided in Appendix B.3"
        },
        {
            "heading": "4 Baseline Models",
            "text": "We employ several state-of-the-art unimodal classifiers and multimodal classifiers in the experiments. We also proposed our own graph neural networkbased multimodal classifier as one baseline model to be compared."
        },
        {
            "heading": "4.1 Existing State-Of-The-Art Classifiers",
            "text": "In this paper, we adopt the following SOTA unimodal classifiers in the experiments: Tabular modality classifiers: \u2022 GBM (Ke et al., 2017) is a light gradient boosting\nframework based on decision trees. Due to its ability to capture nonlinear relationships, handle complex tabular data, provide feature importance insights, and robustness to outliers and missing values, GBM has achieved state-of-the-art results in various tabular data tasks, \u2022 tabMLP (Erickson et al., 2020) is a multilayer perceptron (MLP) model that is specifically designed to work with tabular data. tabMLP contains multiple separate embedding layers to handle categorical and numerical input features.\nTextual modality classifiers: \u2022 RoBERTa (Liu et al., 2019) is a robustly\noptimized transformer-based masked language model (masked LM). RoBERTa builds upon the success of BERT by refining and optimizing its training methodology, and achieves superior performance on a wide range of NLP tasks. \u2022 Electra (Clark et al., 2020) is another variant of the transformer-based model, which differs from traditional masked LMs like BERT or RoBERTa. While masked LMs randomly mask tokens and predict these masked tokens, Electra is trained as a discriminator to identify whether each token is replaced by a generator.\nVisual modality classifiers: \u2022 ViT (Dosovitskiy et al., 2020) extends the trans-\nformer model to image data, by dividing the input image into a grid of patches and processing each patch as a token. Empirical results show that ViT outperforms previous SOTA convolutional neural networks in image classification tasks.\n\u2022 SWIN (Liu et al., 2021) is another vision transformer that benefits from hierarchical architecture and the shifted windowing scheme. The proposed techniques address several key challenges when adapting transformers in image modality, such as large variations in the scale of visual entities and the high resolution of pixels.\nIn practice, we adopt the following multimodal classifiers in the experiments:\n\u2022 AutoGluon (Erickson et al., 2022) is an ensemble-learning model for multimodal classification and regression tasks. The concept of AutoGluon is stack ensembling, where the final prediction is obtained by combining intermediate predictions from multiple base models. To handle multimodal classification, SOTA unimodal classifiers (e.g., tree models, MLPs, CNNs, transformers) are adopted as base models. \u2022 AutoMM (Shi et al., 2021) is a late-fusion model where separate neural operations are conducted on each data type and extracted high-level representations are aggregated near the output layer. Specifically, MLPs are used for tabular modality, and transformers are used for text and image modalities. After that, dense vector embeddings from the last layer of each network are pooled into one vector, and the final prediction is obtained via an additional two-layer MLP."
        },
        {
            "heading": "4.2 MUGNET",
            "text": "MUGNET is our own multimodal classifier which is further proposed as a competitor to existing models. We propose three key components to make MUGNET a powerful graph neural network for the multimodal classification task. They are adaptive multiplex graph construction module, GAT encoder module, and attention-based fusion module, as shown in Figure 3. Firstly, adaptive multiplex graphs are constructed to reflect sample-wise similarity within each modality. Then, separate GAT encoders (Velic\u030ckovic\u0301 et al., 2018) are employed to obtain dense embeddings of samples, by propagating information between neighbors. Finally, tabular, text and image embeddings are combined by intermodality attention to obtaining the fused embedding for multimodal classification. GNNs (Yang et al., 2020; Guo et al., 2021) show great capability to leverage the graph structure, propagate information, integrate features, and capture higher-order relationships. This leads to accurate and robust classification performance across various domains.\nIn this work, we propose to regard the whole samples as a correlation network (Wang et al., 2021; Georgantas and Richiardi, 2022) that represents sample-to-sample similarities, while existing multimodal classifiers rarely consider this before. Adaptive multiplex graph construction module. Following the notation defined in \u00a75.1, the adaptive multiplex graph construction module first utilizes pre-processing pipelines (e.g., monotonically increasing integer mapping for categorical inputs, no alteration for numerical inputs) or pre-trained feature extractors (e.g., CLIP (Radford et al., 2021) for text and image inputs) to obtain dense multimodal features F = f(XL) \u2208 RN\u00d7(d\nt+ds+di), where F = {F t,Fs,F i} denotes feature matrices for tabular, text, and image modalities. The adaptive multiplex graph construction module then derives multiplex sample-wise similarity graph G = {Gt,Gs,Gi} = {(At,F t), (As,Fs), (Ai,F i)}, where each modality-specific adjacency matrix Am \u2208 RN\u00d7N ,\u2200m \u2208 {t, s, i} is calculated based on the multimodal features\nAmi,j = sim(Fmi ,Fmj ). (1)\nIt is worth noting that the sample-wise similarity function sim is adaptive, and is chosen from cosine similarity, radial basis function (RBF) kernel, or k-nearest neighbor. For these modalityspecific graphs, we use separate hyperparameters (e.g., threshold for score-based functions, or the value of k for k-nearest neighbor) to control their sparsity properties. The similarity function and its associated hyperparameters are determined through hyperparameter tuning (Liaw et al., 2018) on the\nheld-out validation set, so that the multiplex graph construction is adaptive to any downstream task.\nGAT encoder module. We use the powerful multi-head graph attention neural network (GAT) (Velic\u030ckovic\u0301 et al., 2018) as the encoder to obtain structure-aware representations of samples. Separate GATs are employed for each view of the multiple graph, so that Hm = GAT (Am,Fm; \u03b8), where Hm \u2208 RN\u00d7dmh , and \u03b8 represents the learnable parameters of the GAT encoder. We want to state there is no information leakage in MUGNET, because we follow the inductive learning setting of GNNs (Hamilton et al., 2017) where the GAT encoder is trained on the multiplex graph G derived from labeled training samples XL, and new unseen multiplex graph is derived from all samples XL \u222a XU at the inference stage. Furthermore, we adopt a graph sampling technique (GraphSAINT (Zeng et al., 2019)) during the GAT training process, to improve the efficiency and generalization. The graph sampling technique essentially samples a subgraph by random walks for each training step, thus the \u201cneighbor explosion\u201d issue is alleviated with a constrained number of neighbors per node and the variance of GAT is reduced with fewer outliers or noise in the sampled graph.\nAttention-based fusion module. After we obtain the structure-aware embeddings of samples from the tabular, text, and image modalities Ht,Hs,Hi, the attention-based fusion module is responsible for fusing them into one single embedding via the attention-based fusion module. The attention weight \u03b1mj \u2208 R for j-th sample of modality m is\ncomputed as:\n\u03b1mj = exp(emj )\u2211\nm\u2032\u2208{t,s,j} exp(e m\u2032 j )\n, (2)\nemj = wa2 \u00b7 tanh(Wma1h m j ), (3)\nwhere emj \u2208 R denotes the unnormalized attention weight, wa2 \u2208 Rd m a \u00d71,Wa1 \u2208 Rd m h \u00d7d m a denote learnable parameters, and hmj \u2208 Rd m h denotes the j-th row of Hm (i.e., embedding of j-th sample of modality m). The fused embedding of j-th sample is then calculated by:\nhj = \u03b1 t jh t j + \u03b1 s jh s j + \u03b1 i jh i i. (4)\nThe fused embedding hj incorporates crossmodalities interactions and provides a complete context for the downstream tasks. An additional two-layer MLP is trained to predict the category of j-th sample y\u0302j = softmax(Wcls2 \u00b7 LeakyReLU(Wcls1hj)). We adopt cross-entropy between prediction y\u0302 and target y as MUGNET\u2019s loss function."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Problem Definition",
            "text": "Given a finite set of categories Y and labeled training pairs (xi, yi) \u2208 XL \u00d7Y , multimodal classification aims at finding a classifier f\u0302 : XL \u2192 Y such that y\u0302j = f\u0302(xj) is a good approxmiation of the unknown label yj for unseen sample xj \u2208 XU . It is worth noting that the each multimodal sample x \u2208 XL \u222a XU consists of tabular fields t, textual fields s, and image fields i (i.e., x = {t, s, i})."
        },
        {
            "heading": "5.2 Experimental Setup",
            "text": "We use the official training, validation, and testing splits provided by MUG to conduct experiments. We choose the log-loss and accuracy to evaluate model performance, since these metrics are reasonable and commonly used in previous studies. For comparable and reproducible results, all models are trained and tested using the same hardware. Specifically, the machine is equipped with 16 Intel Xeon Gold 6254 CPUs (18 cores per CPU) and one 24GB TITAN RTX GPU. We add an 8-hour time limitation for the training process to reflect realworld resource constraints. The implementation and hyperparameter details of evaluated models are put in Appendix C."
        },
        {
            "heading": "5.3 Performance Comparisons",
            "text": "Table 2a and 2b show the performance of all evaluated models on MUG. As can be seen, multimodal classifiers (except AutoMM) consistently outperform unimodal classifiers in both log-loss and accuracy. It demonstrates that the classification tasks in MUG are multimodal-dependent where each modality only conveys partial information about the required outputs. Among the three multimodal classifiers we used, AutoGluon and MUGNET are the top-2 models with well-matched performances. In Table 2a and 2b, AutoGluon achieves the best performance eight times, while MUGNET also achieves the best performance eight times. More specifically, AutoGluon is superior in log-loss whereas MUGNET has better accuracy scores. AutoMM performs the worst among multimodal classifiers, and it sometimes underperforms unimodal classifiers. Considering that AutoMM trains powerful deep neural networks on a small scale of datasets and we have observed the gap between the training loss and validation loss, it is highly possible that AutoMM is overfitting. While AutoGluon and MUGNET also adopt deep neural networks as base models, they are more robust since AutoGluon proposes a repeated bagging strategy and MUGNET utilizes graph sampling techniques to avoid overfitting. Among unimodal classifiers, tabular models seem to outperform textual and visual models in most cases (six out of eight datasets). There is a slight performance gain comparing textual models to visual models because textual models are better on five datasets.\nTo better understand the overall performance of models across multiple datasets, we propose using critical difference (CD) diagrams (Dem\u0161ar, 2006). In a CD diagram, the average rank of each model and which ranks are statistically significantly different from each other are shown. Figure 4a and 4b show the CD diagrams using the Friedman test with Nemenyi post-hoc test at p < 0.05. In summary, we observe that AutoGluon and MUGNET respectively achieve the best rank among all tested models with respect to log-loss and accuracy, although never by a statistically significant margin. Moreover, tabular models obtain higher ranks than other unimodal classifiers. The similar observations from Table 2 and Figure 4 support that effectively aggregating information across modalities is critical for the multimodal classification task."
        },
        {
            "heading": "5.4 Efficiency Evaluations",
            "text": "Although accuracy (or other metrics such as logloss in our case) is the central measurement of a\nmachine learning model, efficiency is also a practical requirement in many applications. Trade-off often exists between how accurate the model is\nand how long it takes to train and infer the model. Therefore, we record the training durations and test durations of models to examine their efficiency. In Figure 5, we show the aggregated training duration of evaluated models via a box plot. As can be seen, tabular models require an order of magnitude less training duration than the other models, while AutoGluon stands out as requiring significantly longer training duration. Among tabular models, tabMLP is 4x faster than GBM in terms of the median training duration. Except for tabular models and AutoGluon, other models are approximately lightweight to train. It is worth noting that AutoGluon hits the 8-hour training duration constraint on every dataset, thus the variance of its training durations across datasets is very small.\nIn Figure 6, we show the trade-offs between mean inference time and mean accuracy of models. Since the accuracy is not commensurable across datasets, we first normalize all accuracies through a dataset-wise min-max normalization. After the normalization, the best model in each dataset is scaled to 1 while the worst model is scaled to 0. Finally, we take the average on the normalized accuracies and the test durations to draw the scatter plot. When both accuracy and efficiency are objectives models try to improve, there does not exist a model that achieves the best in both objectives simultaneously. As an illustration, MUGNET has the highest test accuracy, but tabMLP has the fastest inference speed. Therefore, we adopt the Paretooptimal3 concept to identify which models achieve \u201coptimal\u201d trade-offs. Pareto-optimal is widely used in the decision-making process for multi-objective optimization scenarios. By definition, a solution is Pareto-optimal if any of the objectives cannot\n3Pareto-optimal Definition: https://w.wiki/6sLB\nbe improved without degrading at least one of the other objectives. Following this concept, we observe that tabMLP, GBM, and MUGNET are the models with the best trade-offs between accuracy and efficiency, as these models reside in the Pareto frontier in Figure 6. Meanwhile, other models are suboptimal with regard to this trade-off, since we can always find a solution that has higher accuracy and better efficiency simultaneously than these models."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper presents a benchmark dataset MUGNET along with multiple baselines as a starting point for the machine learning community to improve upon. MUGNET is a multimodal classification benchmark on game data that covers tabular, textual, and visual modalities. All eight datasets and nine evaluated baselines are open-source and easily-extended to motivate rapid iteration and reproducible experiments for researchers. A comprehensive set of analyses is included to provide insight into the characteristics of the benchmark. The experimental results reported in the paper are obtained from models trained with constrained resources, which is often required by real-life applications. However, we also welcome future works that utilize enormous resources. Finally, we hope this work can facilitate research on multimodal learning, and we encourage any extensions to MUGNET to support new tasks or applications such as open-domain retrieval, AI-generated content, multimodal QA, etc.\nLimitations\nWhile our study emphasizes the importance of efficiency in real-world machine learning applications, we acknowledge certain limitations in our approach. Specifically, we deliberately focused on training and evaluating relatively \"small\" models within the context of the current era of large vision and language models (LVLMs) (Li et al., 2023; Liu et al., 2023; Lu et al., 2023). As a result, the performance of LVLMs on our proposed MUG benchmark remains unexplored. Early exploration (Hegselmann et al., 2023) about applying large language models on tabular classification shows that LLMs can be competitive with strong tree-based models. Based on the explorations and conducted experiments, we speculate LLVMs can not beat efficient ensemble or GNN baselines using the same training time constraint. However, to provide a comprehensive understanding of multimodal classification, further research is expected. It would be also intriguing to investigate the performance of LLVMs when provided with unlimited training (fine-tuning) time."
        },
        {
            "heading": "Acknowledgement",
            "text": "This research is partly supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913 and the Division of Mathematical Sciences of the National Science Foundation under Award Number 2208412. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of the National Science Foundation, National Institutes of Health, or the U.S. government."
        },
        {
            "heading": "A Broad Impact of Multimodal Datasets for Tasks beyond Classification",
            "text": "The proposed multimodal classification benchmark, MUG, is a valuable resource to inspire future studies for tasks including but not limited to classification. First, it is natural to utilize MUG to examine the model\u2019s abilities to understand the complex world surrounding us (Liang et al., 2021; Xu et al., 2022). The multimodal perception ability is crucial for open-domain retrieval (Tahmasebzadeh et al., 2021; Tian et al., 2022; Zeng et al., 2021), multimodal classification (Zadeh et al., 2017; Huddar et al., 2018), multimodal question answering (Antol et al., 2015; Talmor et al., 2020; Lu et al., 2022), interactive robots (Nie et al., 2021; Sampat et al., 2022), precision medicine (Soenksen et al., 2022; Lin et al., 2020; Gupta et al., 2020), etc. These applications all involve multi-modal input where each modality conveys partial information and a complete understanding can be achieved by taking all modalities into account. Second, MUG that contains aligned tabular, textual, and visual data is also beneficial to multimodal generation applications (Dong et al., 2023; He et al., 2023; Sun et al., 2023; Peng et al., 2023). For instance, many text-to-image generation models (Ramesh et al., 2021; Poole et al., 2022) employ CLIP (Radford et al., 2021) as their feature encoder, and CLIP is an alignment-based fusion model trained on semantically equivalent text and image pairs. There also exist many studies exploring unimodalto-unimodal generation tasks, such as image-totext captioning (Hossain et al., 2019), table-to-text\ngeneration (Parikh et al., 2020), text-to-table generation (Wu et al., 2022), etc. Following this idea, the aligned triples can be utilized to pre-train multimodal encoders. Furthermore, multimodal datasets that cover more than two modalities can inspire more novel generation applications, such as audiovisual slideshows generation from text (Leake et al., 2020) or textual-visual summarization from videobased news articles (Li et al., 2020)."
        },
        {
            "heading": "B Details of MUG",
            "text": "B.1 Prediction Targets\nWe identify appropriate categorical columns that can serve as the prediction targets from these four games, with a handy reference presented in Table 3. More specifically, we provide detailed elaborations about the prediction targets and their corresponding input multimodal features. Some MUG datasets may share same input multimodal features.\n\u2022 pkm_t1 and pkm_t2: Pok\u00e9mon can be categorized into various elemental types, such as Fire, Ice, Normal (non-elemental), and more. Each Pok\u00e9mon can have up to one primary type (for pkm_t1) and one secondary type (for pkm_t2).\n\u2013 17 tabular features: generation, status, height_m, weight_kg, abilities_num, total_points, hp, attack, defense, sp_attack, sp_defense, speed, catch_rate, base_friendship, base_experience, growth_rate, percentage_male.\n\u2013 5 text features: name, species, ability_1, ability_2, ability_hidden.\n\u2013 1 image feature: image.\n\u2022 hs_ac and hs_as: Each HearthStone card belongs to one certain category (for hs_ac) such as minion, spell, weapon, etc. Moreover, each card is part of a set (for hs_as) where new card sets are\nreleased periodically to introduce new content and strategies to the game.\n\u2013 12 tabular features: health, attack, cost, type, rarity, collectible, spellSchool, race, durability, overload, spellDamage, set (for hs_ac) / cardClass (for hs_as).\n\u2013 5 text features: name, id, artist, text, mechanics.\n\u2013 1 image feature: image.\n\u2022 hs_mr: Each HeathStone minion card is essentially a creature, thus it can be divided into different races (for hs_mr).\n\u2013 7 tabular features: health, attack, cost, rarity, collectible, cardClass, set.\n\u2013 5 text features: name, id, artist, text, mechanics.\n\u2013 1 image feature: image.\n\u2022 hs_ss: Each HeathStone spell card, similarly to the minion race, each spell card belongs to a specific school (for hs_ss) such as Shadow, Nature, etc.\n\u2013 5 tabular features: cost, rarity, collectible, cardClass, set, attack.\n\u2013 5 text features: name, id, artist, text, mechanics.\n\u2013 1 image feature: image.\n\u2022 lol_sc: A champion skin in LoL is a cosmetic alteration to the appearance of the champion. Depending on the rarity and price, a champion\u2019s skin belongs to a specific category (for lol_sc). It is worth noting that the champion skins are stylish decorations that have nothing to do with race, gender, or other unethical variables.\n\u2013 3 tabular features: id, price, soldInGame. \u2013 7 text features: skinName, concept, model,\nparticles, animations, sounds, releaseDate. \u2013 1 image feature: image.\n\u2022 csg_sq: Similar to champion skin in LoL, CS:GO skins alter the appearance of weapons. The prediction target is the skin quality according to its rarity (for csg_sq).\n\u2013 5 tabular features: id, availability, skinCategory, minPrice, maxPrice.\n\u2013 1 text features: skinName. \u2013 1 image feature: image.\nB.2 Definition of Shannon Equitability Index. The Shannon equitability index (Shannon, 1948), also known as the Shannon evenness index or Shannon\u2019s diversity index, is a measure used in ecology to assess the evenness or equitability of species abundance in a given community or ecosystem. It is derived from the Shannon entropy, which quantifies the diversity or richness of species in a community. For the classification task properties (Figure 2a), we adopt it to measure the class balance ratio,\nEH = H\nlog(k) =\n\u2212 \u2211k\ni=1 ci n log ci n log(k) , (5)\nwhere H denotes the entropy of each class\u2019s counts, k denotes the dataset containing k classes. EH ranges from 0 to 1, and the large EH , the more balanced the dataset is.\nB.3 Analysis on Multimodal-dependent. To study the correlation between category labels and input modalities in MUG, we plot 2D tSNE (Van der Maaten and Hinton, 2008) projections of various embeddings for the hs_mr dataset. In the first row of Figure 7, the four subfigures present projections obtained from the raw features of tabular, textual, visual, and fused modalities, separately. Essentially, we conduct unsupervised dimension reduction (e.g., SVD) on the raw features and then use t-SNE to obtain 2D projections. For tabular features, numerical columns are kept as they are, and categorical columns are transformed into numbers between 0 and n_class\u2212 1. For textual features, we first transformed them into token count vectors, and then use TruncatedSVD (Pedregosa et al., 2011) to reduce the number of dimensions to a reasonable amount (e.g., 50) before feeding into t-SNE. For visual features, we conduct PCA (Pedregosa et al., 2011) on each color channel to reduce the number of dimensions (e.g., 30) as well.\nFor a neat presentation, we select a subgroup of categories in the hs_mr dataset and assign different colors to samples belonging to different categories. For the fused raw features, we simply concatenate the three single modality features without any further modifications. The 2D projection of fused features is obtained following the same procedure, as in unimodal features. As can be seen in these four subfigures, samples from different categories are clustered together no matter what modality is used as the input. The second row of Figure 7 shows the\n2D t-SNE projections obtained from embeddings of models trained on tabular, textual, visual, and fused modalities, where these embeddings are obtained from the output of the penultimate layers. The models we used are tabMLP, RoBERTa, Vit, and MUGNET (evaluated baselines as in Sec 4). Compared to the t-SNE projections using raw features, all projections using trained embeddings provide better insights into categorical structures of the data. Among all subfigures, the one using fused embeddings is significantly better than the others, in which the separation between different categories is almost perfect with only a small number of points mis-clustered. In summary, MUG is multimodaldependent that requires multimodal information to distinguish samples from different classes.\nC Implementation and Hyperparameters of Baselines\nWe implement the evaluated models using open-source codebases (Ke et al., 2017; Paszke et al., 2019; Wang, 2019; Erickson et al., 2020; Shi et al., 2021; Erickson et al., 2022), and models\u2019 hyperparameters without specification are set as default. For GBM, we set the maximum number of leaves in one tree as 128, the minimal number of data inside one bin as 3, and the feature fraction ratio in one tree as 0.9. For tabMLP, we follow (Erickson et al., 2020) to adaptively set the\nembedding dimension of each categorical feature as min(100, 1.6 \u2217 num_cates0.56), all hidden layer sizes as 128, and the number of layers as 4. For RoBERTa, we use the \u201cRoBERTa-base\u201d variant. For Electra, we use the \u201cElectra-basediscriminator\u201d variant. For ViT, we use the \u201cvit_base_patch32_224\u201d variant. For SWIN, we use the \u201cswin_base_patch4_window7_224\u201d variant. For AutoGluon, we use its \u201cmultimodalbest_quality\u201d preset. For AutoMM, we use its default preset. For our own MUGNET, we optimize it using AdamW with a learning rate set as 0.001 and a cosine annealing learning rate schedule. Regarding the graph sampling strategy, we set the number of root nodes to generate random walks as 80% of the original number of nodes, and the length of each random walk as 2. MUGNET chooses other hyperparameters via HPO (Liaw et al., 2018). The search space includes the sample-wise similarity function sim \u2208 {cosine sim,RBF kernel, k-nearest neighbor} used in Equation (1). More specifically, (i) when sim := cosine sim or sim := RBF kernel, HPO of MUGNET also search along their associated graph sparsity hyperparameter spy \u2208 {0.5, 0.75, 0.95}. (ii) when sim := k-nearest neighbor, MUGNET search along k \u2208 {5, 10, 32}."
        }
    ],
    "title": "MUG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields",
    "year": 2023
}