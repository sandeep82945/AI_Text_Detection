{
    "abstractText": "Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via sourceor target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Ofir Arviv"
        },
        {
            "affiliations": [],
            "name": "Dmitry Nikolaev"
        },
        {
            "affiliations": [],
            "name": "Taelin Karidi"
        },
        {
            "affiliations": [],
            "name": "Omri Abend"
        }
    ],
    "id": "SP:8eb300cd4dacd240b3bf8207c3888137df8da02d",
    "references": [
        {
            "authors": [
                "\u017deljko Agi\u0107",
                "Anders Johannsen",
                "Barbara Plank",
                "H\u00e9ctor Mart\u00ednez Alonso",
                "Natalie Schluter",
                "Anders S\u00f8gaard."
            ],
            "title": "Multilingual projection for parsing truly low-resource languages",
            "venue": "Transactions of the Association for Computational Linguistics, 4:301\u2013312.",
            "year": 2016
        },
        {
            "authors": [
                "Wasi Ahmad",
                "Zhisong Zhang",
                "Xuezhe Ma",
                "Eduard Hovy",
                "Kai-Wei Chang",
                "Nanyun Peng."
            ],
            "title": "On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing",
            "venue": "Proceedings of the 2019 Conference of the North",
            "year": 2019
        },
        {
            "authors": [
                "Kabir Ahuja",
                "Rishav Hada",
                "Millicent A. Ochieng",
                "Prachi Jain",
                "Harshita Diddee",
                "Samuel Maina",
                "Tanuja Ganu",
                "Sameer Segal",
                "Maxamed Axmed",
                "Kalika Bali",
                "Sunayana Sitaram."
            ],
            "title": "Mega: Multilingual evaluation of generative ai",
            "venue": "ArXiv, abs/2303.12528.",
            "year": 2023
        },
        {
            "authors": [
                "Waleed Ammar",
                "George Mulcaire",
                "Miguel Ballesteros",
                "Chris Dyer",
                "Noah A. Smith."
            ],
            "title": "Many languages, one parser",
            "venue": "Transactions of the Association for Computational Linguistics, 4:431\u2013444.",
            "year": 2016
        },
        {
            "authors": [
                "Ofir Arviv",
                "Dmitry Nikolaev",
                "Taelin Karidi",
                "Omri Abend."
            ],
            "title": "On the relation between syntactic divergence and zero-shot performance",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4803\u20134817, Online",
            "year": 2021
        },
        {
            "authors": [
                "Lauriane Aufrant",
                "Guillaume Wisniewski",
                "Fran\u00e7ois Yvon."
            ],
            "title": "Zero-resource dependency parsing: Boosting delexicalized cross-lingual transfer with linguistic knowledge",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Pi-Chuan Chang",
                "Kristina Toutanova."
            ],
            "title": "A discriminative syntactic word order model for machine translation",
            "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9\u201316, Prague, Czech Republic. Association for",
            "year": 2007
        },
        {
            "authors": [
                "Kehai Chen",
                "Rui Wang",
                "Masao Utiyama",
                "Eiichiro Sumita."
            ],
            "title": "Neural machine translation with reordering embeddings",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1787\u20131799, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Michael Collins",
                "Philipp Koehn",
                "Ivona Ku\u010derov\u00e1."
            ],
            "title": "Clause restructuring for statistical machine translation",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 531\u2013540, Ann Arbor, Michigan. As-",
            "year": 2005
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Leonardo de Moura",
                "Nikolaj Bj\u00f8rner."
            ],
            "title": "Z3: An efficient smt solver",
            "venue": "Tools and Algorithms for the Construction and Analysis of Systems, pages 337\u2013 340, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Timothy Dozat",
                "Christopher D. Manning."
            ],
            "title": "Deep biaffine attention for neural dependency parsing",
            "venue": "ArXiv, abs/1611.01734.",
            "year": 2016
        },
        {
            "authors": [
                "Matthew S Dryer."
            ],
            "title": "The Greenbergian word order correlations",
            "venue": "Language, 68(1):81\u2013138.",
            "year": 1992
        },
        {
            "authors": [
                "Catherine Finegan-Dollak",
                "Jonathan K. Kummerfeld",
                "Li Zhang",
                "Karthik Ramanathan",
                "Sesh Sadasivam",
                "Rui Zhang",
                "Dragomir Radev."
            ],
            "title": "Improving textto-SQL evaluation methodology",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Matt Gardner",
                "Joel Grus",
                "Mark Neumann",
                "Oyvind Tafjord",
                "Pradeep Dasigi",
                "Nelson F. Liu",
                "Matthew Peters",
                "Michael Schmitz",
                "Luke Zettlemoyer."
            ],
            "title": "AllenNLP: A deep semantic natural language processing platform",
            "venue": "Proceedings of Workshop for",
            "year": 2018
        },
        {
            "authors": [
                "Dmitriy Genzel."
            ],
            "title": "Automatically learning sourceside reordering rules for large scale machine translation",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 376\u2013384, Beijing, China. Coling 2010 Orga-",
            "year": 2010
        },
        {
            "authors": [
                "Sonal Gupta",
                "Rushin Shah",
                "Mrinal Mohit",
                "Anuj Kumar",
                "Mike Lewis."
            ],
            "title": "Semantic parsing for task oriented dialog using hierarchical representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "John A Hawkins."
            ],
            "title": "Syntactic weight versus information structure in word order variation",
            "venue": "Informationsstruktur und grammatik, pages 196\u2013219.",
            "year": 1992
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Don\u2019t paraphrase, detect! rapid and effective data collection for semantic parsing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Spanbased semantic parsing for compositional generalization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Julian Hitschler",
                "Laura Jehl",
                "Sariya Karimova",
                "Mayumi Ohta",
                "Benjamin K\u00f6rner",
                "Stefan Riezler."
            ],
            "title": "Otedama: Fast rule-based pre-ordering for machine translation",
            "venue": "The Prague Bulletin of Mathematical Linguistics, 106:159 \u2013 168.",
            "year": 2016
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Mihir Kale",
                "Abhinav Rastogi."
            ],
            "title": "Text-to-text pre-training for data-to-text tasks",
            "venue": "Proceedings of the 13th International Conference on Natural Language Generation, pages 97\u2013102, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Haoran Li",
                "Abhinav Arora",
                "Shuohui Chen",
                "Anchit Gupta",
                "Sonal Gupta",
                "Yashar Mehdad."
            ],
            "title": "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Zuchao Li",
                "Jiaxun Cai",
                "Shexia He",
                "Hai Zhao."
            ],
            "title": "Seq2seq dependency parsing",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3203\u20133214, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Lu Liu",
                "Yi Zhou",
                "Jianhan Xu",
                "Xiaoqing Zheng",
                "Kai-Wei Chang",
                "Xuanjing Huang."
            ],
            "title": "Cross-lingual dependency parsing by POS-guided word reordering",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2938\u20132948, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Arya D. McCarthy",
                "Rachel Wicks",
                "Dylan Lewis",
                "Aaron Mueller",
                "Winston Wu",
                "Oliver Adams",
                "Garrett Nicolai",
                "Matt Post",
                "David Yarowsky."
            ],
            "title": "The Johns Hopkins University Bible corpus: 1600+ tongues for typological exploration",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Tao Meng",
                "Nanyun Peng",
                "Kai-Wei Chang."
            ],
            "title": "Target language-aware constrained inference for cross-lingual dependency parsing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Rudra Murthy",
                "Anoop Kunchukuttan",
                "Pushpak Bhattacharyya."
            ],
            "title": "Addressing word-order divergence in multilingual neural machine translation for extremely low resource languages",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Arijit Nag",
                "Bidisha Samanta",
                "Animesh Mukherjee",
                "Niloy Ganguly",
                "Soumen Chakrabarti."
            ],
            "title": "A data bootstrapping recipe for low-resource multilingual relation classification",
            "venue": "Proceedings of the 25th Conference on Computational Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Minh Van Nguyen",
                "Viet Dac Lai",
                "Amir Pouran Ben Veyseh",
                "Thien Huu Nguyen."
            ],
            "title": "Trankit: A lightweight transformer-based toolkit for multilingual natural language processing",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Dmitry Nikolaev",
                "Ofir Arviv",
                "Taelin Karidi",
                "Neta Kenneth",
                "Veronika Mitnik",
                "Lilja Maria Saeboe",
                "Omri Abend."
            ],
            "title": "Fine-grained analysis of crosslinguistic syntactic divergences",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Dmitry Nikolaev",
                "Sebastian Pado."
            ],
            "title": "Wordorder typology in multilingual BERT: A case study in subordinate-clause detection",
            "venue": "Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pages 11\u201321,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Sadegh Rasooli",
                "Michael Collins."
            ],
            "title": "Cross-lingual syntactic transfer with limited resources",
            "venue": "Transactions of the Association for Computational Linguistics, 5:279\u2013293.",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Sadegh Rasooli",
                "Michael Collins."
            ],
            "title": "Low-resource syntactic transfer with unsupervised source reordering",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Subendhu Rongali",
                "Luca Soldaini",
                "Emilio Monti",
                "Wael Hamza."
            ],
            "title": "Don\u2019t parse, generate! a sequence to sequence architecture for task-oriented semantic parsing",
            "venue": "Proceedings of The Web Conference 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Samanta",
                "Connie Tao",
                "David Ifeoluwa Adelani",
                "Vera Axelrod",
                "Isaac Caswell",
                "Colin Cherry",
                "Dan Garrette",
                "R. Reeve Ingle",
                "Melvin Johnson",
                "Dmitry Panteleev",
                "Partha Pratim Talukdar"
            ],
            "title": "Xtremeup: A user-centric scarce-data benchmark",
            "year": 2023
        },
        {
            "authors": [
                "Tanja Samard\u017ei\u0107",
                "Ximena Gutierrez-Vasques",
                "Rob van der Goot",
                "Max M\u00fcller-Eberstein",
                "Olga Pelloni",
                "Barbara Plank."
            ],
            "title": "On language spaces, scales and cross-lingual transfer of UD parsers",
            "venue": "Proceedings of the 26th Conference on Computational Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Tal Schuster",
                "Ori Ram",
                "Regina Barzilay",
                "Amir Globerson."
            ],
            "title": "Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Ralf Steinberger."
            ],
            "title": "Treating \u2018free word order\u2019 in machine translation",
            "venue": "COLING 1994 Volume 1: The 15th International Conference on Computational Linguistics, Kyoto, Japan.",
            "year": 1994
        },
        {
            "authors": [
                "Dingquan Wang",
                "Jason Eisner."
            ],
            "title": "The galactic dependencies treebanks: Getting more data by synthesizing new languages",
            "venue": "Transactions of the Association for Computational Linguistics, 4:491\u2013505.",
            "year": 2016
        },
        {
            "authors": [
                "Dingquan Wang",
                "Jason Eisner."
            ],
            "title": "Synthetic data made to order: The case of parsing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1325\u20131337,",
            "year": 2018
        },
        {
            "authors": [
                "Yuxuan Wang",
                "Wanxiang Che",
                "Jiang Guo",
                "Yijia Liu",
                "Ting Liu."
            ],
            "title": "Cross-lingual BERT transformation for zero-shot dependency parsing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Menglin Xia",
                "Emilio Monti."
            ],
            "title": "Multilingual neural semantic parsing for low-resourced languages",
            "venue": "Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pages 185\u2013194, Online. Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Haoran Xu",
                "Philipp Koehn."
            ],
            "title": "Zero-shot crosslingual dependency parsing through contextual embedding transformation",
            "venue": "Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 204\u2013213, Kyiv, Ukraine. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "LUKE: Deep contextualized entity representations with entityaware self-attention",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Jinchao Zhang",
                "Mingxuan Wang",
                "Qun Liu",
                "Jie Zhou."
            ],
            "title": "Incorporating word reordering knowledge into attention-based neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2017
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Victor Zhong",
                "Danqi Chen",
                "Gabor Angeli",
                "Christopher D. Manning."
            ],
            "title": "Position-aware attention and supervised data improve slot filling",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios.1"
        },
        {
            "heading": "1 Introduction",
            "text": "Recent multilingual pre-trained language models (LMs), such as mBERT (Devlin et al., 2019), XLMRoBERTa (Conneau et al., 2020), mBART (Liu et al., 2020b), and mT5 (Xue et al., 2021), have shown impressive cross-lingual ability, enabling effective transfer in a wide range of cross-lingual natural language processing tasks. However, even the most advanced LLMs are not effective when dealing with less-represented languages, as shown by recent studies (Ruder et al., 2023; Asai et al., 2023; Ahuja et al., 2023). Furthermore, annotating sufficient training data in these languages is not a\n1Code available at https://github.com/ OfirArviv/ud-based-word-reordering\nfeasible task, and as a result speakers of underrepresented languages are unable to reap the benefits of modern NLP capabilities (Joshi et al., 2020).\nNumerous studies have shown that a key challenge for cross-lingual transfer is the divergence in word order between different languages, which often causes a significant drop in performance (Rasooli and Collins, 2017; Wang and Eisner, 2018; Ahmad et al., 2019; Liu et al., 2020a; Ji et al., 2021; Nikolaev and Pado, 2022; Samard\u017eic\u0301 et al., 2022).2 This is unsurprising, given the complex and interdependent nature of word-order (e.g., verb-final languages tend to have postpositions instead of prepositions and place relative clauses before nominal phrases that they modify, while SVO and VSO languages prefer prepositions and postposed relative clauses, see Dryer 1992) and the way it is coupled with the presentation of novel information in sentences (Hawkins, 1992). This is especially true for the majority of underrepresented languages, which demonstrate distinct word order preferences from English and other well-resourced languages.\nMotivated by this, we present a reordering method applicable to any language pair, which can be efficiently trained even on a small amount of data, is applicable at all levels of the syntactic tree, and is powerful enough to boost the performance of modern multilingual LMs. The method, defined in terms of Universal Dependencies (UD), is based on pairwise constraints regulating the linear order of subtrees that share a common parent, which we term POCs for \u201cpairwise ordering constraints\u201d.\nWe estimate these constraints based on the probability that the two subtree labels will appear in one order or the other when their parent has a given label. Thus, in terms of UD, we expect, e.g., languages that use pre-nominal adjectival modification\n2From a slightly different perspective, this topic has also been actively studied in the machine translation literature (cf. Steinberger, 1994; Chang and Toutanova, 2007; Murthy et al., 2019).\nto assign a high probability to amods\u2019 preceding their headword, while languages with post-nominal adjectival modification are expected to have a high probability for the other direction.\nThe estimated POCs are fed as constraints to an SMT solver3 to produce a general reordering algorithm that can affect reordering of all types of syntactic structures. In addition to being effective, POCs are interpretable and provide a detailed characterization of typical word order patterns in different languages, allowing interpretability as to the effect of word order on cross-lingual transfer.\nWe evaluate our method on three cross-lingual tasks \u2013 dependency parsing, task-oriented semantic parsing, and relation classification \u2013 in the zeroshot setting. Such setting is practically useful (see, e.g., Ammar et al. 2016; Schuster et al. 2019; Wang et al. 2019; Xu and Koehn 2021 for successful examples of employing ZS learning cross-lingually) and minimizes the risk of introducing confounds into the analysis.\nWe further evaluate our method in the scarcedata scenario on the semantic parsing task. This scenario is more realistic as in many cases it is feasible to annotate small amounts of data in specific languages (Ruder et al., 2023).\nExperiments show that our method consistently presents a noticeable performance gain compared to the baselines over different language pairs and model architectures, both in the zero-shot and fewshot scenario. This suggests that despite recent advances, stronger multilingual models still faces difficulties in handling cross-lingual word order divergences, and that reordering algorithms, such as ours, can provide a much needed boost in performance in low-resource languages.\nAdditionally, we investigate the relative effectiveness of our reordering algorithm on two types of neural architectures: encoder-decoder (seq2seq) vs. a classification head stacked on top of a pretrained encoder. Our findings show that the encoder-decoder architecture underperforms in cross-lingual transfer and benefits more strongly from reordering, suggesting that it may struggle with projecting patterns over word-order divergences.\nThe structure of the paper is as follows: Section 2 surveys related work. The proposed approach is introduced in Section 3. Section 4 de-\n3An extension of the SAT solver that can, among other things, include mathematical predicates such as + and < in its constraints and assign integer values to variables.\nscribes the setup for our zero-shot and few-shot experiments, the results of which are presented in Section 5. Section 6 investigates the comparative performance of encoder-based and sequenceto-sequence models, and Section 7 concludes the paper."
        },
        {
            "heading": "2 Related Work",
            "text": "A major challenge for cross-lingual transfer stems from word-order differences between the source and target language. This challenge has been the subject of many previous works (e.g., Ahmad et al., 2019; Nikolaev and Pado, 2022), and numerous approaches to overcome it have been proposed.\nOne of the major approaches of this type is reordering, i.e. rearranging the word order in the source sentences to make them more similar to the target ones or vice versa. Early approaches, mainly in phrase-based statistical machine translation, relied on hand-written rules (Collins et al., 2005), while later attempts were made to extract reordering rules automatically using parallel corpora by minimizing the number of crossing wordalignments (Genzel, 2010; Hitschler et al., 2016).\nMore recent works focusing on reordering relied on statistics of various linguistic properties such as POS-tags (Wang and Eisner, 2016, 2018; Liu et al., 2020a) and syntactic relations (Rasooli and Collins, 2019). Such statistics can be taken from typological datasets such as WALS (Meng et al., 2019) or extracted from large corpora (Aufrant et al., 2016).\nOther works proposed to make architectural changes in the models. Thus, Zhang et al. (2017a) incorporated distortion models into attention-based NMT systems, while Chen et al. (2019) proposed learning reordering embeddings as part of Transformer-based translation systems. More recently, Ji et al. (2021) trained a reordering module as a component of a parsing model to improve cross-lingual structured prediction. Meng et al. (2019) suggested changes to the inference mechanism of graph parsers by incorporating targetlanguage-specific constraintsin inference.\nOur work is in line with the proposed solutions to source-sentence reordering, namely treebank reordering, which aim to rearrange the word order of source sentences by linearly permuting the nodes of their dependency-parse trees. Aufrant et al. (2016) and Wang and Eisner (2018) suggested permuting existing dependency treebanks to make their surface POS-sequence statistics close to those of\nthe target language, in order to improve the performance of delexicalized dependency parsers in the zero-shot scenario. While some improvements were reported, these approaches rely on short POS n-grams and do not capture many important patterns.4 Liu et al. (2020a) proposed a similar method but used a POS-based language model, trained on a target-language corpus, to guide their algorithm. This provided them with the ability to capture more complex statistics, but utilizing black-box learned models renders their method difficult to interpret.\nRasooli and Collins (2019) proposed a reordering algorithm based on UD, specifically, the dominant dependency direction in the target language, leveraging the rich syntactic information the annotation provides. Their method however, leverages only a small part of UD richness, compared to our method.\nWe note that previous work on treebank reordering usually only evaluated their methods on UD parsing, using delexicalized models or simple manually aligned cross-lingual word embeddings, which limited the scope of the analysis. In this paper, we experiment with two additional tasks that are not reducible to syntactic parsing: relation classification and semantic parsing. We further extend previous work by using modern multilingual LMs and experimenting with different architectures."
        },
        {
            "heading": "3 Approach",
            "text": "Given a sentence s = s1, s2, ..., sn in source language Ls, we aim to permute the words in it to mimic the word order of a target language Lt. Similarly to previous works (Wang and Eisner, 2018; Liu et al., 2020a), we make the assumption that a contiguous subsequence that forms a constituent in the original sentence should remain a contiguous subsequence after reordering, while the inner order of words in it may change. This prevents subtrees from losing their semantic coherence and is also vital when dealing with tasks such as relation extraction (RE), where some of the subequence must stay intact in order for the annotation to remain valid. Concretely, instead of permuting the words of sentence s, we permute the subtrees of its UD parse tree, thus keeping the subsequences of s, as\n4Aufrant et al. (2016) further experimented with manually crafting permutations rules using typological data on POS sequences from WALS. This approach is less demanding in terms of data but is more labor intensive and does not lead to better performance.\ndefined by the parse-tree structure, intact.5\nWe define a set of language-specific constraintsbased on the notion of Pairwise Ordering Distributions, the tendency of words with specific UD labels to be linearly ordered before words with other specific labels, conditioned on the type of subtree they appear in. To implement a reodering algorithm we use these constraints as input to an SMT solver."
        },
        {
            "heading": "3.1 Pairwise Ordering Distributions",
            "text": "Let T (s) be the Universal Dependencies parse tree of sentence s in language L, and \u03c0 = (\u03c01, ..., \u03c0n) the set of all UD labels. We denote the pairwise ordering distribution (POD) in language L of two UD nodes with dependency labels \u03c0i, \u03c0j , in a subtree with the root label \u03c0k with:\nP\u03c0k,\u03c0i,\u03c0j = p; p \u2208 [0, 1] (1)\nwhere p is the probability of a node with label \u03c0i to be linearly ordered before a node with label \u03c0j , in a subtree with a root of label \u03c0k. Note that being linearly ordered before a node with index i, means having an index of j < i, and that the nodes are direct children of the subtree root. We include a copy of the root node in the computation as one of its own children. Thus we can distinguish between a node acting as a representative of its subtree and the same node acting as the head of that subtree.6"
        },
        {
            "heading": "3.2 Pairwise Ordering Constraints and Reordering",
            "text": "Given the pairwise ordering distribution of target language Lt, denoted as distLt = P , we define a set of pairwise constraints based on it. Concretely, for dependency labels \u03c0k, \u03c0i, \u03c0j , we define the following constraint:\n\u03c0k : (\u03c0i < \u03c0j) = { 1, if P\u03c0k,\u03c0i,\u03c0j > 0.5 0 otherwise (2)\nwhere \u03c0k : (\u03c0i < \u03c0j) = 1 indicates that a node n with dependency label \u03c0i should be linearly ordered before node n\u2032 with dependency label \u03c0j if they are direct children of a node with label \u03c0k.\nUsing these constraints, we recursively reorder the tokens according to the parse tree T (s) in the following way. For each subtree Ti \u2208 T (s) with\n5In principle, UD allows for linearly overlapping subtrees, but such cases are rare in the existing treebanks.\n6An example set of learned POC statistics is given in Appendix C.\nUD label \u03c0j and children n1, n2, ..., nm, with UD labels n1\u03c0 , n2\u03c0 , ..., nm\u03c0 :\n1. We extract the pairwise constraints that apply to Ti based on the UD labels of its root and children.\n2. We feed the pairwise constraints to the SMT solver7 and use it to compute a legal ordering of the UD labels, i.e. an order that satisfies all the constraints.\n3. If there is such an ordering, we reorder the nodes in Ti accordingly. Otherwise, we revert to the original order.\n4. We proceed recursively, top-down, for every subtree in T , until all of T (s) is reordered to match distLt .\nFor example, assuming the constraints nsubj \u2192 root, obj \u2192 root, and obl \u2192 root for the main clause and obl \u2192 case, corresponding to a typical SOV language, and assuming that the target language does not have determiners,8 the sentence\nShensubj putroot [the book]obj [oncase the table]obl\nwill be first reordered as\nShensubj [the book]obj [oncase the table]obl putroot\nand then as\nShensubj [the book]obj [the table oncase]obl putroot."
        },
        {
            "heading": "3.3 Estimating the Pairwise Ordering Constraints",
            "text": "In this section we describe two possible methods for estimating the POCs of a language, one relying on the availability of a UD corpus in the target language and one relying on the Bible Corpus.\nUsing The UD Treebank. The first method we use to estimate POCs is by extracting them from corresponding empirical PODs in a UD treebank. When there are multiple treebanks per language, we select one of them as a representative treebank. We use v2.10 of the Universal Dependencies dataset, which contains treebanks for over 100 languages.\n7We use Python bindings of the open-source SMT solver Z3 (de Moura and Bj\u00f8rner, 2008).\n8Whose relative position for the sake of the example thus can be selected arbitrarily.\nEstimating POCs without a Treebank. While the UD treebank is vast, there are still hundreds of widely spoken languages missing from it. The coverage of our method can be improved by using annotation projection (Agic\u0301 et al., 2016) on a massively parallel corpus, such as the Bible Corpus (McCarthy et al., 2020). Approximate POCs can then be extracted from the projected UD trees. While we do not experiment with this setting in this work due to resource limitations, we mention it as a promising future work venue, relying on the work done by Rasooli and Collins (2019), which used this approach successfully, to extract UD statistics, and utilize them in their reordering algorithm on top of annotation projection."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We evaluate our reordering algorithm using three tasks \u2013 UD parsing, task-oriented semantic parsing, and relation extraction \u2013 and over 13 different target languages, with English as the source language.9 For each task and target language, we compare the performance of a model trained on the vanilla English dataset against that of a model trained on a transformed (reordered) version of the dataset, using the target-language test set in a zero-shot fashion.\nWe explore two settings: STANDARD, where we reorder the English dataset according to the target language POCs and use it for training, and ENSEMBLE, where we train our models on both the vanilla and the reordered English datasets. The main motivation for this is that any reordering algorithm is bound to add noise to the data. First, the underlying multilingual LMs were trained on the standard word order of English, and feeding them English sentences in an unnatural word order will likely produce sub-optimal representations. Secondly, reordering algorithms rely on surface statistics, which, while rich, are a product of statistical estimation and thus imperfect. Lastly, the use of hard constrains may not be justified for target languages with highly flexible word-order10. The\n9We use English as the source language because it has the biggest variety of training sets for different tasks. It has been shown that for some tasks using a source language with a less strict word order, such as Russian or Czech, can lead to improvements (Nikolaev and Pado, 2022), but in practice they are rather minor and do not outweigh the benefits of having access to more corpora.\n10In the worst-case scenario, given a very flexible order of a particular pair of syntactic elements in the target language, with P\u03c0k,\u03c0i,\u03c0j \u2248 0.51, our method will select only one or-\nENSEMBLE setting mitigates these issues and improves the \u201csignal to noise ratio\u201d of the approach.\nWe use the vanilla multilingual models and the reordering algorithm by Rasooli and Collins (2019) as baselines. To the best of our knowledge, Rasooli and Collins proposed the most recent preprocessing reordering algorithm, which also relies on UD annotation. We re-implement the algorithm and use it in the same settings as our approach.\nLastly, we evaluate our method in the scarce-data setting. We additionally train the models fine-tuned on the vanilla and reordered English datasets on a small number of examples in the target language and record their performance. Due to the large amount of experiments required we conduct this experiment using only our method in the context of the semantic-parsing task, which is the most challenging one in our benchmark (Asai et al., 2023; Ruder et al., 2023), on the mT5 model, and in the ENSEMBLE setting."
        },
        {
            "heading": "4.1 Estimating the POCs",
            "text": "We estimate the POCs (\u00a73.2) by extracting the empirical distributions from UD treebanks (see \u00a73.3). While this requires the availability of an external data source in the form of a UD treebank in the target language, we argue that for tasks other than UD parsing this is a reasonable setting as the UD corpora are available for a wide variety of languages. Furthermore, we experiment with various treebank sizes, including ones with as few as 1000 sentences. Further experimentation with even smaller treebanks is deferred to future work. Appendix B lists the treebanks used and their sizes."
        },
        {
            "heading": "4.2 Evaluation Tasks",
            "text": "In this section, we describe the tasks we use for evaluation, the models we use for performing the tasks, and the datasets for training and evaluation. All datasets, other than the manually annotated UD corpora, are tokenized and parsed using Trankit (Nguyen et al., 2021). Some datasets contain subsequences that must stay intact in order for the annotation to remain valid (e.g., a proper-name sequence such as The New York Times may have internal structure but cannot be reordered). In cases where these subsequences are not part of a single subtree, we manually alter the tree to make them\ndering as the \u201ccorrect\u201d one. If this ordering contradicts the original English one, it will be both nearly 50% incorrect and highly unnatural for the encoder. Ensembling thus ensures that the effect of estimation errors is bounded.\nso. Such cases mostly arise due to parsing errors and are very rare. The hyper-parameters for all the models are given in Appendix D."
        },
        {
            "heading": "4.2.1 UD Parsing",
            "text": "Dataset. We use v2.10 of the UD dataset. For training, we use the UD English-EWT corpus with the standard splits. For evaluation, we use the PUD corpora of French, German, Korean, Spanish, Thai and Hindi, as well as the Persian-Seraji, ArabicPADT, and the Irish-TwittIrish treebanks.\nWe note that our results are not directly comparable to the vanilla baseline because our model has indirect access to a labeled target dataset, which is used to estimate the POCs. This issue is less of a worry in the next tasks, which are not defined based on UD. We further note that we do not use the same dataset for extracting the information about the target language and for testing the method. 11\nModel. We use the AllenNLP (Gardner et al., 2018) implementation of the deep biaffine attention graph-based model of Dozat and Manning (2016). We replace the trainable GloVe embeddings and the BiLSTM encoder with XLM-RoBERTa-large (Conneau et al., 2020). Finally, we do not use gold (or any) POS tags. We report the standard labeled and unlabeled attachment scores (LAS and UAS) for evaluation, averaged over 5 runs."
        },
        {
            "heading": "4.2.2 Task-oriented Semantic Parsing",
            "text": "Datasets. We use the MTOP (Li et al., 2021) and Multilingual TOP (Xia and Monti, 2021) datasets. MTOP covers 6 languages (English, Spanish, French, German, Hindi and Thai) across 11 domains. In our experiments, we use the decoupled representation of the dataset, which removes all the text that does not appear in a leaf slot. This representation is less dependent on the word order constraints and thus poses a higher challenge to reordering algorithms. The Multilingual TOP dataset contains examples in English, Italian, and Japanese and is based on the TOP dataset (Gupta et al., 2018). Similarly to the MTOP dataset, this dataset uses the decoupled representation. Both datasets are formulated as a seq2seq task. We use the standard splits for training and evaluation.\nModels. We use two seq2seq models in our evaluation, a pointer-generator network model (Ron-\n11Note that Thai has only one published UD treebank, so for this experiment we split it in two parts, 500 sentences each, for estimating POCs and testing.\ngali et al., 2020) and mT5 (Xue et al., 2021). The pointer-generator network was used in previous works on these datasets (Xia and Monti, 2021; Li et al., 2021); it includes XLM-RoBERTa-large (Conneau et al., 2020) as the encoder and an uninitialized Transformer as a decoder. In this method, the target sequence is comprised of ontology tokens, such as [IN:SEND_MESSAGE in the MTOP dataset, and pointer tokens representing tokens from the source sequence (e.g. ptr0, which represents the first source-side token). When using mT5, we use the actual tokens and not the pointer tokens as mT5 has copy-mechanism built into it, thus enabling the model to utilize it. In both models, we report the standard exact-match (EM) metric, averaged over 10 runs for the pointer-generator model and 5 runs for mT5."
        },
        {
            "heading": "4.2.3 Relation Classification",
            "text": "Datasets. We use two sets of relation-extraction datasets: (i) TACRED (Zhang et al., 2017b) (TAC) and Translated TACRED (Arviv et al., 2021) (Trans-TAC), and (ii) IndoRE (Nag et al., 2021). TAC is a relation-extraction dataset with over 100K examples in English, covering 41 relation types. The Trans-TAC dataset contains 533 parallel examples sampled from TAC and translated into Russian and Korean. We use the TAC English dataset for training and Trans-TAC for evaluation. As the TAC train split is too large for efficient training, we only use the first 30k examples.\nIndoRE (Nag et al., 2021) contains 21K sentences in Indian languages (Bengali, Hindi, and Telugu) plus English, covering 51 relation types. We use the English portion of the dataset for training, and the Hindi and Telugu languages for evaluation.12\nModel. We use the relation-classification part of the LUKE model (Yamada et al., 2020).13 The model uses two special tokens to represent the head and the tail entities in the text. The text is fed into an encoder, and the task is solved using a linear classifier trained on the concatenated representation of the head and tail entities. For consistency, we use XLM-RoBERTa-large (Conneau et al., 2020) as the encoder. We report the micro-F1 and macroF1 metrics, averaged over 5 runs.\n12The Bengali UD treebank is extremely small (17 sentences), which makes it impossible to extract high-quality POCs.\n13https://github.com/studio-ousia/luke"
        },
        {
            "heading": "5 Results and Discussion",
            "text": "The results on the various tasks, namely UD parsing, semantic parsing, and relation classification, are presented in Tables 1, 2 and 3 respectively. The few-shot experiment results are in Table 4. Standard deviations are reported in Appendix E.\nIn UD parsing, the ENSEMBLE setting presents noticeable improvements for the languages that are more typologically distant from English (2.3-4.1 LAS points and 1.8-3.5 UAS points), with the exception of Arabic, in which the scores slightly drop. No noticeable effect is observed for structurally closer languages.\nIn the STANDARD setting, a smaller increase in performance is present for most distant languages, with a decrease in performance for closes ones, Persian and Arabic. This is in agreement with previous work that showed that reordering algorithms are more beneficial when applied to structurallydivergent language pairs (Wang and Eisner, 2018; Rasooli and Collins, 2019). The ENSEMBLE approach, therefore, seems to be essential for a generally applicable reordering algorithm.\nThe algorithm by Rasooli and Collins (2019) (RC19), in both settings, presents smaller increase in performance for some typologically-distant languages and no noticeable improvements for others, while sometimes harming the results. This suggests that for this task the surface statistics the algorithm uses are not enough and a more fine-grained approach in needed.\nIn the semantic-parsing task, the reordering algorithm presents substantial improvements for all languages but Italian in the ENSEMBLE setting (2-6.1 increase in exact match), for the RoBERTa based model. Noticeably, the gains are achieved not only for typologically-distant languages but also for languages close to English, such as French. In the MTOP dataset, the ENSEMBLE setting proves bigger gains over the STANDARD for all languages. In Multilingual-TOP, we surprisingly observe the opposite. Given that Japanese in terms of word order is comparable to Hindi and Italian, to French, we tend to attribute this result to the peculiarities of the dataset. This, however, merits further analysis.\nWhen compared to RC19, the proposed algorithm consistently outperforms it, by an average of about 2 points (in the ENSEMBLE setting).\nFor mT5 we observe increase in performance, in the ENSEMBLE setting, of 2.5 and 5 points in Thai and Hindi, respectively. For the other languages,\nwe do not observe a strong impact. We note however, that in French and Spanish, there is a slight drop in the score (less then 1 point). When compared to RC19, our method provide larger gains in Hindi and Thai.\nIn the few-shot scenario, we observe improved performances for all languages and sample sizes. Surprisingly, the improvements hold even when training on a large sample size of 500, indicating that the model is not able to easily adapt to the target word-order.\nLastly, in the relation-classification task, in the ENSEMBLE setting we observe an increase in performance for all languages (2.3-10.4 increase in the\nMicro and Macro F1 points). In the STANDARD setting, there is a drop in performance for Hindi and Telugu. When compared to RC19, our algorithm outperforms it in the ENSEMBLE setting, by more than 5 points for Korean, but only by 0.5 points for Russian. In Hindi and Telugu, the performance of both algorithms is close, and RC19 does perform better in some cases."
        },
        {
            "heading": "6 Comparison between Encoder-withClassifier-Head and Seq2Seq Models",
            "text": "Past work has shown that the architecture is an important predictor of the ability of a given model to generalize over cross-lingual word-order divergences. For example, Ahmad et al. (2019) showed that models based on self-attention have a better overall cross-lingual transferability to distant languages than those using RNN-based architectures.\nOne of the dominant trends in recent years in NLP has been using the sequence-to-sequence formulation to solve an increasing variety of tasks (Kale and Rastogi, 2020; Lewis et al., 2020). Despite that, recent studies (Finegan-Dollak et al., 2018; Keysers et al., 2019; Herzig and Berant, 2019) demonstrated that such models fail at compositional generalization, that is, they do not generalize to structures that were not seen at training time. Herzig and Berant (2021) showed that other model architectures can prove advantageous over seq2seq architecture in that regard, but their work was limited to English.\nHere, we take the first steps in examining the cross-lingual transfer capabilities of the seq2seq encoder-decoder architecture (S2S) vs. a classification head stacked over an encoder (E+C), focusing on their ability to bridge word-order divergences."
        },
        {
            "heading": "6.1 Experimental Setup",
            "text": "We compare the performance of an E+C model against a S2S one on the task of UD parsing over various target languages. Similar to \u00a74, we train each model on the vanilla English dataset and compare it against a model trained on a version of the dataset reordered using our algorithm. We evaluate the models using the target-language test set in a zero-shot setting.\nDataset and POCs Estimates. We use the same UD dataset and POCs as in \u00a74. For the S2S task, we linearize the UD parse tree using the method by Li et al. (2018).\nModels. For the E+C model we use the deep biaffine attention graph-based model with XLMRoBERTa-large as the encoder, as in \u00a74.2.1. For S2S model, we use the standard transformer architecture with XLM-RoBERTa-large as the encoder and an uninitialized self-attention stack as the decoder. The hyper-parameters for the models are given in Appendix D."
        },
        {
            "heading": "6.2 Results and Discussion",
            "text": "The results for LAS (averaged over 5 runs), normalized by the base parser performance on the English test-set, are presented in Table 5 (UAS follow the same trends. See full scores in Appendix F). The zero-shot performance of the S2S model is subpar compared to the E+C one (less\nthen 50%), despite relying on the same underline multilingual LM. Furthermore, the S2S architecture benefits more strongly from reordering for distant languages (more than twice as much), compared to the E+C one. This suggests that seq-to-sequence architecture may be less effective in handling crosslingual divergences, specifically word-order divergences, and may gain more from methods such as reordering."
        },
        {
            "heading": "7 Conclusion",
            "text": "We presented a novel pre-processing reordering approach that is defined in terms of Universal Dependencies. Experiments on three tasks and numerous architectures and target languages demonstrate that this method is able to boost the performances of modern multilingual LMs both in the zero-shot and few-shot setting. Our key contributions include a new method for reordering sentences based on fine-grained word-order statistics, the Pairwise Ordering Distributions, using an SMT solver to convert the learned constraints into a linear ordering, and a demonstration of the necessity of combining the reordered dataset with the original one (the ENSEMBLE setting) in order to consistently boost performance.\nOur results suggest that despite the recent improvements in multilingual models, they still face difficulties in handling cross-lingual word order divergences, and that reordering algorithms, such as ours, can provide a much needed boost in performance in low-resource languages. This result holds even in the few-shot scenario, when the model is trained on few hundred examples, underscoring the difficulty of models to adapt to varying word orders, as well as the need for more typologically diverse data, additional inductive bias at the training time, or a pre-processing approach such as ours to be more effective. Furthermore, our experiments suggest that seq2seq encoder-decoder architectures may suffer from these difficulties to a bigger extent than more traditional modular ones.\nFuture work will include, firstly, addressing the limitations of the proposed approach in order to make it less language-pair dependent and reduce the computational and storage overhead, and secondly, leveraging the POCs in order to compute the word-order distance between languages in a rich, rigorous corpus-based way,14 and to more precisely predict when the reordering algorithm will be beneficial as well as to provide a fine-grained analysis of the connection between word order and cross-lingual performance, in line with Nikolaev et al. (2020).\nLimitations\nThere are several limitation to our work. First, as shown in the experiment results, for some tasks, reordering, even with ensembling, is not beneficial\n14WALS, for example, only provides a single categorical label for \u201cdominant word order\u201d.\nfor closely-related languages. Secondly, as this is a pre-processing algorithm, it incurs time and computation overhead for tokenization, parsing, and reordering of the source dataset. Most importantly, if one wishes to train a model on several word order patterns, it will be necessary to apply the algorithm repeatedly to create a new dataset for each pattern, increasing the overall size of the dataset. Furthermore, extracting the POCs requires external resources, either UD corpora in the target language or a multi-parallel corpus for annotation projection. More technical limitations of the reordering algorithm are described in Appendix A."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the Israel Science Foundation (grant no. 2424/21)."
        },
        {
            "heading": "A Shortcomings of the Reordering Algorithm",
            "text": "We note a couple of possible shortcomings to this approach. First, while a pair of constraints cannot be straightforwardly contradictory (both values\ncannot be set to 1), it can be uninformative (both values set to 0) when not enough label-ordering data is presented in the training treebank, which means that the ordering of the corresponding nodes is not subject to any constraint.\nMoreover, it is possible to encounter loops or transitivity conflicts when joining different constraints, which makes it a priori impossible for the solver to satisfy them. To alleviate this problem, for each subtree we aim to reorder, we only consider the constraints that are relevant to it. For example, if the subtree does not contain any token with label nmod, we discard all the constraints which include this label, such as amod : (nmod < amod). This, together with the tendency of languages to have a preferred ordering to their constituent elements, makes it so that only a small percentage of subtrees cannot be ordered.\nLast, sparsity issues may prevent some constraints from being statistically justified, and rounding the constraints to a hard 0 or 1 may result in information loss and thus be detrimental. For example, if our pairwise distributions are Pnmod,nmod,amod = 0.51 and Pnmod,amod,nmod = 0.49, deriving a constraint of nmod : (nmod < amod) may not be warranted. This may also happens in the case of a highly flexible order of a particular pair of syntactic elements in the target language. If this ordering contradicts the original English one, it will be both nearly 50% incorrect and highly unnatural for the encoder. This is, however, partially mitigated by the ENSEMBLE$ method. For the purposes of this work we do not distinguish between POCs according to their statistical validity and defer this question to future work."
        },
        {
            "heading": "B Estimating the POCs",
            "text": "We estimate the POCs (\u00a73.2) by extracting the empirical distributions from UD treebanks. The UD treebanks used are reported in Table 6."
        },
        {
            "heading": "C Example of Learned Distributions",
            "text": "Here are the statistics of the pairwise ordering of main elements of the matrix clause15 learned on the Irish-IDT treebank:\n\u2022 acl vs. advcl\n\u2013 acl->advcl: 10 \u2013 advcl->acl: 4\n15Elements that are directly under the root node.\n\u2022 acl vs. advmod\n\u2013 advmod->acl: 6 \u2013 acl->advmod: 1\n\u2022 acl vs. amod\n\u2013 amod->acl: 29 \u2013 acl->amod: 1\n\u2022 acl vs. appos\n\u2013 appos->acl: 3 \u2013 acl->appos: 4\n\u2022 acl vs. nsubj\n\u2013 nsubj->acl: 35 \u2013 acl->nsubj: 8\n\u2022 acl vs. obj\n\u2013 obj->acl: 3\n\u2022 acl vs. obl\n\u2013 obl->acl: 14\nTarget Base Ours OursE\nUAS LAS UAS LAS UAS LAS\nFrench 1 1.1 0.5 0.6 1.6 1.7 German 1.2 1.2 0.4 0.3 0.4 0.3 Spanish 0.7 0.6 0.4 0.4 0.2 0.3 Korean 0.5 0.5 0.7 0.6 0.4 0.5 Persian 0.5 0.4 0.7 0.6 0.4 0.4 Hindi 0.7 0.5 0.6 0.5 0.7 0.7 Thai 0.9 0.7 2.8 2.6 1.2 2.2\n\u2013 obj->advmod: 71 \u2013 advmod->obj: 77\n\u2022 advmod vs. obl\n\u2013 obl->advmod: 231 \u2013 advmod->obl: 326\n\u2022 advmod vs. root\n\u2013 advmod->root: 111 \u2013 root->advmod: 486\n\u2022 amod vs. appos\n\u2013 amod->appos: 2\n\u2022 amod vs. nsubj\n\u2013 nsubj->amod: 13 \u2013 amod->nsubj: 41\nAs expected, Irish behaves as a strict head-initial language: root overwhelmingly precedes all other constituents, including subordinate clauses, and modifier subordinate clauses (acl, advcl) follow nominal clause participants (nsubj, obj, obl). Adjectival modifiers, however, mostly precede nominal elements; this may be due to the fact that some frequent pronominal adjectives, such as uile \u2018all\u2019 and gach \u2018every\u2019 do not follow the general rule and precede the nouns they modify.\nThe position of adverbial modifiers is not restricted by the grammar, and it may be noted that it generally follows nominal subjects but as often as not precedes direct objects, and in 3/5 of cases precedes obliques, which suggests the general order root\u2192 nsubj\u2192 advmod/obj\u2192 obl."
        },
        {
            "heading": "D Models Hyperparameters",
            "text": "The hyperparameters of the UD parser are given in Table 9. For the seq2seq pointer-generator network model \u2013 in Table 10, for mT5 \u2013 in Table 11, and for LUKE relation classification model \u2013 in Table 12.\nFor the UD Seq2Seq parser, we use the same hyperparameters as for the seq2seq pointer-generator network model, with the following exceptions: we train for only 50 epochs and set the learning rate to 1e\u20135 for both the encoder and decoder."
        },
        {
            "heading": "E Standard Deviations",
            "text": "The standard deviations of the results of the experiments in UD parsing, semantic parsing, and relation classification are presented in Tables 14, 13, and 15 respectively."
        },
        {
            "heading": "F UD Seq2Seq Model Performances",
            "text": "The full results (averaged over 5 models) of the S2S model in UD parsing, are presented in Table 7. The standard deviations are in Table 8."
        }
    ],
    "title": "Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering",
    "year": 2023
}