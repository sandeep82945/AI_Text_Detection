{
    "abstractText": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models\u2019 probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models\u2019 linguistic knowledge. Broadly, we find that LLMs\u2019 metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jennifer Hu"
        },
        {
            "affiliations": [],
            "name": "Roger Levy"
        }
    ],
    "id": "SP:7b0e6def6ba3a74ed24ee4cf49b466a568f7f39a",
    "references": [
        {
            "authors": [
                "Marco Baroni."
            ],
            "title": "On the proper role of linguistically-oriented deep net analysis in linguistic theorizing",
            "venue": "Shalom Lappin and Jean-Philippe Bernardy, editors, Algebraic Structures in Natural Language. Taylor & Francis.",
            "year": 2022
        },
        {
            "authors": [
                "Ga\u0161per Begu\u0161",
                "Maksymilian D\u0105bkowski",
                "Ryan Rhodes"
            ],
            "title": "Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs",
            "year": 2023
        },
        {
            "authors": [
                "Anne Beyer",
                "Sharid Lo\u00e1iciga",
                "David Schlangen."
            ],
            "title": "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Kristy Choi",
                "Chris Cundy",
                "Sanjari Srivastava",
                "Stefano Ermon"
            ],
            "title": "LMPriors: Pre-Trained Language Models as Task-Specific Priors",
            "year": 2022
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "Aspects of the Theory of Syntax",
            "venue": "MIT Press.",
            "year": 1965
        },
        {
            "authors": [
                "Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling Instruction-Finetuned Language Models",
            "year": 2022
        },
        {
            "authors": [
                "Pablo Contreras Kallens",
                "Ross Deans KristensenMcLachlan",
                "Morten H. Christiansen."
            ],
            "title": "Large Language Models Demonstrate the Potential of Statistical Learning in Language",
            "venue": "Cognitive Science, 47(3):e13256. Publisher: John Wiley & Sons, Ltd.",
            "year": 2023
        },
        {
            "authors": [
                "Vittoria Dentella",
                "Elliot Murphy",
                "Gary Marcus",
                "Evelina Leivada"
            ],
            "title": "Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning",
            "year": 2023
        },
        {
            "authors": [
                "Gabe Dupre"
            ],
            "title": "What) Can Deep Learning Contribute to Theoretical Linguistics",
            "venue": "Minds and Machines,",
            "year": 2021
        },
        {
            "authors": [
                "Chaz Firestone."
            ],
            "title": "Performance vs",
            "venue": "competence in human\u2013machine comparisons. Proceedings of the National Academy of Sciences, 117(43):26562\u2013 26571. Publisher: Proceedings of the National Academy of Sciences.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Futrell",
                "Ethan Wilcox",
                "Takashi Morita",
                "Peng Qian",
                "Miguel Ballesteros",
                "Roger Levy."
            ],
            "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Jon Gauthier",
                "Jennifer Hu",
                "Ethan Wilcox",
                "Peng Qian",
                "Roger Levy."
            ],
            "title": "SyntaxGym: An Online Platform for Targeted Evaluation of Language Models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System",
            "year": 2020
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger."
            ],
            "title": "On Calibration of Modern Neural Networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1321\u20131330.",
            "year": 2017
        },
        {
            "authors": [
                "Michael Hahn",
                "Richard Futrell",
                "Roger P. Levy",
                "Edward Gibson."
            ],
            "title": "A resource-rational model of human processing of recursive linguistic structure",
            "venue": "119(43).",
            "year": 2022
        },
        {
            "authors": [
                "Jennifer Hu",
                "Jon Gauthier",
                "Peng Qian",
                "Ethan Wilcox",
                "Roger Levy."
            ],
            "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Ben Mann",
                "Sam McCandlish",
                "Chris Olah",
                "Jared Kaplan"
            ],
            "title": "Language Models (Mostly) Know What They Know",
            "year": 2022
        },
        {
            "authors": [
                "Roni Katzir"
            ],
            "title": "Why large language models are poor theories of human linguistic cognition: A reply",
            "year": 2023
        },
        {
            "authors": [
                "Carina Kauf",
                "Anna A. Ivanova",
                "Giulia Rambelli",
                "Emmanuele Chersoni",
                "Jingyuan S. She",
                "Zawad Chowdhury",
                "Evelina Fedorenko",
                "Alessandro Lenci"
            ],
            "title": "Event knowledge in large language models: the gap between the impossible and the unlikely",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Xinxi Lyu",
                "Sewon Min",
                "Lianhui Qin",
                "Kyle Richardson",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Sameer Singh",
                "Yejin Choi"
            ],
            "title": "Prompt Waywardness: The Curious Case of Discretized Interpretation",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang (Shane) Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large Language Models are Zero-Shot Reasoners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Kyle Lampinen"
            ],
            "title": "Can language models handle recursively nested grammatical structures? A case study on comparing models and humans",
            "year": 2023
        },
        {
            "authors": [
                "Nur Lan",
                "Emmanuel Chemla",
                "Roni Katzir"
            ],
            "title": "Large Language Models and the Argument From the Poverty of the Stimulus",
            "year": 2022
        },
        {
            "authors": [
                "Belinda Z. Li",
                "William Chen",
                "Pratyusha Sharma",
                "Jacob Andreas"
            ],
            "title": "LaMPP: Language Models as Probabilistic Priors for Perception and Action",
            "year": 2023
        },
        {
            "authors": [
                "Tal Linzen",
                "Emmanuel Dupoux",
                "Yoav Goldberg."
            ],
            "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
            "venue": "Transactions of the Association for Computational Linguistics, 4:521\u2013 535.",
            "year": 2016
        },
        {
            "authors": [
                "Benjamin Lipkin",
                "Lionel Wong",
                "Gabriel Grand",
                "Joshua B. Tenenbaum."
            ],
            "title": "Evaluating statistical language models as pragmatic reasoners",
            "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society, volume 45.",
            "year": 2023
        },
        {
            "authors": [
                "R. Thomas McCoy",
                "Shunyu Yao",
                "Dan Friedman",
                "Matthew Hardy",
                "Thomas L. Griffiths"
            ],
            "title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve",
            "year": 2023
        },
        {
            "authors": [
                "Sabrina J. Mielke",
                "Arthur Szlam",
                "Emily Dinan",
                "Y-Lan Boureau."
            ],
            "title": "Reducing Conversational Agents\u2019 Overconfidence Through Linguistic Calibration",
            "venue": "Transactions of the Association for Computational Linguistics, 10:857\u2013872.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Milway"
            ],
            "title": "A Response to Piantadosi (2023)",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Minderer",
                "Josip Djolonga",
                "Rob Romijnders",
                "Frances Hubis",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Dustin Tran",
                "Mario Lucic."
            ],
            "title": "Revisiting the Calibration of Modern Neural Networks",
            "venue": "Advances in Neural Information Processing Systems, volume 34,",
            "year": 2021
        },
        {
            "authors": [
                "Arseny Moskvichev",
                "Victor Vikram Odouard",
                "Melanie Mitchell"
            ],
            "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain",
            "year": 2023
        },
        {
            "authors": [
                "Elliot Murphy"
            ],
            "title": "Notes on Large Language Models and Linguistic Theory",
            "year": 2023
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan",
                "Charles Sutton",
                "Augustus Odena"
            ],
            "title": "Show Your Work: Scratchpads for Intermedi",
            "year": 2021
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Roma Patel",
                "Ellie Pavlick."
            ],
            "title": "Mapping Language Models to Grounded Conceptual Spaces",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Francisco Pereira",
                "Bin Lou",
                "Brianna Pritchett",
                "Samuel Ritter",
                "Samuel J. Gershman",
                "Nancy Kanwisher",
                "Matthew Botvinick",
                "Evelina Fedorenko."
            ],
            "title": "Toward a universal decoder of linguistic meaning from brain activation",
            "venue": "Nature Communications,",
            "year": 2018
        },
        {
            "authors": [
                "Steven T. Piantadosi"
            ],
            "title": "Modern language models refute Chomsky\u2019s approach to language",
            "year": 2023
        },
        {
            "authors": [
                "Archiki Prasad",
                "Peter Hase",
                "Xiang Zhou",
                "Mohit Bansal."
            ],
            "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Julian Salazar",
                "Davis Liang",
                "Toan Q. Nguyen",
                "Katrin Kirchhoff."
            ],
            "title": "Masked Language Model Scoring",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699\u20132712, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Miles Turpin",
                "Julian Michael",
                "Ethan Perez",
                "Samuel R. Bowman"
            ],
            "title": "Language Models Don\u2019t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
            "year": 2023
        },
        {
            "authors": [
                "Tomer Ullman"
            ],
            "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
            "year": 2023
        },
        {
            "authors": [
                "Paolo Vassallo",
                "Emmanuele Chersoni",
                "Enrico Santus",
                "Alessandro Lenci",
                "Philippe Blache."
            ],
            "title": "Event Knowledge in Sentence Processing: A New Dataset for the Evaluation of Argument Typicality",
            "venue": "LREC 2018 Workshop on Linguistic and Neurocognitive",
            "year": 2018
        },
        {
            "authors": [
                "Yiwen Wang",
                "Jennifer Hu",
                "Roger Levy",
                "Peng Qian."
            ],
            "title": "Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5604\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Alex Warstadt",
                "Samuel R. Bowman."
            ],
            "title": "What artificial neural networks can tell us about human language acquisition",
            "venue": "Shalom Lappin and JeanPhilippe Bernardy, editors, Algebraic Structures in Natural Language. Taylor & Francis.",
            "year": 2022
        },
        {
            "authors": [
                "Alex Warstadt",
                "Alicia Parrish",
                "Haokun Liu",
                "Anhad Mohananey",
                "Wei Peng",
                "Sheng-Fu Wang",
                "Samuel R. Bowman."
            ],
            "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English",
            "venue": "Transactions of the Association for Computational Linguistics, 8. Pub-",
            "year": 2020
        },
        {
            "authors": [
                "Taylor Webb",
                "Keith J. Holyoak",
                "Hongjing Lu"
            ],
            "title": "Emergent Analogical Reasoning in Large Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Albert Webson",
                "Alyssa Marie Loo",
                "Qinan Yu",
                "Ellie Pavlick"
            ],
            "title": "Are Language Models Worse than Humans at Following Prompts? It\u2019s Complicated",
            "year": 2023
        },
        {
            "authors": [
                "Albert Webson",
                "Ellie Pavlick"
            ],
            "title": "Do PromptBased Models Really Understand the Meaning of Their Prompts",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou"
            ],
            "title": "2022b. Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Wilcox",
                "Jon Gauthier",
                "Jennifer Hu",
                "Peng Qian",
                "Roger P. Levy."
            ],
            "title": "Learning syntactic structures from string input",
            "venue": "Shalom Lappin and JeanPhilippe Bernardy, editors, Algebraic Structures in Natural Language. Taylor & Francis.",
            "year": 2022
        },
        {
            "authors": [
                "Victor H. Yngve."
            ],
            "title": "A Model and an Hypothesis for Language Structure",
            "venue": "Proceedings of the American Philosophical Society, 104(5):444\u2013466. Publisher: American Philosophical Society.",
            "year": 1960
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Few technologies have been as exciting \u2014 and divisive \u2014 for language science as large language models (LLMs). LLMs are capable of incredibly sophisticated linguistic behaviors, which emerge through statistical learning with massive amounts of text data and highly expressive, domain-agnostic learning architectures. On the one hand, the success of these models has sparked a growing movement to treat them as candidate models of human language acquisition and processing (e.g., Baroni, 2022; Warstadt and Bowman, 2022; Wilcox et al., 2022; Contreras Kallens et al., 2023) \u2013 indeed, Piantadosi (2023) even claims that they \u201crefute\u201d Chomsky\u2019s approach to language. On the other hand, linguists have highlighted shortcomings of\nCode and data are available at https://github.com/jennhu/ metalinguistic-prompting.\ncurrent models that make them unsuitable as cognitive theories (e.g., Dupre, 2021; Lan et al., 2022; Katzir, 2023; Milway, 2023; Murphy, 2023).\nNo matter their theoretical position, researchers need a way to assess the capabilities of LLMs in order to substantiate such claims. The fundamental unit of LLM computation is P (token|context), which, in principle, can be directly read out from a model by accessing its output layer of vocabulary logits. The distribution that this implies over word strings reflects the model\u2019s linguistic generalizations: that is, a generative model of the language seen during training, which can be used to evaluate the likelihood of previously unseen strings. Direct measurements of model-derived string probabilities have revealed capabilities such as syntactic generalizations (e.g., Linzen et al., 2016; Futrell et al., 2019; Hu et al., 2020; Warstadt et al., 2020), semantic plausibility judgments (Kauf et al., 2022), and certain coherence inferences (Beyer et al., 2021).\nRecently, there has been a growing trend to use prompting to evaluate LLMs\u2019 capabilities. Prompting (popularized by Brown et al., 2020) enables end-to-end interaction with models through natural language, and can be done entirely through inference (i.e., without gradient updates). This method has revealed new classes of emergent abilities in LLMs, such as arithmetic, instruction-following, and grounded conceptual mappings (Brown et al., 2020; Wei et al., 2022a; Patel and Pavlick, 2022), as well as the ability to render sentence acceptability judgments (Dentella et al., 2023). From a sociological angle, prompting has also made LLM evaluations more accessible for domain experts in linguistics and cognitive science, sparking new discussion about the limitations of LLMs\u2019 abilities (Katzir, 2023; Dentella et al., 2023; Ullman, 2023). For example, Dentella et al. (2023) prompt GPT-3 to produce grammaticality judgments of infrequent linguistic constructions, and conclude that the model \u201cshow[s] a critical lack of understanding even of\nhigh-frequency words\u201d (p. 1). Similarly, Katzir (2023) argues that \u201cLLMs are poor theories of human linguistic cognition\u201d (p. 2) based on prompting ChatGPT to compare the well-formedness of two English sentences.\nAs these examples illustrate, researchers have been making substantial theoretical claims based on prompting. However, there is an important caveat to using prompts to evaluate models\u2019 linguistic knowledge. Prompt-based methods test not only whether a model represents the generalization of interest (e.g., a certain ordering of probabilities), but also whether the model can report the outcome of applying the generalization to the sentence in the prompt. In this way, prompting implicitly tests a new type of emergent ability \u2014 metalinguistic judgment \u2014 which has not yet been systematically explored as a way to evaluate model capabilities.\nTo demonstrate the difference between direct probability measurements and metalinguistic prompting, consider the case of English subjectverb agreement. A direct approach might compare the probability assigned by the model to singular and plural verbs, given a particular subject noun phrase (e.g., Linzen et al., 2016). For example, we might compare P (\u201cis\u201d) and P (\u201care\u201d), conditioned on the prefix given by (1-a). In contrast, a prompting approach might present a sentence prefix, and pose a question about this linguistic content. For example, we might compare P (\u201cis\u201d) and P (\u201care\u201d), conditioned on the prompt in (1-b).\n(1) a. The keys to the cabinet b. Here is a sentence: The keys to the\ncabinet... What word is most likely to come next?\nA model that perfectly performs the metalinguistic task posed in prompt (1-b) should have identical probability distributions over the next word given (1-a) and (1-b). However, there is no guarantee that a model\u2019s response to a metalinguistic prompt will match its underlying internal representations. In light of this, how should we interpret models\u2019 responses to metalinguistic prompts? How do these responses correspond to models\u2019 internal representations? And when should we use metalinguistic prompts as opposed to direct measurements? These questions are becoming increasingly important, as prompting plays a growing role in the debate about LLMs as models of human language processing.\nIn this study, we do not intend to take a stance\non this theoretical debate. Rather, our goal is to evaluate the validity of metalinguistic prompting as a way of measuring LLMs\u2019 internal knowledge. We pose two research questions: (1) How well do models perform under direct and metalinguistic evaluation methods? and (2) How consistent are the metalinguistic methods with the direct method? We investigate these questions through four experiments, covering a range of tasks and linguistic domains. Our findings (and their supporting figures) are summarized below:\n1. The metalinguistic judgments elicited from LLMs through prompting are not the same as quantities directly derived from model representations. (Figures 2 and 3)\n2. Direct probability measurements generally yield better or similar task performance, compared to metalinguistic prompting. (Figure 2)\n3. Minimal pairs help reveal models\u2019 generalization capacities, compared to isolated judgments. (Figure 2c vs. Figure 2d)\n4. In general, the less similar the task/prompt is to a direct probability measurement, the worse the alignment between metalinguistic and direct measurements. (Figure 3)\nTaken together, our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. These findings suggest a possible basis for a competence\u2013 performance distinction in LLMs: namely, the distinction between the information encoded in a model\u2019s isolated-sentence string probability distribution versus the model\u2019s behavioral responses to prompts. We discuss this topic in greater detail in Section 6.1.\nOur results also highlight the value that is lost as researchers move toward interacting with LLMs through closed APIs, where access to models\u2019 underlying probability distributions is limited. Indeed, only two days before acceptance of this paper to EMNLP, the ability to obtain arbitrary token logprobabilities from gpt-3.5-turbo-instruct was removed from the OpenAI API, reinforcing the timeliness of the issue. We urge future research to clearly motivate the evaluation methods used to assess LLM abilities, and highlight the importance of developing and using open-source models with access to internal probabilities."
        },
        {
            "heading": "2 Related work",
            "text": "Our study is related to model calibration, or the problem of estimating predictive uncertainty (e.g., Guo et al., 2017; Minderer et al., 2021; Kadavath et al., 2022; Mielke et al., 2022). Most relevant to our work, Kadavath et al. (2022) perform a broadscale evaluation of \u201chonesty\u201d in LMs. In particular, they analyze models\u2019 \u201ctruthfulness\u201d and \u201cselfknowledge\u201d, which are conceptually similar to our ground-truth and internal consistency evaluations. Similarly, Mielke et al. (2022) find that chatbots\u2019 expressions of confidence and doubt are poorly calibrated with the likelihood that the models\u2019 responses are actually correct. These approaches differ from ours in that they analyze models\u2019 metacognition by annotating verbalized expressions (e.g., \u201cI don\u2019t know, but...\u201d), or by using models to evaluate the correctness of their own generated answers.\nOther studies have also demonstrated that models can respond to prompts in unexpected ways (Khashabi et al., 2022; Min et al., 2022; Webson and Pavlick, 2022; Webson et al., 2023; Prasad et al., 2023), in some cases achieving successful outcomes even when prompts are misleading or incoherent. Similarly, Turpin et al. (2023) find that explanations elicited through chain-of-thought prompting (Nye et al., 2021; Wei et al., 2022b) can systematically misrepresent the underlying reasons and causes for a model\u2019s prediction. McCoy et al. (2023) also demonstrate how LLMs\u2019 responses to prompts are highly sensitive to word probabilities, due to their original training objectives.\nBegu\u0161 et al. (2023) also evaluate LLMs\u2019 \u201cmetalinguistic\u201d abilities, but in a different sense of the word. They investigate whether LLMs can perform theoretical analyses of the structures and regularities of linguistic expressions \u2014 for example, drawing the syntactic tree diagram of a given sentence in valid LATEX code. Begu\u0161 et al.\u2019s tested abilities are metalinguistic in the sense that they require\njudgments about linguistic objects given linguistic input. In contrast, our study investigates whether models can access and report their internal probability distributions through linguistic prompts."
        },
        {
            "heading": "3 General methods",
            "text": ""
        },
        {
            "heading": "3.1 Overview of tasks",
            "text": "Our experiments feature four tasks, summarized in Table 1. Together, the tasks cover both wordand sentence-level computations, as well as both isolated judgments and minimal-pair comparisons. Since we aim to analyze the validity of using metalinguistic prompts to reveal linguistic knowledge, the experiments also cover semantic plausibility and syntax as linguistic domains of interest.\nAs mentioned in Section 1, word prediction (Expt. 1) is a fundamental task for LLMs, and involves the most straightforward operation on models\u2019 representations: reading out the logits over the vocabulary in the output layer. Because we know that models represent next-token-probabilities, we can treat the direct probability measurements as providing ground-truth values. We therefore treat word prediction as a baseline task, where we can perform a tightly controlled comparison between probability measurements and metalinguistic prompting. The other three tasks induce an intuitive (partial) ordering in terms of their similarity to the baseline word prediction task (word comparison, sentence judgment, sentence comparison). We return to this ordering in Section 5.2."
        },
        {
            "heading": "3.2 Overview of prompts",
            "text": "In each experiment, we evaluate models using four methods: a direct method, and three zero-shot metalinguistic prompting methods. The direct method involves computing probabilities of tokens or full sentences based on the models\u2019 internal logits over vocabulary items. In contrast, the metalinguistic\nprompts ask a question or specify a task requiring a judgment about a linguistic expression.\nTaking direct probability measurements as our baseline evaluation method, we also identify an ordering of the three metalinguistic methods in terms of their similarity to baseline. In the MetaQuestionSimple and MetaQuestionComplex prompts, the linguistic object of interest (e.g., a sentence prefix) is linearly closest and farthest from the position where the model is asked to make a prediction, respectively. The MetaInstruct prompts are structured as an imperative instruction, and fall in between the two MetaQuestion* prompts. Tables 2- 4b show example prompts for all experiments.1"
        },
        {
            "heading": "3.3 Models",
            "text": "We test six models across all of our experiments: three Flan-T5 models (small, large, XL; Chung\n1To test the generalizability of our findings beyond English, we also conducted preliminary experiments in Mandarin Chinese. Details and results can be found in Appendix D.\net al., 2022),2 and three GPT-3/3.5 models (textcurie-001/GPT-3, text-davinci-002/GPT-3.5, textdavinci-003/GPT-3.5). Flan-T5 models were accessed through Huggingface, and GPT-3/3.5 models were accessed through the OpenAI API.\nThe Flan-T5 models have an encoder-decoder architecture and were pre-trained on a span corruption task before fine-tuning on a large collection of instruction-based tasks. To measure nextword probabilities under these models, we take the sentence prefix \u27e8w1, w2, . . . , wn\u22121\u27e9 (where wn is the word to be predicted) and append the sentinel token <extra_id_0>. We then create the output sequence <extra_id_0> wn, and sum the probabilities corresponding to the tokens of wn. To measure full-sentence probabilities, we compute a pseudo-likelihood inspired by Salazar et al.\u2019s (2020) method for scoring sentences under masked language models. Going left to right, we mask out each word in the sentence using the T5 sentinel tokens, and then sum the log probabilities assigned to each true word.\nWhile the Flan-T5 models differ only in size, the OpenAI models also differ in training regime: textcurie-001 is an autoregressive language model at its core (GPT-3; Brown et al., 2020), whereas textdavinci-002 has additional supervised fine-tuning, and text-davinci-003 has additional reinforcement learning (Ouyang et al., 2022).3"
        },
        {
            "heading": "4 Details of experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment 1: Word prediction",
            "text": "Task and stimuli. The aim of this experiment is to evaluate models\u2019 ability to predict the next word given a preceding context. Instead of a standard language modeling task, where models are evaluated on their ability to predict every word in a text, we use the simplified task of predicting the final word of a sentence. This makes the metalinguistic evaluations more tractable, as we only need to construct a single prompt for each item.\nWe use two datasets with contrasting style. Our first dataset, taken from Pereira et al. (2018) (\u201cP18\u201d), consists of 384 simple declarative sentences that state a fact about familiar concepts, such\n2Parameter counts: small 80M, large 780M, XL 3B. 3Many LLM evaluations use ChatGPT (Piantadosi, 2023; Katzir, 2023) or GPT-4 (Begu\u0161 et al., 2023; Webb et al., 2023; Moskvichev et al., 2023). However, we exclude these models from our analyses because the OpenAI API does not provide access to token probabilities for chat-based models.\nas accordion or butterfly. All pronouns are dereferenced, making the dataset useful for testing prediction in simple contexts with minimal dependencies.\nThere are at least two concerns with the P18 sentences. First, their simple structure might not be representative of the text that models encounter during training. Second, they have been publicly available since 2018, making it possible that they may be in the models\u2019 training data. To address these concerns, we constructed a second dataset (\u201cNews\u201d) containing sentences that are more naturalistic, but highly unlikely to occur in the training data. We used the NewsData tool4 to find English news articles published in the United States in the date range of March 20-26, 2023.5 The articles cover a span of topics, such as business, politics, and food. For each article, we construct a prefix by concatenating the headline with the first sentence (up to, but not including, the last word), separated by the string \u201c \u2013 \u201d. There are 222 items in total.\nPrompts. Example prompts are shown in Table 2 (only examples from the P18 corpus are shown, for simplicity). The Direct prompt feeds the sentence prefix to the model, and we measure the model\u2019s probability of the ground-truth next word (indicated\n4https://newsdata.io/ 5These are unlikely to be in the Flan-T5 training data, as the models were publicly released in 2022. The OpenAI model documentation also states that text-curie-001 only received training data up to October 2019, and the text-davinci-* models only received training data up to June 2021.\nin blue boldface). The other prompts are designed to elicit metalinguistic judgments, through questions (MetaQuestionSimple, MetaQuestionComplex) and instructions (MetaInstruct). As a conceptual illustration, Figure 1a shows a comparison of the Direct and MetaQuestionComplex methods.\nEvaluation. Our measure of task performance is the log probability assigned by each model to the ground-truth sentence continuation. To measure internal consistency (see Section 5.2), we analyzed the relationship between log probabilities assigned to ground-truth continuations, as measured by the direct method and each metalinguistic method."
        },
        {
            "heading": "4.2 Experiment 2: Word comparison",
            "text": "Task and stimuli. The aim of this experiment is to evaluate models\u2019 ability to judge which of two words is a more likely continuation of a sentence. While Experiment 1 tested word prediction without focus on any particular linguistic phenomenon, here we use the word-comparison task to assess knowledge of semantic plausibility.\nWe use a set of 395 minimal sentence pairs from Vassallo et al. (2018). Each pair consists of two sentences that differ only in the final word, which alters the plausibility of the described event (e.g., \u201cThe archer released the arrow/interview\u201d). Each sentence has a simple syntactic structure.\nPrompts. The prompts are similar to those from Experiment 1, but here we ask models to make a\ncomparison between two potential continuations of the sentence prefix (Table 3). For the Direct method, we present the model with the shared sentence prefix, and compare the probability of the plausible (e.g., \u201carrow\u201d; indicated in blue) and implausible (e.g., \u201cinterview\u201d; indicated in red) continuations. For the Meta* prompts, we create two versions of the prompt by shuffling the order in which the answer options are presented.\nEvaluation. Accuracy is the proportion of items where the model assigns higher probability to the plausible sentence continuation than to the implausible continuation. Random performance is 50%.\nTo measure internal consistency, for each evaluation method we computed the item-level log probability differentials between the plausible and implausible sentence continuations. We then computed the correlation between the differentials elicited by the direct method and the differentials elicited by each metalinguistic method."
        },
        {
            "heading": "4.3 Experiment 3a: Sentence judgment",
            "text": "Task and stimuli. The aim of this experiment is to evaluate models\u2019 ability to judge whether a sentence is a \u201cgood\u201d sentence of English. For any particular sentence, we compare the model\u2019s judgment of that sentence to the model\u2019s judgment of another sentence that forms a minimal pair, only differing in a critical syntactic feature that manipulates grammaticality or acceptability.\nWe use minimal pairs from two datasets designed to test knowledge of English syntax: SyntaxGym (Hu et al., 2020; Gauthier et al., 2020), and the Benchmark of Linguistic Minimal Pairs (BLiMP; Warstadt et al., 2020). Since SyntaxGym was not designed for full-sentence probability comparisons, we first converted the SyntaxGym materials into sentence-level minimal pairs.6 We then took a random sample of 15 items from each of the 23 remaining suites, resulting in 345 items. For BLiMP, we extracted the items that were compatible with the \u201csimple LM method\u201d and then took a random sample of 30 items from each of the 13 categories, resulting in 390 items. See Appendix A for details on the tested phenomena.\nPrompts. For the Direct method, we measure the probability of each sentence in the minimal pair. For the Meta* prompts, we construct a separate prompt for each sentence in the minimal pair asking whether the sentence is \u201ca good sentence of English\u201d (see Table 4a), and then compare the probability assigned by the model to \u201cYes\u201d versus \u201cNo\u201d.\nEvaluation. To measure accuracy for the direct condition, we compute the proportion of items where the model assigns higher probability to the grammatical sentence in the minimal pair. For\n6We created full sentences by combining content across regions, and then turned each success criterion inequality into a minimal pair (see Hu et al., 2020, for details). For simplicity, we omitted test suites with success criteria involving the conjunction of \u2265 3 inequalities, or probability differentials.\nthe metalinguistic prompts, we report balanced accuracy, or the mean of the true positive rate and true negative rate. A true positive occurs when the model assigns higher probability to \u201cYes\u201d than \u201cNo\u201d for a grammatical sentence, and a true negative occurs when the model assigns higher probability to \u201cNo\u201d than \u201cYes\u201d for an ungrammatical sentence.\nTo measure internal consistency, we compare the log probability differentials for each method. For the direct method, the differential is the difference in log probability of the grammatical and ungrammatical sentences. For each metalinguistic method, the differential is the difference in log probability of the \u201cYes\u201d token conditioned on the grammatical and ungrammatical sentence prompts."
        },
        {
            "heading": "4.4 Experiment 3b: Sentence comparison",
            "text": "Task and stimuli. As in Experiment 3a (Section 4.3), the goal is to measure models\u2019 syntactic judgments. However, instead of presenting the model with sentences in isolation and asking for judgments, in Experiment 3b we present the model with the minimal pair of sentences, and probe which sentence it takes to be a \u201cbetter\u201d sentence of English. For our stimuli, we use the same subsets of SyntaxGym and BLiMP as in Experiment 3a.\nPrompts. The direct evaluation method is the same as in Experiment 3a: we compare probabilities of each sentence in the minimal pair. For the metalinguistic prompts, we have a single prompt for each minimal pair that presents both sentences at once. We assign an identifier (1 or 2) to each sentence in the pair, present a multiple-choice question comparing both sentences, and compare the probabilities assigned by the model to each answer option (i.e., \u201c1\u201d or \u201c2\u201d). As in Experiment 2, we average model results over two versions of the prompt that counterbalance the order of answer options (for metalinguistic prompts). Example prompts are shown in Table 4b. As a conceptual illustration, Figure 1b shows a comparison of the Direct and MetaQuestionComplex methods.\nEvaluation. Accuracy is measured as the proportion of items where the model assigns higher probability to the grammatical sentence in the minimal pair (direct method), or to the answer option corresponding to the grammatical sentence (metalinguistic prompts). Random performance is 50%.\nTo measure internal consistency, we compare the log probability differentials between the grammatical and ungrammatical sentences (measured by the\ndirect method) to the log probability differentials between the answer options corresponding to the grammatical and ungrammatical sentences (measured by each metalinguistic prompting method)."
        },
        {
            "heading": "5 Results",
            "text": "We now return to our main research questions, laid out in the Introduction: (1) How well does each evaluation method perform on each task? (2) How consistent are the metalinguistic evaluation methods with the direct evaluation method? We address these in Sections 5.1 and 5.2, respectively."
        },
        {
            "heading": "5.1 Task performance",
            "text": "Result #1: Metalinguistic judgments are not the same as direct measurements. Figure 2 shows task performance for each experiment. At the coarsest level, the different methods (hues) yield different performance scores, demonstrating that metalinguistic and direct responses are not identical.\nResult #2: Direct measurements generally perform \u2265 metalinguistic methods. Aross all experiments, the direct method nearly always yields best performance of all tested methods. There are a few exceptions: in Experiment 1, Flan-T5-SM performs best under MetaInstruct, and Flan-T5-XL performs relatively well under MetaQuestionComplex, as do the davinci models in Experiment 2.\nResult #3: Minimal pairs help reveal models\u2019 generalization capacities. The difference between Experiments 3a and 3b lies in the presentation of minimal pairs. Comparing Figures 2c and 2d, we first note that the direct results (darkest bars) are identical by definition: they reflect comparisons of full-sentence probabilities. For the metalinguistic prompts, there is an increase in accuracy going from the isolated sentence judgments (Figure 2c) to minimal-pair comparisons (Figure 2d), for all models with above-chance performance."
        },
        {
            "heading": "5.2 Internal consistency",
            "text": "Result #4: Consistency gets worse as we get further from direct measurement of next-word probabilities. Figure 3 illustrates alignment between direct and metalinguistic measurements (see Appendix Figure 6 for by-model internal consistency results). Each cell shows the average correlation coefficient (Pearson\u2019s r) between the itemlevel differentials measured by the direct method\nand a particular metalinguistic prompting method.7\nRecall from Section 3 that the tasks and prompts in our study induce intuitive orderings in terms of how similar they are to word prediction (at the tasklevel) and direct probability measurements (at the prompt-level). The columns of Figure 3 (prompt types) are loosely ordered by similarity to direct probability measurements, and the rows (tasks) are loosely ordered by similarity to word prediction. Broadly speaking, we find that as distance in either dimension increases, the correlations get weaker. These results further support Result #1: while direct and metalinguistic responses are highly correlated for some combinations of tasks and prompts, the relationship is far from perfect."
        },
        {
            "heading": "6 Discussion",
            "text": "In this study, we compared metalinguistic prompting and direct measurements as ways of evaluating\n7For Experiment 1, the correlation is computed between the item-level probabilities of the ground-truth final word.\nLLMs\u2019 linguistic knowledge. Broadly, we find that metalinguistic judgments are inferior to direct measurements of token- or sentence-level probabilities. We also find evidence that minimal-pair comparisons help reveal models\u2019 generalization capacities.\nWe do not intend to claim that prompting should be categorically dispreferred in favor of other evaluation methods. Prompting is useful for eliciting open-ended responses, such as chain-of-thought reasoning. Metalinguistic prompting could also be used to ask questions that would be challenging to translate into direct probability measurements (e.g., \u201cWhich of the following two sentences is more semantically plausible, but less syntactically wellformed?\u201d). In addition, prompting lowers the technical barrier for domain experts to conduct LLM evaluations, which could contribute to the development of more robust behavioral benchmarks.\nWhat do our findings mean for researchers interested in the linguistic abilities of LLMs? While there are valid reasons to prefer both prompting\nand direct probability measurements, these methods will not necessarily generate consistent results. More specifically, our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive.\nWhile our paper focuses on the value of direct probability measurements for evaluating linguistic generalizations, other endeavors in cognitive science and machine learning also rely on access to LLM probabilities. For example, token probabilities enable multiple-choice evaluation and beam search, as well as investigating mode collapse (Janus, 2022) and obtaining quantities for Bayesian inference (e.g., Choi et al., 2022; Li et al., 2023; Lipkin et al., 2023).8 Thus, the value that is lost as we move toward closed APIs extends beyond linguistic analysis, further underscoring the importance of using and developing LLMs with open access to internal probabilities."
        },
        {
            "heading": "6.1 Competence vs. performance in LLMs",
            "text": "Humans use language in diverse contexts with varying task demands and constraints, but the underlying linguistic knowledge is relatively stable. This is the competence\u2013performance distinction (e.g., Yngve, 1960; Chomsky, 1965): an individual\u2019s performance in a particular context may not reflect an individual\u2019s full underlying competence. Whether it is productive to distinguish between\n8These examples were inspired by discussion by Tan Zhi Xuan on Twitter (https://twitter.com/xuanalogue/status/ 1637302507389984769).\n\u201ccompetence\u201d and \u201cperformance\u201d in an AI model has been a topic of debate. Katzir (2023) argues that \u201c[LLMs\u2019] behavior directly reflects their competence, and when they fail it is their competence that is at fault\u201d (pp. 4-5). We take Katzir to mean that an LLM\u2019s next-word probability distribution is deterministic given its architecture, weights, and the preceding context, and this probability distribution is always computed when the LLM is used. Therefore, as Katzir says, while humans may recover from linguistic errors (e.g., in agreement or parsing) given additional time or resources, \u201cfurther time and resources are of no use\u201d to LLMs. Firestone (2020) and Lampinen (2023), in contrast, argue that performance conditions need to be carefully controlled in both humans and machines in order to make fair comparisons between the two.9\nOur work suggests that if a competence\u2013 performance distinction is to be made for LLMs, a natural locus is the contrast between the information contained in an LLM\u2019s string probability distribution (corresponding to its competence) versus the behavior the LLM exhibits when prompted (corresponding to its performance). A model\u2019s failure to exhibit a linguistic generalization when prompted might not reflect a lack of the relevant information in its underlying conditional probability distributions, but instead an inability to access and behave in accordance with that information in response to a prompt that poses a metalinguistic query. This view remains consistent with the fact that LLM behavioral errors may be corrected when illustrative examples are included in the prompt \u2014 whether or not such prompts pose metalinguistic queries, they offer opportunity for in-context learning (Brown et al., 2020) \u2014 or by allowing the model to produce a reasoning trace before outputting an answer (Nye et al., 2021; Wei et al., 2022b; Kojima et al., 2022).\nTo conclude, prompting is not a substitute for direct probability measurements in LLMs. We underscore the importance of specifying the assumptions underlying methodological choices in LLM evaluation, and using open models with direct access to probabilities for scientific research. If our interactions with LLMs are limited to high-level prompting, we lose the opportunity to measure capabilities that could advance our understanding of these models and their relation to human language.\n9Hahn et al. (2022), for example, show that imposing performance constraints on LLMs (by degrading context representations) derives human language processing behavior in complex nested dependency constructions.\nLimitations\nOur experiments only test three types of metalinguistic prompts, and only perform zero-shot evaluations. In practice, the small number of metalinguistic prompt types was sufficient to illustrate the difference between direct and metalinguistic responses. However, it would be beneficial to consider more types of prompts to determine how well the phenomenon generalizes. We also note that models might achieve better task performance under the metalinguistic prompts with few-shot prompting or in-context learning. We did not include few-shot analyses due to space limitations, and because many recent LLM evaluations rely on zero-shot metalinguistic prompts. It remains to be seen whether metalinguistic and direct responses are better aligned when models have access to examples in the prompt.\nAnother limitation of our study is that we only tested a small class of models. An important direction for future work would be to replicate our experiments on models of different sizes and training objectives (e.g., chat-based models). We also note that the results from the OpenAI models are not necessarily reproducible due to the models being behind a closed API. Timestamps of our calls to the OpenAI API are available in our data files (https: //github.com/jennhu/metalinguistic-prompting).\nEthics Statement\nThis work does not release any new models or datasets. Instead, the goal is to provide insights into methodology for evaluating the internal knowledge of modern LLMs, and in turn contribute to the interpretability of these models. We hope that our results illustrate the importance of open access to model representations and the risks of relying on high-level API interactions for scientific research.\nWith that said, the broader ethical concerns about LLMs are still relevant to our work. LLMs have been shown to produce output that is factually incorrect, offensive, or discriminatory, and should therefore be used with extreme caution, especially in commercial applications or user-facing settings. Any demonstrations of LLMs\u2019 linguistic generalizations should not imply that they are safe to use, or can be expected to behave in a way that is aligned with human preferences and values."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Jon Gauthier and the anonymous reviewers for feedback on earlier versions of this work, as well as Peng Qian for translating the prompts into Chinese. J.H. gratefully acknowledges support from an NSF Graduate Research Fellowship (#1745302), an NSF Doctoral Dissertation Research Improvement Grant (BCS-2116918), and the Simons Center for the Social Brain at MIT. R.P.L. gratefully acknowledges support from NIH grant U01-NS121471, a Newton Brain Science award, and the Simons Center for the Social Brain at MIT."
        },
        {
            "heading": "A Details of syntactic phenomena",
            "text": "Table 5 summarizes the syntactic phenomena covered in the datasets used in Experiments 3a and 3b, which were taken from SyntaxGym (Hu et al., 2020; Gauthier et al., 2020) and BLiMP (Warstadt et al., 2020)."
        },
        {
            "heading": "B Dataset-level task performance",
            "text": "Figure 4 shows task performance for each of the tested datasets in Experiment 1 (left: P18; right: News). Figure 5 shows task performance for each of the tested datasets in Experiments 3a and 3b (left: SyntaxGym; right: BLiMP)."
        },
        {
            "heading": "C Relationship between metalinguistic and direct predictions",
            "text": "Figure 6 shows the average Pearson r correlations between metalinguistic and direct responses, for each of the tested models.\nFigures 7 to 10 show the relationship between responses measured by the direct and metalinguistic prompting methods, for Experiments 1-3b, respectively."
        },
        {
            "heading": "D Experiments in Mandarin Chinese",
            "text": "To explore the robustness of our results beyond English, we performed a preliminary investigation of GPT-3.5 (text-davinci-003) on materials in Mandarin Chinese. We first consulted a native speaker to translate the metalinguistic prompts into Chinese. We then tested GPT-3.5 on two datasets: (1) a set of recent news articles for word prediction, mirroring Experiment 1; and (2) a set of controlled minimal pairs that cover semantic and syntactic phenomena (Wang et al., 2021), mirroring a combination of the phenomena tested in Experiments 2 and 3. We tested the tasks of Experiments 3a and 3b on this set of minimal pairs.\nFigure 11 shows the results (task performance) for Experiments 1, 3a, and 3b in Chinese. Like our English experiments, in all our Chinese experiments we find that metalinguistic prompting and direct probability measurements do not yield the same results. Like our Experiment 1 in English, we also find that in the Chinese word prediction task, GPT-3.5 assigns highest probability to the groundtruth continuation under the \u201cDirect\u201d method. We also find that model performance improves when using minimal pairs, mirroring our original findings comparing Experiments 3a and 3b. These findings demonstrate how our English results may generalize to languages with different syntactic structures and grammatical properties.\nHowever, we also found differences between the Chinese and English results: in the Chinese version of Experiment 3b, the \u201cDirect\u201d method underperformed the metalinguistic methods (accuracy scores: Direct 0.6; others 0.8). One potential explanation for this is that according to the OpenAI documentation, the models are \u201coptimized for use in English,\u201d although in practice they may work well for other languages.10 Therefore, the models may not be well-suited to scoring probabilities of Chinese sentences with no context (as in the Direct condition); instead, they may benefit from seeing additional Chinese text in the prompt before the sentence (as in the metalinguistic conditions).\n10https://help.openai.com/en/articles/6742369-how-do-iuse-the-openai-api-in-different-languages\nFlan T5\n\u00a0SM\nFlan T5\n\u00a0LG\nFlan T5\n\u00a0XL\ntext cur\nie0\n01\ntext da\nvinc\ni00\n2\ntext da\nvinc\ni00\n3\nFlan T5\n\u00a0SM\nFlan T5\n\u00a0LG\nFlan T5\n\u00a0XL\ntext cur\nie0\n01\ntext da\nvinc\ni00\n2\ntext da\nvinc\ni00\n3"
        }
    ],
    "title": "Prompting is not a substitute for probability measurements in large language models",
    "year": 2023
}