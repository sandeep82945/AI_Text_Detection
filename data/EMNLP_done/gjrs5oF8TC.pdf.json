{
    "abstractText": "Recent advancements in Large language models (LLMs) have enabled them to hold free form conversations over multiple turns, but they exhibit a tendency to make unfounded and incorrect statements, commonly known as hallucinations. In particular, LLMs hallucinate frequently when given invalid questions, i.e. ones with incorrect assumptions. The most common approach to evaluate LLMs on hallucinations is to test them on Question Answering (QA) test sets such as TruthfulQA. However, LLMs are increasingly pretrained on massive text corpora scraped from the Internet, which may inevitably expose these test sets to the model during training, leading eventually to an overestimation of model performances on these test sets. In this work, we present an alternative framework to address this risk and to foster further research towards making LLMs robust against invalid questions. We name our framework INVITE: a testbed of automatically generated INValId questions to evaluaTE large language models for hallucinations. In each instantiation, our framework is set up to create a fresh batch of invalid questions by distorting valid facts in which subjects or objects are replaced by similar entities. We evaluate several state of the art LLMs against a testset generated by our framework and highlight its capacity to trigger hallucinations in these models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Anil Ramakrishna"
        },
        {
            "affiliations": [],
            "name": "Rahul Gupta"
        },
        {
            "affiliations": [],
            "name": "Jens Lehmann"
        },
        {
            "affiliations": [],
            "name": "Morteza Ziyadi"
        }
    ],
    "id": "SP:b49d594e0a003c7716ab06b3f8bbd8b893a21db9",
    "references": [
        {
            "authors": [
                "Razvan Azamfirei",
                "Sapna R Kudchadkar",
                "James Fackler."
            ],
            "title": "Large language models and the perils of their hallucinations",
            "venue": "Critical Care, 27(1):1\u20132.",
            "year": 2023
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Sid Black",
                "Gao Leo",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "venue": "If you use this software, please cite it using these metadata.",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Daphne Ippolito",
                "Matthew Jagielski",
                "Katherine Lee",
                "Florian Tramer",
                "Chiyuan Zhang."
            ],
            "title": "Quantifying memorization across neural language models",
            "venue": "arXiv preprint arXiv:2202.07646.",
            "year": 2022
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "arXiv preprint arXiv:2207.10397.",
            "year": 2022
        },
        {
            "authors": [
                "Nan Duan",
                "Duyu Tang",
                "Peng Chen",
                "Ming Zhou."
            ],
            "title": "Question generation for question answering",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866\u2013874, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Emilio Ferrara."
            ],
            "title": "Should chatgpt be biased? challenges and risks of bias in large language models",
            "venue": "arXiv preprint arXiv:2304.03738.",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2020
        },
        {
            "authors": [
                "Julia Holze"
            ],
            "title": "Dbpedia snapshot 2022-12 release. Accessed: 2023-06-18",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Adversarial examples for evaluating reading comprehension systems",
            "venue": "arXiv preprint arXiv:1707.07328.",
            "year": 2017
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S Weld",
                "Luke Zettlemoyer."
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "arXiv preprint arXiv:1705.03551.",
            "year": 2017
        },
        {
            "authors": [
                "Jens Lehmann",
                "Robert Isele",
                "Max Jakob",
                "Anja Jentzsch",
                "Dimitris Kontokostas",
                "Pablo N Mendes",
                "Sebastian Hellmann",
                "Mohamed Morsey",
                "Patrick Van Kleef",
                "S\u00f6ren Auer"
            ],
            "title": "Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia",
            "year": 2015
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "Truthfulqa: Measuring how models mimic human falsehoods",
            "venue": "arXiv preprint arXiv:2109.07958.",
            "year": 2021
        },
        {
            "authors": [
                "Sergey I Nikolenko."
            ],
            "title": "Synthetic data for deep learning",
            "venue": "arXiv preprint arXiv:1909.11512.",
            "year": 2019
        },
        {
            "authors": [
                "OpenAI. a"
            ],
            "title": "Models - openai api",
            "year": 2023
        },
        {
            "authors": [
                "Nedjma Ousidhoum",
                "Xinran Zhao",
                "Tianqing Fang",
                "Yangqiu Song",
                "Dit-Yan Yeung"
            ],
            "title": "Probing toxic content in large pre-trained language models",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for squad",
            "venue": "arXiv preprint arXiv:1806.03822.",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Krithika Ramesh",
                "Sunayana Sitaram",
                "Monojit Choudhury."
            ],
            "title": "Fairness in language models beyond english: Gaps and challenges",
            "venue": "arXiv preprint arXiv:2302.12578.",
            "year": 2023
        },
        {
            "authors": [
                "Max Sch\u00e4fer",
                "Sarah Nadi",
                "Aryaz Eghbali",
                "Frank Tip."
            ],
            "title": "Adaptive test generation using a large language model",
            "venue": "arXiv preprint arXiv:2302.06527.",
            "year": 2023
        },
        {
            "authors": [
                "Ben Wang."
            ],
            "title": "Mesh-Transformer-JAX: ModelParallel Implementation of Transformer Language Model with JAX",
            "venue": "https://github.com/ kingoflolz/mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Laura Weidinger",
                "John Mellor",
                "Maribeth Rauh",
                "Conor Griffin",
                "Jonathan Uesato",
                "Po-Sen Huang",
                "Myra Cheng",
                "Mia Glaese",
                "Borja Balle",
                "Atoosa Kasirzadeh"
            ],
            "title": "Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359",
            "year": 2021
        },
        {
            "authors": [
                "Yuheng Zha",
                "Yichi Yang",
                "Ruichen Li",
                "Zhiting Hu."
            ],
            "title": "AlignScore: Evaluating factual consistency with a unified alignment function",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Despite their recent success, LLMs have long been known to exhibit several patterns of concern (Weidinger et al., 2021) such as generating statements which may be toxic (Ousidhoum et al., 2021), biased (Ferrara, 2023), unfair (Ramesh et al., 2023) and factually incorrect (Azamfirei et al., 2023). The last pattern of generating factually incorrect yet seemingly confident statements is commonly labeled as hallucinations in the literature (Ji et al., 2023). It is an important area of study since the confident tone of these generated statements can lead\nto end users accepting them as accurate without any subsequent validation.\nModel hallucinations occur in a variety of textual generative applications such as NLG, MT, QA, dialog systems, data to text systems, etc. It is believed to be caused by discrepancies in data used to train the models, or in the model training itself (Ji et al., 2023). Hallucinations are also believed to be caused by the supervised fine tuning process in which the model may learn to make factually ungrounded connections within its parametric memory in order to accurately answer the current question it is being trained on, which can trigger new ungrounded responses as hallucinations during inference.\nTypical approaches to evaluate newly developed models for hallucinations have been to test them on Question Answering datasets such as TruthfulQA (Lin et al., 2021), which provides a curated set of challenging questions with valid answers, against which the model generated responses are compared. However, this approach of using a fixed test set with LLMs is inherently limited; the typical development cycle of a new LLM release involves pretraining on large text corpora regularly scraped from the Internet, and any new challenge dataset may eventually get scraped into this pre-training corpus. Given that LLMs have been shown to memorize training data (Carlini et al., 2022), this form of data leakage can lead to a false sense of improvement on the challenge test set in subsequent model releases. To address this risk, in this work, we instead propose to test LLM models using an evaluation framework which uses carefully crafted rules to create new challenge questions in each round. We call our framework INVITE: a testbed of automatically generated INValId questions to evaluaTE large language models for hallucinations. INVITE leverages valid facts from knowledge bases to create new invalid questions which may not have an answer. Our framework can be used to evaluate\nnew LLM release candidates on their robustness against invalid questions which can trigger specific forms of hallucinations, as well as when developing new algorithms to mitigate hallucinations in existing models. The key contributions of our work are as follows:\n\u2022 We create a new framework to create invalid questions to evaluate robustness of LLMs against hallucinations1.\n\u2022 We test our framework on several latest LLMs, exploring different model sizes and training datasets.\n\u2022 We conduct a pilot human evaluation study on the generated responses for these questions, and highlight the effectiveness of the test sets in triggering hallucinations in the models being evaluated."
        },
        {
            "heading": "2 Related Work",
            "text": "Question Answering datasets A number of QA datasets are available in literature to test LLMs for hallucinations, including TruthfulQA (Lin et al., 2021), SQUAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), among others. While challenging and effective, all of these test sets suffer from the previously described risk of possibly getting scraped and consumed in model training. Adversarial QA datasets The QA datasets listed above test models in their ability to retain facts, and new models are tested by comparing their response to these questions against an expected answer. However, we argue that this strategy alone would not test a model against all possible failure modes related to hallucinations since this issue stems from the models\u2019 ability to concoct new facts as statements. Even if a model were to learn the correct answer to a particular QA question, it can still retain the general tendency to hallucinate. Hence, a more robust strategy would be to also test these models using adversarial questions with invalid assumptions. A model capable of avoiding hallucinations would identify that it does not have an answer to the question, or detect that the question itself is not plausible and that it cannot generate an answer, and hence choose to disengage.\nNotable adversarial datasets in NLP literature include (Jia and Liang, 2017), where the authors\n1Full code available at https://github.com/amazonscience/invite-llm-hallucinations.\nused a rule based framework to add adversarial statements to passages from reading comprehension task in order to confuse target models. Subsequently, Rajpurkar et al. (2018) developed SQUAD 2, a richer set of unanswerable questions using human annotators for the same reading comprehension task. While rich in diversity and volume, these datasets still suffer from the same risk of getting consumed in model training noted above. Further, their datasets are limited to reading comprehension tasks and hence do not necessarily test the full boundary of a model\u2019s knowledge. Automated Testset Generation Automatically created unit tests have been explored in deterministic applications such as software testing (Chen et al., 2022; Sch\u00e4fer et al., 2023). With machine learning models such as LLMs, we can leverage a variety of generative models to create new datasets (Duan et al., 2017; Nikolenko, 2019), but to the best of our knowledge, no prior works have tried to generate questions with invalid assumptions. Our proposed approach addresses this gap by setting up a framework to automatically create a new test set of challenge questions with verifiably invalid assumptions, which are likely to trigger hallucinations in the target model."
        },
        {
            "heading": "3 The INVITE Framework",
            "text": "To create new test questions, we collect valid facts from a knowledge base and distort these to create new unanswerable questions. We describe our process in more detail below, and create a test set using this framework."
        },
        {
            "heading": "3.1 Creating Invalid Questions",
            "text": "We use the DBpedia knowledge base (Lehmann et al., 2015) as a source of valid facts to create our questions. The choice of knowledge base here is arbitrary and can be replaced by an alternate appli-\ncation specific knowledge base as necessary. DBpedia extracts structured factual information from Wikipedia, the world\u2019s largest encyclopedia. It contains a large volume of facts which are stored in the Resource Description Framework (RDF) format of subject\u2013predicate\u2013object triples. The most recent release of DBpedia contains over 850 million such factual triples (Holze), making it a decidedly rich source of information to create new test questions for our task. Of these, we use a subset of 42 million triples containing facts about objects and literals extracted from the Wikipedia Infoboxes, which are reported to be of higher quality because of their standardized format. For operational simplicity, we limit our scope here to the 100 most frequent predicate types by volume from this subset. We further discard noisy predicate types which contain ambiguous entries after manual inspection (for example, we discard the nationality predicate type since it contains answers of the form country-name as well as citizen/people of country-name, making facts of this type difficult to fit in a consistent question template). The exact list of predicates selected in our dataset creation is listed in Table 1.\nTo create new questions, we first curated over 300 predicate specific template questions which were manually crafted by annotators on Amazon Mechanical Turk, and further denoised by the authors. Next, we further refined a subset of these to create high quality question templates by posing the questions on a search engine, and iterated this process until the responses were unambiguous. We also created template answers for these selected high quality question templates, which we use in our subsequent experiments reported below. The specific prompts we used in our experiments are listed in Appendix A2.\nFor each new question generation, given a predicate, we first sample a fact triple from this predicate type and create a valid question using the corresponding template. Next, to create the invalid question, we create an invalid fact triple by sampling new subjects or objects found in facts from the same predicate type. We verify that this new triple does not exist as factual predicate in the dataset; if such a triple exists, then our created fact is actually valid, so we discard the same and repeat the sampling process above until we have an invalid triple. Given this (invalid) triple, we use our template for\n2Our curated question templates, along with model responses with labels can be downloaded from https://github.com/amazon-science/invite-llm-hallucinations.\nthis predicate type to create a new invalid question and a corresponding answer, subsequently adding both to our test set.\nQuestions with Invalid Dates In addition to the questions extracted above, we create two more categories containing questions with invalid dates. Using regular expressions, we sample questions containing dates and years from the TriviaQA dataset\u2019s test set (Joshi et al., 2017) and create various distortions before adding these questions to our test set. Specifically, we distort full dates containing months by randomly selecting a new date beyond valid dates of the month (for example: March 32nd, 2023) and replace the old date. Similarly, we distort years by randomly sampling a new year from [2025, 2100] and replace the old year."
        },
        {
            "heading": "4 Experiments",
            "text": "Testing model responses for hallucinations is a challenging task which needs a comprehensive fact verification system for automated evaluations. We instead use human verification to test for hallucinations in the generated responses. To evaluate the efficacy of our proposed framework, we first created a pilot test set of 100 questions, sampling uniformly from each category listed in Table 1. Next we generated responses to each of these questions using the models described below, leading to a total set of 600 generated responses. Finally, we manually examine these generations and label them for hallucinations, utilizing a search engine for additional validation of model responses. While manually labeling samples, we only treat responses which explicitly make an inaccurate statement as hallucinations, treating all others (including empty or degenerate responses) as non-hallucinations."
        },
        {
            "heading": "4.1 Models",
            "text": "We evaluate the test set described in Section 4 on a list of open source and proprietary large language\nmodels described below. We chose a diverse set of models with varied size, and training datasets for a detailed evaluation of our test set. All open source models were downloaded from Huggingface and evaluated on Nvidia A100 Tensor Core GPUs, while the proprietary GPT models were evaluated using OpenAI APIs3. We ran inference without decoder sampling to further reduce the models\u2019 tendency for hallucinations, and stopped inference after 150 tokens.\nGPT-Neo-2.7B GPT-Neo (Black et al., 2021) is a 2.7 billion parameter model developed by EleutherAI, and it follows the architecture of GPT3. It was trained on the Pile (Gao et al., 2020), a large-scale dataset curated by EleutherAI for this task, which spans diverse tasks.\nGPT-J-6B GPT-J-6B is 6 billion parameter model trained using Mesh Transformer JAX (Wang, 2021), and also trained on the Pile dataset from EleutherAI.\nOpen-LLaMA-7b-Open-Instruct This is an instruction tuned, open sourced release of the 7 billion parameter LLaMA model (Touvron et al., 2023), trained on the Open-Instruct-v1 dataset which consists of 63000 instruction training samples.\nRedPajama-INCITE-7B-Instruct The RedPajama models were developed by a team of open source developers from several organizations. The base model was trained on the RedPajama dataset, a 1T token open-source implementation of the LLaMA dataset. Several model variants were available at the time of writing, and we used the 7B instruction tuned version of the model in our evaluations.\nGPT models We also ran evaluations on OpenAI\u2019s GPT3.5-Turbo (OpenAI, a) and GPT4 (OpenAI, b) models. GPT3.5-Turbo is a text only model which supports understanding and generation of\n3Accessed on August 27, 2023\nnatural language and code, while GPT4 is OpenAI\u2019s most powerful LLM at the time of this writing. We used model snapshots from June 13, 2023 in both cases. These models are first pretrained on a large corpus and subsequently aligned using Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). Additional details have not been disclosed by OpenAI."
        },
        {
            "heading": "5 Results",
            "text": "We present our hallucination rates for the models described above in Table 2. As observed in the table, all the open source models exhibit significant rates of hallucinations triggered by invalid assumptions in the test questions. Examples of the generated hallucinations from each model are shown in Table 4. Among all the open source models, RedPajama-7B exhibited the least hallucination rate at 81% while LLaMA-7B demonstrated the highest rate of hallucinations at 88%, closely followed by GPTNeo-2.7B (83%) and GPTJ-6B (82%).\nOn the other hand, OpenAI\u2019s GPT3.5-Turbo and GPT4 models showed considerably lower (yet nontrivial) hallucination rates at 17% and 6% respectively. Since OpenAI does not release internal details of these models, it is unclear if they incorporate any form of external knowledge retrieval which would provide an effective solution to handle questions with invalid assumptions. However, given the growing popularity of LLMs and the demand to deploy them in various low latency regimes, smaller ungrounded open source models such as those evaluated above are likely production candidates in many applications, and INVITE provides a valuable framework to evaluate these for hallucinations, without relying on a fixed test set which runs the risk of getting ingested in future training data crawls. Further, even in GPT4 and GPT3.5-Turbo,\nidentifying the 6% and 17% examples which were hallucinated highlights existing gaps in these models and can provide valuable insights needed to make them safe for deployment."
        },
        {
            "heading": "5.1 Automated Evaluation Metrics",
            "text": "Since human labeling is slow and expensive, they may not always be feasible to obtain. As an alternative, we also computed various automated metrics commonly employed in evaluating natural language generations. To estimate these metrics, we compare model generations to the invalid questions against reference answers created using the category wise answer templates listed in Appendix A. As additional references, we also included two disengagement answers: I don\u2019t know and I can\u2019t answer that for comparison. Table 3 lists estimates for these metrics. From our experiments, n-gram count based metrics such BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) do not correlate well with human labeled estimates of hallucination rate. Model based metrics such as BERTScore (Zhang* et al., 2020) and AlignScore (Zha et al., 2023) perform rela-\ntively better than n-gram based metrics as shown in Table 3, but they still do not perfectly align with gold standard labels from human labeling, which appears to be the most reliable estimate of whether a model response is hallucinated."
        },
        {
            "heading": "6 Conclusion",
            "text": "We developed a new framework called INVITE to evaluate large language models for hallucinations, in which new test questions are automatically generated in each round, thereby avoiding reliance on fixed test sets which carry the risk of getting ingested in future training corpora. Our framework creates a diverse (in both domains and entities) set of questions, obtained by distorting valid factual triples from a knowledge base. It is also flexible and easily extensible to new knowledge bases and predicate types. We evaluate an example test set generated by our framework against several state of the art LLMs, establishing the challenging nature of questions generated by our framework. Implementation of our framework, along with the curated question templates and labeled model responses are being released with the paper.\nLimitations and Future Work\nAny test set of limited size would not cover the entire possible space of invalid questions. Instead, we chose to sample a random subset of this space and obtain an empirical estimate of the model performance.\nWe define an invalid fact triple/relationship as one which does not exist in the knowledge base and this assumption maybe violated in boundary cases where facts may not have been entered into Wikipedia; however, we expect this to be marginal.\nGenerating invalid questions from a fixed set of templates may lead to limited diversity in questions. We used annotators from Amazon Mechanical Turk to address this but our coverage was limited to a relatively small set of predicates, which we will expand on in future work.\nIn current version of the framework, we only create single hop questions with two entities for simplicity. We will expand on this by creating multi-hop questions which necessitate complex reasoning in future work.\nFinally, in future work we can also leverage this framework in LLM training by using such questions to teach the boundary of plausible knowledge to the model."
        },
        {
            "heading": "A Question Templates",
            "text": "We list questions used in our experiments in Table 5."
        }
    ],
    "title": "INVITE: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations",
    "year": 2023
}