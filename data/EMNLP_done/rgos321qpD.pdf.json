{
    "abstractText": "The increasing use of foundation models highlights the urgent need to address and eliminate implicit biases present in them that arise during pretraining. In this paper, we introduce PEFTDebias, a novel approach that employs parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation models. PEFTDebias consists of two main phases: an upstream phase for acquiring debiasing parameters along a specific bias axis, and a downstream phase where these parameters are incorporated into the model and frozen during the fine-tuning process. By evaluating on four datasets across two bias axes namely gender and race, we find that downstream biases can be effectively reduced with PEFTs. In addition, we show that these parameters possess axis-specific debiasing characteristics, enabling their effective transferability in mitigating biases in various downstream tasks. To ensure reproducibility, we release the code to do our experiments1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sumit Agarwal"
        },
        {
            "affiliations": [],
            "name": "Aditya Srikanth Veerubhotla"
        },
        {
            "affiliations": [],
            "name": "Srijan Bansal"
        }
    ],
    "id": "SP:6c2b0448ab0db6d3d8bca8f208ebddb0c2fb42da",
    "references": [
        {
            "authors": [
                "Jaimeen Ahn",
                "Alice Oh."
            ],
            "title": "Mitigating languagedependent ethnic bias in BERT",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 533\u2013549, Online and Punta Cana, Dominican Republic. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Ponti",
                "Anna Korhonen",
                "Ivan Vuli\u0107."
            ],
            "title": "Composable sparse fine-tuning for crosslingual transfer",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1778\u20131796,",
            "year": 2022
        },
        {
            "authors": [
                "Soumya Barikeri",
                "Anne Lauscher",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Zou",
                "Venkatesh Saligrama",
                "Adam Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "Proceedings of the 30th International Conference on Neural Information",
            "year": 2016
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Aylin Caliskan",
                "Joanna J. Bryson",
                "Arvind Narayanan."
            ],
            "title": "Semantics derived automatically from language corpora contain human-like biases",
            "venue": "Science, 356(6334):183\u2013186.",
            "year": 2017
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai."
            ],
            "title": "Bias in bios",
            "venue": "Proceedings of the Conference on Fairness, Account-",
            "year": 2019
        },
        {
            "authors": [
                "Sunipa Dev",
                "Tao Li",
                "Jeff Phillips",
                "Vivek Srikumar"
            ],
            "title": "On measuring and mitigating biased inferences of word embeddings",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Emily Dinan",
                "Angela Fan",
                "Ledell Wu",
                "Jason Weston",
                "Douwe Kiela",
                "Adina Williams."
            ],
            "title": "Multidimensional gender bias classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Sun"
            ],
            "title": "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models",
            "year": 2022
        },
        {
            "authors": [
                "Lukas Hauzenberger",
                "Shahed Masoudian",
                "Deepak Kumar",
                "Markus Schedl",
                "Navid Rekabsaz."
            ],
            "title": "Modular and on-demand bias mitigation with attribute-removal subnetworks",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Jacqueline He",
                "Mengzhou Xia",
                "Christiane Fellbaum",
                "Danqi Chen."
            ],
            "title": "Mabel: Attenuating gender bias using textual entailment data",
            "venue": "arXiv preprint arXiv:2210.14975.",
            "year": 2022
        },
        {
            "authors": [
                "Marius Hessenthaler",
                "Emma Strubell",
                "Dirk Hovy",
                "Anne Lauscher."
            ],
            "title": "Bridging fairness and environmental sustainability in natural language processing",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "year": 2021
        },
        {
            "authors": [
                "Sophie Jentzsch",
                "Cigdem Turan."
            ],
            "title": "Gender bias in BERT - measuring and analysing biases through sentiment rating in a realistic downstream classification task",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Xisen Jin",
                "Francesco Barbieri",
                "Brendan Kennedy",
                "Aida Mostafazadeh Davani",
                "Leonardo Neves",
                "Xiang Ren."
            ],
            "title": "On transferability of bias mitigation effects in language model fine-tuning",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Brendan Kennedy",
                "Mohammad Atari",
                "Aida M Davani",
                "Leigh Yeh",
                "Ali Omrani",
                "Yehsong Kim",
                "Kris Coombs",
                "Shreya Havaldar",
                "Gwenyth Portillo-Wightman",
                "Elaine Gonzalez"
            ],
            "title": "Introducing the gab hate corpus: Defining and applying hate-based rhetoric",
            "year": 2018
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Deepak Kumar",
                "Oleg Lesota",
                "George Zerveas",
                "Daniel Cohen",
                "Carsten Eickhoff",
                "Markus Schedl",
                "Navid Rekabsaz."
            ],
            "title": "Parameter-efficient modularised bias mitigation via AdapterFusion",
            "venue": "Proceedings of the 17th Conference of the European Chapter",
            "year": 2023
        },
        {
            "authors": [
                "Keita Kurita",
                "Nidhi Vyas",
                "Ayush Pareek",
                "Alan W Black",
                "Yulia Tsvetkov."
            ],
            "title": "Measuring bias in contextualized word representations",
            "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 166\u2013172, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Faisal Ladhak",
                "Esin Durmus",
                "Mirac Suzgun",
                "Tianyi Zhang",
                "Dan Jurafsky",
                "Kathleen McKeown",
                "Tatsunori Hashimoto."
            ],
            "title": "When do pre-training biases propagate to downstream tasks? a case study in text summarization",
            "venue": "Proceedings of the 17th",
            "year": 2023
        },
        {
            "authors": [
                "Anne Lauscher",
                "Tobias Lueken",
                "Goran Glava\u0161"
            ],
            "title": "Sustainable modular debiasing of language models",
            "year": 2021
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Irene Mengze Li",
                "Emily Zheng",
                "Yao Chong Lim",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "Towards debiasing sentence representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Liu",
                "Wentao Wang",
                "Yiqi Wang",
                "Hui Liu",
                "Zitao Liu",
                "Jiliang Tang."
            ],
            "title": "Mitigating gender bias for neural dialogue generation with adversarial learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel R. Bowman."
            ],
            "title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Ji Ho Park",
                "Jamin Shin",
                "Pascale Fung."
            ],
            "title": "Reducing gender bias in abusive language detection",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799\u20132804, Brussels, Belgium. Association for Com-",
            "year": 2018
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterFusion: Non-destructive task composition for transfer learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Hila Gonen",
                "Michael Twiton",
                "Yoav Goldberg."
            ],
            "title": "Null it out: Guarding protected attributes by iterative nullspace projection",
            "venue": "Proceedings of the 58th Annual Meeting of",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Steed",
                "Swetasudha Panda",
                "Ari Kobren",
                "Michael Wick."
            ],
            "title": "Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Cagri Toraman",
                "Furkan \u015eahinu\u00e7",
                "Eyup Yilmaz."
            ],
            "title": "Large-scale hate speech detection with crossdomain transfer",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2215\u20132225, Marseille, France. European Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Kellie Webster",
                "Xuezhi Wang",
                "Ian Tenney",
                "Alex Beutel",
                "Emily Pitler",
                "Ellie Pavlick",
                "Jilin Chen",
                "Ed Chi",
                "Slav Petrov"
            ],
            "title": "Measuring and reducing gendered correlations in pre-trained models",
            "year": 2021
        },
        {
            "authors": [
                "Kellie Webster",
                "Xuezhi Wang",
                "Ian Tenney",
                "Alex Beutel",
                "Emily Pitler",
                "Ellie Pavlick",
                "Jilin Chen",
                "Slav Petrov."
            ],
            "title": "Measuring and reducing gendered correlations in pre-trained models",
            "venue": "ArXiv, abs/2010.06032.",
            "year": 2020
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Brian Hu Zhang",
                "Blake Lemoine",
                "Margaret Mitchell."
            ],
            "title": "Mitigating unwanted biases with adversarial learning",
            "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201918, page 335\u2013340, New York, NY, USA. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Guanhua Zhang",
                "Bing Bai",
                "Junqi Zhang",
                "Kun Bai",
                "Conghui Zhu",
                "Tiejun Zhao."
            ],
            "title": "Demographics should not be the reason of toxicity: Mitigating discrimination in text classifications with instance weighting",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Ran Zmigrod",
                "Sabrina J. Mielke",
                "Hanna Wallach",
                "Ryan Cotterell."
            ],
            "title": "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "asian",
                "(black",
                "white",
                "asian) A"
            ],
            "title": "Paramter Efficient Fine-Tuning (PEFT) We explore the use of multiple PEFTs, Adapters",
            "venue": "(Pfeiffer et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Tuning : (Lester"
            ],
            "title": "2021) which involves incorporating task-specific vectors (prompts) into the input sequence, LoRA : (Hu et al., 2021) which integrates trainable low-rank matrices into transformer layers in order to approximate weight",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "The increasing use of foundation models highlights the urgent need to address and eliminate implicit biases present in them that arise during pretraining. In this paper, we introduce PEFTDebias, a novel approach that employs parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation models. PEFTDebias consists of two main phases: an upstream phase for acquiring debiasing parameters along a specific bias axis, and a downstream phase where these parameters are incorporated into the model and frozen during the fine-tuning process. By evaluating on four datasets across two bias axes namely gender and race, we find that downstream biases can be effectively reduced with PEFTs. In addition, we show that these parameters possess axis-specific debiasing characteristics, enabling their effective transferability in mitigating biases in various downstream tasks. To ensure reproducibility, we release the code to do our experiments1."
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, it has become evident that foundation models such as BERT or GPT-3 (Devlin et al., 2019; Brown et al., 2020) are susceptible to a range of stereotypical societal biases (Jentzsch and Turan, 2022) such as sexism (gender) (Kurita et al., 2019) and racism (race) (Ahn and Oh, 2021), that are present in the training data. Such bias axes can lead to unfair or discriminatory outcomes (Webster et al., 2021; Barikeri et al., 2021) in various socio-technical scenarios.\nRecent research (Ladhak et al., 2023) suggests that biases acquired during pre-training can propagate to downstream models, resulting in superficial text dependencies and potential implicit bias, and a\n\u2217Equal contribution, names ordered randomly. 1https://github.com/sumit-agrwl/peft-debias\nhigher likelihood of subsequent harmful effects, a concept known as bias transfer hypothesis (Bolukbasi et al., 2016; Caliskan et al., 2017). However, most approaches for bias mitigation are primarily applied during fine-tuning to reduce bias in specific downstream tasks or datasets (Park et al., 2018; Zhang et al., 2018). It involves incorporating auxiliary training objectives (Jin et al., 2021), annotation of bias attributes (Liang et al., 2020) and task-specific fairness metrics (Zhang et al., 2020), which poses a challenge for the expanding community of fine-tuning language models.\nPrevious studies have attempted to address this\nissue by first debiasing the model and then finetuning it for a specific task. This process referred to as upstream debiasing by Jin et al. (2021), and entails fine-tuning the model on upstream tasks while incorporating bias-attribute annotations for debiasing. Subsequently, the model is fine-tuned for the target downstream task. Nevertheless, this approach possesses certain limitations: (i) it requires annotated bias attributes for the upstream task as well as supervised data for both tasks and (ii) there is no guarantee that the model will exhibit reduced bias in the downstream task (Steed et al., 2022). This uncertainty arises due to the fact that modifying all parameters of the debiased upstream model might result in the loss of debiased representations. This phenomenon is commonly referred to as fairness forgetting (Lauscher et al., 2021).\nInspired by the promising outcomes of PEFT methods, which effectively capture debias information and yield competitive results compared to full model-tuning (Kumar et al., 2023; Lauscher et al., 2021), we hypothesize that employing PEFTs for debiasing on an upstream bias axis could be a viable approach to mitigate bias in a foundation model for any downstream task on the same bias axis. To address this, we present a novel method called PEFTDebias. This approach utilizes PEFTs to capture debiasing information by training the model on axis-specific data during the upstream stage. Subsequently, in the downstream task, the model is fine-tuned while keeping the PEFTs frozen, thereby preserving the upstream debiasing information along that axis. Our contribution can be summarized as:\n\u2022 We explore the efficacy of training PEFT parameters along a specific bias axis by utilizing axis-based data to transfer bias information to downstream tasks aligned with that axis.\n\u2022 We evaluate the effectiveness of various PEFT methods in mitigating social biases to determine whether certain PEFT techniques are more efficient than others.\n\u2022 We examine the transfer capabilities of PEFTs across different datasets to mitigate social biases along specific axes."
        },
        {
            "heading": "2 Related Work",
            "text": "Several debiasing methods have been proposed in conjunction with the downstream task, including counterfactual data augmentation (Zmigrod et al.,\n2019), dropout regularization (Webster et al., 2020), null-space projection (Ravfogel et al., 2020), adversarial training (Liu et al., 2020), contrastive learning (He et al., 2022). However, these techniques necessitate expensive additional annotation, such as the inclusion of protected attributes, along with the task data. Conversely, (Jin et al., 2021) demonstrate debiasing using only task data, showing its potential for improving generalization. In contrast, (Steed et al., 2022) indicate that debiasing a language model (LM) prior to fine-tuning does not guarantee unbiasedness in the resulting fine-tuned model. Jin et al. (2021) investigate the transferability of debiasing techniques. They begin by applying bias mitigation to a pre-trained model through fine-tuning and subsequently employ it for downstream fine-tuning.\nLauscher et al. (2021); Kumar et al. (2023) show that PEFT methods like Adapters (Houlsby et al., 2019), can be used to debias language models (LMs) while keeping the LM backbone frozen. Hauzenberger et al. (2023) present a method to do debiasining by identifying sparse subnetworks that correspond to different bias axes, which can subsequently be composed. A notable advantage of these approaches is the reduced computational cost and environmental impact associated with debiasing LMs (Hessenthaler et al., 2022). Additionally, it holds the potential for preventing catastrophic forgetting of pre-trained knowledge caused by finetuning (Kirkpatrick et al., 2017). However, these techniques are typically applied during the downstream phase and possess the limitations discussed earlier."
        },
        {
            "heading": "3 Bias Factors and Datasets",
            "text": "We validate our hypothesis by conducting validation on two widely recognized factors of social bias: gender stereotyping and racial identifiers. To address occupation-based gender stereotypes, we utilize the BiasBios dataset (De-Arteaga et al., 2019). For the bias related to race, we address the issue of elevated occurrences of false positive outcomes in hate speech predictions using GHC (Kennedy et al., 2018). To show our generalizibility of capturing debiasing information along a specific axis using PEFTs, we show transfer to datasets MNLI (multi genre NLI) (Williams et al., 2018) and LHC (large hate corpus) (Toraman et al., 2022) along gender and race axis respectively.\nIn order to assess the effectiveness of our debi-\nasing techniques in mitigating gender and racial biases, we utilize two intrinsic bias benchmarks, namely CrowS-Pairs (Nangia et al., 2020) and StereoSet (Nadeem et al., 2021), during the initial phase of our evaluation, referred to as the upstream stage. StereoSet evaluates a language model\u2019s stereotypical associations by employing fill-in-the-blank problems with intra-sentence examples across different bias categories. CrowSPairs is an intra-sentence dataset of minimal pairs that compares the language model\u2019s masked token probabilities of sentences with disadvantaged or advantaged races fulfilling or violating stereotypes.\nIn the subsequent downstream stage, we evaluate the performance gap of PEFTs across different protected attributes within the specific domain using extrinsic bias metrics. To measure gender bias, we adopt the method proposed by De-Arteaga et al. (2019) to calculate the gender gap in the True Positive Rate (TPR) for each occupation (TPR-GAP). To assess racial bias, we compute the False Positive Rate Difference (FPRD) by comparing the FPR of examples mentioning protected racial attributes to the overall FPR. We calculate FPRD for both the in-domain data and the Identity Phrase Templates Test Sets (IPTTS) (Zhang et al., 2020), which consist of 77k instances. These instances comprise hate and non-hate sentences that mention 25 racial identifiers and are generated using predefined templates. To measure transferability, we evaluate MNLI using FN (fraction of neutrals) in Bias-NLI (Dev et al., 2019), a NLI dataset to measure gender bias, and LHC using IPTTS."
        },
        {
            "heading": "4 Methodology",
            "text": "Kumar et al. (2023) demonstrates that incorporating adapters in debiasing during the finetuning process helps. However, transferring adapters between different datasets/tasks is not feasible due to the need to learn data-specific modules. While Lauscher et al. (2021) indicate that learning adapters in the upstream phase contributes to better results during downstream fine-tuning. We propose a novel approach called PEFTDebias which combines elements from both aforementioned methods. It consists of two main phases: the upstream phase, responsible for selecting debiasing parameters through PEFTs, and the downstream phase, which employs the debiased PEFTs for task debiasing during fine-tuning, as illustrated in Figure 1 and outlined in pseudo-code A.3. We investigate\nthe viability of multiple PEFTs, including Adapters (Pfeiffer et al., 2021), Prompt Tuning (Lester et al., 2021), LoRA (Hu et al., 2021), and Sparse Finetuning (Ansell et al., 2022) (refer A.2)."
        },
        {
            "heading": "4.1 Upstream Phase",
            "text": "Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) is a data-based debiasing technique that swaps attribute words pertaining to a bias (e.g, he/she for binary gender). Parameter efficient debiasing with Adapters (Lauscher et al., 2021) has demonstrated the effectiveness of using CDA to capture debiasing information while minimizing the number of parameters. Consequently, our study aims to explore the application of CDA using PEFT methods for obtaining debiasing parameters. Specifically, we utilize a PEFT to perform CDA on axis-specific data. We extract attribute words from a particular axis and apply them through CDA to obtain debiasing PEFT parameters. Our hypothesis posits that these parameters will proficiently capture task-agnostic debiasing information that is specific to the designated axis."
        },
        {
            "heading": "4.2 Downstream Phase",
            "text": "To enable the transferability of debiasing PEFT parameters across datasets, we propose learning debiasing parameters during the upstream phase and injecting them into a trainable language model while keeping PEFT parameters frozen during downstream task fine-tuning. Our hypothesis is that this set of frozen parameters will retain the upstream debiasing effect and safeguard the model against acquiring biases during task finetuning. Consequently, it effectively mitigates biases along the specific axis in the finetuned model."
        },
        {
            "heading": "5 Results",
            "text": "Our experimental setup is described in A.4. We present three sets of results: evaluation of the upstream and downstream phases on the same datasets, and the transferability to other datasets."
        },
        {
            "heading": "5.1 Upstream Phase",
            "text": "In Table 1, we present the results of our experiments in the upstream setting. The results clearly indicate that the utilization of PEFTs with CDA not only enhances the performance of LM, but also diminishes intrinsic bias. Remarkably, both the Prompt Tuning and Adapter techniques demonstrate substantial debiasing effectiveness while either preserving or even enhancing the LM score\nwhen compared to other techniques. For BiasBios, Prompt Tuning shows the highest performance in bias intrinsic scores of CrowS and StereoSet."
        },
        {
            "heading": "5.2 Downstream Phase",
            "text": "The results of the downstream experiments are presented in Table 2 where the dataset used in the upstream phase is same as the one in the downstream phase, demonstrating that the PEFTs attain comparable task performance to the BERT baseline (within a 5% margin) with a significant improvement in extrinsic bias metric. This observation suggests that it is possible to achieve efficient debiasing without significant performance loss. Among the PEFTs, Prompt Tuning stands out for its superior ability to reduce bias. This finding implies that Prompt Tuning effectively debiases the model in the upstream phase while maintaining its task performance, possibly due to minimal modifications inside the language model (Ding et al., 2022) during forward pass as compared to other PEFTs. Additionally, both BiasBios and GHC exhibit a positive correlation between upstream debiasing performance and downstream bias reduction. This correlation indicates that upstream debiasing can effectively transfer to downstream tasks using PEFTs, facilitating bias mitigation across similar axes. We also study in detail the reduction in bias in BiasBios dataset in A.5"
        },
        {
            "heading": "5.3 PEFT Transfer",
            "text": "To evaluate the task-agnostic nature of the learned upstream debiasing parameters along a specific axis, we conduct experiments where we apply these\nparameters during the finetuning process for a corresponding task in the same axis on MNLI and LHC. By comparing these results with the ones reported in Table 2, we observe that the performance of the transferred debiasing parameters is comparable to that of full finetuning (FT). While parameters learned from the same task data exhibit the least bias, as indicated by the FPRD and FPRDIPTTS metrics, Table 2 demonstrates that comparable performance can still be achieved through transfer. Notably, the SFT and Prompt Tuning outperform full finetuning on in-domain FPRD metrics when it comes to transfer which also aligns with our findings from previous experiments. In case of MNLI, the performance remains similar to that of full finetuning while Prompt Tuning showing impressive performance for bias scores calculated using BiasNLI. This indicates that task-agnostic axis-based patch generated by PEFTs work effectively to debias along the same axis across different datasets."
        },
        {
            "heading": "6 Conclusion & Future Work",
            "text": "This research paper introduces PEFTDebias, a novel debiasing approach that utilizes PEFTs to mitigate the biases. PEFTDebias involves two phases: an upstream phase for learning debiasing PEFTs along specific bias axes, and a downstream phase where these PEFTs are incorporated into the model and kept frozen while fine-tuning. Experimental results highlight the effectiveness of Prompt\nTuning for downstream debiasing and the transferability of axis-specific debiasing parameters in mitigating biases across different tasks. Future work includes extending our technique for generative models and tasks, as well as exploring the composition of multiple bias axes (Jin et al., 2021) to address various biases in datasets."
        },
        {
            "heading": "7 Limitation",
            "text": "Our research specifically targeted the debiasing of BERT, a widely used language model, and did not encompass other foundational language models such as GPT-3 limiting its scope to the specific context of BERT and its associated biases. We demonstrated the effectiveness of our debiasing techniques on downstream classification tasks. However, it is important to note that these findings may not directly translate to generative language models, as they approach every task as a generation problem. To extend the applicability of our approaches to the broader landscape of all foundational language models, further analysis and investigation would be necessary. We focus our study on mitigating the biases within the dataset, and do not focus on the biases in the annotation of the task labels."
        },
        {
            "heading": "8 Ethical Considerations",
            "text": "In this research, we employed a binary gender definition while examining gender bias in pre-trained language models. However, we acknowledge that gender is non-binary and recognize the importance of using a more flexible definition in future studies on gender bias drawing inspiration from previous research (Dinan et al., 2020). Likewise, our investigation of racial bias is limited to a specific set of biased attribute words, representing a narrow definition. It is important to note that we did not explore the potential reduction in harm through the implementation of our debiasing techniques in real-world scenarios. Furthermore, we want to emphasize that all the intrinsic bias benchmarks used in this study possess only positive predictive power. This means that they can identify biased models but cannot confirm a model as unbiased. For instance, a stereotype score of 50% on StereoSet or CrowS-Pairs does not necessarily indicate an unbiased model. The extrinsic measures also rely on few words or templates and cannot comprehensively capture all the stereotypical variations used by humans, Due to these considerations, we urge readers to refrain\nfrom making definitive claims about the debiasing techniques outlined in this paper or applying them directly in real-world settings."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "We thank Professors Emma Strubell and Maarten Sap for their valuable guidance and feedback on this work."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Bias Axes & Attribute Words\nWe describe the bias axes and attribute words that we will use in our studies. We mention two different biases, gender and race. Hereby, we present a list of some attribute word examples as well along with the biases. Gender (actor, actress), (boy, girl), (brother, sister), (he, she) Race (black, caucasian, asian), (african, caucasian, asian), (black, white, asian)\nA.2 Paramter Efficient Fine-Tuning (PEFT)\nWe explore the use of multiple PEFTs, Adapters: (Pfeiffer et al., 2021) which are task-specific modules inserted between transformer layers, Prompt Tuning : (Lester et al., 2021) which involves incorporating task-specific vectors (prompts) into the input sequence, LoRA : (Hu et al., 2021) which integrates trainable low-rank matrices into transformer layers in order to approximate weight updates, and Sparse Fine Tuning : (Ansell et al., 2022) builds upon the Lottery Ticket Hypothesis (LTH) to select a sparse sub-network based on the parameters that undergo the most significant changes.\nA.3 Algorithm\nAlgorithm 1 PEFTDebias training algorithm Require: Du = {xi}Ni=1 // unlabelled Require: Dl = {(xi, yi) \u223c P (X,Y )}Nj=1 // labelled\nInitialize \u03b8FM Initialize \u03d5PEFT\n/* Upstream stage */ \u03d5APEFT \u2217 \u2190 Debias(\u03b8FM , \u03d5PEFT , Du, A)\n/* Downstream stage */ \u03b8\u2217FM \u2190 FT (\u03b8FM , \u03d5APEFT \u2217 , Dl)\nreturn \u03b8\u2217FM \u222a \u03d5APEFT \u2217\nOur algorithm for debiasing is described in 1. Our method requires an unlabeled in-domain corpus Du for upstream debasing and a labeled corpus Dl for task-specific fine-tuning in the downstream phase. We use a pretrained foundation model \u03b8FM , and a set of PEFT parameters \u03d5PEFT which will be used for debiasing the model. In the upstream stage, the backbone model is kept frozen and domain and axis-specific PEFT parameters \u03d5APEFT \u2217 for the axis A are obtained. These are then used to finetune the foundation model on the downstream\ntask while keeping the PEFT frozen to obtain \u03b8\u2217FM . The final debiased task-specific model is the union of the axis-specific PEFT and the foundation model (\u03b8\u2217FM \u222a \u03d5\u2217PEFT )\nA.4 Experimental Setup We used pre-trained BERT (Devlin et al., 2018) as the starting point for all of our models. We also applied text normalization to GHC datasets to remove URLs and user mentions using tweet based processing 2. For the upstream experiments, we trained our models with MLM and CDA on the BiasBios dataset and the other datasets using a learning rate of 1e\u22125 and a batch size of 128 and 32 respectively. We ran MLM for 10,000 steps and evaluated the models every 1,000 steps. We selected the models with the lowest loss for our experiments. For the downstream experiments, we used a batch size of 32 and trained our models for 10 epochs. We ensured that all PEFTs have similar number of parameters, being 1% of the base LM, to keep them comparable. For the downstream experiments, we used a batch size of 32 and trained our models for 10 epochs. We chose the models with the best task metrics for analysis. For GHC and Stormfront datasets, which had few hateful examples compared to non-hateful ones, we weighted the loss of hateful examples by a factor of 10 for GHC and 6.7 for Stormfront, based on their proportions in the data. We compared our methods with two baselines: BERT in the pre-trained setting and BERT in the fine-tuned setting (Full-Debias). Our implementation is based on the AdapterHub 3.\nA.5 Reduction in bias We conducted a comparison of the TPR-GAP performance of CDA debiasing techniques using FT and Prompt Tuning on the BiasBios dataset (see Figure 2, specifically focusing on occupations categorized as male and female. Our findings indicate that debiasing with Prompt Tuning yields better results compared to FT, as evidenced by a decrease in the TPR for gender-dominant professions. We observed that certain female-dominated professions such as dietitian and interior designer exhibit reduced correlation with the female gender, while male-dominated professions like surgeon and comedian also demonstrate a decrease in correlation with the male gender. Although we did not observe significant changes in the gap for professions\n2link to script 3https://adapterhub.ml/\nlike rapper and psychologist, we encountered an issue of over-correction, resulting in a reversed gap for poet and accountant. This discrepancy can be attributed to the limited number of examples available for these particular professions."
        }
    ],
    "title": "PEFTDebias : Capturing debiasing information using PEFTs",
    "year": 2023
}