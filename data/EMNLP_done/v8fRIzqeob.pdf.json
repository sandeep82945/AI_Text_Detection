{
    "abstractText": "Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs\u2019 knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models\u2019 knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zheyuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Jifan Yu"
        },
        {
            "affiliations": [],
            "name": "Juanzi Li"
        },
        {
            "affiliations": [],
            "name": "Lei Hou"
        }
    ],
    "id": "SP:4dc6017b3167b15275082d47e9915d417d477a59",
    "references": [
        {
            "authors": [
                "Lorin W Anderson",
                "David R Krathwohl."
            ],
            "title": "A taxonomy for learning, teaching, and assessing: A revision of Bloom\u2019s taxonomy of educational objectives",
            "venue": "Longman,.",
            "year": 2001
        },
        {
            "authors": [
                "Isaac I. Bejar."
            ],
            "title": "Educational diagnostic assessment",
            "venue": "Journal of Educational Measurement, 21(2):175\u2013 189.",
            "year": 1984
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "ArXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Fran\u00e7ois Chollet."
            ],
            "title": "On the measure of intelligence",
            "venue": "ArXiv preprint, abs/1911.01547.",
            "year": 2019
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "James Deese",
                "Roger A Kaufman."
            ],
            "title": "Serial effects in recall of unorganized and sequentially organized verbal material",
            "venue": "Journal of experimental psychology, 54(3):180.",
            "year": 1957
        },
        {
            "authors": [
                "Jean-Claude Falmagne",
                "Eric Cosyn",
                "Jean-Paul Doignon",
                "Nicolas Thi\u00e9ry."
            ],
            "title": "The assessment of knowledge, in theory and in practice",
            "venue": "Formal Concept Analysis: 4th International Conference, ICFCA 2006, Dresden, Germany, February 13-17, 2006. Proceed-",
            "year": 2006
        },
        {
            "authors": [
                "Ahmad Ghazal",
                "Todor Ivanov",
                "Pekka Kostamaa",
                "Alain Crolotte",
                "Ryan Voong",
                "Mohammed Al-Kateb",
                "Waleed Ghazal",
                "Roberto V Zicari."
            ],
            "title": "Bigbench v2: the new and improved bigbench",
            "venue": "Proc. of ICDE, pages 1225\u20131236. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "Proc. of ICLR. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Yuzhen Huang",
                "Yuzhuo Bai",
                "Zhihao Zhu",
                "Junlei Zhang",
                "Jinghan Zhang",
                "Tangjun Su",
                "Junteng Liu",
                "Chuancheng Lv",
                "Yikai Zhang",
                "Jiayi Lei",
                "Yao Fu",
                "Maosong Sun",
                "Junxian He"
            ],
            "title": "C-eval: A multi-level multi-discipline chinese evaluation suite",
            "year": 2023
        },
        {
            "authors": [
                "Jacqueline Leighton",
                "Mark Gierl."
            ],
            "title": "Cognitive diagnostic assessment for education: Theory and applications",
            "venue": "Cambridge University Press.",
            "year": 2007
        },
        {
            "authors": [
                "Junling Liu",
                "Peilin Zhou",
                "Yining Hua",
                "Dading Chong",
                "Zhongyu Tian",
                "Andrew Liu",
                "Helin Wang",
                "Chenyu You",
                "Zhenhua Guo",
                "Lei Zhu",
                "Michael Lingzhi Li"
            ],
            "title": "Benchmarking large language models on cmexam \u2013 a comprehensive chinese medical exam",
            "year": 2023
        },
        {
            "authors": [
                "Nils J Nilsson"
            ],
            "title": "Human-level artificial intelligence? be serious! AI magazine, 26(4):68\u201368",
            "year": 2005
        },
        {
            "authors": [
                "Manmeet Singh",
                "Vaisakh SB",
                "Neetiraj Malviya"
            ],
            "title": "Mind meets machine: Unravelling gpt-4\u2019s cognitive psychology",
            "venue": "ArXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Quan-Hoang Vuong",
                "Viet-Phuong La",
                "Manh-Toan Ho",
                "Thanh-Hang Pham",
                "Thu-Trang Vuong",
                "Ha-My Vuong",
                "Minh-Hoang Nguyen"
            ],
            "title": "A data collection on secondary school students\u2019 stem performance and reading practices in an emerging country",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "Proc. of ICLR. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Jifan Yu",
                "Mengying Lu",
                "Qingyang Zhong",
                "Zijun Yao",
                "Shangqing Tu",
                "Zhengshan Liao",
                "Xiaoya Li",
                "Manli Li",
                "Lei Hou",
                "Hai-Tao Zheng"
            ],
            "title": "Moocradar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in moocs",
            "year": 2023
        },
        {
            "authors": [
                "Jifan Yu",
                "Yuquan Wang",
                "Qingyang Zhong",
                "Gan Luo",
                "Yiming Mao",
                "Kai Sun",
                "Wenzheng Feng",
                "Wei Xu",
                "Shulin Cao",
                "Kaisheng Zeng",
                "Zijun Yao",
                "Lei Hou",
                "Yankai Lin",
                "Peng Li",
                "Jie Zhou",
                "Bin Xu",
                "Juanzi Li",
                "Jie Tang",
                "Maosong Sun"
            ],
            "title": "Mooccubex: A large",
            "year": 2021
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Sharifah Mahani Aljunied",
                "Chang Gao",
                "Yew Ken Chia",
                "Lidong Bing"
            ],
            "title": "M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
            "year": 2023
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Ruixiang Cui",
                "Yiduo Guo",
                "Yaobo Liang",
                "Shuai Lu",
                "Yanlin Wang",
                "Amin Saied",
                "Weizhu Chen",
                "Nan Duan"
            ],
            "title": "Agieval: A human-centric benchmark for evaluating foundation models",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs), such as GPT series (Brown et al., 2020), Flan (Wei et al., 2022), and PaLM (Chowdhery et al., 2022), have gained significant attention worldwide due to their remarkable ability. Given their unprecedented human-like performances, researchers have started to explore alternative evaluation metrics beyond traditional benchmarks like MMLU (Hendrycks et al., 2021) and Big-Bench (Ghazal et al., 2017).\nExisting Works on LLMs Evaluation with Exams. Researchers have long sought models capable of passing human exams (Nilsson, 2005). Recently, a new approach simulates professional exams designed for humans to evaluate LLMs. For example, OpenAI (2023) reports the performance of GPT series on a variety of exams, including AP exams, SAT, Leetcode, and so on. There are also emerging\n\u2217 Equal contribution \u2020 Corresponding Author\nbenchmarks that comprise common standardized exams, such as AGIEval (Zhong et al., 2023), CEval (Huang et al., 2023), M3Exam (Zhang et al., 2023), and CMExam (Liu et al., 2023). However, although standardized exams contain diverse information, these works condense them into a single overall score, lacking structured understanding of LLMs\u2019 knowledge and cognitive patterns.\nFor example, while LLMs demonstrate exceptional performance on tasks challenging for humans, they might still struggle with basic knowledge, as illustrated in Figure 1, which may lead to over-estimation of the validity of model generated contents. Therefore, there is a pressing need for further research of models\u2019 knowledge and cognitive distribution in comparison to humans.\nProposed Research. To investigate this problem, we draw inspiration from psychometric methods that use cognitive psychology theories to evaluate LLMs. This topic has gained traction as LLMs continue to demonstrate exceptional performances (Chollet, 2019; Singh et al., 2023; Bubeck et al., 2023). In this work, we adopt the Educational Diagnostic Assessment approach and leverage MoocRadar (Yu et al., 2023), a novel student exercise dataset annotated with Bloom\u2019s Taxonomy (Anderson and Krathwohl, 2001), to assess the cognitive capability of LLMs. Specifically, we delve into three primary research questions: 1) Performance Analysis: the proficiency and robustness of LLMs across various question domains; 2) Deficit Assessment: the knowledge structure and the extent to which LLMs are similar with humans; and 3) Error Assessment: the error pattern of LLMs in answers and explanations. Our findings contribute to a deeper understanding of the knowledge structure of LLMs and insights for evaluation.\nContributions. Our main contributions are: (i) We introduce the topic of the cognitive knowl-\nedge structure of LLMs. (ii) We propose a method of Educational Diag-\nnostic Assessment to evaluate LLMs on their cognitive knowledge structure.\n(iii) We assess LLMs\u2019 performance, deficits, and errors, gaining insights into their capabilities."
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 Educational Diagnostic Assessment",
            "text": "In education scenarios, Diagnostic Assessment is a widely employed method to gauge students\u2019 knowledge structure (Falmagne et al., 2006), discovering their proficiency on certain subject matters and learning styles (Vuong et al., 2021), typically through sets of questions (Leighton and Gierl, 2007). Two main approaches of Diagnostic Assessment include deficit assessment, which focuses on identifying and addressing knowledge gaps in various domains and the degree of knowledge mastery, and error assessment, which focuses on error patterns and strategies for correction (Bejar, 1984). Drawing inspiration from Diagnostic Assessment methods, in this work, based on Bloom\u2019s Taxonomy, we use deficit assessment to test the accuracy of models on a wide range of exercises, and error assessment on their answers and explanations."
        },
        {
            "heading": "2.2 Experimental Setup",
            "text": "Dataset. In this section, we introduce the dataset utilized for assessment, MoocRadar, offering an extensive overview of its general information. MoocRadar is a fine-grained and multi-aspect exercise repository designed for cognitive modeling and educational diagnostic assessment. According to Bloom\u2019s Taxonomy, questions in MoocRadar are categorized into four Knowledge-Types: Factualknowledge, Conceptual-knowledge, Proceduralknowledge, and Meta-knowledge; and six cognitive dimensions: Remember, Understand, Apply, Ana-\nlyze, Evaluate, and Create. Table 1 demonstrates a detailed description of Bloom\u2019s Taxonomy.\nWe carefully select 8453 questions appropriate for model evaluation, which fall into three types: single choice (SC), multiple choice (MC), and true or false (TF). Additionally, we exclude the dimension of Create because of the scarcity of related exercises. We further classify these questions into four disciplines by their course information, including STEM, social science, humanity, and others. We test the performance of models on them and analyze the distribution of these features. More details of MoocRadar are illustrated in the appendix.\nModel Selection. We carefully choose 3 advanced models that have consistently demonstrated leading performance and are widely recognized in the field, including: Text-Davinci-003, ChatGPT, and GPT-4, which represent a series of most acknowledged models. All experiments are performed using the APIs provided by OpenAI. Specif-\nically, we use the completion API for Text-Davinci003 and the chat completion API for ChatGPT and GPT-4. To ensure consistency in the quality of the responses, we set the temperature to 0 to get greedy search responses generated by each model.\nExperimental Design. As shown in Figure 1, we design different prompts tailored to each type of exercises to query LLMs for both answers and explanation. All tasks are conducted in zero-shot scenario. To simulate human-like behavior that solving exercises with relevant knowledge, we leverage the BM25 algorithm to retrieve the two most related discussions from the subtitles in the corresponding courses in MOOCCubeX (Yu et al., 2021) and test their effect. Moreover, we extract real student behaviors on MoocRadar dataset from MOOCCubeX and calculate their average scores to serve as a reference of humans. Based on both results from human and LLMs, this work provides a road map with investigation to the following research questions:\n(RQ1) Performance Analysis: What\u2019s the features of LLMs\u2019 basic performance on different disciplines and their robustness to these questions?\n(RQ2) Deficit Assessment: According to Bloom Taxonomy, compared with humans, what knowledge distribution does LLMs demonstrate? Are they similar to humans in knowledge structure?\n(RQ3) Error Assessment: Based on answers and explanations, what\u2019s their pattern of errors?"
        },
        {
            "heading": "3 Experiment",
            "text": "In this section, we conduct experiments and analyze the results from three perspectives in the following subsections. We assign a score of 1 to each question type. Following standardized exams, for multiple questions, models receive a score of 0.5 if they fail to select all correct options. We then calculate the average score across questions."
        },
        {
            "heading": "3.1 Performance Analysis",
            "text": "Firstly, we assess their performance both with and without contexts, compare their performance in different disciplines, and examine their robustness.\nDisciplines and Context. We exhibit scores of model answers with or without context on the four disciplines (STEM, social science, humanity, and others). As shown in Table 2, the later versions of GPT significantly outperform previous models, with GPT-4 being the most advanced, but not better than humans\u2019 average. Additional knowledge from context indeed enhances the performance of\nthe models. Comparatively, STEM exercises are more challenging as illustrated in human results, while LLMs demonstrate impressive capability in STEM knowledge. GPT-4 even outperforms humans with context. However, it is surprising that LLMs don\u2019t perform as effectively in social science and humanities exercises, even though these disciplines primarily involve natural language.\nRobustness. In single choice questions, we manipulate the order of the options by either placing the correct answer at the beginning or the end. This allows us to examine if such modifications affect the model\u2019s accuracy. As shown in table 3, we find that 1) ChatGPT is more robust to changing of options, while the other two exhibits a cognitive bias as Primacy Effect (Deese and Kaufman, 1957) that early appearance aids performance; 2) if the correct answer appears later in GPT-3.5 and GPT-4, they mistakenly change their answers and explanations; 3) later appearance causes less consistency in answers and explanations in less robust models."
        },
        {
            "heading": "3.2 Deficit Assessment",
            "text": "We utilize Bloom\u2019s Taxonomy in MoocRadar to demonstrate models\u2019 distribution in cognitive dimensions and knowledge types and design a score to measure similarities of models to humans.\nBloom Taxonomy Distribution. As shown in Figure 2, we demonstrate the distribution based on Bloom\u2019s taxonomy, where deeper colors represent better performance. The 0 and 1 grids are due to the limited number of exercises, typically only one. Generally, both in knowledge types and cognitive dimensions, questions in the intermediate range are more challenging for models and humans. We design a similarity score for deeper understanding.\nSimilarity Score. According to the accuracy of models in various dimensions of knowledge and cognition, we develop a metric to measure their similarity to humans, which primarily considers knowledge structure, beyond mere performance, and estimates the extent to which their cognitive structure is proportional to that of humans. Specifically, given a model M , the 4*5 vector of the model distribution in bloom\u2019s taxonomy x and human distribution y, convert x and y into 1*20 vectors x\u0303 and y\u0303, the similarity between M and human can be defined as: Likeness(M) = \u03c1(x\u0303, y\u0303), where \u03c1(x\u0303, y\u0303) represents the Pearson Correlation Coefficient of x\u0303 and y\u0303. We calculate the Likeness of the three models in Table 4. The likeness also exhibits a rising tendency as the models evolve. Models that follow human instructions better are also more similar to humans in knowledge structure."
        },
        {
            "heading": "3.3 Error Assessment",
            "text": "In this section, we analyze the error of each models, by delving into their explanation of their answers.\nExplanation Accuracy. Table 5 demonstrate the accuracy of answers in each type. We mainly find that: 1) Models perform best on TF and worst on MC. MC could be more difficult than SC and TF, because of more thinking steps (determine TF of each options, and select multiple ones). 2) Explanations and answers are more consistent in TF than in SC and MC for the same reason, as there are more chances to make errors. 3) Accuracy of explanations falls behind answers in MC, where models can select some of the correct options for the wrong reason. 4) Context does not necessary aid and even hurt explanation performances, but indeed aids answer accuracy. More advanced models are more consistent in their answers and explanations."
        },
        {
            "heading": "3.4 Discussion",
            "text": "In this section, we discuss our findings on the proposed three research questions:\nPerformance Analysis. We exhibit different models\u2019 performance. Comparing with humans, they are less proficient in disciplines primarily involve natural language, but better at STEM. Though with sufficient knowledge, they might have hallucination on specific long-tail concepts in humanity and social science. LLMs are not robust in option orders, and exhibit a cognitive bias as Primacy Effect rather than Recency Effect.\nDeficit Assessment. Models are less proficiency in the intermediate range of Bloom\u2019s Taxonomy. The reason could be that application-based ques-\ntions, such as solving mathematical problems and making deductions using chemical theorems, are prone to errors and are inherently challenging for models. For analyzing and evaluating questions, the strong linguistic capabilities of models allow them to excel in these tasks, and perform even better than intermediate-level questions. More advanced models demonstrate more similarity with humans in knowledge structure, which might be an additional effect of human alignment.\nError Assessment. By comparing different kinds of questions, we find that gap exists for models between knowledge and answers. They perform worse in multiple choices, as there are more thinking steps and error chances. Accuracy of explanations can be worse than answers: as models were asked to generate answers first, their explanation could shift due to wrong answers and question orders, and cause their hallucinations. Due to the limitations of autoregressive architecture (Bubeck et al., 2023), their errors could snowball."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this work, we introduce a new research question on LLMs analyzing, which calls for a deeper understanding of the knowledge structure of these models. We use Educational Diagnostic Assessment as a tool to test the performance of LLMs on various dimensions, and develop a metric to measure the similarity of their knowledge structure with humans. We provide findings and discussion for insight into research on the cognition of LLMs.\nLimitations\nIn this section, we describe the limitations of this work in terms of the dataset and experiments.\nDataset. We investigated the knowledge distribution of LLMs based on the MoocRadar dataset. MoocRadar is a fine-grained, well-structured dataset that is distinct from commonly used benchmarks in terms of knowledge annotation. However, as a dataset for educational diagnostic assessment, it\u2019s still limited in the following aspects: 1) Different categories of exercises (e.g. question type, disciplines) have an unbalanced distribution; 2) As demonstrated in the Robustness section, the performance of the models can vary due to different forms of exercises.\nExperiment. Due to time and cost constrains, 1) we only included three LLMs by OpenAI, which are all closed-source models. Therefore, we did\nnot conduct experiments at the parameter level. 2) though we have discovered some phenomena, further experiments and deeper analysis are not conducted. We include some of them in the case study section in the appendix.\nFuture Works. Future works include 1) more models for experiments, 2) further exploration on robustness and similarity with humans, and 3) as the next step of diagnostic assessment, investigate how to optimize the knowledge structure of LLMs.\nEthics Statement\nWe foresee no ethic concerns in this work. The MoocRadar dataset employed in our research is publicly available, and it does not contain any personal information."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by a grant from the Institute for Guo Qiang, Tsinghua University (2019GQB0003), and also supported by Tsinghua University Initiative Scientific Research Program."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Details of Experiment\nThis subsection shows details of the dataset we use, and experiment for diagnostic assessment.\nProblem Statistics. Table 6 demonstrates the details of the dataset we use, which is selected from the original MoocRadar. Generally, we include three question types (single choice, multiple choice, and true or false), four knowledge types (factual knowledge, conceptual knowledge, procedural knowledge, and meta knowledge), and five cognitive dimensions (remember, understand, apply, analyze, and evaluate), to form a total dataset of 8430 questions.\nProblem Examples. Table 7 demonstrates examples for each type of questions. There are two or more than two options in single choices and only one correct options, while multiple choices have more than one correct options. True or false questions should be answered as True or False.\nQuerying Details. For context settings, we use the BM25 algorithm to retrieve the two most related contexts from the subtitles of the corresponding class. As illustrated in Figure 3, for the question about the pioneer of mathematical logic, the BM25 algorithm retrieves context about the emergence and development of logic and the concept of mathematical logic. The two contexts will be placed before instruction, along with the questions and options to form the prompt, and fed into LLMs. In non-context settings, the context position will simply be empty. We also test different instructions to make sure that models will follow them to provide answers and explanations.\nAnnotation details. To accurately assess the models\u2019 performance on both answers and explanations, we first invited educational experts to filter questions, because a part of questions in the dataset have accompanying pictures, which can\u2019t be input into models. Then we asked human annotators to evaluate the answers and explanations generated by the three models respectively, to prevent misjudgment in automatic evaluation tools. These annotators are familiar with MOOC exercises, and provided with the original questions and correct answers, and have access to the internet to determine accurately.\nA.2 Cases In this subsection, we demonstrate some of the error cases we have seen during our experiments, and hope to provide insights into the model\u2019s error patterns."
        }
    ],
    "title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
    "year": 2023
}