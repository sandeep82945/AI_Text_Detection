{
    "abstractText": "The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of encoder-only language models and the decoderonly language models. Our findings reveal that while some decoder-only LLMs demonstrate notable performance across most financial tasks via zero-shot prompting, they generally lag behind the fine-tuned expert models, especially when dealing with proprietary datasets. We hope this study provides foundation evaluations for continuing efforts to build more advanced LLMs in the financial domain.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yue Guo"
        },
        {
            "affiliations": [],
            "name": "Zian Xu"
        },
        {
            "affiliations": [],
            "name": "Yi Yang"
        }
    ],
    "id": "SP:837487c4b35eabfdc2ee2b297a9736f73a43a482",
    "references": [
        {
            "authors": [
                "Julio Cesar Salinas Alvarado",
                "Karin Verspoor",
                "Timothy Baldwin."
            ],
            "title": "Domain adaption of named entity recognition to support credit risk assessment",
            "venue": "Proceedings of the Australasian Language Technology Association Workshop, ALTA 2015, Parramatta, Aus-",
            "year": 2015
        },
        {
            "authors": [
                "Dogu Araci."
            ],
            "title": "Finbert: Financial sentiment analysis with pre-trained language models",
            "venue": "CoRR, abs/1908.10063.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Allen H Huang",
                "Hui Wang",
                "Yi Yang."
            ],
            "title": "Finbert: A large language model for extracting information from financial text",
            "venue": "Contemporary Accounting Research, 40(2):806\u2013841.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Pekka Malo",
                "Ankur Sinha",
                "Pekka J. Korhonen",
                "Jyrki Wallenius",
                "Pyry Takala."
            ],
            "title": "Good debt or bad debt: Detecting semantic orientations in economic texts",
            "venue": "J. Assoc. Inf. Sci. Technol., 65(4):782\u2013796.",
            "year": 2014
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Agam Shah",
                "Suvan Paturi",
                "Sudheer Chava."
            ],
            "title": "Trillion dollar words: A new financial dataset, task & market analysis",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Raj Shah",
                "Kunal Chawla",
                "Dheeraj Eidnani",
                "Agam Shah",
                "Wendi Du",
                "Sudheer Chava",
                "Natraj Raman",
                "Charese Smiley",
                "Jiaao Chen",
                "Diyi Yang."
            ],
            "title": "When FLUE meets FLANG: Benchmarks and large pretrained language model for financial domain",
            "venue": "Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Ankur Sinha",
                "Tanmay Khandait."
            ],
            "title": "Impact of news on the commodity market: Dataset and results",
            "venue": "CoRR, abs/2009.04202.",
            "year": 2020
        },
        {
            "authors": [
                "Chi Sun",
                "Luyao Huang",
                "Xipeng Qiu."
            ],
            "title": "Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Yan",
                "Iliyan Zarov",
                "Yuchen Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aur\u00e9lien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom."
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "CoRR, abs/2307.09288.",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association",
            "year": 2023
        },
        {
            "authors": [
                "Shijie Wu",
                "Ozan Irsoy",
                "Steven Lu",
                "Vadim Dabravolski",
                "Mark Dredze",
                "Sebastian Gehrmann",
                "Prabhanjan Kambadur",
                "David S. Rosenberg",
                "Gideon Mann."
            ],
            "title": "Bloomberggpt: A large language model for finance",
            "venue": "CoRR, abs/2303.17564.",
            "year": 2023
        },
        {
            "authors": [
                "Qianqian Xie",
                "Weiguang Han",
                "Xiao Zhang",
                "Yanzhao Lai",
                "Min Peng",
                "Alejandro Lopez-Lira",
                "Jimin Huang."
            ],
            "title": "PIXIU: A large language model, instruction data and evaluation benchmark for finance",
            "venue": "CoRR, abs/2306.05443.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Yang",
                "Yixuan Tang",
                "Kar Yan Tam."
            ],
            "title": "Investlm: A large language model for investment using financial domain instruction tuning",
            "venue": "arXiv preprint arXiv:2309.13064.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Yang",
                "Mark Christopher Siy Uy",
                "Allen Huang."
            ],
            "title": "Finbert: A pretrained language model for financial communications",
            "venue": "CoRR, abs/2006.08097.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent progress in natural language processing (NLP) demonstrates that large language models (LLMs), like ChatGPT, achieve impressive results on various general domain NLP tasks. Those LLMs are generally trained by first conducting self-supervised training on the unlabeled text (Radford et al., 2019; Brown et al., 2020; Touvron et al., 2023a) and then conducting instruction tuning (Wang et al., 2023; Taori et al., 2023) or reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to let them perform tasks following human instructions.\nFinancial NLP, in contrast, demands specialized knowledge and specific reasoning skills to tackle tasks within the financial domain. However, for general language models like ChatGPT, their selfsupervised training is performed on the text from various domains, and the reinforcement learning feedback they receive is generated by non-expert\nworkers. Therefore, how much essential knowledge and skills are acquired during the learning process remains uncertain. As a result, a comprehensive investigation is necessary to assess its performance on financial NLP tasks.\nTo fill this research gap, we are motivated to evaluate language models on financial tasks comprehensively. For doing so, we propose a framework for Financial Language Model Evaluation (FinLMEval). We collected nine datasets on financial tasks, five from public datasets evaluated before. However, for those public datasets, it is possible that their test sets are leaked during the training process or provided by the model users as online feedback. To eliminate this issue, We used four proprietary datasets on different financial tasks: financial sentiment classification (FinSent), environmental, social, and corporate governance classification (ESG), forward-looking statements classification (FLS), and question-answering classification (QA) for evaluation.\nIn the evaluation benchmark, we evaluate the encoder-only language models with supervised fine-tuning, with representatives of BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), FinBERT (Yang et al., 2020) and FLANG (Shah et al., 2022). We then compare the encoder-only models with the decoder-only models, with representatives of ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), PIXIU (Xie et al., 2023), LLAMA2-7B (Touvron et al., 2023b) and Bloomberg-GPT (Wu et al., 2023) by zero-shot prompting. Besides, we evaluate the efficacy of in-context learning of ChatGPT with different in-context sample selection strategies.\nExperiment results show that (1) the fine-tuned task-specific encoder-only model generally performs better than decoder-only models on the financial tasks, even if decoder-only models have much larger model size and have gone through more pre-training and instruction tuning or RLHF; (2) when the supervised data is insufficient, the\nzero-shot decoder-only models have more advantages than fine-tuned encoder-only models; (3) the performance gap between fine-tuned encoder-only models and zero-shot decoder-only models is more significant on private datasets than the publicly available datasets; (4) in-context learning is only effective under certain circumstances.\nTo summarize, we propose an evaluation framework for financial language models. Compared to previous benchmarks in the financial domain like FLUE (Shah et al., 2022), our evaluation includes four new datasets and involves more advanced LLMs like ChatGPT. We show that even the most advanced LLMs still fall behind the fine-tuned expert models. We hope this study contributes to the continuing efforts to build more advanced LLMs in the financial domain."
        },
        {
            "heading": "2 Related Works",
            "text": "The utilization of language models in financial NLP is a thriving research area. While some general domain language models, like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT (Brown et al., 2020; OpenAI, 2023) and LLAMA (Touvron et al., 2023a,b) have been applied to financial NLP tasks, financial domain models like FinBERT (Araci, 2019; Yang et al., 2020; Huang et al., 2023), FLANG (Shah et al., 2022), PIXIU (Xie et al., 2023), InvestLM (Yang et al., 2023) and BloombergGPT (Wu et al., 2023) are specifically designed to contain domain expertise and generally\nperform better in financial tasks. Recent work such as FLUE (Shah et al., 2022) has been introduced to benchmark those language models in the finance domain. However, the capability of more advanced LLMs, like ChatGPT and GPT-4, has not been benchmarked, especially on proprietary datasets. In this work, in addition to the public tasks used in FLUE, we newly include four proprietary tasks in FinLMEval and conduct comprehensive evaluations for those financial language models."
        },
        {
            "heading": "3 Methods",
            "text": "We compare two types of models in FinLMEval: the Transformers encoder-only models that require fine-tuning on the labeled dataset, and decoder-only models that are prompted with zero-shot or fewshot in-context instructions. Figure 1 provides an outline of evaluation methods of FinLMEval."
        },
        {
            "heading": "3.1 Encoder-only Models",
            "text": "Our experiments explore the performance of various notable encoder-only models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), FinBERT (Yang et al., 2020) and FLANG (Shah et al., 2022). BERT and RoBERTa are pre-trained on general domain corpora, while FinBERT and FLANG are pre-trained on a substantial financial domain corpus. We fine-tune the language models on specific tasks. Following the fine-tuning process, inference can be performed on the fine-tuned models for specific applications."
        },
        {
            "heading": "3.2 Decoder-only Models",
            "text": "We also evaluate the performance of various popular decoder-only language models: ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), PIXIU (Xie et al., 2023), LLAMA2-7B (Touvron et al., 2023b) and Bloomberg-GPT (Wu et al., 2023). ChatGPT and GPT-4, developed by OpenAI, are two advanced LLMs that showcase exceptional language understanding and generation abilities. The models are pre-trained on a wide array of textual data and reinforced by human feedback. PIXIU is a financial LLM based on finetuning LLAMA (Touvron et al., 2023a) with instruction data. LLAMA2 is a popular open-sourced LLM pre-trained on extensive online data, and BloombergGPT is an LLM for finance trained on a wide range of financial data. As the model size of the evaluated decoder-only models is extremely large, they usually do not require fine-tuning the whole model on downstream tasks. Instead, the decoder-only models provide answers via zero-shot and few-shot in-context prompting.\nWe conduct zero-shot prompting for all decoderonly models. We manually write the prompts for every task. An example of prompts for the sentiment classification task is provided in Figure 1, and the manual prompts for other tasks are provided in Appendix A. Furthermore, to evaluate whether few-shot in-context learning can improve the model performance, we also conduct in-context learning experiments on ChatGPT. We use two strategies to select the in-context examples for few-shot incontext learning: random and similar. The former strategy refers to random selection, and the latter selects the most similar sentence regarding the query sentence. All in-context examples are selected from the training set, and one example is provided from each label class."
        },
        {
            "heading": "4 Datasets",
            "text": "Our evaluation relies on nine datasets designed to evaluate the financial expertise of the models from diverse perspectives. Table 1 overviews the number of training and testing samples and the source information for each dataset. Below, we provide an introduction to each of the nine datasets.\nFinSent is a newly collected sentiment classification dataset containing 10,000 manually annotated sentences from analyst reports of S&P 500 firms.\nFPB Sentiment Classification (Malo et al., 2014) is a classic sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences divided by the agreement rate of 5-8 annotators. We use the subset of 75% agreement.\nFiQA SA (FiQA) is a aspect-based financial sentiment analysis dataset. Following the \"Sentences for QA-M\" method in (Sun et al., 2019), for each (sentence, target, aspect) pair, we transform the sentence into the form \"what do you think of the {aspect} of {target}? {sentence}\" for classification.\nESG evaluates an organization\u2019s considerations on environmental, social, and corporate governance. We collected 2,000 manually annotated sentences from firms\u2019 ESG reports and annual reports.\nFLS, the forward-looking statements, are beliefs and opinions about a firm\u2019s future events or results. FLS dataset, aiming to classify whether a sentence contains forward-looking statements, contains 3,500 manually annotated sentences from the Management Discussion and Analysis section of annual reports of Russell 3000 firms.\nQA contains question-answering pairs extracted from earnings conference call transcripts. The goal of the dataset is to identify whether the answer is valid to the question.\nHeadlines (Sinha and Khandait, 2020) is a dataset for the commodity market that analyzes\nnews headlines across multiple dimensions. The tasks include the classifications of Price Direction Up (PDU), Price Direction Constant (PDC), Price Direction Down (PDD), Asset Comparison(AC), Past Information (PI), Future Information (FI), and Price Sentiment (PS).\nNER (Alvarado et al., 2015) is a named entity recognition dataset of financial agreements.\nFOMC (Shah et al., 2023) aims to classify the stance for the FOMC documents into the tightening or the easing of the monetary policy.\nAmong the datasets, FinSent, ESG, FLS, and QA are newly collected proprietary datasets."
        },
        {
            "heading": "5 Experiments",
            "text": "This section introduces the experiment setups and reports the evaluation results."
        },
        {
            "heading": "5.1 Model Setups",
            "text": "Encoder-only models setups. We use the BERT (base,uncased), RoBERTa (base), FinBERT (pretrain), and FLANG-BERT from Huggingface1, and the model fine-tuning is implemented via Trainer 2. For all tasks, we fix the learning rate as 2\u00d7 10\u22125, weight decay as 0.01, and the batch size as 48. We randomly select 10% examples from the training set as the validation set for model selection and\n1https://huggingface.co/ 2https://huggingface.co/docs/transformers/\nmain_classes/trainer\nfine-tune the model for three epochs. Other hyperparameters remain the default in Trainer.\nDecoder-only models setups. In the zero-shot setting, for ChatGPT and GPT-4, We use the \"gpt3.5-turbo\" and \"gpt-4\" model API from OpenAI, respectively. We set the temperature and top_p as 1, and other hyperparameters default by OpenAI API. The ChatGPT results are retrieved from the May 2023 version, and the GPT-4 results are retrieved in August 2023. For PIXIU and LLAMA2, we use the \"ChanceFocus/finma-7bnlp\" and \"meta-llama/Llama-2-7b\" models from Huggingface. The model responses are generated greedily. All prompts we used in the zero-shot setting are shown in Appendix A. Besides, as the BloombergGPT (Wu et al., 2023) is not publicly available, we directly adopt the results from the original paper.\nFor in-context learning, we conduct two strategies for in-context sample selection: random and similar. We select one example from each label with equal probability weighting for random sample selection. For similar sample selection, we get the sentence embeddings by SentenceTransformer (Reimers and Gurevych, 2019) \"all-MiniLM-L6v2\" model3 and use cosine similarity as the measure of similarity. Then, we select the sentences with the highest similarity with the query sentence as the\n3https://www.sbert.net/\nin-context examples. The prompts for in-context learning are directly extended from the corresponding zero-shot prompts, with the template shown in Figure 1."
        },
        {
            "heading": "5.2 Main Results",
            "text": "Table 2 compares the results of the fine-tuned encoder-only models and zero-shot decoder-only models in 9 financial datasets. We have the following findings:\nIn 6 out of 9 datasets, fine-tuned encoderonly models can perform better than decoderonly models. The decoder-only models, especially those that have experienced RLHF or instructiontuning, demonstrate considerable performance on zero-shot settings on the financial NLP tasks. However, their performance generally falls behind the fine-tuned language models, implying that these large language models still have the potential to improve their financial expertise. On the other hand, fine-tuned models are less effective when the training examples are insufficient (FiQA SA) or imbalanced (FOMC).\nThe performance gaps between fine-tuned models and zero-shot LLMs are larger on pro-\nprietary datasets than publicly available ones. For example, the FinSent, FPB, and FiQA SA datasets are comparable and all about financial sentiment classification. However, zero-shot LLMs perform the worst on the proprietary dataset FinSent. The performance gaps between fine-tuned models and zero-shot LLMs are also more significant on other proprietary datasets (ESG, FLS, and QA) than the public dataset.\nTable 3 compares the zero-shot and in-context few-shot learning of ChatGPT. In ChatGPT, the zero-shot and few-shot performances are comparable in most cases. When zero-shot prompting is ineffective, adding demonstrations can improve ChatGPT\u2019s performance by clarifying the task, as the results of ESG and Headlines-PI tasks show. Demonstrations are ineffective for easy and welldefined tasks, such as sentiment classifications and Headlines (PDU, PDC, PDD, AC, and FI), as the zero-shot prompts clearly instruct ChatGPT."
        },
        {
            "heading": "6 Conclusions",
            "text": "We present FinLMEval, an evaluation framework for financial language models. FinLMEval comprises nine datasets from the financial domain, and we conduct the evaluations on various popular language models. Our results show that fine-tuning expert encoder-only models generally perform better than the decoder-only LLMs on the financial NLP tasks, and adding in-context demonstrations barely improves the results. Our findings suggest that there remains room for enhancement for more advanced LLMs in the financial NLP field. Our study provides foundation evaluations for continued progress in developing more sophisticated LLMs within the financial sector."
        },
        {
            "heading": "7 Limitations",
            "text": "This paper has several limitations to improve in future research. First, our evaluation is limited to some notable language models, while other advanced LLMs may exhibit different performances from our reported models. Also, as the LLMs keep evolving and improving over time, the future versions of the evaluated models can have different performance from the reported results. Second, FinLMEval only focuses on financial classification tasks, and the analysis of the generation ability of the LLMs still needs to be included. Future work can be done toward developing evaluation benchmarks on generation tasks in the financial domain."
        }
    ],
    "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing"
}