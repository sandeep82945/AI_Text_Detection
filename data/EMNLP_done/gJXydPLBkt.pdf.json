{
    "abstractText": "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDEVAL, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both finetuned systems and LLMs. Using QUDEVAL, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored highly by our human evaluators, suggesting that there is headroom for further progress on language modeling to improve both QUD parsing and QUD evaluation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yating Wu"
        },
        {
            "affiliations": [],
            "name": "Ritika Mangla"
        },
        {
            "affiliations": [],
            "name": "Greg Durrett"
        },
        {
            "affiliations": [],
            "name": "Junyi Jessy Li"
        }
    ],
    "id": "SP:adb9f697a9019ed93ffb8f6cca506add2e96711f",
    "references": [
        {
            "authors": [
                "References Jacopo Amidei",
                "Paul Piwek",
                "Alistair Willis."
            ],
            "title": "Evaluation methodologies in automatic question generation 2013-2018",
            "venue": "Proceedings of the 11th International Conference on Natural Language Genera-",
            "year": 2018
        },
        {
            "authors": [
                "Ron Artstein",
                "Massimo Poesio."
            ],
            "title": "Inter-coder agreement for computational linguistics",
            "venue": "Computational linguistics, 34(4):555\u2013596.",
            "year": 2008
        },
        {
            "authors": [
                "Nicholas Asher",
                "Nicholas Michael Asher",
                "Alex Lascarides."
            ],
            "title": "Logics of conversation",
            "venue": "Cambridge University Press.",
            "year": 2003
        },
        {
            "authors": [
                "David I Beaver",
                "Brady Z Clark."
            ],
            "title": "Sense and sensitivity: How focus determines meaning",
            "venue": "John Wiley & Sons.",
            "year": 2009
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Anton Benz",
                "Katja Jasinskaja."
            ],
            "title": "Questions under discussion: From sentence to discourse",
            "venue": "Discourse Processes, 54(3):177\u2013186.",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Buring"
            ],
            "title": "What\u2019s new (and what\u2019s given) in the theory of focus",
            "venue": "In Annual Meeting of the Berkeley Linguistics Society,",
            "year": 2008
        },
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "Controllable openended question generation with a new question type ontology",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Asli Celikyilmaz",
                "Elizabeth Clark",
                "Jianfeng Gao."
            ],
            "title": "Evaluation of text generation: A survey",
            "venue": "arXiv preprint arXiv:2006.14799.",
            "year": 2020
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Justin Lewis",
                "Smaranda Muresan."
            ],
            "title": "CONSISTENT: Open-ended question generation from news articles",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6954\u20136968, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Kordula De Kuthy",
                "Nils Reiter",
                "Arndt Riester."
            ],
            "title": "QUD-based annotation of discourse structure and information structure: Tool and evaluation",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Xuan Long Do",
                "Bowei Zou",
                "Liangming Pan",
                "Nancy Chen",
                "Shafiq Joty",
                "Aiti Aw."
            ],
            "title": "Cohs-cqg: Context and history selection for conversational question generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Sujatha Das Gollapalli",
                "See-Kiong Ng."
            ],
            "title": "QSTS: A question-sensitive text similarity measure for question generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 3835\u20133846.",
            "year": 2022
        },
        {
            "authors": [
                "Jing Gu",
                "Mostafa Mirshekari",
                "Zhou Yu",
                "Aaron Sisto."
            ],
            "title": "ChainCQG: Flow-Aware Conversational Question Generation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yufang Hou."
            ],
            "title": "End-to-end neural information status classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1377\u20131388.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Kehler",
                "Hannah Rohde."
            ],
            "title": "Evaluating an expectation-driven question-under-discussion model of discourse interpretation",
            "venue": "Discourse Processes, 54(3):219\u2013238.",
            "year": 2017
        },
        {
            "authors": [
                "Gangwoo Kim",
                "Sungdong Kim",
                "Kang Min Yoo",
                "Jaewoo Kang."
            ],
            "title": "Generating information-seeking conversations from unlabeled documents",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2362\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Jen Ko",
                "Te-yuan Chen",
                "Yiyan Huang",
                "Greg Durrett",
                "Junyi Jessy Li."
            ],
            "title": "Inquisitive question generation for high level text comprehension",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6544\u20136555.",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Jen Ko",
                "Cutter Dalton",
                "Mark Simmons",
                "Eliza Fisher",
                "Greg Durrett",
                "Junyi Jessy Li."
            ],
            "title": "Discourse comprehension: A question answering framework to represent sentence connections",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Jen Ko",
                "Yating Wu",
                "Cutter Dalton",
                "Dananjay Srinivas",
                "Greg Durrett",
                "Junyi Jessy Li."
            ],
            "title": "Discourse analysis via questions and answers: Parsing dependency structures of questions under discussion",
            "venue": "Findings of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Tom Kocmi",
                "Christian Federmann."
            ],
            "title": "Large language models are state-of-the-art evaluators of translation quality",
            "venue": "pages 193\u2013203.",
            "year": 2023
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing Krippendorff\u2019s alpha-reliability",
            "year": 2011
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Mohit Iyyer."
            ],
            "title": "Generating question-answer hierarchies",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2321\u20132334.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Lavie",
                "Abhaya Agarwal."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228\u2013231.",
            "year": 2007
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou."
            ],
            "title": "ChatGPT as a factual inconsistency evaluator for abstractive text summarization",
            "venue": "arXiv preprint arXiv:2303.15621.",
            "year": 2023
        },
        {
            "authors": [
                "William C Mann",
                "Sandra A Thompson."
            ],
            "title": "Rhetorical structure theory: Toward a functional theory of text organization",
            "venue": "Text-interdisciplinary Journal for the Study of Discourse, 8(3):243\u2013281.",
            "year": 1988
        },
        {
            "authors": [
                "Katja Markert",
                "Yufang Hou",
                "Michael Strube."
            ],
            "title": "Collective classification for fine-grained information status",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 795\u2013804.",
            "year": 2012
        },
        {
            "authors": [
                "Mao Nakanishi",
                "Tetsunori Kobayashi",
                "Yoshihiko Hayashi."
            ],
            "title": "Towards answer-unaware conversational question generation",
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 63\u201371.",
            "year": 2019
        },
        {
            "authors": [
                "Shashi Narayan",
                "Joshua Maynez",
                "Reinald Kim Amplayo",
                "Kuzman Ganchev",
                "Annie Louis",
                "Fantine Huot",
                "Anders Sandholm",
                "Dipanjan Das",
                "Mirella Lapata."
            ],
            "title": "Conditional generation with a questionanswering blueprint",
            "venue": "Transactions of the Association",
            "year": 2023
        },
        {
            "authors": [
                "Preksha Nema",
                "Mitesh M Khapra."
            ],
            "title": "Towards a better metric for evaluating question generation systems",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950\u20133959.",
            "year": 2018
        },
        {
            "authors": [
                "Benjamin Newman",
                "Luca Soldaini",
                "Raymond Fok",
                "Arman Cohan",
                "Kyle Lo."
            ],
            "title": "A controllable QA-based framework for decontextualization",
            "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Technical Report.",
            "year": 2019
        },
        {
            "authors": [
                "Sudha Rao",
                "Hal Daum\u00e9 III."
            ],
            "title": "Answer-based adversarial training for generating clarification questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Arndt Riester",
                "Lisa Brunetti",
                "Kordula De Kuthy."
            ],
            "title": "Annotation guidelines for questions under discussion and information structure",
            "venue": "Information structure in lesser-described languages. Studies in prosody and syntax, pages 403\u2013443.",
            "year": 2018
        },
        {
            "authors": [
                "Craige Roberts."
            ],
            "title": "Information structure: Towards an integrated formal theory of pragmatics",
            "venue": "Semantics and pragmatics, 5:6\u20131.",
            "year": 2012
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford Alpaca: An Instruction-following LLaMA model",
            "venue": "https: //github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Jan Van Kuppevelt."
            ],
            "title": "Discourse structure, topicality and questioning",
            "venue": "Journal of linguistics, 31(1):109\u2013 147.",
            "year": 1995
        },
        {
            "authors": [
                "Manya Wadhwa",
                "Jifan Chen",
                "Junyi Jessy Li",
                "Greg Durrett."
            ],
            "title": "Using natural language explanations to rescale human judgments",
            "venue": "arXiv preprint arXiv:2305.14770.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Is ChatGPT a good NLG evaluator? A preliminary study",
            "venue": "arXiv preprint arXiv:2303.04048.",
            "year": 2023
        },
        {
            "authors": [
                "Matthijs Westera",
                "Laia Mayol",
                "Hannah Rohde."
            ],
            "title": "TED-Q: TED talks and the questions they evoke",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, pages 1118\u20131127.",
            "year": 2020
        },
        {
            "authors": [
                "Yating Wu",
                "William Sheffield",
                "Kyle Mahowald",
                "Junyi Jessy Li."
            ],
            "title": "Elaborative simplification as implicit questions under discussion",
            "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text comprehension at the discourse level entails understanding higher level structures between sentences and paragraphs, beyond the meaning of individual words. The linguistic framework of Questions Under Discussion (QUD) (Van Kuppevelt, 1995; Roberts, 2012; Benz and Jasinskaja, 2017) views the progression of discourse as continuously posing (implicit) questions (i.e., QUDs) and answering them; thus each sentence in a monologue is an answer, or part of an answer, to a QUD. In Figure 2, the third sentence answers the implicit QUD, \u201cWhat does Glenn say about the situation?\u201d, elicited from sentence 2. The advent of large language models (LLMs) makes viewing discourse in a question generation and answering fashion increasingly\ntractable for settings that require higher-level processes, e.g., planning in text generation (Narayan et al., 2023), contextualization in question answering (Newman et al., 2023), and elaborative simplification (Wu et al., 2023). However, rigorous evaluation of the question generation aspect of QUD frameworks remains an open challenge.\nQUD parsing is relatively new to the NLP community, with most of the prior work in discourse parsing focusing on structures like Rhetorical Structure Theory (Mann and Thompson, 1988), Segmented Discourse Representation Theory (Asher et al., 2003), and the Penn Discourse Treebank (Prasad et al., 2008), all of which use a fixed, hierarchical discourse relation taxonomy and are relatively straightforward to evaluate using accuracies or F measures. QUD parsing (illustrated in Figure 2), however, entails a question generation task that is contextually grounded (Ko et al., 2023).\nThis structure necessitates two key principles for evaluation. First, the questions are raised after a certain amount of common ground is established, so parsing QUD involves identifying where in the context a question can be triggered (also called the anchor sentence). These anchors are produced by parsers and form part of the structure to evaluate. Second, for a question to become a QUD, it needs to satisfy several linguistic constraints (Riester et al., 2018) and be plausible without knowing a priori how the discourse will progress.\nThe above characteristics make QUD parsing much tricker to evaluate than other discourse parses; thus, prior work in QUD dependency parsing (Ko et al., 2023) performed only manual evaluation. Automatic evaluation for generation tasks is known to be challenging (Celikyilmaz et al., 2020) and standard evaluation metrics for question generation (Amidei et al., 2018; Nema and Khapra, 2018; Gollapalli and Ng, 2022) tend to be superficial linguistically. They do not evaluate the aforementioned desiderata of QUD.\nOutlined in Figure 1, this work provides foundational building blocks for the automatic evaluation of QUD dependency parsers. Building on prior theoretical work on QUD and its annotation (Riester et al., 2018; De Kuthy et al., 2018), as well as error analysis on a large number of machine outputs from several different models, we design four key criteria to assess QUD question generation quality, covering language quality, answer compatibility, question givenness, and anchor relevance.\nWith these criteria, we present QUDEVAL, 2,190 generated question-anchor pairs annotated\nby trained linguists across 51 news articles. These annotations evaluate three QUD dependency parsing models: Ko et al. (2023)\u2019s pipeline system, as well as our newly designed prompts for QUD parsing with LLMs using OpenAI ChatGPT (GPT 3.5 Turbo), GPT-4, and Stanford Alpaca (Taori et al., 2023). Human annotations using the QUDEVAL framework reveal that modern LLMs, while able to generate fluent questions, often fail at satisfying these constraints; the errors vary across systems, rendering such fine-grained evaluation necessary.\nQUDEVAL also provides a valuable testbed for the assessment of automatic evaluation metrics for QUD parsers. We engage with rule-based and LLM-based baselines, prior work relevant for specific criteria, and reference-based metrics standard in question generation evaluation. Results show that while these metrics align with QUDEVAL\u2019s annotations to some extent, they give poor estimates of QUD generation quality (except for anchor groundedness) in comparison to an estimated human upper bound. This points to both challenges and substantial headroom for the development of automatic metrics.\nTo sum, QUDEVAL points to clear directions for future progress: (1) human evaluation reveals consistently higher quality for crowdsourced QUDs compared to generated ones, suggesting opportunities for better QUD parsers; (2) the development of linguistically-informed automatic metrics targeting QUD parsing quality is necessary, for which QUDEVAL can serve as a benchmark.\nQUDEVAL is available at https://github. com/lingchensanwen/QUDeval."
        },
        {
            "heading": "2 Background and Related Work",
            "text": "QUD Annotation and Parsing Despite its rich linguistic history (see Benz and Jasinskaja (2017) for an overview), QUD\u2019s presence in NLP is in its infancy as annotated datasets have only recently started emerging. These datasets follow different QUD paradigms, reflecting its diverse interpretation: Westera et al. (2020) and Ko et al. (2020) presented QUDs in an expectation-driven (Kehler and Rohde, 2017) manner, where questions are elicited while reading (i.e., without seeing the rest of the article). Unanswerable questions are thus a natural artifact of this process. De Kuthy et al. (2018) annotated QUD trees with full, hierarchical questions (i.e., the answer to a parent question entails the answer to a child question (Roberts, 2012)); however, due to the challenging annotation process, the dataset only contains two sections of an interview transcript, too small to train automatic parsers.\nThe parsing framework that this paper engages with is from Ko et al. (2023), trained with the DCQA dataset from Ko et al. (2022). Ko et al. (2023) views the QUD structure as a dependency tree, where each sentence in a document is connected to an anchor sentence in its prior context via a QUD (formalized in Section 3). This is a lightweight approach to QUD as the questions themselves are not guaranteed to be hierarchical; however this simplification allows large-scale, crowdsourced data collection (Ko et al., 2022). To date, this remains the only QUD parser available.\nEvaluation of Question Generation Prior work has tackled contextual question generation, including conversational (Nakanishi et al., 2019; Gu et al., 2021; Do et al., 2022; Kim et al., 2022), open-ended (Ko et al., 2020; Cao and Wang, 2021; Chakrabarty et al., 2022), and nested questions (Krishna and Iyyer, 2019). While some of these works perform human evaluation, they typically cover dimensions such as fluency, relevance/plausibility, scope, and question-answer consistency. In clarification question generation (Rao and Daum\u00e9 III, 2019), dimensions such as usefulness and informativeness are also considered. However, the above criteria are insufficient for a linguistically meaningful evaluation of QUD generation. Ko et al. (2023) used a human evaluation taxonomy for their QUD parser; this work reflects an extended version of that earlier taxonomy, better aligned with theoretical principles.\nSimilar to automatic evaluation of Natural Lan-\nguage Generation (NLG) tasks in general (Celikyilmaz et al., 2020), automatic evaluation of question generation is known to be challenging and inconsistent (Amidei et al., 2018). We evaluate commonlyused metrics, as well as a recent metric from Gollapalli and Ng (2022), using our linguisticallygrounded frameworks."
        },
        {
            "heading": "3 QUD Evaluation Framework",
            "text": "QUDs are subjective: different readers often come up with distinct questions even with the same answer sentence (Ko et al., 2022). Thus our goal is to target common principles that QUDs need to satisfy. First, we instantiate theoretical constraints of QUD (Riester et al., 2018; De Kuthy et al., 2018) in a concrete protocol. Second, we consider common errors in text generation systems, including question generation systems, and ensure they are captured in our taxonomies.\nSetup and Definitions Each evaluation instance corresponds to an individual edge in a dependency tree where a QUD connects two sentences: the head of an edge is an anchor sentence Sk (the kth sentence in a document) where the question is elicited, and the child node is an answer sentence Sa that answers the question (Figure 2). Thus we denote an edge as (Q,Sk, Sa) where Q is the question string. QUD dependency parsing entails considering each sentence in a document as Sa and, for that answer sentence, generating Q\u0302 and predicting k\u0302, the anchor index.\nWe present four criteria that assess both tasks separately and jointly, with the full annotatorfacing instructions listed in Appendix Figure 4. Note that it is both theoretically and operationally possible that more than one sentence satisfies the criteria of being an anchor. In practice, we see that this depends on varying levels of specificity of the question and the document context. Because of this, our evaluation framework independently evaluates each QUD and its predicted anchor.\nLanguage Quality (Lang.) This criterion is designed to filter out obviously bad questions. These include: badly formed questions that are not accommodatable, questions that are irrelevant to the article, and questions with content that obviously contradicts the article. Note that we direct annotators to skip the rest of the evaluation if Q\u0302 fails to satisfy this criterion. Examples of such questions\nare shown in Appendix Table 17.1\nAnswer Compatibility (Comp.) This criterion states that Sa should answer Q\u0302. This is one of the key criteria used both in Riester et al. (2018)\u2019s guidelines (called \u201ccongruence\u201d), as well as in human evaluations for QG systems, e.g., called \u201canswer validity\u201d in Krishna and Iyyer (2019) and \u201cconsistency\u201d in Gu et al. (2021). We additionally consider the important role QUD plays in information structure in pragmatics (Buring, 2008; Beaver and Clark, 2009), i.e., QUDs are used to tease apart the main part (called focus or at-issue content) from background information in a sentence (Riester et al., 2018). Thus it is the focus of Sa that should answer Q (Ko et al., 2022). We introduce a graded notion of Answer Compatibility:\n(1) Direct and explicit: full compatibility described above (examples in Figure 2).\n(2) Unfocused: Sa contains the answer, but the answer is not its focus. An example is shown in Appendix Table 18.\n(3) Not answered: Sa does not answer Q\u0302. In Figure 2, sentence 5 should be the answer for the generated QUD (1,5); however, sentence 5 does not state the reason for the export ban. Another example is shown in Figure 3.\nGivenness (Givn.) Since QUDs should be reader questions invoked at Sk\u0302 for upcoming, unseen discourse given the common ground already established, Q\u0302 should only contain concepts that are accessible by the reader from prior context; this is called \u201cQ-Givenness\u201d in Riester et al. (2018). While they loosely define givenness as \u201cgiven (or, at least, highly salient) material\u201d, we concretely specify this notion with information status (Markert et al., 2012): Q\u0302 should only contain concepts (entities, events, or states) that were mentioned in the question context CQ\u0302 = S1, ..., Sk\u0302 (discourseold), or concepts that have not been directly mentioned but are generally known or inferrable from mentioned ones (mediated).\nBased on observations of machine errors from Ko et al. (2023), generated questions can often contain concepts that are from Sa itself, which is a particular error that prevents a QG model being used more widely in conversations (Nakanishi et al., 2019). We call this answer leakage. This criterion is divided into the following categories:\n1For wrong predictions of k\u0302 = a, the annotators also skip the QUD; this happens in about 3% of the LLM parsers and does not happen with Ko et al. (2023)\u2019s parser.\n(1) No new concepts: all concepts in Q\u0302 are discourse-old or mediated (examples in Figure 2).\n(2) Answer leakage: Q\u0302 contains new concepts not in question context CQ\u0302 = S1, ..., Sk\u0302 but in Sa. In Figure 2, QUD (1,6) states \u201cWhat is missing in the report?\u201d; yet the notion of \u201cmissing\u201d is absent in Sk\u0302 (S1) and only introduced in Sa (S6), i.e., based on the common ground established in Sk\u0302, it is quite a stretch for a reader to ask this question. Figure 3 shows another example.\n(3) Hallucination: Q\u0302 contains new concepts that are not answer-leakage. An example is shown in Appendix Table 19.\nNote that since Sk\u0302\u2019s position k\u0302 is predicted, this criterion jointly evaluates Q\u0302 and k\u0302. Anchor Relevance (Relv.) Our last criterion evaluates the anchor prediction k\u0302 given Q\u0302. This diverges significantly from Riester et al. (2018) since anchor prediction is not a task in human annotation. Yet, we align with their work stating that Q\u0302 should be relevant (i.e., contains given material) to the context where it was elicited. Referencing observations from our experiments and Ko et al. (2023), we design three categories:\n(1) Fully grounded: k\u0302 satisfies the desiderata above, which typically means that content in Q\u0302 follows mostlyfrom Sk\u0302 (examples in Figure 2).\n(2) Partially grounded: some content from Q\u0302 is grounded in Sk\u0302. For QUDs (1,5) and (1,6) in Figure 2, Sk\u0302 (S1) only contain some of the concepts accessible from Q\u0302.\n(3) Not grounded: content from Q\u0302 is largely not from Sk\u0302. For the ChatGPT-generated question in Figure 3, the concept of \u201crestrictions\u201d is the main focus of the question, which is irrelevant to Sk\u0302 (S5) that provides more information on the contents of the report.\n4 The QUDEVAL Dataset\nThis section describes our linguist-annotated QUDEVAL dataset evaluating fine-tuned (Ko et al., 2023) and LLM parsers, and their results."
        },
        {
            "heading": "4.1 Parsers Evaluated",
            "text": "Ko et al. (2023) This parser is trained as a pipeline, with a Longformer model (Beltagy et al., 2020) for anchor prediction and GPT-2 (Radford et al., 2019) for question generation; both are finetuned on the DCQA dataset (Ko et al., 2022). LLMs We also experiment with ChatGPT (GPT 3.5 Turbo), as well as Stanford Alpaca-7B (Taori\net al., 2023), as LLM-based parsers.2 We first prompt the model to generate Q\u0302 given the article and Sa. Subsequently, we used Q\u0302, the article, and Sa to generate the Sk\u0302. We experimented with various zero-shot and few-shot variations; our best prompt containing four in-context examples is provided in Table 8 in the Appendix. In particular, we explicitly prompted the model not to introduce phrases or ideas in the question which are newly introduced in Sa.\nFinally, we collected QUDs with GPT-4 after its API became available. This portion of the data (510 QUDs) was triply annotated as an addition to the dataset. However, due to the lack of API access at the time of experimentation, this portion of the data was not part of metric evaluation in Section 5."
        },
        {
            "heading": "4.2 Human Data Collection",
            "text": "Data Sourcing We run the above parsers on all news articles from the validation and test sets from Ko et al. (2023), which came from the DCQA dataset (Ko et al., 2022). For each document, we sample 10 sentences as the set of answer sentences; for each answer sentence, 4 distinct (Q\u0302, Sk\u0302) pairs were generated, one from each system. This amounts to 2,040 machine-generated QUDs in total. Additionally, we annotated 150 crowdsourced questions in DCQA across 15 articles both as an assessment of prior annotated QUD questions and a validation of our taxonomy. Thus the total number of distinct QUDs annotated is 2,190. Annotation Procedure and Agreement Our annotation team consists of 3 students in a linguistics department; these students had extensive prior experience in data annotation before this task. The students were trained with our annotation guide-\n2A temperature of 0 is used throughout the paper for LLMs. We noticed that increasing the temperature for our setting resulted in the generated questions and anchors containing concepts which are outside of the context provided.\nlines on 20 questions (2 news articles \u00d7 2 systems) our team has iterated on. We release our annotation interface, presented in Appendix A.\nNext, all three annotators annotated 200 distinct QUDs (10 articles, 10 answer sentences each, and 2 QUDs for each answer sentence). This set is used to calculate inter-annotator agreement, reported in Table 2. Krippendorff\u2019s \u03b1 (Krippendorff, 2011) values indicate moderate agreement (Artstein and Poesio, 2008) across all criteria. Table 2 also shows the % of questions where all 3 annotators agreed on a label, and an aggregated pairwise F1 score that reflects how accurately one annotator captures another\u2019s labels. The F1 can be viewed as a human upper bound for automatic metrics (Section 5).\nThe annotation team discussed a representative portion of the data on which they did not fully agree. In almost all cases, disagreements are genuine differences in interpretation of the article itself or borderline cases between labels; some of these examples are shown in Table 20 in the Appendix. In addition, there were 25 QUDs that involved at least one criterion without a majority label; these questions were adjudicated after agreement was calculated. The adjudicated labels are included in QUDEVAL.\nGiven (a) the high cognitive load of this task, (b) how well-trained the annotators are, and (c) the fact that we found no case of erroneous task interpretation for the triply annotated set above, the rest of the questions are annotated by one annotator."
        },
        {
            "heading": "4.3 QUD Parser Evaluation Results",
            "text": "Table 1 shows the distribution of annotated labels for each system evaluated, and for the set of 150 DCQA questions. All models score greater than 92% in terms of Language Quality, a clear indication of their question generation capability in general. However, for all other criteria, there is a large percentage of errors. For Answer Compatibility, GPT-4 clearly leads the other models. Ko et al. (2023)\u2019s system generates questions that are more grounded in the context, and predicts anchors that are the most relevant; however, their system scored the worst in hallucinations. None of the LLMs outperforms Ko et al. (2023)\u2019s fine-tuned Longformer model for anchor prediction.\nNotably, for all criteria other than Answer Compatibility, the models underperform crowdsourced questions in DCQA by a large margin. For Answer Compatibility, ChatGPT and GPT-4 scored higher in terms of direct answers, but we observe that this is directly linked to its tendency to leak answers in the first place. These results show that QUD parsing remains a challenging task and that QUDEVAL\u2019s evaluation framework is able to capture errors from distinct types of models.\nQualitative analysis reveals that outputs from these models are of different styles (Figure 3). Ko et al. (2023)\u2019s model tends to generate more general questions, likely resulting in fewer errors in Givenness and Anchor Relevance. On the other hand, GPT models generate more verbose questions that are too grounded in the answer sentence itself, resulting in profuse answer leakage. Table 3 shows the average lengths of questions for each system. Alpaca makes a distinct set of errors: it generates identical questions across different anchor and answer sentences up to 20% of the time, much more frequently than other models (Table 3)."
        },
        {
            "heading": "5 Automatic Evaluation Metrics",
            "text": "QUDEVAL can serve as the first benchmark for automatic evaluation metrics for QUD dependency\nparsers. As a first step, we evaluate a set of baseline reference-free and reference-based metrics.3 Note that we do not evaluate the Language Quality criterion, since all modern LLMs are capable of generating fluent questions (Table 1)."
        },
        {
            "heading": "5.1 Reference-Free Metrics",
            "text": "Baselines tested here include rule-based metrics, prior work on information status (Hou, 2021), and classification/scoring with GPT models. The latter has shown promise for evaluation in machine translation (Kocmi and Federmann, 2023) and summarization (Luo et al., 2023; Wang et al., 2023). We held out the two articles used for annotator training as the validation set (60 questions). Metric Preliminaries All metrics are classifiers taking the form f : (Sa, Sk\u0302, Q\u0302, CQ\u0302) \u2192 Y , mapping from a QUD (answer sentence, anchor sentence, predicted question, and context) to one of the labels (in the set Y) for one of our criteria. Note that not all criteria necessarily use every piece of information; anchor relevance does not use anything about the answer sentence, for example.\nWe define two types of GPT4-based metrics throughout this section. First, GPT-Cls uses ChatGPT as either a zero-shot (-zs) or few-shot (-fs) classifier with an appropriate prompt. This directly maps into the label set Y . Second, GPT-Scr follows Kocmi and Federmann (2023) and Wang et al. (2023) to use GPT-4 to assign a real-valued score between 1 and 100 for the quantity of interest. We\n3As stated in Section 4.1, human evaluation results of GPT4 generated QUDs were not included to test automatic metrics due to the lack of API access at the time of experimentation.\nthen use a mapping function to convert values in this interval to labels in Y , tuned for macro-F1 on the validation set. Such a mapping from this interval is possible because many of our error categories have an ordinal semantics associated with them. Answer Compatibility (1) GPT-Scr assesses how well Sa answers Q\u0302. Mapping function and prompt detailed in Appendix B. (2) GPT-Ans: Wadhwa et al. (2023) showed the efficacy of GPT-4 on a QA setting similar to QUD. We prompt GPT-4 using their prompt to generate an answer, given the article, Q\u0302, Sk\u0302. We then prompt it again to find the closest sentence in the article to the generated answer (prompt in Appendix Table 10). Givenness (1) Rule-based: We scan lemmatized content words in Q\u0302 for new words not present in CQ\u0302; if they appear only in Sa, then we label Q as \u2018answer leakage\u2019, otherwise as \u2019hallucination\u2019. (2) Hou (2021): we run the state-of-the-art information status classification model from Hou (2021), using the top-level labels (new, old, mediated). Since both new and mediated concepts are allowed, we merge these two classes. We use a similar rule as (1) to differentiate between answer leakage and hallucination, detailed in Appendix B. (3) GPT-Cls: the prompts can be found in Appendix Tables 12 (zero-shot) and 13 (few-shot). Anchor Relevance (1) Rule-based: This method checks if the focus (approximated by maximum NP) of Q\u0302 overlaps with the predicted anchor Sk\u0302.\nWe check for content word overlap with the Sk\u0302. Q\u0302 is labeled as \u2018fully grounded\u2019 if all words in its max NP are present in Sk\u0302, \u2018not grounded\u2019 if none are present, and \u2018partially grounded\u2019 otherwise. (2) GPT-Cls: the prompts are in Appendix Tables 14 (zero-shot) and 15 (few-shot). (3) GPT-Scr: mapping function and prompt are in Appendix B. (4) BLEU1-sim scores an answer by computing BLEU-1 (Papineni et al., 2002) between Q\u0302 and Sk\u0302; mapping function detailed in Appendix B."
        },
        {
            "heading": "5.2 Reference-Based Metrics",
            "text": "We further evaluate reference-based metrics standard in question generation that capture the similarity between Q\u0302 and Q: (1) BLEU-1 (Papineni et al., 2002);4 (2) BERTScore (rescaled) (Zhang et al., 2020); (3) METEOR (Lavie and Agarwal, 2007); (4) ROUGE-F1 (Lin, 2004); (5) QSTS (Gollapalli and Ng, 2022), a recent reference-based metric for question generation evaluation that explicitly represents the question class and named entities in a given question pair, and combines them with dependency tree information and word embeddings to measure the semantic similarity between two questions;5 (6) GPT-Scr (prompt in Appendix Ta-\n4While it is standard to report BLEU 1, 2, 3, 4, Nema and Khapra (2018) found that BLEU-1 correlates with human annotations better than other settings.\n5The original QSTS uses the harmonic mean to combine several sub-scores, which yields a score of zero when there\nble 11). The reference-based metric values are mapped to labels using the same mapping function mechanism described in Section 5.1."
        },
        {
            "heading": "5.3 Results and Analysis",
            "text": "Results are shown in Tables 4 (reference-free) and 5 (reference-based), respectively. For reference, we report a random baseline that samples according to the distribution of labels in QUDEVAL.\nReference-free metrics score substantially better than reference-based ones, which are only slightly better than random. This questions the validity of using reference-based metrics for QUD parser evaluation, which we analyze further.\nNotably, only GPT-Scr for the Relevance criterion is close to the human upper bound (Table 2). The performance on minority classes (see Table 1 for class frequencies) are especially low. GPT-Scr turned out to be often the best metric across different criteria; however, we point out caveats with its usage later in analysis."
        },
        {
            "heading": "Do these metrics rank systems correctly? It is",
            "text": "conceivable that, despite these metrics being imperfect detectors of errors, they might still give reliable aggregate judgments about systems. To visualize the impact of applying these metrics to score systems, we show the predicted system performance using the best metrics for each category\nis no entity overlap between Q\u0302 and Q. We found this too restrictive for QUDs that are less factoid and entity-centric; thus our variant uses the arithmetic mean instead.\nin Table 6. In general, the metrics do not adequately capture system-level ordering determined by human judges in Table 1. GPT-Scr is used for the Givenness and Anchor Relevance criterion; in both, it scores ChatGPT-generated QUDs much higher than it should, and even higher than human QUDs. This confirms the bias that GPT can favor itself when being used as an evaluation tool (Liu et al., 2023), although we did not include GPT-4 generated QUDs in the evaluation data per se. Do reference-based metrics capture similarity to annotated QUDs? One hypothesis is that standard reference-based metrics are not actually capturing similarity with ground-truth QUDs. To check this, our annotators rated a subset of 200 pairs of (Q\u0302,Q) for similarity (detailed in Appendix C). This annotation allows us to examine label distributions stratified by how similar generated QUDs are to the reference QUDs (Appendix D, Figure 9). While questions similar to the reference QUDs tend to score better, our metrics do not clearly capture question similarity well (Table 7). Furthermore, even knowing similarity may not be enough to evaluate all cases of QUD, particularly when there are many possible QUDs that models can generate. Do reference-based metrics capture QUD quality? Appendix Figures 7, 8 show the score distributions given by the best reference-based metrics split by ground-truth annotator label. For both METEOR and BERTScore, the score distributions are nearly identical across the categories, suggesting that it is tricky if not infeasible to tune a good threshold for such automatic metrics to map to our evaluation framework. This observation aligns with results from Table 5 where the metrics perform only slightly better than the random baseline. Since QUDs reflect discourse interpretations and hence are subjective, we believe this is strong evidence\nthat imperfect standard reference-based methods should not be used for QUD evaluation."
        },
        {
            "heading": "5.4 Discussion and Future Directions",
            "text": "As we established previously, QUD has utility for downstream generation tasks (Narayan et al., 2023; Newman et al., 2023; Wu et al., 2023). These recent lines of work indicate that question generation and answering is becoming more and more prominent as a way of handling discourse representation, despite a lack of rigorous evaluation of the generated questions themselves. This paper fills this gap by establishing a benchmark to evaluate automatic metrics, which are annotated by trained linguists. The standard metrics evaluated in this section show that we are far from having a reliable automatic metric for QUD evaluation; future work can iterate on more sophisticated methods to derive metrics, e.g., via supervised learning, iterative prompting, or chain-of-thought prompting."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present a dataset, QUDEVAL, for evaluating contextual question generation in the context of QUD discourse parsing. QUDEVAL implements prior theoretical evaluation criteria for QUD, operationalized as four criteria for which expert linguistics annotators give high-quality judgments. This dataset sheds light on the divergent performance of existing systems, and enables future work on developing stronger automated metrics for evaluating QUD discourse parsing.\nLimitations\nQUDEVAL evaluates each QUD edge independently thus does not take into account the relationship between questions. While full, hierarchical QUD trees constrain the answer of a QUD to entail the answers of its descendants (Roberts, 2012), a QUD dependency tree does not inherit this property (Ko et al., 2023). Thus we leave for future work to explore potential global constraints.\nQUDEVAL is collected by trained linguistics students. We are yet to explore reliable ways to scale this to a crowdsourcing platform with the hopes of speeding up data collection. QUDEVAL is also limited to English newswire data; future work should explore other languages and genres.\nThe metrics we evaluated are baseline metrics. As pointed out in Section 5.4, this leaves headroom for future metric development."
        },
        {
            "heading": "Acknowledgements",
            "text": "Special thanks to Kathryn Kazanas, Keziah Reina and Karim Villaescusa F. for providing data annotation for this project. This research was partially supported by National Science Foundation (NSF) grant IIS-2145479. We acknowledge the Texas Advanced Computing Center (TACC)6 at UT Austin for many of the results within this paper."
        },
        {
            "heading": "A Annotation Interface",
            "text": "Figure 4 shows the full annotator instruction. In Figure 5, we present our system UI for QUD evaluation. The answer sentence, anchor sentence, and prior context are highlighted. Questions to be evaluated are bolded, with other information displayed in gray.\nEach criterion has selectable options, and there is also a text box for annotators to provide additional comments on specific errors. When the evaluation for a QUD is completed, the corresponding block is marked in light blue to track progress. If a question does not make sense, all criteria are marked as \u201cskipped\u201d and the block is also highlighted in light blue.\nAfter submitting their answers, annotators can view the feedback in the table shown in Figure 6 to check their annotations and they can also copy the survey code directly.\nB Implementation Details for Reference-free Metrics\n(1) GPT-Scr (Answer Compatibility): We prompted GPT-4 to generate a score between 1 to 100 for how well Sa answers Q\u0302. Our mapping uses tuned thresholds on the validation set: a score of over 80 was mapped to \u2018Direct and explicit\u2019, between 60 and 80 was mapped to \u2018Unfocused\u2019 and a score lower than 80 was mapped to \u2018Not answered\u2019. Our best prompt is present in Table 9.\n(2) Hou (2021) (Givenness): We found that running the full mention detection process is prohibitively slow, thus we extracted the maximum NP and their heads to speed up mention detection.\nWe differentiate answer leakage vs. hallucinations in \u2018new\u2019 mentions with the same mechanism as our rule-based metric: we check if content words (lemmatized; also excluding question (wh-) words, pronouns and names) in Q\u0302 are present in CQ\u0302 and Sa. If there are content words absent in CQ\u0302 but found in Sa, Q\u0302 is labeled as \u2018answer leakage\u2019. If there are content words absent in both, they are deemed as \u2018hallucination\u2019. Otherwise, we label Q\u0302 as \u2018no new concepts\u2019.\n(3) GPT-Scr (Anchor Relevance): The prompt is shown in Table 16. A score of 80 and above\nis mapped to \u2018fully grounded\u2019, 80-20 to \u2018partially grounded\u2019, and below 20 to \u2018ungrounded\u2019.\n(4) BLEU1-sim: If the BLEU-1 score between the anchor and the question is greater than 0.05, we consider the QUD fully grounded in its predicted anchor. If the score falls between 0.01 and 0.05, we classify it as partially grounded. Otherwise, it is considered not grounded."
        },
        {
            "heading": "C Question Similarity Annotation",
            "text": "The annotators were given pairs of (Q\u0302,Q) and Sa, as well as the full article context (66, 66, and 67 questions generated from Ko et al. (2023), ChatGPT and Alpaca respectively). Each question pair consisted of a human-generated question and a model-generated question for the same sentence within an article.\nSimilar to the setup in DCQA, scores between 1 and 5 were given based on the extent of similarity, 5 being the score given to identical or paraphrased questions. Each question is annotated by 2 annotators and their average score is considered for our analysis. The inter-annotator correlation is 0.728 and Krippendorff\u2019s \u03b1 for the annotators is 0.687."
        },
        {
            "heading": "D Trends on Question Similarity and QUD Quality",
            "text": "In Figures 9a and 9b, we visualize the categorization of Answer Compatibility and Givenness for the 3 QUD parsers across 5 question similarity intervals. We can observe that for Answer Compatibility, instances with high similarity scores tend to mostly have Sa that explicitly answers Q\u0302. As the similarity score decreases, a greater percentage of instances are either Unfocused or Not answered. For ChatGPT however, across all similarity score intervals, the focus of Sa answers Q\u0302 most frequently, owing to the verbose nature of its generated questions which leak the answer concepts.\nA similar trend can be observed for Givenness, where the percentage of answer leakage and hallucination increases with the decrease in similarity score. However, across the 3 parsers, even the most similar (Q\u0302,Q) pairs show answer leakage. Since Givenness is also an evaluation of Sk\u0302, a plausible cause for such an observation is a bad k\u0302 prediction. To further investigate this, we analyze the instances with the highest similarity scores (between 4 and 5) with the parser\u2019s Sk\u0302 same as gold Sk\u0302, generated for the same Sa. It was observed that barring ChatGPT (which showed answer leakage for similarity score\n= 5), all instances were categorized as \u201cNo new concepts\u201d.\nThese observations indicate that the humanannotated in-context similarity scores somewhat correlate with both criteria experimented on here.\nStep 1. Question Generation Examples for this question generation are: Context: The stock market\u2019s woes spooked currency traders but prompted a quiet little party among bond investors. Prices of long-term Treasury bonds moved inversely to the stock market as investors sought safety amid growing evidence the economy is weakening. But the shaky economic outlook and the volatile stock market forced the dollar lower against major currencies. The bond market got an early boost from the opening-hour sell-off in stocks. That rout was triggered by UAL Corp.\u2019s announcement late Monday that the proposed management-labor buy-out had collapsed. The 80-point decline in the Dow Jones Industrial Average during the morning trading session touched off a flight to safety that saw investors shifting assets from stocks to Treasury bonds. Target Answer: At its strongest, the Treasury\u2019s benchmark 30-year bond rose more than a point, or more than $10 for each $1,000 face amount. Question: How much did the prices of long-term Treasury bonds increase? ...[3 more in-context examples] By reading the context, generate a question that indicates how the Target Answer elaborates on earlier sentences. The Target Answer given should be the answer to the generated question. The question should reflect the main purpose of the Target Answer. It should not use information first introduced in the Target Answer and shouldn\u2019t copy-paste phrases newly introduced in the Target Answer. Step 2. Anchor Selection Context: ...[same as above] Question: ...[same as above] Anchor Sentence: Prices of long-term Treasury bonds moved inversely to the stock market as investors sought safety amid growing evidence the economy is weakening ...[3 more in-context examples] By reading the Context, pick a sentence from the Context such that the above Question arises from it. An Anchor Sentence is a sentence from the Context that the Question is most related to. The words and concepts from the Anchor Sentence are used to generate the Question.The Target Answer cannot be the Anchor Sentence.\narticle: FORT LAUDERDALE, Fla. - Researchers are looking to the sun to give hunted and overfished sharks a new ray of hope. Using a special solar-powered tag, marine scientists now can study a shark\u2019s movements for up to two years by way of data beamed to satellites. Previously, researchers relied on tags that ran on batteries and sometimes died before all the information could be transmitted. The new tags are like a smartphone for marine animals,\u2019 said Marco Flagg, CEO of Desert Star, a Marina, Calif., company that offers solar devices.\u2019Just like smartphones, the tags have many sensors and communication capability.The Guy Harvey Research Institute, based in Dania Beach, Fla., is looking to use solar tags to track certain species of fierce fish, including tigers, makos, hammerheads, oceanic white tip and sand sharks.The goal is to better understand their migratory patterns and ultimately keep their population healthy.Sharks are critical to the overall balance of ocean ecosystems, but commercial fishermen catch them by the millions for their fins, cartilage and meat.\u2019We\u2019ve learned a lot from tagging sharks, not least of which is that they are highly migratory,\u2019 said Antonio Fins, executive director of the Guy Harvey Ocean Foundation, which supports the institute.\u2019They are not American sharks or Bahamian sharks or Mexican sharks.They don\u2019t know borders or nationalities. question: Why are researchers studying sharks and using solar-powered tags to track their movements? answer: Sharks are critical to the overall balance of ocean ecosystems, but commercial fishermen catch them by the millions for their fins, cartilage and meat. score: ... For the above article, give a score between 1 to 100 for how well the answer actually answers the question.\narticle: FORT LAUDERDALE, Fla. - Researchers are looking to the sun to give hunted and overfished sharks a new ray of hope. Using a special solar-powered tag, marine scientists now can study a shark\u2019s movements for up to two years by way of data beamed to satellites. Previously, researchers relied on tags that ran on batteries and sometimes died before all the information could be transmitted. The new tags are like a smartphone for marine animals,\u2019 said Marco Flagg, CEO of Desert Star, a Marina, Calif., company that offers solar devices.\u2019Just like smartphones, the tags have many sensors and communication capability.The Guy Harvey Research Institute, based in Dania Beach, Fla., is looking to use solar tags to track certain species of fierce fish, including tigers, makos, hammerheads, oceanic white tip and sand sharks.The goal is to better understand their migratory patterns and ultimately keep their population healthy.Sharks are critical to the overall balance of ocean ecosystems, but commercial fishermen catch them by the millions for their fins, cartilage and meat.\u2019We\u2019ve learned a lot from tagging sharks, not least of which is that they are highly migratory,\u2019 said Antonio Fins, executive director of the Guy Harvey Ocean Foundation, which supports the institute.\u2019They are not American sharks or Bahamian sharks or Mexican sharks.They don\u2019t know borders or nationalities. Which sentence in the article is closest to the sentence: \u2019Researchers are studying the movements of sharks using a special solar-powered tag that can transmit data to satellites for up to two years. \u2019 A:\nTable 10: GPT-Ans prompt for Answer Compatibility (with example article and GPT-4 generated answer)\nContext: The Justice Department is in the process of trying to gain control over a law that federal Judge David Sentelle recently called a \u2019monster.\u2019 Needless to say, he was talking about RICO.With its recently revised guidelines for RICO, Justice makes it clear that the law currently holds too many incentives for abuse by prosecutors.The text of the \u2019new policy\u2019 guidelines from the Criminal Division are reprinted nearby.They strongly suggest that Justice\u2019s prosecutions of Drexel Burnham Lambert, Michael Milken and Princeton/Newport violated notions of fundamental fairness.Justice is attempting to avoid a replay of these tactics.This amounts to an extraordinary repudiation of the tenure of New York mayoral candidate and former U.S. Attorney Rudolph Giuliani, who was more inclined to gathering scalps than understanding markets.The new guidelines limit the pretrial forfeitures of assets of RICOed defendants and their investors, clients, bankers and others.This follows earlier new guidelines from the Tax Division prohibiting Princeton/Newport-like tax cases from masquerading as RICO cases. Reference Question: What is the rationale for limiting the pretrial forfeitures? Candidate Question: In what way are forfeitures limited now? Score: ... Given the Context, score the above Candidate Question for similarity with respect to the Reference Question on a continuous scale from 1 to 5, where a score of 1 means \u2019no similarity\u2019 and a score of 5 means \u2019similar intent and phrasing\u2019\nTable 11: Prompt for assessing similarity between (Q\u0302,Q) (with example context, reference and candidate question)\nContext: 1 CHARLESTON, W.Va. - Downtown businesses and restaurants began to reopen after water was declared safe to drink in portions of West Virginia\u2019s capital, but life has yet to return to normal for most of the 300,000 people who haven\u2019t been able to use running water in the five days since a chemical spill. 2 It could still be days before everyone in the Charleston metropolitan area is cleared to use water, though officials say the water in certain designated areas was safe to drink and wash with as long as people flushed out their systems. 3 They cautioned that the water may still have a slight licorice-type odor, raising the anxieties of some who believed it was still contaminated. 4 \u2019I wouldn\u2019t drink it for a while. I\u2019m skeptical about it,\u2019 said Wanda Blake, a cashier in the electronics section of a Charleston Kmart who fears she was exposed to the tainted water before she got word of the spill. Question: How widespread were the effects of the spill? Answer: Thursday\u2019s spill affected 100,000 customers in a nine-county area, or about 300,000 people in all. Does the question contain new concepts that a reader would be hard to come up with? (By \"new concepts\", I mean concepts that cannot be easily inferred by world knowledge from existing ones). There are several possibilities here as well: This question does not contain new concepts. Answer leakage: The question contains new concepts that are in the answer sentence AND not in the context. Hallucination: The question contains new concepts. This includes: Concepts not in the article. The question contains new concepts that are not in the context, but can be found later in the document.\nGiven the Context, Question, and Answer, select one of the following options on the basis of your understanding of the instructions. 1: No new concepts 2: Answer leakage 3: Hallucination\nHere are a few examples for all cases: Example 1: Context: 1 U.S. exports of nuclear material cannot be adequately traced from country to country, according to a congressional report. Question: What does the report say is the reason for the export ban? Answer Sentence: The report says hundreds of tons of plutonium and highly enriched uranium have accumulated worldwide, mostly from nuclear power generation. Selected option: [3: Hallucination] ...[5 more in-context examples; each option has two examples]\nDoes the question contain new concepts that a reader would be hard to come up with? (By \"new concepts\", I mean concepts that cannot be easily inferred by world knowledge from existing ones). There are several possibilities here as well: This question does not contain new concepts. Answer leakage: The question contains new concepts that are in the answer sentence AND not in the context. Hallucination: The question contains new concepts. This includes: Concepts not in the article. The question contains new concepts that are not in the context, but can be found later in the document.\nGiven the Context, Question, and Answer, select one of the following options on the basis of your understanding of the instructions. 1: No new concepts 2: Answer leakage 3: Hallucination\nTable 13: GPT-Cls-fs prompt for Givenness\nQuestion: How widespread were the effects of the spill? Anchor Sentence: \u2019I know I\u2019ve ingested it\u2019. By Tuesday morning, officials had given the green light to about 35 percent of West Virginia American Water\u2019s customers. Does the question contain new concepts that a reader would be hard to come up with? (By \"new concepts\", I mean concepts that cannot be easily inferred by world knowledge from existing ones). There are several possibilities here as well: This question does not contain new concepts. Answer leakage: The question contains new concepts that are in the answer sentence AND not in the context. Hallucination: The question contains new concepts. This includes: Concepts not in the article. The question contains new concepts that are not in the context, but can be found later in the document.\nIs the question well-grounded in the anchor sentence? Please evaluate using the following scale: 1: The question is fully grounded in the anchor sentence. 2: Some parts of the question are grounded in the anchor sentence. 3: The question is not grounded at all in the anchor sentence.\nBased on the question and the anchor, please choose one of the above options. If the question refers to the same entity as the anchor, we consider the question to be grounded.\nHere are a few examples for all cases: Example 1: Question: What do lawmakers think about this issue? Anchor Sentence: U.S. exports of nuclear material cannot be adequately traced from country to country, according to a congressional report. Result: [1: The question is fully grounded in the anchor sentence.] ...[5 more in-context examples; each option has two examples]\nIs the question well-grounded in the anchor sentence? Please evaluate using the following scale:\n1: The question is fully grounded in the anchor sentence. 2: Some parts of the question are grounded in the anchor sentence. 3: The question is not grounded at all in the anchor sentence. Based on the question and the anchor, please choose one of the above options. If the question refers to the same entity as the anchor, we consider the question to be grounded.\nTable 15: GPT-Cls-fs prompt for Anchor Relevance\nQuestion: What do foreign policy experts say about the issue? Anchor Sentence: U.S. exports of nuclear material cannot be adequately traced from country to country, according to a congressional report. Based on the question and the anchor, give a score between 1 to 100 for how confident you are about the question is grounded in anchor sentence. If the question refers to the same entity as the anchor, we consider the question to be grounded.\nTable 16: GPT-Scr prompt for Anchor Relevance (with example question and answer sentence)\nQuestion\nWhat is the main objective of Clinton in forging a wedge between Milosevic and the Serbs in\nWhat will happen to owners who cannot distinguish their owners\u2019 voices?\nTable 17: Examples of questions that failed the Language Quality criterion.\nQuestion Answer\nWhat happened after he took the students to Poland? But he wasn\u2019t prepared for the uproar that followed.\nWhat was the reason for the scheduled resumption of peace talks in Ingushetia? The scheduled resumption of talks in the town of Sleptsovsk came two days after agreement on a limited cease-fire, calling for both sides to stop using heavy artillery Tuesday.\nTable 18: Examples where the answer sentence is an \u201cunfocused\u201d label for Answer Compatibility. In the first example, the answer does not directly talk about an event. In the second example, the answer is not the focus of the sentence. This is because the cease fire alone doesn\u2019t seem to be the reason for the resumption of peace talks\nAnchor (S1): SEATTLE - There\u2019s little lyrical language to be found in the most recent international report on climate change. Question: What is the essence of the IPCC report? Answer: So when a bad cold kept him in the house one weekend, Johnson decided to distill the report to its essence via a centuries-old Japanese art form: haiku.\nAnchor (S1): Fundamental freedoms are lagging behind rapid economic growth in Vietnam, according to a new U.N. report. Question What is the purpose of the U.N. Human Rights Ombudsman\u2019s visit? Answer The group visited Vietnam for one week in October last year.\nTable 19: Examples where Q\u0302 is labeled \u201challucination\u201d for Givenness. Both anchor sentences are identified to be the first sentence. In the first example, IPCC is not in the common ground (S1) nor in Sa, and inferring that the international report was written by IPCC is quite a leap; in the second sentence, \u201cU.N. Human Rights Ombudsman\u2019s\u201d is also unknown to the reader.\nAnswer Compatibility Annotation1: not answered, Annotation2: not answered, Annotation3: unfocused Anchor Sentence: But the technological sleuthing it took a group of Carnegie Mellon University students and alumni to recover and preserve some digital images apparently created and stored by Andy Warhol on old-school floppy computer disks nearly 30 years ago is a tale worth telling. Question: What was unique about these digital images? Answer Sentence: Those three images of an altered Botticelli\u2019s \u2019Venus,\u2019 a Warhol self-portrait, and a Campbell\u2019s soup can - of 28 that were found on the disks - were enough to excite Warhol fanatics around the world over the possibility that something - anything - new by the King of Pop Art had been revealed.\nAnchor Relevance Annotation1: fully grounded, Annotation2: partially grounded, Annotation3: partially grounded Anchor Sentence: Every spring, from Florida to New Jersey, crabs that look more like fossils than a postcard for passion make their way ashore by the thousands when the moon is bright to lay millions of eggs that provide critical food for migrating shorebirds. Question: What happened to their numbers? Answer Sentence: But in the 1990s, their numbers began falling.\nTable 20: Examples where annotators disagree. In the first example, the Answer Sentence mainly focuses on the excitement around Warhol fanatics on the revelation of the three digital images and what they were. Although it mentions that this maybe new work, suggesting it might be unique (quite subjective), it doesn\u2019t seem to be the main focus of the Answer Sentence, justifying the \u2019unfocused\u2019 annotation. For the second example, no concept in the question is discourse-new and wondering about what happened to the numbers is natural to some readers, justifying the \u2019fully grounded\u2019 annotation of one of the readers."
        }
    ],
    "title": "QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing",
    "year": 2023
}