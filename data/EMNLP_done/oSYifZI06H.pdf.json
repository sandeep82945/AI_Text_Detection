{
    "abstractText": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from wordbased LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Robin Algayres"
        },
        {
            "affiliations": [],
            "name": "Yossi Adi"
        },
        {
            "affiliations": [],
            "name": "Tu Anh Nguyen"
        },
        {
            "affiliations": [],
            "name": "Jade Copet"
        },
        {
            "affiliations": [],
            "name": "Gabriel Synnaeve"
        },
        {
            "affiliations": [],
            "name": "Benoit Sagot"
        },
        {
            "affiliations": [],
            "name": "Emmanuel Dupoux"
        }
    ],
    "id": "SP:bf8cd1668a8f531e373741ee67de050d4e5b32c0",
    "references": [
        {
            "authors": [
                "Robin Algayres",
                "Adel Nabli",
                "Beno\u00eet Sagot",
                "Emmanuel Dupoux"
            ],
            "title": "2022a. Speech sequence embeddings using nearest neighbors contrastive learning",
            "year": 2022
        },
        {
            "authors": [
                "Robin Algayres",
                "Tristan Ricoul",
                "Julien Karadayi",
                "Hugo Lauren\u00e7on",
                "Salah Zaiem",
                "Abdelrahman Mohamed",
                "Beno\u00eet Sagot",
                "Emmanuel Dupoux"
            ],
            "title": "2022b. Dpparse: Finding word boundaries from raw speech with an instance lexicon",
            "year": 2022
        },
        {
            "authors": [
                "Alexei Baevski",
                "Steffen Schneider",
                "Michael Auli."
            ],
            "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
            "venue": "CoRR, abs/1910.05453.",
            "year": 2019
        },
        {
            "authors": [
                "Alexei Baevski",
                "Henry Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations. CoRR, abs/2006.11477",
            "year": 2020
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability,",
            "year": 2021
        },
        {
            "authors": [
                "Chris Biemann."
            ],
            "title": "Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems",
            "venue": "Proceedings of TextGraphs: the First Workshop on Graph Based Methods for Natural Language Processing, pages 73\u2013",
            "year": 2006
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Armand Joulin",
                "Tom\u00e1s Mikolov."
            ],
            "title": "Alternative structures for character-level rnns",
            "venue": "CoRR, abs/1511.06303.",
            "year": 2015
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Olivier Teboul",
                "David Grangier",
                "Marco Tagliasacchi",
                "Neil Zeghidour."
            ],
            "title": "Audiolm: a language modeling approach to audio generation",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Peter F. Brown",
                "Vincent J. Della Pietra",
                "Peter V. deSouza",
                "Jenifer C. Lai",
                "Robert L. Mercer."
            ],
            "title": "Class-based n-gram models of natural language",
            "venue": "Computational Linguistics, 18(4):467\u2013480.",
            "year": 1992
        },
        {
            "authors": [
                "Mingjie Chen",
                "Thomas Hain."
            ],
            "title": "Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders",
            "venue": "Proc. Interspeech 2020, pages 4866\u20134870.",
            "year": 2020
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Yu-An Chung",
                "James R. Glass."
            ],
            "title": "Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech",
            "venue": "CoRR, abs/1803.08976.",
            "year": 2018
        },
        {
            "authors": [
                "Ewan Dunbar",
                "Robin Algayres",
                "Julien Karadayi",
                "Mathieu Bernard",
                "Juan Benjumea",
                "Xuan-Nga Cao",
                "Lucie Miskic",
                "Charlotte Dugrain",
                "Lucas Ondel",
                "Alan W. Black",
                "Laurent Besacier",
                "Sakriani Sakti",
                "Emmanuel Dupoux"
            ],
            "title": "The zero resource",
            "year": 2019
        },
        {
            "authors": [
                "Ewan Dunbar",
                "Nicolas Hamilakis",
                "Emmanuel Dupoux."
            ],
            "title": "Self-supervised language learning from raw audio: Lessons from the zero resource speech challenge",
            "venue": "IEEE Journal of Selected Topics in Signal Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Ewan Dunbar",
                "Nicolas Hamilakis",
                "Emmanuel Dupoux."
            ],
            "title": "Self-supervised language learning from raw audio: Lessons from the zero resource speech challenge",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, 16(6):1211\u20131226.",
            "year": 2022
        },
        {
            "authors": [
                "Ali Elkahky",
                "Wei-Ning Hsu",
                "Paden Tomasello",
                "Tu-Anh Nguyen",
                "Robin Algayres",
                "Yossi Adi",
                "Jade Copet",
                "Emmanuel Dupoux",
                "Abdelrahman Mohamed"
            ],
            "title": "Do coarser units benefit cluster predictionbased speech pre-training",
            "year": 2023
        },
        {
            "authors": [
                "Siyuan Feng",
                "Tan Lee",
                "Zhiyuan Peng."
            ],
            "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
            "venue": "Proc. Interspeech 2019, pages 1093\u2013 1097.",
            "year": 2019
        },
        {
            "authors": [
                "Philip Gage."
            ],
            "title": "A new algorithm for data compression",
            "venue": "The C Users Journal archive, 12:23\u201338.",
            "year": 1994
        },
        {
            "authors": [
                "Sharon Goldwater",
                "Thomas Griffiths",
                "Mark Johnson."
            ],
            "title": "A bayesian framework for word segmentation: Exploring the effects of context",
            "venue": "Cognition, 112:21\u2013",
            "year": 2009
        },
        {
            "authors": [
                "Alex Graves"
            ],
            "title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850",
            "year": 2013
        },
        {
            "authors": [
                "Michael Gutmann",
                "Aapo Hyv\u00e4rinen."
            ],
            "title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceed-",
            "year": 2010
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed."
            ],
            "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "CoRR, abs/2106.07447.",
            "year": 2021
        },
        {
            "authors": [
                "Rongjie Huang",
                "Max W.Y. Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu",
                "Yi Ren",
                "Zhou Zhao"
            ],
            "title": "Fastdiff: A fast conditional diffusion model for high-quality speech synthesis",
            "year": 2022
        },
        {
            "authors": [
                "Keith Ito",
                "Linda Johnson"
            ],
            "title": "The lj speech dataset",
            "year": 2017
        },
        {
            "authors": [
                "Christiaan Jacobs",
                "Yevgen Matusevych",
                "Herman Kamper."
            ],
            "title": "Acoustic word embeddings for zeroresource languages using self-supervised contrastive learning and multilingual adaptation",
            "venue": "2021 IEEE Spoken Language Technology Workshop (SLT), pages",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "CoRR, abs/1702.08734.",
            "year": 2017
        },
        {
            "authors": [
                "hamed",
                "Emmanuel Dupoux"
            ],
            "title": "Libri-light: A benchmark for ASR with limited or no supervision. CoRR, abs/1912.07875",
            "year": 2019
        },
        {
            "authors": [
                "Herman Kamper."
            ],
            "title": "Truly unsupervised acoustic word embeddings using weak top-down constraints in encoder-decoder models",
            "venue": "CoRR, abs/1811.00403.",
            "year": 2018
        },
        {
            "authors": [
                "Herman Kamper"
            ],
            "title": "Word segmentation on discovered phone units with dynamic programming and self-supervised scoring",
            "year": 2022
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Jade Copet",
                "Kushal Lakhotia",
                "Tu Anh Nguyen",
                "Paden Tomasello",
                "Ann Lee",
                "Ali Elkahky",
                "Wei-Ning Hsu",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux",
                "Yossi Adi"
            ],
            "title": "textlesslib: a library for textless spoken language processing",
            "year": 2022
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Ann Lee",
                "Adam Polyak",
                "Yossi Adi",
                "Jade Copet",
                "Kushal Lakhotia",
                "Tu Anh Nguyen",
                "Morgane Rivi\u00e8re",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux",
                "Wei-Ning Hsu"
            ],
            "title": "2021a. Text-free prosody-aware generative spoken language modeling",
            "year": 2021
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Ann Lee",
                "Adam Polyak",
                "Yossi Adi",
                "Jade Copet",
                "Kushal Lakhotia",
                "Tu-Anh Nguyen",
                "Morgane Rivi\u00e8re",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "2021b. Text-free prosody-aware generative spoken language modeling",
            "year": 2021
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae."
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "CoRR, abs/2010.05646.",
            "year": 2020
        },
        {
            "authors": [
                "Vijay Korthikanti",
                "Jared Casper",
                "Sangkug Lym",
                "Lawrence McAfee",
                "Michael Andersch",
                "Mohammad Shoeybi",
                "Bryan Catanzaro"
            ],
            "title": "Reducing activation recomputation in large transformer models",
            "year": 2022
        },
        {
            "authors": [
                "Felix Kreuk",
                "Joseph Keshet",
                "Yossi Adi."
            ],
            "title": "Self-supervised contrastive learning for unsupervised phoneme segmentation",
            "venue": "arXiv preprint arXiv:2007.13465.",
            "year": 2020
        },
        {
            "authors": [
                "Dupoux",
                "Yossi Adi."
            ],
            "title": "Textless speech emotion conversion using decomposed and discrete representations",
            "venue": "arXiv preprint arXiv:2111.07402.",
            "year": 2021
        },
        {
            "authors": [
                "Taku Kudo."
            ],
            "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375,",
            "year": 2018
        },
        {
            "authors": [
                "Brian Kulis",
                "Michael I. Jordan."
            ],
            "title": "Revisiting kmeans: New algorithms via bayesian nonparametrics",
            "venue": "CoRR, abs/1111.0352.",
            "year": 2011
        },
        {
            "authors": [
                "Kundan Kumar",
                "Rithesh Kumar",
                "Thibault de Boissiere",
                "Lucas Gestin",
                "Wei Zhen Teoh",
                "Jose Sotelo",
                "Alexandre de Brebisson",
                "Yoshua Bengio",
                "Aaron Courville"
            ],
            "title": "Melgan: Generative adversarial networks for conditional waveform synthesis",
            "year": 2019
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Evgeny Kharitonov",
                "Wei-Ning Hsu",
                "Yossi Adi",
                "Adam Polyak",
                "Benjamin Bolte",
                "Tu Anh Nguyen",
                "Jade Copet",
                "Alexei Baevski",
                "Adelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "Generative spoken language modeling from raw audio",
            "year": 2021
        },
        {
            "authors": [
                "Andy T. Liu",
                "Po chun Hsu",
                "Hung-Yi Lee."
            ],
            "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
            "venue": "Proc. Interspeech 2019, pages 1108\u20131112.",
            "year": 2019
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "year": 2013
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Anoop Deoras",
                "Hai Son Le",
                "Stefan Kombrink",
                "Jan Honzaernocky"
            ],
            "title": "Subword language modeling with neural networks",
            "year": 2011
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Maureen de Seyssel",
                "Robin Algayres",
                "Patricia Roze",
                "Ewan Dunbar",
                "Emmanuel Dupoux"
            ],
            "title": "Are word boundaries useful for unsupervised language learning",
            "year": 2022
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Maureen de Seyssel",
                "Patricia Roz\u00e9",
                "Morgane Rivi\u00e8re",
                "Evgeny Kharitonov",
                "Alexei Baevski",
                "Ewan Dunbar",
                "Emmanuel Dupoux"
            ],
            "title": "The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken lan",
            "year": 2020
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Eugene Kharitonov",
                "Jade Copet",
                "Yossi Adi",
                "Wei-Ning Hsu",
                "Ali Elkahky",
                "Paden Tomasello",
                "Robin Algayres",
                "Benoit Sagot",
                "Abdelrahman Mohamed"
            ],
            "title": "Generative spoken dialogue language modeling",
            "venue": "arXiv preprint arXiv:2203.16502",
            "year": 2022
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Benoit Sagot",
                "Emmanuel Dupoux"
            ],
            "title": "Are discrete units necessary for spoken language modeling",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zizheng Pan",
                "Bohan Zhuang",
                "Jing Liu",
                "Haoyu He",
                "Jianfei Cai"
            ],
            "title": "Scalable vision transformers with hierarchical pooling",
            "year": 2021
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Librispeech: An asr corpus based on public domain audio books",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206\u20135210.",
            "year": 2015
        },
        {
            "authors": [
                "Puyuan Peng",
                "David Harwath"
            ],
            "title": "Word discovery in visually grounded, self-supervised speech models",
            "year": 2023
        },
        {
            "authors": [
                "Puyuan Peng",
                "Herman Kamper",
                "Karen Livescu"
            ],
            "title": "A correspondence variational autoencoder for unsupervised acoustic word embeddings",
            "year": 2020
        },
        {
            "authors": [
                "Wei Ping",
                "Kainan Peng",
                "Andrew Gibiansky",
                "Sercan Arik",
                "Ajay Kannan",
                "Sharan Narang",
                "Jonathan Raiman",
                "John Miller"
            ],
            "title": "Deep voice 3: 2000-speaker neural text-to-speech",
            "year": 2017
        },
        {
            "authors": [
                "Mark A. Pitt",
                "Keith Johnson",
                "Elizabeth Hume",
                "Scott Kiesling",
                "William Raymond."
            ],
            "title": "The buckeye corpus of conversational speech: labeling conventions and a test of transcriber reliability",
            "venue": "Speech Communication, 45(1):89\u201395.",
            "year": 2005
        },
        {
            "authors": [
                "Adam Polyak",
                "Yossi Adi",
                "Jade Copet",
                "Eugene Kharitonov",
                "Kushal Lakhotia",
                "Wei-Ning Hsu",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux."
            ],
            "title": "Speech resynthesis from discrete disentangled selfsupervised representations",
            "venue": "CoRR, abs/2104.00355.",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Prenger",
                "Rafael Valle",
                "Bryan Catanzaro."
            ],
            "title": "Waveglow: A flow-based generative network for speech synthesis",
            "venue": "CoRR, abs/1811.00002.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Tao Xu",
                "Greg Brockman",
                "Christine McLeavey",
                "Ilya Sutskever."
            ],
            "title": "Robust speech recognition via large-scale weak supervision",
            "venue": "Technical report, Tech. Rep., OpenAI.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Okko Johannes R\u00e4s\u00e4nen",
                "Gabriel Doyle",
                "Michael C. Frank."
            ],
            "title": "Pre-linguistic segmentation of speech into syllable-like units",
            "venue": "Cognition, 171:130\u2013150.",
            "year": 2018
        },
        {
            "authors": [
                "Fl\u00e1vio Ribeiro",
                "Dinei Flor\u00eancio",
                "Cha Zhang",
                "Michael Seltzer."
            ],
            "title": "Crowdmos: An approach for crowdsourcing mean opinion score studies",
            "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2416\u20132419.",
            "year": 2011
        },
        {
            "authors": [
                "Morgane Riviere",
                "Jade Copet",
                "Gabriel Synnaeve"
            ],
            "title": "Asr4real: An extended benchmark for speech models",
            "year": 2021
        },
        {
            "authors": [
                "Morgane Rivi\u00e8re",
                "Emmanuel Dupoux."
            ],
            "title": "Towards unsupervised learning of speech features in the wild",
            "venue": "2021 IEEE Spoken Language Technology Workshop (SLT), pages 156\u2013163.",
            "year": 2021
        },
        {
            "authors": [
                "Shane Settle",
                "Karen Livescu."
            ],
            "title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches",
            "venue": "CoRR, abs/1611.02550.",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "Rj Skerrv-Ryan",
                "Rif A. Saurous",
                "Yannis Agiomvrgiannakis",
                "Yonghui Wu"
            ],
            "title": "Natural tts synthesis",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Rif A. Saurous",
                "Yannis Agiomyrgiannakis",
                "Yonghui Wu"
            ],
            "title": "Natural tts synthesis",
            "year": 2017
        },
        {
            "authors": [
                "Alexis Thual",
                "Corentin Dancette",
                "Julien Karadayi",
                "Juan Benjumea",
                "Emmanuel Dupoux."
            ],
            "title": "A Knearest neighbours approach to unsupervised spoken term discovery",
            "venue": "IEEE Spoken Language Technology SLT-2018, Proceedings of SLT 2018, Ath\u00e8nes,",
            "year": 2018
        },
        {
            "authors": [
                "Andros Tjandra",
                "Berrak Sisman",
                "Mingyang Zhang",
                "Sakriani Sakti",
                "Haizhou Li",
                "Satoshi Nakamura."
            ],
            "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
            "venue": "Proc. Interspeech 2019, pages 1118\u2013",
            "year": 2019
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew W. Senior",
                "Koray Kavukcuoglu."
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "CoRR, abs/1609.03499.",
            "year": 2016
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "CoRR, abs/1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Lisa Van Staden",
                "Herman Kamper"
            ],
            "title": "A comparison of self-supervised speech representations as input features for unsupervised acoustic word embeddings",
            "year": 2020
        },
        {
            "authors": [
                "A. Vasuki",
                "P.T. Vanathi."
            ],
            "title": "A review of vector quantization techniques",
            "venue": "IEEE Potentials, 25(4):39\u2013",
            "year": 2006
        },
        {
            "authors": [
                "Maarten Versteegh",
                "Xavier Anguera",
                "Aren Jansen",
                "Emmanuel Dupoux."
            ],
            "title": "The zero resource speech challenge 2015: Proposed approaches and results",
            "venue": "Procedia Computer Science, 81:67\u201372. SLTU-2016 5th Workshop on Spoken Language Technologies",
            "year": 2016
        },
        {
            "authors": [
                "Junjie Wu."
            ],
            "title": "The Uniform Effect of K-means Clustering, pages 17\u201335",
            "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg.",
            "year": 2012
        },
        {
            "authors": [
                "Li",
                "Shinji Watanabe",
                "Abdelrahman Mohamed",
                "Hung yi Lee."
            ],
            "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
            "venue": "Proc. Interspeech 2021, pages 1194\u20131198.",
            "year": 2021
        },
        {
            "authors": [
                "Neil Zeghidour",
                "Alejandro Luebs",
                "Ahmed Omran",
                "Jan Skoglund",
                "Marco Tagliasacchi."
            ],
            "title": "Soundstream: An end-to-end neural audio codec",
            "venue": "CoRR, abs/2107.03312.",
            "year": 2021
        },
        {
            "authors": [
                "Yaoming Zhu",
                "Sidi Lu",
                "Lei Zheng",
                "Jiaxian Guo",
                "Weinan Zhang",
                "Jun Wang",
                "Yong Yu."
            ],
            "title": "Texygen: A benchmarking platform for text generation models",
            "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval,",
            "year": 2018
        },
        {
            "authors": [
                "George Kingsley Zipf."
            ],
            "title": "Human behavior and the principle of least effort",
            "venue": "Addison-Wesley Press.",
            "year": 1949
        },
        {
            "authors": [
                "Algayres"
            ],
            "title": "Wav2vec2.0 Base is a stack of 7 convolution layers and 12 transformer layers. The SSE is composed of a one GLU convolution layer (kernel size: 4, number of channels: 512, stride: 1), a transformer layer (attention",
            "year": 2022
        },
        {
            "authors": [
                "Lakhotia"
            ],
            "title": "2022) was trained on LJ",
            "year": 2022
        },
        {
            "authors": [
                "Jordan",
                "Brown (Brown"
            ],
            "title": "1992)), require in input either this unknown number of clusters or a threshold parameter that controls the number of clusters9",
            "venue": "By misestimating the number of clusters,",
            "year": 1992
        },
        {
            "authors": [
                "downstream LM"
            ],
            "title": "The second problem is the highly skewed distribution of the frequency of word types. This phenomenon, known as Zipf\u2019s Law (Zipf, 1949), states that, in text, there is a linear relation between the log-rank of word types",
            "year": 1949
        },
        {
            "authors": [
                "Algayres"
            ],
            "title": "2022a). Then, we clustered with k-means and hierarchicalk-means all the SSEs in the LibriSpeech into K classes. To simplify, we set K to the true number of word",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent work has opened up the possibility of learning generative language models directly from the raw audio signals, without using either text or Automatic Speech Recognition (ASR) (Lakhotia et al., 2021; Kharitonov et al., 2021b; Nguyen et al., 2022b; Borsos et al., 2022). The basic idea of these model is to rely on traditional text-based language models (LM), but replace the text input with some other discrete tokens directly learned from audio in an unsupervised fashion. The advantage of learning units from speech instead of relying on ASR is that this procedure can capture non-verbal vocalizations (like laughter) or intonation and rhythm, which are typically not transcribed, resulting in more expressive generations (Kreuk et al., 2021;\n1Audio examples are available at our website.\nKharitonov et al., 2021b). In addition, ASR may not be available in many languages that have insufficient textual resources and can make errors, which may then perturb the learning of the LM.\nThe problem of using self-discovered units, however, is that these units are typically very small, in fact, usually smaller than phonemes (Lakhotia et al., 2021; Borsos et al., 2022). We think that increasing the size of the units will favourably impact the semantic capabilities of a downstream spoken LM. This intuition comes from the NLP literature. Among others, Graves (2013); Mikolov et al. (2011); Bojanowski et al. (2015); Nguyen et al. (2022a) have shown a performance gap between character-based LM and word-based LM. The main reason is that at the level of characters, it is difficult for a text LM to extract long-range syntactic and semantic relationships. This is one of the reasons why recent state-of-the-art text-based LM (Radford et al., 2019) typically use a tokenizer representing word or subword units (Byte Pair Encoding (Gage, 1994), WordPiece (Wu et al., 2016), Unigram (Kudo, 2018)). Another advantage of large units is to save GPU memory at training time that enables to use both larger batches and longer sequences.\nIn speech, building the equivalent of a text-based tokenizer is hampered by two difficulties. First, the boundary problem is that contrary to text in most orthographic systems, speech does not have spaces and punctuation to delimit between word units. Finding word boundaries from raw audio is itself a difficult challenge (Dunbar et al., 2022a). Second, the clustering problem, is that even if boundaries were available, the clustering of speech fragments is challenging because the same word may surface in a variety of forms depending on speaker, accent, speech rate, etc. This problem may be even more difficult to solve than the first one (Dunbar et al., 2022a) because of the highly skewed distribution of word frequencies (Algayres et al., 2022b). Here,\nwe investigate the possibility of building a continuous tokenizer that sidesteps these two problems by using tokens that have neither perfect boundaries nor require a clustering step. In Appendix B, we explain in more detail why we wish to avoid the clustering of speech fragments and what methods have been applied to tackle this problem so far.\nHaving a continuous tokenizer instead of a discrete one results in drastic changes from the point of view of the downstream LM. With a discrete tokenizer, one can define a finite list of tokens over which the LM can learn a lookup embedding table at the input of the model and use a softmax layer at the output of the model. The softmax is used in training mode to compute the loss function through a cross-entropy with the target token and at inference time to sample sentences. With continuous representations, the list of tokens is unbounded, making these computations intractable. We tackle this problem with a Lexical Embedder, a semi-learnable function that maps continuous tokens to a practically infinite list of embeddings.\nThe key question addressed in this paper is whether it is possible to generate speech using large (word-size) continuous units instead of short discrete ones. Our major technical contribution is to replace the three standard elements of a text-based LM (lookup table, cross-entropy loss function, multinomial sampling) with elements adapted to a virtually infinite list of continuous embeddings. We show that with these changes, it is possible to generate speech of the same quality as discrete unit models. This is interesting because our units are 200ms long which amounts to a 5-time memory reduction compared to regular discrete units (Lakhotia et al., 2021; Borsos et al., 2022), opening up the possibility to train spoken LMs on longer speech sequences. In addition, our model builds interpretable representations thanks to the Lexical Embedder which learns a mapping between an acoustic space, with phonetic properties, to a lexical space, with semantic and syntactic properties. We call the resulting model tGSLM (token-based GSLM)."
        },
        {
            "heading": "2 Related work",
            "text": "Unsupervised speech representations like CPC, Wav2vec2.0 and HuBERT (van den Oord et al., 2018; Baevski et al., 2020; Hsu et al., 2021) are fixed-size representations (10 to 20ms long) that\noutperform traditional features, like mel-filterbanks and MFCCs, in many applications (Yang et al., 2021). In parallel to these works, there is a growing literature on variable-length acoustic encoding called speech sequence embeddings (SSE) (Peng et al., 2020; Algayres et al., 2022a; Jacobs et al., 2021; Kamper, 2018; Settle and Livescu, 2016). SSE models take a sequence of speech of any length and return a fixed-size vector. These models encode speech by maximizing phonetic information while minimizing speaker identity and recording conditions. SSEs are used for spoken term discovery (Thual et al., 2018), speech segmentation into phones or words (Kamper, 2022; Algayres et al., 2022b) but also as input to a BERT model (Algayres et al., 2022b) for spoken language modelling.\nSpeech generation is often performed with a neural vocoder conditioned on mel-filterbanks (van den Oord et al., 2016; Kumar et al., 2019; Kong et al., 2020; Prenger et al., 2018). In a text-to-speech pipeline, the mel-filterbanks are obtained with another neural network, which is conditioned on text (Ping et al., 2017; Shen et al., 2018). In the next step, the mel-filterbanks are decoded into natural-sounding speech by a neural vocoder (van den Oord et al., 2016; Kumar et al., 2019; Kong et al., 2020; Prenger et al., 2018). For the Zerospeech Challenge 2019, Dunbar et al. (2019) proposed to remove text and replace it with unsupervised discrete units. This challenge has fueled a large body of works on learning low bitrate speech representations for speech compression, voice conversion and spoken language modelling (Chen and Hain, 2020; Liu et al., 2019; Feng et al., 2019; Baevski et al., 2019; Tjandra et al., 2019; Kharitonov et al., 2021b; Lakhotia et al., 2021; Nguyen et al., 2020). For evaluation, the Zero-Resource challenge used bitrate and human evaluation.\nSpoken Language Model are neural networks trained to predict missing parts of a spoken sentence with predictive or contrastive losses. GSLM (Lakhotia et al., 2021) is the first spoken LM able to generate expressive and consistent spoken sentences in a pure textless fashion. It uses a causal transformer LM trained with NLL loss on sequences of discrete units obtained with a kmeans clustering (with k=100) of HuBERT frames. Once trained, GSLM can generate a sequence of discrete units by multinomial sampling that is de-\ncoded into speech with a separate vocoder. Specifically, the sampled HuBERT units are mapped to mel-filterbanks with Tacotron2.0 and decoded into speech with WaveGlow (Prenger et al., 2018), a neural vocoder. Lakhotia et al. (2021) also provides a way to evaluate their spoken LM using an ASR to transcribe their spoken generations and an external LM to compute the perplexity of the resulting transcriptions. In addition, the Zerospeech Challenge 2021 (Nguyen et al., 2020) designed a set of zero-shot metrics to probe what spoken LMs learn. A recent paper (Borsos et al., 2022), audioLM, came to our attention, which we did not have the time to include in our experiments. AudioLM works similarly to GSLM yet with the ability to generate speech that preserves the identity of the speaker. In another line of work, Algayres et al. (2022b) trained a BERT model with a contrastive loss function on sentences represented as a series of SSEs. They showed the resulting BERT is able to model semantics and syntax. This work suggests that discrete tokenizers and the NLL loss are not necessary to tackle language modelling on speech. We take inspiration on their work to design our approach."
        },
        {
            "heading": "3 Approach",
            "text": ""
        },
        {
            "heading": "3.1 tGSLM: training",
            "text": "The general structure of tGSLM is presented in Figure 1. It is composed of an encoder which segments the input speech into sequences of possibly varying size, and computes a fixed-sized Speech Sequence Embedding (SSE), which we call acoustic tokens (Section 3.1.1). These tokens are turned into lexical tokens through a learnable Lexical Embedder (Section 3.1.2), and fed into a causal Language Model that has been modified to deal with continuous inputs (Section 3.1.3)."
        },
        {
            "heading": "3.1.1 Acoustic tokens",
            "text": "In Figure 1, a speech sequence, S, is turned into n acoustic tokens, (a0, ..., an), after applying speech segmentation and an SSE model.\nSpeech segmentation consists in finding word boundaries in a speech sentence (Algayres et al., 2022b; Kamper, 2022; Kreuk et al., 2020). In this work, we rely on a naive method by placing a boundary every 200 ms, regardless of the content of the speech signal. In the Appendix A.1, we show that this method leads to better results than recent, more complex speech segmentation systems.\nThe acoustic tokens (ai)i\u2264n are built by first encoding the speech sentence S into a series of n\u2032 frames (fi)i\u2264n\u2032 with the 8th layer of Wav2vec2.0 Base from Baevski et al. (2020). For any two boundaries (k, l), ai = SSE([fk, ..., fl]) where SSE is a self-supervised system from Algayres et al. (2022a) trained with contrastive learning. This model has state-of-the-art performances on phonetic representation of pre-segmented words as measured by the Mean-Average-Precision metric. The acoustic tokens are extracted in a preprocessing step and stored before the training of the subsequent LM."
        },
        {
            "heading": "3.1.2 Lexical tokens",
            "text": "In a text-based transformer LM, there is often an embedding lookup table before the transformer, that has the size of the vocabulary and that maps discrete word tokens to lexical tokens (Vaswani et al., 2017). These lexical tokens, also known as word embeddings (Mikolov et al., 2013), learn during training semantic and syntactic properties that have been studied extensively in the NLP literature. In our case, the situation is different. First, instead of discrete word tokens, our LM takes as input continuous acoustic tokens which latent vo-\ncabulary size is unknown. Second, the mapping between acoustic and lexical space cannot be linear, as two speech segments may sound the same, i.e. be close in the acoustic space, while being semantically/syntactically different, i.e. far in the lexical space. This highly non-linear function between acoustic and lexical space is learned by our lexical embedder: LexEmb = L \u25e6 q function. L is a stack of non-linear fully connected layers learned jointly with the LM. q is an information bottleneck quantization function that we had to introduce to minimize the presence of low-level non-linguistic acoustic information. For a speech sequence S composed of n acoustic tokens (ai)i\u2264n, we note the sequence of lexical tokens (li)i\u2264n such as \u2200i \u2264 n, li = LexEmb(ai).\nTo understand why we need q, we have to go back to the LexEmb function input: the acoustic tokens. The acoustic tokens are derived from Wav2vec2.0, which is a transformer architecture whose attention mechanism covers the whole sentence. Each wav2vec2 frame, therefore, contains potential information about relative positions (through the transformer\u2019s positional embeddings), adjacent acoustic materials (through self-attention) or global properties like speaker. What we\u2019ve found in preliminary experiments is that this information may leak into the acoustic tokens and be amplified by the prediction or contrastive loss of the downstream causal LM. Fortunately, it turns out that this information has low variance and can be partially removed by slightly degrading the quality of the acoustic tokens. The degradation of the acoustic tokens is the role of the function q. q is composed of a PCA reduction and a quantization step that we call d-k-means, which stands for perdimension k-means. Specifically, given a speech database that has been segmented and encoded into N acoustic tokens, (ai)i\u2264N , we reduce their dimensions to d with a PCA. Then, we train d different k-means, one for each dimension of the PCA. In other words, for each j \u2264 d, we train a k-means on (PCA(ai)[j])i\u2264N . We chose the number of centroids per k-means to be proportional to the explained variance of each of the PCA dimensions. Once the k-means are trained, each dimension of each acoustic token is mapped to its cluster id. Finally, the cluster ids are turned into one-hot vectors and concatenated into one vector (see Appendix A.2 for more detailed explanations). d-k-means is inspired from multi-stage vector quantizer (VQ)\n(Vasuki and Vanathi, 2006) where several VQ codebooks are learned in parallel as in Baevski et al. (2020); Zeghidour et al. (2021). The PCA and the d-k-means are trained over the whole training set as a preprocessing step, before the transformer LM. We ablate the use of q in Appendix A.2 and show that it is necessary for the LM to generate sentences2."
        },
        {
            "heading": "3.1.3 Causal language model",
            "text": "The LM is a standard causal transformer with two modifications: the loss function and the prediction heads. First, in a standard LM, the number of possible types is fixed beforehand and remains tractable even for a very large corpus (10k to 100k). Here, because the number of different lexical tokens is virtually infinite, we cannot use a standard softmax and cross-entropy loss. We first tried a simple L2 reconstruction loss with an additional decoder but it did not work for us in practice. Instead, we use a contrastive loss: the Noice Contrastive Estimation (NCE) loss (Gutmann and Hyv\u00e4rinen, 2010). This loss works by maximizing the similarity between a pair of positive samples while minimizing the similarity between the positive samples and various negative samples. However, even though the SSE model from Algayres et al. (2022a) has learned to be speaker invariant, there is still a lot of speaker-related information encoded into the acoustic tokens. This is a problem already encountered in Algayres et al. (2022a); van den Oord et al. (2018) that is dealt with by sampling the negative tokens from the same speaker as the positive tokens.\nSecond, in a standard LM, the output head typically predicts the next word. However, in the case of speech, the boundary between individual phonemes is blurred by coarticulation. It is therefore easy to predict the next word by just attending to very local acoustic information at the end of the last word (something impossible to do with characters which are sequentially disentangled). We, therefore, introduce three prediction heads (three linear fully connected layers: h1,h2,h3) which do not only predict the first next token, but also the second and third as they cannot be co-articulated with the last token encoded by the LM. These prediction layers are trained jointly with the LM. We\n2Due to this quantization step, the resulting vectors (PCA+ d-k-means) could in principle be mapped to a finite dictionary of tokens, but, in practice, there is little or no collision and the number of classes remains identical to the number of tokens, i.e., way too high to apply a softmax.\njustify the choice of three prediction heads with a grid-search available in Appendix Table 5."
        },
        {
            "heading": "3.2 tGSLM: generation",
            "text": "Once tGSLM training is done, we use it to generate spoken sentences. We do that in two steps: we generate a sequence of acoustic tokens (Section 3.2.1) and then decode this sequence into speech (Section 3.2.2)."
        },
        {
            "heading": "3.2.1 Sampling",
            "text": "To generate a spoken sentence, we take inspiration of the popular top-k sampling method used in NLP to generate text sentences. This method requires sampling series of word tokens by sampling among the most probable word types. In our case, we do not have access to types so we are going to sample among the most probable lexical tokens. Our sampling method is summarized in Figure 2. We start by collecting a few dozen hours of speech that have not been seen during tGSLM training. The utterances are segmented and encoded into N speech segments and stored in their acoustic and lexical forms: (ai, li)i\u2264N . Using the FAISS library (Johnson et al., 2017), we index (li)i\u2264N into a k-NN graph called the lexical space. Given a prompt of t acoustic tokens (a0, ..., at), we do a forward pass into tGSLM. Then, we compute the cosine similarity of h1 output and its k closest\nneighbours in the lexical space. We apply a softmax on the vector of cosine similarities and treat it as a multinomial distribution to sample one element: lt+1. The softmax function contains a temperature parameter that controls the range of the sampling area. The acoustic tokens at+1 that correspond lt+1 is retrieved from the stored database and appended to (a0, ..., at). Once the desired length is reached, the sequence of acoustic tokens is decoded into a spoken sentence as explained in the next section."
        },
        {
            "heading": "3.2.2 Speech generation",
            "text": "Lakhotia et al. (2021); Kharitonov et al. (2022) trained a Tacotron2.0 decoder (Shen et al., 2018) to map deduplicated HuBERT units into mel filterbanks. Then, speech is generated from the mel filterbanks by a WaveGlow vocoder (Prenger et al., 2018). In order to make use of this pretrained Tacotron2.0 decoder, we trained an encoderdecoder transformer model to map series of acoustic tokens to series of HuBERT units. During training, the encoder computes an attention over a series of acoustic tokens while the decoder predicts HuBERT units auto-regressively. At inference, given a series of acoustic tokens, a corresponding sequence of HuBERT units is obtained by taking the argmax of the decoder softmax function. Finally, the HuBERT units are given as input to the pre-trained Tacotron2.0 to be decoded into spoken utterances."
        },
        {
            "heading": "4 Evaluation and datasets",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and settings",
            "text": "LJ Speech (LJ), LibriSpeech (LS), Libri-light 6k clean (LL6k-clean) are three corpora of studio recordings of read English of respectively 24, 1k and 6k hours (Ito and Johnson, 2017; Panayotov et al., 2015; Rivi\u00e8re and Dupoux, 2021). These corpora are used to train the different parts of the pipeline. The training details and specific model architectures can be found in Appendix Section A.3."
        },
        {
            "heading": "4.2 Generation metrics",
            "text": "Perplexity (PPX) is a text-based metrics used by Lakhotia et al. (2021) to evaluate the overall quality of generated spoken sentences. The authors propose to transcribe the spoken generations with an external ASR system and to compute the mean perplexity score over batches of transcribed speech\nwith an external transformer LM3. The spoken generation process is guided by a temperature parameter that controls how diverse generated sentences are. The diversity of a batch of sentences can be computed as in Lakhotia et al. (2021) with the VERT score that is an average of self-BLEU (Zhu et al., 2018) and auto-BLEU (Lakhotia et al., 2021) scores. Typically, low temperatures produce high diversity and low perplexity, whereas high temperatures produce low diversity and high perplexity.\nFinally, the perplexity of spoken generation is a metric that presents a high variance, therefore, as a compromise between acceptable generation time and low variance, we compute perplexity over batches of 100 generated utterances whose transcriptions are each exactly 30 words (around 10 seconds of audio).\nSubjective judgements are computed with the meaningful Mean Opinion Scores (MMOS) in which human raters were asked to evaluate how natural (considering both grammar and meaning) a given spoken generation is. For both subjective tests, raters evaluate the samples on a scale of 1- 5 with an increment of 1. We follow the method from Lakhotia et al. (2021) where they evaluated 100 samples from each of the evaluated methods while enforcing at least 15 raters for each sample. The CrowdMOS package (Ribeiro et al., 2011) was used with the recommended recipes for detecting and discarding inaccurate scores. As for the perplexity measures, the sentences are generated without conditioning on a prompt."
        },
        {
            "heading": "4.3 Zero-shot metrics",
            "text": "sWUGGY and sBLIMP are zero-shot tasks to evaluate spoken language models introduced in the Zerospeech Challenge 2021 (Nguyen et al., 2020):. These metrics are inspired by psycholinguistics and are used for interpreting what spoken LM learns. sWUGGY is a list of pairs of word/non-word synthesized with the Google TTS API and filtered for the words that are in the LibriSpeech training set. sBLIMP is a list of pairs of syntactically correct/incorrect synthesized sentences. Both sWUGGY and sBLIMP require the spoken LM to attribute a higher probability to\n3ASR transcripts are obtained with a pretrained large Wav2Vec 2.0 model, trained on LibriSpeech-960h combined with a standard KenLM 4-gram LM. The external LM used for perplexity is trained on the English NewsCrawl dataset and accessible at https: //github.com/facebookresearch/fairseq/ tree/main/examples/language_model\nthe correct element in each pair. Probabilities are computed by applying the spoken LM training loss directly on the test items. ABXsem and ABXPOS are additional zeroshot tasks introduced in Algayres et al. (2022b) to evaluate the semantic encoding and Part-Of-Speech (POS) tagging, this time not based on probabilities but on distances between embeddings. An ABX task is a list of triplets A,B and X where A and B belong to the same category and X is a distractor. The task is to encode the triplet with a distance d and show that d(A,B) < d(A,X). In this case, A,B, and X are spoken words given in the context of a sentence. For ABXsem, A and B are close semantically, and X is random. For ABXPOS A and B share the same POS tag, and X has different POS tags. Normalised Edit Distance (NED) introduced in Versteegh et al. (2016) is a term discovery task that consists in finding clusters or pairs of speech segments from unsegmented audio that have the same phonetic transcription. For each discovered pair, the NED is computed as the edit distance normalized by the length of the longest item in the pair. As for ABX tasks, the NED is also based on the distance between embeddings. To compute a NED score, we take inspiration of the procedure introduced in Thual et al. (2018). Given a segmentation of the LibriSpeech dev-clean subset, all speech segments are embedded into fixed-size vectors. With a k-NN, we search for the pairs of closest embeddings and sort them by cosine similarity. Starting from the higher similarities, we retrieve as much pair as necessary to cover the whole dev-clean set. With the phoneme-level transcription of the devclean set, all pairs can be transcribed into series of phonemes. The final NED score is obtained by averaging the NED over all pairs of transcriptions. NED and ABX tasks both rely on embeddings that can be extracted at any level of a multi-layer neural model."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Generation performances",
            "text": ""
        },
        {
            "heading": "5.1.1 Perplexity and diversity",
            "text": "Figure 3 provides a comparison of the original discrete unit-based GSLM with two versions of our continuous unit model: 200ms-tGSLM, trained on speech segmented every 200ms and gold-tGSLM, trained on speech segmented on the true word boundaries. GSLM and 200ms-tGSLM are trained\non LL6k-clean4 while the topline, gold-tGSLM, is trained only on LibriSpeech corpus5. The dots in Figure 3 represent batches of generated sentences conditioned on different temperatures. Color curves are the 3rd-degree polynomial interpolation of the dots. In green dashed lines appear two VERT anchor points LJ-VERT(=0.113) and LSVERT(=0.189). These points are the mean VERT scores obtained on batches of sentences from, respectively LJ and LibriSpeech datasets. The intersection of the dashed lines and the curves gives the scores PPX@LS-VERT and PPX@LJ-VERT that are reported in Table 16.\n4Training 200ms-tGSLM on Libri-light 60k (Kahn et al., 2019), a larger but noisier corpus, slightly undermined the performance.\n5word boundaries cannot be computed for LL6k-clean because sentence-level speech and text alignments are missing\n6For a given spoken LM, its PPX@LS-VERT score is the perplexity score obtained by that spoken LM when conditioned on a temperature that makes it generate spoken sen-\nRegarding the perplexity scores from Table 1, compared to GSLM, 200ms-tGSLM is slightly better at LJ-VERT and slightly worse at LS-VERT. The measure of perplexities being very noisy, these scores show that both models have similar performances. Some examples of transcribed spoken generations are available in Appendix Tables 8,9 and 10.\nThe topline gold-tGSLM produces much lower perplexities than GSLM and 200ms-tGSLM. Yet, we have experienced a problem with the speech decoder (described in Section 3.2.2) of gold-tGSLM. The scores of our topline are obtained by retrieving the exact transcriptions of the sampled SSEs instead of decoding them with the speech decoder. We had to do this because our speech decoder makes a lot of decoding mistakes when it tries to decode SSEs of variable-size speech fragments. It seems to generate fully intelligible speech only when it is trained to decode SSEs of same-size speech chunks, as is the case for 200ms-tGSLM. We think this happened because, for a lack of time and resources, we chose a poor decoding strategy (decoder from SSEs to HuBERT frames and HuBERT frames to speech). In our future works, we will focus on training a model to decode the SSEs directly into speech, using, for instance, recent diffusion models or a Hi-Fi Gan (Polyak et al., 2021; Huang et al., 2022). As a consequence of the poor performances of our speech decoder, we have not been able to leverage recent progress in speech segmentation into words (Algayres et al., 2022b; Kamper, 2022; Peng and Harwath, 2023) that provide word boundaries more aligned with real words than our 200ms chunks. In Appendix A.1 are the results of our attempts at using speech segmentation systems.\ntences with a VERT equal to the VERT of the LibriSpeech."
        },
        {
            "heading": "5.1.2 Subjective judgements",
            "text": "As for perplexity, we report in Table 1, the MMOS for batches of spoken generations that have a diversity score equal to the VERT of either LibriSpeech (MMOS@LS-VERT) or LJ (MMOS@LJ-VERT). In addition to 200ms-tGSLM and GSLM we evaluate a topline called character-gold that are speech utterances obtained with Text-To-Speech (Tacotron2.0 from Shen et al. (2017)) taking in input the transcriptions of LJ and LibriSpeech utterances. From Table 1, for the high-temperature regime that leads to diversity scores in the range of LJ and Librispeech, 200ms-tGSLM is slightly better than GSLM and gets close scores with the topline. MMOS scores are not available for gold-tGSLM has the speech decoder did not work properly. Nonetheless, our table of results does not show the performances of tGSLM in a much lower temperature regime. When conditioned on very low temperature, GSLM can generate very simple and intelligible sentences, whereas 200ms-tGSLM start to produce gibberish. Therefore, both models have their strengths and weaknesses."
        },
        {
            "heading": "5.2 Zero-shot performances",
            "text": "To complete our analysis, we provide in Table 1, performances on the zero-shot tasks scores that are comparable for GSLM and 200ms-tGSLM. GSLM has a little advantage on sWUGGY and sBLIMP and an 200ms-tGSLM a slight advantage on ABXsem and ABXPOS . The topline goldtGSLM, once again gets much stronger results. ABX scores are obtained, for GSLM at the 9th layer of the transformer and for tGSLM with the lexical tokens."
        },
        {
            "heading": "5.3 Interpretability",
            "text": "In order to analyze what is learned by LexEmb we measure the ABX and NED of lexical tokens and acoustic tokens. In Table 2, the ABX scores show that the acoustic tokens are at chance level on semantic and syntactic encoding. After the LexEmb function, the lexical tokens lose a bit of their phonetic encoding (NED increases) but gain the ability to represent semantics and syntax. However, the NED is not at chance level, meaning that a bit of acoustic information has leaked into the lexical tokens. To visualize the difference between acoustic and lexical spaces, we provide t-SNE maps in Appendix Section A.4."
        },
        {
            "heading": "5.4 Memory consumption",
            "text": "GSLM model (Lakhotia et al., 2021) and 200mstGSLM use the same transformer LM but with different types of inputs. Compared to the 200ms-long units of our model, GSLM is trained on discrete units that are 40ms long on average (when contiguous duplicates are removed). Therefore, we expected our model to be more memory efficient than GSLM7 which can be observed by the maximal batch size that both models can handle. Indeed, on the one hand, we managed to train GSLM with 34 60-seconds-long sentences on a 32G V100 GPU without OOM error. On the other hand, 200mstGSLM can fit as many as 162 sentences, which shows almost a 5-time reduction (\u2248 4.76) of memory use.\nTraining spoken LMs on long sequences of audio will become necessary in order to learn long-term semantic relations. The usage of very short acoustic units can become a bottleneck which our method helps to alleviate. To complete our analysis, we provide in Appendix A.5 a theoretical analysis of memory reduction."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduced a generative spoken LM based on continuous word-sized acoustic tokens. Our model is able to generate speech with the same level of diversity and accuracy as a model based on discrete units. This shows that building a lexicon of types is not necessary for spoken language modelling, which is encouraging considering the difficulty of clustering large segments of speech without degrading the representation (see Appendix B). In addition, this performance was obtained with segments that were not very well aligned with word boundaries (200ms segments). The good result obtained with gold word boundaries indicates that there is room for improvement by using segments\n7The acoustic tokens that are the input of 200ms-tGSLM are extracted in a preprocessing step. They do not impact memory usage at training time.\nbetter aligned with word boundaries and of course a better speech decoder. Further work is also needed to better limit the leakage of low-level acoustic information into the LM through continuous units, which our analysis has shown is detrimental to the performance of the generative model (see also Nguyen et al. (2022c)). Finally, the fact that the units are about 5 times larger than standard GSLM units aligns with the NLP literature that is in favour of word-based LMs. It opens the possibility to fit larger spans of audio in GPUs and capture longdistance relationships."
        },
        {
            "heading": "7 Limitations",
            "text": "Our method has some limitations that range from GPU consumption, potential overfitting on the English language and sub-optimal decoding method. First, tGSLM is trained on 32 Nvidia V100-32Go GPUs for 30 hours. Due to the several modules at work in tGSLM (SSE model, LexEmb function, transformer decoder and seq2seq decoder), a large grid-search on hyper-parameters has been necessary which makes this work quite resourceconsuming. Secondly, during the grid-search we chose hyper-parameters to optimize the semantic and syntactic ABX scores on English. By doing so, we might have overfitted the English language and made tGSLM specifically good at generating English speech. Further analysis is required to see if our method generalizes well to syntactically and morphologically different languages, like French or Mandarin. Finally, our decoding method is based on a seq2seq transformer that produces HuBERT frames which are decoded into speech with a combination of Tacotron2.0 and WaveGlow. We chose that method as this later speech synthesiser comes pre-trained in the textlesslib Python library (Kharitonov et al., 2022). Yet, recent work on textless speech synthesis Kreuk et al. (2021); Kharitonov et al. (2021a) skip the spectrogram prediction of Tacotron2.0 and directly train a HifiGan model to generate speech from HuBERT units. This latter model shows close to human-level performances. We leave the use of Hifi-Gan instead of Tacotron2.0 for future works on tGSLM."
        },
        {
            "heading": "8 Ethical statement",
            "text": "tGSLM is a LM that learns to generate speech sentences by predicting its training data. Therefore, tGSLM inherits from ethical concerns associated with text-based LM, speech encoders and speech\nsynthesizers. It is of paramount importance to safeguard against these issues.\nFirst, generative text-based LMs are known to repeat stereotypes and biases that belong to the training corpus which can cause a series of harms Chowdhery et al. (2022); Bender et al. (2021). One way to mitigate this is to apply post-processing on generated sentences to detect harmful content. Yet, from what we have heard, tGSLM still struggles to generate sentences that fully make sense, so we do not think that post-processing is required at the moment.\nSecond, if tGSLM is used to continue a speech prompt, the continuation might be inconsistent for accents of underrepresented groups in the training data. Indeed, speech systems are known to encode poorly accents and dialects out of the training distribution (Riviere et al., 2021).\nFinally, tGSLM continuations will not preserve any regional accentuation from the prompt, as our model only generates speech in the voice of the single speaker of the LJ dataset."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded in part, to the authors in their academic capacities, by the Agence Nationale pour la Recherche (ANR-17-EURE-0017 Frontcog, ANR-10-IDEX-0001-02 PSL*, ANR19-P3IA-0001 PRAIRIE 3IA Institute), CIFAR (Learning in Machines and Brains) and Meta AI Research (Research Grant). This work was performed using HPC resources from GENCI-IDRIS (Grant 2021-[AD011011217])."
        },
        {
            "heading": "A Supplementary materials",
            "text": "A.1 Speech segmentation\nTo study the impact of speech segmentation on tGSLM, we trained this model on LibriSpeech with two extra segmentation methods: SylSeg (R\u00e4s\u00e4nen et al., 2018), and DP-Parse (Algayres et al., 2022b)8. Sylseg segments speech into syllable-like units, using damped oscillators that exploit rhythmic cues of syllabic structure in speech. DP-Parse (Algayres et al., 2022b) segments speech into wordlike units with state-of-the-art performances. This model adapts a non-parametric Bayesian model for text segmentation (Goldwater et al., 2009)\n8We did not train these models on LL6k-clean because DP-Parse is hard to scale to large datasets.\nto speech. Table 3 shows generation and zeroshot scores. Overall, regarding speech generation, 200ms-tGSLM outperforms sylseg-tGSLM, dpparse-tGSLM and also GSLM. For zero-shot tasks, once again, all models score similarly. ABX scores are again obtained for GSLM with embeddings extracted from the 9th layer of the transformer and for tGSLM from the lexical tokens.\nEven though true word boundaries strongly benefit tGSLM, using unsupervised speech segmentation methods did not prove beneficial. We think this is due to the low performances of state-of-the-art speech segmentation systems. These latter are only marginally better than random segmentations and lag largely behind text segmentation performances (Dunbar et al., 2022b; Algayres et al., 2022b). This result suggests that progress is needed in unsupervised speech segmentation to be able to combine segmented units into intelligible speech. After all, the best segmentation method that we works for us is the 200ms method. We have also experimented with other durations as 120ms,280ms and 360ms. We chose to go on with 200ms based on a compromise between maximal duration and maximal zero-shot task performances. These scores can be found in Appendix Table 4.\nA.2 Discussion on q\nA.2.1 Mathematical details on q\nLet us now derive q computation. Given a training corpus, that is segmented and encoded into a collection of acoustic tokens (ai)i\u2264N . A PCA is trained on (ai)i\u2264N and the d first dimensions are kept, let us write (a\u2032i)i\u2264N the resulting vectors and (v0, ..., vd\u2032) the explained variance of each PCA dimensions. Then, we train d separate k-means on each dimension of the PCA. The number of cluster per k-means is computed as ( \u2308 K v0v0 \u2309 , , ..., \u2308 K vd\u2032 v0 \u2309 ). The values of d and K were set to maximize the scores at the zero-shot tasks. Once the k-means are trained, the centroids are stored in d dictionaries (k0, ..., kd). For any i \u2264 N , we compute q(ai) by assigning \u2200j \u2264 d, q(ai)[j] to its closest centroids in kj . Finally, cluster ids are turned into one-hot vectors and concatenated into a single vector. The following operations\nsum up the process.\n\u2200i \u2264 n, q(ai)\u2190  argmax j\u2264K (ai[0]\u2212 k0[j]) argmax j\u2264 \u2308 K v1 v0 \u2309(ai[1]\u2212 k1[j]) ...\nargmax j\u2264 \u2308 K\nvd v0\n\u2309(ai[d]\u2212 kd[j])\n\nq(ai)\u2190  onehot(q(ai[0])) onehot(q(ai[1]))\n... onehot(q(ai[d]))  q(ai)\u2190 concatenate(q(ai[0]), ..., q(ai[d]))\nA.2.2 Ablation on q The function q introduced in Section 3.1.2, composed of a PCA and our d-k-means method, is ablated in Table 6. In all configurations, the embeddings right after the LexEmb function are used to compute the ABX and NED scores. On the one hand, q degrades the phonetic information in the lexical tokens (NED increases) and makes training harder (validation loss increases). On the other hand, q maximizes semantic and syntactic information (ABX increases) as well as generation quality (PPX decreases). A null value in Table 6 means that the model is not able to produce intelligible sentences with this setup. First, these experiments show the necessity of q for the 200ms-tGSLM to generate spoken sentences. Second, the combination of these results reveals that q prevents the model from converging quickly to a bad local minimum that hinders generalization.\nIt follows our intuition from Section 3.1.2: there seems to be a low-variance signal encoded in the\nacoustic tokens that interfere with the semantic and syntactic modelling. In our opinion, this signal gives away both local information, direct right and left context due to coarticulation, and global sentence-level information (relative token position and speaker identity).\nA.2.3 On using MFCCs instead of Wav2vec2.0\nOne may say that if q is used to mitigate the downsides of the attention mechanism of Wav2vec2.0, why not use more local features like MFCC or Melfilterbanks? We argue that even though these latter features are still good for supervised tasks as ASR Radford et al. (2022), they are substantially outperformed by recent self-supervised speech models (Wav2vec2.0, CPC, HuBERT,...) at the tasks of zero-shot word discrimination (Algayres et al., 2022a; Van Staden and Kamper, 2020) and keyword spotting (Yang et al., 2021). To prove our point, we compare the performances of MFCCs compare to Wav2vec2.0 at the task of discriminating acoustic tokens. As a reminder, the acoustic tokens that we used in our model, are the output of an SSE model from Algayres et al. (2022a), pre-trained to embed variable-length sequences of Wav2vec2.0 frames into fixed-size vectors. The same kind of SSE model but pre-trained on MFCC frames is also provided by Algayres et al. (2022a). Let us segment the LibriSpeech corpus every 200ms and embed the speech segments with both SSE models so that we get two collections of acoustic tokens: (amfcci )i\u2264N and (a w2v2 i )i\u2264N . Let us also apply the q function on Wav2vec2.0 acoustic tokens so that we get: (q(aw2v2i ))i\u2264N . To measure performances, we use our NED metric on the three collections of embeddings. From the\nresults Table 7, we see that Wav2vec2.0 leads to much better acoustic tokens than MFCCs. Moreover, even when q is applied on Wav2vec2.0 acoustic tokens, the NED score of (q(aw2v2i ))i\u2264N is still much lower than on (amfcci )i\u2264N . This latter has a NED score of 65%, which means that two neighbouring MFCC acoustic tokens have on average less than half of their phonemes in common. For that reason, why we excluded MFCC from our experiments on speech generation.\n.\nA.3 Hyperparameters Wav2vec2.0 and SSE are trained on the LibriSpeech corpus respectively by Baevski et al. (2020) and Algayres et al. (2022a). Wav2vec2.0 Base is a stack of 7 convolution layers and 12 transformer layers. The SSE is composed of a one GLU convolution layer (kernel size: 4, number of channels: 512, stride: 1), a transformer layer (attention heads: 4, size of attention matrices: 512 neurons, and FFN: 2048 neurons) and a final max-pooling layer along the time axis.\nLexEmb is composed of two functions L \u25e6 q. L is a stack of five three-layers blocks each formed by a 1024-neurons fully connected layer, a layer\nnorm and a ReLU activation. q is of a PCA and a collection of k-means that are trained on LL6kclean. The PCA has d = 24 dimensions and the number of centroids for the first k-means is K = 10.\nTransformer is identical to the one used in the original GSLM paper (Lakhotia et al., 2021). It contains 12 transformer layers with 16 heads, 1024-neuron attention matrices, and 4096-neurons FFN. On top of the transformer, the three parallel h1,h2,h3 functions are 1024-neurons fully connected layers. L,h1,h2,h3 and the transformer are trained on 32 GPUs, for 200k iterations on either the LibriSpeech or LL6k-clean. Each batch is composed of 64 audio sentences that are composed of 64 tokens. The learning rate is set at 5\u22124 with a warm-up of 5000 updates and polynomial decay. We use Adam optimizer with a weight decay of 0.1. A dropout of 0.1 is applied during training. The loss function is the NCE loss with a temperature of 0.1 and 500 negative samples.\nSampling is performed in a FAISS k-NN (Johnson et al., 2017) that contains all the lexical tokens segmented in the dev-clean and test-clean from the LibriSpeech (roughly 10 hours of speech). The number of nearest neighbours from which the next token is sampled is set to 1000.\nSpeech generation model is an encoder and a decoder that shares the same architecture: 4 transformer layers with 8 heads, 512-neurons attention matrices, and 3072-neurons FFN. It is trained on 32 GPUs, for 30k iterations on the LibriSpeech. Each batch is composed of four audio sentences that are at maximum 20 seconds long. The learning rate is set at 5\u22125 with a warm-up of 103 updates and\npolynomial decay. We use a dropout probability of 0.1 and Adam optimizer with a weight decay of 0.1. The Tacotron2.0 from Lakhotia et al. (2021); Kharitonov et al. (2022) was trained on LJ.\nA.4 Probing acoustic and lexical spaces Figure 4 is a visualization of the acoustic and lexical representation learned by gold-tGSLM which echo a work on speech word embeddings from Chung and Glass (2018). All speech segments corresponding to real words in the LibriSpeech dev-clean set are indexed in k-NN graphs on their acoustic or lexical form. Each embedding is labelled with its true transcription. By searching for the nearest neighbors of a centre word (in red in the figure), we highlight in green the neighbours that we judged semantically related to the centre word. Figure 4 shows that an acoustic token has usually no semantically related neighbour other than ones with the same transcription. By contrast, lexical tokens have semantic and syntactic properties: \u2019London\u2019 is close to other cities and countries, \u2019blue\u2019 is close to colour names, beautiful is close to other positive adjectives, and \u2019chair\u2019 is close to \u2019desk\u2019 and \u2019table\u2019. Nonetheless, it appears acoustic information has leaked from the acoustic tokens into the lexical tokens. For instance, the lexical neighbours of \u2019blue\u2019 are colours or shades that start with a \u2019b\u2019 and \u2019chest\u2019 appears in the neighbourhood of \u2019chair\u2019.\nA.5 Estimation of memory consumption To estimate the memory consumption of a transformer LM with L = 16 layers, a single attention head, a batch size of 1, and an embedding size d = 1024 , let us write x \u2208 IRn\u00d7d a sentence of n tokens represented with embeddings of size d . Using the formula expressed in Korthikanti et al. (2022), the number of activations to store in memory during backpropagation is approximately (buffers and negligible values being omitted) \u03d5(L, n, d) = Lnd(34+5nd ). In the LL6kclean corpus, sentences are 60s-long on average with make n = 1500 for GSLM and n = 300 for 200ms-tGSLM. 200ms-tGSLM should expect a memory reduction by a factor of \u03d5(16,1500,1024)\u03d5(16,300,1024) \u2248 5.83 compared to GSLM. In practice, we observe a lower memory reduction (\u2248 4.76) which can be explained by the additional parameters that are present in 200ms-tGSLM and not in GSLM, namely the LexEmb function and three prediction heads.\nA.6 Inference time complexity Taking the calculation of a forward cost from Pan et al. (2021), for a sequence length of size n, and a transformer of L = 16 layers and dimension d = 1024, a forward pass costs (12nd+2nd) \u2217L. This would be the cost of a forward pass in the GSLM model, but our tGSLM costs a little bit more with its LexEmb function and its sampling procedure. A forward through our LexEmb function costs 5nd2 (5 linear layers) and the sampling procedure costs d \u2217 100.000 (we usually take 100k items for the k-NN search). Therefore, tGSLM cost (12nd + 2nd)L + 5nd + d \u2217 100000. For a sequence of 1 second (therefore n = 5 for tGSLM and n = 25 for GSLM), by replacing those values in the former calculation, we find that a forward in tGSLM cost 1.1e6 which is 5 times less costly than a forward in GSLM that would cost 5.2e6. Therefore, even at inference time, our tGSLM should be much faster than GSLM to run."
        },
        {
            "heading": "B Clustering SSEs",
            "text": "B.1 The problem of clustering large units\nIn the introduction, we argued that the clustering of a large collection of word-size speech fragments is a daunting challenge. The first difficulty is the very large number of word types in a corpus. For instance, there are \u22484.5k word types in the 10- hour-long Buckeye corpus (Pitt et al., 2005), and \u224890k in the 960-hour-long LibriSpeech (Panayotov et al., 2015). Most clustering algorithms (k-means, hierarchical-k-means, Spectral, Chinese Whisper (Biemann, 2006), Dirichlet Process-means (Kulis and Jordan, 2011), Brown (Brown et al., 1992)), require in input either this unknown number of clusters or a threshold parameter that controls the number of clusters9. By misestimating the number of clusters, we introduce errors that can mislead the downstream LM. The second problem is the highly skewed distribution of the frequency of word types. This phenomenon, known as Zipf\u2019s Law (Zipf, 1949), states that, in text, there is a linear relation between the log-rank of word types and their log-frequencies. In practice, it means that, in a text, most word tokens are hapaxes (i.e. they have a word type that appears only once), and a small proportion of word types account for most tokens. Therefore, even if the number of word types could be correctly estimated, a clustering algorithm would have to produce many singleton clusters, which is a hard task for clustering models, especially for k-means that tend to create equal-size clusters (Wu, 2012). For those reasons, a clustering algorithm is likely to produce non-recoverable errors that will negatively impact the downstream spoken LM.\n9Some unsupervised methods to find the number of clusters exist, like the \u2019elbow methods\u2019 in k-means, but these methods are quite noisy and hard to apply when the number of classes is that large.\nB.2 Attempt at clustering SSEs Here are some attempts and results at the task of clustering speech fragments. First, we retrieved the gold-standard word segmentation of the LibriSpeech corpus and embedded all word tokens with the SSE model from Algayres et al. (2022a). Then, we clustered with k-means and hierarchicalk-means all the SSEs in the LibriSpeech into K classes. To simplify, we set K to the true number of word types. Finally, we trained a transformer LM to predict the next discrete clusters. We observed that the training loss decreased but not the validation loss. The reason for this failure was simple: most of the clusters found by k-means or hierarchical-k-means that were in the validation set were not present in the training set. These results show that those baseline clustering algorithms are not suited to the task of spoken language modelling.\nRegarding more elaborate clustering methods, Elkahky et al. (2023) has used a combination of HuBERT units, smoothing technics, BytePair-Encoding (BPE) and Brown clustering (BC) (Brown et al., 1992) with either 30k clusters or 2k clusters. The result is a discretisation of speech with word-size units. We have proved in Elkahky et al. (2023) that these units can be used to train a HuBERT model and improves its downstream ASR performances. Here, we discretised the LibriLight 6k clean (Rivi\u00e8re and Dupoux, 2021) with these large units using either 30k and 2k clusters and gave them in input to a transformer LM trained to predict the next discrete units. We report in Table 11 sWUGGY and sBLIMP scores of GSLM, 200ms-tGSLM and the large units (called BC-30k and BC-2k). The scores show that these units perform equally well to GSLM and 200ms-tGSLM. These results show that even the most elaborate methods of clustering do not bring better results than our method to adapt spoken LM to continuous inputs."
        }
    ],
    "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
    "year": 2023
}