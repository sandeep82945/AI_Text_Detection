{
    "abstractText": "With the rise of social media, users are exposed to many misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the important claims from such posts is arduous and time-consuming, yet it is an underexplored problem. Here, we aim to bridge this gap. We introduce a novel task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and noisy social media posts into more straightforward and understandable forms, termed normalized claims. We propose CACN, a pioneering approach that leverages chain-of-thought and claim check-worthiness estimation, mimicking human reasoning processes, to comprehend intricate claims. Moreover, we capitalize on the in-context learning capabilities of large language models to provide guidance and to improve claim normalization. To evaluate the effectiveness of our proposed model, we meticulously compile a comprehensive real-world dataset, CLAN, comprising more than 6k instances of social media posts alongside their respective normalized claims. Our experiments demonstrate that CACN outperforms several baselines across various evaluation measures. Finally, our rigorous error analysis validates CACN\u2019s capabilities and pitfalls.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Megha Sundriyal"
        },
        {
            "affiliations": [],
            "name": "Tanmoy Chakraborty"
        },
        {
            "affiliations": [],
            "name": "Preslav Nakov"
        },
        {
            "affiliations": [],
            "name": "Bin Zayed"
        }
    ],
    "id": "SP:f5630eff41d5c3e2c9cd083c7090c0ee0e279ba0",
    "references": [
        {
            "authors": [
                "Britt Bruntink",
                "Preslav Nakov."
            ],
            "title": "Fighting the COVID-19 infodemic: Modeling the perspective of journalists, fact-checkers, social media platforms, policy makers, and the society",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Hunt Allcott",
                "Matthew Gentzkow."
            ],
            "title": "Social media and fake news in the 2016 election",
            "venue": "Journal of Economic Perspectives, 31(2):211\u201336.",
            "year": 2017
        },
        {
            "authors": [
                "Gabor Angeli",
                "Melvin Jose Johnson Premkumar",
                "Christopher D. Manning."
            ],
            "title": "Leveraging linguistic structure for open domain information extraction",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Tamer Elsayed",
                "Preslav Nakov",
                "Giovanni Da San Martino",
                "Maram Hasanain",
                "Reem Suwaileh",
                "Fatima Haouari."
            ],
            "title": "CheckThat! at CLEF 2020: Enabling the automatic identification and verification of claims in social media",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Emily M. Bender",
                "Jonathan T. Morgan",
                "Meghan Oxley",
                "Mark Zachry",
                "Brian Hutchinson",
                "Alex Marin",
                "Bin Zhang",
                "Mari Ostendorf."
            ],
            "title": "Annotating social acts: Authority claims and alignment moves in Wikipedia talk pages",
            "venue": "Proceedings of the Work-",
            "year": 2011
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Christopher Hidey",
                "Kathy McKeown."
            ],
            "title": "IMHO fine-tuning improves claim detection",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Daxenberger",
                "Steffen Eger",
                "Ivan Habernal",
                "Christian Stab",
                "Iryna Gurevych."
            ],
            "title": "What is the essence of a claim? Cross-domain claim identification",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Markus Dreyer",
                "Mengwen Liu",
                "Feng Nan",
                "Sandeep Atluri",
                "Sujith Ravi."
            ],
            "title": "Evaluating the tradeoff between abstractiveness and factuality in abstractive summarization",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2089\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Esin Durmus",
                "He He",
                "Mona Diab."
            ],
            "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Angela Fan",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "Controllable abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 45\u201354, Melbourne, Australia. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Sai Chetan Chinthakindi",
                "Yi R. Fung",
                "Kevin Small",
                "Heng Ji."
            ],
            "title": "A zero-shot claim detection framework using question answering",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6927\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Sai Chetan Chinthakindi",
                "Zhenhailong Wang",
                "Yi Fung",
                "Kathryn Conger",
                "Ahmed ELsayed",
                "Martha Palmer",
                "Preslav Nakov",
                "Eduard Hovy",
                "Kevin Small",
                "Heng Ji"
            ],
            "title": "2022b. NewsClaims: A new benchmark for claim detection from news",
            "year": 2022
        },
        {
            "authors": [
                "Pepa Gencheva",
                "Preslav Nakov",
                "Llu\u00eds M\u00e0rquez",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Ivan Koychev."
            ],
            "title": "A contextaware approach for detecting worth-checking claims in political debates",
            "venue": "Proceedings of the International Conference on Recent Advances in Natural",
            "year": 2017
        },
        {
            "authors": [
                "Shreya Gupta",
                "Parantak Singh",
                "Megha Sundriyal",
                "Md. Shad Akhtar",
                "Tanmoy Chakraborty."
            ],
            "title": "LESA: Linguistic encapsulation and semantic amalgamation based generalised claim detection from online content",
            "venue": "Proceedings of the 16th Conference",
            "year": 2021
        },
        {
            "authors": [
                "Andreas Hanselowski",
                "Hao Zhang",
                "Zile Li",
                "Daniil Sorokin",
                "Benjamin Schiller",
                "Claudia Schulz",
                "Iryna Gurevych."
            ],
            "title": "UKP-Athene: Multi-sentence textual entailment for claim verification",
            "venue": "Proceedings of the First Workshop on Fact Extraction and",
            "year": 2018
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Anton Chernyavskiy",
                "Ivan Koychev",
                "Dmitry Ilvovsky",
                "Preslav Nakov."
            ],
            "title": "CrowdChecked: Detecting previously fact-checked claims in social media",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Naeemul Hassan",
                "Gensheng Zhang",
                "Fatma Arslan",
                "Josue Caraballo",
                "Damian Jimenez",
                "Siddhant Gawsane",
                "Shohedul Hasan",
                "Minumol Joseph",
                "Aaditya Kulkarni",
                "Anil Kumar Nayak"
            ],
            "title": "Claimbuster: The first-ever end-to-end fact-checking system",
            "year": 2017
        },
        {
            "authors": [
                "Israa Jaradat",
                "Pepa Gencheva",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Llu\u00eds M\u00e0rquez",
                "Preslav Nakov."
            ],
            "title": "ClaimRank: Detecting check-worthy claims in Arabic and English",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Irina Khaldarova",
                "Mervi Pantti."
            ],
            "title": "Fake news",
            "venue": "Journalism Practice, 10(7):891\u2013901.",
            "year": 2016
        },
        {
            "authors": [
                "Preslav Nakov",
                "David Corney",
                "Maram Hasanain",
                "Firoj Alam",
                "Tamer Elsayed",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Paolo Papotti",
                "Shaden Shaar",
                "Giovanni Da San Martino."
            ],
            "title": "Automated fact-checking for assisting human fact-checkers",
            "venue": "Proceedings of the 30th Inter-",
            "year": 2021
        },
        {
            "authors": [
                "Preslav Nakov",
                "Giovanni Da San Martino",
                "Tamer Elsayed",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Rub\u00e9n M\u00edguez",
                "Shaden Shaar",
                "Firoj Alam",
                "Fatima Haouari",
                "Maram Hasanain",
                "Watheq Mansour"
            ],
            "title": "2021b. Overview of the CLEF-2021 CheckThat! lab on detecting",
            "year": 2021
        },
        {
            "authors": [
                "Liangming Pan",
                "Xiaobao Wu",
                "Xinyuan Lu",
                "Anh Tuan Luu",
                "William Yang Wang",
                "Min-Yen Kan",
                "Preslav Nakov."
            ],
            "title": "Fact-checking complex claims with program-guided reasoning",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Compu-",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander M. Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389, Lisbon, Portugal.",
            "year": 2015
        },
        {
            "authors": [
                "Shaden Shaar",
                "Firoj Alam",
                "Giovanni Da San Martino",
                "Preslav Nakov."
            ],
            "title": "The role of context in detecting previously fact-checked claims",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1619\u20131631, Seattle, United",
            "year": 2022
        },
        {
            "authors": [
                "Shaden Shaar",
                "Nikolay Babulkov",
                "Giovanni Da San Martino",
                "Preslav Nakov."
            ],
            "title": "That is a known lie: Detecting previously fact-checked claims",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3607\u20133618.",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Shaar",
                "Nikola Georgiev",
                "Firoj Alam",
                "Giovanni Da San Martino",
                "Aisha Mohamed",
                "Preslav Nakov."
            ],
            "title": "Assisting the human fact-checkers: Detecting all previously fact-checked claims in a document",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Amir Soleimani",
                "Christof Monz",
                "Marcel Worring."
            ],
            "title": "BERT for evidence retrieval and claim verification",
            "venue": "Advances in Information Retrieval, 12036:359.",
            "year": 2020
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum."
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Megha Sundriyal",
                "Atharva Kulkarni",
                "Vaibhav Pulastya",
                "Md. Shad Akhtar",
                "Tanmoy Chakraborty."
            ],
            "title": "Empowering the fact-checkers! Automatic identification of claim spans on Twitter",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Megha Sundriyal",
                "Parantak Singh",
                "Md. Shad Akhtar",
                "Shubhashis Sengupta",
                "Tanmoy Chakraborty."
            ],
            "title": "DESYR: Definition and syntactic representation based claim detection on the web",
            "venue": "Proceedings of the 30th ACM International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Prasetya Utama",
                "Joshua Bambrick",
                "Nafise Moosavi",
                "Iryna Gurevych."
            ],
            "title": "Falsesum: Generating document-level NLI examples for recognizing factual inconsistency in summarization",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Chris van der Lee",
                "Albert Gatt",
                "Emiel van Miltenburg",
                "Emiel Krahmer."
            ],
            "title": "Human evaluation of automatically generated text: Current trends and best practice guidelines",
            "venue": "Computer Speech & Language, 67:101151.",
            "year": 2021
        },
        {
            "authors": [
                "Slavena Vasileva",
                "Pepa Atanasova",
                "Llu\u00eds M\u00e0rquez",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Preslav Nakov."
            ],
            "title": "It takes nine to smell a rat: Neural multi-task learning for check-worthiness prediction",
            "venue": "Proceedings of the International Conference on Recent Advances in",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Dustin Wright",
                "Isabelle Augenstein."
            ],
            "title": "Claim check-worthiness detection as positive unlabelled learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 476\u2013488, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Hanqing Zhang",
                "Haolin Song",
                "Shaoyu Li",
                "Ming Zhou",
                "Dawei Song."
            ],
            "title": "A survey of controllable text generation using transformer-based pre-trained language models",
            "venue": "ACM Comput. Surv., 56(3).",
            "year": 2023
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "Proceedings of the International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "BERTScore: Evaluating text generation with BERT",
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR \u201919\u2019.",
            "year": 2019
        },
        {
            "authors": [
                "Shi Zhi",
                "Yicheng Sun",
                "Jiayi Liu",
                "Chao Zhang",
                "Jiawei Han."
            ],
            "title": "ClaimVerif: A real-time claim verification system using the web and fact databases",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM \u201917,",
            "year": 2017
        },
        {
            "authors": [
                "Prompt-Tuning Raffel"
            ],
            "title": "Comparative top-k precision evaluations of normalized claim vs. original posts in evidence retrieval",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Social media have enabled a new way of communication, breaking down geographical barriers and bringing unprecedented opportunities for knowledge exchange. However, this has also presented a growing threat to society, e.g., during the 2016 US Presidential Election (Allcott and Gentzkow, 2017), the COVID-19 pandemic (Alam et al., 2021; Rocha et al., 2021; Nakov et al., 2022a), the Ukraine\u2013 Russia conflict (Khaldarova and Pantti, 2016), etc.\n1We release our dataset and code at https://github. com/LCS2-IIITD/CACN-EMNLP-2023\nFalse claims are an intrinsic aspect of fabricated news, rumors, propaganda, and misinformation. Journalists and fact-checkers work tirelessly to assess the factuality of such claims in spoken and/or written form, sifting through an avalanche of claims and pieces of evidence to determine the truth. To further address this pressing issue, several independent fact-checking organizations have emerged in recent years, such as Snopes,2 FullFact,3 and PolitiFact,4 which play a crucial role in verifying the accuracy of online content. However, the rate at which online information is being disseminated far outpaces the capacity of fact-checkers, making it difficult to verify every single claim. This, in turn, leaves numerous unverified claims circulating online, potentially reaching millions before they can be verified.\n2https://www.snopes.com 3https://fullfact.org 4https://www.politifact.com\nWhile the complete automation of the factchecking pipeline may pose hazards to accountability and reliability, several recent studies have targeted identifying downstream tasks suitable for automation, such as detecting claims (Daxenberger et al., 2017; Chakrabarty et al., 2019; Gangi Reddy et al., 2022b), evaluating their worthiness for factchecking (Gencheva et al., 2017; Jaradat et al., 2018; Wright and Augenstein, 2020), making sure they were not fact-checked before (Shaar et al., 2020, 2022a,b; Hardalov et al., 2022), and validating them by retrieving relevant shreds of evidence (Zhi et al., 2017; Hanselowski et al., 2018; Soleimani et al., 2020; Pan et al., 2023). See also a recent survey on automated fact-checking for assisting human fact-checkers (Nakov et al., 2021a).\nIn light of the growing challenges faced by factcheckers in verifying the factuality of social media claims, we propose the novel task of claim normalization. This task aims to extract and to simplify the central assertion made in a long, noisy social media post. This can improve the efficacy and curtail the workload of fact-checkers while maintaining high precision and conscientiousness. We provide a more detailed explanation of why the claim normalization task is essential and illustrate its significance in Appendix A.1.\nIn our problem formulation, given an input social media claim, the system needs to simplify it in a concise form that contains the post\u2019s central assertion that fact-checkers can easily verify. To better understand our motivation, we illustrate the task in Figure 1. The first social media post reads,\n\u2018Cyanocobalamin is a synthetic form of Vitamin B12...If you\u2019re on B12 supplements, throw them away.\u2019 This post contains some extraneous information that has no relevance for fact-checkers. As a result, they distil the information and summarize it as, \u2018Cyanocobalamin, the most common form of Vitamin B12, is toxic.\u2019 Fact-checkers tasked with verifying the accuracy of such noisy posts need to read through them and condense their content to obtain a concise claim that can be easily fact-checked. Unfortunately, this process can be exceedingly timeconsuming. By automating the claim normalization process, fact-checkers can work more efficiently. Another aspect is that fact-checkers often choose what to fact-check based on the virality of a claim, for which they need to be able to recognize when the same claim appears in a slightly different form, and claim normalization is essential for this.\nOur contributions are as follows:\n\u2022 We introduce the novel task of claim normalization, which seeks to detect the core claim in a given piece of text.\n\u2022 We present a meticulously curated high-quality dataset specifically tailored for claim normalization of noisy social media posts.\n\u2022 We propose a robust framework for claim normalization, incorporating chain-of-thought, incontext learning, and claim check-worthiness estimation to comprehend intricate claims.\n\u2022 We conduct a thorough error analysis, which can inform future research."
        },
        {
            "heading": "2 Related Work",
            "text": "Claim Analysis. Previous work has focused on distinct aspects of claims, including claim detection (Daxenberger et al., 2017; Gupta et al., 2021; Sundriyal et al., 2021; Gangi Reddy et al., 2022a,b), claim check-worthiness estimation (Hassan et al., 2017; Gencheva et al., 2017; Barr\u00f3n-Cede\u00f1o et al., 2018; Jaradat et al., 2018; Vasileva et al., 2019; Barr\u00f3n-Cede\u00f1o et al., 2020; Konstantinovskiy et al., 2021), claim span identification (Sundriyal et al., 2022), etc. By curating the AAWD corpus, Bender et al. (2011) pioneered the efforts in claim detection, the foremost step in the fact-checking tasks. Following this, linguistically motivated features, including sentiment, syntax, context-free grammar, and parse trees, were frequently used (Daxenberger et al., 2017; Lippi and Torroni, 2015; Levy et al., 2017; Sundriyal et al., 2021). Recently, large language models (LLMs) have also been used for claim detection (Chakrabarty et al., 2019; Barr\u00f3n-Cede\u00f1o et al., 2020; Gupta et al., 2021; Gangi Reddy et al., 2022a,b).\nMost previous work on claim detection and extraction primarily concentrated on adapting to text that comes from similar distributions or topics. Moreover, it often relied on well-structured formal writing. In contrast, our objective is to develop a system that specifically addresses the challenges posed by posts in social media and aims to extract the central claim in a more simplified manner, which goes beyond extracting a text subspan in a social media post and aims at abstractive claim extraction that mimics what professional fact-checkers do. To the best of our knowledge, we are the first to address the task of claim extraction in this very practical formulation.\nText Summarization. The task of claim normalization is closely related to the task of text summarization. In the latter, given a lengthy document, the goal is to summarize it into a much shorter summary. Previous work on text summarization has explored various approaches, including large pretrained seq2seq models to generate high-quality summaries (Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020).\nOne issue has been the faithfulness of the summary with respect to the source. To address this, Kryscinski et al. (2020) introduced FactCC, a weakly-supervised BERT-based entailment model, which augments the dataset with artificially introduced faithfulness errors. Similarly, (Utama et al., 2022) trained a model for detecting factual inconsistencies in data from controllable text generation that perturbs human-annotated summaries, introducing varying types of factual inconsistencies. Durmus et al. (2020) proposed a questionanswering framework that compares answers from the summary to those from the original text.\nAll these approaches primarily focused on general-purpose summarization and did not provide means for models to generate summaries primarily focusing on specific needs. To address this limitation, controlled summarization was introduced (Fan et al., 2018). One aspect of controlled summarization is length control, in which users can set their preferred summary length (Rush et al., 2015; Kikuchi et al., 2016). Recent research has discovered that, despite their fluency and coherence, state-of-the-art abstractive summarization systems produce summaries with contradictory information.\nWhile text summarization systems can assist in condensing social media posts into shorter summaries, their primary goal is not to ensure verifiability. It aims to capture the key points of the text rather than emphasizing the specific claims within the text that need to be fact-checked. Our task of claim normalization, on the other hand, works at an entirely different level. It needs a thorough understanding of the claims made in the social media post and strives to ensure that the normalized claims are not only consistent with the original post, but are also self-contained and verifiable.\nDespite the progress in text summarization, the task of claim normalization remains underexplored. In this work, we aim to tackle this challenging problem by developing a robust approach specifically tailored to the unique aspects of this task."
        },
        {
            "heading": "3 Dataset",
            "text": "Existing text summarization datasets have not specifically addressed the need for claim-oriented summaries. To address this gap, we propose a novel dataset CLAN (Claim Normalization), consisting of fact-checked social media posts paired with concise claim-oriented summaries (known as normalized claims), created by fact-checkers as part of the verification process. As a result, our dataset is not subjected to external annotation, thus averting potential biases and ensuring its high quality."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "We gathered our fact-checked post and claim pairs from two sources: (i) Google Fact-Check Explorer5 and (ii) ClaimReview Schema.6\nGoogle Fact-Check Explorer. We acquired a list of fact-checked claims from multiple reputed factcheck sources via Google Fact-Check Explorer\u2019s API (GFC). This data collection pipeline followed a three-step process. First, we extracted the title, which is usually a single-sentence short summary of the information being fact-checked, and the factchecking site\u2019s URL. This step yielded a total of 22,405 unique fact-checks. We then proceeded to retrieve the social media post and the associated claim review if they were available on the factchecking site. Due to the collected posts having already undergone fact-checking and containing misleading claims, a significant number of them were unavailable for inclusion in our dataset. Moreover, a significant number of the posts only contained images or videos, which were unsuitable for our task at hand. As a result, we were left with a considerably smaller number of relevant instances. We also noted that in certain instances, the title in the Google Fact-Check Explorer and the claim review were identical; consequently, we included only one in the final dataset.\nThe ClaimReview Schema. We targeted the ClaimReview Schema elements with an entry for reviewed items as they were relevant to our requirements. Out of 44,478 entries, only 22,428 had this particular field. Therefore, we had to filter out the remaining entries. Next, we extracted all the links to social media posts and their corresponding claim reviews provided by the fact-checkers.\n5https://toolbox.google.com/factcheck/explorer 6https://schema.org/ClaimReview\nAs mentioned above, we only processed textual claims and excluded other modalities, such as audio or video. Further, we ensured that all the entries were in English."
        },
        {
            "heading": "3.2 Data Statistics and Analysis",
            "text": "By using both of these data collection methods and by exercising careful consideration, we curated a total of 6,388 instances. To ensure the creation of a diverse and high-quality test set, we chose posts that comprised not one, but two reference normalized claims (c.f. Sec 3.1). This enabled us to capture different aspects and perspectives of the normalized claims by including multiple references, thereby increasing the test set\u2019s robustness and reliability. Representative examples from our dataset are shown in Table 1, and the final dataset statistics are shown in Table 2."
        },
        {
            "heading": "Dataset Train Val Test Overall",
            "text": "Figure 2 shows an analysis of the cosine similarities between the social media posts and the corresponding normalized claims. We can see that the cosine similarities are consistently low for most examples, demonstrating that claim normalization involves more than just summarizing the social media post. This highlights the need for a specialized effort to accurately identify, extract, and normalize the claims within social media posts."
        },
        {
            "heading": "4 Proposed Approach",
            "text": "In this section, we explain our proposed approach, Check-worthiness Aware Claim Normalization (CACN), which aims to integrate task-specific information with large language models (LLMs). We focus our experiments on GPT-3 (text-davinci-003) (Brown et al., 2020). Our approach amalgamates two key ideas: (i) chain-of-thought prompting and (ii) reverse check-worthiness.\nChain-of-Thought Prompting. The realm of chain-of-thought (CoT) prompting has emerged as a veritable tour de force within LLMs (Wei et al., 2022). Instead of undergoing the laborious process of fine-tuning individual model checkpoints for every new task, we use CoT to navigate the complexity of claim normalization by using step-by-step reasoning. To accomplish this, we use claim checkworthiness, as described in the following subsection. This enables the model to iteratively enhance its comprehension and effectively generate precise normalized claims while eliminating the need for extensive fine-tuning.\nOur proposed prompt example is shown in Figure 3. Chain-of-thought approaches a complicated problem by efficiently breaking it into a sequence of simpler intermediate stages.\nReverse Check-Worthiness. The idea about reverse check-worthiness originates from the task of check-worthiness estimation, which in turn is an integral part of the manual fact-checking process (Nakov et al., 2021b). We leverage checkworthiness to steer the model\u2019s attention toward salient and pertinent information. By giving the model the ability to produce rationales in natural language that clearly explain the sequence of reasoning stages leading to the solution, we strengthen its capacity for cognitive reasoning with unwavering efficacy. Based on prior research on claim check-worthiness (Barr\u00f3n-Cede\u00f1o et al., 2018; Shaar et al., 2021; Nakov et al., 2022b), we direct our model to prioritize claims that meet specific criteria within the given social media post. These criteria include identifying claims within social media posts that (i) contain verifiable factual claims, (ii) have a higher likelihood of being false, (iii) are of general public interest, (iv) are likely to be harmful, and (v) are worth fact-checking. For instance, in Figure 3, the claim normalization process begins by identifying the central claim within the input social media post. Subsequently, we reckon the claim\u2019s verifiability, i.e., whether it is selfcontained and verifiable (e.g., as opposed to not containing a claim or expressing an opinion, etc.). We further evaluate the likelihood of the claim being false and its overall check-worthiness. This step-by-step process ensures a comprehensive analysis of the central claim\u2019s characteristics, allowing for effective claim normalization. By incorporating these aspects into our approach, we aim to improve the model\u2019s ability to identify and prioritize claims that require scrutiny and verification."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "Baseline Models. For comparison, we use several state-of-the-art generative systems and categorize them into two groups: (i) Pre-trained Large Language Models (PLMs): T5 (Raffel et al., 2020), BART (Lewis et al., 2020), FLAN-T5 (Chung et al., 2022), and PEGASUS (Zhang et al., 2020). For T5, BART, and FLAN-T5, we use base and large model sizes. For PEGASUS, we use the reddit model. (ii) In-context Learning Model: GPT-3 (text-davinci-003) (Brown et al., 2020).\nEvaluation Measures. To evaluate lexical overlap, we use ROUGE (1, 2, L) and BLEU-4 (Papineni et al., 2002). We further use METEOR (Banerjee and Lavie, 2005) and BERTScore (Zhang et al., 2019) to assess the similarity between the gold and the generated normalized claims.\nZero-Shot Learning. Zero-shot learning aims to apply the previously acquired capabilities of PLMs to similar tasks in a low domain. We hereby assess its suitability for the claim normalization task.\nFew-Shot Learning. We adopt few-shot learning with 10, 20, 50, and 100 training examples. This gradual exposure to additional labeled data aims to enhance the models\u2019 ability to generate accurate and contextually appropriate normalized claims.\nPrompt Tuning. Prompt-tuning entails adding a specific prefix to the model\u2019s input customized to the downstream tasks (Zhang et al., 2023). We investigate the impact of affixing different prompts to the given posts on the performance of T5-based and GPT-3 models. To exert control over the generated normalized claims, we use five control aspects:\ntokens, abstractness, number of sentences, claimcentricity, and entity-centricity. A comprehensive description of all these prompts is given in the Appendix (A.2).\nIn-Context Learning. LLMs have the remarkable ability to tackle diverse tasks with a minimal amount of examples given in-context learning prompts (Brown et al., 2020). We use GPT-3 (text-davinci-003) with three different prompts: (i) direct prompt (DIRECT), (ii) question-guided prompt (Q-GUIDED), and (iii) zero-shot chain-ofthought (ZS-CoT). Detailed prompt templates are given in Appendix A.3."
        },
        {
            "heading": "6 Experiments and Evaluation",
            "text": "Our experiments reveal that our CACN outperforms all baselines across most evaluation measures. We further examine all systems aiming to answer the research questions listed below."
        },
        {
            "heading": "Do meticulously crafted prompts enhance the",
            "text": "performance of generative models? The findings exhibit a significant performance improvement when using prompt-tuning, specifically with incontext examples. Table 3 shows the effectiveness of various prompts across all evaluation measures. However, a notable enhancement of approximately 2\u20133 points absolute is observed for all semantic measures when transitioning from conventional prompts to our proposed approach when using the same in-context examples. This emphasizes the importance of our framework tailored for the specific task. Moreover, an upsurge in ROUGE-F1 scores (1, 2, and L) emphasizes the resemblance between the generated normalized claims and such created by humans. This, in turn, validates the incorporation of the \u201creverse check-worthiness\u201d chain-ofthought process, which effectively integrates taskspecific information into the generative system. We also attempt prompt-tuning in a zero-shot setup; the results are shown in the Appendix (A.2). To summarize, the deliberate design of prompts, along with in-context learning, substantially enhances the performance of generative models."
        },
        {
            "heading": "Is training models on a specific task less effective",
            "text": "than in-context learning with a few examples? We observe substantial disparities in the performance of models trained on task-specific data compared to using in-context learning with a limited number of examples, as shown in Table 3.\nWe can see that the models exposed to in-context examples showcase superior performance, highlighting their efficacy in capturing task-specific patterns. While the trained models exhibit excellence in lexical metrics, their performance in semantic metrics is noticeably lower. Notably, BARTLARGE , trained on our dataset, outperforms other trained models by sizable margins. These results strongly underline that, within the realm of LLMs, incorporating prompt-tuning with incontext learning holds more promise, leading to enhanced generalization capabilities."
        },
        {
            "heading": "Do models demonstrate inherent proficiency in",
            "text": "generating normalized claims with minimal or no prior training? We examine the potential benefits of zero-shot and few-shot learning to investigate the inherent proficiency in generating normalized claims. The zero-shot and the few-shot results are shown in Table 4. Zero-shot learning, which relies solely on the pre-trained language model without any task-specific fine-tuning, performs quite well. On the other hand, few-shot learning does not result in significant improvements. Surprisingly, the models trained using few-shot learning perform slightly worse than zero-shot learning, where the models have no exposure to task-specific data. After training on ten examples, the performance of FLAN-T5LARGE drops by 6 BERTScore points absolute, and it continues to decline as more examples are provided.7 This unexpected result suggests that few-shot learning may be unsuitable for this intricate and complex task. The limited number of examples provided during few-shot learning may have been insufficient for the models to generalize and capture the underlying patterns of normalized claims effectively. Moreover, introducing task-specific data might have introduced conflicting information as these models were never trained on this task, leading to a degradation in performance."
        },
        {
            "heading": "7 Qualitative Analysis",
            "text": "Error Analysis. To comprehend the performance of CACN, we strive to qualitatively analyze the errors committed by our model in this section. Table 5 shows some randomly selected instances from our test dataset, along with gold normalized claims and predictions from CACN. For comparison, we also show predictions from two best-performing baselines, BARTLARGE and DIRECT.\n7See Appendix (A.4) for 50-shot and for 100-shot results.\nNaturally, the predictions in the fine-grained analysis are much more intricate than in the coarsegrained quantitative setup. During our manual qualitative analysis, we unveiled several interesting patterns and errors in the generated responses. For example, although BARTLARGE generated responses with a high BERTScore in example 1, we noticed that the factual alignment is incorrect, making this model untrustworthy for downstream tasks such as claim check-worthiness and claim verification. In contrast, our proposed model produced a response that is both correct and precise. The response generated by DIRECT is also accurate, but it is excessively long, which contradicts the objective of the normalized claims being concise and straightforward.\nThis problem is also evident in example 3, where DIRECT produces a factually correct claim but is overly long.\nIn example 2, we observe that the BARTLARGE model demonstrates the lowest number of hallucinations and adheres closely to the input social media post. In contrast, our model\u2019s BERTScore performed the worst for this example. However, upon closer inspection, we noticed that the normalized claim that our model generated was indeed correct and most relevant for fact-checking. These findings highlight the complexity and the trade-offs involved in generating normalized claims. While certain models may excel in certain cases, there is often a compromise in other aspects, such as factual accuracy and conciseness.\nHuman Evaluation. We conducted an extensive human evaluation to assess the linguistic proficiency of the generated normalized claims. Building upon the measures proposed by van der Lee et al. (2021), we evaluated the generated claims based on four aspects: fluency, coherence, relevance, and factual consistency.8 We further introduced the parameter of self-contextualization to measure the extent to which the normalized claims included the necessary context for fact-checking within themselves. Each of these measures played a unique and vital role in evaluating the quality of the generated claims. To conduct the evaluation, we randomly selected 50 instances from our test set and assigned five human evaluators to rate every normalized claim on a scale of 1 to 5 for each of these five aspects. All evaluators were fluent English speakers with a Bachelor\u2019s or Master\u2019s degree. To ensure reliability, each example was evaluated by all five evaluators independently, and then we averaged their scores.\n8See Appendix A.6 for more detail.\nThe average scores are presented in Table 6. For comparison, we also included the results from the best-performing baseline systems, namely BARTLARGE and DIRECT. Our analysis reveals that the outputs generated by CACN exhibit qualitative superiority compared to the baseline systems across all dimensions."
        },
        {
            "heading": "8 Conclusion and Future Work",
            "text": "We introduced the novel task of claim normalization, which holds substantial value on multiple fronts. For human fact-checkers, claim normalization is a useful tool that can assist them in effectively removing superfluous texts from subsequent processing. This also benefits downstream tasks such as identifying previously fact-checked claims, estimating claim check-worthiness, etc. We further compiled a dataset of social media posts comprising over 6k posts and their normalized claims. We further benchmarked this dataset with a novel approach, CACN, and showed its superior performance compared to different state-of-the-art generative models across multiple assessment measures. We also documented our data collection process, providing valuable insights for future research in this domain. In future work, we plan to extend the dataset, including with new languages. We also plan to use more powerful LLMs."
        },
        {
            "heading": "Limitations",
            "text": "While our study has made major contributions to claim normalization, it is critical to recognize and to address its potential limitations. During our data collection process, we excluded claims about images and videos. Yet, we believe that including multimodal information may help improve claim normalization. Another key problem is that each fact-checking organization adheres to its own set of editorial norms, procedures, and subjective interpretations of claims. These variations in writing style and judgments make it challenging to establish a standardized claim normalization. Addressing this issue will necessitate attempts to develop consensus or guidelines among fact-checking organizations in order to ensure greater consistency and coherence in claim normalization. By acknowledging and addressing these limitations, we may endeavor to improve the reliability and soundness of claim normalization systems in the future."
        },
        {
            "heading": "Ethics and Broader Impact",
            "text": "Data Bias. It is important to acknowledge the possibility of biases within our dataset. Our data collection process involves gathering normalized claims from multiple fact-checking sites, each with its own set of editorial norms, procedures, and subjective interpretations. These elements can introduce systemic biases that impact the overall assessment of normalized claims. However, it is important to acknowledge that these biases are beyond our control.\nEnvironmental Footprint. Large language models (LLMs) require a substantial amount of energy for training, which can contribute to global warming (Strubell et al., 2019). Our proposed approach, on the other hand, leverages few-shot in-context learning rather than training models from scratch, leading to a lower carbon footprint. It is worth mentioning that using LLMs for inference still consumes a considerable amount of energy, and we are actively seeking to reduce it by using more energy-efficient techniques.\nBroader Impact and Potential Use. Our model can interest the general public and save time for human fact-checkers. Its applications extend beyond fact-checking to other downstream tasks such as detecting previously fact-checked claims, claim matching, and even estimating claim checkworthiness of new claims."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Task Motivation",
            "text": "Claim normalization holds significant promise for combating the spread of misinformation by streamlining fact-checking processes and enhancing the reliability of retrieved evidence. To substantiate our hypothesis regarding the effectiveness of claim normalization, we conducted a well-structured retrieval experiment using the Google API. The objective was to demonstrate the practical benefits of claim normalization in assisting fact-checkers. We randomly selected a sample of 35 instances from our dataset, encompassing social media posts and their normalized claims. Leveraging the capabilities of the Google API, we sought the top-5 most relevant articles for each post and its normalized claim. In a meticulous evaluation process, three annotators individually assessed the relevance (0 or 1) of each retrieved article to the input (post or normalized claim). We then used majority voting to determine the final relevance score for each retrieved article. As depicted in Table 7, the results of our experiment consistently demonstrated the advantage of normalized claims in evidence retrieval. In top-k precision evaluations for various values of k (1, 3, and 5), normalized claims consistently outperformed their corresponding source posts. This observation indicates that claim normalization is not merely a theoretical concept, but significantly enhances the efficiency of evidence retrieval, resulting in more concise and effective tools for aiding the fact-checking process."
        },
        {
            "heading": "A.2 Prompt-Tuning",
            "text": "Raffel et al. (2020) demonstrated that prompttuning could enable controllable text generation in T5-based models. We also investigate the impact of affixing different prompts to the given input on the performance of T5-based models for normalized claim generation, along with GPT-3 (text-davinci-003) (Brown et al., 2020). We experimented with various prompts suffixed to the input text before inference, in a zero-shot setting. We discuss our different prompts Pi below.\nUncontrolled. We investigated the use of the traditional prompt \u2018summarize\u2019 for uncontrollable models. This prompt lacks specific control signals, making it uncontrolled as it does not provide explicit guidance with specific attributes.\nToken Limit. We found that normalized claims written by fact-checkers typically adhered to around ten words, as shown in Figure 4. Thus, in order to control the length of the normalized claims, we used the following prompt: \u201csummarize within the length of 10 tokens\u201d.\nAbstractness. Abstractness quantifies how much the generated text\u2019s words and phrases differ from those extracted directly from it: a fully abstractive summary expresses the central points of the input in very different words and sequences of words compared to the original input. Precisely, the more n-grams overlap between a summary and its original document, the less abstractive a summary is. Thus, in order to control the abstractness of the generated normalized claims, we use the prompt \u201csummarize with abstractness of a\u201d, where a represents a value within the range [0;1], denoting the desired level of abstractness. Inspired by Dreyer et al. (2023), we compute the abstractness score ai for each pair of a post pi and a normalized claim si, and then we take an average across all examples:\nai = (1\u2212X(pi, si)) (1)\nwhere X is the harmonic mean of unigram overlaps precision, bi-gram overlap precision, and longest sub-sequence overlap precision. We found the average abstractness (a) to be around 0.8.\nSingle Sentence. The normalized claim written by fact-checkers often consists of a concise singlesentence summary of the post. We use the prompt \u201csummarise in one sentence\u201d in order to limit the normalized claims to a single-sentence summary, ensuring brevity and conciseness in delivering the pivotal assertion.\nClaim-Centricity. The task at hand is more than just text summarization; it transcends conventional text summarization by seeking not only to condense the input social media post, but also to discern and to encapsulate the central claim within that input post concisely. Thus, we use the prompt \u201csummarize the text identifying the central assertion\u201d to helm the model to focus on the main assertion in the input text.\nEntity-Centricity. Similarly to the claim-centric prompt, we investigated the technique of creating entity-centered summaries using the prompt, \u201csummarize the text focusing on the given keywords (kw1, kw2, kw3, ...kwn)\u201d. For this approach, we use Open Information Extraction (Angeli et al., 2015) in order to extract subject\u2013verb\u2013object triples from the input texts.9 Subsequently, we compile a keyword list encompassing all subjects and all objects within the text. The objective is to direct the model to produce summaries that align well with the subjects and the objects mentioned in the input text.\nResults. We report the results for our zeroshot prompt-tuning experiments in Table 8. We can see that for the T5-based models, prompttuning did not yield any major improvements; rather, it decreased the performance as compared to the uncontrolled prompt. However, GPT-3 (text-davinci-003) showed some improvements when using these prompts.\n9https://github.com/philipperemy/ Stanford-OpenIE-Python\nA.3 In-Context Learning Templates\nIn Figure 5, we show the templates used for the three in-context learning methods used for GPT-3 (text-davinci-003) as mentioned in Section 5."
        },
        {
            "heading": "A.4 Few-Shot Additional Results",
            "text": "We report 50-shot and 100-shot experimental results in Table 9. Interestingly, we observe that introducing more examples to the model did not help it much.\nA.5 Implementation Details\nWe performed basic data cleaning, e.g., removing non-alphanumeric characters, removing links and hashtags, etc. on our dataset CLAN, using nltk. For a standardized evaluation, we relied on widely recognized evaluation libraries such as py-rouge,10 nltk-bleu,11 nltk-meteor,12 and hugging-face bertscore.13 We trained all models for 50 epochs, with early stopping based on validation loss. We set the patience value at 5, and we optimized the models using the Adam optimizer. We set the weight decay to 0.01. For our proposed approach CACN, we used GPT-3 (text-davinci-0003) as base model. Finally, we set the maximum length of the generated response to 120 with a temperature of 0.6.\n10https://pypi.org/project/py-rouge/ 11https://www.nltk.org/_modules/nltk/translate/\nbleu_score.html 12https://www.nltk.org/api/nltk.translate. meteor_score.html 13https://huggingface.co/spaces/ evaluate-metric/bertscore"
        },
        {
            "heading": "A.6 Human Evaluation Criteria",
            "text": "Following van der Lee et al. (2021), we define the four human evaluation measures as follows:\n1. Fluency: measures the linguistic proficiency exhibited by the generated responses;\n2. Coherence: evaluates the intrinsic structure and the organization of the generated normalized claims;\n3. Relevance: appraises the discerning selection of contextually appropriate content within the generated response;\n4. Factual consistency: examines the intricate alignment between the factual accuracy of the generated response and the source text."
        }
    ],
    "title": "From Chaos to Clarity: Claim Normalization to Empower Fact-Checking",
    "year": 2023
}