{
    "abstractText": "The NLP community has long advocated for the construction of multi-annotator datasets to better capture the nuances of language interpretation, subjectivity, and ambiguity. This paper conducts a retrospective study to show how performance scores can vary when a dataset expands from a single annotation per instance to multiple annotations. We propose a novel multi-annotator simulation process to generate datasets with varying annotation budgets. We show that similar datasets with the same annotation budget can lead to varying performance gains. Our findings challenge the popular belief that models trained on multi-annotation examples always lead to better performance than models trained on single or few-annotation examples.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pritam Kadasi"
        },
        {
            "affiliations": [],
            "name": "Mayank Singh"
        }
    ],
    "id": "SP:9a56e63f3bff446b9a670f19afe550b89c132741",
    "references": [
        {
            "authors": [
                "Fan Bai",
                "Alan Ritter",
                "Wei Xu."
            ],
            "title": "Pre-train or annotate? domain adaptation with a constrained budget",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5002\u20135015, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Valerio Basile",
                "Michael Fell",
                "Tommaso Fornaciari",
                "Dirk Hovy",
                "Silviu Paun",
                "Barbara Plank",
                "Massimo Poesio",
                "Alexandra Uma."
            ],
            "title": "We need to consider disagreement in evaluation",
            "venue": "Proceedings of the 1st Workshop on Benchmarking: Past, Present and",
            "year": 2021
        },
        {
            "authors": [
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "arXiv preprint arXiv:1908.05739.",
            "year": 2019
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Derek Chen",
                "Zhou Yu",
                "Samuel R. Bowman."
            ],
            "title": "Clean or annotate: How to spend a limited data collection budget",
            "venue": "Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing, pages 152\u2013168, Hybrid. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Aida Mostafazadeh Davani",
                "Mark D\u00edaz",
                "Vinodkumar Prabhakaran."
            ],
            "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
            "venue": "Transactions of the Association for Computational Linguistics, 10:92\u2013110.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael Macy",
                "Ingmar Weber."
            ],
            "title": "Automated hate speech detection and the problem of offensive language",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, 11(1).",
            "year": 2017
        },
        {
            "authors": [
                "Emily Denton",
                "Mark D\u00edaz",
                "Ian Kivlichan",
                "Vinodkumar Prabhakaran",
                "Rachel Rosen"
            ],
            "title": "Whose ground truth? accounting for individual and collective identities underlying dataset annotation",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Yejin Choi",
                "Swabha Swayamdipta"
            ],
            "title": "Understanding dataset difficulty with V-usable information",
            "year": 2022
        },
        {
            "authors": [
                "S\u00f8gaard."
            ],
            "title": "Challenges and strategies in crosscultural NLP",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013, Dublin, Ireland. Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Jigsaw."
            ],
            "title": "Toxic comment classification challenge",
            "venue": "Accessed: 2021-05-01.",
            "year": 2018
        },
        {
            "authors": [
                "Artur Kulmizev",
                "Joakim Nivre."
            ],
            "title": "Investigating UD treebanks via dataset difficulty measures",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1076\u20131089, Dubrovnik, Croatia. As-",
            "year": 2023
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Johannes Mario Meissner",
                "Napat Thumwanit",
                "Saku Sugawara",
                "Akiko Aizawa."
            ],
            "title": "Embracing ambiguity: Shifting the training target of NLI models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Nie",
                "Xiang Zhou",
                "Mohit Bansal"
            ],
            "title": "What can we learn from collective human opinions on natural language inference data",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Mihir Parmar",
                "Swaroop Mishra",
                "Mor Geva",
                "Chitta Baral"
            ],
            "title": "Don\u2019t blame the annotator: Bias already starts in the annotation",
            "year": 2023
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Tom Kwiatkowski."
            ],
            "title": "Inherent Disagreements in Human Textual Inferences",
            "venue": "Transactions of the Association for Computational Linguistics, 7:677\u2013694.",
            "year": 2019
        },
        {
            "authors": [
                "Barbara Plank."
            ],
            "title": "The \u201cproblem\u201d of human label variation: On ground truth in data, modeling and evaluation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10671\u201310682, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Vinodkumar Prabhakaran",
                "Aida Mostafazadeh Davani",
                "Mark Diaz."
            ],
            "title": "On releasing annotator-level labels and information in datasets",
            "venue": "Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR)",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "year": 2020
        },
        {
            "authors": [
                "Yejin Choi"
            ],
            "title": "Dataset cartography: Mapping",
            "year": 2020
        },
        {
            "authors": [
                "yarajh"
            ],
            "title": "2022) extended this measure to a new measure called Pointwise V-Information (PVI)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The process of creating datasets often involves practical constraints such as time, resources, and budget that limit the number of annotators or experts available for collecting annotations (Sheng et al., 2008). As a result, there is a prevalence of single or few labels per instance (depending on the limited number of annotators) in the collected data. However, training models on these datasets pose challenges to their generalization abilities, primarily because the data lacks diversity. With a scarcity of different perspectives and variations in the training data (Basile et al., 2021; Plank, 2022), models may struggle to learn robust representations and fail to generalize effectively (Nie et al., 2020; Meissner et al., 2021).\nTo address these challenges, the NLP community has highlighted the advantages of utilizing multi-annotator datasets (Davani et al., 2022) and also emphasized the importance of releasing multiannotator datasets and associated information (cultural and demographic, etc.) (Sap et al., 2022; Hershcovich et al., 2022). However, this approach introduces its own set of challenges. Collecting\ndata with multiple annotators requires significant time, annotation budget, and annotator expertise to ensure the creation of high-quality datasets with diverse perspectives.\nMoreover, with a limited annotation budget, it becomes crucial to determine the optimal number of annotators within the given constraints. This not only helps save annotation time and budget but also ensures efficient utilization of available resources. While some research (Wan et al., 2023; Zhang et al., 2021) has provided insights and suggestions on finding the optimal number of annotators, a definitive solution to this problem has yet to be achieved.\nAnother challenge is the restricted number of annotations available per instance, typically not exceeding 6 \u2013 10, even with a large number of recruited annotators (Plank, 2022). This limitation arises from the considerable annotation efforts required for a large volume of instances. As a result, when models are trained on such datasets, they only capture the opinions and information of a small subset of the annotator pool. Additionally, certain datasets have not released annotator-specific labels or established mappings to individual annotators (Nie et al., 2020; Jigsaw, 2018; Davidson et al., 2017). However, the trend is gradually shifting, and there is a growing recognition that annotatorlevel labels should be made available (Prabhakaran et al., 2021; Basile et al., 2021; Denton et al., 2021).\nThis study aims to tackle the challenge of lacking annotator-specific labels by simulating a multiannotation process. Through this study, we provide insights into how the inclusion of more annotators can introduce variations in model performance and identify the factors that influence this variation. Considering that previous research (Swayamdipta et al., 2020) has highlighted the influence of individual instance difficulty on model performance, we examine how the addition of more annotations alters the difficulty level of instances and conse-\nquently affects model performance. In summary, our main contributions are: \u2022 We propose a novel multi-annotator simula-\ntion process to address the issue of missing annotator-specific labels. \u2022 We demonstrate, that increasing the number of annotations per instance does not necessarily result in significant performance gains. \u2022 We also demonstrate, that altering the number of annotations per instance has a noticeable impact on the difficulty of instances as perceived by the model and consequently affects the model performance."
        },
        {
            "heading": "2 The Multi-annotated Dataset",
            "text": "In practical scenarios, the annotation process begins by hiring one or more annotators who annotate each instance in the dataset. To enhance the representation of the true label distribution, we have the option to extend this process by recruiting additional annotators. We continue this iterative process until either the annotation budget is exceeded or we observe saturation in the model\u2019s performance in predicting the true label distribution. As a result, we obtain multiple annotations assigned to each instance in this multi-annotated dataset.\nA multi-annotator dataset D is formally characterized as a triplet D = (X,A, Y ) in this research paper. The set X represents N text instances, denoted as x1, x2, . . . , xN . The set A corresponds to M annotators, represented as a1, a2, . . . , aM . The annotation matrix Y captures the annotations, with rows indexed by X and columns indexed by A. Specifically, Y = Y [X;A] = Y [x1, x2, . . . , xN ; a1, a2, . . . , aM ]. In simpler terms, the entry Y [xi; aj ] stores the label yi,j assigned to instance xi by annotator aj . Furthermore, an annotator-set Ak, which comprises k annotators where 1 \u2264 k \u2264 M , is defined. Consequently, the subset of D restricted to Ak is denoted as Dk = (X,Ak, Y \u2032 ), where Y \u2032 = Y [X;Ak]. This paper refers to Dk as the dataset subset with k annotations per instance. Figure 1 illustrates a toy multi-annotator dataset, showcasing M annotators, and N instances along with its subsets comprising 2 and k annotators."
        },
        {
            "heading": "3 Simulating the Multi-annotation Process",
            "text": "Based on our current knowledge, it is worth noting that existing multi-annotator datasets typically\ndo not include annotator-specific labels. Instead, the available information is limited to the label distribution for each instance (Nie et al., 2020; Jigsaw, 2018; Davidson et al., 2017). For instance, in cases with M annotations per instance and three possible labels, the label distribution is commonly represented by a list [p, q, r], where p, q, and r are positive integers that sum up to M . To address this constraint, we introduce a simulation process for multi-annotator scenarios that leverages the instance-level label distribution. Our proposed approach (see Algorithm 1), encompasses the following steps:\n\u2022 Initially, we generate a list of annotations for each instance by considering the actual instance-level label distribution. [Line 1] \u2022 Subsequently, we randomize these annotation lists using a consistent random seed across instances. [Lines 5\u20136] \u2022 Next, we select the first k annotations from each randomized list, creating the dataset subset Dk. [Lines 4\u20138]\nBy employing this algorithm, we can generate k annotations per instance, thereby addressing the limitation of annotator-specific labels in existing multi-annotator datasets. By repeating the algorithm with different random seeds or parameters, we can create multiple datasets subsets Dk, each containing k annotations per instance. This flexibility enables the generation of diverse subsets, expanding the range of multi-annotator scenarios that can be explored and analyzed in our research."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We selected the ChaosNLI dataset (Nie et al., 2020) for our study, as it contains the highest number of\nAlgorithm 1 Creation of Annotator Datasets Input: X: set of N instances\nCL: list of C class labels LC: label counts of shape N \u00d7 C M : number of annotators\nOutput: D\u2032 = {D1,D2, . . . ,DM} 1: AL\u2190 GETANNOTATIONLIST() 2: Initialize an empty set D\u2032\n3: for k \u2190 1 to M do 4: Initialize an empty list Y \u2032 5: for i\u2190 1 to N do 6: SL\u2190 RANDOMSHUFFLE(AL[i]) 7: Choose first k annotations from AL\nand add it to Y \u2032\n8: end for 9: Dk \u2190 (X,Y \u2032 )\n10: Add Dk to D \u2032 11: end for 12: Return D\u2032\nannotations (=100) per instance among the publicly available datasets (Plank, 2022). ChaosNLI is a Natural Language Inference (NLI) task dataset known for its high ambiguity. Additionally, the ChaosNLI dataset includes sub-datasets, namely ChaosNLI-S and ChaosNLI-M, which are subsets extracted from the development sets of SNLI (Bowman et al., 2015) and MNLI-matched(Williams et al., 2018), respectively. Another sub-dataset, ChaosNLI-\u03b1, is created from the entire development set of AbductiveNLI hereafter, referred to as \u03b1-NLI (Bhagavatula et al., 2019).\nThe ChaosNLI dataset consists of 4,645 instances, each annotated with 100 new annotations. Additionally, the dataset already includes 5 old annotations for ChaosNLI-S and ChaosNLI-M, and 1 old annotation for ChaosNLI-\u03b1. Subsequently, we create Dk\u2019s (see \u00a73) utilizing these datasets and then divide these Dk\u2019s into train, development, and test sets using an 80:10:10 ratio. Table 1 provides detailed statistics of the datasets used in our study."
        },
        {
            "heading": "4.2 Pretrained Language Models (PLMs)",
            "text": "In our study, we utilize all the pretrained language models (PLMs) reported in the ChaosNLI work by Nie et al. (2020). Specifically, we experiment with BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2020), ALBERT (Lan et al., 2020), and DistilBERT (Sanh et al., 2020). It is important to clarify that our objective is not to showcase state-of-the-art (SOTA) performance using these models, but rather to demonstrate the variations in performance as we incrementally add annotations to the dataset."
        },
        {
            "heading": "4.3 Training Strategies",
            "text": "In this section, we describe two variants of training strategies. Majority Label (ML): The PLMs are finetuned using the majority label, which is determined by aggregating annotations from the target list of annotations. The training objective aims to minimize the cross-entropy between the output probability distribution and the one-hot encoded majority label. Label Distribution (LD): The PLMs are finetuned using the label distribution from the target list of annotations (Meissner et al., 2021). The training objective aims to minimize the cross-entropy between the output probability distribution and the target label distribution."
        },
        {
            "heading": "4.4 Evaluation",
            "text": "To evaluate the performance of our models, we utilize the classification accuracy computed on the test dataset. In the ML setting, the accuracy is computed by comparing the label associated with the highest softmax probability predicted by the model with the majority label derived from the target annotations. In the LD setting, the accuracy is computed by comparing the label corresponding to the highest softmax probability predicted by the model with the label that has the highest relative frequency in the target label distribution."
        },
        {
            "heading": "4.5 Experimental Settings",
            "text": "Following the approaches described in the studies (Nie et al., 2020; Meissner et al., 2021), we construct base models by finetuning PLMs (described in \u00a74.2) on the combined train sets of SNLI and\n1#Instances corresponding to SNLI, MNLI and \u03b1-NLI are of train set as only train set is used for training base models in our study.\nMNLI for both ChaosNLI-S and ChaosNLI-M. For the ChaosNLI-\u03b1 dataset, we construct base models by finetuning on the train set of \u03b1-NLI. We further finetune these base models with increasing sizes of annotators. Specifically, we finetune models for each Dk, where k \u2208 [1, 100]. For each k, we report average performance scores over test sets of 10 Dk\u2019s (see \u00a73)\nWe choose hyperparameters from the experimental settings of the following work (Nie et al., 2020; Meissner et al., 2021; Bhagavatula et al., 2019). Our optimization technique involves employing the AdamW optimizer (Loshchilov and Hutter, 2019). More details on hyperparameters can be found in \u00a7A.2. To ensure reproducibility, we conduct our experiments using the open-source Hugging Face Transformers2 library (Wolf et al., 2020). Furthermore, all experiments are performed using 2 \u00d7 NVIDIA RTX 2080 Ti GPUs."
        },
        {
            "heading": "5 Results and Discussion",
            "text": ""
        },
        {
            "heading": "5.1 Is higher performance always guaranteed by increasing the number of annotations?",
            "text": "Figure 2 presents the accuracy scores as the number of annotations increases. Notably, the trends observed in the performance of ChaosNLI-S, ChaosNLI-M, and ChaosNLI-\u03b1 challenge the prevailing belief that increased annotations invariably lead to improved performance. Specifically, for ChaosNLI-S and ChaosNLI-M, the accuracy scores exhibit a non-monotonic increasing pattern. In contrast, the trend observed for ChaosNLI-\u03b1, particularly with BERT and DistilBERT models, deviates from this expected behavior. In these cases, the accuracy scores show a decreasing trend as the number of annotations increases. Upon examining the RoBERTa accuracy scores for the LD setting\n2https://huggingface.co/docs/transformers/\nin ChaosNLI-S, it is observed that the performance reaches a saturation point between 20 to 80 annotations. This means that increasing the number of annotations beyond this range does not result in significant improvement in the accuracy scores.\nTable 2 provides a complementary perspective on the observed trends. It highlights that the minimum performance is not consistently associated with the dataset having the fewest annotations, and vice versa. In the case of ChaosNLI-\u03b1 with BERT and DistilBERT, it is interesting to note that the optimal performance is achieved with just three annotations. This represents an extreme scenario where a minimal number of annotations can lead to the best performance. In general, these findings shed light on the optimization of our annotation budget. Similarly, the performance gain (maximum - minimum accuracy) across different datasets also significantly varies. The average performance gain for ChaosNLI-M, ChaosNLI-S and ChaosNLI-\u03b1 is 0.106, 0.177, and 0.031, respectively. The notable variability in performance gain across different datasets further emphasizes that the impact of increasing annotations on performance improvement is not consistent. It underscores the need to carefully analyze and understand the specific characteristics of each dataset and model combination to ascertain the relationship between annotation quantity and performance.\nTo provide an explanation for the observed complex behavior, we utilize the V-Information (Ethayarajh et al., 2022). V-information is a measure that quantifies the ease with which a model can predict the output based on a given input. The higher the Vinformation, the easier it is for the model to predict the output given input. Furthermore V-information cannot be negative unless model overfits, etc. (see \u00a7A.1).\nFigure 3 provides a visual representation of\nthe V-information scores for the three datasets across five different PLMs. As anticipated, the Vinformation scores are higher for the ChaosNLI-S and ChaosNLI-M datasets. Models that exhibit higher V-information scores also tend to yield higher accuracy scores in the LD-based performance evaluation. For instance, RoBERTa outperforms other models (except XLNet, for which the performance is similar) in terms of accuracy for the ChaosNLI-S dataset. The saturation of V-information scores starting at k = 20 for the ChaosNLI-S dataset effectively explains the observed saturation of LD-based accuracy after 20 annotations, as depicted in Figure 2. This phenomenon suggests that the model reaches a point where additional annotations provide diminishing returns in terms of extracting valuable insights from the instances. Therefore, the model\u2019s performance ceases to improve significantly beyond this threshold. For the ChaosNLI-\u03b1 dataset, except RoBERTa and XLNet (V-Information \u2208 [0, 0.25], comparatively low), all models yielded approximately zero V-information scores3. This implies that adding\n3We used same hyperparameters for all k\u2019s due to which\nmore annotations to the ChaosNLI-\u03b1 dataset does not establish a clear relationship between the input and output label distribution. This observation suggests that, for this particular variant of the dataset, the model might rely on factors other than the provided annotations to make accurate predictions.\nThe aforementioned findings indicate that not all datasets yield similar performance when trained under the same budget, underscoring the importance of selecting the appropriate dataset for a specific task. Furthermore, these findings emphasize the significance of determining the optimal number of annotators, as the model\u2019s performance varies with the increase in annotations."
        },
        {
            "heading": "5.2 Does the number of annotations influence the difficulty of instances as perceived by the model?",
            "text": "To investigate this question, we employ the concept of dataset cartography as proposed by Swayamdipta et al. (2020), which leverages training dynamics to distinguish instances based on their (1) confidence, measured as the mean probability of the correct label across epochs, and (2) variability, represented by the variance of the aforementioned confidence. This analysis generates a dataset map that identifies three distinct regions of difficulty: easy-to-learn, hard-to-learn, and instances that are ambiguous with respect to the trained model. Easy-to-learn (e) instances exhibit consistently high confidence and low variability, indicating that the model can classify them correctly with confidence. hard-to-learn (h) instances, on the other hand, have low confidence and low variability, indicating the model\u2019s struggle to consistently classify\nmodels for k \u2264 3 overfitted resulting in negative VInformation.\nthem correctly over multiple epochs. Ambiguous (a) instances display high variability in predicted probabilities for the true label. We investigate the proportion of the transitions between these categories with the incorporation of additional annotations. For example, e\u2192 a represents proportion of the transitions from easy-to-learn to ambiguous category among all transitions. This provides valuable insights into the underlying factors that contribute to the observed improvements or lack thereof in the model\u2019s performance.\nFigure 4 illustrates an interesting pattern in ChaosNLI-S and ChaosNLI-M datasets: as the number of annotations increases, a significant proportion of training instances transition from the a \u2192 e category. For instance, more than 60% of all transitions between 1 to 10 annotations involve instances moving from the a\u2192 e category. However, beyond 10 annotations, the proportion of instances transitioning to the e from the a category does not show a substantial increase. On the other hand, the reverse transition from the e\u2192 a category is the second most common transition, with an average proportion of 20%. The difference in proportions between the transition from a \u2192 e and the transition from e \u2192 a becomes more substantial (at least 29%) as more annotations are added. In the ChaosNLI-M dataset, we observe a higher proportion of instances transitioning from category a to category h compared to the ChaosNLI-S dataset. Specifically, over 15% of the ambiguous instances in ChaosNLI-M exhibit a shift towards the hard region, which is more than 50% of similar transitions observed in ChaosNLI-S. We argue that this substantial difference in transition patterns has a direct impact on the performance of models on the ChaosNLI-S dataset compared to ChaosNLI-M.\nDespite the presence of higher proportions of a to e transitions in ChaosNLI-M compared to ChaosNLIS, the a to category h consistently leads to better performance on the ChaosNLI-S dataset across all models analyzed.\nChaosNLI-\u03b1 exhibits distinct trends across various models. Specifically, in the case of BERT and DistillBERT, where accuracy scores decline as the annotation increases (see Figure 2), we witness significant proportions of e\u2192 a (\u223c 80%) and a\u2192 h (\u223c 43%) transitions, respectively. These transitions suggest that the models struggle to comprehend\nthe instances and classify them with reduced confidence. For XLNet and ALBERT, the combined proportion of low confidence transitions, e \u2192 a and a \u2192 h either surpasses or remains equal to the proportion of high confidence transition a\u2192 e. In the case of RoBERTa, it behaves the same as ChaosNLI-S and ChaosNLI-M.\nThese results suggest adding more annotations has indeed its effects on the difficulty of instance thereby affecting the performance of the model."
        },
        {
            "heading": "6 Related Works",
            "text": "Human disagreements in annotations. Traditional approaches like majority voting or averaging can overlook important nuances in subjective NLP tasks, where human disagreements are prevalent. To address this issue, Multi-annotator models treat annotators\u2019 judgments as separate subtasks, capturing the distribution of human opinions, which challenges the validity of models relying on a majority label with the high agreement as ground truth (Davani et al., 2022; Nie et al., 2020). Human variation in labeling, which is often considered noise (Pavlick and Kwiatkowski, 2019), should be acknowledged to optimize and maximize machine learning metrics, as it impacts all stages of the ML pipeline (Plank, 2022). Incorporating annotation instructions that consider instruction bias (Parmar et al., 2023), which leads to the over-representation of similar examples, is crucial. This bias can limit model generalizability and performance. Future data collection efforts should focus on evaluating model outputs against the distribution of collective human opinions to address this issue. All of the above works study annotator disagreements and how they affect the performance of models on downstream tasks. However, in our work, considering disagreements\u2019 effect on model performance, we try to find out how the model performance varies as we increase the number of annotations per instance, i.e., varying the annotator disagreement, Overall, we try to answer, does more annotation per instance leads to better performance or is the other way around?\nAnnotation under restricted annotation budget. Also, prior studies have investigated how to achieve optimal performance in natural language processing (NLP) models under restricted annotation budgets. One such study by (Sheng et al., 2008) examined the impact of repeated labeling on the quality of data and model performance when labeling is imperfect and/or costly. Another study by (Bai et al., 2021) framed domain adaptation with a constrained budget as a consumer choice problem and evaluated the utility of different combinations of pretraining and data annotation under varying budget constraints. Another study by (Zhang et al., 2021) explored new annotation distribution schemes, assigning multiple labels per example for a small subset of training examples, and proposed a learning algorithm that efficiently\ncombines signals from uneven training data. Finally, a study by (Chen et al., 2022) proposed an approach that reserves a fraction of annotations to explicitly clean up highly probable error samples to optimize the annotation process. All these studies contribute to the understanding of how to maximize the performance of NLP models under restricted annotation budgets. Our study aimed to address a specific question within this context: assuming a fixed annotation budget, which dataset would yield the highest performance?\nPrevious studies have demonstrated that annotation disagreements affect model performance. However, our study aims to explore how performance varies as we change the level of disagreement. we consider ideas from (Zhang et al., 2021) who proposed a learning algorithm that can learn from training examples with different amounts of annotation (5-way, 10-way, 20-way) in a multilabel setting, but we expand the number of annotations from 1-way till 100-way and train our model in a label distribution setting rather than in a multi-label setting. To investigate the reasons for performance variation as we increase the number of annotations, we incorporate (Swayamdipta et al., 2020)\u2019s ideas and (Ethayarajh et al., 2022)\u2019s concepts of dataset difficulty. While previous studies focused on building datasets and models and their impact on performance when the annotation budget is restricted, our work answers whether increasing the annotation budget necessarily leads to improved model performance. Overall, our study aims to demonstrate that, even with less annotation budget than its upper bound, it is possible to achieve optimal performance compared to the performance at the upper bound thereby saving annotation budget and time. Our findings provide insights into optimizing annotation budgets."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we introduced a novel approach to handle the absence of annotator-specific labels in the dataset through a multi-annotator simulation process. Additionally, we investigated the impact of varying the number of annotations per instance on the difficulty of instances and its effect on model performance. Our results highlighted that increasing the number of annotations does not always lead to improved performance, emphasizing the need to determine an optimal number of annotators. This has important implications for optimizing annota-\ntion budgets and saving time. Our findings provide valuable insights for optimizing annotation strategies and open up new possibilities for future research in this direction.\nLimitations\nThe current study acknowledges several limitations that deserve attention. Firstly, the experiments were conducted using small-size Language Models due to resource constraints. It is important to recognize that employing larger language models, such as BLOOM, GPT, and others, could potentially yield different outcomes and should be explored in future research. Furthermore, the scope of the discussion is constrained by the availability of datasets with a large number of labels per instance, leading to the utilization of the ChaosNLI dataset (Nie et al., 2020). Consequently, the generalizability of the findings to other datasets, if they emerge in the future, might be restricted."
        },
        {
            "heading": "Acknowledgements",
            "text": "We express our gratitude to the anonymous reviewers for their insightful feedback. Our research has received support through the UGC-JRF fellowship from the Ministry of Education, Government of India. Additionally, we would like to extend our thanks to our colleague, Mr. Shrutimoy Das, a Ph.D. student at IIT Gandhinagar, who provided the initial review of this paper and generously shared GPU resources to conduct essential side experiments during critical phases of our research. We are grateful for these contributions, which significantly contributed to the success of this study."
        },
        {
            "heading": "A More Details",
            "text": "A.1 V-Information V-Information (Kulmizev and Nivre, 2023; Ethayarajh et al., 2022), where V represents specific model families such as BERT, GPT, etc., measures the level of ease with which model V can predict the output variable Y given the input X . The higher the V-Information, the easier it is for the model V to predict the output variable Y given X . To measure V-Information, we use predictive V-entropy:\nHV(Y ) = inf f\u2208V [\u2212 log2 f [\u2205](Y )]\nand conditional V-entropy:\nHV(Y |X) = inf f\u2208V [\u2212 log2 f [X](Y )]\nIn simple terms, our goal is to find the f \u2208 V that maximizes the log-likelihood of the label data with and without input X . Using these two quantities, V-Information can be calculated using the formula:\nIV(X \u2192 Y ) = HV(Y )\u2212HV(Y |X)\nIt is important to note that V-Information is computed with respect to HV(Y ), so IV(X \u2192 Y ) \u2265 0. Additionally, if X is independent of Y , then IV(X \u2192 Y ) = 0.\nWhile V-Information functions as an aggregated measure calculated for the whole dataset, (Ethayarajh et al., 2022) extended this measure to a new measure called Pointwise V-Information (PVI), which allows for the calculation of the difficulty of individual instances. The higher the PVI, the easier the instance is for V in the given distribution. It can be depicted by the formula:\nPVI(x\u2192 y) = \u2212 log2 pf \u2032(y\u2217|\u2205) + log2 pf (y\u2217|x)\nwhere f\u03b8, f \u2032 \u03b8 \u2208 V are models trained with and without input x \u2208 X , respectively, and y\u2217 refers to the gold label. Unlike V-Information, PVI can be negative, indicating that the model predicts the majority class better without considering the input x compared to when considering the input.\nRefer to Table 6 for a sample of instances from the ChaosNLI-\u03b1 dataset with very low PVI, which demonstrates the high ambiguity in these instances.\nA.2 Hyperparameter Details\nReferring to Table 4, we initially trained the models using the hyperparameters provided by (Nie et al., 2020). However, during our experiments, we observed signs of overfitting to our datasets. Consequently, we adjusted the hyperparameters, leading to the set provided in the table. More hyperparameter details can be found in Tables 3 and 5\nA.3 Detailed Plots for Figure 2\nFor a more comprehensive view of the phenomenon where performance decreases with an increasing number of annotations, we provide detailed plots for BERT and DistilBERT, as shown in Figure 5. While Figure 2 maintains a consistent y-axis for datasets ChaosNLI-(S, M, and \u03b1), these plots feature distinct axes.\nA.4 Data Maps Refer to the RoBERTa datamaps in the LD setting in Figures 7, 8, and 9. For ChaosNLI-\u03b1, you can find datamaps for BERT and DistilBERT in the LD setting in Figures 10 and 11, respectively."
        },
        {
            "heading": "B Results on Absolute Ground Truth",
            "text": "We have extended our evaluation by testing our models on the absolute ground truth, which represents the majority label derived from all 100 annotations. In Figure 6, we provide plots for models trained on datasets with identical training and validation instances as the Dk datasets. However, the\ntest set remains the same, retaining 100 annotations for the LD setting, where the label distribution of these 100 annotations is considered. In the ML setting, we use the majority label of the 100 annotations.\nIn Figure 6, on the whole, we observe little to no change in performance as we incrementally increase the number of annotations except few cases. Additionally, it\u2019s important to note that the hyperparameters for these models are consistent with those listed in Tables 3, 4 and 5."
        }
    ],
    "title": "Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance",
    "year": 2023
}