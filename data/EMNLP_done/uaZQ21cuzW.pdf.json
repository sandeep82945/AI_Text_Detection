{
    "abstractText": "Addressing the challenge of adapting pretrained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a Recursive Visual Explanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation quality step by step until the answer converges. We find that this multi-step approach guides the model to correct its own answers and outperforms single-step explanation generation. Furthermore, explanations generated by ReVisE also serve as valuable annotations for few-shot self-training. Our approach outperforms previous methods while utilizing merely 5% of the human-annotated explanations across 10 metrics, demonstrating up to a 4.2 and 1.3 increase in BLEU-1 score on the VCR and VQAX datasets, underscoring the efficacy and dataefficiency of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaxin Ge"
        },
        {
            "affiliations": [],
            "name": "Sanjay Subramanian"
        },
        {
            "affiliations": [],
            "name": "Trevor Darrell"
        },
        {
            "affiliations": [],
            "name": "Boyi Li"
        }
    ],
    "id": "SP:b6231ed9ea47c2d484bfd11e46bcb77e86b44008",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Nathan Anderson",
                "Caleb Wilson",
                "Stephen D. Richardson."
            ],
            "title": "Lingua: Addressing scenarios for live interpretation and automatic dubbing",
            "venue": "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas",
            "year": 2022
        },
        {
            "authors": [
                "Peter Anderson",
                "Basura Fernando",
                "Mark Johnson",
                "Stephen Gould."
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "Computer Vision\u2013 ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,",
            "year": 2016
        },
        {
            "authors": [
                "Peter Anderson",
                "Qi Wu",
                "Damien Teney",
                "Jake Bruce",
                "Mark Johnson",
                "Niko S\u00fcnderhauf",
                "Ian Reid",
                "Stephen Gould",
                "Anton Van Den Hengel"
            ],
            "title": "Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environ",
            "year": 2018
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433.",
            "year": 2015
        },
        {
            "authors": [
                "Alexei Baevski",
                "Wei-Ning Hsu",
                "Qiantong Xu",
                "Arun Babu",
                "Jiatao Gu",
                "Michael Auli."
            ],
            "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
            "venue": "International Conference on Machine Learning, pages 1298\u20131312.",
            "year": 2022
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza-",
            "year": 2005
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Songhao Piao",
                "Furu Wei."
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "arXiv preprint arXiv:2106.08254.",
            "year": 2021
        },
        {
            "authors": [
                "Hangbo Bao",
                "Wenhui Wang",
                "Li Dong",
                "Qiang Liu",
                "Owais Khan Mohammed",
                "Kriti Aggarwal",
                "Subhojit Som",
                "Songhao Piao",
                "Furu Wei."
            ],
            "title": "Vlmo: Unified vision-language pre-training with mixture-ofmodality-experts",
            "venue": "Advances in Neural Information",
            "year": 2022
        },
        {
            "authors": [
                "Khaled Bayoudh",
                "Raja Knani",
                "Fay\u00e7al Hamdaoui",
                "Abdellatif Mtibaa."
            ],
            "title": "A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets",
            "venue": "The Visual Computer, pages 1\u201332.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Kevin Swersky",
                "Mohammad Norouzi",
                "Geoffrey E Hinton."
            ],
            "title": "Big self-supervised models are strong semi-supervised learners",
            "venue": "Advances in neural information processing systems, 33:22243\u201322255.",
            "year": 2020
        },
        {
            "authors": [
                "Xi Chen",
                "Xiao Wang",
                "Soravit Changpinyo",
                "AJ Piergiovanni",
                "Piotr Padlewski",
                "Daniel Salz",
                "Sebastian Goodman",
                "Adam Grycner",
                "Basil Mustafa",
                "Lucas Beyer"
            ],
            "title": "Pali: A jointly-scaled multilingual language-image model",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Chen",
                "Yan Zhang",
                "Chen Zhang",
                "Grandee Lee",
                "Ran Cheng",
                "Haizhou Li."
            ],
            "title": "Revisiting selftraining for few-shot learning of language model",
            "venue": "arXiv preprint arXiv:2110.01256.",
            "year": 2021
        },
        {
            "authors": [
                "Ching-Yao Chuang",
                "Varun Jampani",
                "Yuanzhen Li",
                "Antonio Torralba",
                "Stefanie Jegelka."
            ],
            "title": "Debiasing vision-language models via biased prompts",
            "venue": "arXiv preprint arXiv:2302.00070.",
            "year": 2023
        },
        {
            "authors": [
                "Virginie Do",
                "Oana-Maria Camburu",
                "Zeynep Akata",
                "Thomas Lukasiewicz."
            ],
            "title": "e-snli-ve: Corrected visual-textual entailment with natural language explanations",
            "venue": "arXiv preprint arXiv:2004.03744.",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Dognin",
                "Igor Melnyk",
                "Youssef Mroueh",
                "Inkit Padhi",
                "Mattia Rigotti",
                "Jarret Ross",
                "Yair Schiff",
                "Richard A Young",
                "Brian Belgodere."
            ],
            "title": "Image captioning as an assistive technology: Lessons learned from vizwiz 2020 challenge",
            "venue": "arXiv preprint",
            "year": 2020
        },
        {
            "authors": [
                "Jiaxin Ge",
                "Hongyin Luo",
                "Siyuan Qian",
                "Yulu Gan",
                "Jie Fu",
                "Shanghang Zhan."
            ],
            "title": "Chain of thought prompt tuning in vision language models",
            "venue": "arXiv preprint arXiv:2304.07919.",
            "year": 2023
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Zeynep Akata",
                "Marcus Rohrbach",
                "Jeff Donahue",
                "Bernt Schiele",
                "Trevor Darrell."
            ],
            "title": "Generating visual explanations",
            "venue": "Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Pro-",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Mantas Mazeika",
                "Saurav Kadavath",
                "Dawn Song."
            ],
            "title": "Using self-supervised learning can improve model robustness and uncertainty",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Maxime Kayser",
                "Oana-Maria Camburu",
                "Leonard Salewski",
                "Cornelius Emde",
                "Virginie Do",
                "Zeynep Akata",
                "Thomas Lukasiewicz"
            ],
            "title": "e-vil: A dataset and benchmark for natural language explanations in vision-language tasks",
            "year": 2021
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim."
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "International Conference on Machine Learning, pages 5583\u20135594. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi."
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ming Li",
                "Zhi-Hua Zhou."
            ],
            "title": "Setred: Self-training with editing",
            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 611\u2013621. Springer.",
            "year": 2005
        },
        {
            "authors": [
                "Wei Li",
                "Can Gao",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Hao Liu",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
            "venue": "arXiv preprint arXiv:2012.15409.",
            "year": 2020
        },
        {
            "authors": [
                "Xinzhe Li",
                "Qianru Sun",
                "Yaoyao Liu",
                "Qin Zhou",
                "Shibao Zheng",
                "Tat-Seng Chua",
                "Bernt Schiele."
            ],
            "title": "Learning to self-train for semi-supervised few-shot classification",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "Computer Vision\u2013 ECCV 2014: 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Pan Lu",
                "Swaroop Mishra",
                "Tanglin Xia",
                "Liang Qiu",
                "KaiWei Chang",
                "Song-Chun Zhu",
                "Oyvind Tafjord",
                "Peter Clark",
                "Ashwin Kalyan."
            ],
            "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "venue": "Advances in Neural Information",
            "year": 2022
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Faithful chain-ofthought reasoning",
            "venue": "arXiv preprint arXiv:2301.13379.",
            "year": 2023
        },
        {
            "authors": [
                "Ana Marasovic",
                "Iz Beltagy",
                "Doug Downey",
                "Matthew Peters."
            ],
            "title": "Few-shot self-rationalization with natural language prompts",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 410\u2013424, Seattle, United States. Association",
            "year": 2022
        },
        {
            "authors": [
                "Ishan Misra",
                "Ross Girshick",
                "Rob Fergus",
                "Martial Hebert",
                "Abhinav Gupta",
                "Laurens Van Der Maaten."
            ],
            "title": "Learning by asking questions",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11\u201320.",
            "year": 2018
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Ahmed Awadallah."
            ],
            "title": "Uncertainty-aware self-training for few-shot text classification",
            "venue": "Advances in Neural Information Processing Systems, 33:21199\u201321212.",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Dong Huk Park",
                "Trevor Darrell",
                "Anna Rohrbach."
            ],
            "title": "Robust change captioning",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4624\u20134633.",
            "year": 2019
        },
        {
            "authors": [
                "Dong Huk Park",
                "Lisa Anne Hendricks",
                "Zeynep Akata",
                "Anna Rohrbach",
                "Bernt Schiele",
                "Trevor Darrell",
                "Marcus Rohrbach."
            ],
            "title": "Multimodal explanations: Justifying decisions and pointing to the evidence",
            "venue": "Proceedings of the IEEE conference on computer",
            "year": 2018
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik."
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "Proceedings of the IEEE",
            "year": 2015
        },
        {
            "authors": [
                "Bj\u00f6rn Pl\u00fcster",
                "Jakob Ambsdorf",
                "Lukas Braach",
                "Jae Hee Lee",
                "Stefan Wermter."
            ],
            "title": "Harnessing the power of multi-task pretraining for ground-truth level natural language explanations",
            "venue": "arXiv preprint arXiv:2212.04231.",
            "year": 2022
        },
        {
            "authors": [
                "Fawaz Sammani",
                "Tanmoy Mukherjee",
                "Nikos Deligiannis."
            ],
            "title": "Nlx-gpt: A model for natural language explanations in vision and vision-language tasks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8322\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Dustin Schwenk",
                "Apoorv Khandelwal",
                "Christopher Clark",
                "Kenneth Marino",
                "Roozbeh Mottaghi."
            ],
            "title": "A-okvqa: A benchmark for visual question answering using world knowledge",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel,",
            "year": 2022
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "Proceedings of the IEEE international conference",
            "year": 2017
        },
        {
            "authors": [
                "Masahiro Suzuki",
                "Yutaka Matsuo."
            ],
            "title": "A survey of multimodal deep generative models",
            "venue": "Advanced Robotics, 36(5-6):261\u2013278.",
            "year": 2022
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575.",
            "year": 2015
        },
        {
            "authors": [
                "Boshi Wang",
                "Xiang Deng",
                "Huan Sun."
            ],
            "title": "Iteratively prompt pre-trained language models for chain of thought",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2714\u20132730.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Zirui Wang",
                "Jiahui Yu",
                "Adams Wei Yu",
                "Zihang Dai",
                "Yulia Tsvetkov",
                "Yuan Cao."
            ],
            "title": "Simvlm: Simple visual language model pretraining with weak supervision",
            "venue": "arXiv preprint arXiv:2108.10904.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Jialin Wu",
                "Raymond Mooney."
            ],
            "title": "Self-critical reasoning for robust visual question answering",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jialin Wu",
                "Raymond J Mooney"
            ],
            "title": "Faithful multimodal explanation for visual question answering",
            "venue": "arXiv preprint arXiv:1809.02805",
            "year": 2018
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le."
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687\u201310698.",
            "year": 2020
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Tianlang Chen",
                "Liwei Wang",
                "Jiebo Luo."
            ],
            "title": "Improving one-stage visual grounding by recursive sub-query construction",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720\u20136731.",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "Merlot: Multimodal neural script knowledge models",
            "venue": "Advances in Neural Information Processing Systems, 34:23634\u201323651.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "arXiv preprint arXiv:2210.03493.",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola."
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923.",
            "year": 2023
        },
        {
            "authors": [
                "Fengda Zhu",
                "Yi Zhu",
                "Xiaojun Chang",
                "Xiaodan Liang."
            ],
            "title": "Vision-language navigation with selfsupervised auxiliary reasoning tasks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10012\u201310022.",
            "year": 2020
        },
        {
            "authors": [
                "Yang Zou",
                "Zhiding Yu",
                "Xiaofeng Liu",
                "BVK Kumar",
                "Jinsong Wang."
            ],
            "title": "Confidence regularized self-training",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5982\u20135991.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Explanations for visual reasoning are important in real-world applications (Anderson et al., 2018; Hendricks et al., 2016) like assistive technologies (Dognin et al., 2020) and interactive learning (Misra et al., 2018), but collecting human annotations for these explanations is expensive. The use of language models (LMs) and pre-trained visionlanguage models (VLMs) have shown promise in explanation generation (Sammani et al., 2022; Pl\u00fcster et al., 2022). However, generating high quality explanations remains a considerable challenge when annotations are scarce (Bayoudh et al., 2021; Suzuki and Matsuo, 2022). Previous work has aimed to ameliorate this issue by focusing on enhancing model architecture and subsequent finetuning using large amounts of\n\u2217 Work done while visiting UC Berkeley; Code Available at https://github.com/para-lost/ReVisE\n\u2020 Equal advising.\nhuman-annotated explanations (Sammani et al., 2022; Pl\u00fcster et al., 2022). Nonetheless, such techniques, reliant on extensive fine-tuning, fall short in the face of limited annotations. Thus, we propose an approach to amplify the model\u2019s own reasoning capabilities during inference to generate highquality explanations. Recent research has demonstrated the efficacy of step-by-step reasoning in language and multimodal reasoning, particularly in contexts where samples are limited (Wei et al., 2022b; Lu et al., 2022; Zhang et al., 2023; Ge et al., 2023). As such, we adopt a phased approach, integrating visual and linguistic components for stepby-step vision-language explanation. In this work, we introduce the Recursive Visual Explanation (ReVisE) \u2014 a method for generating visual reasoning explanations that surpasses previous methods while using merely 5% of the human-annotated explanations. Initially, we finetune BLIP-v2 (Li et al., 2023) to generate explanations on 5% of the dataset. During inference, we generate an initial explanation, then iteratively generate new explanations based on the preceding one. Each step involves computing new visual features, guided by the preceding sentence. This sentence and the new visual features then serve as inputs to generate a new sentence in the next step. Crucially, ReVisE serves as a dynamic, selfcorrecting mechanism by progressively redirecting visual attention on the image and regenerating the explanation over steps. Additionally, ReVisE generates pseudo-ground truth explanations for few-shot self-training, producing pseudo-labels that considerably aid self-improvement compared to traditional pseudo-labels. We evaluate ReVisE on four vision-language natural language explanation (VL-NLE) tasks \u2014 e-SNLI-VE (Do et al., 2020), VQA-X (Park et al., 2018), AOK-VQA (Schwenk et al., 2022), and VCR (Zellers et al., 2019). Our results show improvements across ten evaluation metrics,\nwith enhancements of up to 4.2 and 1.3 in the BLEU-1 score on VCR and VQA-X respectively. Furthermore, self-training using our method\u2019s pseudo-ground truth explanations shows consistent progress compared with traditional generationbased self-training. Further in-depth ablation studies elucidate the impact of ReVisE and the insights behind it, indicating that sentences can effectively guide visual attention, and that sentence structure and phrasing style are pivotal for few-shot self-training. Our contribution are summarized as follows:\n\u2022 We demonstrate that recent pre-trained models require only a fraction of the annotations used by previous explanation generation approaches to reach the same quality.\n\u2022 We proposed and implemented the Recursive Recursive Visual Explanation (ReVisE), a method that iteratively refines explanations by re-computing visual features.\n\u2022 We show that self-training using ReVisE to produce pseudo-ground truth annotations further improves the quality of explanations."
        },
        {
            "heading": "2 Related Work",
            "text": "Vision-Language Models (VLM) Large VisionLanguage models have showcased significant potential in vision-language tasks, including VQA, image captioning, and image-text retrieval (Li et al., 2023; Alayrac et al., 2022; Bao et al., 2021; Chen et al., 2022; Li et al., 2021, 2020; Wang et al., 2021; Kim et al., 2021; Bao et al., 2022). Recently, BLIPv2 (Li et al., 2023) was proposed. This model aligns vision with language through a lightweighted transformer architecture (QFormer), rendering it computationally efficient for training on downstream tasks. Vision-Language Natural Language Explanation (VL-NLE) VL-NLE tasks demand a comprehensive understanding and reasoning across both vision and language modalities (Kim et al., 2021). There are two prevailing strategies: the first is a modular approach integrating two separate modules\u2014one for predicting an answer, and another for generating an explanation\u2014represented by works such as e-UG (Kayser et al., 2021), PJ-X (Park et al., 2018), FME (Wu and Mooney, 2018), RTV (Marasovic et al., 2022), and QA-only (Kayser et al., 2021). The second approach is a unified one that uses a single model to generate an answer and explanation simultaneously; relevant works include NLX-GPT (Sammani et al., 2022) and OFA-XMT (Pl\u00fcster et al., 2022). Our work utilizes the more efficient and training-effective unified approach. However, these methods fall short in effectively integrating the reasoning alignment between vision and language, a gap we address with ReVisE. Vision-Language Reasoning Vision-language reasoning is a cornerstone of vision-language explanation generation. (Hendricks et al., 2016) spearheaded the field by deriving visual explanations from deep networks. (Anderson et al., 2022) focused on visually-grounded navigation instructions, while (Park et al., 2019) applied temporal reasoning to visual tasks. Recently, chain-of-thought (Wei et al., 2021, 2022b,a) has been harnessed to approach tasks using a step-by-step reasoning methodology, effectively enhancing the coherence and logical flow of language reasoning (Wei et al., 2022b; Wang et al., 2022a), self-consistency (Wang et al., 2022b; Lyu et al., 2023) and multimodal reasoning. (Lu et al., 2022; Zhang et al., 2023; Ge et al., 2023)\nFew-Shot Self Training Self-training, a technique which uses a trained model to generate pseudolabels for unlabeled data for further model training, improves the model\u2019s robustness (Chen et al., 2020; Hendrycks et al., 2019) and benefits visionlanguage tasks (Baevski et al., 2022; Zhu et al., 2020; Wu and Mooney, 2019). Few-shot selftraining, which trains on a small number of samples with pseudo-labels, is used to enhance model performance when data resources are scarce or training costs are high (Li et al., 2019; Mukherjee and Awadallah, 2020; Chen et al., 2021). However, the quality of self-generated pseudo labels greatly influences the effectiveness of few-shot self-training (Zou et al., 2019; Xie et al., 2020; Li and Zhou, 2005). In this work, we demonstrate that ReVisE can generate robust pseudo-explanations beneficial for few-shot vision-language self-training. Iterative computation of visual features The iterative computation of visual features based on text have been applied in previous works to refine visual grounding of an image (Yang et al., 2020), which showed that re-computing visual feature can benefit the visual attention. However, how re-computing benefits text generation remains unexplored. Our work focuses on the text-generation task and shows that the iterative approach can simultaneously benefit the grounding of an image and the quality of the generated text."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we first provide an overview of the architecture of BLIPv2 (Li et al., 2023) and how we trained BLIPv2 for VL-NLE tasks. Then, we provide a detailed introduction and pseudo code for ReVisE. Finally, we discuss how ReVisE is employed for self training."
        },
        {
            "heading": "3.1 Finetuning BLIPv2 on VL-NLE",
            "text": "BLIPv2 is a generative vision-language model that provides a powerful tool for bridging the divide between vision and language. Its architecture features a lightweight, multi-layer transformer, the QFormer, which computes cross-attention between K = 32 pretrained query tokens and encoded image features. Instead of jointly training a text encoder and a vision encoder as in traditional models, BLIPv2 takes a novel approach by freezing the language model parameters and only train the vision encoder and QFormer to convert image features into tokens interpretable by the language model.\nThis strategy enhances the integration between visual and language components, leading to better task comprehension. Given an image denoted as I . We use the image encoder Eimage to encode the image to get the image features FI . We denote K BLIPv2 pretrained tokens as T . These tokens, together with FI , are passed to the QFormer QF , which processes FI and T to produce the image queries QI :\nQI = QF (Eimage(I), T ) (1)\nWe denote the tokenized prompt as P , with the format \"Answer the question by reasoning step by step. Question: {} Answer:\". We concatenate QI and P to form the full input F to the language model L, then we feed it into the language model and obtain the output generated by language model O:\nO = L(concat(QI , P )) (2)\nWe calculate a cross-entropy loss LCE between the generated output O and the ground truth sentence G, which is constructed in the format \"[answer] because [explanation]\". :\nLCE = \u2212sum(G \u2217 log(O)) (3)\nThe model parameters are updated to minimize this loss. Only the parameters of the QFormer and the vision encoder are updated while the parameters of the language model are kept frozen."
        },
        {
            "heading": "3.2 Recursive Visual Explanation (ReVisE)",
            "text": "Algorithm 1 Pseudo Code for ReVisE 1: Input: Image I , Question Q 2: Output: Final Answer An, Explanation En 3: FI = Eimage(I) 4: n = 0 5: while An \u0338= An\u22121 do 6: En = Tokenize(An) 7: En,embedded = Embed(En) 8: Concatn = concat(En,embedded, T ) 9: QI,n = QF (Concatn, FI) 10: An+1, En+1 = L(QI,n) 11: n = n+ 1 12: end while 13: return An, En\nGiven an image I and question Q, we first encode the image into a feature set FI through the image encoder FI = Eimage(I) and obtain initial image queries using the pretrained K queries through the QFormer QI = QF (FI , T ). We then initialize our iterative steps indexed by n = 0. At\nthe very first step n = 0, we feed the image queries QI and question Q into the model to generate an initial answer A0 and explanation E0,\nA0, E0 = L(concat(Q,FI)) (4)\nFor each following iteration n > 0, the output On is of the form \u201c[answer] because [explanation]\u201d. We tokenize the explanation part of On, denoted as En. The tokenized explanation En is then fed through an embedding layer.\nEn,embedded = Embed(En). (5)\nWe then concatenate En,embedded with the K BLIPv2 pretrained tokens T to create Concatn = concat(T,En,embedded). This concatenated structure Concatn is then passed into the QFormer to calculate a cross attention with the image feature set FI , which then generates a new image query QI,n based on the explanation En, T , and FI\nQI,n = QF (Concatn, FI) (6)\nThis new image query QI,n is then used as input to the language model L, which regenerates an explanation and an answer for the next step n+ 1, denoted as An+1 and En+1\nAn+1, En+1 = L(QI,n) (7)\nThis process is repeated recursively until the model converges in its answer.In practice, we limit the maximum iteration number to 5 to prevent potential non-convergence. We provide a pseudo code in Algorithm 1 and a method pipeline in Figure 1."
        },
        {
            "heading": "3.3 ReVisE for Self Training",
            "text": "ReVisE\u2019s recursive querying process allows the model to correct its own answers, which could lead to further performance improvement. Leveraging this, we propose a few-shot self-training mechanism using the explanations generated by ReVisE. Suppose we have a set of samples S for which we have the ground-truth answers but lack annotated explanations. Initially, we randomly select a fewshot subset S \u2032 \u2286 S such that the model originally incorrectly answers these instances, but corrects its answers through ReVisE. Let Acorri denote the correct answer and EReV isEi the explanation generated by ReVisE for the ith sample in S \u2032. We then use these pairs, (Acorri , E ReV isE i ), to further finetune the model. During this phase, we freeze both\nthe language model and the vision encoder, leaving only the QFormer for further finetuning.\n\u03b8newQF = argmin\u03b8QF \u2211 i\u2208S\u2032 L(Acorri , EReV isEi ; \u03b8QF ) (8) where L denotes the loss function, \u03b8QF represents the parameters of the QFormer, and \u03b8newQF are the updated parameters. This finetuning procedure is designed to bolster the model\u2019s ability to generate accurate and explanatory responses. We contrast this self-training strategy with a traditional approach. In the traditional approach, the model is given the correct answer directly to generate an explanation Egeni whereas in our approach Ei is generated through recursive querying. In the traditional self-training approach, the model parameters are updated as follows:\n\u03b8newQF = argmin\u03b8QF \u2211 i\u2208S\u2032 L(Acorri , E gen i ; \u03b8QF ),\n(9) By juxtaposing these two self-training strategies, we aim to assess the potential benefits of our proposed method, where explanations generated by ReVisE serve as a corrective mechanism, over the conventional approach that relies solely on the model\u2019s ability to self-generate explanations from provided answers. A pseudo code is in Appendix C."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we first introduce the basic settings including the task formulation, training details, baselines, and metrics. Then, we provide detailed experiment results and in-depth analysis for the results."
        },
        {
            "heading": "4.1 Settings",
            "text": "Task Formulation Our focus is on VisionLanguage Natural Language Explanation (VLNLE) tasks which demand generating an answer and a high-quality explanation given an imagequestion pair. We test our method on three established VL-NLE datasets (VQA-X (Park et al., 2018), e-SNLI-VE (Do et al., 2020), and VCR (Zellers et al., 2019)), and provide additional results for AOK-VQA (Schwenk et al., 2022). Appendix E provides detailed dataset descriptions. Implementation Details For finetuning BLIPv2 on VL-NLE tasks, we maintain language model frozen and concurrently fine-tune the vision encoder with the QFormer, adopting a learning rate of 1e \u2212 5. We use the entirety of VQA-X while only selecting\na random 5% subset from e-SNLI-VE and VCR and AOK-VQA. Under the few-shot self-training scenario, we use 32 examples and exclusively finetune the QFormer, applying a learning rate of 1e\u22126. More implementation details are provided in Appendix B. Baselines For finetuned BLIPv2, we compare it with previous state of the art models that uses either unified approach or modular approach on the three VL-NLE datasets, incluing e-UG (Kayser et al., 2021), PJ-X (Park et al., 2018), FME (Wu and Mooney, 2018), RTV (Marasovic et al., 2022), QA-only (Kayser et al., 2021), NLX-GPT (Sammani et al., 2022), OFA-XMT (Pl\u00fcster et al., 2022). We provide backbone information in Appendix A. Evaluation Metrics In keeping with established practices, we employ N-gram scores, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), and BERTScore (Zhang et al., 2019). We also use a more recent metric, G-Eval (Liu et al., 2023), which uses GPT4 (Bubeck et al., 2023) and Auto-Chain-Of-Thought (Zhang et al., 2022) for evaluation that has been shown to align better with\nhuman evaluations. Details of these metrics are available in Appendix D. In accordance with established methods, we present filtered scores that represent results for explanations accompanied by correct answers. Additionally, we also report scores for instances where incorrect answers were given, providing a comprehensive view of the model\u2019s performance."
        },
        {
            "heading": "4.2 Finetuned BLIPv2",
            "text": "In Table 1, we present our method\u2019s performance against other state-of-the-art models using filtered scores for consistency. Leveraging only 5% of the VCR and e-SNLI-VE datasets and the entire VQA-X dataset, we managed to match or exceed benchmark scores with substantially less data. This highlights that advanced pre-trained models like BLIPv2 can achieve comparable performance using fewer annotations. The unique design of BLIPv2, which preserves the language model while transforming visual features into language modelinterpretable tokens, offers a promising avenue for future vision-language model architecture research."
        },
        {
            "heading": "4.3 Recursive Visual Explanation (ReVisE)",
            "text": "In Table 2, we showcase ReVisE\u2019s impact on augmenting model performance. As our approach aims at self-correcting and refining initially poor-quality explanations, we evaluate ReVisE on samples initially misinterpreted by the BLIPv2 model. The process involves using recursive language querying to extract pertinent image features, progressively refining the model\u2019s output. We find that ReVisE persistently enhances the quality of the generated explanations, underscoring the critical role of language as a guide for image feature extraction. Figure 2 exhibits representative examples from the VL-NLE datasets, clearly demonstrating the selfcorrecting mechanism of ReVisE. Employing gradCAM visualizations (Selvaraju et al., 2017), we\nelucidate how ReVisE guides the model\u2019s attention allocation over steps. While initially, the attention maps are broad or focus on areas irrelevant to the question, ReVisE\u2019s language-guided procedure redirects the model\u2019s attention towards areas pertinent to the question at hand, suggesting an improvement in the model\u2019s interpretability. By taking the explanation from one iteration and using it as input for the next, the model refines its interpretation and visual attention. Conceptually, it\u2019s analogous to a person rephrasing a statement repeatedly to enhance clarity."
        },
        {
            "heading": "4.4 Few-Shot Self-Training",
            "text": "In Table3, we show results for few-shot selftraining. We use explanations generated by Re-\nTable 3: Performance comparison of ReVisE in a few-shot self-training context for e-SNLI-VE, VQA-X, AOKVQA, and VCR. The table depicts results without self-training, with traditional self-training, and with ReVisE self-training. We use 32-shot in all these experiments.\nB1 B2 B3 B4 M R-L C S BS G-Eval e-SNLI-VE\nNo Self-train 35.0 22.7 15.2 10.3 17.9 29.9 101.0 30.6 79.30 6.21 w/o ReVisE 34.9 22.7 15.3 10.4 17.9 29.8 100.7 30.5 79.21 6.49 w/ReVisE 36.2 23.5 15.8 10.9 18.2 30.5 103.2 30.7 79.61 6.75 VQA-X No Self-train 51.1 34.3 22.6 14.6 15.8 39.6 51.7 12.6 83.28 2.98 w/o ReVisE 51.2 34.1 22.4 14.3 15.9 39.6 50.6 12.6 83.00 3.21 w/ReVisE 53.5 36.6 24.8 16.2 16.9 40.7 58.9 13.8 83.65 4.41 AOK-VQA No Self-train 57.5 39.9 28.1 19.0 16.5 44.4 59.1 15.3 86.36 4.46 w/o ReVisE 57.3 40.2 28.4 19.2 16.6 44.8 61.1 15.7 86.44 4.44 w/ReVisE 60.0 41.1 28.7 19.6 18.6 45.1 62.4 18.1 85.28 4.71 VCR No Self-train 26.7 19.4 15.6 12.7 13.1 24.6 19.6 21.7 79.25 3.65 w/o ReVisE 26.9 19.6 15.7 12.7 13.3 25.2 21.1 21.9 79.35 3.99 w/ReVisE 27.1 20.1 16.2 13.3 13.7 25.4 21.6 23.1 79.55 4.14\nFigure 3: We display the distribution of convergence steps, indicating the percentage of samples that reach convergence at each respective step. We show results of e-SNLI-VE, VQA-X, AOKVQA and VCR and found that most samples converge by step2 and at least 90% samples converge by step3.\nVisE to self-train on samples that are initially incorrect but self-corrected during the ReVisE process. When compared to providing the model with the correct answers directly to let it generate an explanation on the same samples, self-training with ReVisE explanations led to better performance, indicating the model benefits more from its own reasoning process than simply digesting the correct answers. Qualitative results in Figure 4 reveal minor semantic differences between ReVisE-generated pseudoexplanations and explanations crafted with provided ground-truth answers. The variations typically lie in phrasing style or sentence structures, suggesting that attention to sentence structure or phrasing pattern could prove crucial for high-quality pseudo-explanations in few-shot selftraining.\nThere is a clock on the wall above\nthe stairs\nThere is a clock on the wall\nIf a squirrel is jumping in the snow\nit is not attacking a small child\nJust because a squirrel jumps does not mean it is attacking a\nsmall child\nThey are looking down the\nbalcony. They are standing on the balcony\nlooking down.\nFigure 4: Comparison between pseudo-explanations generated by ReVisE(in the box above) and pseudoexplanations generated directly providing groundtruth answers(in the box below).\nAdditionally, we explored the impact of varying the number of self-training samples. As shown in Table 5, while any addition of few-shot samples enhances self-training, even as few as 8-shot samples can improve the model\u2019s performance."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "Implicit VS Explicit Language In our approach, we forward both the integrated K queries and the language queries after cross-attention. We compare this procedure with forwarding only K queries after cross-attention, as illustrated in Figure 6. The K queries integrate language information implicitly through the cross-attention process but do not forward the encoded text directly.\nThe ablation study results in Table 6 indicate that implicit language integration through the K queries alone does not significantly enhance performance. Explicitly combining language queries offer crucial semantically-grounded context which cannot be captured by the K learned queries alone, thus providing a more substantial advantage in refining the model\u2019s image comprehension.\nLimit Iteration Steps Recursive querying may be time-consuming, so we limit the maximum number of steps to 2 and 3 to investigate its impact. As shown in Table 4, limiting the steps to 3 achieved performance nearly on par with that of unrestricted steps. Furthermore, Figure 3 presents the percentage of samples that reach convergence at each respective step, indicating that most samples converge by the second step and 90% of the samples converge by step3. The e-SNLI-VE samples exhibit the fastest convergence, potentially due to the simplicity of their answer options."
        },
        {
            "heading": "A: A very large and dangerous one.",
            "text": ""
        },
        {
            "heading": "Q: Why is person1 in red region' s mouth ajar?",
            "text": "Failure Cases We notice certain instances ( 2%) where additional iterations negatively affect the quality of the generates explanations, as illustrated in Figure 5. For most failure cases, the model enters into a recursive loop. In some others, the model initially generates explanations closely aligning with the ground truth but diverged with subsequent iterations. This reveals the importance for a balance between the depth of reasoning and model certainty in recursive reasoning.\nData Efficiency We provide further ablation on the amount of training data used. On the e-SNLIVE dataset, we tried 1%, 3%, and 5% of the dataset and report the filtered score. The results are shown in Table 7. This illustrates that our model, leverag-\ning recent advancements in pre-trained models, can deliver high-quality explanations even with substantially fewer annotations than traditional methods."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we introduce ReVisE, a method for generating natural language explanations for visual reasoning tasks with a small number of annotations. We demonstrate its high performance in generating language explanations using significantly less data compared to existing models, enabling the model\u2019s self-correction mechanism, and effectively facilitating few-shot self-training by generating robust pseudo-explanations. Our work raises the question of whether recursive procedures like ReVisE can be used to improve performance in other multimodal domains as well.\nLimitations\nAlthought BLIPv2 has a large potential for visionlanguage explanation, it might encode social bias. As (Chuang et al., 2023) illustrated, visionlanguage models have been shown to inherit biases from their training datasets. Conducting a thorough investigation of the potential bias of BLIPv2 and addressing it would be an important future work. Also, further enhancing the method to identify and address the failure cases is also a future work to improve this method.\nEthics Statement\nThe proposed methods, rooted in the principles of transparency and interpretability, promote the ethical goal of developing AI systems that are easily comprehensible and verifiable. By enabling AI to generate more coherent explanations, we contribute to the objective of trustworthy AI."
        },
        {
            "heading": "A Model Backbone",
            "text": "We present model parameters and vision transformer backbone for the different models in Table 8"
        },
        {
            "heading": "B Additional Implementation Details",
            "text": "We provide additional implementation details. When training on BLIPv2 we use beam search with num beam = 5 during decoding. For AOK-VQA, we also set length penalty to -1, consistent with the original BLIPv2. During training, we use cosine annealing sceduler and AdamW optimizer and train for 6 epochs. Since BLIPv2 does not have any regional proposals, we followed (Zellers et al., 2021) and add colored bounding boxes around the people/objects referred to and refer to them as \"person1 in red region\" or \"person2 in yellow region\". As (Zellers et al., 2021) demonstrates, through finetuning, the model learns a matching between the color referred to in language and the color denoted in the image."
        },
        {
            "heading": "C Pseudo Algorithm For ReVisE self-training",
            "text": "We also provide a pseudo code for self-training in algorithm 2, which is a pseudo code description of the self-training process in the Method section."
        },
        {
            "heading": "D Detailed Metrics",
            "text": "We provide details of the recent new metric GEval (Liu et al., 2023). This metric commences by formulating a task and employs GPT3.5 (Brown et al., 2020) and GPT4 (Bubeck et al., 2023) to autonomously generate evaluation steps using the Auto Chain-of-Thought (AutoCoT) (Zhang et al., 2022). Subsequently, the task instruction along with the AutoCoT evaluation steps and the sample under consideration are fed to the GPT model together to obtain a comprehensive score from 1-10.\nAlgorithm 2 ReVisE for Self Training 1: Input: Model M , Samples S, Few-shot size k 2: Output: Finetuned Model M \u2032 3: Initialize: TrainingSet\u2190 {} 4: for each sample \u2208 S do 5: (Aold, Eold) \u2190\nM.generateAnswerWithoutReVisE(sample) 6: (Anew, Enew) \u2190\nM.generateAnswerWithReVisE(sample) 7: if M.checkAnswer(Aold) is False and\nM.checkAnswer(Anew) is True then 8: TrainingSet.add((sample, Egenerated)) 9: end if\n10: end for 11: Randomly select k samples from TrainingSet to form\nFewShotSet for few-shot self training 12: M \u2032 = M.finetuneQFormer(FewShotSet) 13: return M \u2032\nThis metric has been shown to align better with human evaluations than previous metrics."
        },
        {
            "heading": "E Data Details",
            "text": "VQA-X and A-OKVQA both augments the VQAv2 dataset (Antol et al., 2015) with explanations for each answer. The images in VQA-X are sourced from the COCO dataset (Lin et al., 2014), and it comprises 33K QA pairs drawn from 28K images. e-SNLI-VE provides explanations for the Visual Entailment Prediction task, which involves answering whether a given image and hypothesis are in entailment, contradiction, or neutral relationship.The images for this dataset are drawn from Flickr30k (Plummer et al., 2015), and it contains over 430K examples. VCR is a dataset that presents a model with an image, a question, and a list of objects that are annotated with bounding boxes, and requires the model to first select an answer and then explain it. VCR includes 290K samples of questions, answers, and rationales. For each of the dataset, we use the original train set of each dataset and their own test set."
        }
    ],
    "title": "From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation",
    "year": 2023
}