{
    "abstractText": "Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first holistic multilingual politeness dataset and exploring how politeness varies across four languages. Our approach enables an effective evaluation of how distinct linguistic categories contribute to stylistic variations and provides interpretable insights into how people communicate differently around the world.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shreya Havaldar"
        },
        {
            "affiliations": [],
            "name": "Matthew Pressimone"
        },
        {
            "affiliations": [],
            "name": "Eric Wong"
        },
        {
            "affiliations": [],
            "name": "Lyle Ungar"
        }
    ],
    "id": "SP:218e3f922e4f59de29363849cb34f6edd4755354",
    "references": [
        {
            "authors": [
                "Malika Aubakirova",
                "Mohit Bansal."
            ],
            "title": "Interpreting neural networks to improve politeness comprehension",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2035\u20132041, Austin, Texas. Association",
            "year": 2016
        },
        {
            "authors": [
                "Anshul Bawa",
                "Pranav Khadpe",
                "Pratik Joshi",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "Do multilingual users prefer chat-bots that code-mix? let\u2019s nudge and find out! Proc",
            "venue": "ACM Hum.-Comput. Interact., 4(CSCW1).",
            "year": 2020
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the Association for Computational Linguistics, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Eleftheria Briakou",
                "Di Lu",
                "Ke Zhang",
                "Joel Tetreault."
            ],
            "title": "Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Cho",
                "Kei Sawada."
            ],
            "title": "Pre-learning model for japanese natural language processing",
            "venue": "Japanese Society for Artificial Intelligence Research Group Material Language/Speech Understanding and Dialogue Processing, 93:169\u2013170.",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pre-trained models for Chinese natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
            "year": 2020
        },
        {
            "authors": [
                "Cristian Danescu-Niculescu-Mizil",
                "Moritz Sudhof",
                "Dan Jurafsky",
                "Jure Leskovec",
                "Christopher Potts."
            ],
            "title": "A computational approach to politeness with application to social factors",
            "venue": "arXiv preprint arXiv:1306.6078.",
            "year": 2013
        },
        {
            "authors": [
                "Javier De la Rosa",
                "Eduardo G. Ponferrada",
                "Manu Romero",
                "Paulo Villegas",
                "Pablo Gonz\u00e1lez de Prado Salas",
                "Mar\u00eda Grandury."
            ],
            "title": "Bertin: Efficient pre-training of a spanish language model using perplexity sampling",
            "venue": "Procesamiento del",
            "year": 2022
        },
        {
            "authors": [
                "Fatima-zahra El-Alami",
                "Said Ouatik El Alaoui",
                "Noureddine En Nahnahi."
            ],
            "title": "A multilingual offensive language detection method based on transfer learning from transformer fine-tuning model",
            "venue": "Journal of King Saud University-Computer and Informa-",
            "year": 2022
        },
        {
            "authors": [
                "As\u0131m Ersoy",
                "Gerson Vizcarra",
                "Tasmiah Tahsin Mayeesha",
                "Benjamin Muller"
            ],
            "title": "In what languages are generative language models the most formal? analyzing formality distribution across languages",
            "year": 2023
        },
        {
            "authors": [
                "Liye Fu",
                "Susan Fussell",
                "Cristian Danescu-NiculescuMizil."
            ],
            "title": "Facilitating the communication of politeness through fine-grained paraphrasing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Xavier Garcia",
                "Noah Constant",
                "Mandy Guo",
                "Orhan Firat."
            ],
            "title": "Towards universality in multilingual text rewriting",
            "venue": "arXiv preprint arXiv:2107.14749.",
            "year": 2021
        },
        {
            "authors": [
                "Edward T Hall."
            ],
            "title": "Beyond culture",
            "venue": "Anchor.",
            "year": 1976
        },
        {
            "authors": [
                "Shreya Havaldar",
                "Bhumika Singhal",
                "Sunny Rai",
                "Langchen Liu",
                "Sharath Chandra Guntuku",
                "Lyle Ungar."
            ],
            "title": "Multilingual language models are not multicultural: A case study in emotion",
            "venue": "Proceedings of the 13th Workshop on Computational",
            "year": 2023
        },
        {
            "authors": [
                "Shreya Havaldar",
                "Adam Stein",
                "Eric Wong",
                "Lyle Ungar"
            ],
            "title": "2023b. Topex: Topic-based explanations for model comparison",
            "year": 2023
        },
        {
            "authors": [
                "Shirley Anugrah Hayati",
                "Dongyeop Kang",
                "Lyle Ungar"
            ],
            "title": "Does bert learn as humans perceive? understanding linguistic styles through lexica",
            "year": 2021
        },
        {
            "authors": [
                "Shirley Anugrah Hayati",
                "Kyumin Park",
                "Dheeraj Rajagopal",
                "Lyle Ungar",
                "Dongyeop Kang"
            ],
            "title": "Stylex: Explaining style using human lexical annotations",
            "year": 2023
        },
        {
            "authors": [
                "S\u00f8gaard."
            ],
            "title": "Challenges and strategies in crosscultural NLP",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013, Dublin, Ireland. Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Dirk Hovy",
                "Diyi Yang."
            ],
            "title": "The importance of modeling social factors of language: Theory and practice",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Dongyeop Kang",
                "Eduard Hovy."
            ],
            "title": "Style is not a single variable: Case studies for cross-stylistic language understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Naresh Khatri."
            ],
            "title": "Consequences of power distance orientation in organisations",
            "venue": "Vision, 13(1):1\u20139.",
            "year": 2009
        },
        {
            "authors": [
                "Donghoon Kim",
                "Yigang Pan",
                "Heung Soo Park."
            ],
            "title": "High-versus low-context culture: A comparison of chinese, korean, and american cultures",
            "venue": "Psychology & Marketing, 15(6):507\u2013521.",
            "year": 1998
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Deepak Nathani",
                "Xavier Garcia",
                "Bidisha Samanta",
                "Partha Talukdar."
            ],
            "title": "Fewshot controllable style transfer for low-resource multilingual settings",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Sotiris Lamprinidis",
                "Federico Bianchi",
                "Daniel Hardt",
                "Dirk Hovy."
            ],
            "title": "Universal joy a data set and results for classifying emotions across languages",
            "venue": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and",
            "year": 2021
        },
        {
            "authors": [
                "Martin Laville",
                "M\u00e9ri\u00e8me Bouhandi",
                "Emmanuel Morin",
                "Philippe Langlais"
            ],
            "title": "Word representations, seed lexicons, mapping procedures, and reference lists: What matters in bilingual lexicon induction from comparable corpora",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Leech"
            ],
            "title": "Politeness: is there an east-west divide",
            "year": 2007
        },
        {
            "authors": [
                "Darrin R Lehman",
                "Chi-yue Chiu",
                "Mark Schaller."
            ],
            "title": "Psychology and culture",
            "venue": "Annu. Rev. Psychol., 55:689\u2013714.",
            "year": 2004
        },
        {
            "authors": [
                "Mingyang Li",
                "Louis Hickman",
                "Louis Tay",
                "Lyle Ungar",
                "Sharath Chandra Guntuku."
            ],
            "title": "Studying politeness across cultures using english twitter and mandarin weibo",
            "venue": "Proc. ACM Hum.-Comput. Interact., 4(CSCW2).",
            "year": 2020
        },
        {
            "authors": [
                "Jind\u0159ich Libovick\u00fd",
                "Rudolf Rosa",
                "Alexander Fraser."
            ],
            "title": "On the language neutrality of pre-trained multilingual representations",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1663\u20131674, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin A Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul",
                "Benjamin Bossan."
            ],
            "title": "Peft: State-of-the-art parameterefficient fine-tuning methods",
            "venue": "https://github. com/huggingface/peft.",
            "year": 2022
        },
        {
            "authors": [
                "Kshitij Mishra",
                "Mauajama Firdaus",
                "Asif Ekbal."
            ],
            "title": "Please be polite: Towards building a politeness adaptive dialogue system for goal-oriented conversations",
            "venue": "Neurocomputing, 494:242\u2013254.",
            "year": 2022
        },
        {
            "authors": [
                "Angela Moorjani",
                "Thomas T Field."
            ],
            "title": "Semiotic and sociolinguistic paths to understanding culture",
            "venue": "Toward a new integration of language and culture. Reports of the Northeast Conference on the Teaching of Foreign Languages, pages 25\u201345.",
            "year": 1988
        },
        {
            "authors": [
                "Benjamin Muller",
                "Yanai Elazar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "First align, then predict: Understanding the cross-lingual ability of multilingual BERT",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Tong Niu",
                "Mohit Bansal."
            ],
            "title": "Polite Dialogue Generation Without Parallel Data",
            "venue": "Transactions of the Association for Computational Linguistics, 6:373\u2013 389.",
            "year": 2018
        },
        {
            "authors": [
                "Xing Niu",
                "Sudha Rao",
                "Marine Carpuat."
            ],
            "title": "Multi-task neural models for translating between styles within and across languages",
            "venue": "arXiv preprint arXiv:1806.04357.",
            "year": 2018
        },
        {
            "authors": [
                "Emily \u00d6hman",
                "Kaisla Kajava",
                "J\u00f6rg Tiedemann",
                "Timo Honkela."
            ],
            "title": "Creating a dataset for multilingual fine-grained emotion-detection using gamification-based annotation",
            "venue": "Proceedings of the 9th Workshop on Computational Approaches to",
            "year": 2018
        },
        {
            "authors": [
                "Emily \u00d6hman",
                "Marc P\u00e0mies",
                "Kaisla Kajava",
                "J\u00f6rg Tiedemann."
            ],
            "title": "Xed: A multilingual dataset for sentiment analysis and emotion detection",
            "venue": "arXiv preprint arXiv:2011.01612.",
            "year": 2020
        },
        {
            "authors": [
                "Damilola Omitaomu",
                "Shabnam Tafreshi",
                "Tingting Liu",
                "Sven Buechel",
                "Chris Callison-Burch",
                "Johannes Eichstaedt",
                "Lyle Ungar",
                "Jo\u00e3o Sedoc"
            ],
            "title": "Empathic conversations: A multi-level dataset of contextualized conversations",
            "year": 2022
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette."
            ],
            "title": "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy",
            "venue": "Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Reza Pishghadam",
                "Safoora Navari."
            ],
            "title": "A study into politeness strategies and politeness markers in advertisements as persuasive tools",
            "venue": "Mediterranean Journal of Social Sciences, 3(2):161\u2013171.",
            "year": 2012
        },
        {
            "authors": [
                "Mar\u00eda Elena Placencia",
                "Carmen Garcia-Fernandez."
            ],
            "title": "Research on politeness in the Spanishspeaking world",
            "venue": "Routledge.",
            "year": 2017
        },
        {
            "authors": [
                "Flor Miriam Plaza-del Arco",
                "Carlo Strapparava",
                "L Alfonso Urena Lopez",
                "M Teresa Mart\u00edn-Valdivia."
            ],
            "title": "Emoevent: A multilingual emotion corpus based on different events",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Elijah Rippeth",
                "Sweta Agrawal",
                "Marine Carpuat."
            ],
            "title": "Controlling translation formality using pretrained multilingual language models",
            "venue": "arXiv preprint arXiv:2205.06644.",
            "year": 2022
        },
        {
            "authors": [
                "Diogo Silva",
                "David Semedo",
                "Jo\u00e3o Magalh\u00e3es"
            ],
            "title": "Polite task-oriented dialog agents: To generate or to rewrite",
            "venue": "In Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Helen Spencer-Oatey",
                "D\u00e1niel Z K\u00e1d\u00e1r."
            ],
            "title": "The bases of (im) politeness evaluations: Culture, the moral order and the east-west debate",
            "venue": "East Asian Pragmatics, 1(1):73\u2013106.",
            "year": 2016
        },
        {
            "authors": [
                "Anirudh Srinivasan",
                "Eunsol Choi."
            ],
            "title": "Tydip: A dataset for politeness classification in nine typologically diverse languages",
            "venue": "arXiv preprint arXiv:2211.16496.",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Stolcke",
                "Klaus Ries",
                "Noah Coccaro",
                "Elizabeth Shriberg",
                "Rebecca Bates",
                "Daniel Jurafsky",
                "Paul Taylor",
                "Rachel Martin",
                "Carol Van Ess-Dykema",
                "Marie Meteer"
            ],
            "title": "Dialogue act modeling for automatic tagging and recognition",
            "year": 2000
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "International conference on machine learning, pages 3319\u2013 3328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Qinqin Wang",
                "Hanbing Yan",
                "Zhihui Han."
            ],
            "title": "Explainable apt attribution for malware using nlp techniques",
            "venue": "2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), pages 70\u201380.",
            "year": 2021
        },
        {
            "authors": [
                "Qingyu Zhang",
                "Xiaoyu Shen",
                "Ernie Chang",
                "Jidong Ge",
                "Pengke Chen."
            ],
            "title": "Mdia: A benchmark for multilingual dialogue generation in 46 languages",
            "venue": "arXiv preprint arXiv:2208.13078.",
            "year": 2022
        },
        {
            "authors": [
                "Hayati"
            ],
            "title": "2021) to get the parameters for the power calculation (\u03b1 = 0.05, \u03b2 = 0.2.) A.2 Annotation To annotate our dataset, we set up surveys hosted using Prolific",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Communication practices vary across cultures. Inherent differences in how people think and behave (Lehman et al., 2004) influence cultural norms, which in turn, have a significant impact on communication (Moorjani and Field, 1988). One key way that cultural variation influences communication is through linguistic style. In this work, we introduce an explanation framework to extract stylistic differences from multilingual language models (LMs), enabling cross-cultural style comparison.\nStyle is a complex and nuanced construction. In communication, style is heavily used to convey certain personal or social goals depending on the speakers\u2019 culture (Kang and Hovy, 2021). For example, cultures that are high-context1 tend to use more indirect language than those that are lowcontext (Kim et al., 1998), and cultures that have a high power-distance2 tend to have more formal\n1Conversations in high-context cultures have a lot of subtlety and require more collective understanding. (Hall, 1976)\n2Power distance refers to the strength of a society\u2019s social hierarchy. (Hofstede, 2005)\ninteractions in a workplace setting. (Khatri, 2009). For example, consider the two conversation snippets in Figure 1. Though these two utterances are direct translations of each other, a multilingual LM fine-tuned for politeness classification outputs different labels, with the Chinese utterance labeled as impolite. We ask a bilingual English/Chinese speaker to provide further insights, and she observes that the same request to \u201cstop editing as soon as possible\u201d sounds more aggressive (and thus, impolite) in Chinese than in English, as cultural norms in China do not typically condone giving such harsh, direct instructions to a stranger.\nStylistically appropriate language is crucial to successful communication within a certain culture. However, multilingual LMs sometimes struggle to\ngenerate language that is stylistically appropriate in non-English languages (Hershcovich et al., 2022; Ersoy et al., 2023; Zhang et al., 2022). Standard training methods for multilingual models lead to little stylistic variation in generated text across languages (Pires et al., 2019; Libovick\u00fd et al., 2020; Muller et al., 2021), and multilingual systems rarely address these socially-driven factors of language (Hovy and Yang, 2021). As a result, downstream applications of these systems, like chatbots, are not as usable or beneficial to a non-American audience (Bawa et al., 2020; Havaldar et al., 2023a).\nOne step towards correcting this is to understand how styles differ across languages. Though modern multilingual LMs struggle with generating stylistically appropriate language, they are generally quite successful at classifying stylistic language (Briakou et al., 2021; Plaza-del Arco et al., 2020; El-Alami et al., 2022; Srinivasan and Choi, 2022). Psychological or linguistic studies to analyze language are resource-heavy and time-intensive; alternatively, we can utilize these trained LMs to computationally capture both the overt and subtle ways a language reflects a certain style.\nMost current methods to extract feature importances from multilingual LMs are at the token-level (Lundberg and Lee, 2017; Ribeiro et al., 2016; Sundararajan et al., 2017) and specific to each language; as a result, there is no \u201ccommon language\u201d for comparison. In addition, most explanations are not easily human-interpretable, and it is difficult to extract useful takeaways from them.\nIn this work, we present a framework to extract differences in style from multilingual LMs and explain these differences in a functional, interpretable\nway. Our framework consists of two components:\n1. Multilingual Lexica Creation: We utilize embedding-based methods to translate and expand style lexica in any language.\n2. Feature Set Aggregation: We extract feature importances from LMs and consolidate them into comparable lexical categories.\nTo study how styles differ across languages, we create a holistic politeness dataset that encompasses a rich set of linguistic variations in four languages. Politeness varies greatly across languages due to cultural influences, and it is necessary to understand this variation in order to build multilingual systems with culturally-appropriate politeness. Trustworthy and successful conversational agents for therapy, teaching, customer service, etc. must be able to adapt levels and expressions of politeness to properly reflect the cultural norms of a user.\nPrevious work by Danescu-Niculescu-Mizil et al. (2013) and Srinivasan and Choi (2022) uses NLP to study politeness, but their datasets reflect only a small subset of language \u2014 the conversational utterances they analyze consist of only questions on either extremes of the impolite/polite spectrum.\nOur dataset is the first holistic multilingual politeness dataset that includes all types of sentences (i.e. dialogue acts) across the full impolite/polite spectrum. We include English, Spanish, Japanese, and Chinese \u2014 we select these languages as they are all high-resource, (and therefore well-supported by modern LMs) and each have a unique way of expressing politeness. For instance, politeness in Japan frequently arises from acknowledging the place of others (Spencer-Oatey and K\u00e1d\u00e1r, 2016),\nwhile politeness in Spanish-speaking countries often relies on expressing mutual respect (Placencia and Garcia-Fernandez, 2017). This global variation in politeness (Leech, 2007; Pishghadam and Navari, 2012) makes it an important style to understand for effective cross-cultural communication.\nOur contributions are as follows:\n1. We present an explanation framework to extract differences in styles from multilingual LMs and meaningfully compare styles across languages.\n2. We provide the first holistic politeness dataset that reflects a realistic distribution of conversational data across four languages.\n3. We use this framework and dataset to show differences in how politeness is expressed (e.g. words like \u201cbro\u201d and \u201cmate\u201d are rude in English, but polite in Japanese, and yes/no questions are rude in Chinese but not in English.)\nFigure 1 shows an example comparison of English and Chinese politeness using this framework. We explain differences in politeness using both lexical categories and dialogue acts, with Figure 1 highlighting three chosen lexical categories. We make all code and data available publicly.3"
        },
        {
            "heading": "2 A Framework for Multilingual Style Comparison",
            "text": "In this section, we detail our two-part framework for multilingual style comparison. (1) Multilingual Lexica Creation takes a curated style lexica and uses embedding-based methods to refine lexica translation into another language, creating a set of parallel lexical categories. (2) Feature Set Aggregation maps extracted feature importances from trained LMs to these parallel lexical categories across languages. This framework helps us to interpret what multilingual LMs learn about style during training, and allows us to meaningfully compare how style is expressed across languages."
        },
        {
            "heading": "2.1 Multilingual Lexica Creation (MLC)",
            "text": "Lexica provide an interpretable grouping of words into meaningful categories. The traditional use of lexica in NLP relies on a simple bag-of-words representation, but allows humans to easily visualize which lexical categories appear in a dataset.\n3https://github.com/shreyahavaldar/ multilingual_politeness\nMuch work has already been done in curating theory-grounded lexica that classify style. DanescuNiculescu-Mizil et al. (2013) curate lexical strategies that inform politeness in English, and Li et al. (2020) extend these strategies to Chinese.\nThough they provide insight into how politeness is expressed, lexica have limited predictive power; lexica-based models are drastically outperformed by modern LMs. Rather than relying on lexica to classify style, we instead use lexica to curate a common language for interpretable multilingual comparison. We present our method, Multilingual Lexica Creation (MLC), to expand a curated style lexica into multiple languages.\nMotivation. When using standard 1:1 machine translation to translate a style lexica, a number of issues arise. Sometimes, there is no 1:1 mapping between words \u2014 a word in one language may have 0 words or 2+ words that express it in another. Additionally, context and culture influence how linguistic style is expressed (Moorjani and Field, 1988) \u2014 a word that reflects politeness in one language may not reflect politeness the same way in another. To combat these issues, MLC uses word embeddings to improve the translation process.\nStep 1: Expansion. In the expansion step, we tackle the flawed assumption that there always exists a 1:1 mapping between words in different languages. First, we machine translate a curated style lexica into the target language. We refer to the words in this machine translated lexica as our set of seed words. Next, we embed each seed word us-\ning FastText (Bojanowski et al., 2017) and perform synonym expansion on each seed word and concept expansion on each lexical category. Figure 3 details this two-stage expansion process.\nFor synonym expansion, we find the nearest neighbors of each seed word in embedding space (within a tunable distance threshold) and append them to the corresponding lexical category. This adds any synonyms of seed words that may have been bypassed during machine translation. For instance, \u201csorry\u201d in English is expanded to \u201clo siento\u201d, \u201cperd\u00f3n\u201d, and \u201cperdona\u201d in Spanish.\nFor concept expansion, we average the embeddings of all seed words within a lexical category and find the centroid embedding of the category. We then append the nearest neighbors of this centroid (within a tunable distance threshold) to the overall category. This adds any additional words conceptually similar to the lexical category that were not included via machine translation. For example, the Hedge category in Figure 3 is expanded to additionally include \u201cevidente\u201d, \u201caparente\u201d, and \u201cse\u00f1ala\u201d in Spanish.\nWe choose FastText over other embedding models as it has a fixed vocabulary size, and thus, efficient nearest neighbors functionality. Additionally, FastText performs well in uncontextualized settings (Laville et al., 2020) and supports 157 languages. However, embeddings from any model can be used.\nStep 2: Purification. In the purification step, we tackle the issue that a word reflecting a style in one language may not reflect that style in the same way when translated to another language. So, after combining the words returned from synonym and concept expansion, we ensure that each category of the expanded lexica contains words that are both pertinent and internally correlated.\nWe first filter out rare words (i.e. any words below a given usage frequency). This addresses issues where machine translation results in words not commonly used in day-to-day conversation.\nNext, we ensure that the words in each lexical category reflect a given style similarly in the target language. Style is highly influenced by culture \u2013 a word that might indicate rudeness in English (e.g. \u201cstubborn\u201d, \u201cbossy\u201d, etc.) may not necessarily do so in other languages. To remove uncorrelated words, we first apply our lexica on any large corpus, and use a pre-trained LM to calculate a style score (e.g. politeness level) for each utterance in the corpus. Then, for each word w within a lexi-\ncal category C, we correlate the style scores of all utterances containing w against the style scores of all utterances containing any word in C. We then remove all words that do not correlate positively with their category (product-moment correlation < 0.15). For example, \u201cse\u00f1ala\u201d does not have a similar role to other Hedge words when indicating politeness in Spanish (r = \u22120.08), and so, we remove it from the final lexica. This guarantees that all words within a lexical category play a similar role in determining an utterance\u2019s style score, ensuring internal correlation within each category.\nMLC takes a curated style lexica and creates a parallel style lexica in a target language, correcting issues with 1:1 machine translation. Though the lexical categories are parallel, the words within each category are selected to best reflect how a style is expressed in the target language."
        },
        {
            "heading": "2.2 Feature Set Aggregation",
            "text": "We now seek to leverage the fact that trained multilingual LMs do successfully learn to encode how multiple languages reflect a certain style.\nMotivation. However, traditional feature attribution methods to explain what LMs learn cannot be used for multilingual comparison, as extracted features always are specific to a single language. We bypass this limitation by aggregating extracted attributions into directly comparable categories.\nSpecifically, we extract feature attributions from trained multilingual style LMs and aggregate them into the parallel lexical categories from MLC. This enables an interpretable common language for comparison. We detail aggregation with token-level Shapley values (Lundberg and Lee, 2017), but any additive feature attribution method can be used.\nToken-to-word grouping. Given a word w consisting of tokens w = [x1, x2, . . . , xK ], let [v1, v2, . . . , vK ] be the corresponding token-level Shapley values. Equation (1) first aggregates tokenlevel Shapley values into word-level importance scores.\nImp(w) = \u2211\nk : xk\u2208w vk (1)\nCategory-level importances. Next, we derive category-level importance scores for each lexical category by aggregating local word-level importance scores. For each category C, we iterate over all utterances and sum the word-level importance\nscores for all words in C. Finally, we divide by N , or the total number of times a word in C appears in the dataset. This process is detailed in Equation (2). Let wij denote the jth word in the ith utterance.\nImp(C) = 1\nN \u2211 ij 1[wij\u2208C]Imp(wij) (2)\nThis gives us an importance score for each lexical category across languages. Now, we can easily compare how important certain categories are at linguistically reflecting a style in different languages."
        },
        {
            "heading": "3 A Holistic Politeness Dataset",
            "text": "Politeness, like all linguistic styles, is a complex, nuanced construction. In order to compare how politeness differs across languages, it is necessary to analyze the full distribution of conversational data, without any oversimplifying assumptions. We follow the process of the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al., 2013) and TYDIP(Srinivasan and Choi, 2022) when creating and evaluating our politeness dataset, but with three key differences:\n\u2022 We include all dialogue acts to replicate the real distribution of conversational data. (Both previous datasets only include questions.)\n\u2022 We include all annotated data in model training and evaluation, as we want to compare language along the full politeness spectrum. (Both previous evaluations only consider the highest and lowest 25% of politeness utterances, eliminating all neutral utterances.)\n\u2022 We treat politeness as a regression task rather than a binary classification task. (Both previous evaluations classify an utterance as \u201cpolite\u201d or \u201cimpolite\u201d, destroying the nuance between slight and strong politeness.)"
        },
        {
            "heading": "3.1 Data Collection & Annotation Overview",
            "text": "We provide a high-level overview of our dataset construction and annotation process, and give specific details in Appendix A.\nDataset. Our dataset contains 22,800 conversation snippets, or utterances, scraped from Wikipedia Talk Pages4 in English, Spanish, Chinese, and Japanese (5,700 utterances per language).\n4Wikipedia Talk Pages are used by editors of the platform to communicate about proposed edits to articles. This is the same domain as the Stanford Politeness Corpus and TYDIP.\nEach utterance is 2-3 sentences long, and randomly sampled from all 41,000 scraped talk pages in each language. Note that we only scrape talk pages of articles that exist in all four languages, ensuring a similar distribution of topics across our dataset.\nAnnotation. To label the dataset, we use Prolific to source annotators. Respondents are required to be native speakers of the language they annotate, as well as indicate that it is their primary spoken language day-to-day. We include attention and fluency checks to ensure our annotations are of high quality. Annotators use a 5-point scale for labeling: \u201cRude\u201d, \u201cSlightly Rude\u201d, \u201cNeutral\u201d, \u201cSlightly Polite\u201d, and \u201cPolite\u201d, with three annotators labeling each utterance. We observe an average annotator agreement (Fleiss\u2019 kappa) of 0.186 across languages.\nGiven the highly subjective nature of politeness, we expect to see a score in this range. Additionally, we convert the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al., 2013) to a 5-point scale and observe a Fleiss\u2019 kappa of 0.153, indicating our agreement aligns with past work.\nA key distinction between our dataset and the Stanford Politeness Corpus/TYDIPis that we do not normalize the scores of each annotator to be centered at neutral. Levels of politeness vary culturally (Leech, 2007; Pishghadam and Navari, 2012); we therefore do not make any assumptions about the politeness level of an average utterance.\nFigure A6 provides a visualization of score distributions in our final annotated dataset. We observe that the average utterance from English, Spanish, and Chinese is closest to neutral, while the average Japanese utterance is closer to slightly polite."
        },
        {
            "heading": "4 Comparing Politeness via PoliteLex",
            "text": "PoliteLex (Li et al., 2020) consists of curated lexica to measure politeness in English and Chinese, based on the twenty politeness strategies introduced by Danescu-Niculescu-Mizil et al. (2013). We use MLC to expand PoliteLex to cover all four of our languages.\nChinese PoliteLex has some additional lexical categories, such as taboo words and honorifics, to account for cultural and linguistic norms in Chinese that do not have an English equivalent. As politeness is expressed more similarly within Eastern and Western cultures than between them (SpencerOatey and K\u00e1d\u00e1r, 2016), we use English PoliteLex as the seed for Spanish and Chinese PoliteLex as\nthe seed for Japanese, to create a set of four lexica with parallel, comparable categories.\nWhen purifying our expanded lexica in Spanish and Japanese, we use the full set of 41,000 scraped talk pages to calculate internal correlation and remove uncorrelated words from each category.\nNext, we fine-tune XLM-RoBERTa models (Conneau et al., 2020) on our holistic politeness dataset (see Appendix A.4 for training details) and use the SHAP library (Lundberg and Lee, 2017) to extract Shapley values for each utterance. Finally, we apply Feature Set Aggregation to calculate importance scores for each PoliteLex category.\nDataset coverage. Table 1 analyzes our generated politeness lexica in Spanish and Japanese. We measure dataset coverage for each language \u2013 an utterance is \u201ccovered\u201d if it contains at least one word in a lexical category, and we define dataset coverage as the percent of covered utterances. In both cases, the lexica generated by MLC has better coverage than 1:1 machine translation using Google Translate.\nResults. Figure 4 details the resulting categorylevel importances from select PoliteLex categories, with the frequency of words in each category given in parentheses. Categories with positive importance are indicators of politeness, as the extracted"
        },
        {
            "heading": "Language Lexica % of DatasetCovered",
            "text": "Shapley values are highest for words within those categories. Similarly, categories with negative importance scores indicate rudeness.\nFor certain categories, we see a strong similarity across languages. Apologetic expressions (e.g. \u201cI\u2019m sorry\u201d, \u201cmy bad\u201d) and expressions of gratitude (e.g. \u201cthank you\u201d, \u201cI appreciate it\u201d) tend to universally indicate politeness. Interestingly, we see Japanese speakers using expressions of gratitude and apology with the highest frequency across languages.\nWe also notice interesting differences. The word\n\u201cplease\u201d in English, Spanish, and Chinese does not indicate politeness, despite being used with similar frequency in all four languages. For example, the following English and Spanish utterances\n\u201cThis has been debated to death; please read the archives.\u201d\n\u201cAntes de cuestionar si lo que digo es verdad, por favor trate de corroborarlo usted mismo.\u201d (\u201cBefore you question whether what I say is true, please try to verify it yourself.\u201d)\nare both labeled by annotators to be quite rude, despite containing the word \u201cplease\u201d. In Japanese however, \u201cplease\u201d strongly indicates politeness.\nAdditionally, contrary to the findings of Li et al. (2020), expressions of in-group identity (e.g. \u201cbro\u201d, \u201cmate\u201d) are indicators of rudeness in English and Chinese, but indicators of politeness in Japanese. This may be because these terms are uncomfortably familiar, and so taken as rude, or due to sarcastic uses of these terms in English and Chinese. This phenomenon does not appear to be paralleled in Japanese, as terms of in-group identity are very polite. We give examples of top words in each PoliteLex category in Table A6 and our full set of results for all categories in Figure A7."
        },
        {
            "heading": "5 Comparing Politeness via Dialogue Acts",
            "text": "Given our dataset is the first politeness dataset to include multiple types of sentences (i.e. dialogue\nacts), we additionally apply the second part of our framework to dialogue act groupings as categories. In the previous section, we compared how linguistic expressions of politeness differ across languages. In this section, we seek to compare how the linguistic form of politeness differs as well.\nTo classify the dialogue acts of each utterance, we machine translate our dataset to English. We then run a trained English dialogue act classifier provided by Omitaomu et al. (2022) on the translated dataset and label each sentence of an utterance with one of 42 dialogue acts (Stolcke et al., 2000). Table 2 shows examples for select dialogue acts.\nAs dialogue acts are sentence-level, we modify Equation (1) to aggregate over all tokens in a given sentence, as opposed to all tokens in a given word. Finally, we treat each dialogue act as a unique category (analogous to a lexical category) and use our feature set aggregation method to map sentencelevel SHAP values to their corresponding dialogue acts across our four languages.\nResults. Figure 5 shows the average importance of each dialogue act, with the frequency of each dialogue act given in parentheses. Once again, we observe some similarities across languages: conventional openings, conventional closings, and sentences of thanks are strong indicators of politeness across languages.\nHowever, statements appear to have differing roles across languages. Declarative statements\nmostly lean polite across languages, while statements expressing an opinion lean slightly rude in English, Spanish, and Chinese. Surprisingly, yes/no questions only indicate politeness in English, and are viewed as mildly rude in all other languages, particularly Chinese. Consider the following yes/no questions in English and Chinese:\n\u201cTo be pedantic, are we sure that he was born in Milton, West Dunbartonshire?\u201d\n\u201c\u6700\u540e\u5e94\u7528\u4e00\u8282\uff0c\u6709\u6ca1\u6709\u5fc5\u8981\u52a0\u5165\u90a3\u4e48\u591a \u56fe\u7247\uff1f\u201d (\u201cIn the last application section, is it necessary to add so many pictures?\u201d)\nTo an English speaker, both the English sentence and the Chinese translation appear to be similar levels of politeness. However, American annotators label the English question as \u201cNeutral\u201d while Chinese annotators label the Chinese question as \u201cSlightly Rude,\u201d highlighting the ways in which cultural norms influence perceptions of politeness.\nInterestingly, we do not observe any major differences in the frequency of dialogue acts across languages; conversations in all four languages appear to have a similar distribution of dialogue acts, though the average politeness of each dialogue act often varies based on language. Results for all dialogue acts are shown in Figure A8."
        },
        {
            "heading": "6 Ablation Analysis",
            "text": "The PoliteLex category-level importances in Figure 4 and dialogue act importances in Figure 5 are dependent on the Shapley values extracted from fine-tuned XLM-RoBERTa models. In this section, we analyze the effect of using alternate models and training paradigms.\nEffect of model size. To investigate the role of LM size and architecture, we fine-tune Llama-2-7b (Touvron et al., 2023) to analyze politeness. Comparing the results from Llama-2-7b to the results\nfrom XLM-RoBERTa, we notice stability in the direction of importance score (i.e. positive, negative, and neutral lexical categories/dialog acts are stable across both LMs). Interestingly, we observe differences in the magnitude of importance score (e.g. Llama-2-7b sees the \"Greeting\" category in Chinese to be a larger indicator of politeness than XLM-RoBERTa does).\nEffect of language-specific pretraining. To investigate the role of language-specific pretraining, we fully fine-tune four RoBERTa models trained on only their respective languages. Similar to Llama2-7b, we observe high similarity in the direction of the importance score compared to XLM-RoBERTa. We also notice much more similarity in the magnitude of importance scores. This may be due to inherent similarities between all RoBERTa models (parameter size, training methods, training data, etc.), which do not exist between XLM-RoBERTa and Llama-2-7b.\nOur ablation analysis reveals that different language models pay attention to the same markers when learning to predict politeness, but learn to weigh these markers in different ways. Overall, we notice stability in which lexical categories and dialog acts indicate politeness vs. rudeness. This suggests that our groupings for feature set aggregation are both stable and successful. Section C contains additional details."
        },
        {
            "heading": "7 Related Work",
            "text": "Multilingual style. Previous work on multilingual style predominantly focuses on training LMs to perform cross-lingual and multilingual style classification and style transfer. Key styles studied include formality (Briakou et al., 2021; Krishna et al., 2022; Rippeth et al., 2022) and emotion (\u00d6hman et al., 2018; Lamprinidis et al., 2021; \u00d6hman et al., 2020), with another body of work focusing on style-\naware multilingual generation with any subset of chosen styles (Niu et al., 2018; Garcia et al., 2021).\nExplaining style. One line of work builds on existing techniques (Lundberg and Lee, 2017; Ribeiro et al., 2016) to explain style within a single LM (Aubakirova and Bansal, 2016; Wang et al., 2021). Another line of work interprets style LMs by comparing learned features to those humans would consider important (Hayati et al., 2021), mapping feature attributions to topics (Havaldar et al., 2023b), and training models to output relevant features alongside their predictions (Hayati et al., 2023).\nPoliteness. Danescu-Niculescu-Mizil et al. (2013) presents one of the earliest quantitative analyses of linguistic politeness, with Srinivasan and Choi (2022) following in a multilingual setting. Other computational work focusing on politeness uses LMs to generate or modify text with a specified politeness level (Niu and Bansal, 2018; Fu et al., 2020; Mishra et al., 2022; Silva et al., 2022).\nPrevious work focused on multilingual style has little emphasis on investigating how style differs amongst languages. Additionally, most work on explaining style LMs is English-scoped, and thus, developed methods do not easily allow for cultural comparison. We are the first to present a method to compare styles across languages and provide quantitative, human-interpretable insights into how communication differs globally."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we present a framework to extract the knowledge implicit in trained multilingual LMs. This knowledge enables comparison of styles across languages via a human-interpretable common language for explanation. We also provide the first holistic multilingual politeness dataset, which we hope will encourage future research exploring cultural differences in style.\nOur framework provides insights into how style is expressed differently across languages. These insights can improve multilingual LMs \u2014 understanding how and why an LM generation is not stylistically appropriate can inform culturallyadaptable models. These insights can also help people learning a second language become more aware of the language\u2019s culturally-informed stylistic nuances.\nAt a higher level, our framework provides a general methodology for explaining LMs; using feature attributions such as Shapley values to provide explanations in terms of human-interpretable categories (e.g. lexica and dialogue acts) gives explanations that are both grounded in the model and useful to humans."
        },
        {
            "heading": "Limitations",
            "text": "When detailing our framework, we used FastText for Multilingual Lexica Generation and Partition SHAP (Lundberg and Lee, 2017) for Feature Set Aggregation. Our results are dependent on these two choices. Because FastText only supports word embeddings, we could only apply our MLC framework to words in the lexica. A contextual embedding could be used to additionally expand phrases. Additionally, we did not use native speakers to further purify our final lexica, as is the case in most past work curating multilingual lexica.\nIndividuals view politeness differently even within cultures, and as such, it is a highly subjective task. The subjectivity of politeness has been studied previously (Leech, 2007; Pishghadam and Navari, 2012; Spencer-Oatey and K\u00e1d\u00e1r, 2016; Placencia and Garcia-Fernandez, 2017) and as a result, there is no \u201ccorrect\u201d label for an utterance; this subjectivity contributes heavily to our annotator agreement scores and fine-tuned LM accuracy.\nWe also draw conclusions about politeness in various cultures from a single context. We only study Wikipedia Talk Page data, which reflects politeness in workplace communication, but expressions of politeness are likely to differ in other settings: non-work conversations, communication between friends or family, social media, etc."
        },
        {
            "heading": "Ethics Statement",
            "text": "When comparing styles across languages in this study, we treat language and culture as monolithic. However, we recognize different communities of people who speak the same language will use it differently. For example, politeness is likely expressed differently in Spain vs. Mexico, even though they are both Spanish-speaking countries.\nIn studying politeness, we recognize that it is highly contextual \u2013 calling a stranger \u201cbro\u201d can be perceived as an insult, while calling a close friend \u201cbro\u201d can be viewed as an expression of bonding and closeness. Age, gender, and other demographics also play an important role in perceived po-\nliteness in a conversation. Politeness is a deeply interpersonal act, but it is decontextualized when studied computationally; NLP studies of politeness destroy key components of it.\nWe additionally recognize the implications, both good and bad, of working towards culturallyappropriate stylistic language generation. Our method can be used to inform more culturally attuned LMs \u2013 multilingual polite agents are important for beneficial uses like teaching and therapy, but these polite agents could potentially also be used for more effective manipulation or misinformation tactics."
        },
        {
            "heading": "A Dataset Construction and Annotation: Additional Details",
            "text": ""
        },
        {
            "heading": "A.1 Data Collection",
            "text": "We scrape Wikipedia Editor Talk Pages in English, Spanish, Japanese, and Chinese using Wikimedia5 and keep the Talk Pages discussing articles that are present in all four languages. Next, we extract utterances, or two\u2013three sentence snippets from a single user, such that the length of a single utterance does not exceed 512 tokens. We anonymize utterances to remove names, replacing mentions with \u201c@user\u201d and hyperlinks, replacing hyperlinks with \u201c<url>\u201d. We then randomly sample 5,700 utterances from each language.\nWe select 5,700 as the size of the dataset for each language based on a power calculation to observe a discernible difference in politeness level between four languages. We use the results from a pre-trained English-only politeness classifier taken from Hayati et al. (2021) to get the parameters for the power calculation (\u03b1 = 0.05, \u03b2 = 0.2.)"
        },
        {
            "heading": "A.2 Annotation",
            "text": "To annotate our dataset, we set up surveys hosted using Prolific. We source annotators who are native speakers of the target language and additionally indicate that the target language is their primary spoken language day-to-day. We have each annotator label 100 utterances on a five-point scale: \u201cRude\u201d, \u201cSlightly Rude\u201d, \u201cNeutral\u201d, \u201cSlightly Polite\u201d, and \u201cPolite\u201d. Every 25 utterances, we give an attention check to each annotator. The attention check is in the form \u2018To ensure you are still paying attention, please select [indicated option]\u201d and translated to the target language. If the annotator fails any one of the four total attention checks, we discard all of their annotations. We additionally use this as a fluency check, as the attention check is fully written in the target language. We set up the survey such that each utterance is labeled by three different annotators.\nEach annotator is paid $12 an hour, and each survey took, on average, 20 minutes to complete. We do not notice any differences in survey time between languages."
        },
        {
            "heading": "A.3 Data Processing",
            "text": "Upon the completion of all four Prolific studies, we calculate annotator agreement using Fleiss\u2019 Kappa.\n5https://dumps.wikimedia.org/backup-index. html\nFigure A6: Distribution of annotated politeness scores per language.\nLanguage LearningRate Test RMSE Test r\nEnglish 1e\u2212 5 .667 .662 Spanish 1e\u2212 5 .738 .642 Japanese 1e\u2212 5 .659 .650 Chinese 5e\u2212 5 .612 .646\nTable A3: Details on model training. We fine-tune four XLM-RoBERTa models on our holistic politeness dataset. We show Test RMSE (root mean squared error) and product-moment correlation r for the final model.\nTable A4 shows agreement for each language. We observe a slightly higher agreement in all languages than that of the Stanford Politeness Corpus (Fleiss\u2019 kappa = 0.15)(Danescu-Niculescu-Mizil et al., 2013), indicating the highly subjective nature of politeness.\nNext, we convert the annotations to a numeric scale: \u201cRude\u201d= \u22122, \u201cSlightly Rude\u201d= \u22121, . . . , \u201cPolite\u201d= 2. Finally, we use each utterance\u2019s average numeric rating as its final label."
        },
        {
            "heading": "A.4 Model Fine-Tuning",
            "text": "To create LMs capable of detecting politeness in multiple languages, we fine-tune RoBERTa models on our holistic politeness dataset.\nWe train four XLM-RoBERTa (Conneau et al., 2020) models, one for each language. For training, we randomly split each language\u2019s utterances into an 80/10/10 train/validation/test split. We use the Huggingface Trainer library to fine-tune our models. All our models are trained for 50 epochs, and we select the epoch with the lowest validation loss as our final model. The results for the best iteration of each model can be found in Table A3.\nLanguage Num Utterances Fleiss\u2019s Kappa Mean Politeness Standard Deviation\nEnglish 5700 0.186 0.035 0.88 Spanish 5700 0.188 0.053 0.93 Japanese 5700 0.162 0.332 0.82 Chinese 5700 0.206 -0.123 0.80\nTable A4: Dataset Statistics: annotator agreement and details on annotation distribution.\nNote that we choose to train separate models for each language (as opposed to a single one for all languages) as we want our trained models to encode how politeness is expressed in each target language uniquely, without being influenced by utterances from other languages."
        },
        {
            "heading": "B Comparing Politeness Across Languages: Full Results",
            "text": "PoliteLex. Figure A7 shows category-level importance scores across all PoliteLex categories. Note that \u201cNA\u201d indicates that the category is not present in that language. We use English PoliteLex as the seed for Spanish PoliteLex, and Chinese PoliteLex as the seed for Japanese PoliteLex; the categories present in the expanded lexica parallel those present in the seed lexica.\nInteresting findings include:\n\u2022 Deference is seen as polite in English, Spanish, and Japanese, but is seen as slightly rude in Chinese.\n\u2022 Contrary to the findings of DanescuNiculescu-Mizil et al. (2013), we observe no concrete difference in politeness between utterances that use the subjunctive form of a word (\u201ccould\u201d, \u201cwould\u201d) vs. the indicative form (\u201ccan\u201d, \u201cwill\u201d).\n\u2022 Indirectness (\u201cby the way\u201d) is seen as polite in English and Japanese, but rude in Spanish and Chinese.\nAn overall analysis of our results reveals that all Japanese PoliteLex strategies indicate either neutrality or politeness. This is not the case in other languages; certain strategies do indicate rudeness. These findings suggest that politeness is expressed in a unique way in Japan, and perhaps rudeness is more subtle and cannot be fully captured by the lexical categories used in Chinese PoliteLex.\nDialogue acts. Figure A8 shows act-level importance scores across all dialogue acts, or types of sentences. We only show and compare dialogue acts that appear over ten times in each language, to ensure we have enough data for a meaningful comparison.\nInteresting findings include:\n\u2022 Rhetorical questions are seen as rude in English, Spanish, and Chinese, but closer to neutral in Japanese.\n\u2022 \u201cWh-\u201d questions (e.g. \u201cWhy did you do that?\u201d or \u201cWhere are you going?\u201d) are seen as universally rude.\n\u2022 Apology statements are seen as polite in all languages, but most polite in Chinese, despite the fact that Chinese speakers use apology statements with the lowest frequency.\nUnlike PoliteLex categories, certain dialogue acts do reflect rudeness in Japanese, namely \u201cWh-\u201d questions and statements that summarize or reformulate as a response."
        },
        {
            "heading": "C Ablation Analysis: Additional Details",
            "text": "To fine-tune Llama-2-7b with the computational resources available to us, we use parameter efficient fine-tuning (Liu et al., 2022). Specifically, we use the PEFT library (Mangrulkar et al., 2022) to finetune a subset of the weights. We use a learning rate of 1e\u2212 5 and default LORA parameters. We achieve similar performance to the metrics shown in Table A3.\nTo fine-tune the monolingual models (shown in Table A5), we use an identical set-up as Table A3, as the architecture of monolingual RoBERTa models and multilingual XLM-RoBERTa models is identical. Again, we observe similar performance to the metrics shown in Table A3.\nWe provide the resulting heatmaps in our repository: https://github.com/shreyahavaldar/ multilingual_politeness\nSetting Model Name\nMonolingual English roberta-base (Liu et al., 2019) Monolingual Spanish bertin-roberta-base-spanish (De la Rosa et al., 2022) Monolingual Chinese chinese-roberta-wwm-ext (Cui et al., 2020) Monolingual Japanese japanese-roberta-base (Cho and Sawada, 2021) Multilingual Llama-2-7b (Touvron et al., 2023)\nTable A5: LMs used in our ablation studies. We experiment with a different multilingual model to investigate the role of model size and architecture, as well as four separate monolingual models to investigate the role of language-specific pretraining.\nPoliteLex Category Three Most Frequent Words (English)\nGratitude thanks, thank you, i appreciate Deference good, great, interesting Greeting hello, hi, hey Apologetic sorry, apologize, apologies Please please, pls, plse Please Start please Indirect (btw) by the way, btw Direct Question what, when, how Direct Start but, and, so Subjunctive could you, would you Indicative can you, will you 1st Person Start i, my, mine 1st Person Plural we, us, our 1st Person i, my, me 2nd Person you, your, u 2nd Person Start you, your Hedges should, think, seems Factuality actually, really, in fact Emergency right now, immediately, at once Ingroup Identity mate, homie, dude Praise great, excellent, super Promise sure, must, certainly Together together Direct \"You\" you, u Positive like, well, good Negative issue, wrong, problem\nTable A6: For each PoliteLex category, we show the three most frequently occurring words in our holistic politeness dataset. Note that this table only shows words in English PoliteLex; Spanish/Chinese/Japanese PoliteLex contains different words in each category.\nFigure A7: The full set of PoliteLex category-level importances across languages. Each importance score indicates the category\u2019s average numerical contribution to an utterance\u2019s politeness label, where \u22122 = Rude, 0 = Neutral, and 2 = Polite. We additionally show the frequency of each category (% of total sentences that contain a word from the category.)\nFigure A8: The full set of dialogue act importances across languages. Each dialogue act importance score indicates the acts\u2019 average numerical contribution to an utterance\u2019s politeness label, where \u22122 = Rude, 0 = Neutral, and 2 = Polite. We additionally show the frequency of each dialogue act (% of total sentences classified as that act.)"
        }
    ],
    "title": "Comparing Styles across Languages",
    "year": 2023
}