{
    "abstractText": "Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9%, while using a domain expert 23x smaller.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aitor Ormazabal"
        },
        {
            "affiliations": [],
            "name": "Mikel Artetxe"
        },
        {
            "affiliations": [],
            "name": "Eneko Agirre"
        }
    ],
    "id": "SP:e569b95d532a60af0c8fb1b63774e961a2e1a39b",
    "references": [
        {
            "authors": [
                "Roee Aharoni",
                "Yoav Goldberg."
            ],
            "title": "Unsupervised domain clusters in pretrained language models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7747\u2013 7763, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Morishita",
                "Masaaki Nagata",
                "Ajay Nagesh",
                "Toshiaki Nakazawa",
                "Matteo Negri",
                "Santanu Pal",
                "Allahsera Auguste Tapo",
                "Marco Turchi",
                "Valentin Vydrin",
                "Marcos Zampieri."
            ],
            "title": "Findings of the 2021 conference on machine translation (WMT21)",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Galen Andrew",
                "Jianfeng Gao."
            ],
            "title": "Scalable training of L1-regularized log-linear models",
            "venue": "Proceedings of the 24th International Conference on Machine Learning, pages 33\u201340.",
            "year": 2007
        },
        {
            "authors": [
                "Monz",
                "Makoto Morishita",
                "Masaaki Nagata",
                "Toshiaki Nakazawa",
                "Santanu Pal",
                "Matt Post",
                "Marcos Zampieri."
            ],
            "title": "Findings of the 2020 conference on machine translation (WMT20)",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages",
            "year": 2020
        },
        {
            "authors": [
                "Samuel Weinbach"
            ],
            "title": "Gpt-neox-20b: An opensource autoregressive language model",
            "year": 2022
        },
        {
            "authors": [
                "Daixuan Cheng",
                "Shaohan Huang",
                "Junyu Bi",
                "Yuefeng Zhan",
                "Jianfeng Liu",
                "Yujing Wang",
                "Hao Sun",
                "Furu Wei",
                "Denvy Deng",
                "Qi Zhang"
            ],
            "title": "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Markus Freitag",
                "Ricardo Rei",
                "Nitika Mathur",
                "Chi-kiu Lo",
                "Craig Stewart",
                "Eleftherios Avramidis",
                "Tom Kocmi",
                "George Foster",
                "Alon Lavie",
                "Andr\u00e9 F.T. Martins"
            ],
            "title": "Results of WMT22 metrics shared task: Stop using BLEU \u2013 neural metrics are better and more",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2021
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Mike Lewis",
                "Ari Holtzman",
                "Noah A. Smith",
                "Luke Zettlemoyer."
            ],
            "title": "DEMix layers: Disentangling domains for modular language modeling",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Margaret Li",
                "Mike Lewis",
                "Weijia Shi",
                "Tim Althoff",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Scaling expert language models with unsupervised domain discovery",
            "year": 2023
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Ruining He",
                "Julian McAuley."
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "Proceedings of the 25th International Conference on World Wide Web, WWW \u201916, page 507\u2013517, Republic and Can-",
            "year": 2016
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Yangsibo Huang",
                "Daogao Liu",
                "Zexuan Zhong",
                "Weijia Shi",
                "Yin Tat Lee"
            ],
            "title": "knn-adapter: Efficient domain adaptation for black-box language models",
            "year": 2023
        },
        {
            "authors": [
                "Bryan Klimt",
                "Yiming Yang."
            ],
            "title": "The enron corpus: A new dataset for email classification research",
            "venue": "In",
            "year": 2004
        },
        {
            "authors": [
                "Margaret Li",
                "Suchin Gururangan",
                "Tim Dettmers",
                "Mike Lewis",
                "Tim Althoff",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Branch-train-merge: Embarrassingly parallel training of expert language models",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Julian McAuley",
                "Christopher Targett",
                "Qinfeng Shi",
                "Anton van den Hengel."
            ],
            "title": "Image-based recommendations on styles and substitutes",
            "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2015
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Naman Goyal",
                "Xi Lin",
                "Xian Li",
                "James Cross",
                "Sebastian Riedel",
                "Mikel Artetxe."
            ],
            "title": "Lifting the curse of multilinguality by pre-training modular transformers",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Vin Sachidananda",
                "Jason Kessler",
                "Yi-An Lai."
            ],
            "title": "Efficient domain adaptation of language models via adaptive tokenization",
            "venue": "Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 155\u2013165, Virtual. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Hong Qian",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "title": "Black-box tuning for language-model-as-a-service",
            "year": 2022
        },
        {
            "authors": [
                "Marlies van der Wees",
                "Arianna Bisazza",
                "Christof Monz."
            ],
            "title": "Dynamic data selection for neural machine translation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1400\u20131410, Copenhagen, Denmark.",
            "year": 2017
        },
        {
            "authors": [
                "Biao Zhang",
                "Barry Haddow",
                "Alexandra Birch"
            ],
            "title": "Prompting large language model for machine translation: A case study",
            "year": 2023
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pretrained transformer language models",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Natural language processing (NLP) has witnessed remarkable progress in recent years thanks to the development of increasingly powerful LMs (Brown et al., 2020; Andrew and Gao, 2007; Chowdhery et al., 2022; Touvron et al., 2023). Since these models are usually generalists, it is often of interest to adapt them to new domains, underrepresented or not found in the original training data. Typically, domain adaptation techniques assume white-box access to the model parameters, for example by fine-tuning on a particular target domain (Gururangan et al., 2020).\nHowever, this approach has become increasingly infeasible given the ongoing paradigm shift in the field\u2014state-of-the-art models like GPT-4 and PaLM-2 are only accessible as black-boxes through inference APIs and, even when the model weights\nare available, the computational cost of fine-tuning large models can be prohibitive. Consequently, domain adaptation methods that cannot leverage the power of black-box LLMs are likely to fall behind.\nIn this work, we propose a simple and lightweight approach to adapt black-box LMs to new domains, without requiring access to weights or intermediate activations. Our method consists of two main steps: (1) training a small, white-box model on the desired target domain, and (2) learning a function that combines the probability distributions from the large black-box LM and the small domain expert LM, producing a new probability distribution. The combination function is a small neural network that is trained on a small validation dataset.\nWe evaluate our method by adapting a black-box model to three distinct domains and a downstream task\u2014machine translation (MT). In all cases, we observe that the combined model outperforms both the large black-box model and the small domain expert. This shows that it is possible to adapt blackbox LMs to new domains, opening an exciting line\nof research."
        },
        {
            "heading": "2 Proposed method",
            "text": "Our approach works in two steps: (1) we train a small domain expert LM, and (2) we learn a function that combines the outputs of the domain expert LM and a large black-box LM at the probability level.\nMore concretely, an LM defines a probability distribution over the possible continuations of any given text. That is, given a sequence of tokens x = (x1, x2, ..., xn) \u2208 V \u2217, where V is the model vocabulary, an LM parametrizes PLM (ynext|x), the probability that ynext is the continuation of x in a text. We let PS denote our small domain expert LM, and PL denote the large black-box generalist LM. Our combination function f defines a new combined probability distribution PC : PC(ynext|x) = f(PS(\u00b7|x), PL(\u00b7|x))ynext . Here f : R|V | \u00d7 R|V | \u2192 R|V | is a vector-valued function that receives full probability distributions, and outputs a new probability distribution.\nTo train the domain expert LM, we fine-tune a pre-trained model on a small domain-specific dataset. For the combination function, we consider several alternatives of varying capacity:\n1. Mean. The arithmetic mean of the two distributions: f(y1,y2) = (y1 + y2)/2.\n2. Constant-scalar. A linear combination of the two input distributions, with a constant combination factor \u03bb: f(y1,y2) = \u03bby1 + (1\u2212 \u03bb)y2.\n3. Constant-vector. A token-wise version of the previous combination, where \u03bb \u2208 R|V | is a constant vector, and the combination factor varies per-token: f(y1,y2) \u221d \u03bb \u25e6 y1 + (1 \u2212 \u03bb) \u25e6 y2, where \u25e6 is the Hadamard (elementwise) product. Note the proportionality instead of equality in the definition, as a re-normalization is required when combining distributions per-token.\n4. Entropy-scalar. A scalar \u03bb is predicted from the entropies of each distribution, \u03bb = g(H(y1),H(y2)), and the output is a linear combination as in constant-scalar: f(y1,y2) = \u03bby1 + (1\u2212 \u03bb)y2. The function g is parametrized by a small neural network.\n5. Entropy-vector. An token-wise version of the previous combination, where a vector \u03bb =\ng(H(y1),H(y2)) \u2208 R|V | is predicted , and then the per-token combination is done as in constant-vector.\n6. Full-scalar. A single \u03bb is predicted from full input distributions, \u03bb = g(y1,y2), and then the output is a linear combination as in the constant combination: f(y1,y2) = \u03bby1 + (1\u2212 \u03bb)y2. The function g is parametrized by a small neural network.\n7. Full-vector. Token-wise version of the previous combination, where a vector \u03bb = g(y1,y2) \u2208 R|V | is predicted , and the pertoken combination is done as in constantvector.\nOn one end of the spectrum, the mean and constant-scalar combinations have very low capacity, having zero and one learnable parameters, respectively. On the other end, the full combinations can represent rich combination functions, taking advantage of the information in the full output distributions. The entropy combinations are motivated by the fact that we expect output distribution entropies to be informative to the combination function; intuitively, knowing how certain each model is should be helpful when deciding which model to give more weight to. Additionally, token-wise versions of each method further increase the capacity of the combination function. This setup allows us to study how important combination function capacity is for the performance of the adapted model, as well as how this relates to the amount of data used for learning the combination.\nThese combination functions can be learned without any access to the LMs\u2019 weights or internal states, and require only a forward pass through the small set used to train the combination network. We refer to the process of training the small network that parametrizes the combination function as fitting the combination function. Once the combination function is fit, the combined model outputs valid probability distributions over continuations, and can be used as a regular LM."
        },
        {
            "heading": "3 Experimental setup",
            "text": ""
        },
        {
            "heading": "3.1 Models",
            "text": "We use OPT-30B and OPT-1.3B (Zhang et al., 2022) as our large black-box and small white-box LMs, respectively. Our choice of OPT is motivated by the following reasons:\n1. Both the small and large models must share the tokenizer in our current formulation.1\nSince we want to train the small domain experts by fine-tuning an existing model, we need a model family that has both large and small models sharing the same tokenizer, which OPT provides.\n2. To rigorously determine what constitutes a new domain for the models, we need to know what data they were trained on, which is not public for most proprietary models behind APIs.2\nWe report results for the large model and the small fine-tuned model, which can be taken as the baselines, as well as their combination through our proposed method. For the parametrization of the combination functions, we use small neural networks, with the following architectures:\n\u2022 Constant-scalar: A single neuron with no input, passed through a sigmoid to force it into (0, 1).\n\u2022 Constant-vector: A vector of neurons with no input, passed through a sigmoid to force it into (0, 1)|V |.\n\u2022 Entropy-scalar: Input layer is twodimensional, consisting of both entropies, followed by 1D BatchNorm, two hidden layers of dimension 512, with ReLU nonlinearities, and a one-dimensional output layer with a sigmoid non-linearity, to force it into (0, 1).\n\u2022 Entropy-vector: Input layer is same as for entropy-scalar, followed by 1D BatchNorm, two hidden layers of dimension 512, with ReLU non-linearities, and a |V |-dimensional output layer with a sigmoid non-linearity, to force it into (0, 1)|V |.\n\u2022 Full-scalar: Input layer is 2|V |-dimensional, consisting on the concatenated output distributions for each model, followed by 1D BatchNorm, two hidden layers of dimension\n1Although it is possible to either adapt LMs to a new vocabulary or extend our approach to work with different tokenizers, that would add a new dimension to our experiments, separate from the core research question that we want to study.\n2While this is not a problem for applying our method in practice, it does rule out proprietary black-box models for scientific study.\n512, with ReLU non-linearities, and a onedimensional output layer with a sigmoid nonlinearity, to force it into (0, 1).\n\u2022 Full-vector: Input layer same as for fullscalar, 2|V |-dimensional, followed by 1D BatchNorm, two hidden layers of dimension 512, with ReLU non-linearities, and a |V |- dimensional output layer with a sigmoid nonlinearity, to force it into (0, 1)|V |.\nWe train all combination networks using the Adam optimizer and a learning rate of 2e\u22123 with the exception of constant-vector, for which we use a learning rate of 1e\u22122, and a batch size of 1024. We run optimization for a single epoch in all cases, as we found this to be enough in preliminary experiments.\nNote that the mean combination function has no learnable parameters. Finally, we also report maxprob oracle results as the upper-bound, which simulates a perfect combination function that gives 100% of the weight to the best model for any given token."
        },
        {
            "heading": "3.2 Evaluation",
            "text": "For evaluation, we adapt our model for three new domains and a downstream task. The three new domains are defined by three datasets:\n\u2022 The Amazon Reviews dataset (McAuley et al., 2015; He and McAuley, 2016), consisting of a large collection of reviews and ratings entered by users on the Amazon website.\n\u2022 The Enron Emails dataset (Klimt and Yang, 2004), consisting of internal emails made public by the Federal Energy Regulatory Commission during the investigation of the Enron company.\n\u2022 The FreeLaw subset of The Pile (Gao et al., 2021), consisting of a large collection of court opinions from federal and state courts.\nFor each dataset, we extract two sets of 1000 1024-token sequences, which we call train-fit and test, respectively, and use the rest for the train set. The train-fit sets are used to fit the combination functions, and we report perplexity on the test sets for evaluation. We use the train set to fine-tune OPT-1.3B using the Adam optimizer, a 1024-token sequence length, a fixed learning rate of 4e\u22124, and a batch size of 1024 \u2217 90 = 92160 tokens. In the\ncase of Enron Emails we fine-tuned for a single epoch, corresponding to 3000k steps. For Amazon Reviews and FreeLaw we performed 30k steps, and had to stop well before reaching the first epoch, due to compute constraints. Unless otherwise stated, the full train-fit sets are used to fit the combination functions.\nFor downstream evaluation, we experiment on English-Czech and English-German MT using the WMT21 dataset (Barrault et al., 2020). We create a training set by verbalizing all the sentence pairs and concatenating them into a single corpus. Details of the verbalization templates can be found in Appendix A. We create a validation set following the same procedure on the WMT20 test set (Akhbardeh et al., 2021), and extract a train-fit set of 1000 1024-token sequences for fitting the combination functions, as we do in domain adaptation. Following the recommended practice in the area (Freitag et al., 2022), we use BLEURT (Sellam et al., 2020) on the WMT21 test set as our evaluation metric, and report additional results with BLEU (Papineni et al., 2002) in Appendix B. We used 3-shot prompting for evaluation, as longer sequence lenghts resulted in OOM issues in our hardware. We use the training set to fine-tune OPT1.3B using the exact same settings described above. We train for 2k steps, corresponding to a total of around 2.5 million parallel sentences.3\n3Although the full combined training set for EnglishGerman and English-Czech is bigger than 2.5M parallel sentences, we were interested in simulating the setting where limited translation data is available. Given enough parallel data, one can train a strong translation system from scratch, without having to adapt a generalist model."
        },
        {
            "heading": "4 Results",
            "text": "We next present our main results on domain adaptation (\u00a74.1) and MT (\u00a74.2)."
        },
        {
            "heading": "4.1 Domain adaptation",
            "text": "We report domain adaptation results in Table 1. We observe that the combined models are able to achieve substantially lower perplexities than either of the individual models. Even simple averaging works remarkably well, improving over both baselines in Amazon Reviews and FreeLaw, but learned combinations perform best. The entropy-scalar combination works best across the board, achieving a relative improvement in perplexity of 9% in Amazon Reviews, 2% in Enron Emails and 4% in FreeLaw over the best single model. This supports our hypothesis that output distribution entropies are informative to the combination function. However, higher capacity combination functions like full-scalar work better in some cases, as is the case for Amazon Reviews.\nOverall, our results show that the adapted model is able to leverage domain-specific knowledge in the small model, as well as the knowledge in the large generalist model, in order to improve over either of them. However, there is still a significant gap between the adapted models and the max-prob oracle, suggesting gains could still be made through a better combination function."
        },
        {
            "heading": "4.2 Machine translation",
            "text": "Table 2 reports downstream results on MT. As for domain adaptation, all the learned combinations outperform both the small fine-tuned model and the large black-box model. This shows that our approach can work for adaptation to downstream tasks, and is not limited to domain adaptation. Once again, the simple mean combination per-\nforms very well, obtaining the second best results after entropy-vector. In any case, the combination function has a relatively small impact in MT, and even the worst performing approach brings large improvements over the baseline."
        },
        {
            "heading": "5 Analysis",
            "text": "In this section, we study the following aspects of our approach:\n\u2022 How dependent is the quality of the resulting model on the amount of data used to fit the combination function?\n\u2022 How dependent is the quality of the resulting model on the amount of data used to fine-tune the small LM?\n\u2022 How much is general language modeling performance degraded by domain adaptation?\n\u2022 Is the learned combination interpretable?"
        },
        {
            "heading": "5.1 Effect of the amount of data for fitting",
            "text": "In order to study how the performance of the adapted model varies with respect to the amount of data used to fit the combination function, we fit each combination function three times, on a varying number of tokens. We report results for the Amazon Reviews dataset in Table 3, and additional results in Appendix B.\nAs expected, performance improves with more training data. However, the difference varies across methods. For example, constant-scalar, which has a very low capacity, performs equally well when trained on 100 or 1000 sequences. On the other hand, the full-scalar and full-vector functions, that take the entire probability distribution as input, benefit from more training sequences. The entropyscalar combination strikes a good balance, performing well across the board, and retaining strong performance when fit on as little as 100 sequences."
        },
        {
            "heading": "5.2 Effect of fine-tuning steps",
            "text": "Figure 2 shows the performance of the adapted models, when fine-tuning the small model for a varying number of sequences. At step 0 (i.e., before fine-tuning begins), the small LM corresponds to vanilla OPT-1.3B, which performs considerably worse than OPT-30B on Amazon Reviews. Even in that case, entropy-scalar performs on par with OPT30B, while mean is slightly worse. This shows that learnable combination functions are able to\navoid any loss in performance when combining with a poor domain expert. At the same time, it is also remarkable that the combination of vanilla OPT-1.3B and OPT-30B is not better than OPT30B alone. This can also be seen in Table 4, which compares using vanilla OPT-1.3B and fine-tuned OPT-1.3B as the small model. This shows that our reported improvements do not solely come from an ensembling effect, and our proposed approach effectively combines the power of the large LM and the domain expertise of the small LM.\nIn addition, we observe that our combined LM substantially improves upon each individual LM as early as step 3000. In fact, the gap between the small fine-tuned LM and our combined LM slightly narrows as training progresses. For instance, for entropy-scalar, the gap between the small LM and the combined LM is 2.18 perplexity points at step 3000 (12% relative improvement), which goes down to 1.5 for the fully fine-tuned model (9% relative improvement). This is intuitive, as the more data is available in the target domain, the less useful will be integrating the general knowledge in the large LM."
        },
        {
            "heading": "5.3 Effect on general language modeling",
            "text": "We are also interested in measuring how well the adapted models retain the general language modeling ability of the original large model. We use perplexity on The Pile (Gao et al., 2021) as a proxy of general language modeling performance, as it is a large collection of many datasets from different domains, often used to train generalist LMs (Black et al., 2022; Biderman et al., 2023). To this end, we also extract random train-fit and test subsets from The Pile. While some subsets of The Pile are also present in the training data for OPT, we do not measure performance on The Pile as a benchmark for model quality, and are only interested in it as a proxy for degradation in general language modeling ability of the adapted models.\nWe compare fitting the combination function on the target domain train-fit, as done throughout the paper, as well as on the combination of the target domain and The Pile train-fit sets. Table 5 reports results for Amazon Reviews, and full results can be found in Appendix B.\nWhen fitting the combination function on Amazon Reviews, we observe a significant degradation on The Pile. However, different combination methods behave differently in this regard. For example, entropy-scalar and full-vector perform similarly in Amazon Reviews (15.50 vs 15.43), but the former performs much better on The Pile (7.35 vs 10.07). It is also remarkable that The Pile perplexity of the combined model remains far better than the small fine-tuned LM (e.g. 7.35 for entropy-scalar vs 19.78 for the small LM), while also performing better in-domain.\nWhen fitting the combination function on the mixin set, we observe that performance on The\nPile is almost entirely preserved, at the expense of a slight degradation on Amazon Reviews. For example, for full-scalar, the combination fit on the mixin set achieves a perplexity of 15.45 on Amazon Reviews and 6.85 on The Pile, both within 0.1 of the best results for each dataset.\nOverall, these results show that a large model can be adapted to a particular domain while mitigating degradation in the general domain by mixing in-domain and general text to train the combination function. Additionally, we find that different combination methods exhibit different behavior when it comes to general performance degradation, even when they exhibit similar in-domain performance."
        },
        {
            "heading": "5.4 Is the model combination interpretable?",
            "text": "We next analyze whether the weights given to each model by the combination function are interpretable. Figure 3 illustrates this over a random sample from Amazon Reviews: we show which tokens are better predicted by each model, along with which model is given a higher weight for each token. Although we do not identify a clear pattern for which tokens are better predicted by each model, we do observe that the coloring in the top and the bottom visualizations match quite closely. This means that the learned combination function is quite good at predicting when each model should be given a higher weight.4\nIn order to quantitatively analyze this, we measure the Spearman correlation between the weight given by the combination function, and the actual difference in log probabilities for each token. Re-\n4A perfect combination function (corresponding to the max-prob oracle in Table 1) would always give 100% of the weight to the best model for any given token, and both images would match up perfectly.\nsults are shown in Table 6. We limit our analysis to entropy-scalar and full-scalar, as they are the only ones that output a single combination factor that depends on the input. We observe significant correlations for all datasets, with entropy-scalar achieving better correlation than full-scalar, especially on The Pile. This is consistent with the results in Table 5, where full-scalar suffers a bigger performance loss on The Pile. Somewhat surprisingly, correlation for entropy-scalar is better on The Pile than on the in-domain dataset, even though the combination function is fit on the in-domain train-fit. One possible explanation is that The Pile better represents the training distribution of the large LM, making it better calibrated on it, which makes it easier for entropy-scalar to make predictions."
        },
        {
            "heading": "6 Related work",
            "text": "We present related work on domain adaptation of LMs (\u00a76.1), and language modeling through domain experts (\u00a76.2)."
        },
        {
            "heading": "6.1 Domain adaptation of LMs",
            "text": "Domain adaptation of LMs is an extensively studied line of research. Traditional approaches include fine-tuning the model on domain-specific corpora, (Devlin et al., 2019; Liu et al., 2019; Gururangan et al., 2020), data selection on the original general corpus (Aharoni and Goldberg, 2020; van der Wees et al., 2017), and adapting or extending the tokenizer to achieve better performance on the target domain (Sachidananda et al., 2021).\nAlthough effective, these full fine-tuning techniques are often infeasible at scale due to the excessive compute required. Some approaches aim to reduce the resources required to fine-tune large\nmodels through parameter-efficient adaptation techniques, such as adapters (Houlsby et al., 2019), soft-prompt tuning (Liu et al., 2022), or low-rank adaptation (Hu et al., 2022). However, all of these techniques require white-box access to the original model and full backward passes, making them incompatible with black-box models.\nIn contrast, discrete prompt tuning approaches allow for treating the large model as a black-box (Shin et al., 2020; Sun et al., 2022; Zhang et al., 2023; Cheng et al., 2023). However, these approaches have only been proven in the limited setting of retrieving zero- or few-shot prompts that improve performance in a set of NLP tasks that the base black-box is already capable of performing, as opposed to a general method of black-box model adaptation.\nConcurrent to our work, Huang et al. (2023) propose leveraging KNN retrieval from a data-store to augment an existing black-box LM. However, they only experiment with small GPT2 models as the black-box, and the adaptation depends on finding an adequate datastore, limiting application to downstream tasks such as MT."
        },
        {
            "heading": "6.2 Domain experts for language modeling",
            "text": "Another line of research explores language modeling through a combination of separate domain experts. Li et al. (2022) achieve better performance than compute-matched single transformer models and highly parallel pre-training, by training independent domain experts, and combining them at the parameter level at inference time. Gururangan et al. (2023) extend this approach to automatically discovered domain clusters. Other approaches replace components of the transformer network with independent domain-dependent modules, as is the case of Gururangan et al. (2022) for metadata-defined domains, or Pfeiffer et al. (2022) for per-language modules. All of these are pre-training approaches and seek to train better or more efficient LMs, but cannot leverage existing powerful black-box models. Our work, in contrast, seeks to adapt an existing powerful black-box through leveraging a much smaller domain expert."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this work, we present a method for adapting black-box LMs to new domains and tasks, requiring access to probability-level outputs only. We first fine-tune a small domain expert white-box LM\n(a) Log-probability difference between the small and large LM. The small fine-tuned LM gave higher probabilities to the green tokens, while the large black-box LM gave higher probability to the red ones.\n(b) Weight given to each model by entropy-scalar. Tokens for which a higher weight was assigned to the small fine-tuned LM are shown in green, while tokens for which the large black-box was given a higher weight are shown in red.\nFigure 3: Difference between the small fine-tuned LM and the large black-box LM according to log-probability (a) and predicted weight (b). The closer the two match, the better the learned combination is at predicting which model will be \u201cright\u201d for a given token. The text sample was chosen randomly from the Amazion Reviews testset.\non a domain-specific corpus, and then combine it with the large black-box through a combination function learned on a small fitting set, yielding an adapted LM. Additionally, our method requires only access to probability level outputs, and thus allows to leverage powerful models optimized for inference or behind APIs, without the need for white-box access to the weights. We experiment on several datasets and a downstream task, as well as performing extensive analysis of our method, reaching several conclusions:\n\u2022 By combining a small domain expert and a large black-box model, the combined model outperforms either of the original ones in all cases, by as much as 9% perplexity for domain adaptation, and 6% BLEURT for MT, showing the effectiveness of our approach.\n\u2022 While higher capacity combination functions can perform better when more data is used to learn the combination, lower capacity combination methods remain competitive, and perform better when learned on little data. In particular, the entropy based combinations, entropy-scalar and entropy-vector, perform well across the board, even when fit on as little as 100 sequences.\n\u2022 Our approach is effective even when little\nis data available to fine-tune the domain expert. In fact, the gains are biggest in this scenario, as the advantage of leveraging a good black-box generalist decreases when a big indomain corpus is available.\n\u2022 While adaptation to new domains incurs a loss of general language modeling ability, this varies per combination method, and seems to be largely mitigated by augmenting the small set on which the combination function is fit.\nWhile our approach is effective, observed performance is still not close to the max prob oracle, which represents the ideal system where 100% of the weight is given to the best model at each time step. In future work, we would like to investigate the reasons for this gap, and potential ways of addressing it.\nLimitations\nWhile our method requires no access to the blackbox model\u2019s weights and intermediate activations, it does assume access to the full output probability distribution, which might not be available for some models served through APIs. The OpenAI API, for example, only returns the probabilities for the top 5 tokens. This is not an issue for the Constant combinations, and the Entropy methods could potentially\nalso be adapted, by estimating the entropy from top K probabilities.\nAdditionally, even though we don\u2019t fine-tune the black-box at all, our method does require a forward pass of the large black-box through a fitting set, which might potentially be costly if done through APIs, depending on pricing and the size of the fitting set."
        },
        {
            "heading": "Acknowledgements",
            "text": "Aitor and Eneko were partially supported by the Basque Government (IXA excellence research group IT-1805-22; IKER-GAITU project). Aitor was supported by a doctoral grant from the Spanish MECD."
        },
        {
            "heading": "A MT verbalizations",
            "text": "We verbalize the MT task by first adding a prompt describing the task, and then adding several translation examples. We chunk the translation examples in blocks of 5, that is, adding 5 translation examples per verbalization. We use two different task descriptiopns, shown in Table 7, and alternate evenly between both variations to create the verbalized training corpus. For inference, we use verbalization #1 and draw 3 random translation pairs from the WMT21 development set to construct a 3-shot prompt. We draw the random translation pairs once, and keep the 3-shot prompt fixed for all models."
        },
        {
            "heading": "B Full results",
            "text": "Full results for all combination methods, dataset sizes, and evaluation settings are shown in Table 8. Table 9 reports additional MT results using BLEU."
        }
    ],
    "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models",
    "year": 2023
}