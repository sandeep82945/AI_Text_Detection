{
    "abstractText": "Inspired by the superior language abilities of large language models (LLM), large visionlanguage models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yifan Li"
        },
        {
            "affiliations": [],
            "name": "Yifan Du"
        },
        {
            "affiliations": [],
            "name": "Kun Zhou"
        },
        {
            "affiliations": [],
            "name": "Jinpeng Wang"
        },
        {
            "affiliations": [],
            "name": "Wayne Xin Zhao"
        },
        {
            "affiliations": [],
            "name": "Ji-Rong Wen"
        }
    ],
    "id": "SP:704e2cec9591ad208e541be0c60882a62499c028",
    "references": [
        {
            "authors": [
                "Harsh Agrawal",
                "Peter Anderson",
                "Karan Desai",
                "Yufei Wang",
                "Xinlei Chen",
                "Rishabh Jain",
                "Mark Johnson",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "nocaps: novel object captioning at scale",
            "venue": "2019 IEEE/CVF International Conference on Computer",
            "year": 2019
        },
        {
            "authors": [
                "Marianne Monteiro",
                "Jacob L. Menick",
                "Sebastian Borgeaud",
                "Andy Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Kar\u00e9n Simonyan"
            ],
            "title": "Flamingo: a visual language model",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: visual question answering",
            "venue": "ICCV, pages 2425\u20132433. IEEE Computer Society.",
            "year": 2015
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: visual question answering",
            "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile,",
            "year": 2015
        },
        {
            "authors": [
                "Jinze Bai",
                "Shuai Bai",
                "Shusheng Yang",
                "Shijie Wang",
                "Sinan Tan",
                "Peng Wang",
                "Junyang Lin",
                "Chang Zhou",
                "Jingren Zhou."
            ],
            "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "venue": "CoRR, abs/2308.12966.",
            "year": 2023
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hal",
            "year": 2023
        },
        {
            "authors": [
                "Ali Furkan Biten",
                "Llu\u00eds G\u00f3mez",
                "Dimosthenis Karatzas."
            ],
            "title": "Let there be a clock on the beach: Reducing object hallucination in image captioning",
            "venue": "IEEE/CVF Winter Conference on Applications of",
            "year": 2022
        },
        {
            "authors": [
                "Keqin Chen",
                "Zhao Zhang",
                "Weili Zeng",
                "Richong Zhang",
                "Feng Zhu",
                "Rui Zhao."
            ],
            "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic",
            "venue": "CoRR, abs/2306.15195.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi."
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Wenliang Dai",
                "Zihan Liu",
                "Ziwei Ji",
                "Dan Su",
                "Pascale Fung."
            ],
            "title": "Plausible may not be faithful: Probing object hallucination in vision-language pre-training",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Zhe Gan",
                "Linjie Li",
                "Chunyuan Li",
                "Lijuan Wang",
                "Zicheng Liu",
                "Jianfeng Gao."
            ],
            "title": "Vision-language pretraining: Basics, recent advances, and future trends",
            "venue": "Found. Trends Comput. Graph. Vis., 14(3-4):163\u2013 352.",
            "year": 2022
        },
        {
            "authors": [
                "Peng Gao",
                "Jiaming Han",
                "Renrui Zhang",
                "Ziyi Lin",
                "Shijie Geng",
                "Aojun Zhou",
                "Wei Zhang",
                "Pan Lu",
                "Conghui He",
                "Xiangyu Yue",
                "Hongsheng Li",
                "Yu Qiao."
            ],
            "title": "Llama-adapter V2: parameter-efficient visual instruction model",
            "venue": "CoRR, abs/2304.15010.",
            "year": 2023
        },
        {
            "authors": [
                "Tao Gong",
                "Chengqi Lyu",
                "Shilong Zhang",
                "Yudong Wang",
                "Miao Zheng",
                "Qian Zhao",
                "Kuikun Liu",
                "Wenwei Zhang",
                "Ping Luo",
                "Kai Chen."
            ],
            "title": "Multimodal-gpt: A vision and language model for dialogue with humans",
            "venue": "arXiv preprint arXiv:2305.04790.",
            "year": 2023
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2017
        },
        {
            "authors": [
                "Micah Hodosh",
                "Peter Young",
                "Julia Hockenmaier."
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation metrics (extended abstract)",
            "venue": "IJCAI, pages 4188\u20134192. AAAI Press.",
            "year": 2015
        },
        {
            "authors": [
                "Yi-Chong Huang",
                "Xia-Chong Feng",
                "Xiao-Cheng Feng",
                "Bing Qin."
            ],
            "title": "The factual inconsistency problem in abstractive text summarization: A survey",
            "venue": "CoRR, abs/2104.14839.",
            "year": 2021
        },
        {
            "authors": [
                "Drew A. Hudson",
                "Christopher D. Manning."
            ],
            "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,",
            "year": 2019
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Yejin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "CoRR, abs/2202.03629.",
            "year": 2022
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu."
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "CoRR, abs/2305.03726.",
            "year": 2023
        },
        {
            "authors": [
                "Dongxu Li",
                "Junnan Li",
                "Hung Le",
                "Guangsen Wang",
                "Silvio Savarese",
                "Steven C.H. Hoi."
            ],
            "title": "LAVIS: A library for language-vision intelligence",
            "venue": "CoRR, abs/2209.09019.",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "CoRR, abs/2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal-",
            "year": 2022
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Oscar: Object-semantics aligned pretraining for vision-language tasks",
            "venue": "Computer Vi-",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: common objects in context",
            "venue": "Computer Vision ECCV 2014 - 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "CoRR, abs/2304.08485.",
            "year": 2023
        },
        {
            "authors": [
                "Haley MacLeod",
                "Cynthia L. Bennett",
                "Meredith Ringel Morris",
                "Edward Cutrell."
            ],
            "title": "Understanding blind people\u2019s experiences with computer-generated captions of social media images",
            "venue": "Proceedings",
            "year": 2017
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara L. Berg."
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "NIPS, pages 1143\u20131151.",
            "year": 2011
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Trevor Darrell",
                "Kate Saenko."
            ],
            "title": "Object hallucination in image captioning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31",
            "year": 2018
        },
        {
            "authors": [
                "Dustin Schwenk",
                "Apoorv Khandelwal",
                "Christopher Clark",
                "Kenneth Marino",
                "Roozbeh Mottaghi."
            ],
            "title": "A-OKVQA: A benchmark for visual question answering using world knowledge",
            "venue": "Computer Vision ECCV 2022 - 17th European Conference, Tel Aviv,",
            "year": 2022
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "ACL (1), pages 2556\u20132565. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Qinghao Ye",
                "Haiyang Xu",
                "Guohai Xu",
                "Jiabo Ye",
                "Ming Yan",
                "Yiyang Zhou",
                "Junyang Wang",
                "Anwen Hu",
                "Pengcheng Shi",
                "Yaya Shi",
                "Chenliang Li",
                "Yuanhong Xu",
                "Hehong Chen",
                "Junfeng Tian",
                "Qian Qi",
                "Ji Zhang",
                "Fei Huang"
            ],
            "title": "mplug-owl: Modularization",
            "year": 2023
        },
        {
            "authors": [
                "Peng Zhang",
                "Yash Goyal",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Yin and yang: Balancing and answering binary visual questions",
            "venue": "CVPR, pages 5014\u20135022. IEEE Computer Society.",
            "year": 2016
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Vinvl: Revisiting visual representations in vision-language models",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2021
        },
        {
            "authors": [
                "Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen."
            ],
            "title": "A survey of large language models",
            "venue": "CoRR, abs/2303.18223.",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny."
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "CoRR, abs/2304.10592.",
            "year": 2023
        },
        {
            "authors": [
                "Xueyan Zou",
                "Jianwei Yang",
                "Hao Zhang",
                "Feng Li",
                "Linjie Li",
                "Jianfeng Gao",
                "Yong Jae Lee."
            ],
            "title": "Segment everything everywhere all at once",
            "venue": "CoRR, abs/2304.06718.",
            "year": 2023
        },
        {
            "authors": [
                "Dai"
            ],
            "title": "2023b) as baseline results. B Additional Qualitative Analysis Results To better validate our hypotheses, We expand the analysis scope to all 80 objects in MSCOCO",
            "venue": "2022b) and OFA (Wang et al.,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) (Zhao et al., 2023) have shown remarkable abilities to solve various complex tasks by following human instructions in a zero-shot manner. The success of LLMs drives the researchers to devise more powerful multimodal models based on the superior capacity of LLMs, to enhance the understanding of visual semantics (Alayrac et al., 2022; Li et al., 2023b). As an exemplified work, GPT-4 (OpenAI, 2023) has exhibited the exciting performance of LLMs on multimodal tasks and scenarios.\nFollowing this line of research, a surge of studies (Zhu et al., 2023; Gao et al., 2023; Li et al.,\n*Equal contribution. \u2020Corresponding author.\n2023a) have been proposed to enhance the visionlanguage pre-trained model (VLPM) (Gan et al., 2022) by incorporating powerful LLMs (Touvron et al., 2023; Chiang et al., 2023), which are called large vision-language model (LVLM). Typically, existing work reuses the visual encoder in VLPMs to handle image data, while replacing the original language encoder with LLMs. After vision-language pre-training (Alayrac et al., 2022; Li et al., 2022b) and visual instruction tuning (Liu et al., 2023), LVLMs can fulfill complex tasks according to human instructions, demonstrating strong capacities in solving various vision-language tasks, e.g., image captioning (Ordonez et al., 2011; Hodosh et al., 2015; Sharma et al., 2018; Agrawal et al., 2019) and visual question answering (Antol et al., 2015a; Zhang et al., 2016; Goyal et al., 2017).\nDespite the success of LVLMs, previous work has revealed that their main components, i.e., LLMs and VLPMs, both suffer from hallucination. Especially, LLMs tend to hallucinate unintended text (Huang et al., 2021; Bang et al., 2023), and VLPMs might generate nonexistent objects in the image (Biten et al., 2022) (termed as object hallucination). It is generally believed that the hallucination would degrade the model performance and greatly harm the user experiences in real-world applications (MacLeod et al., 2017; Ji et al., 2022). Therefore, it is natural to ask the question: does hallucination still exist in LVLMs? In this paper, we systematically evaluate the issue of object hallucination in existing LVLMs, which refers to generating contents that are inconsistent with ground-truth objects in the given image.\nTo conduct our study, we first use the CHAIR (Caption Hallucination Assessment with Image Relevance) metric (Rohrbach et al., 2018), and examine the hallucination degree of several representative LVLMs on the MSCOCO dataset. Our preliminary experiments (Table 1) show that most of LVLMs severely suffer from object hallucina-\ntion, and are even more prone to hallucinate than small vision-language models. Besides, we find that the existing object hallucination evaluation method may not be best suited for LVLMs and further propose a Polling-based Object Probing Evaluation (POPE) method. The basic idea is to convert the evaluation of hallucination into a binary classification task by prompting LVLMs with simple Yes-or-No short questions about the probing objects (e.g., Is there a car in the image?). We show that such a method is more stable and flexible. Besides, by using different object sampling strategies, we validate that existing LVLMs are prone to hallucinate objects which frequently appear or co-occur in the visual instruction dataset.\nOur main contributions are as follows: (1) We conduct an empirical study on object hallucination for several representative LVLMs and find that they are highly affected by object hallucination. (2) We discuss the potential reasons behind this promblem, e.g., LVLMs tend to generate frequently appearing or co-occurring objects in the instruction corpora. (3) We propose an object hallucination evaluation approach called POPE, which is more stable and can be easily extended to unannotated datasets."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Large Vision-Language Model",
            "text": "Since LLMs have been shown to be general task solvers in a zero-shot/few-shot manner, a number of studies are devoted to improving VLPM by integrating powerful LLMs for more accurate language understanding and generation (Zhu et al., 2023; Liu et al., 2023; Dai et al., 2023a). In this paper, we re-\nfer to the enhanced VLPMs with the integration of LLMs as Large Vision-Language Models (LVLM).\nGenerally speaking, an LVLM consists of a vision encoder, a language encoder (i.e., an LLM), and a cross-modal alignment network. The training of LVLMs is generally composed of three major steps. First, a vision encoder and a language encoder are pre-trained on large-scale unimodal data (i.e., image and text data, respectively). Second, these two encoders are aligned through image-text alignment pre-training, which enables the LLM to generate a meaningful caption for a given image. Third, the aligned model is further fine-tuned on image-text instructions, so that it can generate satisfactory answers w.r.t. to a natural language question regarding a specific image. Note that in the second and third steps, we can optionally finetune different components instead of performing full-parameter fine-tuning.\nOnce the visual encoder and the LLM are well aligned, the derived LVLM can demonstrate a superior visual understanding ability. It can not only grasp the visual semantics of objects in the image, but also deeply understand the linguistic semantics for these objects by leveraging the parametric knowledge in the LLM. Further, the LVLM can perform complex reasoning over the related concepts about these objects, thus achieving an improved performance on a variety of multimodal tasks, e.g., visual question answering (VQA)."
        },
        {
            "heading": "2.2 Object Hallucination",
            "text": "Although LVLMs are powerful in solving visionlanguage tasks, they also suffer from the issue of object hallucination as VLPMs. In the literature\nof computer vision field (Rohrbach et al., 2018; Biten et al., 2022), object hallucination refers that the model generating descriptions or captions that contain objects which are inconsistent with or even absent from the target image. In general, object hallucination can be defined at different semantic levels. The most straightforward way is to define it over the object level, while more finegrained definitions might be concerned with the attributes or characteristics of objects. In this work, we focus on coarse-grained object hallucinations in the model-generated captions and leave finegrained object hallucinations such as the number, attributes, and positions of the object for future work. We present an example of object hallucination in Figure 1, where the hallucinated object \u201cmeat bowl\u201d,\u201cbottle\u201d, \u201cbeverage\u201d, \u201ccondiment\u201d are generated by the underlying LVLMs.\nThe hallucination phenomenon hinders the safe use of LVLMs in real-world deployment, as it may result in unexpected consequences caused by these hallucinated objects (MacLeod et al., 2017). For example, due to an incorrect understanding of the external environment, an autonomous driving system would make wrong decisions when encountering unexpected events, which might lead to serious safety issues. In order to mitigate these issues, this work aims to study how object hallucination exists in LVLMs from an evaluation perspective."
        },
        {
            "heading": "3 Object Hallucination in LVLMs",
            "text": "In this section, we evaluate the object hallucination problem in popular LVLMs using an existing method. We first introduce the evaluation settings and then analyze the experimental results."
        },
        {
            "heading": "3.1 Evaluation Settings",
            "text": "Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2018) is a popular metric for evaluating object hallucination in image captioning tasks. Given the ground truth objects in the image, CHAIR calculates the proportion of objects that appear in the caption but not the image. Existing work commonly adopts its two variants, i.e., CHAIRI and CHAIRS , which evaluate the hallucination degree at the object instance level and sentence level respectively. They can be formulated as:\nCHAIRI = |{hallucinated objects}| |{all mentioned objects}| , (1)\nCHAIRS = |{captions with hallucinated objects}|\n|{all captions}| . (2)\nWe select five recently released LVLMs, i.e., mPLUG-Owl (Ye et al., 2023), LLaVA (Liu et al., 2023), Multimodal-GPT (Gong et al., 2023), MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023a) and prompt them with following instructions to generate captions about images in MSCOCO (Lin et al., 2014): \u2022 I1: Generate a short caption of the image. \u2022 I2: Provide a brief description of the given image. Then, we calculate CHAIR on these captions. We leave more details about the introduction to the dataset and evaluated models in Appendix A."
        },
        {
            "heading": "3.2 Evaluation Results",
            "text": "Severity of Hallucinations. As the evaluation results illustrated in Table 1, most instruction-tuned LVLMs suffer from the object hallucination problem, even more serious than small models, e.g., LLaVA (32.7) v.s. OSCARbase (13.0) on CHAIRS using Instruction 1. It indicates that object hallucination is an important problem for LVLMs and deserves to be concerned about. As a comparison, InstructBLIP hallucinates less than other LVLMs. A possible reason is that its visual instructions are collected from a wide variety of publicly available datasets, which are relatively short. In contrast, other LVLMs mostly employ the visual instructions generated by unimodal LLMs (Liu et al., 2023). Such synthetic visual instructions are gener-\nally longer and more informative, but may involve unexpected descriptive information (hallucination inherent from LLMs) that is inconsistent with the image, which could mislead LVLMs.\nDisadvantages of CHAIR. As Table 1 shows, the evaluation results can be affected by other factors, e.g., instruction designs and the length of captions. Specifically, although the adopted two instructions have similar semantic meanings, LVLMs prompted by Instruction 2 can even result in doubled values of CHAIR metrics compared with those prompted by Instruction 1, and the performance order of some LVLMs also changes (e.g., CHAIRI values of LLaVA and MultiModal-GPT). It indicates the instability of the CHAIR metric when different instructions are employed. Besides, as CHAIR requires to examine whether the mentioned objects are hallucinated in the generated caption, it needs complex human-crafted parsing rules to perform exact matching, which has not been adapted to the special generation styles of LVLMs and may lead to misclassification errors.\nThus, it is necessary to consider a more suitable method that can stably and conveniently evaluate the object hallucination problem in LVLMs."
        },
        {
            "heading": "4 Influence of Instruction Data on Object Hallucination",
            "text": "Considering their impressive performance on complex vision-language tasks (Chen et al., 2023; Bai et al., 2023; Li et al., 2023a), it is counter-intuitive that the hallucination problem of LVLMs is so severe. Since smaller VLPMs suffer less from object hallucination, it is possible that the visual instruction-tuning process of LVLMs exacerbates object hallucination. In this section, we investigate the influence of the visual instruction data. We first make two basic hypotheses in Section 4.1 and then conduct qualitative and quantitative analysis to verify them in Section 4.2 and Section 4.3."
        },
        {
            "heading": "4.1 Hypotheses",
            "text": "As the visual instruction datasets of these LVLMs are mostly constructed based on MSCOCO (Lin et al., 2014), they generally share a similar unbalanced object distribution where top frequent objects occupy a major part of the dataset. After being fine-tuned on them, LVLMs may also be prone to generate (or hallucinate) frequently appearing objects in MSCOCO. Additionally, the presence of frequently co-occurring object groups (e.g., laptop, mouse and keyboard) may also contribute to\nobject hallucination. LVLMs can be elicited by the existing objects in the image to hallucinate other objects that frequently co-occur with them. Therefore, we hypothesize that (1) LVLMs are prone to hallucinate frequently appearing objects in the visual instruction datasets; (2) LVLMs are prone to hallucinate objects that frequently co-occur with ground-truth objects in the image. We conduct qualitative and quantitative analyses in the following parts to verify them."
        },
        {
            "heading": "4.2 Qualitative Analysis",
            "text": "We first qualitatively analyze the correlation between the appearance frequency and hallucination. For the first hypothesis, we plot a bar chart between the top ten frequently appearing objects in MSCOCO and their hallucination times in the validation set of MSCOCO; for the second hypothesis, we select the top ten frequently co-occurring objects with \u201cdining table\u201d and also plot a bar chart to show their hallucination times across images that really contain \u201cdining table\u201d. We show the results of MiniGPT-4, LLaVA, MultiModal-GPT and mPLUG-Owl in Figure 2. Obviously, with the decreasing of the occurrence frequency of objects (from right to left), there is a notable decrease in the hallucination times for all four LVLMs. It reveals that the frequently appearing and co-occurring objects in the visual instruction dataset are indeed more likely to be hallucinated by LVLMs. To better support our results, we also list the full statistics of all 80 COCO objects in Appendix B."
        },
        {
            "heading": "4.3 Quantitative Analysis",
            "text": "To further consolidate the above findings, we employ the top-k hit ratio (HR@k) to measure the consistency between the appearance frequency and hallucination times of objects, which is defined as:\nHRA@k = 1\nn n\u2211 i=1 Hit@k(i) Hallucinated(i) , (3)\nHRC@k(o) = 1\nm m\u2211 i=1 Hit@k(i, o) Hallucinated(i) , (4)\nwhere HRA and HRC quantify the correlations between hallucination times and appearing and co-occurring frequency respectively. n is the total number of images, Hallucinated(i) denotes the number of hallucinated objects in the i-th example, Hit@k(i) denotes the number of top-k frequently appearing MSCOCO objects in Hallucinated(i), and Hit@k(i, o) denotes the number of top-k frequently co-occurring objects with the probing object o in Hallucinated(i). Therefore, HR@k can reflect the proportion of top-k frequently appearing or co-occurring objects in all hallucinated objects.\nWe present the HRA and HRC(dining table) of top 30 objects in Table 2 and leave HRC@(chair) and HRC@(car) in Appendix C. The HRA@10 and HRC@10(dining table) of all LVLMs are near 0.5 and 0.6, respectively. It indicates that, on average, approximately half of the hallucinated objects in each image belong to the top 10 frequently appearing COCO objects, while more than half are among the top 10 frequently cooccurring objects with the objects already present in the image. When we broaden our observation to the top 30 objects, this proportion continues to increase. These findings further verify that LVLMs mostly hallucinate common objects in the visual instruction data and inspire us to design three sampling strategies in our evaluation pipeline."
        },
        {
            "heading": "5 POPE",
            "text": "In this section, we devise Polling-based Object Probing Evaluation (POPE), a simple yet effective approach for evaluating hallucination in LVLMs. We first provide an overview of POPE, and then evaluate the representative LVLMs with POPE. Finally, we discuss the stability and scalability of our method, and also analyze the impact of hallucina-\ntion on VQA task."
        },
        {
            "heading": "5.1 Overview of POPE",
            "text": "In the empirical results of Section 3, we have revealed the severity of the object hallucination problem in LVLMs and highlighted the limitations of the existing evaluation method, e.g., sensitive to instructions and biased to short captions. Besides, existing methods mostly rely on parsing the generated captions to extract the predicted objects, which usually require human-crafted complex rules and are still inevitable to omit or misclassify objects.\nTherefore, we consider devising a more suitable method for the stable, fair and flexible object hallucination evaluation of LVLMs, namely pollingbased object probing evaluation (POPE). Specifically, POPE formulates the evaluation of object hallucination as a binary classification task that prompts LVLMs to output \u201cYes\u201d or \u201cNo\u201d, e.g., \u201cIs there a chair in the image?\u201d. In this way, by sampling objects that LVLMs are prone to hallucinate, we can construct a set of hard questions to poll LVLMs. As standard answers to these questions are just \u201cYes\u201d or \u201cNo\u201d, we can easily identify them without complex parsing rules, and avoid the influence of instruction designs and caption length, thus guaranteeing stability, fairness and flexibility.\nDefinition. Given an image caption dataset, POPE focuses on constructing a set of triples, each of which consists of an image, multiple questions and their answers (\u201cYes\u201d or \u201cNo\u201d). The formulated definition of a triple can be described as:\n\u27e8x, {q(oi), ai}li=1\u27e9, (5)\nwhere x denotes the image, q(oi) is the question probing oi based on a template \u201cIs there a/an <object> in the image?\u201d, oi is the i-th object to be probed, ai is the answer to the question (\u201cYes\u201d or \u201cNo\u201d) and l denotes the number of polling questions per image. oi can be obtained either from annotations or the results of automatic segmentation tools like SEEM (Zou et al., 2023). We set the ratio between ground-truth and nonexistent objects as 1:1 for label balance. After constructing the evaluation triples, we can directly poll LVLMs with them and collect the predicted answers.\nPipeline. The whole POPE pipeline is presented in Figure 3. After obtaining objects in the image, we can start to building polling questions. Questions whose answers are \u201cYes\u201d can be directly built using ground-truth objects, while questions with the answer \u201cNo\u201d can be built by sampling from negative objects. Therefore, by devising different sampling strategies, we can validate whether LVLMs are prone to hallucinate specific objects, e.g., frequently appearing or co-occurring objects discussed in Section 4. Thus, we devise the following three sampling strategies: \u2022 Random Sampling: we randomly sample the objects that do not exist in the image. \u2022 Popular Sampling: we select the top-k most frequent objects in the whole image dastaset that do not exist in the current image, where k = l2 .\n\u2022 Adversarial Sampling: we first rank all objects according to their co-occurring frequencies with the ground-truth objects, and then select the top-k frequent ones that do not exist in the image.\nUnder the above three settings, we can build the evaluation questions of different difficulty levels. We evaluate previously mentioned LVLMs on them with the following metrics.\nMetrics. We adopt Accuracy, Precision, Recall and F1 score as the evaluation metrics. Accuracy reflects the proportion of correctly answered questions. Precision and Recall reflect the ratios of correctly answering questions whose answers are \u201cYes\u201d or \u201cNo\u201d, respectively. F1 score combines the results of Precision and Recall and we select it as the major metric for evaluation. Besides, we also report the ratio that LVLMs answer \u201cYes\u201d as a reference to analyze the model behaviors."
        },
        {
            "heading": "5.2 Evaluation on MSCOCO",
            "text": "We evaluate all the LVLMs with POPE built on the validation set of MSCOCO (Lin et al., 2014). We randomly select 500 images with more than 3 ground-truth objects in the annotations and construct 6 questions for each image (i.e., l = 6).\nThe results are presented in Table 3, where we can obtain a similar conclusion as in Table 1 that InstructBLIP performs the best, while LLaVA, MultiModal-GPT and mPLUG-Owl suffer more severe hallucination problem, whose F1 Score are below 70. It indicates that POPE can well estimate the degree of the hallucination problem in LVLMs. Besides, we find that LLaVA, MultiModal-GPT and\nmPLUG-Owl are extremely prone to answer \u201cYes\u201d (near 99%). It reveals that these three LVLMs are over confident, leading to lower accuracy on questions with the answer \u201cNo\u201d. Furthermore, the performance of LVLMs consistently decreases, from random settings, to popular and adversarial. It is consistent with our findings in Section 4, as LVLMs are prone to hallucinate the frequently appearing and co-occurring objects."
        },
        {
            "heading": "5.3 Advantages of POPE",
            "text": "As previously stated, the current approach for evaluating object hallucination in LVLMs like CHAIR is instruction-based, which is hindered by LVLMs\u2019 sensitivity to prompts and requires object annotations and manually designed rules for evaluation. In contrast, POPE is more stable to prompt forms and can be easily extended to unannotated datasets. Its probing result is also highly consistent with model\u2019s caption.\nStability. Regardless of the variations in prompt templates, POPE requires LVLMs to answer simple closed-ended questions, which is less likely to introduce ambiguity compared to instruction-based methods. Such characteristic contributes to its stability. To validate it, we evaluate LLaVA using both POPE and CHAIRI with four different prompts for each. The evaluation results are presented in Table 4. It can be observed that the standard deviation\nof the F1 score is significantly lower than CHAIRI , which confirms that POPE exhibits higher stability when faced with different prompts.\nScalability. As mentioned before, with the assistance of automatic segmentation tools, POPE can be easily extended to datasets without annotations. To validate it, we adopt SEEM (Zou et al., 2023) to annotate images from three datasets (i.e., MSCOCO, A-OKVQA (Schwenk et al., 2022) and GQA (Hudson and Manning, 2019)) and build POPE based on the segmentation results. We evaluate InstructBLIP, MiniGPT-4 and LLaVA on them and report the results in Table 5 and Table 11 (presented in Appendix D). In Table 5, the performances of all LVLMs mostly follow the same trend as annotation-based POPE in Table 3, i.e., Random > Popular > Adversarial, and InstructBLIP > MiniGPT-4 > LLaVA. Such consistency indicates the reliability of the SEEM-based POPE. Whereas, we also notice the performance gap between the two settings, e.g., F1 Score 71.37 v.s. 62.70 for MiniGPT-4 under the Adversarial setting. This phenomenon can be attributed to the finer granularity of the segmentation results generated by SEEM, which makes the POPE more challenging. In summary, when combined with automated segmentation tools, POPE can be easily extended to unannotated datasets and conduct effective evalua-\ntions on them.\nConsistency. A potential concern for POPE is whether the Yes/No responses of LVLMs genuinely reflect their perception of objects. To validate this, we measure the consistency between the POPE responses and captions generated by LVLMs. Specifically, we examine if objects that receive \"No\" responses seldom appear in the captions, and if objects frequently mentioned in captions usually receive \"Yes\" answers. We collect data from InstructBLIP and MiniGPT-4, given their relatively balanced yes/no distributions. Our findings reveal that out of the 1303 and 1445 objects that are given \"No\" responses by InstructBLIP and MiniGPT-4, merely 0 and 5 of those objects were referenced in captions. Moreover, out of the 664 and 1034 objects mentioned in the captions by these models, 664 and 961 respectively received a \"Yes\" verdict. Such results underscore a robust correlation between objects\u2019 presence in captions and Yes/No responses in POPE questions about them, validating the reliability of the POPE assessment."
        },
        {
            "heading": "5.4 Impact of Hallucination on Vision Tasks",
            "text": "Although existing LVLMs do suffer from significant object hallucination issues, it remains an open question whether these hallucinations have a strong impact on other vision tasks. Therefore, we compare their performance on POPE with VQA and\nimage captioning tasks. For VQA tasks, we evaluate the SEEM-based POPE and VQA scores of LVLMs on A-OKVQA and GQA datasets. Since LVLMs are prone to generate answers in an openended manner, we utilize ChatGPT to help parse the generated results to better evaluate the VQA performance. The details of evaluation settings are presented in Appendix E. For image captioning tasks, we evaluate the captions of 500 images in POPE with traditional metrics. The evaluation results are left in Appendix F.\nThe evaluation results are shown in Table 6. InstructBLIP performs the best under all settings, highlighting the importance of instruction-tuning on large visual instruction corpora. Note that since InstructBLIP has been trained on A-OKVQA, the result should be considered with caution. Furthermore, despite MiniGPT-4 achieving a higher F1 score compared to LLaVA, its performance on VQA tasks is relatively poor. A possible reason is that the instruction dataset of MiniGPT-4 only derives from image caption data, while LLaVA uses 158K visual instructions data involving complex visual questions. The results imply that the degree of hallucination may not be always consistent with the VQA performance and these two evaluation aspects are both important and should be considered in real-world applications."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we conducted evaluation experiments on several LVLMs and examined how they suffer from the object hallucination issue. By investigating the reasons for object hallucination, we empirically revealed that the object distributions of the visual instructions would affect the object hallucination of LVLMs. Besides, we also found that the existing hallucination evaluation methods might be affected by the input instructions and the generated\ntext of LVLMs, thus leading to less reliable evaluation results. To address this issue, we proposed a polling-based query method called POPE, to provide an improved evaluation approach for the object hallucination of LVLMs. Experimental results have shown that our proposed POPE can better evaluate the object hallucination issue of LVLMs."
        },
        {
            "heading": "7 Limitations",
            "text": "Despite that we have made extensive explorations, this work still has several limitations. First, we only focus on the object hallucination problem in LVLMs, while do not consider other aspects that can reflect the capacities of LVLMs. It means that the current evaluation task cannot measure the overall performance of LVLMs. In other words, if some model got a higher score in our evaluation setting, it does not necessarily indicate a stronger overall capacity than the one with a lower score. Second, due to the limitation of computation resources, we have to evaluate all models on a part of the validation set for each dataset. The reported results might be affected by the corresponding data distribution, though we have carefully set up the experiments. Third, our proposed POPE utilizes a matching-based method to determine whether LVLMs answer \u201cYes\u201d or \u201cNo\u201d, while empirically, LVLMs may occasionally fail to provide answers explicitly containing these words, which may lead to inaccurate evaluation results. Fourth, when combined with the automatic segmentation tool, the objects would be annotated based on the label set by the tool, which may be inconsistent with the collected human annotations, leading to a divergence in evaluation results. Finally, this work has only compared a small number of LVLMs, without including some recently released or closed-source ones. We leave the evaluation of more LVLMs as our future work.\nAlthough we have extensively discussed the hallucination issues of LVLMs, it does not indicate that we hold an negative opinion on their progress. Instead, it will be a very promising direction to develop LVLMs by leveraging the powerful LLMs. These models that were evaluated in this work have been excellent demonstrations for this direction. While, we do hope that our work can bring new ideas or insights to develop more reliable and human-aligned LVLMs."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. L233008 and 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. This research was also supported by Meituan."
        },
        {
            "heading": "A Details of Evaluation Settings",
            "text": "Dataset. MSCOCO (Lin et al., 2014) is a largescale image recognition, segmentation, and captioning dataset. Here, we randomly sample 2,000 images with annotations about contained objects and human-labeled captions from its validation set as our evaluation dataset. For computing the CHAIR metric on MSCOCO, we follow the settings in Rohrbach et al. (2018) which only considers 80 objects appearing in the MSCOCO segmentation challenge.\nModels. The evaluated LVLMs basically consist of three parts: a visual encoder, an alignment model, and a large language model. All the above models have been tuned on collected visual instruction data. A detailed comparison (e.g., backbones and trainable components) of these LVLMs is shown in Table 7. We also collect the evaluation results of smaller VLPMs, i.e., OSCAR (Li et al., 2020), VinVL (Zhang et al., 2021), BLIP (Li et al., 2022b) and OFA (Wang et al., 2022) from Dai et al. (2023b) as baseline results."
        },
        {
            "heading": "B Additional Qualitative Analysis Results",
            "text": "To better validate our hypotheses, We expand the analysis scope to all 80 objects in MSCOCO and present the result in this part.\nFor hypothesis (1), we present the cumulative proportions of the hallucination times of all 80 COCO objects in Table 8. The table demonstrates that, across all models, the top 30 objects comprise approximately 70% of all hallucinated objects. For hypothesis (2), we present the cumulative proportions of the hallucination times of all COCO objects that co-occur with dining table in Table 9. We also arrange these objects by their co-occurrence frequency. Similarly, the top 20 objects comprise about 80% of all hallucinated objects."
        },
        {
            "heading": "C Additional Quantitative Analysis Results",
            "text": "We present the HRC results of two other common objects, i.e., chair and car in Table 10, which show a similar trend with Table 2."
        },
        {
            "heading": "D Results of SEEM-based POPE on A-OKVQA and GQA",
            "text": "We adopt SEEM (Zou et al., 2023) to annotate images from A-OKVQA and GQA and build POPE\nLarge Language Model, respectively. denotes frozen and denotes trainable. The fine-tuning of LLM in MultiModal-GPT and mPLUG-Owl is implemented by LoRA.\nbased on segmentation results. We evaluate InstructBLIP, MiniGPT-4 and LLaVA. We also evaluate a full-data supervised-tuned smaller model, BLIP (Li et al., 2022b) to better reflect the degree of hallucination. The evaluation results are presented in Table 11."
        },
        {
            "heading": "E ChatGPT-assisted VQA Evaluation.",
            "text": "We employ the VQA score (Antol et al., 2015b) to assess VQA tasks with the help of ChatGPT. Considering that LVLMs generally produce openended responses, we enlist ChatGPT to assess whether the model\u2019s reply aligns with potential answers. The prompt we provide to ChatGPT is as follows: \u2022 \u201cYou are an examiner who can judge whether a student\u2019s answer matches the correct answers. Next, I will provide you with 10 correct answers in the form of a list and a student\u2019s answer. Please\njudge whether the student\u2019s answer matches one of the 10 correct answers. If it matches, please output the correct answer directly (must be an element in the list, if it matches multiple correct answers, please output the most frequent occurrence in the list); if not, please output <NAN> directly. Do NOT output anything else! correct answers: student answer:\u201d"
        },
        {
            "heading": "F Results of Image Captioning",
            "text": "The MSCOCO captioning results of LVLMs are showcased in Table 12. Generally, their captioning performance aligns with the POPE assessments, suggesting that object hallucination influences the efficacy of LVLMs in other vision tasks."
        }
    ],
    "title": "Evaluating Object Hallucination in Large Vision-Language Models",
    "year": 2023
}