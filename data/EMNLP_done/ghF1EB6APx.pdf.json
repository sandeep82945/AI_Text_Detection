{
    "abstractText": "Concept Bottleneck Models (CBMs) (Koh et al., 2020) assume that training examples (e.g., x-ray images) are annotated with high-level concepts (e.g., types of abnormalities), and perform classification by first predicting the concepts, followed by predicting the label relying on these concepts. The main difficulty in using CBMs comes from having to choose concepts that are predictive of the label and then having to label training examples with these concepts. In our approach, we adopt a more moderate assumption and instead use text descriptions (e.g., radiology reports), accompanying the images in training, to guide the induction of concepts. Our cross-modal approach treats concepts as discrete latent variables and promotes concepts that (1) are predictive of the label, and (2) can be predicted reliably from both the image and text. Through experiments conducted on datasets ranging from synthetic datasets (e.g., synthetic images with generated descriptions) to realistic medical imaging datasets, we demonstrate that cross-modal learning encourages the induction of interpretable concepts while also facilitating disentanglement. Our results also suggest that this guidance leads to increased robustness by suppressing the reliance on shortcut features.",
    "authors": [
        {
            "affiliations": [],
            "name": "Danis Alukaev"
        },
        {
            "affiliations": [],
            "name": "Semen Kiselev"
        },
        {
            "affiliations": [],
            "name": "Ilya Pershin"
        },
        {
            "affiliations": [],
            "name": "Bulat Ibragimov"
        },
        {
            "affiliations": [],
            "name": "Vladimir Ivanov"
        },
        {
            "affiliations": [],
            "name": "Alexey Kornaev"
        },
        {
            "affiliations": [],
            "name": "Ivan Titov"
        }
    ],
    "id": "SP:df5d935bb9adbf9440cfd68baf72bd2c760835c1",
    "references": [
        {
            "authors": [
                "David Alvarez Melis",
                "Tommi Jaakkola."
            ],
            "title": "Towards robust interpretability with self-explaining neural networks",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein."
            ],
            "title": "Neural module networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39\u201348.",
            "year": 2016
        },
        {
            "authors": [
                "Jasmijn Bastings",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Interpretable neural predictions with differentiable binary variables",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2963\u20132977, Florence, Italy. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "James Bergstra",
                "R\u00e9mi Bardenet",
                "Yoshua Bengio",
                "Bal\u00e1zs K\u00e9gl."
            ],
            "title": "Algorithms for hyper-parameter optimization",
            "venue": "Advances in neural information processing systems, 24.",
            "year": 2011
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Gon\u00e7alo M. Correia",
                "Vlad Niculae",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Adaptively sparse transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Yin Cui",
                "Yang Song",
                "Chen Sun",
                "Andrew Howard",
                "Serge Belongie."
            ],
            "title": "Large scale fine-grained categorization and domain-specific transfer learning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4109\u20134118.",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Jay DeYoung",
                "Sarthak Jain",
                "Nazneen Fatema Rajani",
                "Eric Lehman",
                "Caiming Xiong",
                "Richard Socher",
                "Byron C. Wallace."
            ],
            "title": "ERASER: A benchmark to evaluate rationalized NLP models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Cian Eastwood",
                "Christopher KI Williams."
            ],
            "title": "A framework for the quantitative evaluation of disentangled representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Ilyas",
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Logan Engstrom",
                "Brandon Tran",
                "Aleksander Madry."
            ],
            "title": "Adversarial examples are not bugs, they are features",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg"
            ],
            "title": "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole."
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Seth J Berkowitz",
                "Nathaniel R Greenbaum",
                "Matthew P Lungren",
                "Chihying Deng",
                "Roger G Mark",
                "Steven Horng"
            ],
            "title": "Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports",
            "year": 2019
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang."
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "International conference on machine learning, pages 1885\u20131894. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Thao Nguyen",
                "Yew Siang Tang",
                "Stephen Mussmann",
                "Emma Pierson",
                "Been Kim",
                "Percy Liang."
            ],
            "title": "Concept bottleneck models",
            "venue": "International Conference on Machine Learning, pages 5338\u20135348. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Locatello",
                "Dirk Weissenborn",
                "Thomas Unterthiner",
                "Aravindh Mahendran",
                "Georg Heigold",
                "Jakob Uszkoreit",
                "Alexey Dosovitskiy",
                "Thomas Kipf."
            ],
            "title": "Object-centric learning with slot attention",
            "venue": "Advances in Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Anita Mahinpei",
                "Justin Clark",
                "Isaac Lage",
                "Finale DoshiVelez",
                "Weiwei Pan."
            ],
            "title": "Promises and pitfalls of black-box concept learning models",
            "venue": "proceeding at the International Conference on Machine Learning: Workshop on Theoretic Foundation, Criticism, and",
            "year": 2021
        },
        {
            "authors": [
                "Andrei Margeloiu",
                "Matthew Ashman",
                "Umang Bhatt",
                "Yanzhi Chen",
                "Mateja Jamnik",
                "Adrian Weller"
            ],
            "title": "Do concept bottleneck models learn as intended? proceeding at the International Conference on Learning Representations: Workshop on Respon",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Oikarinen",
                "Subhro Das",
                "Lam M Nguyen",
                "Tsui-Wei Weng."
            ],
            "title": "Label-free concept bottleneck models",
            "venue": "International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel."
            ],
            "title": "Learning knowledge base inference with neural theorem provers",
            "venue": "Proceedings of the 5th workshop on automated knowledge base construction, pages 45\u201350.",
            "year": 2016
        },
        {
            "authors": [
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi."
            ],
            "title": "Neuron-level interpretation of deep nlp models: A survey",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1285\u20131303.",
            "year": 2022
        },
        {
            "authors": [
                "Sungbin Shin",
                "Yohan Jo",
                "Sungsoo Ahn",
                "Namhoon Lee."
            ],
            "title": "A closer look at the intervention procedure of concept bottleneck models",
            "venue": "Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman."
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada,",
            "year": 2014
        },
        {
            "authors": [
                "Leslie N Smith",
                "Nicholay Topin."
            ],
            "title": "Superconvergence: Very fast training of neural networks using large learning rates",
            "venue": "Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369\u2013386. SPIE.",
            "year": 2019
        },
        {
            "authors": [
                "David Steinmann",
                "Wolfgang Stammer",
                "Felix Friedrich",
                "Kristian Kersting."
            ],
            "title": "Learning to intervene on concept bottlenecks",
            "venue": "arXiv preprint arXiv:2308.13453.",
            "year": 2023
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "International conference on machine learning, pages 3319\u2013 3328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826.",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Jesse Vig",
                "Sebastian Gehrmann",
                "Yonatan Belinkov",
                "Sharon Qian",
                "Daniel Nevo",
                "Simas Sakenis",
                "Jason Huang",
                "Yaron Singer",
                "Stuart Shieber."
            ],
            "title": "Causal mediation analysis for interpreting neural nlp: The case of gender bias",
            "venue": "arXiv preprint",
            "year": 2020
        },
        {
            "authors": [
                "Elena Voita",
                "David Talbot",
                "Fedor Moiseev",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Yue Yang",
                "Artemis Panagopoulou",
                "Shenghao Zhou",
                "Daniel Jin",
                "Chris Callison-Burch",
                "Mark Yatskar."
            ],
            "title": "Language in a bottle: Language model guided concept bottlenecks for interpretable image classification",
            "venue": "Proceedings of the IEEE/CVF Conference",
            "year": 2023
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Maggie Wang",
                "James Zou."
            ],
            "title": "Post-hoc concept bottleneck models",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Matthew D Zeiler."
            ],
            "title": "Adadelta: an adaptive learning rate method",
            "venue": "arXiv preprint arXiv:1212.5701.",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The limited interpretability of modern deep learning poses a significant barrier, hindering their practical application in many scenarios. In addition to enhancing user trust, interpretations can assist in identifying data and model limitations, as well as facilitating comprehension of the causes behind model errors. Explainable ML has become a major field, with various methods developed over the year to offer different types of explanations, such as input attribution (Sundararajan et al., 2017; Simonyan et al., 2014), free-text rationales (Camburu et al., 2018) or training data attribution (Koh and Liang, 2017). Moreover, methods pursue diverse\nobjectives, for example, aiming to generate humanlike plausible rationales or extract faithful explanation from the model (Jacovi and Goldberg, 2020; DeYoung et al., 2020).\nIn this work, our specific focus is on a line of research that aims to develop neural models capable of making predictions via an (semi-) interpretable intermediate layer (Andreas et al., 2016; Bastings et al., 2019; Rockt\u00e4schel and Riedel, 2016; Koh et al., 2020). This class of models is appealing because the interpretable component creates a \u201cbottleneck\u201d in the computation, ensuring that all the information regarding the input is conveyed through discrete variables. As a result, these variables play a causal mediation role in the prediction\nprocess (Vig et al., 2020). One notable representative of this model class is Concept Bottleneck Models (Koh et al., 2020). CBMs, for a given input image, first predict which high-level concepts are activated, and then infer the class label based on these concepts. This simple and elegant paradigm is advantageous from a practical standpoint as the intermediate layer can be examined, validated and intervened on by an expert user. For instance, in medical diagnostics, the concepts could represent types of abnormalities detected from an X-ray, which makes the model predictions easier to scrutinize and verify for a specialist.\nDespite their interpretability advantages, CBMs have several critical limitations. Firstly, they require manual creation and annotation of a set of high-level concepts for each training example, which can be challenging or even unattainable in real-world scenarios. Secondly, concepts designed by domain specialists might not possess sufficient predictive power for classification or could be difficult to reliably detect from the examples (e.g., images).\nThis work aims to address these shortcomings of CBMs. It considers concepts as discrete binary latent variables, eliminating the need for an expert to predefine the concepts. Rather than relying on the annotation of examples with concepts our approach \u2013 Cross-Modal Conceptualization in Bottleneck Models (XCBs)1 \u2013 makes a weaker and arguably more realistic assumption that examples\n1Released at https://github.com/DanisAlukaev/XCBs\nare linked to texts (such as radiology reports) which serve as guidance for concept induction. Note that the text is not used at test time in our experiments.\nSpecifically, in training, we use an additional CBM model trained to predict the same target label, but this time using textual data. A regularizer is used to encourage prediction of the same set of concepts in both visual and text CBMs on each training example. In this way, XCB ensures that the concepts can be predicted from both text and images, discouraging the image classifier from relying on features that cannot be expressed through language and, thus, injecting domain knowledge from text into the visual model. Furthermore, an additional regularizer promotes the alignment of concepts with distinct text fragments, thereby enhancing their disentanglement and interpretability.\nWe conducted experiments involving XCBs on various datasets, including synthetic text-image pairs as well as a more realistic medical image dataset. Our experiments demonstrate that XCB tends to surpass the alternative in terms of disentanglement and alignment with ground-truth (or human-annotated) concepts. Additionally, we incorporated synthetic shortcuts into the images and observed that the inclusion of textual guidance reduced the model\u2019s tendency to overly rely on these shortcuts and, thus, made the model more robust."
        },
        {
            "heading": "2 Related Work",
            "text": "Concept Bottleneck Models (CBMs) (Koh and Liang, 2017) can be regarded as a representative\nof a broader category of self-explainable architectures (Alvarez Melis and Jaakkola, 2018). In essence, CBMs incorporate a bottleneck composed of manually-defined concepts, which provides a way of understanding \u2018reasons\u2019 behind the model predictions. Examples must be annotated with corresponding concepts, thus, facilitating the training of two distinct segments of the model: the input-toconcepts and concepts-to-label parts.\nWhile we argued that the bottleneck should be discrete to limit the amount of information it can convey, CBMs do not strictly adhere to this principle. In CBMs, even though the concepts themselves are discrete, their representation within the bottleneck layer is not. Specifically, each concept is represented as a logit from a classifier predicting that concept. This deviation from complete discreteness is non-problematic when the two parts of CBMs (input-to-concepts and concepts-to-label) are trained either independently or sequentially. Under such conditions, the logit would not be incentivized to retain input details unrelated to the concepts. Yet, the problem arises when these model parts are trained simultaneously (Margeloiu et al., 2021). Mahinpei et al., 2021 have shown that this information leakage can be alleviated by adopting strictly discrete variables and promoting disentanglement. These insights drove our modeling decisions, prompting us to employ discrete variables and use text (as well as specific forms of regularization) to encourage disentanglement.\nA recent approach titled label-free CBM (LCBM) (Oikarinen et al., 2023) focuses on addressing some of the same challenges we address with XCBs, specifically CBM\u2019s dependence on predefined concepts and the necessity for manual annotation of examples with such concepts. This study prompts GPT-3 (Radford et al., 2018) to produce a set of concepts appropriate for the domain. Following this, the CLIP model (Radford et al., 2021) is applied to label the images with concepts. In a parallel development, Yang et al. (2023) introduced a similar framework named Language model guided concept Bottleneck model (LaBo), which also incorporates GPT-3 and CLIP as key ingredients. One distinction in their approach is the employment of submodular optimization to generate a set of diverse and discriminative concepts. Although these methods show promise on large visual benchmarks, it is unclear how effectively they would work in specialized domains and how to in-\ntegrate additional human expertise in such models. We consider LCBM as an alternative to XCB in our experiments.\nYuksekgonul et al. (2023) introduced a method that posthoc transforms any neural classifier into a Concept-Based Model (CBM). In contrast to their approach, we leverage text to shape the representation inside the neural classifier, potentially making them more disentangled.\nIn addition to efforts focused on crafting methods for training CBMs, there is an intriguing branch of research dedicated to exploring the utilization of concepts within CBMs to intervene and modify the models (Shin et al., 2022; Steinmann et al., 2023). While this strand of research appears somewhat divergent from our primary interest, it underscores another significant application of CBMs \u2013 extending beyond mere prediction interpretation."
        },
        {
            "heading": "3 Proposed Framework",
            "text": "In this section we introduce a novel method for cross-modal conceptualization in bottleneck models (XCBs). We start with a high-level overview of the framework and introduce its core components. Further, we consider models used to process visual and textual modalities. Finally, we describe techniques that were used to encourage cross-modal agreement and disentanglement between the concepts."
        },
        {
            "heading": "3.1 Overview",
            "text": "The design of the framework (Fig. 1) encourages each element of the discrete latent representation produced by visual encoder to correspond to a single high-level abstraction, which can be expressed by a text span. Accordingly, we use an additional textual model that identifies concepts expressed in natural language and determines the correspondences to visual features.\nXCB can tentatively be divided into symmetric visual and textual components (Fig. 2, right). The former consists of a computer vision model F : x \u2192 f (feature extractor) producing latent representation f for an input image x and predictor Pf : f \u2192 y\u0302f solving the classification task. Similarly, the textual component includes a natural language processing model C : s \u2192 c (concept extractor) producing latent representation c for a sequence of tokens s and predictor Pc : c \u2192 y\u0302c. The discreteness of latent variables is imposed by a straight-through version of the Gumbel sigmoid\n(Jang et al., 2017) applied to bottlenecks. To reinforce agreement in feature and concept extractors at training time latent representations f and c are tied up using a similarity metric. The regularizer encourages model to generate a set of latent factors that could reliably be predicted by both visual and textual encoders."
        },
        {
            "heading": "3.2 Unsupervised Approach",
            "text": "The standard computer vision model for classification (Fig. 2, left) is generally composed of the feature extractor F and predictor Pf . The latent representation f is an output of a non-linear activation function (e.g., sigmoid). Each factor of f could, in principle, correspond to a single highlevel abstraction. Thus, latent representation is not a concept bottleneck per se, but arguably could emulate such behavior. In our experiments we use\nsuch a model with latent sigmoid bottleneck as a baseline and do not impose any pressure to make the representation interpretable or discrete.2"
        },
        {
            "heading": "3.3 Textual Model",
            "text": "The concept extractor is an integral part of XCBs used to process textual descriptions and derive set of concepts. It has been observed that many heads in multi-head attention specialize (Voita et al., 2019), i.e. become receptive to a narrow and unique set of words. Accordingly, we hope that the attention mechanism will reinforce disentanglement in the framework and, thus, use it in the concept extractor C. We will discuss below how we can further encourage disentanglement by modifying the attention computation.\nFig. 3 demonstrates the concept extractor adapting cross-attention mechanism for semantic token clustering. Token embeddings are augmented with a sinusoidal positional encoding that provides the notion of ordering to a model (Vaswani et al., 2017). The weight matrices WK and W V are used to transform input embeddings into matrices of keys K and values V respectively. We compose a trainable query matrix Q aimed to capture semantics specific to a correspondent factor in latent representation c. Averaging matrix of values V with respect to cross-attention scores yields the contextualized embeddings ri, which are further used to estimate elements of c through correspondent predictors Pri .\nSince we established one-to-one correspondence between set of queries and latent factors, the matrix of cross-attention scores could be considered as a relevance of each input token to variables in representation c. Concept extractor aggregates average cross-attention scores for each token in vocabulary. The concept candidates are, thus, selected by an average attention score for a correspondent latent factor.\nThe design of concept extractor also accounts for two special cases. First, some token t in the input sequence s might not align with any query (i.e. not correspond to any concept). Second, the opposite, some query might not align with any tokens in the input sequence s. We thus introduce additional \"dummy\" query to capture cross-attention scores from irrelevant tokens, and \"dummy\" tokens (one\n2In our preliminary experiments, an unsupervised model with a discrete (Gumbel straight-through) layer was not particularly effective, trailing the continuous version according to disentanglement metrics. Thus, we opted to utilize the stronger continuous version as a baseline in our experiments.\nper each query) for the model to indicate that all tokens from the input sequence are irrelevant."
        },
        {
            "heading": "3.4 Cross-Modal Agreement",
            "text": "To ensure alignment between the visual and textual components in XCB, we propose the inclusion of an additional tying loss function that assesses the similarity between the latent representations, denoted as f and c. Since these representations are discrete and stochastic, we employ the JensenShannon divergence to tie the distributions of the visual and textual representations. Given that each representation\u2019s distribution can be factored into Bernoulli distributions for individual features, the calculation of the divergence is straightforward.\nBecause of imposed similarity between representations f and c the set of concepts retrieved by textual component tends to also correspond to visual latent factors, and thereby can be used to explain the visual part. Otherwise, on unseen data, XCB is virtually identical to CBMs: for a given sample x it infers a binary set of human-interpretable attributes f , which is further used to predict the target variable y\u0302f . Note that training with cross-modal agreement yields an additional computational overhead of 15-25% varying by dataset (App. A)."
        },
        {
            "heading": "3.5 Encouraging Disentanglement",
            "text": "To intensify disentanglement between concepts we borrow ideas from slot attention (Locatello et al., 2020). Instead of normalizing cross-attention scores exclusively along the dimension of tokens, we firstly normalize them along the concepts, and only then rescale scores along the tokens. This method forces concepts (i.e. queries) to compete for tokens, and thus discourages the same token from being associated with multiple concepts. This effect is further amplified by the usage of entmax (Correia et al., 2019) activation function that produces sparse distributions of concepts (for each token).\nWe also introduce an additional training objective measuring an average pairwise cosine similarity of contextualized embeddings ri (App. B). This regularization further encourages concepts in the textual component to focus on semantically different parts of an input sequence and, thereby, capture separate factors of variation. Since this procedure is relatively computationally demanding, on each training step the loss is computed based on a subset sampled from ri.\nNote that both the promotion of discreteness and disentanglement have been recognized to decrease the inclusion of irrelevant information into concepts, known as concept leakage (Mahinpei et al., 2021). This offers an additional rationale for their application."
        },
        {
            "heading": "4 Empirical Evaluation",
            "text": "This section describes the datasets, introduces evaluation metrics for interpretability, and describes the procedure used to measure robustness of models."
        },
        {
            "heading": "4.1 Datasets",
            "text": "For our experiments we have used three classification datasets (Tab. 1). Each data point takes a form of (x, s, a, y), where x stands for a visual modality (e.g., chest x-ray image), s \u2014 textual modality (e.g., structured medical report), a \u2014 binary set of attributes (e.g., types of abnormalities), and y \u2014 label of class (e.g., whether a pathology has been detected).\nShapes. Primarily for benchmarking we propose a synthetic dataset of primitive shapes (Fig. 4, top).3 The image x is a white canvas with random shape (i.e., square, triangle, circle) of random color (i.e., red, green, blue), and random size (i.e., small, medium, large). Caption c is generated from visual parameters of a shape, its location, and words sampled from the vocabulary, using a hand-written grammar. The vector of attributes encodes shape and color, their combination represents a target class y.\nCUB-200. Similar to CBMs we evaluate XCB on Caltech-UCSD Birds-200-2011 (CUB-200) (Wah et al., 2011). Due to incoherent intra-class attribute labeling we performed a majority\n3Released at https://github.com/DanisAlukaev/shapes\nvoting as suggested by Koh et al. (2020). Since CUB-200 does not contain captions, description was generated as provided by the vector of attributes (Fig. 4, middle): each attribute ai with heading in a form of \"has_<s>::<t>\" is represented as \"this bird has <t> <s>\", e.g., \"has_beak_shape::dagger\" becomes \"this bird has dagger beak shape\". The caption for a sample is thereby accumulated from all descriptions of active attributes and tokens sampled from vocabulary.\nMIMIC. To assess XCB in realistic environment we use a subset from MIMIC-CXR (Johnson et al., 2019). The structured reports are written by radiologists in a free form and without the use of templates. The set of attributes (Fig. 4, bottom) is comprised of characteristics used by radiologists to differentiate pathological cases, which thereby might be considered as underlying latent factors of a certain generative process and used to evaluate latent representations."
        },
        {
            "heading": "4.2 Measuring Interpretability",
            "text": "Quantitative evaluation of model interpretability is not entirely standardized. The formulation of a metric highly depends on the specifics of the model architecture and available annotations. Our framework belongs to a set of concept-based models, where structural components are guided by a set of symbolic attributes. In the context of XCBs, we thus define the level of interpretability through\nalignment between the latent representation f and vector of ground-truth attributes a.\nThe relationship between latent representation and underlying factors of variation can be quantified in terms of disentanglement, completeness, and informativeness (DCI) (Eastwood and Williams, 2018). Disentanglement indicates degree to which a latent factor captures at most one attribute, completeness reflects degree to which each attribute is captured by a single latent factor, and informativeness accounts for amount of information captured by latent representation. For each attribute k, Eastwood and Williams (2018) suggest training linear regressor Rk : f \u2192 ak with associated feature importance scores Rwk . Disentanglement and completeness are entropic measures derived from Rwk , whereas informativeness is a normalized root mean squared error of Rk computed on the testing subset.\nOne downside of the DCI metric is the requirement of annotated attributes, which are typically available only for benchmark datasets. Consequently, in a real-world scenario, quality of interpretations could be assessed based on human evaluation. Namely, we provide human experts with a set of images and pieces of text representing underlying concept in the framework. One of the text pieces is sampled from a different concept. The goal of a domain expert is to determine which option is irrelevant. For instance, interpretations of XCB on MIMIC-CXR were evaluated by a practicing radiologist through a set of ten questions. A sample question consists of five X-ray images, with which our framework associated tokens \"pneumothorax\", \"effusions\", and \"atelectasis\" (corresponding scaled confidence scores \u03c8i were left unknown to the expert). As an additional option, we add \"pneumonia\" and set its confidence score to \u03c8i = 0. For a selected option j we further define a metric XScore = 1\u2212\u03c8j . Higher values of XScore indicate that model interpretation with the least confidence score is more relevant to provided studies than the random one."
        },
        {
            "heading": "4.3 Exploring Robustness",
            "text": "Models often learn to rely on non-robust (i.e. spurious) features, which are easy to identify in training but which will not be present or will be misleading at usage time (Ilyas et al., 2019). For example, in the medical domain training data is typically collected from both regular and emergency hospitals. Since most such machines put their serial number\nin the corner of the image and emergency hospitals tend to handle harder cases, this ID number becomes a distinctive yet spurious feature (Ribeiro et al., 2016). Reliance of such features is clearly highly undesirable.\nTo explore the robustness of the XCB model we incorporate a non-robust feature in all training subsets. As a shortcut, we use numerical class label located in the upper left corner of the image. The performance and interpretability of standard and proposed models are further evaluated on samples without distractors. The higher the value of the metric is, the higher robustness is expected."
        },
        {
            "heading": "5 Results and Discussion",
            "text": "In this work, we compare the proposed models with the standard \u2018black-box\u2019 model, as well as CBMs and LCBMs (Oikarinen et al., 2023). Due to similarity of LCBM and LaBo (Yang et al., 2023) models, we include in comparison only the former (see Section 2 for their discussion). The experiments were conducted using synthetic and public datasets highlighted in Section 3.1 of the paper. The obtained results are discussed with respect to their performance, interpretability, and robustness.\nThe ratios between training, validation, and test splits are 75%, 15% and 10%, respectively. For all datasets image size was set to 299 with a batch size of 64. At each epoch, images were normalized and the batches were randomly shuffled. The CUB-200 follows data processing pipeline described in Cui et al. (2018) with additional augmentations such as horizontal flip, random cropping, and color jittering. The hyperparameters were optimized using Tree-structured Parzen Estimator sampling algorithm (Bergstra et al., 2011).\nTo concentrate on expressivity of concept extractor C, we restricted our feature extractor F to be fine-tuned InceptionV3 model (Szegedy et al., 2016) pre-trained on the ImageNet dataset (Deng et al., 2009), with multi-layered perceptron of depth of one for Pf and Pc. The baseline model was com-\nposed of feature extractor F followed by sigmoid function and predictor Pf . The size of latent representations was set to 10, 320, and 20 for Shapes, CUB-200, and MIMIC-CXR datasets respectively (Tab. 1). Textual embeddings were randomly initialized as trainable parameters of length 50. The CBM size of bottleneck matched the number of attributes in a dataset (Tab. 1). To run experiments with CBM4 and LCBM5 the official implementations were used. Weights of all models were initialized using Xavier uniform distribution.\nThe feature extractor F and predictor Pf were optimized jointly using AdaDelta optimizer (Zeiler, 2012) with learning rate 0.25 and rho 0.95. The concept extractor C and predictor Pc were optimized jointly via AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 0.001, betas of 0.9 and 0.999, and weight decay 0.01. The learning rates in AdamW and AdaDelta were adjusted using OneCycleLR scheduler (Smith and Topin, 2019) with maximal learning rates set to 0.001 and 0.25, respectively. Temperature of the Gumbel sigmoid followed the exponential schedule (Jang et al., 2017). The experiments were conducted using early stopping policy on validation loss with patience of 5 epochs. Five random seeds were used: 42, 0, 17, 9, and 3. Mean and standard deviation for the obtained results were computed over five training runs.\nAblation study. Components introduced in XCB that aim to improve interpretability and disentanglement of representation cause drop in performance (Tab. 2). Nevertheless, disentanglement and completeness metrics were substantially improved by discretization of latent representations via Gumbel sigmoid and using \"dummy\" tokens in slot normalisation respectively. Remarkably, using Kullback-Leibler divergence\n4Available online at https://github.com/yewsiang/ ConceptBottleneck\n5Available online at https://github.com/Trustworthy-MLLab/Label-free-CBM\nas a tie loss worsens both performance and DCI, which could indicate that neither modality fully guides induction of implicit high-level abstractions.\nPerformance. The standard model outperforms CBMs on all datasets (Tab. 3), which can be attributed to a trade-off between performance and level of interpretability. Another possible reason is that manually derived set of attributes might not be predictive enough for classification. On the medical dataset LCBM is inferior to other models indicating current limitations in the domain knowledge of large language models. XCB achieves comparable to other variations of CBMs performance.\nInterpretability. The highest DCI scores (see Section 3.2 of the paper) were achieved with the proposed XCB model that could be attributed to domain knowledge transferred to the model (Tab. 3). High values of both disentanglement and completeness indicate that alignment of latent representation and set of attributes tends towards bijective. Although absolute values of DCI on medical dataset are modest, out survey indicates that XCB possess\na better quality of interpretations in terms of XScore compared to LCBM: 0.72 vs. 0.66.\nThe framework associates each factor in latent representation f with a set of textual interpretations (Fig. 5). For example, the images corresponding to the fourth neuron activations cluster in terms of shape, i.e., larger value of logit correspond to squares, whereas smaller represent triangles. Explanations of our model are indeed referring to the shape of a figure: highest relevance is achieved by tokens \"triangular\" and \"quadratic\".\nRobustness. Following the procedure presented in Section 3.3 of the paper, the performance and interpretability on non-robust Shapes Dataset were measured. Performance of the standard model on testing subset is lower than for XCB on all datasets (Tab. 4). The DCI of the proposed framework are also superior compared to the standard model. Method of integrated gradients reveals that the baseline mainly focuses on non-robust feature, whereas the proposed framework has higher attribution to the shape of the object (Fig. 6). We speculate that such an increase in robustness can be associated with a textual component of the framework, which regularizes implicit visual abstractions.\nNote that while we encourage textual and visual components to align, it is not a hard constraint but rather a soft regularizer, which is used only in\ntraining. Still, systematic errors (e.g., vital concepts consistently omitted in text) could be detrimental. However, a moderate amount of random noise might be less problematic. We tested this hypothesis on the Shapes dataset by taking 10% of the training examples and replacing their text descriptions with random ones. The performance did not change significantly (Tab. 5) confirming our hypothesis.\nConclusion\nThe objective of this study is to enhance the practicality of Concept Bottleneck Models by eliminating the need for predefined concepts provided by experts and annotating each training example. Instead, our approach utilizes textual descriptions to guide the induction of interpretable concepts. Through experiments, we demonstrate the advantages of using textual guidance compared in terms of interpretability in a range of set-ups. We believe there are numerous ways to expand on this work, particularly by incorporating a large language model into the textual component. We contend that the broader exploration of leveraging guidance from text or language models to shape abstractions in models for various domains remains largely unexplored. This direction holds great promise for enhancing the transparency and robustness of models utilized in decision-making applications. Our study serves as an illustration of one potential method for providing such guidance.\nLimitations\nInformation leakage. A concept has the potential to reveal additional information about the input, which may not be immediately evident upon inspecting the concept itself. To address this concern, we employ discrete representations and promote disentanglement between concepts, drawing on conclusions from a prior investigation\non information leakage (see section 4 in Mahinpei et al. (2021)).\nDomain dependence. Intepretation of neurons is known to not generalize across domains (Sajjad et al., 2022): the same neuron may encode different phenomena in two different domains. It is likely to be the case for latent concepts. Our studies were restricted to fairly narrow domains.\nUser studies. The most reliable way to guarantee the interpretability and usefulness of representations is through user studies. However, conducting these studies for evaluating interpretability can be costly and challenging to design. In this work, we primarily focused on automatic metrics instead. Nevertheless, it is important to note that these metrics are not flawless and constrained us to datasets where ground-truth semantic concepts are available.\nEthics Statement\nLike any other machine learning models, the potential for misuse of XCB technology should not be overlooked. Users of XCB, along with other interpretability technologies, need to be aware that it lacks guarantees of faithfulness. Moreover, being a new model architecture, it runs the risk of introducing harmful biases that are not present in more established architectures. Consequently, its application in practical settings necessitates thorough user studies and scrutiny.\nAlthough our experiments employ a public medical dataset, and we hope that this line of research will ultimately yield systems capable of effectively supporting decision-making in healthcare, it is crucial to emphasize that, at the current stage of development, such unproven technology cannot be employed in any critical application."
        },
        {
            "heading": "Acknowledgements",
            "text": "Danis Alukaev, Semen Kiselev, Ilya Pershin, and Alexey Kornaev were financially supported by The\nAnalytical Center for the Government of the Russian Federation (Agreement No. 70-2021-00143 dd. 01.11.2021, IGK 000000D730321P5Q0002)."
        },
        {
            "heading": "A Computational Requirements",
            "text": "The primary source of computational overhead in XCB (compared to standard models) tends to be the cross-modal agreement of latent representations on training time, which in turn is majorly influenced by the size of bottleneck. Keeping bottlenecks of reasonable size (approximately equal to the number of factors of underlying generative process) results in 15-25% increase in training time (Tab. 6). Note that another possible source of computational complexity could be an average pairwise cosine similarity of contextualised embeddings ri. Although, in practice it was applied to a random subset of 5% of training examples and did not affect the runtime substantially."
        },
        {
            "heading": "B Optimization",
            "text": "Training objective for XCB models is generally consists of four terms:\n\u2022 classification loss functions for visual and textual components of XCB, e.g, for all our setups we used regular cross-entropy loss;\n\u2022 tying loss that ensures cross-modal alignment of two latent representations c and f activated by sigmoid function, e.g., Jensen\u2013Shannon divergence (1);\nDJS(c \u2032||f \u2032) = [ m\u2032 = 1\n2\n( c\u2032 + f \u2032 )] =\n= 1\nN N\u2211 i=0 c\u2032i \u00b7 log ( c\u2032i m\u2032i ) +\n+ 1\nN N\u2211 i=0 ( 1\u2212 c\u2032i ) \u00b7 log ( 1\u2212 c\u2032i 1\u2212m\u2032i ) +\n+ 1\nN N\u2211 i=0 f \u2032i \u00b7 log ( f \u2032i m\u2032i ) +\n+ 1\nN N\u2211 i=0 ( 1\u2212 f \u2032i ) \u00b7 log ( 1\u2212 f \u2032i 1\u2212m\u2032i )\n(1)\n\u2022 sparsity regularizer for contextualised embeddings ri, e.g., pairwise cosine similarity (2).\nlossreg(r) = 1\nn2 n\u2211 i=0 n\u2211 j=0 ri \u00b7 rj |ri||rj |\n(2)\nIn practice, visual modality often tends to be more complicated than textual, which makes model favor updating visual component rather than textual one. Empirically, we found that there are two straightforward ways to improve convergence: (a)\npre-train visual component and with a decrease in its learning rate gradually add gradients from textual component, (b) associate each block of the model with a particular frequency of weights updating so that fvisual < ftextual."
        },
        {
            "heading": "C Examples",
            "text": "Shapes (Fig. 7, top): retrieved concepts include description of color (\"crimson\", \"red\") and shape (\"three\", \"angles\"). Note that concept \"circular\" corresponds to a negative logit in f , so we can speculate that model regards object in the image as the opposite to a \"circular\" one.\nCUB-200 (Fig. 7, middle): retrieved concepts include description of plumage texture (\"buff\", \"eyering\"), beak shape (\"hooked\", \"seabird\"), and color of upperparts (\"grey\").\nMIMIC-CXR (Fig. 7, bottom): retrieved concepts include name of observed pathologies (\"pneumothorax\", \"edema\"), and some indirect factors (\"focal\", \"volume\", \"low\") used in similar context."
        },
        {
            "heading": "D Textual Redundancy",
            "text": "One of the possible applications for XCB models could be medical imaging, where structured reports produced by radiologists often possess redundant data, e.g., in impression and findings sections. To examine how effectively the model could handle and process such data we simulated redundancy by concatenating paraphrased description to the descriptions in the Shapes dataset. We observed no discernible difference in results (Tab. 7) and attribute it to the fact that our sparsity regularizer ensures each text fragment corresponds to a single concept, but does not penalize a concept being inferred from multiple text segments."
        }
    ],
    "title": "Cross-Modal Conceptualization in Bottleneck Models",
    "year": 2023
}