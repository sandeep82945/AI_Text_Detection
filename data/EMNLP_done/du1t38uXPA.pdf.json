{
    "abstractText": "During crisis events, people often use social media platforms such as Twitter to disseminate information about the situation, warnings, advice, and support. Emergency relief organizations leverage such information to acquire timely crisis circumstances and expedite rescue operations. While existing works utilize such information to build models for crisis event analysis, fully-supervised approaches require annotating vast amounts of data and are impractical due to limited response time. On the other hand, semi-supervised models can be biased, performing moderately well for certain classes while performing extremely poorly for others, resulting in substantially negative effects on disaster monitoring and rescue. In this paper, we first study two recent debiasing methods on semi-supervised crisis tweet classification. Then we propose a simple but effective debiasing method, DeCrisisMB, that utilizes a Memory Bank to store and perform equal sampling for generated pseudo-labels from each class at each training iteration. Extensive experiments are conducted to compare different debiasing methods\u2019 performance and generalization ability in both in-distribution and outof-distribution settings. The results demonstrate the superior performance of our proposed method. Our code is available at https: //github.com/HenryPengZou/DeCrisisMB.",
    "authors": [
        {
            "affiliations": [],
            "name": "Henry Peng Zou"
        },
        {
            "affiliations": [],
            "name": "Yue Zhou"
        },
        {
            "affiliations": [],
            "name": "Weizhi Zhang"
        },
        {
            "affiliations": [],
            "name": "Cornelia Caragea"
        }
    ],
    "id": "SP:15e66c1e583460d8796adb16269b0925ab50c015",
    "references": [
        {
            "authors": [
                "Firoj Alam",
                "Shafiq Joty",
                "Muhammad Imran."
            ],
            "title": "Graph based semi-supervised learning with convolution neural networks to classify crisis related tweets",
            "venue": "Proceedings of the international AAAI conference on web and social media, volume 12.",
            "year": 2018
        },
        {
            "authors": [
                "Firoj Alam",
                "Umair Qazi",
                "Muhammad Imran",
                "Ferda Ofli."
            ],
            "title": "Humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, volume 15,",
            "year": 2021
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel."
            ],
            "title": "Mixmatch: A holistic approach to semisupervised learning",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Cornelia Caragea",
                "Adrian Silvescu",
                "Andrea H Tapia."
            ],
            "title": "Identifying informative messages in disasters using convolutional neural networks",
            "venue": "Proceedings of the 13th International Conference on Information Systems for Crisis Response and Management.",
            "year": 2016
        },
        {
            "authors": [
                "Ming-Wei Chang",
                "Lev Ratinov",
                "Dan Roth",
                "Vivek Srikumar."
            ],
            "title": "Importance of semantic representation: Dataless classification",
            "venue": "Proceedings of the 23rd National Conference on Artificial Intelligence Volume 2, AAAI\u201908, page 830\u2013835. AAAI Press.",
            "year": 2008
        },
        {
            "authors": [
                "Hui Chen",
                "Wei Han",
                "Soujanya Poria."
            ],
            "title": "SAT: Improving semi-supervised text classification with simple instance-adaptive self-training",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Vieweg"
            ],
            "title": "Processing social media",
            "year": 2015
        },
        {
            "authors": [
                "Reza Mazloom",
                "HongMin Li",
                "Doina Caragea",
                "Muhammad Imran",
                "Cornelia Caragea."
            ],
            "title": "Classification of twitter disaster data using a hybrid featureinstance adaptation approach",
            "venue": "Proceedings of the 15th International Conference on Information Sys-",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey J McLachlan."
            ],
            "title": "Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis",
            "venue": "Journal of the American Statistical Association, 70(350):365\u2013 369.",
            "year": 1975
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Sadeep Jayasumana",
                "Ankit Singh Rawat",
                "Himanshu Jain",
                "Andreas Veit",
                "Sanjiv Kumar."
            ],
            "title": "Long-tail learning via logit adjustment",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Venkata Kishore Neppalli",
                "Cornelia Caragea",
                "Doina Caragea."
            ],
            "title": "Deep neural networks versus naive bayes classifiers for identifying informative tweets during disasters",
            "venue": "Proceedings of the 15th International Conference on Information Systems for Cri-",
            "year": 2018
        },
        {
            "authors": [
                "Dat Tien Nguyen",
                "Kamela Ali Al Mannai",
                "Shafiq Joty",
                "Hassan Sajjad",
                "Muhammad Imran",
                "Prasenjit Mitra."
            ],
            "title": "Robust classification of crisis-related data on social networks using convolutional neural networks",
            "venue": "Eleventh international AAAI conference",
            "year": 2017
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea",
                "Doina Caragea."
            ],
            "title": "Cross-lingual disaster-related multilabel tweet classification with manifold mixup",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Re-",
            "year": 2020
        },
        {
            "authors": [
                "Henry Scudder."
            ],
            "title": "Probability of error of some adaptive pattern-recognition machines",
            "venue": "IEEE Transactions on Information Theory, 11(3):363\u2013371.",
            "year": 1965
        },
        {
            "authors": [
                "Iustin Sirbu",
                "Tiberiu Sosea",
                "Cornelia Caragea",
                "Doina Caragea",
                "Traian Rebedea."
            ],
            "title": "Multimodal semi-supervised learning for disaster tweet classification",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 2711\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li."
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "Advances in neural",
            "year": 2020
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola."
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Istv\u00e1n Varga",
                "Motoki Sano",
                "Kentaro Torisawa",
                "Chikara Hashimoto",
                "Kiyonori Ohtake",
                "Takao Kawai",
                "JongHoon Oh",
                "Stijn De Saeger."
            ],
            "title": "Aid is out there: Looking for help from tweets during a large scale disaster",
            "venue": "Proceedings of the 51st Annual",
            "year": 2013
        },
        {
            "authors": [
                "Sarah Vieweg",
                "Carlos Castillo",
                "Muhammad Imran."
            ],
            "title": "Integrating social media communications into the rapid assessment of sudden onset disasters",
            "venue": "International Conference on Social Informatics, pages 444\u2013461. Springer.",
            "year": 2014
        },
        {
            "authors": [
                "Fangxin Wang",
                "Lu Cheng",
                "Ruocheng Guo",
                "Kay Liu",
                "Philip Yu."
            ],
            "title": "Equal opportunity of coverage in fair regression",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Xudong Wang",
                "Zhirong Wu",
                "Long Lian",
                "Stella X Yu."
            ],
            "title": "Debiased learning from naturally imbalanced pseudo-labels",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14647\u201314657.",
            "year": 2022
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Qiang Heng",
                "Wenxin Hou",
                "Yue Fan",
                "Zhen Wu",
                "Jindong Wang",
                "Marios Savvides",
                "Takahiro Shinozaki",
                "Bhiksha Raj",
                "Bernt Schiele",
                "Xing Xie."
            ],
            "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
            "venue": "The Eleventh",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le."
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "Advances in neural information processing systems, 33:6256\u20136268.",
            "year": 2020
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le."
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687\u201310698.",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Zhang",
                "Yidong Wang",
                "Wenxin Hou",
                "Hao Wu",
                "Jindong Wang",
                "Manabu Okumura",
                "Takahiro Shinozaki."
            ],
            "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
            "venue": "Advances in Neural Information Processing Systems, 34:18408\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Henry Peng Zou",
                "Yue Zhou",
                "Cornelia Caragea",
                "Doina Caragea."
            ],
            "title": "Crisismatch: Semi-supervised few-shot learning for fine-grained disaster tweet classification",
            "venue": "Proceedings of the 20th International ISCRAM Conference, pages 385\u2013395.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "During natural disasters, real-time sharing of crisis situations, warnings, advice and support on social media platforms is critical in aiding response organizations and volunteers to enhance their situational awareness and rescue operations (Varga et al., 2013; Vieweg et al., 2014). Although existing works utilize such information to build models for crisis event analysis, standard supervised approaches require annotating vast amounts of data during disasters, which is impractical due to limited response time (Li et al., 2015; Caragea et al.,\n2016; Li et al., 2017, 2018; Neppalli et al., 2018; Ray Chowdhury et al., 2020; Sosea et al., 2021). On the other hand, current semi-supervised models can be biased, performing moderately well for certain classes while extremely worse for others, resulting in a detrimentally negative effect on disaster monitoring and analysis (Alam et al., 2018; Ghosh and Desarkar, 2020; Sirbu et al., 2022; Zou et al., 2023; Wang et al., 2023a). For instance, neglecting life-essential classes, such as requests or urgent needs, displaced people & evacuations and injured or dead people, can have severely adverse consequences for relief efforts. Therefore, it is crucial to mitigate bias in semi-supervised approaches for crisis event analysis.\nIn this paper, we investigate and observe that bias in semi-supervised learning can be related to interclass imbalances in terms of numbers and accuracies of pseudo-labels produced during training. We do this analysis using a representative work in semisupervised learning, Pseudo-Labeling (PSL) (Lee et al., 2013; Xie et al., 2020a; Sohn et al., 2020). We then study two different debiasing methods for semi-supervised learning on the task of crisis tweet classification. These two state-of-the-art semisupervised debiasing approaches are: Debiasing via Logits Adjustment (LogitAdjust) (Wang et al., 2022) and Debiasing via Self-Adaptive Thresholding (SAT) (Wang et al., 2023b). Our analysis show that although these methods have effects in debiasing and balancing pseudo labels across classes, their debiasing performance is still unsatisfying and there are drawbacks that need to be addressed. LogitAdjust debiases the pseudo-labeling process by explicitly adjusting predicted logits based on the average probability distribution over all unlabeled data. However, we observe that this explicit adjustment makes it difficult for models to fit data, leading to unstable training, and their classwise pseudo-labels are still highly imbalanced. SAT proposes to dynamically adjust global and local\nthresholds of pseudo-labeling to enforce poorlylearned categories generating more pseudo-labels. This approach does help produce more balanced pseudo-labels but comes at the cost of sacrificing the accuracy of the pseudo-labels (in-depth analysis and visualized comparisons are provided in Section 6.)\nTo address these issues, we propose a simple but effective debiasing method, DeCrisisMB, that utilizes a memory bank to store generated pseudolabels. We then use this memory bank to sample an equal number of pseudo-labels from each class per training iteration for debiasing semi-supervised learning. Extensive experiments are conducted to compare the three debiasing methods for semisupervised crisis tweet classification. Additionally, we evaluate and compare their generalization ability in out-of-distribution datasets and visualize their performance in debiasing semi-supervised models. Our results and analyses demonstrate the substantially superior performance of our debiasing methods.\nThe contributions of this work are summarized as follows:\n\u2022 We provide an analysis which shows that imbalanced pseudo-label quantity and quality can cause bias in semi-supervised learning. We investigate their influence by demonstrating the model improvement after equal sampling and removing erroneous pseudo-labels.\n\u2022 We study two recent semi-supervised debiasing methods on crisis tweet classification and propose DeCrisisMB, a simple but effective debiasing method based on memory bank and equal sampling.\n\u2022 We conduct extensive experiments to compare different debiasing methods and provide outof-distribution results and analysis of their debiasing performance. Experimental results demonstrate our superior performance compared to other methods."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Disaster Tweet Classification",
            "text": "Analyzing social media information shared during natural disasters and crises is crucial for enhancing emergency response operations and mitigating the adverse effects of such events, leading to more resilient and sustainable communities. In recent years, crisis tweet classification has made\nsignificant progress in improving disaster relief efforts (Imran et al., 2013; Li et al., 2015; Imran et al., 2015; Li et al., 2017, 2018; Neppalli et al., 2018; Mazloom et al., 2018; Ray Chowdhury et al., 2020; Sosea et al., 2021). For example, Imran et al. (2013, 2015) propose to classify crisis-related tweets to obtain useful information for better disaster understanding and rescue operations. Caragea et al. (2016) and Nguyen et al. (2017) use Convolutional Neural Networks (CNNs) for classifying and extracting informative disaster-related tweets. Li et al. (2021) combines self-training with BERT pretrained language models to boost the performance of classifying disaster tweets when only unlabeled data is available. Alam et al. (2018); Ghosh and Desarkar (2020); Sirbu et al. (2022); Zou et al. (2023) leverage both labeled and unlabeled data to develop more effective crisis tweet classifiers. However, while these studies are effective in incorporating information from unlabeled data, they do not account for the fact that the trained semi-supervised models can be biased and neglect certain difficultto-learn classes, which can have severely negative consequences for disaster relief efforts."
        },
        {
            "heading": "2.2 Semi-Supervised Learning and Debiasing",
            "text": "Semi-supervised learning aims to reduce the reliance on labeled data in machine learning models by utilizing unlabeled data to improve their performance (Lee et al., 2013; Berthelot et al., 2019; Xie et al., 2020b; Zhang et al., 2021). Selftraining involves using the model\u2019s prediction probability as a soft label for unlabeled data (Scudder, 1965; McLachlan, 1975; Lee et al., 2013; Xie et al., 2020b). Pseudo-labeling is a modification of self-training that reduces confirmation bias by using hard labels and confidence thresholding to select high-quality pseudo-labels (Lee et al., 2013; Zhang et al., 2021). Mean Teacher (Tarvainen and Valpola, 2017) proposes to use the exponential moving average of model weights for predictions. MixMatch (Berthelot et al., 2019) employs sharpening to promote low-entropy prediction on unlabeled data and utilizes MixUp (Zhang et al., 2018) to mix and combine labeled and unlabeled data. MixText (Chen et al., 2020) introduces MixUp to text domains by performing interpolation on hidden representations of texts. Recently, Wang et al. (2022) observe that pseudo-labels generated by semi-supervised models are naturally imbalanced even if their training datasets are balanced.\nThe authors first introduce logit adjustment from long-tail learning (Menon et al., 2021) to debias semi-supervised learning. The recent state-of-theart approach in semi-supervised classification by Wang et al. (2023b) designs a self-adaptive thresholding strategy to lower confidence thresholds of pseudo-labeling for poorly-learned classes. Both of them are helpful in creating more pseudo-labels for difficult classes and debiasing semi-supervised models, but are at the expense of sacrificing pseudolabels accuracy and causing the training process to be unstable. To this end, we investigate their debiasing effect on the semi-supervised crisis tweet classification task. Then we propose a neat debiasing method that utilizes a memory bank for equal sampling, and analyze its effectiveness compared with other debiasing methods."
        },
        {
            "heading": "3 Analysis of Inter-class Bias",
            "text": "Even with balanced datasets, semi-supervised learning may exhibit bias towards certain classes while ignoring others. In this section, we demonstrate that such bias can be related to inter-class imbalances, including the numbers and accuracies of pseudo-labels generated during training. This motivates us to debias semi-supervised learning by balancing pseudo-labels during training."
        },
        {
            "heading": "3.1 Dataset",
            "text": "In this work, we use two crisis tweet datasets: Hurricane and ThreeCrises, both sampled from HumAID (Alam et al., 2021). The Hurricane dataset contains 12,800 human-labeled tweets collected during hurricane disasters that happened between 2016 and 2019. The ThreeCrises dataset consists of 7,120 annotated tweets collected during floods,\nwildfires and earthquake disasters that occurred between 2016 and 2019. As shown in Table 1, both datasets include the same 8 crisis-related classes, and the number of their total labels is balanced. Our train, test, and validation sets are split in a ratio of 0.8:0.1:0.1."
        },
        {
            "heading": "3.2 Inter-Class Biases and Pseudo-Label Imbalances",
            "text": "We conduct a pilot experiment on the crisis datasets to demonstrate the bias in semi-supervised models and their pseudo-labels. Here we use PseudoLabeling (Lee et al., 2013; Xie et al., 2020a; Sohn et al., 2020), a representative work in semisupervised learning, for analysis. Figure 1(a) illustrates the model validation accuracy for different classes during training on the Hurricane dataset. It can be observed that the model favors certain classes while other classes\u2019 performance worsens as training proceeds. We assume this is because the model generates biased pseudo-labels, and training with these biased pseudo-labels will further increase the model\u2019s bias.\nConsistent with our assumption, Figure 1(b) shows the total number of generated pseudo-labels\nin each category as the training progresses. We can see that the number of pseudo-labels is highly imbalanced across different categories and becomes increasingly biased toward leading classes. Figure 1(c) shows the class-wise accuracy of pseudolabels. We can observe that the accuracy of pseudolabels also varies between classes. One interesting finding is that some classes with higher pseudo label accuracy but lower pseudo label numbers perform worse in Figure 1(a) than classes with lower pseudo label accuracy but higher pseudo label numbers. This implies that the quantity or diversity of pseudo labels in one class might play an equally crucial role as the quality of pseudo labels in the learning process."
        },
        {
            "heading": "3.3 Effect of Equal-Sampling and Erroneous Label Removal",
            "text": "Another perspective for examining how inter-class pseudo-label numbers and accuracies can impact semi-supervised learning performance is to investigate model improvement after equal sampling and deleting incorrect pseudo-labels. To this end, we conduct the following experiments:\n(a) Baseline: Pseudo-Labeling that utilizes highconfidence model predictions of unlabeled data as pseudo-labels for iterative training; (b) Delete Incorrect: Delete all incorrect pseudo-labels and use only correct ones for training; (c) Equal Sampling: Sample and use an equal number of pseudo-labels from each class for each training iteration. Note that the pseudo-labels are not guaranteed to be correct. We implement this through equal sampling in a memory bank, which is described in detail in the next section; (d) Delete Incorrect+Equal Sampling: Remove incorrect pseudo-labels and then\nuse the memory bank to sample an equal number of pseudo-labels from each class for training.\nTable 2 shows accuracy and macro-F1 results for different settings. Trivally, deleting incorrect pseudo labels boosts model performance. However, it is intriguing that sampling the same number of pseudo-labels per iteration for training also significantly increases model performance, although the sampled pseudo-labels are not necessarily correct with no oracle information provided. This further indicates that the relevant number of pseudo-labels has equal importance with pseudo-label accuracy for training unbiased semi-supervised models. Finally, deleting incorrect pseudo labels and then performing equal sampling can further increase model performance and achieve the best result. This motivates us to alleviate the bias in terms of inter-class pseudo-label numbers and accuracies."
        },
        {
            "heading": "4 Debiasing Methods",
            "text": "In this section, we first introduce our baseline Pseudo-Labeling (PSL) (Lee et al., 2013; Xie et al., 2020a; Sohn et al., 2020), a representative semisupervised method. We then present and discuss two state-of-the-art semi-supervised debiasing approaches: Debiasing via Logits Adjustment (LogitAdjust) in Wang et al. (2022) and Debiasing via Self-Adaptive Thresholding (SAT) in Wang et al. (2023b). Finally, we introduce our debiasing method DeCrisisMB."
        },
        {
            "heading": "4.1 Pseudo-Labeling",
            "text": "Semi-supervised learning aims to reduce the reliance on labeled data by allowing models to leverage unlabeled data effectively for training bet-\nter models. Suppose we have a labeled batch X = {(xb, yb) : b \u2208 (1, 2, . . . , B)} and an unlabeled batch U = {ub : b \u2208 (1, 2, . . . , \u00b5B)}, where \u00b5 is the ratio of unlabeled data to labeled data, B is the batch size of labeled data. The objective of semi-supervised learning often consists of two terms: a supervised loss for labeled data and an unsupervised loss for unlabeled data. The supervised loss Ls is computed by cross-entropy between predictions and ground truth labels of labeled data xb:\nLs = 1\nB B\u2211 b=1 H(yb, p(y|xb)) (1)\nwhere p denotes the model\u2019s probability prediction. Pseudo-labeling (PSL) (Lee et al., 2013; Xie et al., 2020a; Sohn et al., 2020) takes advantage of unlabeled data by using the model\u2019s prediction of unlabeled data as pseudo-labels to optimize the unsupervised loss:\nLu = 1\n\u00b5B \u00b5B\u2211 b=1 1(max(pb) > \u03c4)L(q\u0302b, pb) (2)\nwhere pb is the model prediction of unlabeled data ub, \u03c4 is the confidence threshold to generate pseudo-labels and q\u0302b is the hard/one-hot pseudolabel of ub, L is L2 loss or cross-entropy loss function. Note that only confident predictions are used to generate pseudo-labels and compute the unsupervised loss."
        },
        {
            "heading": "4.2 Debiasing via Logits Adjustment",
            "text": "The first debiasing method for semi-supervised learning we investigate here is Debiasing via Logits Adjustment (LogitAdjust) in Wang et al. (2022). It is claimed that the average probability distribution on unlabeled data can be utilized to reflect the model and pseudo-label bias: the higher the average probability one class receives, the more pseudo-labels are usually generated in this class. LogitAdjust proposes to debias pseudo-labeling by adjusting logits based on estimated averaged probability distributions. Since computing the average probability distribution of all unlabeled samples at every iteration is very time-consuming, LogitAdjust uses its exponential moving average (EMA) as an approximation. These can be formulated as:\nz\u0304b = zb \u2212 \u03bb log p\u0304 (3)\np\u0304\u2190 mp\u0304+ (1\u2212m) 1 \u00b5B \u00b5B\u2211 b=1 pb (4)\nwhere z\u0304b, zb refers to the logits of the unlabeled data ub after and before adjustment, p\u0304 is the approximated average probability distribution on unlabeled data, m \u2208 (0, 1) is the momentum parameter of EMA, pb is the model prediction on an unlabeled sample, \u03bb is the debias factor, which controls the strength of the debias effect. The logits adjustment in Eq. 3 alleviate the bias by making false majority classes harder to generate pseudo labels, while false minority classes easier to produce pseudo labels."
        },
        {
            "heading": "4.3 Debiasing via Self-Adaptive Thresholding",
            "text": "Another recent method to debias imbalanced pseudo labels is Self-Adaptive Thresholding (SAT) (Wang et al., 2023b). It adaptively adjusts each class\u2019s global and local confidence threshold based on the model\u2019s overall and class-wise learning status. The model\u2019s overall learning status is estimated by the EMA of confidence on unlabeled data, and the classwise learning status is estimated by the EMA of the probability distribution of unlabeled data, similarly to Eq. 4. Formally, at time step t, the self-adaptive global threshold \u03c4t and selfadaptive local threshold \u03c4t(c) for class c are defined as:\n\u03c4t = m\u03c4t\u22121 + (1\u2212m) 1\n\u00b5B \u00b5B\u2211 b=1 max(pb) (5)\n\u03c4t(c) = p\u0304t(c)\nmax{p\u0304t(c) : c \u2208 [C]} \u00b7 \u03c4t (6)\nwhere p\u0304t(c) is the EMA of probability prediction for class c on all unlabeled data, as in Eq. 4. The insight is that the global threshold \u03c4t is low at the beginning of training to utilize more unlabeled data, and grows progressively to eliminate possibly incorrect pseudo-labels as the model becomes more confident during the training process. Meanwhile, the self-adaptive local threshold adjusts classwise local thresholds based on the learning status of each class. This self-adaptive thresholding strategy helps debias the semi-supervised model by enforcing the model to create more pseudo-labels for poorly-behaved classes, but is at the expense of lowering their pseudo-label quality, as shown in Section 6."
        },
        {
            "heading": "4.4 Proposed Approach: Debiasing via Memory Bank",
            "text": "Motivated by our analysis in Section 3, we propose DeCrisisMB, an equal-sampling strategy via memory bank to debias semi-supervised models.\nDataset: Hurricane\nMethods Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1\n#Labels/Class 3 5 10 20 50\nPSL 39.3 \u00b1 1.6 34.7 \u00b1 1.1 55.0 \u00b1 6.8 49.8 \u00b1 7.2 66.7 \u00b1 4.7 63.1 \u00b1 5.7 73.2 \u00b1 1.5 69.6 \u00b1 1.4 78.4 \u00b1 0.2 76.0 \u00b1 0.5 MixMatch 47.4 \u00b1 4.6 42.5 \u00b1 5.5 57.3 \u00b1 1.8 52.8 \u00b1 1.9 67.6 \u00b1 2.6 64.3 \u00b1 2.7 74.1 \u00b1 1.5 70.7 \u00b1 1.6 78.5 \u00b1 0.6 75.4 \u00b1 1.1 FlexMatch 44.8 \u00b1 7.0 38.6 \u00b1 7.9 57.9 \u00b1 3.6 53.2 \u00b1 3.0 70.5 \u00b1 3.0 68.2 \u00b1 3.3 73.4 \u00b1 1.3 70.3 \u00b1 0.9 78.8 \u00b1 0.4 76.0 \u00b1 0.4 LogitAdjust 44.4 \u00b1 2.8 38.4 \u00b1 1.4 58.0 \u00b1 5.2 53.5 \u00b1 5.7 68.6 \u00b1 4.4 65.1 \u00b1 5.7 73.9 \u00b1 1.6 70.1 \u00b1 1.9 78.2 \u00b1 0.4 74.7 \u00b1 0.4 SAT 46.1 \u00b1 0.9 40.5 \u00b1 2.7 60.0 \u00b1 4.5 56.0 \u00b1 5.2 71.4 \u00b1 1.1 68.5 \u00b1 1.0 74.7 \u00b1 0.7 71.3 \u00b1 0.4 79.4 \u00b1 0.4 77.0 \u00b1 0.6 DeCrisisMB 58.1 \u00b1 3.8 54.2 \u00b1 3.5 65.6 \u00b1 7.2 62.5 \u00b1 7.9 73.4 \u00b1 1.0 70.2 \u00b1 1.2 77.0 \u00b1 1.6 74.1 \u00b1 1.5 78.9 \u00b1 1.0 75.7 \u00b1 1.3\nDataset: ThreeCrises\n#Labels/Class 3 5 10 20 50\nPSL 41.9 \u00b1 1.0 37.4 \u00b1 0.7 50.0 \u00b1 1.1 44.3 \u00b1 1.3 64.5 \u00b1 4.9 60.4 \u00b1 5.3 71.9 \u00b1 2.2 67.3 \u00b1 3.2 75.4 \u00b1 2.0 72.8 \u00b1 2.0 MixMatch 43.8 \u00b1 0.7 37.9 \u00b1 0.4 53.5 \u00b1 3.4 48.3 \u00b1 4.3 64.9 \u00b1 1.0 61.1 \u00b1 0.6 72.6 \u00b1 1.2 69.3 \u00b1 1.1 76.8 \u00b1 0.8 73.5 \u00b1 0.7 FlexMatch 44.9 \u00b1 1.6 39.2 \u00b1 1.4 53.8 \u00b1 5.5 49.4 \u00b1 5.8 64.5 \u00b1 5.4 61.3 \u00b1 5.3 71.8 \u00b1 0.9 67.4 \u00b1 0.6 76.2 \u00b1 0.7 73.2 \u00b1 1.5 LogitAdjust 43.9 \u00b1 3.4 37.6 \u00b1 4.1 52.8 \u00b1 2.8 47.2 \u00b1 3.0 65.9 \u00b1 3.5 61.1 \u00b1 3.6 72.0 \u00b1 1.1 67.7 \u00b1 1.2 77.3 \u00b1 1.3 74.7 \u00b1 1.4 SAT 43.5 \u00b1 0.7 37.8 \u00b1 0.6 55.8 \u00b1 4.4 50.4 \u00b1 5.1 68.6 \u00b1 1.4 65.0 \u00b1 1.5 72.5 \u00b1 1.4 68.4 \u00b1 2.2 76.9 \u00b1 1.1 73.4 \u00b1 2.4 DeCrisisMB 52.5 \u00b1 2.5 48.6 \u00b1 2.5 59.8 \u00b1 4.0 56.0 \u00b1 4.1 68.6 \u00b1 1.9 64.2 \u00b1 2.2 73.8 \u00b1 2.0 70.9 \u00b1 2.0 77.0 \u00b1 2.2 74.3 \u00b1 2.5\nTable 3: Accuracy and Macro-F1 results of different debiasing methods on Hurricane and ThreeCrises datasets. All results are averaged over 3 runs. Best results are shown in bold.\nAs illustrated in Figure 2, the memory bank consists of C independent queues, where C is the number of classes. For a batch of unlabeled data, the data that receive high-confidence model prediction will be assigned pseudo-labels. Since each class may receive a highly imbalanced number of pseudo-labels, we first push these pseudo-labeled data to the corresponding queue in the memory bank. At each training iteration, we randomly sample an equal number of pseudo-labeled data from the memory bank for each class. Those rebalanced samples and pseudo-labels are then passed to the model and used for training. We repeat these steps until the model converges. The key is using the memory bank to store and rebalance pseudo-labels for each training iteration without sacrificing their\nquality. In such a manner, the bias from the different numbers of pseudo-labels produced in each class can be significantly alleviated or removed.\nNote that our method is very different from the standard under-sampling approach: For different training iterations, the generated pseudo-labels can be extremely skewed. In many cases/iterations, there are no pseudo-labels generated for some classes, especially hard classes. The standard under-sampling approach cannot promote learning for these ignored classes during these iterations and will lead the model to increasingly ignore them; The standard over-sampling approach also cannot handle these cases because there are no pseudolabels to be oversampled. In contrast, our method stores previously generated high-quality pseudolabels for each class in a memory bank and then we can sample equal numbers of pseudo-labels in each class per training iterations. Results and analysis show that this simple approach is very powerful in debiasing since we effectively balance pseudolabels in each training iteration while maintaining the high quality of pseudo-labels."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Following Chen et al. (2020); Li et al. (2021); Chen et al. (2022), we use the BERT-based-uncased model as our backbone model and the HuggingFace Transformers (Wolf et al., 2020) library for the implementation. We provide a complete list of our hyper-parameters in Appendix A. Our code is released.\nSource: ThreeCrises, Target: Hurricane\nMethods Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1\n#Labels/Class 3 5 10 20\nPSL 37.3 \u00b1 1.7 32.7 \u00b1 2.5 45.8 \u00b1 1.3 40.6 \u00b1 1.4 61.6 \u00b1 4.4 56.9 \u00b1 4.8 69.7 \u00b1 1.6 65.9 \u00b1 2.5 LogitAdjust 42.7 \u00b1 6.2 35.9 \u00b1 6.4 49.7 \u00b1 1.5 44.3 \u00b1 1.3 63.3 \u00b1 2.4 59.8 \u00b1 1.8 69.0 \u00b1 0.5 65.4 \u00b1 0.5 SAT 42.1 \u00b1 0.8 36.0 \u00b1 1.2 51.1 \u00b1 5.7 45.7 \u00b1 6.8 66.0 \u00b1 1.8 61.9 \u00b1 1.5 69.7 \u00b1 2.1 66.3 \u00b1 2.5 DeCrisisMB 49.3 \u00b1 3.1 45.2 \u00b1 3.6 58.2 \u00b1 6.2 53.9 \u00b1 7.2 68.3 \u00b1 1.0 64.6 \u00b1 2.2 72.7 \u00b1 1.3 70.0 \u00b1 1.4\nSource: Hurricane, Target: ThreeCrises\n#Labels/Class 3 5 10 20\nPSL 38.3 \u00b1 3.5 33.4 \u00b1 3.9 52.9 \u00b1 5.8 47.2 \u00b1 6.9 63.4 \u00b1 3.1 58.6 \u00b1 3.1 68.5 \u00b1 2.2 63.6 \u00b1 2.7 LogitAdjust 36.0 \u00b1 4.5 30.7 \u00b1 3.1 55.1 \u00b1 3.6 50.2 \u00b1 4.6 63.6 \u00b1 3.0 59.1 \u00b1 4.4 68.3 \u00b1 1.5 64.2 \u00b1 2.2 SAT 34.9 \u00b1 5.2 28.2 \u00b1 5.0 57.5 \u00b1 3.2 53.4 \u00b1 2.8 67.1 \u00b1 3.6 62.9 \u00b1 3.8 71.1 \u00b1 1.7 66.4 \u00b1 2.3 DeCrisisMB 52.0 \u00b1 3.3 47.4 \u00b1 2.8 64.9 \u00b1 2.3 61.8 \u00b1 3.4 67.6 \u00b1 2.6 64.3 \u00b1 1.7 71.3 \u00b1 1.1 67.6 \u00b1 1.3"
        },
        {
            "heading": "5.2 Main Results",
            "text": "The comprehensive evaluation results on Hurricane and ThreeCrises are reported in Table 3. In addition to the debiasing methods discussed in Section 4, we also compare our method with two other prior and competitive approaches in semi-supervised learning and pseudo-label debiasing: MixMatch (Berthelot et al., 2019) and FlexMatch (Zhang et al., 2021). We can see that DeCrisisMB significantly outperforms these approaches in most cases, indicating its effectiveness in leveraging unlabeled data and debiasing pseudo-labels. When there are 50 labels for each class, all the baseline models and the DeCrisisMB produce very close results. With the decreased number of labels, it is noteworthy that the DeCrisisMB achieves the best performance by yielding a larger accuracy margin in comparison with the other three baseline methods. Under the extremely weakly-supervised setting (with 3 labels for each class), the debiasing process of DeCrisisMB is surprisingly efficient, bringing 13.7% and 10.8% Macro-F1 improvement than the secondbest method SAT. All these results imply the strong debiasing capability of the proposed DeCrisisMB and justify its superiority."
        },
        {
            "heading": "5.3 Out-of-Distribution Results",
            "text": "In Table 4, among all the competitors, our DeCrisisMB model achieves the best out-of-distribution performance in all evaluation settings, particularly exceeding the second-best SAT approach by the accuracy of 10.45% and Macro-F1 of 11.6% on average when the number of labels is limited to 3. All the above results prove the effectiveness of the DeCrisisMB under distribution shift and also further reveal its potential to be deployed into more realistic and challenging application scenarios."
        },
        {
            "heading": "5.4 Generalizability Results",
            "text": "To demonstrate the generalizability of our method, we further test our method on two standard semisupervised learning benchmark datasets: AG News and Yahoo! Answers, both of which are in different domains and provide larger test and validation sets. A detailed breakdown and statistics of these additional datasets are provided in Appendix B. The performance comparisons on the 5-shot setting are presented in Table 5. It can be observed that DeCrisisMB consistently outperforms other methods across different domains and diverse datasets, demonstrating its strong generalizability."
        },
        {
            "heading": "6 Analysis",
            "text": "To further understand each debiasing approach and why the proposed method achieves the best debasing results, we provide several visualizations on Hurricane datasets. Figure 3 demonstrates qualitative comparisons of different debiasing methods. Figure 4 shows the comparison on total numbers\nof produced pseudo-labels. Figure 5 presents the average pseudo-label accuracy over the worst 1, 4 and 8 classes. Overall, all three debiasing methods have some effects in debiasing and balancing pseudo-label numbers across the class, as shown in Figure 3, 4. However, there are some differences:\n\u2022 LogitAdjust can be observed to be unstable over the training process (Figure 3(b)) and its generated pseudo-labels are still highly imbalanced (Figure 4(b)). One potential reason behind this is that explicit logit adjustments make the model difficult to fit data and make training unstable.\n\u2022 SAT does improve the performance of hardto-learn classes but is still slightly unstable in the training process (Figure 3(c)) and has lower pseudo-labels accuracy than the PSL baseline (Figure 5). Our assumption is that although lowering thresholds helps generate more pseudo-labels for the poorly-learned classes (Figure 4(c)), this comes at the expense of reducing their pseudo-labels accuracy (Figure 5).\n\u2022 Our proposed DeCrisisMB achieves the best results and is powerful in debiasing the semisupervised models (Figure 3(d)) since we effectively balance pseudo-labels in each training iteration (Figure 4(d)) while maintaining the high quality of pseudo-labels (Figure 5), and thus the training is also more stable."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we demonstrate that semi-supervised models and their pseudo-labels generated on social media data posted during crisis events can be biased, and balancing pseudo-labels used in training can effectively debias semi-supervised models. We then study and compare two recent debiasing approaches in semi-supervised learning with our proposed debiasing method for crisis tweet classification. Experimental results show that our method based on memory bank and equal sampling achieves the best debiasing results quantitatively on both in-distribution and out-of-distribution settings. We believe our work can serve as a universal and effective adds-on debiasing module for semisupervised learning in different domains.\nLimitations\nThis work examines various debiasing methods, primarily in the context of classification settings. However, it should be worthwhile to investigate the debiasing effects of these methods in other settings, such as in generative tasks and large language models. Such exploration would help further demonstrate the generality of these methods. We plan to conduct such exploration in the future.\nBroader Impact\nFor our crisis domain, the strongest contribution of this paper is the debiasing strategy that helps alleviate the negative effect of models being biased towards the more frequent classes. Using Twitter data, we reliably improve the performance of lifeessential classes such as requests or urgent needs, displaced people and evacuations, and injured or dead people. Currently, crisis responders can track weather data to know where a hurricane hits an affected population or what are potentially flooded areas in rainy seasons, but they cannot know in real time the effect that a disaster is having on the population. They often ask, \u201cHow bad is it out there?\u201d. Traditionally, they rely on either eyewitness accounts after the fact from survivors, or eye-\nwitness information offered in real-time by those who are able to make phone calls. Our model can be integrated into systems that can help response organizations to have a real-time situational awareness. In time, such systems could pinpoint the joy of having survived a falling tree, the horror of a bridge washing out or the fear of looters in action. Responders might be able to use such a system to provide real-time alerts of the situation on the ground and the status of the affected population. Thus, our research is aimed at having a positive impact on sustainable cities and communities."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research is partially supported by National Science Foundation (NSF) grants IIS-1912887, IIS2107487, ITE-2137846. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF. We thank our reviewers for their insightful feedback and comments."
        },
        {
            "heading": "A Hyperparameters",
            "text": "A complete list of hyperparameters of evaluated methods is provided in Table 6. We use the same hyperparameters and AdamW optimizer across all datasets."
        },
        {
            "heading": "B Statistics of Added Datasets",
            "text": "The detailed breakdown and statistics of the additional datasets, AG News (Zhang et al., 2015) and Yahoo! Answers (Chang et al., 2008), are provided in Table 7."
        },
        {
            "heading": "C Further Augment DeCrisisMB",
            "text": "As inspired by the two previous debiasing methods discussed in Section 4, different classes have varied learning statuses; thus, it might be beneficial\nto sample more pseudo-labels for poorly-learned classes and sample fewer pseudo-labels for leading classes. To this end, we propose an adaptive sampling strategy (AdSampling) to explore a way of augmenting our DeCrisisMB method. Specifically, the number of pseudo-labels to be sampled from different classes depends on their learning status, which is estimated by p\u0304t(c) as in Eq. 6 and Eq. 4. Formally, the sampling number for class c at time t is defined as:\nNt(c) =\n1 |C|\np\u0304t(c) \u2217N (7)\nwhere C is the number of classes, and N is the original sampling number for each class queue in the memory bank. This helps in over-balancing pseudo-labels used per iteration for poorly-behaved classes and speeding up the debiasing process.\nTable 8 indicates the accuracy and Macro-F1 results of DeCrisisMB with and without AdSampling on the Hurricane dataset. AdSampling, the sampling strategy to prioritize the poorly-learned classes, further boosts the DeCrisisMB performance and achieves better debiasing results in most cases. Note that adaptive sampling is just a simple add-on and exploration inspired by the two baseline methods and can be further optimized."
        }
    ],
    "title": "DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet Classification via Memory Bank",
    "year": 2023
}