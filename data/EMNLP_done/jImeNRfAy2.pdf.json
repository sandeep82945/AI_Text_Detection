{
    "abstractText": "Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. In this paper, we propose a more lightweight approach that enables the PLM itself to achieve \u201cselfdetoxification\u201d. Our method is built upon the observation that prepending a negative steering prompt can effectively induce PLMs to generate toxic content. At the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the PLM as an information stream facilitated by the attention layers. Drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. Experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Chak Tou Leong"
        },
        {
            "affiliations": [],
            "name": "Yi Cheng"
        },
        {
            "affiliations": [],
            "name": "Jiashuo Wang"
        },
        {
            "affiliations": [],
            "name": "Jian Wang"
        },
        {
            "affiliations": [],
            "name": "Wenjie Li"
        }
    ],
    "id": "SP:d9a6ec4e866e7cf2990b8da5c4cfaa86ff35b584",
    "references": [
        {
            "authors": [
                "Nora Belrose",
                "Zach Furman",
                "Logan Smith",
                "Danny Halawi",
                "Igor Ostrovsky",
                "Lev McKinney",
                "Stella Biderman",
                "Jacob Steinhardt"
            ],
            "title": "Eliciting latent predictions from transformers with the tuned lens",
            "year": 2023
        },
        {
            "authors": [
                "Guy Dar",
                "Mor Geva",
                "Ankit Gupta",
                "Jonathan Berant"
            ],
            "title": "Analyzing transformers in embedding space",
            "year": 2022
        },
        {
            "authors": [
                "Jackson Kernion",
                "Liane Lovitt",
                "Kamal Ndousse",
                "Dario Amodei",
                "Tom Brown",
                "Jack Clark",
                "Jared Kaplan",
                "Sam McCandlish",
                "Chris Olah."
            ],
            "title": "A mathematical framework for transformer circuits",
            "venue": "Transformer Circuits Thread. Https://transformer-",
            "year": 2021
        },
        {
            "authors": [
                "Bowman",
                "Jared Kaplan"
            ],
            "title": "The capacity for moral self-correction in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2020
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Mor Geva",
                "Avi Caciularu",
                "Kevin Ro Wang",
                "Yoav Goldberg"
            ],
            "title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Gokaslan",
                "Vanya Cohen."
            ],
            "title": "Openwebtext corpus",
            "venue": "http://Skylion007.github.io/ OpenWebTextCorpus.",
            "year": 2019
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R. Varshney",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Ctrl: A conditional transformer language model for controllable generation",
            "year": 2019
        },
        {
            "authors": [
                "Tomasz Korbak",
                "Kejian Shi",
                "Angelica Chen",
                "Rasika Bhalerao",
                "Christopher L. Buckley",
                "Jason Phang",
                "Samuel R. Bowman",
                "Ethan Perez"
            ],
            "title": "Pretraining language models with human preferences",
            "year": 2023
        },
        {
            "authors": [
                "Ben Krause",
                "Akhilesh Deepak Gotmare",
                "Bryan McCann",
                "Nitish Shirish Keskar",
                "Shafiq Joty",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "GeDi: Generative discriminator guided sequence generation",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Matthew Jones",
                "Tengyu Ma",
                "Percy Liang."
            ],
            "title": "Finetuning can distort pretrained features and underperform out-of-distribution",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Sachin Kumar",
                "Vidhisha Balachandran",
                "Lucille Njoo",
                "Antonios Anastasopoulos",
                "Yulia Tsvetkov"
            ],
            "title": "Language generation models can cause harm: So what can we do about it? an actionable survey",
            "year": 2023
        },
        {
            "authors": [
                "Jin Myung Kwak",
                "Minseon Kim",
                "Sung Ju Hwang"
            ],
            "title": "Language detoxification with attributediscriminative latent space",
            "year": 2022
        },
        {
            "authors": [
                "Alisa Liu",
                "Maarten Sap",
                "Ximing Lu",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv., 55(9).",
            "year": 2023
        },
        {
            "authors": [
                "Varvara Logacheva",
                "Daryna Dementieva",
                "Sergey Ustyantsev",
                "Daniil Moskovskiy",
                "David Dale",
                "Irina Krotova",
                "Nikita Semenov",
                "Alexander Panchenko"
            ],
            "title": "ParaDetox: Detoxification with parallel data",
            "year": 2022
        },
        {
            "authors": [
                "Ximing Lu",
                "Sean Welleck",
                "Jack Hessel",
                "Liwei Jiang",
                "Lianhui Qin",
                "Peter West",
                "Prithviraj Ammanabrolu",
                "Yejin Choi."
            ],
            "title": "Quark: Controllable text generation with reinforced unlearning",
            "venue": "Advances in Neural Information Processing Systems, volume 35,",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The refinedweb dataset for falcon llm: Outperforming curated corpora with web",
            "year": 2023
        },
        {
            "authors": [
                "Shrimai Prabhumoye",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Bryan Catanzaro."
            ],
            "title": "Adding instructions during pretraining: Effective way of controlling toxicity in language models",
            "venue": "Proceedings of the 17th Conference of the European Chap-",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2020
        },
        {
            "authors": [
                "Tilman R\u00e4uker",
                "Anson Ho",
                "Stephen Casper",
                "Dylan Hadfield-Menell."
            ],
            "title": "Toward transparent ai: A survey on interpreting the inner structures of deep neural networks",
            "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pages",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP",
            "venue": "9:1408\u20131424.",
            "year": 2021
        },
        {
            "authors": [
                "Jesse Vig",
                "Sebastian Gehrmann",
                "Yonatan Belinkov",
                "Sharon Qian",
                "Daniel Nevo",
                "Yaron Singer",
                "Stuart Shieber."
            ],
            "title": "Investigating gender bias in language models using causal mediation analysis",
            "venue": "Advances in neural information processing systems, 33:12388\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Boxin Wang",
                "Wei Ping",
                "Chaowei Xiao",
                "Peng Xu",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Bo Li",
                "Anima Anandkumar",
                "Bryan Catanzaro."
            ],
            "title": "Exploring the limits of domain-adaptive training for detoxifying large-scale language models",
            "venue": "Ad-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Albert Xu",
                "Eshaan Pathak",
                "Eric Wallace",
                "Suchin Gururangan",
                "Maarten Sap",
                "Dan Klein."
            ],
            "title": "Detoxifying language models risks marginalizing minority voices",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Canwen Xu",
                "Zexue He",
                "Zhankui He",
                "Julian McAuley."
            ],
            "title": "Leashing the inner demons: Selfdetoxification for language models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11530\u201311537.",
            "year": 2022
        },
        {
            "authors": [
                "Dawn Song",
                "Matt Fredrikson",
                "J. Zico Kolter",
                "Dan Hendrycks"
            ],
            "title": "Representation engineering: A top-down approach to ai transparency",
            "year": 2023
        },
        {
            "authors": [
                "deterioration. As Logacheva"
            ],
            "title": "2022) demonstrates that their method produces fluent cleaned text, this deterioration could be attributed to a loss of context relevance, rather than fluency issues",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the past few years, pretrained language models (PLMs) have exhibited remarkable performance in various applications (Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020). However, the abundance of toxic content within the pretraining data makes PLMs prone to generate offensive and biased content (Gehman et al., 2020). With the aim of promoting safer deployment of PLMs, this critical\n1Code is available at https://github.com/ cooperleong00/ToxificationReversal\n*Equal contribution\nissue of language model detoxification has attracted increasing research attention (Kumar et al., 2023).\nAmong the proposed methods, the majority necessitate fine-tuning of the PLMs. This can be done either on cleaner data that has filtered out the potentially toxic content (Gururangan et al., 2020; Wang et al., 2022) or through alignment with human preferences for more polite behaviors (Ouyang et al., 2022; Korbak et al., 2023). Despite their effectiveness, these methods involve updating all parameters of the model, which can be extremely resourceintensive considering the massive sizes of today\u2019s PLMs. Additionally, the fine-tuning process could also negatively impact the PLM\u2019s generalization\nacross different tasks, ultimately hindering its overall performance (Kumar et al., 2022).\nApart from the fine-tuning paradigm, another line of research focuses on how to detoxify PLMs during its decoding process (Dathathri et al., 2020; Liu et al., 2021; Krause et al., 2021). They manipulate the PLM\u2019s predicted distribution of the next token to reduce the probabilities of the ones that may lead to toxic content. A classifier, typically based on a PLM as well, needs to be trained specifically to identify those potentially toxic tokens. One drawback of such methods is the potential decrease in the fluency of the generated content, arising from directly modifying the original probability predicted by the PLM (Xu et al., 2021).\nIn this paper, we present a more lightweight approach for language model detoxification, with no need to fine-tune the PLM or incorporate additional components like toxicity classifiers. Our method is built upon the observation that prepending negative steering prompts (e.g., \"The following text is harmful:\") can effectively induce the model to generate toxic content (Schick et al., 2021). At the same time, we draw inspiration from Elhage et al. (2021), who mathematically demonstrate that the evolving contextualized representations within the inner layers of the PLM can be conceptualized as an information stream, primarily facilitated by the attention heads between layers for information movement. Drawing on this idea, we regard the toxicity permeating from the negative steering prompt to the ultimate toxic output as a \u201ctoxification process\u201d within the information stream of contextualized representations. As shown in Figure 1, our proposed method is to find the toxification direction from the normal generation process to the toxification process, and then steer the generation process to the reversed direction by manipulating the information movement within the attention layers. It enables the PLM itself to achieve \u201cselfdetoxification\u201d simply using two forward passes during inference, which will be explained in detail in Section 2.\nOur contributions are summarized as follows. (1) We propose a lightweight approach that enables self-detoxification of the PLM by finding the toxification direction from the normal generation to the toxification process and then steering the generation to the reversed direction. (2) Experimental results show that our approach, without any finetuning or extra components, can achieve compa-\nrable performance with state-of-the-art methods. (3) We conduct extensive analyses of our approach, which reveals internal mechanisms of the toxification process within PLMs and may contribute to future research that explores detoxification through direct manipulation of computational mechanisms."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Task Formalization Given a context in the prompt T = {t1, t2, . . . , tN} with N tokens, a language model (LM) will generate a continuation that naturally extends the prompt. The task of language detoxification is to reduce the risk of generating toxic content in the continuation. Here, toxic content refers to text that exhibits a high likelihood of possessing toxic attributes, such as rude, disrespectful, insulting, etc (Gehman et al., 2020; Schick et al., 2021). Our work focuses on detoxification of causal LM, e.g., GPT-2 (Radford et al., 2019).\nForward Pass Process in Causal Language Model Each token in the prompt is first embedded to a vector x0i \u2208 Rd using a vocabulary embedding matrix and fused with position embeddings via summation. The input embeddings go through a sequence of L transformer layers. Each layer performs read-write processes, namely multi-head self-attention (MHSA) and MLP computation, over a residual stream. Layer normalization (Ba et al., 2016) is ignored for simplicity. The residual stream is initially the input embeddings x0 before getting into the first layer.\nThe l-th MHSA sub-layer contains three projection matrices W \u2113Q,W \u2113 K ,W \u2113 V \u2208 Rd\u00d7d and an output matrix W \u2113O \u2208 Rd\u00d7d. As per Elhage et al. (2021), each projection matrix\u2019s columns and the output matrix\u2019s rows can be split into H parts, giving W \u2113,hQ ,W \u2113,h K ,W \u2113,h V \u2208 R d\u00d7 d H and W \u2113,hO \u2208 R d H \u00d7d for h \u2208 [1, H]. The h-th attention head computes the attention matrix A\u2113,h \u2208 RN\u00d7N as follows:\nA\u2113,h = \u03c6\n(( x\u2113\u22121W \u2113,hQ )( x\u2113\u22121W \u2113,hK )T \u221a\nd/H +M \u2113,h\n) ,\nwhere \u03c6 denotes row-wise softmax normalization, and M \u2113,h is a mask making A\u2113,h a lower triangular matrix and thus the attention to be causal. Then, the output of MHSA can be computed by a sum of\nmatrices given by different attention heads:\na\u2113 = H\u2211 h=1 A\u2113,h ( x\u2113\u22121W \u2113,hV ) W \u2113,hO\n= H\u2211\nh=1\nv\u2113,hW \u2113,hO , (1)\nwhere each v\u2113,hi \u2208 Rd is a contextualized value vector at position i.\nSubsequently, the residual stream is updated through x\u2113 + a\u2113. An MLP sub-layer further performs a token-wise transformation for each representation in the residual stream and updates it via summation. After L layers\u2019 update, the residual stream is converted to a probability distribution of the next token, and a new token is sampled from this distribution and then appended to the prompt for the next forward pass."
        },
        {
            "heading": "3 Method",
            "text": "Our proposed method does not involve any finetuning of the PLM or the training of any additional components. At the inference stage, it performs two successive forward passes to generate each token. As shown in Figure 2, in the first pass, we send two prompts to the model, prepended with negative and positive prefixes, respectively, to identify the toxification direction in each attention layer. Then, we input the original prompt and use the reverse toxification direction to steer the representations away from toxicity in the second forward pass."
        },
        {
            "heading": "3.1 Toxification Direction Discovery",
            "text": "In the first forward pass, we feed a batch of two prompt inputs to the PLM, prepended with a negative and a positive prefix, respectively. The negative prefix induces the model to generate harmful and offensive content, while the positive one serves as a contrasting reference for a better toxification di-\nrection discovery2. Suggesting that the toxification process mainly happens in the information movement facilitated by the MHSA layers, we extract the toxification direction by comparing the attention heads\u2019 outputs resulting from the negative and the positive inputs.\nFormally, we denote the negative and positive prefixes as T\u2212prefix = {t1, t2, . . . , tK\u2212} and T+prefix = {t1, t2, . . . , tK+}, respectively, where K\u2212 and K+ are the number of tokens in T\u2212prefix and T+prefix. We concatenate the two prefixes with the context, respectively, obtaining the negative input T\u2212 = [T\u2212prefix;T ] and the positive input T+ = [T+prefix;T ]. Correspondingly, the lengths of T\u2212 and T+ are denoted as N\u2212 and N+, and these values dynamically increase as new tokens are generated and appended to T . Then, we put these two inputs in the same batch and feed it to the PLM to conduct inference of the next generated token.\nWe obtain the toxification direction by contrasting the contextualized value vectors derived from the negative and positive inputs. Specifically, this direction \u2206v\u2113,h is calculated as:\n\u2206v\u2113,h = v \u2212,(\u2113,h) N\u2212 \u2212 v +,(\u2113,h) N+ , (2)\nwhere v\u2212,(\u2113,h) N\u2212 is the contextualized value vector of the last token in negative input, and v+,(\u2113,h) N+\nis the last token in the positive one. We only consider the last token because modifying previous tokens\u2019 representations in the prompt would deviate the continuation from context. The toxification direction \u2206v\u2113,h measures the difference between the information captured by the attention heads from the two prefixes. This difference represents the toxification tendency that occurs in the MHSA layers."
        },
        {
            "heading": "3.2 Adaptive Toxification Reversal",
            "text": "In the second forward pass, the original context prompt would be fed into the model. To detoxify the continuation conditioned on this input, we use the opposite direction of \u2206v\u2113,h to guide the current value vector\u2019s update, steering it away from the\n2It is also applicable to find the toxification direction by comparing the toxification process and the generation process prompted by the original context without any prefix. Nevertheless, in practice, we conduct comparison with the one prompted with a positive prefix due to its better performance. See Appendix D for more details.\ntoxification direction:\nv new,(\u2113,h) N = v \u2113,h N \u2212\u2206v \u2113,h. (3)\nTo emphasize the modification effect on those attention heads which are more likely to toxify the generated text, we propose two scaling factors that make our detoxification more adaptive. As we use a difference vector that represents the direction of toxification, we can infer that the size of this vector reflects the degree of toxification brought by the corresponding head. Thus, we use the L2-norm of the difference vector to further scale the strength of modification:\n\u03bbnorm = 1 + \u2225\u2206v\u2113,h\u22252. (4)\nAs the negative prompt is able to toxify the generated text, which means that the representation of negative prompt is encoded with toxicity, we are able to measure the toxicity of the value vector by computing the similarity between these two vectors. This similarity-based scaling factor can be induced as:\n\u03bbsim = 1 +max { 0, cos ( v\u2113,hN ,v \u2212,(\u2113,h) K\u2212 )} , (5)\nwhere cos (u, v) = u\u00b7v\u2225u\u22252\u00b7\u2225v\u22252 is the similarity measurement, and we only further scale the modification when cos (\u00b7, \u00b7) > 0. In all, we adaptively apply the detoxification as:\nv new,(\u2113,h) N = v \u2113,h N \u2212 \u03bb \u03b1 norm \u00b7 \u03bb \u03b2 sim \u00b7\u2206v \u2113,h, (6)\nwhere \u03b1 and \u03b2 are two hyperparameters that control the strength of these two adaptive scaling factors.\nTo preserve the model\u2019s original capabilities as much as possible, we renormalize the updated value vectors to align with the total L2-norm of all head-wise value vectors before the update:\nv new,(\u2113) N = v new,(\u2113) N \u00b7 \u2225v\u2113N\u22252 \u2225vnew,(\u2113)N \u22252 . (7)\nThis ensures that the modified value vectors remain close to the representations typically accepted by the subsequent output matrix."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets We use the RealToxicityPrompts (RTP) dataset for experiments (Gehman et al., 2020). It contains 100K text paragraphs extracted from English web text, with the first half of each paragraph\nused as the prompt for continuation. They also annotate the toxicity scores of all these prompts, by measuring their probability of being toxic with the Perspective API 3. Our experimental setup follows the practice in Liu et al. (2021). Specifically, we randomly sample 10,000 prompts and filter out those samples without annotation of toxicity score, resulting in a total of 9,907 prompts. Among them, we use the 7,785 prompts whose toxicity scores are below 0.5 for the non-toxic prompt experimental setting, and the other 2,122 prompts with scores higher than 0.5 are used for the toxic setting. Conditioned on each prompt, the model needs to generate a minimum of 5 and a maximum of 20 tokens as continuations for evaluation.\nBaselines Our baselines include two finetuningbased methods: DAPT (Gururangan et al., 2020) and ATCON (Keskar et al., 2019); two decodingbased methods: GeDi (Krause et al., 2021), DEXPERTS (Liu et al., 2021); a prompt-based method: SD (Schick et al., 2021). Our approach can also be categorized as prompted-based. We illustrate the difference between our method and SD in Section 6. More details about the baselines are provided in Appendix A.\nImplementation Details For all methods, we use GPT2-large 4 as the base model and use nucleus sampling (Holtzman et al., 2020) with p = 0.9 to sample 25 continuations for each prompt. As per DAPT (Gururangan et al., 2020), We used the checkpoint fine-tuned by Liu et al. (2021). In our experiments, we utilized the outputs of AT-\n3https://perspectiveapi.com 4https://huggingface.co/gpt2-large\nCON provided by Gehman et al. (2020). For both two decoding-based methods, we used the models\u2019 weights released by the authors. To ensure a fair comparison, we used the same negative prefix as in our proposed method for SD. Further discussion on prefixes can be found in Appendix D. We use \u03b1 = 0.4 and \u03b2 = 0.6 to scale \u03bbnorm and \u03bbsim. The values are selected via running around \u03b1 \u2208 {0.4, 0.5, 0.6, 0.8} and \u03b2 \u2208 {0.2, 0.4, \u00b7 \u00b7 \u00b7 , 1.6}, aimming for a trade-off between toxicity reduction and fluency. More details about how to adjust \u03b1 and \u03b2 are shown in Appendix C."
        },
        {
            "heading": "4.2 Automatic Evaluation",
            "text": "Following the practice in previous research, we adopt Expected Maximum Toxicity (Exp. Max. Tox.) and Toxicity Probability (Tox. Prob.) to assess the performance of detoxification. The former computes the average of the highest toxicity\nscores across the 25 samples for a specific prompt, taking into account all prompts, while the latter represents the likelihood of generating a continuation with a toxicity score of 0.5 or greater at least once within the 25 samples. Here, we fine-tune a DeBERTa-v3-large 5 (He et al., 2023) model to mark the toxicity scores using the hold-out 90k samples in the RTP dataset, which can achieve 94.87% accuracy and a 98.54% AUROC score (see Appendix B for more details). Besides, we also adopt Perplexity (PPL) to assess the generation fluency. A pre-trained language model larger than the compared models, GPT2-XL6, is utilized to measure perplexity.\nThe automatic evaluation results are presented in Table 1. We can see that compared with other prompt-based baselines, our method can achieve significantly better performance in terms of all the metrics. At the same time, it can also achieve comparable performance with the fine-tuning based methods. Comparing the methods with the same number of parameters, we can see that our approach outperforms the finetuning-based baselines and other prompt-based methods in terms of the detoxification performance and the perplexity score. Though the decoding-based can achieve better performance than ours regarding the two automatic metrics of detoxification, it requires many more model parameters. Besides, our calculation of the two metrics for detoxification relies on an automatic evaluator to measure the probability of the continuation being toxic, which is trained on the hold-out samples in the RTP dataset and is not entirely precise. The two decoding-based baselines also needs to fine-tune an extra PLM to avoid generating toxic content at the decoding stage. These extra components may capture some similar patterns with the automatic evaluator, as we observe that their generation are more often misclassified as non-toxic by the automatic evaluator after our manual evaluation. Thus, the two automatic detoxification metrics of DEXPERTS and GeDi are very likely to be inflated. We conduct human evaluation for more comprehensive evaluation."
        },
        {
            "heading": "4.3 Human Evaluation",
            "text": "We randomly select 150 samples (i.e., 50 for \u201dOurs vs. DAPT\u201d, 50 for \u201dOurs vs. DExperts\u201d, and 50 for \u201dOurs vs. SD\u201d) from the test set for human\n5https://huggingface.co/microsoft/deberta-v3-large 6https://huggingface.co/gpt2-xl\nevaluation. We recruit three graduate students with related background as evaluators. Given the continuations generated by our approach and a compared model for the same prompt, they are asked to choose which one performs better (or select tie) in terms of the following dimensions: (1) Less Toxic: which continuation is less rude, offensive and harmful; (2) More Fluent: which continuation is more well-formed and natural; (3) More Coherent: which continuation has a more consistent language style and topic with the prompt.\nThe human evaluation results shown in Figure 3 suggest that our method significantly outperforms SD in all the dimensions, which is also a promptedbased method. Its detoxification performance is also superior to DAPT and DEXPERTS, with its winning rate more than twice of its losing rate. At the same time, it achieves comparable performance regarding fluency and coherence compared with DAPT and DEXPERTS. We report a Fleiss\u2019s Kappa of \u03ba = 0.244. It indicates a fair agreement (0.21 < \u03ba < 0.40) among human annotators."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Layer-wise Ablation Study",
            "text": "We conduct layer-wise ablation study to analyze the effects of conducting toxification reversal in different layers. Specifically, we consider the following variants of our method: (1) Ablation from bottom, which is a set of variants that remove the toxification reversal operations in the k bottom\nlayers, where k \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 32};7 (2) Ablation from top, which remove those in the k top layers, where k \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 32} (3) Ablation in the middle, which remove the reversal operations from the k-th to the (k + 3)-th layer (indexing from the bottom side), where k is an increment of 4 layers, i.e., k \u2208 {0, 4, 8, \u00b7 \u00b7 \u00b7 , 32}.\nThe results of layer-wise ablation study are presented in Figure 4. We can see that all three variants exhibit non-linear changes, indicating that the contributions of different layers to detoxification are uneven. Specifically, when ablating the middlelower layers (i.e., below 16 layers), the loss of toxicity reduction is slight. When only using the middlelower layers for toxification reversal, the toxicity reduction is also insignificant. This suggests that the middle-lower layers may contribute less to language detoxification. In contrast, when ablating the middle-upper layers, the expected maximum toxicity decreases remarkably, indicating that the toxification reversal performed in the middle-upper layers significantly reduces toxicity."
        },
        {
            "heading": "5.2 Analysis on Head-wise Scaling Factors",
            "text": "We also analyze the effects of the two scaling factors, \u03bbnorm and \u03bbsim, due to their critical roles in enabling the adaptability of our method. We randomly select 1000 non-toxic prompts and use the base model without detoxification and the one detoxified with our method to respectively generate 25 continuations for each prompt. For each prompt, we measure the average toxicity of the generated continuations from the base model and our method, respectively, and then use the difference between their average toxicity as the average\n7Here, we refer to the layers closer to the input side as the \u201cbottom\u201d layers.\ntoxicity reduction. When using toxification reversal for generation, each attention head has a \u03bbnorm and a \u03bbsim during the prediction of each token. For each prompt, we took the average \u03bb\u00b7 of each attention head across all generated tokens.\nIn Figure 5, we visualize the Spearman correlations between \u03bbnorm/sim and the average toxicity reduction. The left shows the correlation between \u03bbnorm and toxicity reduction. It can be seen that attention heads in the middle-lower layers generally have lower correlations. In comparison, those in the middle-upper layers have significantly higher correlations than the middle-lower layers. This is in line with the previous conclusion that the middlelower layers contribute less to toxicity reduction, while the middle-upper layers have a significant contribution. On the right is the correlation between \u03bbsim and toxicity reduction, and it can be seen that the attention heads with higher correlations are relatively sparse. This is consistent with the finding in Appendix C that adjusting \u03bbsim has a smaller impact on reducing toxicity compared to scaling \u03bbnorm. In two correlation distributions, there are a small number of attention heads with higher correlations compared to other heads, indicating that these heads are more likely to have functions related to toxicity information in the text, such as routing style or semantic information."
        },
        {
            "heading": "5.3 Analysis on Detoxification Dynamics",
            "text": "Our method gradually detoxifies the representation layer by layer, so we are interested in the dynamic of toxicity in the representation during this process. We find an example where the base model accepts a seemingly non-toxic prompt input but is prone to triggering a highly toxic continuation. The prompt is \u201c\\\"Put your hand in my face and I\u2019m going to\u201d, and the base model inclines to predict a verb that makes the continuation toxic or violent. We select 14 obviously toxic verbs from the top predicted tokens as \"negative verbs.\" To observe how our toxification reversal method suppresses the probabilities of these verbs, we use the logit lens technique (Belrose et al., 2023; Dar et al., 2022), which multiplies the residual stream at any position with the vocabulary embedding and then obtains the probability distribution of each token through softmax. Specifically, we choose the input and output of the Layer Normalization(LN) before attention and before the MLP. Since GPT-2 uses pre-LN, the input of the LN is the residual stream\nthat has been updated by previous modules. The results are shown in Figure 6. In the base model, the probability sum of the selected negative verbs increases to nearly 100% at 24-th layer; although it eventually falls back, the final output probability sum is still over 20%. When using toxification reversal, the probability sum of negative verbs remains at a very low level, and is suppressed to nearly 0% at around 16-th layer. For the token \"slap\", its probability gradually increases to a final 4% in the base model after 25-th layer. Using toxification reversal, the probability of this token is similarly suppressed at around 16-th layer. In both cases, the layer where suppression begins also coincides with the layer that starts to play a major role in detoxification, as previously analyzed. The dynamics of the rest 13 negative verbs and the completed sampled continuations for this prompt are discussed in Appendix E."
        },
        {
            "heading": "6 Related Works",
            "text": "Pre-trained language models (PLMs) (Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020) have become general-purpose processors for natural language processing tasks by reducing any task to a text generation task (Liu et al., 2023; Wei et al., 2022). The general text generation capability of PLMs comes from pre-training on\nlarge-scale, multi-domain text corpora (Prabhumoye et al., 2023; Korbak et al., 2023). However, these corpora, which are scraped from the internet, inevitably contain toxic content (Gehman et al., 2020; Gao et al., 2020; Penedo et al., 2023; Kumar et al., 2023), posing a risk for PLMs to generate toxic content. Some existing works mitigate toxicity in language models by further training the models, such as fine-tuning PLMs on non-toxic corpora (Gururangan et al., 2020; Wang et al., 2022; Lu et al., 2022) or inserting control codes in the corpora (Keskar et al., 2019), and then using nontoxic control codes during prediction. Recent work has explored fine-tuning PLMs to generate content aligned with human preferences (Ouyang et al., 2022). Another line of work proposes preventing toxic text generation during model decoding by suppressing the probability of potential toxic tokens with additional modules or fine-tuned language models (Liu et al., 2021; Krause et al., 2021; Xu et al., 2022; Kwak et al., 2022). However, these approaches require extra training, and the growing parameter size of PLMs makes this increasingly computationally expensive.\nThe most similar work with ours is Schick et al. (2021). They explored detoxification through negative prompts without additional training, where prefixes are used to find toxic token candidates and suppress them to achieve detoxification. Instead of directly filtering out tokens, our work seeks to find the updated direction of negative prefixes for the context and then perform reverse updates to achieve detoxification at the representation level. Our method does not modify the model output, preserving the model\u2019s capabilities as much as possible without additional fine-tuning.\nUnderstanding the effects in output distribution caused by modifying internal representations helps explain the intrinsic mechanisms of models (Elhage et al., 2021; R\u00e4uker et al., 2023; Belrose et al., 2023; Dar et al., 2022). Vig et al. (2020) finds that bias effects are concentrated in specific model components. Geva et al. (2022) demonstrates that each MLP update can be broken down into sub-updates, promoting different vocabulary concepts. They prove that detoxification can be achieved by \"turning on\" non-toxic sub-updates. Our work could also be seen as one successful instance of applying representation engineering to AI safety issues (Zou et al., 2023)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we propose a prompt-based approach for detoxifying pre-trained language models without fine-tuning or auxiliary models. Our method performs toxification reversal by manipulating the information flow within the attention mechanism during inference. Specifically, we first discover the toxification direction of each attention head and then reverse this direction to detoxify the representation of each generated token adaptively.\nEmpirical results show that our method can significantly reduce the toxicity of generated text upon the base model while maintaining its fluency. Further analysis reveals the contributions of the detoxification reversal operations conducted in different parts of the model, as well as the process of toxicity gradually being removed from token representations. Our research potentially benefits the research on safe and responsible AI from the perspective of understanding the internal mechanisms within language models.\nLimitations\nOur approach involves first toxifying the model with an additional prompt prefix, followed by detoxifying the model. This implies that the scope and degree of detoxification depend on the model\u2019s knowledge of toxicity obtained during pre-training. Only those toxic concepts and forms that are associated with the prefix in the pre-training corpus can be evoked from the model\u2019s weights when using this prefix. These specific concepts and forms are the ones that our method can suppress. Therefore, if harmful concepts are not associated with the words in the prefix due to the model\u2019s capacity or forgetting, these harmful contents might not be removed. Consequently, our method\u2019s performance relies on the pre-training corpus and techniques of the PLM and may not be suitable for models with smaller capacities.\nAdditionally, our method necessitates modifying the representations within the model during the forward pass process. This requires full access to the pre-trained language model, which means our method is not applicable to language models that only offer APIs. However, we believe and advocate for pre-trained language models to become increasingly open and transparent. Our research also potentially contributes to the investigation of safety issues in these open-sourced language models from an internal mechanism perspective.\nEthics Statement\nWe recognize that pretrained language models can inadvertently learn and propagate biases present in the training data, resulting in outputs that may be harmful or offensive. Our work aims to reduce harmful outputs by detoxifying pretrained language models. While we strive to improve the safety of these models, we acknowledge that the detoxification method may have limitations, such as over-detoxification (removing valid content), under-detoxification (retaining harmful content), or introducing new biases.\nMoreover, there is a risk of misuse by adversaries who may attempt to bypass the detoxification process or exploit its weaknesses. We encourage further research into robust countermeasures and ongoing monitoring to minimize such risks and enhance model security."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the Research Grants Council of Hong Kong (15207122, 15213323, 15204018) and National Natural Science Foundation of China (62076212). It was also supported in part by PolyU internal grants (ZVQ0, ZVVX)."
        },
        {
            "heading": "A Baselines",
            "text": "Retraining-based The retraining-based method detoxifies the Language Model (LM) by finetuning it on a non-toxic dataset. We adopted two Retraining-based methods as baselines, i.e., Domain-Adaptive Pretraining (DAPT) (Gururangan et al., 2020) and Attribute Conditioning (ATCON) (Keskar et al., 2019). DAPT further pretrained the base LM on the non-toxic subset of OpenWebText (Gokaslan and Cohen, 2019). ATCON fine-tuned LM using control code prefixes (e.g., <|toxic|>, <|nontoxic|>). During inference, <|nontoxic|> was added to the prompts to generate non-toxic continuations.\nDecoding-based The decoding-based method aims to detoxify LM during inference by suppressing the probability of potential toxic tokens. Although updating the base model\u2019s parameters is not required, maintaining fluency in the generated text and achieving better detoxification effects still necessitate training an additional guiding module or fine-tuning another language model. For comparison, we selected two representative decoding-based methods, i.e., GeDi (Krause et al., 2021) and DEXPERTS (Liu et al., 2021).\nGeDi employed a language model conditioned on class (similar to ATCON) to derive classification likelihoods for every potential subsequent token using Bayes\u2019 theorem, while DEXPERTS integrated the original LM with two distinct LMs, including the toxic LM known as the \"anti-expert\", and the non-toxic LM referred to as the \"expert\". The intention behind this combination was to promote tokens considered likely by the experts and unlikely by the anti-experts.\nPrompt-based The prompt-based approach leverages the inherent knowledge of toxicity in LM by employing prompts for detoxification. The Self-Debiasing (SD) (Schick et al., 2021) method entailed adding a negative prefix to the input text, guiding the model to generate toxic content. Then, by re-inputting the text without the prefix for standard generation, the method suppressed tokens with a higher probability from the initial generation, which are more likely to be toxic tokens."
        },
        {
            "heading": "B Offline Toxicity Scorer",
            "text": "We did not use the Perspective API to assess the toxicity of newly generated text due to its limitations on request throughput. Instead, we trained an offline toxicity scorer on 90k RTP samples not used for evaluation to improve efficiency. Specifically, we fine-tuned a DeBERTa-v3-large 8 (He et al., 2023) model to fit the original API\u2019s toxicity probabilities by minimizing the KL divergence. This fine-tuned model achieved 94.87% accuracy and a 98.54% AUROC score on the hold-out 10k subset, which indicates that it can effectively estimate text toxicity as a substitute for the API. With this accurate estimation performance guarantee, the model has a much higher throughput than the API, i.e., 27,000 samples per second versus typically 25 queries per second using the API."
        },
        {
            "heading": "C Effect of Different Scaling Strategies",
            "text": "Figure 7 (upper) shows the Expected Maximum Toxicity and Average Perplexity results under different combinations of \u03b1 and \u03b2. We can see that increasing both parameters enhances the detoxification effect but raises perplexity. Adjusting \u03bbnorm has a more significant impact on the detoxification effect than adjusting \u03bbsim. This is also reflected in Figure 5, where the \u03bbsim in different attention\n8https://huggingface.co/microsoft/deberta-v3-large\nheads with a high positive correlation with toxicity reduction is sparser than \u03bbnorm. From Figure 7 (bottom), it can be seen that renormalizing modified representations can effectively reduce the loss of perplexity and preserve the model\u2019s capabilities."
        },
        {
            "heading": "D Discussion on Prefix",
            "text": "The negative prefix we use is \"The following text is abusive, harmful, negative, obscene, racist, rude and toxic: \". And the positive prefix is \"The following text is kind, polite, positive, respectful and supportive: \".We craft these prompts based on the definition of toxic content provided by Perspective API 9, keeping them as simple as possible. Although different prefix selections do lead to different toxicity reduction results, our objective is not to identify a singular \u2019best\u2019 prompt. We aim to explore a method that makes the best use of the existing capabilities of language models to detoxify themselves, where the prompt serves as an intermediary to unleash their potential rather than being the research objective.\nThe positive prefix serves as a contrasting reference for a better toxification direction discovery, minimizing the impact of detoxification on the parts of the representation related to the contextual semantics. Table 2 illustrates that using the positive prefix gives a comparable toxicity reduction performance, while having a lower perplexity. Ganguli et al. (2023) shows that only large models (>22B) have the capability to avoid producing harmful outputs using positive prefixes alone, emphasizing the importance of using two contrasting prefixes."
        },
        {
            "heading": "E Cases of Detoxification Dynamic",
            "text": "We find an example where the base model accepts a seemingly non-toxic prompt input but is prone to triggering a highly toxic continuation. The prompt is \"\\\"Put your hand in my face and I\u2019m going to\", and the base model is inclined to predict a verb that makes the continuation toxic or violent. We\n9https://support.perspectiveapi.com/s/about-the-apiattributes-and-languages\nselect 14 obviously toxic verbs from the top predicted tokens as \"negative verbs.\" Except the one, \" slap\", that is discussed in Section 5.3, the other 13 negative verbs are \" beat\", \" break\", \" fuck\", \" hit\", \" hurt\", \" kick\", \" kill\", \" knock\", \" punch\", \" rape\", \" rip\", \" shoot\", \" smash\". The detoxification dynamic of them is shown in Figure 8-20. And the completed continuations are shown in Table 3\nWe can see a similar detoxification dynamic among these figures, where the base model gradually increases the probability of negative verbs after approximately 20-th layer, while our method suppresses the probability of them before 20-th layer. Interestingly, the probability of negative verbs from the output of LN tends to deviate from the one from the input layer-by-layer, which indicates that LNs play non-negligible roles in increasing toxicity, remaining for future research."
        },
        {
            "heading": "F Additional Comparison with Text",
            "text": "Detoxification\nOur work aims at LM detoxification, which is formulated as making LM generate non-toxic texts.\nThis task shares an ultimate similar goal with Text Detoxification, which is to get non-toxic text content, but has a different research question. LM detoxification seeks answers to avoid toxic generation from pretrained LMs, while text detoxification develops methods to convert a given toxic text into a non-toxic one. Nevertheless, one can obtain nontoxic texts by generating them and then detoxifying them. Thus, we provide an additional experiment comparing one text detoxification method, bartdetox-base (Logacheva et al., 2022), with our LM detoxification one.\nThe automatic evaluation results are summarized in Table 4. The results indicate that applying our detoxification method to the sampling procedure results in only a slight increase in the conditional perplexity (PPL). Given that this PPL of continuations is calculated conditioned on their prompt by a larger LM, we infer that there is no noticeable\ncontext deviation in our continuations. Thus, we believe the generated texts remain relevant to the input, similar to the original language model. Moreover, our results suggest that solely cleaning the generated continuations leads to a remarkable PPL deterioration. As Logacheva et al. (2022) demonstrates that their method produces fluent cleaned text, this deterioration could be attributed to a loss of context relevance, rather than fluency issues. Further, this approach does not significantly reduce toxicity."
        },
        {
            "heading": "G Discussion on Computational Cost",
            "text": "In discussing the computational cost, we draw attention to the fact that our method, despite introducing additional computational steps, does not significantly escalate the computational costs. A critical comparison can be drawn with finetuningbased and decoding-based models.\nFirstly, in comparison to finetuning-based models, our method does not require any additional training. The fine-tuning process for language models (LM) is computationally demanding, especially given the increasing size of LMs. Our method, conversely, eliminates this need, reducing the computational load.\nSecondly, when compared to decoding-based methods, our model does not incorporate any extra modules. Table 5 illustrates that two decodingbased baselines introduce additional parameters to the base model, while ours does not. Consequently, our method\u2019s basic memory requirements are less than these alternative approaches.\nThirdly, our method demonstrates superior inference latency compared to the sota decoding-based method, as presented in Table 6. We derived these results from 100 randomly sampled prompts in the dataset with a batch size of 1, conducted on a 3090 GPU.\nWhen combined with the results in Table 1, our method provides a competitive performance without significantly increasing computational costs."
        }
    ],
    "title": "Self-Detoxifying Language Models via Toxification Reversal",
    "year": 2023
}