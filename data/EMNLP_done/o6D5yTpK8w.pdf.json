{
    "abstractText": "Existing studies tend to extract the sentiment elements in a generative manner in order to avoid complex modeling. Despite their effectiveness, they ignore importance of the relationships between sentiment elements that could be crucial, making the large pre-trained generative models sub-optimal for modeling sentiment knowledge. Therefore, we introduce two pre-training paradigms to improve the generation model by exploring graph pre-training that targeting to strengthen the model in capturing the elements\u2019 relationships. Specifically, We first employ an Element-level Graph Pretraining paradigm, which is designed to improve the structure awareness of the generative model. Then, we design a Task-level Graph Pre-training paradigm to make the generative model generalizable and robust against various irregular sentiment quadruples. Extensive experiments show the superiority of our proposed method, and validate the correctness of our motivation. Our code can be found in https://github.com/HoraceXIaoyiBao/ EGP4ABSA-EMNLP2023.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaoyi Bao"
        },
        {
            "affiliations": [],
            "name": "Zhongqing Wang"
        },
        {
            "affiliations": [],
            "name": "Guodong Zhou"
        }
    ],
    "id": "SP:f1c7aa92cb817f157c768df0d56234ba8c9574d0",
    "references": [
        {
            "authors": [
                "Xuefeng Bai",
                "Yulong Chen",
                "Yue Zhang."
            ],
            "title": "Graph pre-training for AMR parsing and generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6001\u20136015, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoyi Bao",
                "Xiaotong Jiang",
                "Zhongqing Wang",
                "Yue Zhang",
                "Guodong Zhou"
            ],
            "title": "Opinion tree parsing for aspect-based sentiment analysis",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoyi Bao",
                "Zhongqing Wang",
                "Xiaotong Jiang",
                "Rong Xiao",
                "Shoushan Li."
            ],
            "title": "Aspect-based sentiment analysis with opinion tree generation",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna,",
            "year": 2022
        },
        {
            "authors": [
                "Jiahao Bu",
                "Lei Ren",
                "Shuang Zheng",
                "Yang Yang",
                "Jingang Wang",
                "Fuzheng Zhang",
                "Wei Wu."
            ],
            "title": "ASAP: A Chinese review dataset towards aspect category sentiment analysis and rating prediction",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Hongjie Cai",
                "Yaofeng Tu",
                "Xiangsheng Zhou",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Aspect-category based sentiment analysis with hierarchical graph convolutional network",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 833\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Hongjie Cai",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "Aspectcategory-opinion-sentiment quadruple extraction with implicit aspects and opinions",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Chenhua Chen",
                "Zhiyang Teng",
                "Zhongqing Wang",
                "Yue Zhang."
            ],
            "title": "Discrete opinion tree induction for aspect-based sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Hao Chen",
                "Zepeng Zhai",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "Enhanced multi-channel graph convolutional network for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Mengting Hu",
                "Yike Wu",
                "Hang Gao",
                "Yinhao Bai",
                "Shiwan Zhao."
            ],
            "title": "Improving aspect sentiment quad prediction via template-order data augmentation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Mengting Hu",
                "Shiwan Zhao",
                "Li Zhang",
                "Keke Cai",
                "Zhong Su",
                "Renhong Cheng",
                "Xiaowei Shen."
            ],
            "title": "CAN: Constrained attention networks for multi-aspect sentiment analysis",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Junjie Li",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Generative cross-domain data augmentation for aspect and opinion co-extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Jian Liu",
                "Zhiyang Teng",
                "Leyang Cui",
                "Hanmeng Liu",
                "Yue Zhang."
            ],
            "title": "Solving aspect category sentiment analysis as a text generation task",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4406\u20134416, Online",
            "year": 2021
        },
        {
            "authors": [
                "Yue Mao",
                "Yi Shen",
                "Jingchao Yang",
                "Xiaoying Zhu",
                "Longjun Cai."
            ],
            "title": "Seq2Path: Generating sentiment tuples as paths of a tree",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Rajdeep Mukherjee",
                "Tapas Nayak",
                "Yash Butala",
                "Sourangshu Bhattacharya",
                "Pawan Goyal."
            ],
            "title": "PASTE: A tagging-free decoding framework using pointer networks for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2021 Conference on Em-",
            "year": 2021
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Haiyun Peng",
                "Lu Xu",
                "Lidong Bing",
                "Fei Huang",
                "Wei Lu",
                "Luo Si."
            ],
            "title": "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8600\u20138607.",
            "year": 2020
        },
        {
            "authors": [
                "Guang Qiu",
                "Bing Liu",
                "Jiajun Bu",
                "Chun Chen."
            ],
            "title": "Opinion Word Expansion and Target Extraction through Double Propagation",
            "venue": "Computational Linguistics, 37(1):9\u201327.",
            "year": 2011
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Ronald Seoh",
                "Ian Birle",
                "Mrinal Tak",
                "Haw-Shiuan Chang",
                "Brian Pinette",
                "Alfred Hough."
            ],
            "title": "Open aspect target sentiment classification with natural language prompts",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Duyu Tang",
                "Bing Qin",
                "Xiaocheng Feng",
                "Ting Liu."
            ],
            "title": "Effective lstms for target-dependent sentiment classification",
            "venue": "COLING 2016, pages 3298\u20133307.",
            "year": 2016
        },
        {
            "authors": [
                "Hai Wan",
                "Yufei Yang",
                "Jianfeng Du",
                "Yanan Liu",
                "Kunxun Qi",
                "Jeff Z. Pan."
            ],
            "title": "Target-aspect-sentiment joint detection for aspect-based sentiment analysis",
            "venue": "AAAI 2020, pages 9122\u20139129.",
            "year": 2020
        },
        {
            "authors": [
                "Qianlong Wang",
                "Zhiyuan Wen",
                "Qin Zhao",
                "Min Yang",
                "Ruifeng Xu."
            ],
            "title": "Progressive self-training with discriminator for aspect term extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 257\u2013268,",
            "year": 2021
        },
        {
            "authors": [
                "Lu Xu",
                "Hao Li",
                "Wei Lu",
                "Lidong Bing."
            ],
            "title": "Position-aware tagging for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2339\u20132349, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Mi Zhang",
                "Tieyun Qian."
            ],
            "title": "Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3540\u20133549, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Yang Deng",
                "Xin Li",
                "Yifei Yuan",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Aspect sentiment quad prediction as paraphrase generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9209\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Xin Li",
                "Yang Deng",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Towards generative aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Zhang",
                "Zili Zhou",
                "Yanna Wang."
            ],
            "title": "SSEGCN: Syntactic and semantic enhanced graph convolutional network for aspect-based sentiment analysis",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Aspect-based sentiment analysis (ABSA) has drawn increasing attention in the community, which includes four fine-grained elements: aspect term, opinion term, aspect category, and opinion polarity. The first two terms exist as a raw text span in the review sentence while the remaining two are the classification result of aspect and opinion respectively. Each four mapped sentiment elements form an aspect-level sentiment quadruple. For instance, for the given review \"The apps are hard to use.\", the corresponding quadruple is (apps, hard, Software, Negative).\nThe joint extraction of quadruples is the most complex and challenging subtask among all the ABSA tasks, previous work usually formulate it as\n\u2217 Corresponding author\neither sequence-level (Qiu et al., 2011; Peng et al., 2020; Cai et al., 2021) or token-level classification problems (Tang et al., 2016) in joint learning or pipeline manner. However, these methods not only require sophisticated and complex modeling of sentiment elements but also suffer severely from error propagation since the overall prediction performance hinges on the accuracy of every step (Peng et al., 2020).\nMore recently, studies tend to tackle the ABSA problem with a unified generative approach (Zhang et al., 2021b,a; Yan et al., 2021; Bao et al., 2022). They organize the target sequence in different approaches, namely listing (Zhang et al., 2021b): \u201c(apps, hard, Software, Negative)\u201d, indexing(Yan et al., 2021): \u201c(1,1,3,3)\u201d, paraphrasing (Zhang et al., 2021a): \u201c(Software is good because apps are hard)\u201d or opinion tree(Bao et al., 2022): \u201c((Root,(Quad,( Aspect ( Software, apps ),( Opinion ( Negative, hard )))))\u201d. However, they ignore the importance of the relationships among elements (e.g. sentiment polarity should be identified based on opinion words, like great identifies a positive polarity and disappointing identifies a negative polarity).\nIn this situation, a natural question is how to\nstrengthen the generative model in modeling aspectlevel sentiment structure. We believe the challenges locate in two aspects. First is structural modeling: the huge gap between the pre-training and finetuning phases makes it difficult to model its succinct yet distinctive structure : certain components ( e.g. aspect term ) in sentiment structure obviously more important than others. Another challenge is the generalization and robustness of the generative model: the generative model should be generalizable and robust against irregular sentiment quadruples. It is crucial since the structure is built depending on the quadruples and the challenging scenarios in real practice are usually brought by the irregular sentiment quadruples.\nIn this study, we proposed two novel graph pretraining paradigms to address above challenges. As shown in Figure 1, we first introduce an optimal self-encoding method called Element-level Graph Pre-training. We abandon the traditional indiscriminate masking strategy (equally random masking every node or edge ) and depending on the characteristics of the opinion tree, adopt sentiment element level masking. Given the opinion tree of the review \"The apps are hard to use.\", only sentiment nodes (namely apps, hard, Software, Negative ) or the sub-trees they composed in the graph will be masked. In this case, this method can serve as an effective addition to structural modeling in opinion tree generation.\nWe then propose a Task-level Graph Pre-training paradigm, which mimics the human learning pro-\ncedure to learn to handle the task in stages. Specifically, we first decompose the quadruple extraction task into multiple subtasks. Each subtask corresponds to mapping the steps for manually building an opinion tree from scratch. Afterwards, we feature a prompt-based learning strategy to separately acquire the knowledge of subtasks and finally employ the learned knowledge to tackle the main task, i.e., generating the entire opinion tree. The decomposed subtasks build fundamental knowledge of irregular sentiment quadruples for generation.\nAs shown in Figure 2, we then jointly pre-train the model with the two paradigms above and finetune the model with the Finetune task. The advantages of our pre-training method over previous learning methods are threefold: 1) both the Element-level Graph Pre-training and Task-level Graph Pre-training are designed depending on the intrinsic characteristics of the opinion tree instead of treating it as a plain graph.2) the Element-level Graph Pre-training abandons the strategy of capturing the complex structure but focuses directly on the core elements. 3) the Task-level Graph Pretraining explicitly forces the model to learn the irregular quadruples with an easy-to-hard routine, making it easier for the model to learn the fundamental knowledge required. The detailed evaluation shows that our model significantly advances the state-of-the-art performance on several benchmark datasets."
        },
        {
            "heading": "2 Related Work",
            "text": "There are four aspect-level sentiment elements in ABSA, the various combination of these elements form the numerous sub-tasks of ABSA. The researches on ABSA generally follow a route from handling single sub-task to complex compositions of them. The starting point usually locates in the prediction of a single sentiment element, which is the target of fundamental sub-tasks, such as extracting the aspect term (Qiu et al., 2011; Tang et al., 2016; Wang et al., 2021), classifing the aspect category mentioned in the sentence (Bu et al., 2021; Hu et al., 2019), and detecting the sentiment polarity for a given aspect (Tang et al., 2016; Chen et al., 2022a; Liu et al., 2021; Seoh et al., 2021; Zhang et al., 2022).\nSince the sentiment elements are naturally correlated, many studies further focus on exploring the co-extraction of sentiment elements, including aspect and opinion term extraction (Xu et al., 2020; Li et al., 2022); aspect term extraction and its polarity detection (Zhang and Qian, 2020); aspect category and polarity detection (Cai et al., 2020). Furthermore, recent studies also employed end-toend models to extract all the sentiment elements in triplet or quadruple format (Peng et al., 2020; Wan et al., 2020; Cai et al., 2021; Zhang et al., 2021a; Chen et al., 2022b; Mukherjee et al., 2021).\nMore recently, studies tend to design a unified framework to extract quadruples at one stop with pre-trained encoder-decoder language models, achieving great improvements in ABSA (Zhang et al., 2021a). The target sequence of them is formed by either class index (Yan et al., 2021) or the desired sentiment element (Zhang et al., 2021b). OTG (Bao et al., 2022) addressed the importance of semantic correlations among sentiment\nelements, proposed a sentiment tree structure called opinion tree, and employed generative model to extract the linearized tree. However, the generative model is pre-trained to solve textual sequence tasks(e.g. masked language model) but finetuned for structure generation, between which exists a huge gap, making generative models sub-optimal for modeling structural knowledge.\nDifferent from previous studies, we introduce two pre-training paradigms for opinion tree generation without treating it as a plain graph. To our knowledge, we are the first to consider designing methods depending on the intrinsic characteristics of the opinion tree."
        },
        {
            "heading": "3 Opinion Tree Generation Model",
            "text": "In this section, we introduce the basic opinion tree generation model we employed to generate in the pre-train and finetune phases, along with the objective functions and training."
        },
        {
            "heading": "3.1 Opinion Tree Construction",
            "text": "For further strengthen the relationship between elements, we build a structure called opinion tree, which aims to jointly model all sentiment elements in a tree for a given review sentence. The opinion tree can be considered as a semantic representation in order to better represent the structure of sentiment elements. Inside the opinion tree, each sentiment element would be connected with another node as either the child or parent relation to represent the crucial relationship.\nAs shown in Figure 3, we construct the opinion tree using a rooted directed acyclic graph, including nodes of aspect, opinion, category, and polarity, along with the semantic relations between them. After that, we linearize the opinion tree to the target sequence via depth-first traversal."
        },
        {
            "heading": "3.2 Generation Model",
            "text": "We employ the pre-trained language model T5 (Raffel et al., 2020) to generate the linearized opinion tree. As shown in Figure 3, it is an encoderdecoder architecture model, the input would be the raw review and the output is linearized opinion tree. Given the token sequence x = x1, ..., x|x| as input, the sequence-to-sequence model outputs the linearized representation y = y1, ..., y|y|. To this end, the sequence-to-sequence model first computes the hidden vector representation:\nH = (x1, ..., x|x|) (1)\nAfter the input token sequence is encoded, the decoder predicts the output sequence token-bytoken with the sequential input tokens\u2019 hidden vectors. At the i-th step of generation, the selfattention decoder predicts the i-th token yi in the linearized form, and decoder state hdi as:\nyi, h d i = ([H;h d 1, ..., h d i\u22121], yi\u22121) (2)\nThe conditional probability of the whole output sequence p(y|x) is progressively combined by the probability of each step p(yi|y<i, x):\np(y|x) = |y|\u220f i=1 p(yi|y<i, x) (3)\nwhere y<i = y1...yi\u22121, and p(yi|y<i, x) are the probabilities over target vocabulary V .\nThe objective functions is to maximize the output linearized opinion tree XT probability given the review sentence XO. Therefore, we optimize the negative log-likelihood loss function:\nL = \u2212 1 |\u03c4 | \u2211 (XO,XT )\u2208\u03c4 log p(XT |XO; \u03b8) (4)\nwhere \u03b8 is the model parameters, and (XO, XT ) is a (sentence, tree) pair in training set \u03c4 , then\nlog p(XT |XO; \u03b8) =\n= n\u2211\ni=1\nlog p(xiT |x1T , x2T , ...xi\u22121T , XO; \u03b8) (5)\nwhere p(xiT |x1T , x2T , ...x i\u22121 T , XO; \u03b8) is calculated by the decoder."
        },
        {
            "heading": "4 Pre-training Paradigms",
            "text": "In this study, we introduce two pre-training paradigms for opinion tree generation. As shown in Figure 2, the two paradigms and finetune task share the same input format with a joint input of prompt, encoded text and tree, each method consists of a set of subtasks focus on respective training targets. The combination of subtasks forms the joint pre-training in our work, we will introduce the paradigms first in this section."
        },
        {
            "heading": "4.1 Element-level Graph Pre-training",
            "text": "The opinion tree is directly composed of subtrees that represent respective quadruples, this naturally decides the noteworthy information must locate within the aspect-level sentiment element instead of the other parts of the opinion tree, which could be other structure nodes. For instance, for a linearized opinion tree \"(Root,(Quad,(Aspect (Software, apps),(Opinion (Negative, hard)\", the indiscriminate masking may mask a sub-sequence \"(Opinion (\" that: 1) logically can not be reform into a valid structure due to the non-closing brackets. 2) contains nodes (e.g.\"Opinion\" ) not included in the crucial sentiment elements.\nOn the other hand, our Element-level Graph Pre-training paradigm masks aspect-level element nodes (including aspect term, opinion term, aspect category, and opinion polarity) in the opinion tree, as shown in Figure 4, the masked sequence \"(Software, apps )\" represent legitimate struct and covers core sentiment element only. If continuous nodes are masked, the corresponding sub-graph will be masked as a whole. The method can not only make sure the masked node are crucial sentiment elements but also guarantee the corresponding subsequence is logically legitimate.\nWith the element-level graph mask strategy introduced above, we propose a set of pre-training\nsubtasks. The inputs would be a concat of a prompt, a sentence, and an opinion tree. The sentence and tree will be masked with different masking rates while the prompt illustrates the output target, either the sentence or tree. For a given review s = (x1, x2, ...xn\u22121, xn) and linearized tree t = (t1, t2, ...tn\u22121, tn), We design the 5 subtasks in the Element-level Graph Pre-training paradigm, which can be found in Table 1. Among which, EPG1 and EPG4 are designed to help the model generate the complete tree t by adding text information while EPG2, EPG3 and EPG5 help the model to generate the full review s by adding the structural information.\nTo further emphasize the interaction between the pre-training and finetune phases, we designed a dynamic masking rate for Element-level Graph Pre-training paradigms: a small masking rate is used in the initial phase, and then the masking rate increases with training rounds, so that at the end of pre-training, all partially masked pre-training tasks be very close to the finetune tasks (which can be considered as 100% masking rate), the specific masking rate is shown in Table 2. Note our masking rate obviously lower than previous work (Bai et al., 2022), that is because recovering a nearly all-\nmasked text from an opinion tree is unreasonable since opinion tree contains limited information as we discussed before."
        },
        {
            "heading": "4.2 Task-level Graph Pre-training",
            "text": "Inspired by the human-learning process we propose a Task-level Graph Pre-training paradigm, whose subtasks follow the routine of human learning procedure to learn to build the opinion tree from scratch. Specifically, we first decompose the quadruple extraction task into multiple subtasks. Each subtask corresponds to mapping the steps for manually building an opinion tree from scratch. The paradigm consists of six subtasks, four (Aspect, Opinion, Category, Polarity) of which extract sentiment structure as the fundamental knowledge for building an opinion tree, the rest (Pair, Triple) target the intermediate state of the procedure with co-extraction. The subtasks and the corresponding steps of building can be found in Appendix A. In this case, we force the model to focus directly on irregular cases with a gradual process to build fundamental knowledge for OTG. The inputs of Task-level Graph Pre-training are similar to the previous paradigm, which would be a concat of a prompt and a sentence. Then the subtasks in Task-level Graph Pre-training paradigm can be given as shown in Figure 5."
        },
        {
            "heading": "4.3 Joint Pre-training",
            "text": "We use a joint pre-training method to combine the advantages of the Element-level Graph Pre-training paradigm and Task-level Graph Pretraining paradigms. In addition, we include the\nfinetune task Finetune in the pre-train phase for narrowing the gap between two phases and avoiding overfitting. During pre-training, the model will be cyclically trained in the order of a loop started with the subtasks of the Element-level Graph Pretraining, followed by Task-level Graph Pre-training, the gradient will be updated after accumulating the loss in each epoch. After that, we save the model weights and finetune the model with finetune task Finetune."
        },
        {
            "heading": "5 Experiments",
            "text": "In this section, we introduce the datasets used for evaluation and the baseline methods employed for comparison. We then report the experimental results conducted from different perspectives, and analyze the effectiveness of the proposed model with different factors."
        },
        {
            "heading": "5.1 Setting",
            "text": "In this study, we use ACOS dataset (Cai et al., 2021) for our experiments. Following the setting from (Cai et al., 2021), we divide the original dataset into a training set, a validation set, and a testing set. In addition, we choose 20,000 sentences from Yelp1, and 20,000 sentences from the laptop domain in Amazon2 to pre-train the opinion tree generation model, the sentences are annotated by the OTG model without pre-training.\nFollowing the setting of Bao et al. (2023), we divide the quadruples into 4 types, apart from the basic situation, there are 3 irregular situations: Oneto-Many, Mono-Implicit and Bi-Implicit. The statistic can be found in Figure 6.\n1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/\nWe employ T53 and fine-tune its parameters for our opinion tree generation model. We tune the parameters of our models by grid searching on the validation dataset. We select the best models by early stopping using the Accuracy results on the validation dataset. The model parameters are optimized by Adam (Kingma and Ba, 2015), the learning rate of pre-training and finetuning is 3e-5 and 1e-4 respectively. The batch size is 16. Our experiments are carried out with an Nvidia RTX 3090 GPU. The experimental results are obtained by averaging ten runs with random initialization.\nIn evaluation, a quadruple is viewed as correct if and only if the four elements, as well as their combination, are exactly the same as those in the gold quadruple. On this basis, we calculate the Precision and Recall, and use F1 score as the final evaluation metric for aspect sentiment quadruple extraction (Cai et al., 2021; Zhang et al., 2021a)."
        },
        {
            "heading": "5.2 Main Results",
            "text": "We compare the proposed method with several classification-based aspect-based sentiment analysis models, including, DP (Qiu et al., 2011), JET (Xu et al., 2020), TAS-BERT (Wan et al., 2020) and Extract-Classify (Cai et al., 2021). In addition, generative models are also compared, such as BARTABSA (Yan et al., 2021), GAS (Zhang et al., 2021b), Paraphrase (Zhang et al., 2021a),TODA (Hu et al., 2022), Seq2Path (Mao et al., 2022) and OTG (Bao et al., 2022).4.\nParticularly, we build two Large Language Model (LLM) baselines: ChatGPT5 is a sibling model to InstructGPT (Ouyang et al., 2022), which is trained to follow instruction in a prompt and provide a detailed response. We ask it to generate all the sentiment elements from the input review sentences. LLaMA6 (Touvron et al., 2023) is a collection of foundation language models, these models are trained on trillions of tokens, and have shown that it is possible to train state-of-the-art models using publicly available datasets exclusively. We use LLaMA-7B, and fine-tune it on the ABSA dataset.\nAs shown in Table 3, we find that generative models outperform previous classification-based methods and the structural generative method sur-\n3T5base, https://huggingface.co/transformers/ model_doc/t5.html\n4We directly adopt the result from Bao et al. (2022) 5https://openai.com/blog/chatgpt. 6https://huggingface.co/docs/transformers/\nmain/model_doc/llama.\npasses non-structural methods, this indicates that semantic structure does contribute to quadruple extraction. Meanwhile, our proposed model outperforms all the previous studies significantly (p < 0.05), which has an advantage of 2.36% and 0.92% in Restaurant and Laptop domain respectively. The result shows that the proposed joint pre-training is effective in modeling tree structural constraints for generative model, while the large gap between pre-training and finetuning significantly encumbers previous systems. Furthermore, the results also indicate the effectiveness of our Element-level Graph Pre-training and Task Decomposition paradigms, which are used to unify the pre-train and finetune task with special task designs depending on the intrinsic characteristics of the opinion tree instead of treating it as a plain graph."
        },
        {
            "heading": "6 Analysis and Discussion",
            "text": "In this section, we first give some analysis and discussion to show the influence of Element-level Graph Pre-training (EGP) and Task-level Graph Pre-training (TGP) paradigms. After that, we will investigate our search over masking rate, the influence of pre-training subtasks."
        },
        {
            "heading": "6.1 Influence of Different Factors",
            "text": "We first investigate the difference between the two paradigms, from Table 4 we can find, all the paradigms are beneficial to extract the opinion tree. Among which TGP paradigm\u2019s contribution outperforms EGP paradigm, the removal of TGP cause an\navgerage drop of 0.52% while EGP\u2019s cause 0.21%, this may due to the generalization and robustness being more effective than the structural association."
        },
        {
            "heading": "6.2 Effect of Element-level Graph Pre-training",
            "text": "Under the setting of our element-level masking design for graph pre-train, previous graph-masking strategies can be classified into the indiscriminate paradigm, which means indiscriminately masking random nodes and words in tree or text. In\nthis situation, there will be one intuitive question: Whether the element-level masking design does achieve a performance better than the indiscriminate paradigm as we expect?\nWe investigate this question by employing ablation experiments. We first design an indiscriminate paradigm under similar settings, then we give the performance of using different paradigms in Table 5. As we can see, our element-level paradigm outperforms the indiscriminate paradigm, this result shows the superiority of our element-level masking design, and also validated our motivation: for target graphs that contain limited knowledge like opinion tree, indiscriminate masking strategies would be sub-optimal and fine-grained masking should be adopted.\nWe then investigate the impact of subtasks in EGP paradigm. We add the subtasks in paradigm gradually. As we can see in Table 5, the subtask pair of EPG5 and EPG4 (+All EGPs) contributes the most to the performance (0.58% and 0.42% in each domain respectively), which aims to integrate the complementary information from both formations to generate text and tree respectively, indicating the significance of the complementary association."
        },
        {
            "heading": "6.3 Effect of Task-level Graph Pre-training",
            "text": "As shown in Table 6, the OTG model obviously be short in its generalization and robustness against irregular sentiment quadruples when compared with\nthe basic situation. Thus we mimic the human learning procedure for building an opinion tree from scratch with Task-level Graph Pre-training to strengthen its fundamental knowledge.\nWe investigate the paradigm\u2019s effect by comparing the model\u2019s performance on each irregular quadruple situation. As shown in Table 6 , our model\u2019s improvement in all of the irregular classes surpasses the basic situation when compared with OTG. This result indicates that our pretrain method significantly improves the model\u2019s performance with a burst in generalization and robustness against irregular sentiment quadruples, which accomplish the foundation for building an opinion tree and should be taken into consideration apart from improving the structural awareness.\nWe then investigate the impact of subtasks in TGP paradigm. We remove the subtasks in the paradigms gradually. Table 7 shows the result for Task Decomposition paradigm: the contributions of subtasks stay in a similar scope, among which the Aspect surpasses others with a tiny gap, this may due to the lower implicit rate of aspect terms7.\nIn addition, all the subtasks are beneficial to extract the opinion tree. It is worth noting that, the participation of finetune task Finetune demonstrates an obviously positive effect in both paradigms, which improves two domains with an average of 0.31%, this phenomenon gives us a conclusion that adding the finetune task in the pre-train phase is an effective solution for narrowing the gap between them."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this study, we propose two novel pre-train paradigms for opinion tree generation, which are designed depending on the intrinsic characteristics of the opinion tree. Specifically, the Element-level Graph Pre-training paradigm abandons the strategy of capturing the complex structure but focuses directly on the core elements. While the Task-level Graph Pre-training explicitly focuses on improving the generalization and robustness against irregular quadruples with an easy-to-hard routine. Furthermore, we explore a dynamic masking rate and a cyclical train method for jointly combining the pre-training paradigms in order to bridge the gap between the pre-training and finetuning phases in modeling structural knowledge.\n7The average implicit rate of aspect term and opinion term is 22.63% and 24.19% respectively\nExperimental results show that our proposed model can achieve state-of-the-art performance in ABSA. In addition, the results also validate that, for target graphs that contain certain knowledge like opinion tree, the improving strategy should be made based on the intrinsic characteristics of the structure instead of treating it as a plain graph.\nLimitations\nThe limitations of our work can be stated from three perspectives. First, our pre-training method contains many subtasks that will consume vast computational cost during pre-train (the inference cost will not change). If possible, further work should try to explore a time-saving pre-training method. Secondly, more tasks could be further explored, including cross-domain and cross-lingo sentiment analysis tasks. Finally, we focus on opinion tree generation in one major language. The performance of other languages remains unknown."
        },
        {
            "heading": "A Building Procedure",
            "text": "In Task-level Graph Pre-training paradigm, the subtasks are set to follow the routine of building the opinion tree from scratch. For building an opinion tree manually, humans often learn to find fundamental elements, such as aspects or opinions, followed by finding the corresponding classification result such as category and polarity to build a single quadruple unit, then composing multiple units to fulfill a more challenging goal, i.e., writing the entire opinion tree.\nBased on the process introduced, we design the subtasks in Task-level Graph Pre-training paradigm. Each subtask corresponds to mapping the steps for manually building an opinion tree from scratch. The paradigm consists of six subtasks: Aspect, Opinion, Category, Polarity,Pair and Triple. Their prompts and target graph can be found in Figure 7. Among which, Aspect and Opinion focus on searching the basic elements of each quadruple:\n\u2022 Aspect: Extract all the aspect terms in the review in the form of a tree, Figure 7 (a).\n\u2022 Opinion: Extract all the Opinion terms in the review in the form of a tree, Figure 7 (b).\nCategory and Polarity further explore the classification results with the corresponding basic elements:\n\u2022 Category: On the base of Aspect, extract the category classification result of the aspect terms in the review in the form of a tree, Figure 7 (c).\n\u2022 Polarity: On the base of Opinion, extract the polarity classification result of the opinion terms in the review in the form of a tree, Figure 7 (d).\nPair and Triple fulfill the mapping between quadruples.\n\u2022 Pair: On the base of Aspect and Opinion, map the corresponding aspect term and opinion term within a quadruple, Figure 7 (e).\n\u2022 Triple: On the base of Aspect and Polarity, map the corresponding aspect term and opinion term and polarity within a quadruple, Figure 7 (f).\nsurface aspect\napps aspect\nROOT\nAspect Aspect\nsmooth opinion hard opinion\nROOT\nOpinion Opinion\nDesign category Software category\nROOT\nAspect Aspect\nsurface aspect surface aspect\nsmooth opinion\nPositive polarity\nROOT\nOpinion\nhard opinion\nNegative polarity\nOpinion\n[Aspect] [Opinion]\n[Category][Aspect] [Polarity][Opinion]\nQuad\nQuad\nROOT\n[Aspect][Opinion]\n(a) Aspect Extraction (b) Opinion Extraction\n(c) Category Classification (d) Polarity Classification"
        }
    ],
    "title": "Exploring Graph Pre-training for Aspect-based Sentiment Analysis",
    "year": 2023
}