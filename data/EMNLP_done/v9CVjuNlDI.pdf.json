{
    "abstractText": "Dense retrieval models have exhibited remarkable effectiveness, but they rely on abundant labeled data and face challenges when applied to different domains. Previous domain adaptation methods have employed generative models to generate pseudo queries, creating pseudo datasets to enhance the performance of dense retrieval models. However, these approaches typically use unadapted rerank models, leading to potentially imprecise labels. In this paper, we demonstrate the significance of adapting the rerank model to the target domain prior to utilizing it for label generation. This adaptation process enables us to obtain more accurate labels, thereby improving the overall performance of the dense retrieval model. Additionally, by combining the adapted retrieval model with the adapted rerank model, we achieve significantly better domain adaptation results across three retrieval datasets. We release our code for future research.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Che-Wei Chen"
        },
        {
            "affiliations": [],
            "name": "Ching-Wen Yang"
        },
        {
            "affiliations": [],
            "name": "Chun-Yi Lin"
        },
        {
            "affiliations": [],
            "name": "Hung-Yu Kao"
        }
    ],
    "id": "SP:ee53bc643cb430b5ca06fa3600b505bc0ff2e17e",
    "references": [
        {
            "authors": [
                "Devansh Arpit",
                "Stanis\u0142aw Jastrz\u0119bski",
                "Nicolas Ballas",
                "David Krueger",
                "Emmanuel Bengio",
                "Maxinder S Kanwal",
                "Tegan Maharaj",
                "Asja Fischer",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "A closer look at memorization in deep networks",
            "year": 2017
        },
        {
            "authors": [
                "Hans Christian",
                "Mikhael Pramodana Agus",
                "Derwin Suhartono."
            ],
            "title": "Single document automatic text summarization using term frequency-inverse document frequency (tf-idf)",
            "venue": "ComTech: Computer, Mathematics and Engineering Applications, 7(4):285\u2013294.",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Hofst\u00e4tter",
                "Sophia Althammer",
                "Michael Schr\u00f6der",
                "Mete Sertkan",
                "Allan Hanbury"
            ],
            "title": "Improving efficient neural ranking models with crossarchitecture knowledge distillation",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Hofst\u00e4tter",
                "Allan Hanbury."
            ],
            "title": "Let\u2019s measure run time! extending the ir replicability infrastructure to include performance aspects",
            "venue": "arXiv preprint arXiv:1907.04614.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqi Huang",
                "Puxuan Yu",
                "James Allan."
            ],
            "title": "Cross-lingual knowledge transfer via distillation for multilingual information retrieval",
            "venue": "arXiv preprint arXiv:2302.13400.",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Carlos Lassance"
            ],
            "title": "Extending english ir methods",
            "year": 2023
        },
        {
            "authors": [
                "Linguistics. Tie-Yan Liu"
            ],
            "title": "Learning to rank for informa",
            "year": 2009
        },
        {
            "authors": [
                "McDonald"
            ],
            "title": "Zero-shot neural passage retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Alexandra Balahur"
            ],
            "title": "Www\u201918 open challenge",
            "year": 2018
        },
        {
            "authors": [
                "cius",
                "Leon Wong"
            ],
            "title": "High accuracy retrieval",
            "year": 2006
        },
        {
            "authors": [
                "Jimmy Lin"
            ],
            "title": "Document ranking with a pre",
            "year": 2020
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Jimmy Lin",
                "AI Epistemic."
            ],
            "title": "From doc2query to doctttttquery",
            "venue": "Online preprint, 6.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Stephen E Robertson",
                "Steve Walker",
                "Susan Jones",
                "Micheline M Hancock-Beaulieu",
                "Mike Gatford"
            ],
            "title": "Okapi at trec-3",
            "venue": "Nist Special Publication",
            "year": 1995
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Colbertv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Charles Stein."
            ],
            "title": "Estimation with quadratic loss",
            "venue": "Breakthroughs in statistics: Foundations and basic theory, pages 443\u2013460.",
            "year": 1992
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J Gordon."
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "arXiv preprint arXiv:1812.05159.",
            "year": 2018
        },
        {
            "authors": [
                "Ellen Voorhees",
                "Tasmeer Alam",
                "Steven Bedrick",
                "Dina Demner-Fushman",
                "William R Hersh",
                "Kyle Lo",
                "Kirk Roberts",
                "Ian Soboroff",
                "Lucy Lu Wang."
            ],
            "title": "Trec-covid: constructing a pandemic information retrieval test collection",
            "venue": "ACM SIGIR Forum, vol-",
            "year": 2021
        },
        {
            "authors": [
                "David Wadden",
                "Shanchuan Lin",
                "Kyle Lo",
                "Lucy Lu Wang",
                "Madeleine van Zuylen",
                "Arman Cohan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fact or fiction: Verifying scientific claims",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Kexin Wang",
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 671\u2013688.",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Wang",
                "Nandan Thakur",
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval",
            "venue": "arXiv preprint arXiv:2112.07577.",
            "year": 2021
        },
        {
            "authors": [
                "Lidan Wang",
                "Jimmy Lin",
                "Donald Metzler."
            ],
            "title": "A cascade ranking model for efficient ranked retrieval",
            "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 105\u2013114.",
            "year": 2011
        },
        {
            "authors": [
                "Lucy Lu Wang",
                "Kyle Lo",
                "Yoganand Chandrasekhar",
                "Russell Reas",
                "Jiangjiang Yang",
                "Darrin Eide",
                "Kathryn Funk",
                "Rodney Kinney",
                "Ziyang Liu",
                "William Merrill"
            ],
            "title": "2020a. Cord-19: The covid-19 open research dataset. ArXiv",
            "year": 2020
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems, 33:5776\u20135788.",
            "year": 2020
        },
        {
            "authors": [
                "Lee Xiong",
                "Chenyan Xiong",
                "Ye Li",
                "Kwok-Fung Tang",
                "Jialin Liu",
                "Paul N. Bennett",
                "Junaid Ahmed",
                "Arnold Overwijk."
            ],
            "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
            "venue": "International Conference on Learning",
            "year": 2021
        },
        {
            "authors": [
                "Wei Yang",
                "Yuqing Xie",
                "Aileen Lin",
                "Xingyu Li",
                "Luchen Tan",
                "Kun Xiong",
                "Ming Li",
                "Jimmy Lin."
            ],
            "title": "End-to-end open-domain question answering with BERTserini",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Qi Zhang",
                "Zijian Yang",
                "Yilun Huang",
                "Ze Chen",
                "Zijian Cai",
                "Kangxu Wang",
                "Jiewen Zheng",
                "Jiarong He",
                "Jin Gao."
            ],
            "title": "Enhancing model performance in multilingual information retrieval with comprehensive data engineering techniques",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Muhao Chen."
            ],
            "title": "Learning from noisy labels for entity-centric information extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Thakur"
            ],
            "title": "We compare our approach with previous zero-shot and domain adaptation approaches. For GPL, MLM, Qgen and TSDAE, we utilize the data provided from Wang et al. (2021b)",
            "venue": "For docT5query,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The goal of information retrieval (IR) is to enable users to input a query and retrieve relevant passages or documents from the retrieval system. A standard IR system (Matveeva et al., 2006; Liu et al., 2009; Wang et al., 2011; Yang et al., 2019) typically comprises two main stages (refer to Fig. 1):\n1. First-stage retrieval model: This model is designed to retrieve a small subset of relevant passages based on the given query.\n2. Rerank model: Responsible for reordering the retrieved passages, the rerank model aims to enhance the overall user experience.\nRecent advancements in contextualized word embeddings (Liu et al., 2019; Devlin et al., 2019; Lewis et al., 2020) have established dense retrieval\n1https://github.com/eric88525/UDADF\n(Karpukhin et al., 2020; Xiong et al., 2021) as the mainstream approach in information retrieval (IR). This approach effectively addresses the issue of missing words encountered in lexical methods like BM25 (Robertson et al., 1995) or TF-IDF (Christian et al., 2016) and has powerful retrieval ability.\nHowever, the introduction of the BEIR Benchmark (Thakur et al., 2021) has highlighted the limitations of dense models. Surprisingly, traditional lexical-based models like BM25 have exhibited superior performance compared to dense retrieval models in out-of-domain scenarios. Additionally, training dense retrieval models from scratch necessitates a significant amount of domain-specific training data, posing challenges in collecting largescale data for each specific domain. As a result, there is a growing demand for domain adaptation methods that can enhance model performance without relying on labeled data.\nPrevious unsupervised domain adaptation approaches, such as Qgen (Ma et al., 2021) and GPL (Wang et al., 2021b), have employed generative models to generate pseudo queries and augment the training data. Qgen and GPL utilized a finetuned rerank model (Nogueira and Cho, 2019) on the MSMARCO dataset (Nguyen et al., 2017) to assign relevance scores to the generated queries and passages, creating a pseudo dataset for training the\ndense retrieval model. However, they did not adapt the rerank model to the target domain, resulting in potential label imprecision and failure to improve the two-stage retrieval score.\nIn this paper, we present a novel technique for unsupervised domain adaptation (Fig.2). Our approach focuses on simultaneously adapting the dense retrieval model and rerank model within a two-stage retrieval system. The process begins by generating pseudo queries for target domain passages, treating them as relative pairs. We employ sparse and dense negative miners to identify challenging negatives and create a pseudo dataset by combining queries with relevant and irrelevant passages. The rerank model is then fine-tuned using this dataset.\nSince this dataset is not labeled by human, it is likely to contain noisy labels. Therefore, we propose a denoise-finetuning approach that incorporates random batch warm-up and co-regularization learning. Our experimental results validate the effectiveness of this approach in alleviating the impact of noisy labels.\nIn our domain adaptation approach, we utilize models that have been fine-tuned on the extensive MSMARCO (Nguyen et al., 2017) IR dataset, which serves as a strong foundation for adapting to various domains. Remarkably, our approach outperforms previous domain adaptation methods in both one-stage and two-stage retrieval tasks, showcasing superior performance.\nIn summary, our pipeline offers the following contributions:\n\u2022 Full adaptation of both the rerank model and dense retrieval model without requiring target domain labeling data.\n\u2022 Successful training of the rerank model on the generated dataset, effectively mitigating the influence of noisy labels through random batch warm-up and co-regularization techniques.\n\u2022 Transfer of domain-adapted knowledge from the rerank model to the dense retrieval model using knowledge distillation (Hofst\u00e4tter et al., 2020), leading to significant performance improvements in the final two-stage retrieval."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Two-stage Retrieval",
            "text": "Two-stage retrieval (Matveeva et al., 2006; Liu et al., 2009; Wang et al., 2011; Yang et al., 2019)\nis a widely adopted approach that combines the strengths of retrieval models and rerank models for effective information retrieval. It has emerged as the preferred pipeline for competitive IR competition tasks (Lassance, 2023; Huang et al., 2023; Zhang et al., 2023).\nRerank models, such as cross-encoder (Nogueira and Cho, 2019) and mono-T5 (Nogueira et al., 2020), demonstrate exceptional performance compared to retrieval models. However, their demanding computational requirements (Khattab and Zaharia, 2020) restrict their practical application to reranking a limited subset of passages.\nIn this study, we adapt the BERT base retrieval model (Devlin et al., 2019) using the bi-encoder architecture (Reimers and Gurevych, 2019), while the rerank model is implemented using the crossencoder architecture (Nogueira and Cho, 2019).\nThe bi-encoder serves as the initial retrieval model, where we input a query Q and a passage P to obtain their respective mean-pooled output vectors E(Q) and E(P ). The similarity score between the query and passage is computed by taking the dot product of these embeddings (Eq.1). To optimize efficiency, we pre-compute and store the passage embeddings, allowing for efficient retrieval by encoding the query input and calculating similarity scores.\nSim(Q,P ) = E(Q) \u00b7 E(P ) (1)\nOn the other hand, the cross-encoder model treats reranking as a binary classification task. It involves tokenizing each query and passage, denoted as q(1), \u00b7 \u00b7 \u00b7 , q(n) and p(1), \u00b7 \u00b7 \u00b7 , p(m) respectively. The input sequence then is formed as follows:\n[[CLS], q(1), . . . , q(n), [SEP ], p(1), . . . , p(m)] (2)\nThis sequence is subsequently fed into the BERT model, where the [CLS] embedding is utilized as the input for a single neural network. Crossencoder exhibits strong performance but has higher latency than the bi-encoder (Hofst\u00e4tter and Hanbury, 2019). Hence, it is a more suitable choice to pair it with the first-stage retrieval model."
        },
        {
            "heading": "2.2 Query Generation",
            "text": "In previous research, docT5query (Nogueira et al., 2019) fine-tuned the T5 (Raffel et al., 2020) model as a query generator using labeled query-passage pairs from the MSMARCO (Nguyen et al., 2017)\ndataset. Their method involved generating questions based on passages and concatenating them with the passages themselves, leading to improved retrieval performance for lexical-based methods.\nIn the context of domain adaptation, previous methods QGen (Ma et al., 2021) and GPL (Wang et al., 2021b) utilized the generated questions as training data to enhance the dense retrieval model. These methods varied in their labeling approaches, with QGen using binary labels and GPL utilizing a cross-encoder (Nogueira and Cho, 2019) to assign soft labels to query-passage pairs.\nBuilding upon the work of GPL, we further refined the performance of the cross-encoder model in the target domain. By leveraging the adapted cross-encoder, we achieved more precise labeling, resulting in improved performance of the dense retrieval model. Additionally, we employed a two-stage retrieval strategy by combining the adapted cross-encoder with the adapted retrieval model. This innovative approach led to a significant 10% enhancement in NDCG@10 across all three datasets, surpassing the performance of previous methodologies."
        },
        {
            "heading": "3 Methodology",
            "text": "Our method, illustrated in Fig.2, achieves domain adaptation for both the retrieval model and rerank model components through three steps: (1) constructing a pseudo training set, (2) denoisefinetuning the rerank model for domain adaptation, and (3) distilling the knowledge from the rerank\nmodel to the dense retrieval model."
        },
        {
            "heading": "3.1 Generation of Pseudo Training Dataset",
            "text": "To generate pseudo queries for the passages in a given corpus, we employ a pre-trained T5 model trained on the MSMARCO dataset. To promote diversity, we generate three pseudo queries for each passage, resulting in a total of approximately 300K pseudo queries. For datasets with more than 100K passages, we randomly select 100K passages and generate three pseudo queries for each selected passage. The training data for the target domain comprises around 300K (pseudo query, passage) pairs, denoted as Dpseudo = {(Qi, Pi)}i."
        },
        {
            "heading": "3.2 Cross-Encoder (Rerank Model) Adaptation",
            "text": "The cross-encoder (Nogueira and Cho, 2019) treats the reranking of passages as a binary classification task. To fine-tune it for the target domain, the training data should consist of (query, passage, label) triplets, where the label is either 1 or 0 to indicate relevance or irrelevance. After obtaining the generated training data Dpseudo = {(Qi, Pi)}i, we apply a filter to exclude queries which is shorter than 5 words. This filtering is necessary because we noticed that shorter queries often inquire about the meaning of a word in the format \"What is xxx\", which does not align with the distribution of the queries in our testing datasets. For a clearer example, please refer to Appendix A.\nWe leverage a cross-encoder trained on the MS-\nMARCO (Nguyen et al., 2017) dataset to assign scores to each (query, passage) pair in Dpseudo. From these pairs, we extract the top 100k (Qi, Pi) with the highest cross-encoder prediction scores as positive training samples, forming the positive dataset Dpos = {(Qi, Pi, 1)}100ki .\nTo obtain challenging irrelevant passages, we utilize two retrieval approaches: (1) BM25 and (2) a bi-encoder trained on the MSMARCO dataset. For each query, we retrieve the top-1000 similar passages using both models. From the retrieval pools of these two models, we randomly select one passage from each pool, resulting in two negative samples. This process allows us to create the negative training dataset Dneg = {(Qi, Pi, 0)}200Ki .\nBy combining the positive samples from Dpos and the negative samples from Dneg, we construct the final training dataset for the cross-encoder, denoted as Dce = Dneg \u222aDpos. The cross-encoder is trained to predict the probability of relevance between the query and passage pairs, guided by the provided labels in the training data.\nThis selection process of positive and negative samples ensures that the cross-encoder learns to discriminate between relevant and irrelevant querypassage pairs, enhancing its capability to capture the semantic relationship between queries and passages."
        },
        {
            "heading": "3.3 Denoise Finetuning",
            "text": "However, it is probable that the dataset Dce = {(Qi, Pi, yi)}i, yi \u2208 {0, 1} contains noisy labels, which can negatively impact the model\u2019s performance (refer to Figure.4). These labels can come from (1) low-quality queries generated by the query\ngenerator, (2) false positive passages resulting from imprecise reranking of the cross-encoder, and (3) false negative documents drawn from the top-1000 negative pools.\nTo address the issue of potentially noisy labels, we propose a learning approach called denoisefinetuning (Fig.3). In our experiments, this approach has been proven to effectively mitigate the impact of incorrect labels.\nTaking inspiration from common practices in data science, such as N-fold validation and model ensemble, our objective is for the models to learn from diverse data distributions in the initial stages and subsequently mutually correct each other during the later training phase.\nWe initialize two cross-encoder models {CEi}2i=1 and employ linear warm-up as the learning rate scheduler. We train our models for T steps, this scheduler gradually increases the learning rate until it reaches its maximum value at T \u00d7 \u03b1%, where \u03b1 is a hyperparameter within [0, 100], and subsequently decreases it. During the first \u03b1% of training steps, we randomly discard batches with a probability of P to allow the two models to learn from different data distributions.\nDuring the later stage of training, we utilize an adapted version of the co-regularization method proposed by Zhou and Chen (2021). This method is inspired by previous studies (Arpit et al., 2017; Toneva et al., 2018) that highlight the delayed learning curves associated with noisy labels. This approach enables both models to learn from the batch data as well as their combined predictions. In this phase, the models benefit not only from the provided labels but also from the aggregated predic-\ntions. The influence of co-regularization is controlled by the hyperparameters \u03b3. The total loss (Eq. 3) consists of two components: Llabel, which represents the loss based on the provided labels, and Lagg, which reflects the loss derived from the joint predictions.\nL = Llabel + \u03b3 \u00b7 Lagg (3)\nWhen a new batch B is received, the first step is to calculate the task-specific lossLlabel. In Eq.4, y\u0302k represents the batch prediction of the cross-encoder CEk, and y represents the labels from the batch. We use the binary cross-entropy loss (BCE) for loss calculation.\nLlabel = 1\n2 2\u2211 k=1 BCE(y\u0302k, y) (4)\nThe agreement loss Lagg measures the discrepancy between the model\u2019s prediction and the aggregate prediction agg. To compute this, we first obtain the aggregate prediction agg by averaging the batch predictions from the two cross-encoder models. Subsequently, we calculate the loss Lagg using the cross-entropy loss.\nagg = 1\n2 2\u2211 i=1 (CEk(B)) (5)\nLagg = 1\n2 2\u2211 k=1 BCE(y\u0302k, agg) (6)\nOur experiments demonstrate that by incorporating this co-regularization technique, the models can leverage their shared predictions to enhance their learning and improve overall performance."
        },
        {
            "heading": "3.4 Bi-Encoder (Retrieval Model) Adaptation",
            "text": "Knowledge distillation is a widely used method to transfer knowledge from a high-compute model to a smaller and faster model while preserving performance to some extent. Applying this technique to transfer knowledge from a rerank model to a retrieval model is also meaningful.\nIn a previous study, Hofst\u00e4tter et al. (2020) proposed a cross-architecture training approach to transfer knowledge from a cross-encoder (Nogueira and Cho, 2019) to various dense retrieval models. They used the cross-encoder as the teacher model to label the marginM between pairs of (query, relevant passage) and (query, irrelevant passage). By\nAlgorithm 1: Denoise Finetuning Input: Dataset Dce = {(Qi, Pi, yi)}i;\nhyperparameters T, \u03b3, \u03b1; Output: Adapted cross-encoders\nInitialize cross-encoders {CEk}2k=1 for steps\u2190 1 to T do\nSelect a batch B from Dce if step \u2264 a%\u00d7 T then\nfor k \u2190 1 to 2 do With probability 1\u2212 P:\ny\u0302k \u2190 CEk(B) Llabel \u2190 BCE(y\u0302k, y) Update CEk with Llabel\nelse Compute predictions: {y\u0302k \u2190 CEk(B)}2k=1\nCompute label loss: Llabel by Eq. 4. Compute mean prediction agg by\nEq.5. Compute agreement loss Lagg by Eq. 6. Total loss: L \u2190 Llabel + \u03b3 \u00b7 Lagg. Update model parameters with L.\nutilizing the Margin Mean Squared Error (MarginMSE) loss and the margin value, they successfully trained the student model (bi-encoder) to discriminate between relevant and irrelevant passages.\nThe Margin-MSE loss is defined in Eq. 8, where Q denotes the query, P+ denotes the relevant passage, and P\u2212 denotes the irrelevant passage. The term MSE corresponds to the Mean Squared Error loss function (Stein, 1992), while CE and BE represent the output relevant scores of the crossencoder and bi-encoder, respectively, given a query and passage.\nM = CE(Q,P+)\u2212 CE(Q,P\u2212) (7)\nL = MSE(M, BE(Q,P+)\u2212BE(Q,P\u2212)) (8)\nPrevious experiments conducted by Hofst\u00e4tter et al. (2020) have demonstrated the effectiveness of ensembling the teacher\u2019s scores in improving the accuracy of margin estimation. In our study, we calculate the margin by averaging the scores from both the unadapted cross-encoder and the adapted cross-encoder (referred to as Mix). This approach yields more precise labels for the pseudo queries and passages. For a detailed example, please refer\nto Appendix F."
        },
        {
            "heading": "4 Experiment Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Our experiments are performed on three datasets obtained from the BEIR benchmark (Thakur et al., 2021). Details regarding the test sizes and corpus sizes for each dataset are provided in Table 1 and can be found in Appendix B."
        },
        {
            "heading": "4.2 Baselines",
            "text": "In our evaluation, we compare the performance of our adapted cross-encoder and adapted bi-encoder models with previous domain adaptation methods and zero-shot models. Further details about the baselines can be found in Appendix C."
        },
        {
            "heading": "4.3 Hyperparameters",
            "text": "We adapt existing models trained on the MSMARCO (Nguyen et al., 2017) dataset from the source domain to multiple target domains using our domain adaptation method.\nQuery Generation: To generate queries for passages in the target domain, we utilize the T5 query generator 1 with a temperature setting of 1, which has been shown to produce high-quality queries in previous studies (Ma et al., 2021; Wang et al., 2021b).\nCross-Encoder (Rerank Model) Adaptation: We adapt the Mini-LM cross-encoder (Wang et al., 2020b)2 to the target domains. For retrieving hard negatives, we employ BM25 from Elasticsearch and a bi-encoder3 from Sentence-Transformers. From the top-1000 relevant passages retrieved by each retriever, we select one passage as a hard negative. The input passage is created by concatenating the title and body text. The hyperparameter \u03b3 in Eq.3 is set to 1. The pseudo training dataset Dce\n1https://huggingface.co/BeIR/ query-gen-msmarco-t5-large-v1\n2https://huggingface.co/cross-encoder/ ms-marco-MiniLM-L-12-v2\n3https://huggingface.co/sentence-transformers/ msmarco-distilbert-base-v3\nconsists of 300k samples, with 100k relevant labels and 200k irrelevant labels.\nDenoise-Finetuning: We allocate the initial T\u00d7 \u03b1 training steps for the random batch warm-up stage, with \u03b1 set to 0.1. The remaining steps are dedicated to the co-regularization stage. The crossencoder is trained with a batch size of 32 for 2 epochs. The maximum sequence length is set to 300 for all datasets.\nBi-Encoder (Retrieval Model) Adaptation: We use DistilBERT (Sanh et al., 2019) as the pretrained model for the bi-encoder, following the configuration of GPL (Wang et al., 2021b). For negative retrieval, we employ two dense retrieval models, msmarco-distilbert3 and msmarco-MiniLM4, obtained from Sentence-Transformers. Each retriever retrieves 50 negative passages. From these retrieved passages, we randomly select one negative passage and one positive passage for each training query, forming a single training example. The bi-encoder is trained for 140k steps with a batch size of 32. The maximum sequence length is set to 350 for all datasets."
        },
        {
            "heading": "5 Experiment Results and Analyses",
            "text": ""
        },
        {
            "heading": "5.1 Overall Performance",
            "text": "The main results are summarized in Table 2. A comparison of the recall rates for first-stage retrieval is described in Appendix D. We utilize the adapted bi-encoder (BE(w/Ad)) for initial retrieval, retrieving 100 relevant passages per query, and the adapted cross-encoder (CE(w/Ad)) for reranking. This combination yields substantial improvements in NDCG@10 compared to previous domain adaptation methods and zero-shot models. Notably, our adapted bi-encoder outperforms previous query generation-based domain adaptation methods, GPL and QGen, in the first-stage retrieval. We achieve a remarkable 10% performance increase. Note that we do not experiment with BE(Mix) + CE(Mix) setup, since an online system in practice typically comprises one BE and one CE to ensure shorter retrieval time. Introducing Mix as the cross-encoder would double the inference time for reranking, making it impractical for real-world implementation. Therefore, we opted not to explore this particular setup in our experiments."
        },
        {
            "heading": "5.2 Impact of Denoise-finetuning",
            "text": "During the evaluation of the denoise-finetuning approach, we conducted tests at intervals of 1000 training steps. In the denoise setting, we reported the model with the higher score. The results depicted in Figure 4 illustrate the effectiveness of denoise-finetuning in stabilizing model training and mitigating the influence of noisy labels, particularly in situations involving diverse data distributions. We compare the two methods using three different random seeds, resulting in six scores for each test point."
        },
        {
            "heading": "5.3 Impact of Teacher Models in Distillation",
            "text": "In our distillation experiments, we evaluate three teacher models: (1) adapted cross-encoder, (2) unadapted cross-encoder and (3) Mix model. The Mix model is created by averaging the margin values of models (1) and (2), as described in Section 3.4. We\n4https://huggingface.co/sentence-transformers/ msmarco-MiniLM-L-6-v3\nassess the models every 10k steps and report the best evaluation result.\nInterestingly, we observe that while the adapted cross-encoder performs better in reranking, it\u2019s distillation result do not surpass the unadapted crossencoder over all datasets, such as TREC-COVID. After studying the distribution of the margin and some data cases, we found that the unadapted crossencoder tends to assign a smaller absolute margin score, which means that it exhibits a more conser-\nvative distinction between relevant and irrelevant passages. We speculate that the adapted model widens the gap between relevant and irrelevant passages, while the unadapted model maintains a certain level of regularization. Consequently, the adapted and unadapted cross-encoders together complement each other and surpass single models. Therefore, we combine both models as Mix and utilize it as the optimal teacher model for knowledge distillation. For a detailed example, please refer to Appendix F."
        },
        {
            "heading": "5.4 Extreme Environment",
            "text": "Table 4 presents the results of our experiments on a simulated dataset with significant label noise, demonstrating the effectiveness of denoise finetuning. We inverted {1, 5, 10}% of the labels in the pseudo dataset Dce and report the final test scores. Additionally, we conducted experiments with 20% of the labels inverted, testing with different values of the hyperparameter \u03b1. Further detailed analysis can be found in Appendix E."
        },
        {
            "heading": "6 Conclusion",
            "text": "In conclusion, we have presented a novel approach for unsupervised domain adaptation in information retrieval. Our method addresses the limitations of dense retrieval models and aims to enhance performance in out-of-domain scenarios. By simultaneously adapting the rerank model and dense retrieval model, we achieve significant improvements in two-stage retrieval performance.\nExperiment results demonstrate that our domain adaptation approach outperforms previous methods in both one-stage and two-stage retrieval tasks. The fine-tuned rerank model effectively captures domain-specific information, which is then transferred to the dense retrieval model through knowledge distillation.\nOverall, our work contributes to the field of unsupervised domain adaptation in information retrieval and provides a valuable framework for improving retrieval performance across diverse domains without the need for domain-specific labeled data.\nLimitations\nOur proposed method exhibits superior performance compared to other baselines on FiQA (Finance), SciFact (Science), and TREC-COVID (Bio-Medical) datasets by a significant margin. However, it is important to acknowledge that retrieval datasets from different domains may possess highly diverse distributions of passages and queries. Therefore, further experiments across additional domains are required to comprehensively evaluate the general performance of this method."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the National Science and Technology Council, Taiwan, under Grant NSTC 112-2223-E-006-009. We would like to thank the reviewers for their insightful feedback."
        },
        {
            "heading": "A Example of short generated queries",
            "text": "We have noticed that shorter generated queries frequently inquire about the definition or meaning of a word in the format of \"What is xxx,\" which deviates from the distribution found in our testing dataset. See Table 5 for the comparison of a generated query and a query from the corresponding testing dataset."
        },
        {
            "heading": "B Dataset",
            "text": "We evaluate the performance of our method on three datasets from the BEIR benchmark. To ensure a fair evaluation, we utilize the datasets and evaluation code 5 provided by Thakur et al., 2021.\nThe FiQA dataset (Maia et al., 2018) focuses on the financial domain and utilizes StackExchange posts from 2009 to 2017 as its corpus.\nThe TREC-COVID dataset(Voorhees et al., 2021) is based on the CORD-19 (Wang et al., 2020a) challenge and includes recent publications and historical research on COVID-19 and related coronaviruses.\nThe SciFact dataset (Wadden et al., 2020) comprises evidence-containing abstracts that correspond to expert-written scientific claims. Each claim is labeled, and the dataset also includes rationales that provide supporting evidence for the labels.\nWe selected these three datasets for the following reasons:\nAccessibility: All three datasets are publicly available and open for use.\nObjective Evaluation: We employed the BEIR toolkit, utilizing its provided corpus, queries, qrels, and validation methods, ensuring a fair and unbiased evaluation.\n5https://github.com/beir-cellar/beir.git\nDiverse Domains: Our chosen datasets span diverse domains, including finance, scientific abstracts, and COVID-19 research, representing areas of significant relevance."
        },
        {
            "heading": "C Baseline",
            "text": "We compare our approach with previous zero-shot and domain adaptation approaches. For GPL, MLM, Qgen and TSDAE, we utilize the data provided from Wang et al. (2021b). For docT5query, we use the data provided by the Thakur et al. (2021). For BM25, we employ the Anserini toolkit 6 with a pre-built index.\nZero-shot sparse model: The BM25 (Robertson et al., 1995) and docT5query (Nogueira et al., 2019) models represent the sparse approach. The docT5query model utilizes a T5 model (Raffel et al., 2020) to generate queries and append them to the passages, thereby improving the retrieval performance.\nZero-shot dense model: ANCE (Xiong et al., 2021) is a bi-encoder that utilizes an Approximate Nearest Neighbor (ANN) index of the corpus to generate challenging negative examples. During model fine-tuning, the index is updated in parallel to select these instances. On the other hand, ColBERTV2 (Santhanam et al., 2022) computes contextualized embeddings at the token level, which offers better robustness compared to whole-sentencebased approaches.\nPre-training: MLM (Gururangan et al., 2020) and TSDAE (Wang et al., 2021a) enable unsupervised training of the model in the target domain, facilitating subsequent fine-tuning for downstream tasks.\nQuestion generation domain adaptation: QGen (Ma et al., 2021) and GPL (Wang et al., 2021b) leverage generated questions as training data to enhance the dense retrieval model. These methods differ in their labeling approaches, with QGen using binary labels, and GPL utilizing a cross-encoder (Nogueira and Cho, 2019) to assign soft labels to query-passage pairs."
        },
        {
            "heading": "D Recall Performance",
            "text": "In the context of two-stage retrieval, the recall rate of the first retrieval model holds paramount significance. This is because achieving maximum recall of relevant articles is essential to enable the subsequent rerank model for effective sorting. In Table\n6https://github.com/castorini/anserini.git\n6, we conduct a comparison between the adapted bi-encoder and other domain adaptation methods in terms of Recall@100.\nDue to the fact that the TREC-COVID dataset contains over 100 instances with relevance annotations for each query, utilizing the standard version of recall would result in very low values, as the number of relevant passages always exceeds the parameter \u2019k\u2019. Hence, we adopt the capped recall@k (Eq.10) as a replacement measure, the same as BEIR Benchmark (Thakur et al., 2021).\nRecall@k = 1\nN N\u2211 i=1 TPi GTi\n(9)\nCapped Recall@k = 1\nN N\u2211 i=1 TPi min(k,GTi) (10)\nwhere:\nN : Total number of queries.\nTPi : Number of true positives for retrieval\ninstance i.\nGTi : Number of ground truth positive items\nfor retrieval instance i.\nk : Cutoff value for recall calculation."
        },
        {
            "heading": "E Extremely Noisy Setting",
            "text": "In our experiments, we intentionally create noisy labels by inverting 20% of the labels in the pseudo dataset Dce. This allowed us to simulate an extremely noisy dataset and evaluate the performance\nof the denoise-finetuning method. We conducted tests every 1000 training steps to track the model\u2019s fine-tuning progress. The results presented in Figure 5 and Figure 6 demonstrate the impact of the hyperparameter \u03b3 (defined in Eq. 3). Table 7 shows the last checkpoint rerank performance. These findings highlight the importance of carefully tuning this parameter to achieve optimal performance and minimize the influence of label noise."
        },
        {
            "heading": "F Margin from different teacher models",
            "text": "In Table 8 and Figure 7, different models exhibit variations in margin predictions for positive and negative samples across different datasets. Hence, we can leverage these differences to complement each other\u2019s shortcomings."
        }
    ],
    "title": "Breaking Boundaries in Retrieval Systems: Unsupervised Domain Adaptation with Denoise-Finetuning",
    "year": 2023
}