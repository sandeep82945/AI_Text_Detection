{
    "abstractText": "In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings. Code is available at https://github. com/Chord-Chen-30/UIMER.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhuo Chen"
        },
        {
            "affiliations": [],
            "name": "Chengyue Jiang"
        },
        {
            "affiliations": [],
            "name": "Kewei Tu"
        }
    ],
    "id": "SP:175bff29f37f3786f4e4feb482efdf35f3f3bda5",
    "references": [
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "EMNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Aaron Chan",
                "Maziar Sanjabi",
                "Lambert Mathias",
                "Liang Tan",
                "Shaoliang Nie",
                "Xiaochang Peng",
                "Xiang Ren",
                "Hamed Firooz."
            ],
            "title": "Unirex: A unified learning framework for language model rationale extraction",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Hanjie Chen",
                "Yangfeng Ji."
            ],
            "title": "Learning variational word masks to improve the interpretability of neural text classifiers",
            "venue": "Proceedings of the 2020",
            "year": 2020
        },
        {
            "authors": [
                "Qian Chen",
                "Zhu Zhuo",
                "Wen Wang."
            ],
            "title": "Bert for joint intent classification and slot filling",
            "venue": "arXiv preprint arXiv:1902.10909.",
            "year": 2019
        },
        {
            "authors": [
                "Alice Coucke",
                "Alaa Saade",
                "Adrien Ball",
                "Th\u00e9odore Bluche",
                "Alexandre Caulier",
                "David Leroy",
                "Cl\u00e9ment Doumouro",
                "Thibault Gisselbrecht",
                "Francesco Caltagirone",
                "Thibaut Lavril"
            ],
            "title": "Snips voice platform: an embedded spoken language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Nicola De Cao",
                "Michael Sejr Schlichtkrull",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "How do decisions emerge across layers in neural models? interpretation with differentiable masking",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Mengnan Du",
                "Ninghao Liu",
                "Fan Yang",
                "Xia Hu."
            ],
            "title": "Learning credible deep neural networks with rationale regularization",
            "venue": "2019 IEEE International Conference on Data Mining (ICDM), pages 150\u2013159. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Reza Ghaeini",
                "Xiaoli Fern",
                "Hamed Shahbazi",
                "Prasad Tadepalli."
            ],
            "title": "Saliency learning: Teaching the model where to pay attention",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "Reza Ghaeini",
                "Xiaoli Fern",
                "Prasad Tadepalli."
            ],
            "title": "Interpreting recurrent and attention-based neural models: a case study on natural language inference",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Quzhe Huang",
                "Shengqi Zhu",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Exploring distantly-labeled rationales in neural network models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Sarthak Jain",
                "Byron C Wallace."
            ],
            "title": "Attention is not explanation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
            "year": 2019
        },
        {
            "authors": [
                "Chengyue Jiang",
                "Zijian Jin",
                "Kewei Tu."
            ],
            "title": "Neuralizing regular expressions for slot filling",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9481\u2013 9498.",
            "year": 2021
        },
        {
            "authors": [
                "Chengyue Jiang",
                "Zijian Jin",
                "Kewei Tu."
            ],
            "title": "Neuralizing regular expressions for slot filling",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Siwon Kim",
                "Jihun Yi",
                "Eunji Kim",
                "Sungroh Yoon."
            ],
            "title": "Interpretation of nlp models through input marginalization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3154\u20133167.",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Dan Jurafsky."
            ],
            "title": "Understanding neural networks through representation erasure",
            "venue": "arXiv preprint arXiv:1612.08220.",
            "year": 2016
        },
        {
            "authors": [
                "Frederick Liu",
                "Besim Avci."
            ],
            "title": "Incorporating priors with feature attribution on text classification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6274\u2013 6283.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Vinodkumar Prabhakaran",
                "Ben Hutchinson",
                "Margaret Mitchell."
            ],
            "title": "Perturbation sensitivity analysis to detect unintended model biases",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Slavin Ross",
                "Michael C Hughes",
                "Finale Doshi-Velez."
            ],
            "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
            "venue": "arXiv preprint arXiv:1703.03717.",
            "year": 2017
        },
        {
            "authors": [
                "Sofia Serrano",
                "Noah A Smith"
            ],
            "title": "Is attention interpretable",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Smilkov",
                "Nikhil Thorat",
                "Been Kim",
                "Fernanda Vi\u00e9gas",
                "Martin Wattenberg."
            ],
            "title": "Smoothgrad: removing noise by adding noise",
            "venue": "arXiv preprint arXiv:1706.03825.",
            "year": 2017
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "International conference on machine learning, pages 3319\u2013 3328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Shikhar Vashishth",
                "Shyam Upadhyay",
                "Gaurav Singh Tomar",
                "Manaal Faruqui."
            ],
            "title": "Attention interpretability across nlp tasks",
            "venue": "arXiv preprint arXiv:1909.11218.",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Vig",
                "Yonatan Belinkov."
            ],
            "title": "Analyzing the structure of attention in a transformer language model",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63\u201376.",
            "year": 2019
        },
        {
            "authors": [
                "Yequan Wang",
                "Minlie Huang",
                "Xiaoyan Zhu",
                "Li Zhao."
            ],
            "title": "Attention-based lstm for aspect-level sentiment classification",
            "venue": "Proceedings of the 2016 conference on empirical methods in natural language processing, pages 606\u2013615.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Deep neural networks have been extensively used to solve Natural Language Processing (NLP) tasks and reach state-of-the-art performance. Due to the black-box nature of neural models, there are a lot of studies on how to interpret model decisions by giving attribution scores to input tokens, i.e., how much tokens in an input contribute to the final prediction. We can roughly group these interpretation methods into four categories, namely gradientbased (Ross et al., 2017; Smilkov et al., 2017), attention-based (Vashishth et al., 2019; Serrano and Smith, 2019), erasure/replace-based (Prabhakaran et al., 2019; Kim et al., 2020) and extractor-based (De Cao et al., 2020; Chan et al., 2022) methods.\n\u2217*Corresponding author.\nIn some scenarios, we have access to gold rationales (input tokens critical for predicting correct outputs) during training, or have simple and fast approaches to obtaining gold rationales. In that case, it is intuitively appealing to additionally train the model such that its most attributed tokens match gold rationales (see an example in Fig. 2). In other words, when equipped with interpretation methods, we can train the model on where to look at in the input, in addition to the standard output-matching objective. This can be seen as injecting external knowledge embodied in rationales into the model and is especially beneficial to low-resource scenarios with few training data. This intuitive idea, however, has not been fully explored. There are only a few previous studies based on the gradientbased category of interpretation methods (Huang et al., 2021; Ghaeini et al., 2019; Liu and Avci, 2019) and they neither compare the utilization of different interpretation methods on model enhancement nor experiment on a comprehensive range of\ntasks. In this paper, we first propose a framework named UIMER that Utilizes Interpretation Methods and gold Rationales to improve model performance, as illustrated in Fig. 1. Specifically, in addition to a task-specific loss, we add a new loss that aligns interpretations derived from interpretation methods with gold rationales. We also discuss how the optimization of the task-specific loss and the new loss should be coordinated. For previous methods utilizing gradient-based interpretation for model enhancement (Huang et al., 2021; Ghaeini et al., 2019), we show that they can be seen as instances of our framework.\nWe then propose two novel instances of our framework based on erasure/replace-based and extractor-based interpretation methods respectively. Specifically, in the first instance, we utilize Input Marginalization (Kim et al., 2020) as the interpretation method in our framework, which computes attribution scores by replacing tokens with a variety of strategies and measuring the impact on outputs, and we design a contrastive loss over the computed attribution scores of rationales and non-rationales. In the second instance, we utilize Rationale Extractor (De Cao et al., 2020) as the interpretation method in our framework, which is a neural model that is independent of the task model and trained with its own loss. We again design a contrastive loss over the attribution scores computed by the extractor and in addition, design a new training process that alternately optimizes the task model (using the task-specific loss and our contrastive loss) and the extractor (using its own loss).\nIn summary, our main contributions can be summarized as follows: (1) We propose a framework that can utilize various interpretation methods to enhance models. (2) We utilize two novel types of interpretation methods to enhance the model under our framework. (3) We comprehensively evaluate our framework on diversified tasks including classification, slot filling and natural language inference. Experiments show that our framework is effective in enhancing models with various interpretation methods, especially in the low-resource setting."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Interpretation Methods",
            "text": "Interpretation methods aim to decipher the blackbox of deep neural networks and have been wellstudied recently. Our framework aims to utilize\nthese various interpretation methods for model enhancement. In these interpretation methods, attribution scores are calculated to indicate the importance of input tokens. According to different calculations of attribution scores, we can generally group interpretation methods into the following four categories.\nGradient-based methods Gradient-based interpretation methods are quite popular and intuitive. Li et al. (2016) calculates the absolute value of the gradient w.r.t each input token and interprets it as the sensitiveness of the final decision to the input. Following that, Ghaeini et al. (2019) extends the calculation of gradient to intermediate layers of deep models. Sundararajan et al. (2017) proposes a better method that calculates Integrated Gradients as explanations of inputs.\nAttention-based methods The attention mechanism calculates a distribution over input tokens, and some previous works use the attention weights as interpretations derived from the model (Wang et al., 2016; Ghaeini et al., 2018). However, there is no consensus as to whether attention is interpretable. Jain and Wallace (2019) alters attention weights and finds no significant impact on predictions, while Vig and Belinkov (2019) finds that attention aligns most strongly with dependency relations in the middle layers of GPT-2 and thus is interpretable.\nErasure/replace-based methods The general idea is quite straightforward: erase or replace some words in a sentence and see how the model\u2019s prediction changes. Li et al. (2016) proposes a method to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. Kim et al. (2020) gives a new interpretation method without suffering from the out-of-distribution (OOD) problem by replacing words in inputs and it reaches better interpretability compared with traditional erasure-based methods.\nExtractor-based methods An extractor-based method typically uses an extra model to extract words that the task model pays attention to. De Cao et al. (2020) introduces \u201cDifferentiable Masking\u201d which learns to mask out subsets of the inputs while maintaining differentiability. This decision is made with a simple model based on intermediate hid-\nden layers and word embedding layers of a trained model. Chen and Ji (2020) proposes the variational word mask method to learn to restrict the information of globally irrelevant or noisy word level features flowing to subsequent network layers."
        },
        {
            "heading": "2.2 Utilizing Interpretation Methods to Enhance Models",
            "text": "Some previous work studies whether interpretation methods can be utilized to enhance models. Du et al. (2019) is closely related to one of our contributions (i.e., UIMER-IM in Sec. 3.2). However, their model suffers from the OOD problem and does not consistently outperform baselines in their experiments. Ghaeini et al. (2019) designs a simple extra objective that encourages the gradient of input to have a positive effect on ground truth. They find it useful in event-extraction and Cloze-Style question-answering tasks. To handle Non-Important Rationales and exploit Potential Important Non-rationales, Huang et al. (2021) designs their method following the idea that rationales should get more focus than non-rationales and only part of the rationales should attain higher focus. They focus on text classification tasks and find that, despite the rationale annotations being insufficient and indiscriminative, their method can bring improvements. However, these previous studies neither compare the utilization of various interpretation methods on model enhancement nor conduct experiments on a comprehensive range of tasks."
        },
        {
            "heading": "3 Method",
            "text": "We propose a framework UIMER that utilizes an interpretation method to enhance models based on gold rationales on training data.\nSetup Consider a training example with input x and gold output y. We also have access to gold rationales g of input x indicating the subset of tokens that are critical for predicting the correct output y. The gold rationales g can be annotated by humans or generated automatically from external knowledge sources.\nIn our setup, g is encoded as a 0/1 vector with gi = 1 indicating that xi is a gold rationale and gi = 0 otherwise. Our method, however, can be easily extended to handle g encoded with real numbers indicating the importance of a token.\nGiven a model tasked with predicting output y from input x, an interpretation method produces\nattribution scores a for input x. a can be defined on different levels of granularity. In common cases, ai and xi are one-to-one and ai is defined by a measure calculated by the interpretation method based on the model indicating the importance of token xi for the model in producing its output. For example, in gradient-based interpretation methods, ai is usually some function of the gradient of xi in the model. A higher magnitude of the gradient implies higher importance of input xi.\nLearning Objective Apart from the original taskspecific learning objective Ltask, our framework introduces an extra learning objective Lint that embodies the idea of aligning attribution scores a with gold rationales g. The overall objective on one example x takes the form of:\nL\u03b8 = Ltask(x, y) + \u03b1Lint(a, g) (1)\nwhere \u03b8 is the model parameter and \u03b1 is a coefficient balancing the two objectives.\nWarm-up Training Lint can be seen as measuring whether the model pays attention to the gold rationale words in the input. We deem that compared to teaching a randomly initialized model focusing more on the task-specific rationale words, teaching a model with task knowledge is more effective and reasonable because it might be better at recognizing task-specific rationales with the help of task knowledge rather than rote memorization. Thus, instead of optimizing L\u03b8 at the very beginning, our framework requires the model to be well or at least halfway trained before training on the objective Lint, and during warm-up training, only the objective of the task is optimized. The empirical results (in Sec. 4.2) also support our intuition and show that warm-up training is effective.\nIn the following subsections, we introduce three instances of our framework. The first utilizes gradient-based interpretation methods and subsumes a few previous studies as special cases. The second and third are new methods proposed by us utilizing erasure/replace-based and extractor-based interpretation methods respectively."
        },
        {
            "heading": "3.1 Utilizing Gradient-Based Methods",
            "text": "As introduced in Sec. 2.2, some previous studies utilize gradient-based (GB) interpretation methods to enhance models. They can be seen as instances of our framework, hence denoted as UIMER-GB.\nIn this type of methods, attribution score a is usually defined by a function f of the gradient of input x:\na = f\n( \u2202J\n\u2202x\n) (2)\nwhere usually J refers to the training objective or the task model\u2019s output.\nIn general, Lint is defined as a constraint on the gradients of gold rationales:\nLint(a, g) = D(a, g) (3)\nwhere D is usually a distance function that calculates the error of how a approaches g.\nIn Ghaeini et al. (2019)\u2019s work, f is a function that takes the sum of the gradients of each input embedding dimension and D is to take the sum of the gradients of rationale words. In Huang et al. (2021)\u2019s work, f is the L1 norm that sums up the absolute value of gradients over the input embedding dimensions and D is designed in various ways to give rationale words higher attribution scores."
        },
        {
            "heading": "3.2 Utilizing Erasure/Replace-Based Methods",
            "text": "We incorporate an erasure/replaced-based interpretation method, \u201cInput Marginalization\u201d (IM) (Kim et al., 2020), into our framework in this subsection and name this instance UIMER-IM. We first define the attribution score produced by IM and then define and introduce how to calculate Lint. Other erasure/replace-based methods can be integrated into our framework in a similar way."
        },
        {
            "heading": "3.2.1 Attribution Score by Input Marginalization",
            "text": "Define p\u03b8(y|x) as the probability of the gold output that the model predicts. To calculate the attribution score of token xi, a new set of sentences S needs to be generated, with the size of S being a hyperparameter. Denote x\u2032u as a new sentence with xi replaced by some other token u. Denote q(x\u2032u) as the probability of replacing xi with u to obtain x\u2032u, which can be determined by different strategies1:\n1. BERT: Replace xi by [MASK] to obtain the masked language model probability of u.\n2. Prior: q(x\u2032u) = count(u)/N , where count(u) is the number of times token u appears in corpus and N is the corpus size.\n1In our experiment, we also include a strategy \u201cMASK\u201d, which means xi is simply replaced by the [MASK] token and q(x\u2032[MASK]) = 1\n3. Uniform: q(x\u2032u) = 1 |V | where |V | is the vo-\ncabulary size.\nWe follow one of the strategies to sample set S based on q(x\u2032u) 2, and define ai as:\nai = log2(odds(p\u03b8(y|x)))\u2212 log2(odds(m)) (4)\nwhere\nm = \u2211 x\u2032u\u2208S q(x\u2032u)p\u03b8(y|x\u2032u)\nodds(p) = p/(1\u2212 p)\nFor inputs with only one gold rationale word, computing and optimizing the attribution score is easy. However, there might be more than one rationale in general and the calculation of the attribution score of each rationale token becomes impractical when the input length and the number of rationales get larger. Therefore, we extend the token attribution score defined in the original IM method to multi-token attribution score which can be more efficiently computed.\nFormally, for input x with more than one rationale word, denote x\u2032R as a new sentence in which all rationale words are replaced, and x\u2032N as a new sentence in which the same number of non-rationale words are replaced.3 The way to generate one x\u2032R is by replacing one rationale word at a time using the strategies mentioned before. We denote the score of replacing x to x\u2032R as q(x \u2032 R), and q(x\u2032R) is calculated as the average of the replacing probabilities of rationale words using a certain replacing strategy4. Similarly, x\u2032N is generated and q(x\u2032N ) can be defined. We repeat this generating process and denote the set of generated x\u2032R(x \u2032 N ) as SR(SN ) with the size of SR(SN ) being a hyperparameter. Then the attribution scores aR for the entire set of rationales and aN for the same number\n2When q(x\u2032u) is determined by BERT strategy, we follow the method IM to construct S. Words with the highest probabilities are selected rather than sampled to replace xi.\n3The same number of non-rationales to be replaced are chosen randomly from the input.\n4Here we deviate from the calculation of Kim et al. (2020) that multiplies the probabilities. If there are many rationale words in a sentence and we take the product of probabilities, the sentence with the most probable words replaced by BERT will have a dominant probability compared with others, which often degenerates the calculation of attribution score.\nof non-rationales are defined as:\naR = log2(odds(p\u03b8(y|x)))\u2212 log2(odds(mR))\naN = log2(odds(p\u03b8(y|x)))\u2212 log2(odds(mN )) mR = \u2211\nx\u2032R\u2208SR q(x\u2032R)p\u03b8(y|x\u2032R)\nmN = \u2211\nx\u2032N\u2208SN q(x\u2032N )p\u03b8(y|x\u2032N )\n3.2.2 Definition of Lint in UIMER-IM For a given input x and gold rationale g, with aR and aN defined, we expect the attribution score of rationale words to be higher than that of nonrationale words. Thus, we design a contrastive margin loss:\nLint(a, g) = max(aN \u2212 aR + \u03f5, 0) (5)\nwhere \u03f5 is a positive hyperparameter controlling the margin. Here a is not defined w.r.t each token, and it refers to the attribution score for multi-tokens. Note that when calculating aN \u2212 aR, the term log2(odds(p\u03b8(y|x))) is canceled out and does not need to be computed. We choose to use the margin loss instead of simply maximizing aR and minimizing aN because in many cases non-rationale words may still provide useful information and we do not want to eliminate their influence on the model."
        },
        {
            "heading": "3.3 Utilizing Extractor-Based Methods",
            "text": "In this section, we incorporate \u201cDiffMask\u201d (DM) (De Cao et al., 2020), an extractor-based interpretation method into our framework and name this instance UIMER-DM. Other extractor-based interpretation methods can be integrated into UIMER in a similar way."
        },
        {
            "heading": "3.3.1 Attribution Score by the Extractor",
            "text": "In DM, the attribution scores a are produced by a simple extractor model Ext\u03d5 parameterized by \u03d5.\na = Ext\u03d5(Enc(x)) (6)\nwhere Enc(x) refers to the encoding of input x produced by an encoder model, and a is composed of real numbers in the range of 0 to 1. Here a is defined one-to-one w.r.t. each token in x. The extractor needs to be trained by its own objective LDM\u03d5 composed of 2 parts:\n1. A term to encourage masking the input (or hidden states) as much as possible.\n2. A term to constrain the changes of task model\u2019s prediction after feeding the masked input (or hidden states).\n3.3.2 Definition of Lint in UIMER-DM With attribution scores defined in DM, we define a contrastive loss Lint as follows: Lint(a, g) = \u2211 i:gi=1 min  ai max j:gj=0 aj \u2212 1, 0 2 (7) Intuitively, we encourage the attribution score of rationale words to be higher than the attribution score of non-rationale words. The loss will be zero as long as the maximum attribution score of nonrationale words is lower than the attribution score of any rationale word."
        },
        {
            "heading": "3.3.3 Training in UIMER-DM",
            "text": "When training UIMER-DM, there are two objectives and two sets of parameters, L\u03b8 (Eq. 1) for model parameter \u03b8 and LDM\u03d5 for extractor parameter \u03d5. Intuitively, the two sets of parameters should not be optimized simultaneously. That is because our framework requires an accurate interpretation model (i.e., the extractor here). If the extractor is trained at the same time with the model, then since the model keeps changing, there is no guarantee that the extractor could keep pace with the model, and hence its interpretation of the model may not match the latest model, breaking the requirement of our framework.\nWe adopt the following training schedule to circumvent the problem. First, we follow the warmup strategy and train the model. After that, we alternate between two steps: (1) optimizing the extractor parameters \u03d5 w.r.t. LDM\u03d5 with the model parameters \u03b8 frozen; (2) optimizing the model parameters \u03b8 w.r.t. L\u03b8 with the extractor parameters \u03d5 frozen. The number of rounds that we alternately optimize \u03d5 and \u03b8 and the number of epochs in each round are hyperparameters."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets To evaluate our framework, we experiment with all the methods introduced in the previous section on three tasks: Intent Classification (IC), Slot Filling (SF) and Natural Language Inference (NLI). The three tasks take the forms of single sentence classification, sequence labeling\nand sentence pair classification respectively. For Intent Classification and Slot Filling, we adopt the SNIPS (Coucke et al., 2018) dataset. For Natural Language Inference, we adopt the e-SNLI (Camburu et al., 2018) dataset. SNPIS is widely used in NLU research (Jiang et al., 2021a; Chen et al., 2019). It is collected from SNIPS personal voice assistant. There are 13084, 700 and 700 samples in the training, development and test sets respectively, and there are 72 slot labels and 7 intent types. e-SNLI is a large dataset extended from the Stanford Natural Language Inference Dataset (Bowman et al., 2015) to include human-annotated natural language explanations of the entailment relations. There are 549367, 9842, and 9824 samples in the training, development and test sets respectively.\nRationales We give examples of rationales and show how they are obtained in Table 1. For the Intent Classification task, we ask one annotator to construct a set of keywords for each intent type based on the training set. This only takes less than 15 minutes. For the Slot Filling task, we use 28 regular expressions with simple patterns which reference Jiang et al. (2021b)5 to match the sentences in the SNIPS dataset and regard matched tokens as rationales. The job takes less than 30 minutes (less than 1 minute each on average). The complete rationale sets for Intent Classification and Slot Filling task are shown in App. 8.1. For e-SNLI, we use the explanations provided by Camburu et al. (2018).\nBaselines For Intent Classification task, we choose BERT-base-uncased6 + Softmax as our base model7. For Slot Filling, we choose BERT-baseuncased + CRF as our base model. For Natural Language Inference, we prepare the data into the form \u201c[CLS] premise [SEP] hypothesis [SEP]\u201d, feed it into BERT-base-uncased, and apply a linear layer to the hidden state of the [CLS] token to score the entailment. These base models are natural base-\n5These regular expressions are designed to extract slots. 6https://huggingface.co/bert-base-uncased 7We implement it based on JointBERT\nlines. We also regard the two previously-proposed gradient-based methods introduced in Sec. 3.1 as stronger baselines.\nTraining Settings We mainly focus on lowresource settings with limited training examples, which is when external resources such as rationales can be most beneficial. With less training data, there is an increasing return of utilizing rationales. For Slot Filling and Intent Classification, we compare our methods and baselines on 1-shot, 3-shot, 10-shot, 30-shot and 100% of training data. For Slot Filling, n-shot means that we sample training examples such that each slot label appears at least n times in the sampled examples. For Natural Language Inference, we compare our framework and baselines with 100 and 500 training samples from e-SNLI. Hyperparameters are tuned on the development set and we report the average result of 4 random seeds on the test set. Detailed data about hyperparameters is shown in App. 8.3."
        },
        {
            "heading": "4.2 Main Results",
            "text": "We present the main results in all the settings in Table 2. First, from the Mean column, we see that gradient-based methods (UIMER-GB Ghaeini et al. (2019); Huang et al. (2021)) reach better performance than the base model, and all variants of our proposed UIMER-IM and UIMER-DM methods outperform both UIMER-GB and base models.\nFor UIMER-IM, its variants achieve the best performance in eight of the twelve settings and the second best in the rest of settings, suggesting its general applicability. The \u201c+Uniform\u201d variant of UIMER-IM can be seen to clearly outperform the other variants in the 1/3-shot settings on Intent Classification and we analyze the potential reason behind this in App. 8.2. UIMER-IM with the \u201cBERT (warm.)\u201d variant brings a 14.86% gain for the NLI 100-example setting.\nFor UIMER-DM, it achieves the best performance in the 1/3-shot settings and is competitive in the 10-shot setting on Slot Filling, which indicates\nthat an extra extractor might be more capable of locating rationales than other interpretation methods for structured prediction problems. Applying multi-round training to UIMER-DM can be seen to clearly improve the performance in almost all the settings of all three tasks. We conduct an ablation study on the effect of multi-round training of UIMER-DM in Sec. 5.2.\nFrom the results, it is also evident that in general, the performance gap between any instance method of our framework and the base model becomes larger with less data, implying the usefulness of rationales when training data cannot provide sufficient training signals for model enhancement."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Warm-up Training",
            "text": "We conduct an ablation study on the effect of warmup training of our method UIMER-IM in the 1-\nshot setting of the Intent Classification task. In particular, we inspect the value of objective Lint during training with and without warm-up training, as shown in Fig. 3. It can be seen that with warmup training, the Lint objective starts with a lower value and converges to zero faster, verifying the benefit of warm-up training."
        },
        {
            "heading": "5.2 Multi-Round Training",
            "text": "In our method UIMER-DM, we propose an asynchronous training process that trains the model and the extractor asynchronously for multiple rounds. We conduct a study on the effectiveness of the multi-round training process on 10-shot Slot Filling and the results are shown in Fig. 4. We observe that by iteratively and alternately training the extractor and the model, performances on both the development and the test sets show an upward trend with more rounds. Note that we use fewer training epochs (around 10 for the extractor and 20 for the\nmodel) in each round of multi-round training than one-pass training. 3-round training does not outperform one-pass training simply because it has few total training epochs."
        },
        {
            "heading": "5.3 Case Study",
            "text": "To show that our framework is able to both enhance the model in task performance and give better interpretations, we conduct a case study on 1-shot Intent Classification setting. From the first two examples in Table 3, we can see that the base model can neither predict the correct intent label of the sentence nor produce good interpretations (the attribution scores of non-rationales are higher than the scores of rationales); in comparison, our UIMERIM fixes both problems. In the last two examples, we show that UIMER-DM succeeds in lowering the attribution scores of irrelevant parts in the input and producing high scores for some or all of the rationales. It can also be seen that the extractor trained on the base model in 1-shot settings views most of the inputs as being important, while the extractor in UIMER-DM is much more parsimonious and precise."
        },
        {
            "heading": "5.4 Relation Between Attribution Score and Performance",
            "text": "In this section, we study how the model performs on the test set when it succeeds or fails to give rationale words higher attribution scores. We conduct experiments on 1-shot Intent Classification and calculate the accuracy while giving rationale words higher or lower attribution scores than non-\nrationales, as shown in the first and second columns in Table 4. For methods with the DM interpretation method, aR is calculated by averaging the attribution scores for all rationale words in x and aN is calculated by averaging the attribution scores for all non-rationale words. We can see that when our UIMER-IM/DM method correctly recognizes rationale words, it reaches higher accuracy than the base model, which suggests that helping models pay more attention to rationales can additionally improve the task performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "Though many interpretation methods are studied for deep neural models, only sporadic works utilize them to enhance models. In this paper, we propose a framework that can utilize various interpretation methods to enhance models. We also propose two novel instances utilizing two other types of interpretation methods for model enhancement. In addition, we discuss how the optimization of the task-specific loss and the new loss should\nbe coordinated. Comprehensive experiments are conducted on a variety of tasks including Intent Classification, Slot Filling and Natural Language Inference. Experiments show that our framework is effective in enhancing models with various interpretation methods especially in the low-resource setting, and our two newly-proposed methods outperform gradient-based methods in most settings. For future work, we plan to extend our framework to utilize more forms of rationales and additional interpretation methods."
        },
        {
            "heading": "7 Limitations",
            "text": "It can be inferred from the result that the two newly introduced methods do not give the best performance in rich-resource settings. We prospect that method UIMER-IM plays a role in incorporating the information of rationales by introducing more similar inputs to the model when the training data is scarce. However, when training data is sufficient enough for the task, the effect of this way to supply information on rationales is reduced. For method UIMER-DM, it also does not perform the best in rich-resource settings. We attribute the ordinary performance of UIMER-DM to that with rich data, most knowledge can be implicitly learned by the model, and injecting gold rationale doesn\u2019t help."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the National Natural Science Foundation of China (61976139)."
        },
        {
            "heading": "8 Appendix",
            "text": ""
        },
        {
            "heading": "8.1 Patterns to match rationales",
            "text": "In our experiments, we adopt simple patterns to match rationales. For Intent Classification, a dictionary is constructed, as shown in Table 5 (upper), mapping intent labels to rationales, and for Slot Filling, a group of Regular Expressions, as shown in Table 5 (below), is used to extract rationales. Both tasks are derived from SNIPS dataset that contains 13084 examples, and obtaining both the dictionary and the Regular Expressions is simple and fast. Please note that though the table below seems a little complex, the gray parts are just the syntax of Regular Expressions. Only the black parts contain rational information."
        },
        {
            "heading": "8.2 Why is \u201c+Uniform\u201d Good",
            "text": "In experimenting with Sec. 3.2, we found that replacing rationales/non-rationales randomly, i.e. the \u201cUniform\u201d strategy often produces a better result even than the \u201cBERT\u201d strategy, despite the fact that compared to replacing tokens randomly, a pretrained BERT can apparently produce more fluent sentences. Here we give an example in Intent Classification task that possibly explains the cause of this fact in Fig. 5.\nAs stated in Sec. 3.2, we aim to minimize Lint defined in Eq. 5 in which we want to lower the latter term in aR, and it is a weighted sum of the predictions of these sentences generated by BERT on the ground truth intent. However, we see that BERT pretty much keeps the original meaning of\nthe sentence in this example. Thus lowering the latter term in aR seems to be a contradiction to the origin task. In contrast, if we look at the sentences generated by replacing the rationale randomly they contain much less information about the ground truth intent, and minimizing Lint seems to be more reasonable."
        },
        {
            "heading": "8.3 Hyperparameters",
            "text": "For Intent Classification and Slot filling task, the learning rate L\u03b8 is tuned among {8e-5, 1e-4, 2e-4, 4e-4} in 1/3/10/30-shot settings and {1e-5, 2e-5} in full training data setting. For Natural Language Inference task, the learning rate of L\u03b8 is tuned among {8e-5, 1e-4, 3e-4}. In all three tasks, \u03b1 is tuned in the range [0.001, 20]. \u03f5 is tuned in the range [0.01, 10]. We use AdamW optimizer (Loshchilov and Hutter, 2018) and \u201clinear schedule with warmup\u201d scheduler. Detailed hyperparameters are shown in Table 6-8."
        },
        {
            "heading": "8.4 Result with std.",
            "text": "We show the result with unbiased estimation of standard deviation in Table 9 in all few-shot settings.\nM od\nel IC\nSF N\nL I\n1 3\n10 30\n1 3\n10 30\n10 0\n50 0\nm ea\nn st\nd m\nea n\nst d\nm ea\nn st\nd m\nea n\nst d\nm ea\nn st\nd m\nea n\nst d\nm ea\nn st\nd m\nea n\nst d\nm ea\nn st\nd m\nea n\nst d\nB as\nel in\ne 65\n.7 1\n7. 30\n79 .1\n8 6.\n17 91\n.0 0\n1. 40\n93 .7\n9 0.\n62 38\n.1 4\n1. 92\n50 .9\n7 1.\n23 67\n.0 5\n0. 58\n81 .7\n0 0.\n12 54\n.0 3\n13 .6\n6 62\n.8 4\n7. 38\nOurFramework\nU IM\nE R\n-G B\nG ha\nei ni\net al\n.( 20\n19 )\n65 .7\n1 7.\n30 79\n.1 4\n6. 12\n91 .8\n2 0.\n18 94\n.1 8\n1. 30\n37 .7\n7 1.\n22 51\n.6 9\n0. 77\n67 .5\n7 0.\n47 82\n.1 6\n0. 57\n67 .1\n3 0.\n86 69\n.7 7\n2. 51\nU IM\nE R\n-G B\nH ua\nng et\nal .(\n20 21\n)\n+ ba\nse 67\n.0 4\n9. 11\n83 .0\n4 4.\n83 91\n.4 3\n1. 14\n94 .5\n7 0.\n20 39\n.0 2\n1. 57\n50 .6\n6 0.\n98 67\n.1 1\n1. 22\n80 .2\n0 2.\n64 66\n.1 5\n1. 37\n68 .5\n7 2. 76 + ga te 67 .1 4 5. 19 82 .0 1 3. 19 91 .3 9 0. 05 94 .0 7 1. 02 37 .8 4 1. 77 51 .6 3 1. 76 67 .3 4 0. 53 80 .8 9 2. 24 66 .0 6 1. 13 68 .3 1 4. 22 + or de r 65 .2 8 4. 71 81 .8 2 4. 35 90 .8 2 1. 21 94 .3 6 1. 38 38 .1 8 1. 64 50 .9 9 1. 46 67 .5 5 0. 68 81 .0 8 1. 25 68 .4 4 3. 50 68 .5 7 2. 76 + (g at e+ or de r) 67 .7 1 6. 24 80 .2 5 5. 57 92 .1 1 1. 46 94 .3 9 1. 07 38 .8 6 1. 83 51 .7 3 1. 89 67 .7 6 0. 76 80 .5 5 1. 30 65 .1 0 7. 62 65 .8 4 4. 22\nU IM\nE R\n-I m\n+ M\nA SK\n69 .8\n5 4.\n24 83\n.1 7\n5. 32\n91 .1\n8 0.\n50 93\n.8 6\n1. 32\n38 .6\n8 2.\n73 52\n.4 7\n1. 52\n69 .2\n7 1.\n21 81\n.6 7\n1. 90\n63 .3\n6 1.\n19 70\n.0 4\n5. 22\n+ B\nE R\nT 70\n.6 1\n2. 52\n83 .9\n3 3.\n69 91\n.7 8\n1. 16\n94 .6\n1 0.\n64 39\n.2 8\n1. 80\n51 .9\n6 1.\n18 69\n.2 2\n1. 92\n81 .8\n5 1.\n02 62\n.0 8\n6. 04\n69 .0\n3 4. 70 + Pr io r 73 .7 1 4. 75 83 .9 3 5. 68 91 .0 0 0. 63 93 .9 6 0. 43 38 .0 7 1. 21 51 .6 9 0. 88 68 .3 1 0. 71 81 .9 7 1. 87 66 .5 6 2. 58 70 .3 2 2. 99 + U ni fo rm 73 .3 2 4. 02 86 .0 4 2. 16 91 .6 4 1. 48 94 .0 0 0. 60 39 .3 1 2. 14 51 .3 7 1. 82 69 .0 2 0. 89 81 .6 9 1. 91 64 .3 4 7. 87 69 .1 9 2. 20\nU IM\nE R\n-I m\n+ M\nA SK\n(w ar\nm .)\n70 .8\n2 2.\n40 82\n.9 6\n5. 11\n92 .4\n3 1.\n59 94\n.0 4\n0. 54\n38 .6\n0 2.\n23 51\n.5 5\n1. 35\n67 .9\n3 0.\n90 82\n.2 5\n1. 38\n68 .7\n9 2.\n44 69\n.9 5\n0. 78\n+ B\nE R\nT (w\nar m\n.) 70\n.9 3\n5. 56\n82 .7\n1 4.\n07 92\n.0 0\n1. 64\n94 .3\n2 0.\n11 39\n.5 3\n1. 73\n52 .8\n3 1.\n18 68\n.8 1\n1. 85\n82 .5\n8 0.\n96 68\n.8 9\n0. 88\n69 .1\n8 1. 51 + Pr io r( w ar m .) 73 .8 2 5. 97 83 .1 1 2. 19 91 .9 3 1. 08 94 .0 7 0. 74 38 .2 4 2. 27 51 .6 8 1. 03 68 .2 6 1. 38 82 .6 6 1. 32 68 .2 5 3. 35 71 .8 2 3. 57 + U ni fo rm (w ar m ) 75 .7 9 4. 94 86 .2 9 1. 51 91 .6 7 1. 21 94 .3 2 0. 80 38 .7 1 2. 11 52 .1 8 0. 71 68 .2 1 0. 46 82 .1 7 1. 54 67 .5 8 1. 77 69 .7 9 1. 30\nU IM\nE R\n-D m\nO ne\n-p as\ns 66\n.7 5\n7. 54\n84 .4\n2 5.\n42 91\n.5 3\n0. 58\n93 .7\n8 0.\n62 39\n.8 6\n1. 53\n52 .8\n7 2.\n25 67\n.2 8\n0. 71\n81 .9\n0 1.\n40 63\n.0 3\n4. 92\n66 .9\n1 4. 77 M ul tiro un d 70 .2 1 8. 57 85 .8 6 4. 40 91 .9 2 0. 92 94 .0 0 0. 60 41 .3 2 1. 17 53 .1 0 0. 81 69 .2 6 0. 53 82 .0 0 1. 54 65 .4 4 2. 94 67 .6 0 8. 59\nTa bl\ne 9:\nR es\nul tw\nith st\nd. on\nal lf\new -s\nho ts\net tin\ngs ."
        }
    ],
    "title": "Using Interpretation Methods for Model Enhancement",
    "year": 2023
}