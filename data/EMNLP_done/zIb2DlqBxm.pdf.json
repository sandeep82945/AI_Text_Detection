{
    "abstractText": "The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model\u2019s noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its utility in this domain.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nadav Borenstein"
        },
        {
            "affiliations": [],
            "name": "Phillip Rust"
        },
        {
            "affiliations": [],
            "name": "Desmond Elliott"
        },
        {
            "affiliations": [],
            "name": "Isabelle Augenstein"
        }
    ],
    "id": "SP:5db01309c6c35043c6b512478a03b4ee89c7a1e4",
    "references": [
        {
            "authors": [
                "Blouin Baptiste",
                "Benoit Favre",
                "Jeremy Auguste",
                "Christian Henriot"
            ],
            "title": "Transferring modern named entity recognition to the historical domain: How to take the step? In Workshop on Natural Language Processing for Digital Humanities (NLP4DH)",
            "year": 2021
        },
        {
            "authors": [
                "Marcel Bollmann."
            ],
            "title": "A large-scale comparison of historical text normalization systems",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and",
            "year": 2019
        },
        {
            "authors": [
                "Marcel Bollmann",
                "Anders S\u00f8gaard",
                "Joachim Bingel."
            ],
            "title": "Multi-task learning for historical text normalization: Size matters",
            "venue": "Proceedings of the Workshop on Deep Learning Approaches for LowResource NLP, pages 19\u201324.",
            "year": 2018
        },
        {
            "authors": [
                "Nadav Borenstein",
                "Natalia da Silva Perez",
                "Isabelle Augenstein."
            ],
            "title": "Multilingual event extraction from historical newspaper adverts",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, Toronto, Canada. Association",
            "year": 2023
        },
        {
            "authors": [
                "Nadav Borenstein",
                "Karolina Sta\u0144czak",
                "Thea Rolskov",
                "Natacha Klein K\u00e4fer",
                "Natalia da Silva Perez",
                "Isabelle Augenstein."
            ],
            "title": "Measuring intersectional biases in historical documents",
            "venue": "Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Abhishek Das",
                "Harsh Agrawal",
                "Larry Zitnick",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding, 163:90\u2013100",
            "venue": "Language",
            "year": 2017
        },
        {
            "authors": [
                "Brian Davis",
                "Bryan Morse",
                "Brian Price",
                "Chris Tensmeyer",
                "Curtis Wigington",
                "Vlad Morariu."
            ],
            "title": "End-to-end document recognition and understanding with dessurt",
            "venue": "Computer Vision \u2013 ECCV 2022 Workshops: Tel Aviv, Israel, October 23\u201327, 2022,",
            "year": 2023
        },
        {
            "authors": [
                "Francesco De Toni",
                "Christopher Akiki",
                "Javier De La Rosa",
                "Cl\u00e9mentine Fourrier",
                "Enrique Manjavacas",
                "Stefan Schweter",
                "Daniel Van Strien."
            ],
            "title": "Entities, dates, and languages: Zero-shot on historical texts with t0",
            "venue": "Proceedings of BigScience",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Delteil",
                "Edouard Belval",
                "Lei Chen",
                "Luis Goncalves",
                "Vijay Mahadevan."
            ],
            "title": "MATrIX \u2013 Modality-Aware Transformer for Information eXtraction",
            "venue": "arXiv e-prints, page arXiv:2205.08094.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Senka Drobac",
                "Pekka Kauppinen",
                "Krister Lind\u00e9n."
            ],
            "title": "Ocr and post-correction of historical finnish texts",
            "venue": "Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 70\u201376.",
            "year": 2017
        },
        {
            "authors": [
                "Anne Gerritsen."
            ],
            "title": "Scales of a local: the place of locality in a globalizing world",
            "venue": "A Companion to World History, pages 213\u2013226.",
            "year": 2012
        },
        {
            "authors": [
                "Michiel van Groesen."
            ],
            "title": "Digital gatekeeper of the past: Delpher and the emergence of the press in the dutch golden age",
            "venue": "Tijdschrift voor Tijdschriftstudies, 38:9\u201319.",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick."
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16000\u201316009.",
            "year": 2022
        },
        {
            "authors": [
                "Mark J Hill",
                "Simon Hengchen."
            ],
            "title": "Quantifying the impact of dirty ocr on historical text analysis: Eighteenth century collections online as a case study",
            "venue": "Digital Scholarship in the Humanities, 34(4):825\u2013 843.",
            "year": 2019
        },
        {
            "authors": [
                "Geewook Kim",
                "Teakgyu Hong",
                "Moonbin Yim",
                "JeongYeon Nam",
                "Jinyoung Park",
                "Jinyeong Yim",
                "Wonseok Hwang",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seunghyun Park."
            ],
            "title": "Ocr-free document understanding transformer",
            "venue": "Computer Vision \u2013 ECCV 2022:",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Viet Lai",
                "Minh Van Nguyen",
                "Heidi Kaufman",
                "Thien Huu Nguyen."
            ],
            "title": "Event extraction from historical texts: A new dataset for black rebellions",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2390\u20132400.",
            "year": 2021
        },
        {
            "authors": [
                "Julia Laite."
            ],
            "title": "The emmet\u2019s inch: Small history in a digital age",
            "venue": "Journal of Social History, 53(4):963\u2013 989.",
            "year": 2020
        },
        {
            "authors": [
                "Kenton Lee",
                "Mandar Joshi",
                "Iulia Turc",
                "Hexiang Hu",
                "Fangyu Liu",
                "Julian Eisenschlos",
                "Urvashi Khandelwal",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Junlong Li",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Cha Zhang",
                "Furu Wei."
            ],
            "title": "Dit: Self-supervised pre-training for document image transformer",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, MM \u201922, page 3530\u20133539, New York,",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Li",
                "Tengchao Lv",
                "Jingye Chen",
                "Lei Cui",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Zhoujun Li",
                "Furu Wei."
            ],
            "title": "Trocr: Transformer-based optical character recognition with pre-trained models",
            "venue": "arXiv preprint arXiv:2109.10282.",
            "year": 2021
        },
        {
            "authors": [
                "Peizhao Li",
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad I Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Varun Manjunatha",
                "Hongfu Liu."
            ],
            "title": "Selfdoc: Self-supervised document representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Lijun Lyu",
                "Maria Koutraki",
                "Martin Krickl",
                "Besnik Fetahu."
            ],
            "title": "Neural ocr post-hoc correction of historical corpora",
            "venue": "Transactions of the Association for Computational Linguistics, 9:479\u2013493.",
            "year": 2021
        },
        {
            "authors": [
                "Enrique Manjavacas",
                "Lauren Fonteyn."
            ],
            "title": "Adapting vs",
            "venue": "Pre-training Language Models for Historical Languages. Journal of Data Mining & Digital Humanities, NLP4DH.",
            "year": 2022
        },
        {
            "authors": [
                "Janalyn Moss."
            ],
            "title": "Guides: News and newspapers: Historical newspaper collections",
            "venue": "Iowa\u2019s University Libraries.",
            "year": 2009
        },
        {
            "authors": [
                "Simon P. Newman",
                "Stephen Mullen",
                "Nelson Mundell",
                "Roslyn Chapman."
            ],
            "title": "Runaway Slaves in Britain: bondage, freedom and race in the eighteenth century",
            "venue": "https://www.runaways.gla.ac. uk. Accessed: 2022-12-10.",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
            "venue": "arXiv e-prints, page arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Robertson",
                "Sharon Goldwater"
            ],
            "title": "Evaluating historical text normalization systems: How well do they generalize",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Phillip Rust",
                "Jonas F Lotz",
                "Emanuele Bugliarello",
                "Elizabeth Salesky",
                "Miryam de Lhoneux",
                "Desmond Elliott."
            ],
            "title": "Language modelling with pixels",
            "venue": "International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Phillip Rust",
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Iryna Gurevych."
            ],
            "title": "How good is your tokenizer? on the monolingual performance of multilingual language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Ray Smith."
            ],
            "title": "tesseract: Open source ocr engine",
            "venue": "https://github.com/tesseract-ocr/t esseract.",
            "year": 2023
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research, 9(86):2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "The IEEE International Con-",
            "year": 2015
        },
        {
            "authors": [
                "Loshchilov",
                "Hutter"
            ],
            "title": "2017) with a linear warm",
            "year": 2017
        },
        {
            "authors": [
                "Rust"
            ],
            "title": "2023), with one main difference: the noisy OCR output prevents us from separating the first and second sentence in sentencelevel tasks. Therefore we treat each sentence pair as a single sequence and leave it for the models to",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent years have seen a boom in efforts to digitise historical documents in numerous languages and sources (Chadwyck, 1998; Groesen, 2015; Moss, 2009), leading to a transformation in the way historians work. Researchers are now able to expedite the analysis process of vast historical corpora using NLP tools, thereby enabling them to focus on interpretation instead of the arduous task of evidence collection (Laite, 2020; Gerritsen, 2012).\nThe primary step in most NLP tools tailored for historical analysis involves Optical Character Recognition (OCR). However, this approach poses several challenges and drawbacks. First, OCR\n*This paper shows dataset samples that are racist in nature\nstrips away any valuable contextual meaning embedded within non-textual elements, such as page layout, fonts, and figures.1 Moreover, historical documents present numerous challenges to OCR systems. This can range from deteriorated pages, archaic fonts and language, the presence of nontextual elements, and occasional deficiencies in scan quality (e.g., blurriness), all of which contribute to the introduction of additional noise. Consequently, the extracted text is often riddled with errors at the character level (Robertson and Goldwater, 2018; Bollmann, 2019), which most large language models (LLMs) are not tuned to process. Token-based LLMs are especially sensitive to this, as the discrete structure of their input space cannot handle well the abundance of out-of-vocabulary words that characterise OCRed historical documents (Rust et al., 2023). Therefore, while LLMs have proven remarkably successful in modern domains, their performance is considerably weaker when applied to historical texts (Manjavacas and Fonteyn, 2022; Baptiste et al., 2021, inter alia). Finally, for many languages, OCR systems either do not exist or perform particularly poorly. As training new OCR models is laborious and expensive (Li et al., 2021a), the application of NLP tools to historical documents in these languages is limited.\nThis work addresses these limitations by taking advantage of recent advancements in pixel-based language modelling, with the goal of constructing a general-purpose, image-based and OCR-free language encoder of historical documents. Specifically, we adapt PIXEL (Rust et al., 2023), a language model that renders text as images and is trained to reconstruct masked patches instead of predicting a distribution over tokens. PIXEL\u2019s training methodology is highly suitable for the historical domain, as (unlike other pixel-based language models) it does not rely on a pretraining dataset\n1Consider, for example, the visual data that is lost by processing the newspaper page in Fig 18 in App C as text.\ncomposed of instances where the image and text are aligned. Fig 1 visualises our proposed training approach.\nGiven the paucity of large, high-quality datasets comprising historical scans, we pretrain our model using a combination of 1) synthetic scans designed to resemble historical documents faithfully, produced using a novel method we propose for synthetic scan generation; and 2) real historical English newspapers published in the Caribbeans in the 18th and 19th centuries. The resulting pixelbased language encoder, PHD (Pixel-based model for Historical Documents), is subsequently evaluated based on its comprehension of natural language and its effectiveness in performing Question Answering from historical documents.\nWe discover that PHD displays impressive reconstruction capabilities, being able to correctly predict both the form and content of masked patches of historical newspapers (\u00a74.4). We also note the challenges concerning quantitatively evaluating these predictions. We provide evidence of our model\u2019s noteworthy language understanding capabilities while exhibiting an impressive resilience to noise. Finally, we demonstrate the usefulness of the model when applied to the historical QA task (\u00a75.4).\nTo facilitate future research, we provide the dataset, models, and code at https://gith ub.com/nadavborenstein/pixel-bw."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 NLP for Historical Texts",
            "text": "Considerable efforts have been invested in improving both OCR accuracy (Li et al., 2021a; Smith, 2023) and text normalisation techniques for historical documents (Drobac et al., 2017; Robertson and Goldwater, 2018; Bollmann et al., 2018; Boll-\nmann, 2019; Lyu et al., 2021). This has been done with the aim of aligning historical texts with their modern counterparts. However, these methods are not without flaws (Robertson and Goldwater, 2018; Bollmann, 2019), and any errors introduced during these preprocessing stages can propagate to downstream tasks (Robertson and Goldwater, 2018; Hill and Hengchen, 2019). As a result, historical texts remain a persistently challenging domain for NLP research (Lai et al., 2021; De Toni et al., 2022; Borenstein et al., 2023b). Here, we propose a novel approach to overcome the challenges associated with OCR in historical material, by employing an image-based language model capable of directly processing historical document scans and effectively bypassing the OCR stage."
        },
        {
            "heading": "2.2 Pixel-based Models for NLU",
            "text": "Extensive research has been conducted on models for processing text embedded in images. Most existing approaches incorporate OCR systems as an integral part of their inference pipeline (Appalaraju et al., 2021; Li et al., 2021b; Delteil et al., 2022). These approaches employ multimodal architectures where the input consists of both the image and the output generated by an OCR system.\nRecent years have also witnessed the emergence of OCR-free approaches for pixel-based language understanding. Kim et al. (2022) introduce Donut, an image-encoder-text-decoder model for document comprehension. Donut is pretrained with the objective of extracting text from scans, a task they refer to as \u201cpseudo-OCR\u201d. Subsequently, it is finetuned on various text generation tasks, reminiscent of T5 (Roberts et al., 2020). While architecturally similar to Donut, Dessurt (Davis et al., 2023) and Pix2Struct (Lee et al., 2022) were pretrained by masking image regions and predicting the text in\nboth masked and unmasked image regions. Unlike our method, all above-mentioned models predict in the text space rather than the pixel space. This presupposes access to a pretraining dataset comprised of instances where the image and text are aligned. However, this assumption cannot hold for historical NLP since OCR-independent ground truth text for historical scans is, in many times, unprocurable and cannot be used for training purposes.\nText-free models that operate at the pixel level for language understanding are relatively uncommon. One notable exception is Li et al. (2022), which utilises Masked Image Modeling for pretraining on document patches. Nevertheless, their focus lies primarily on tasks that do not necessitate robust language understanding, such as table detection, document classification, and layout analysis. PIXEL (Rust et al., 2023), conversely, is a text-free pixel-based language model that exhibits strong language understanding capabilities, making it the ideal choice for our research. The subsequent section will delve into a more detailed discussion of PIXEL and how we adapt it to our task."
        },
        {
            "heading": "3 Model",
            "text": "PIXEL We base PHD on PIXEL, a pretrained pixel-based encoder of language. PIXEL has three main components: A text renderer that draws texts as images, a pixel-based encoder, and a pixel-based decoder. The training of PIXEL is analogous to BERT (Devlin et al., 2019). During pretraining, input strings are rendered as images, and the encoder and the decoder are trained jointly to reconstruct randomly masked image regions from the unmasked context. During finetuning, the decoder is replaced with a suitable classification head, and no masking is performed. The encoder and decoder are based on the ViT-MAE architecture (He et al., 2022) and work at the patch level. That is, the encoder breaks the input image into patches of 16 \u00d7 16 pixels and outputs an embedding for each patch. The decoder then decodes these patch embeddings back into pixels. Therefore, random masking is performed at the patch level as well.\nPHD We follow the same approach as PIXEL\u2019s pretraining and finetuning schemes. However, PIXEL\u2019s intended use is to process texts, not natural images. That is, the expected input to PIXEL is a string, not an image file. In contrast, we aim to use the model to encode real document scans. Therefore, we make several adaptations to PIXEL\u2019s\ntraining and data processing procedures to make it compatible with our use case (\u00a74 and \u00a75).\nMost crucially, we alter the dimensions of the model\u2019s input: The text renderer of PIXEL renders strings as a long and narrow image with a resolution of 16 \u00d7 8464 pixels (corresponding to 1 \u00d7 529 patches), such that the resulting image resembles a ribbon with text. Each input character is set to be not taller than 16 pixels and occupies roughly one patch. However, real document scans cannot be represented this way, as they have a natural twodimensional structure and irregular fonts, as Fig 1a demonstrates (and compare to Fig 17a in App C). Therefore, we set the input size of PHD to be 368 \u00d7 368 pixels (or 23 \u00d7 23 patches)."
        },
        {
            "heading": "4 Training a Pixel-Based Historical LM",
            "text": "We design PHD to serve as a general-purpose, pixel-based language encoder of historical documents. Ideally, PHD should be pretrained on a large dataset of scanned documents from various historical periods and different locations. However, large, high-quality datasets of historical scans are not easily obtainable. Therefore, we propose a novel method for generating historical-looking artificial data from modern corpora (see subsection 4.1). We adapt our model to the historical domain by continuously pretraining it on a medium-sized corpus of real historical documents. Below, we describe the datasets and the pretraining process of the model."
        },
        {
            "heading": "4.1 Artificially Generated Pretraining Data",
            "text": "Our pretraining dataset consists of artificially generated scans of texts from the same sources that BERT used, namely the BookCorpus (Zhu et al., 2015) and the English Wikipedia.2 We generate the scans as follows.\nWe generate dataset samples on-the-fly, adopting a similar approach as Davis et al. (2023). First,\n2We use the version \u201c20220301.en\u201d hosted on huggingf ace.co/datasets/wikipedia.\nwe split the text corpora into paragraphs, using the new-line character as a delimiter. From a paragraph chosen at random, we pick a random spot and keep the text spanning from that spot to the paragraph\u2019s end. We also sample a random font and font size from a pre-defined list of fonts (from Davis et al. (2023)). The text span and the font are then embedded within an HTML template using the Python package Jinja,3 set to generate a Web page with dimensions that match the input dimension of the model. Finally, we use the Python package WeasyPrint4 to render the HTML file as a PNG image. Fig 2a visualises this process\u2019 outcome.\nIn some cases, if the text span is short or the selected font is small, the resulting image contains a large empty space (as in Fig 2a). When the empty space within an image exceeds 10%, a new image is generated to replace the vacant area. We create the new image by randomly choosing one of two options. In 80% of the cases, we retain the font of the original image and select the next paragraph. In 20% of the cases, a new paragraph and font are sampled. This pertains to the common case where a historical scan depicts a transition of context or font (e.g., Fig 1a). This process can repeat multiple times, resulting in images akin to Fig 2b.\nFinally, to simulate the effects of scanning ageing historical documents, we degrade the image by adding various types of noise, such as blurring, rotations, salt-and-pepper noise and bleed-through effect (see Fig 2c and Fig 9 in App C for examples). App A.2 enumerates the full list of the degradations and augmentations we use."
        },
        {
            "heading": "4.2 Real Historical Scans",
            "text": "We adapt PHD to the historical domain by continuously pretraining it on a medium-sized corpus of\n3jinja.palletsprojects.com/en/3.1.x 4weasyprint.org\nscans of real historical newspapers. Specifically, we collect newspapers written in English from the \u201cCaribbean Newspapers, 1718\u20131876\u201d database,5 the largest collection of Caribbean newspapers from the 18th\u201319th century available online. We extend this dataset with English-Danish newspapers published between 1770\u20131850 in the Danish Caribbean colony of Santa Cruz (now Saint Croix) downloaded from the Danish Royal Library\u2019s website.6 See Tab 1 for details of dataset sizes. While confined in its geographical and temporal context, this dataset offers a rich diversity in terms of content and format, rendering it an effective test bed for evaluating PHD.\nNewspaper pages are converted into a 368\u00d7 368 pixels crops using a sliding window approach over the page\u2019s columns. This process is described in more detail in App A.2. We reserve 5% of newspaper issues for validation, using the rest for training. See Fig 10 in App C for dataset examples."
        },
        {
            "heading": "4.3 Pretraining Procedure",
            "text": "Like PIXEL, the pretraining objective of PHD is to reconstruct the pixels in masked image patches. We randomly occlude 28% of the input patches with 2D rectangular masks. We uniformly sample their width and height from [2, 6] and [2, 4] patches, respectively, and then place them in random image locations (See Fig 1b for an example). Training hyperparameters can be found in App A.1."
        },
        {
            "heading": "4.4 Pretraining Results",
            "text": "Qualitative Evaluation. We begin by conducting a qualitative examination of the predictions made by our model. Fig 3 presents a visual representa-\n5readex.com/products/caribbean-newspap ers-series-1-1718-1876-american-antiqua rian-society\n6statsbiblioteket.dk/mediestream\ntion of the model\u2019s predictions on three randomly selected scans from the test set of the Caribbean newspapers dataset (for additional results on other datasets, refer to Fig 12 App C). From a visual inspection, it becomes evident that the model accurately reconstructs the fonts and structure of the masked regions. However, the situation is less clear when it comes to predicting textual content. Similar to Rust et al. (2023), unsurprisingly, prediction quality is high and the results are sharp for smaller masks and when words are only partially obscured. However, as the completions become longer, the text quality deteriorates, resulting in blurry text. It is important to note that evaluating these blurry completions presents a significant challenge. Unlike token-based models, where the presence of multiple words with high, similar likelihood can easily be detected by examining the discrete distribution, this becomes impossible with pixel-based models. In pixel-based completions, high-likelihood words may overlay and produce a blurry completion. Clear completions are only observed when a single word has a significantly higher probability compared to others. This limitation is an area that we leave for future work.\nWe now move to analyse PHD\u2019s ability to fill in single masked words. We randomly sample test\nscans and OCRed them using Tesseract.7 Next, we randomly select a single word from the OCRed text and use Tesseract\u2019s word-to-image location functionality to (heuristically) mask the word from the image. Results are presented in Fig 4. Similar to our earlier findings, the reconstruction quality of single-word completion varies. Some completions are sharp and precise, while others appear blurry. In some few cases, the model produces a sharp reconstruction of an incorrect word (Fig 4c). Unfortunately, due to the blurry nature of many of the results (regardless of their correctness), a quantitative analysis of these results (e.g., by OCRing the reconstructed patch and comparing it to the OCR output of the original patch) is unattainable.\nSemantic Search. A possible useful application of PHD is semantic search. That is, searching in a corpus for historical documents that are semantically similar to a concept of interest. We now analyse PHD\u2019s ability to assign similar historical scans with similar embeddings. We start by taking a random sample of 1000 images from our test set and embed them by averaging the patch embeddings of the final layer of the model. We then reduce the dimensionality of the embeddings with\n7github.com/tesseract-ocr/tesseract\nt-SNE (van der Maaten and Hinton, 2008). Upon visual inspection (Fig 13 in App C), we see that scans are clustered based on visual similarity and page structure.\nFig 13, however, does not provide insights regarding the semantic properties of the clusters. Therefore, we also directly use the model in semantic search settings. Specifically, we search our newspapers corpus for scans that are semantically similar to instances of the Runaways Slaves in Britain dataset, as well as scans containing shipping ads (See Fig 16 in App C for examples). To do so, we embed 1M random scans from the corpus. We then calculate the cosine similarity between these embeddings and the embedding of samples from the Runaways Slaves in Britain and embeddings of shipping ads. Finally, we manually examine the ten most similar scans to each sample.\nOur results (Fig 5 and Fig 14 in App C) are encouraging, indicating that the embeddings capture not only structural and visual information, but also the semantic content of the scans. However, the results are still far from perfect, and many retrieved scans are not semantically similar to the search\u2019s target. It is highly plausible that additional specialised finetuning (e.g., SentenceBERT\u2019s (Reimers and Gurevych, 2019) training scheme) is necessary to produce more semantically meaningful embeddings."
        },
        {
            "heading": "5 Training for Downstream NLU Tasks",
            "text": "After obtaining a pretrained pixel-based language model adapted to the historical domain (\u00a74), we now move to evaluate its understanding of natural language and its usefulness in addressing historically-oriented NLP tasks. Below, we describe the datasets we use for this and the experimental settings."
        },
        {
            "heading": "5.1 Language Understanding",
            "text": "We adapt the commonly used GLUE benchmark (Wang et al., 2018) to gauge our model\u2019s understanding of language. We convert GLUE instances into images similar to the process described in \u00a74.1. Given a GLUE instance with sentences s1, s2 (s2 can be empty), we embed s1 and s2 into an HTML template, introducing a line break between the sentences. We then render the HTML files as images.\nWe generate two versions of this visual GLUE dataset \u2013 clean and noisy. The former is rendered using a single pre-defined font without applying degradations or augmentations, whereas the latter is generated with random fonts and degradations. Fig 6 presents a sample of each of the two dataset versions. While the first version allows us to measure PHD\u2019s understanding of language in \u201csterile\u201d settings, we can use the second version to estimate the robustness of the model to noise common to historical scans."
        },
        {
            "heading": "5.2 Historical Question Answering",
            "text": "QA applied to historical datasets can be immensely valuable and useful for historians (Borenstein et al., 2023a). Therefore, we assess PHD\u2019s potential for assisting historians with this important NLP task. We finetune the model on two novel datasets. The first is an adaptation of the classical SQuAD-v2 dataset (Rajpurkar et al., 2016), while the second is a genuine historical QA dataset.\nSQuAD Dataset We formulate SQuAD-v2 as a patch classification task, as illustrated in Fig 11 in App C. Given a SQuAD instance with question q, context c and answer a that is a span in c, we render c as an image, I (Fig 11a). Then, each\npatch of I is labelled with 1 if it contains a part of a or 0 otherwise. This generates a binary label mask M for I , which our model tries to predict (Fig 11b). If any degradations or augmentations are later applied to I , we ensure that M is affected accordingly. Finally, similarly to Lee et al. (2022), we concatenate to I a rendering of q and crop the resulting image to the appropriate input size (Fig 11c).\nGenerating the binary mask M is not straightforward, as we do not know where a is located inside the generated image I . For this purpose, we first use Tesseract to OCR I and generate c\u0302. Next, we use fuzzy string matching to search for a within c\u0302. If a match a\u0302 \u2208 c\u0302 is found, we use Tesseract to find the pixel coordinates of a\u0302 within I . We then map the pixel coordinates to patch coordinates and label all the patches containing a\u0302 with 1. In about 15% of the cases, Tesseract fails to OCR I properly, and a\u0302 cannot be found in c\u0302, resulting in a higher proportion of SQuAD samples without an answer compared to the text-based version.\nAs with GLUE, we generate two versions of visual SQuAD, which we use to evaluate PHD\u2019s performance in both sterile and historical settings.\nHistorical QA Dataset Finally, we finetune PHD for a real historical QA task. For this, we use the English dataset scraped from the website of the Runaways Slaves in Britain project, a searchable database of over 800 newspaper adverts printed between 1700 and 1780 placed by enslavers who wanted to capture enslaved people who had selfliberated (Newman et al., 2019). Each ad was manually transcribed and annotated with more than 50 different attributes, such as the described gender\nand age, what clothes the enslaved person wore, and their physical description.\nFollowing Borenstein et al. (2023a), we convert this dataset to match the SQuAD format: given an ad and an annotated attribute, we define the transcribed ad as the context c, the attribute as the answer a, and manually compose an appropriate question q. We process the resulting dataset similarly to how SQuAD is processed, with one key difference: instead of rendering the transcribed ad c as an image, we use the original ad scan. Therefore, we also do not introduce any noise to the images. See Figure 7 for an example instance. We reserve 20% of the dataset for testing."
        },
        {
            "heading": "5.3 Training Procedure",
            "text": "Similar to BERT, PHD is finetuned for downstream tasks by replacing the decoder with a suitable head. Tab 4 in App A.1 details the hyperparameters used to train PHD on the different GLUE tasks. We use the standard GLUE metrics to evaluate our model. Since GLUE is designed for models of modern English, we use this benchmark to evaluate a checkpoint of our model obtained after training on the artificial modern scans, but before training on the real historical scans. The same checkpoint is also used to evaluate PHD on SQuAD. Conversely, we use the final model checkpoint (after introducing the historical data) to finetune on the historical QA dataset: First, we train the model on the noisy SQuAD and subsequently finetune it on the Runaways dataset (see App A.1 for training details).\nTo evaluate our model\u2019s performance on the QA datasets, we employ various metrics. The primary metrics include binary accuracy, which indicates whether the model agrees with the ground truth regarding the presence of an answer in the context. Additionally, we utilise patch-based accuracy, which measures the ratio of overlapping answer patches between the ground truth mask M and the predicted mask M\u0302 , averaged over all the dataset instances for which an answer exists. Finally, we measure the number of times a predicted answer and the ground truth overlap by at least a single patch. We balance the test sets to contain an equal number of examples with and without an answer."
        },
        {
            "heading": "5.4 Results",
            "text": "Baselines We compare PHD\u2019s performance on GLUE to a variety of strong baselines, covering both OCR-free and OCR-based methods. First, we use CLIP with a ViT-L/14 image encoder in the lin-\near probe setting, which was shown to be effective in a range of settings that require a joint understanding of image and text\u2014including rendered SST-2 (Radford et al., 2021). While we only train a linear model on the extracted CLIP features, compared to full finetuning in PHD, CLIP is about 5\u00d7 the size with \u223c427M parameters and has been trained longer on more data. Second, we finetune Donut (\u00a72.2), which has \u223c200M parameters and is the closest and strongest OCR-free alternative to PHD. Moreover, we finetune BERT and PIXEL on the OCR output of Tesseract. Both BERT and PIXEL are comparable in size and compute budget to PHD. Although BERT has been shown to be overall more effective on standard GLUE than PIXEL, PIXEL is more robust to orthographic noise (Rust et al., 2023). Finally, to obtain an empirical upper limit to our model, we finetune BERT and PIXEL on a standard, not-OCRed version of GLUE. Likewise, for the QA tasks, we compare PHD to BERT trained on a non-OCRed version of the datasets (the Runaways dataset was manually transcribed). We describe all baseline setups in App B.\nGLUE Tab 2 summarises the performance of PHD on GLUE. Our model demonstrates noteworthy results, achieving scores of above 80 for five out of the nine GLUE tasks. These results serve as evidence of our model\u2019s language understanding capabilities. Although our model falls short when compared to text-based BERT by 13 absolute points on average, it achieves competitive results compared to the OCR-then-finetune baselines. Moreover, PHD outperforms other pixel-based models by more than 10 absolute points on average, highlighting the efficacy of our methodology.\nQuestion Answering According to Tab 3, our model achieves above guess-level accuracies on these highly challenging tasks, further strengthening the indications that PHD was able to obtain impressive language comprehension skills. Although the binary accuracy on SQuAD is low, hovering\naround 60% compared to the 72% of BERT, the relatively high \u201cAt least one overlap\u201d score of above 40 indicates that PHD has gained the ability to locate the answer within the scan correctly. Furthermore, PHD displays impressive robustness to noise, with only a marginal decline in performance observed between the clean and noisy versions of the SQuAD dataset, indicating its potential in handling the highly noisy historical domain. The model\u2019s performance on the Runaways Slaves dataset is particularly noteworthy, reaching a binary accuracy score of nearly 75% compared to BERT\u2019s 78%, demonstrating the usefulness of the model in application to historically-oriented NLP tasks. We believe that the higher metrics reported for this dataset compared to the standard SQuAD might stem from the fact that Runaways Slaves in Britain contains repeated questions (with different contexts), which might render the task more trackable for our model.\nSaliency Maps Our patch-based QA approach can also produce visual saliency maps, allowing for a more fine-grained interpretation of model predictions and capabilities (Das et al., 2017). Fig 8 presents two such saliency maps produced by applying the model to test samples from the Runaways Slaves in Britain dataset, including a failure case (Fig 8a) and a successful prediction (Fig 8b). More examples can be found in Fig 15 in App C."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this study, we introduce PHD, an OCR-free language encoder specifically designed for analysing\nhistorical documents at the pixel level. We present a novel pretraining method involving a combination of synthetic scans that closely resemble historical documents, as well as real historical newspapers published in the Caribbeans during the 18th and 19th centuries. Through our experiments, we observe that PHD exhibits high proficiency in reconstructing masked image patches, and provide evidence of our model\u2019s noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, achieving a binary accuracy score of nearly 75%, highlighting its usefulness in this domain. Finally, we note that better evaluation methods are needed to further drive progress in this domain."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was partially funded by a DFF Sapere Aude research leader grant under grant agreement No 0171-00034B, the Danish-Israeli Study Foundation in Memory of Josef and Regine Nachemsohn, the Novo Nordisk Foundation (grant NNF 20SA0066568), as well as by a research grant (VIL53122) from VILLUM FONDEN. The research was also supported by the Pioneer Centre for AI, DNRF grant number P1.\nLimitations\nWe see several limitations regarding our work. First, we focus on the English language only, a high-resource language with strong OCR systems developed for it. By doing so, we neglect lowresource languages for which our model can potentially be more impactful.\nOn the same note, we opted to pretrain our model on a single (albeit diverse) historical corpus of newspapers, and its robustness in handling other historical sources is yet to be proven. To address this limitation, we plan to extend our historical corpora in future research endeavours. Expanding the range of the historical training data would not only alleviate this concern but also tackle another limitation; while our model was designed for historical document analysis, most of its pretraining corpora consist of modern texts due to the insufficient availability of large historical datasets.\nWe also see limitations in the evaluation of PHD. As mentioned in Section 4.4, it is unclear how to empirically quantify the quality of the model\u2019s reconstruction of masked image regions, thus necessitating reliance on qualitative evaluation. This qualitative approach may result in a suboptimal model for downstream tasks. Furthermore, the evaluation tasks used to assess our model\u2019s language understanding capabilities are limited in their scope. Considering our emphasis on historical language modelling, it is worth noting that the evaluation datasets predominantly cater to models trained on modern language. We rely on a single historical dataset to evaluate our model\u2019s performance.\nLastly, due to limited computational resources, we were constrained to training a relatively smallscale model for a limited amount of steps, potentially impeding its ability to develop the capabilities needed to address this challenging task. Insufficient computational capacity also hindered us from conducting comprehensive hyperparameter searches for the downstream tasks, restricting our ability to optimize the model\u2019s performance to its full potential. This, perhaps, could enhance our performance metrics and allow PHD to achieve more competitive results on GLUE and higher absolute numbers on SQuAD."
        },
        {
            "heading": "A Reproducibility",
            "text": "A.1 Training Pretraining We pretrain PHD for 1M steps on with the artificial dataset using a batch size of 176 (the maximal batch size that fits our system) using AdamW optimizer (Kingma and Ba, 2014; Loshchilov and Hutter, 2017) with a linear warmup over the first 50k steps to a peak learning rate of 1.5e\u22124 and a cosine decay to a minimum learning rate of 1e\u22125. We then train PHD for additional 100k steps with the real historical scans using the same hyperparameters but without warm-up. Pretraining took 10 days on 2 \u00d7 80GB Nvidia A100 GPUs.\nGLUE Table 4 contains the hyperparameters used to finetune PHD on the GLUE benchmark. We did not run a comprehensive hyperparameter search due to compute limitations; these settings were manually selected based on a small number of preliminary runs.\nSQuAD To finetune PHD on SQuAD, we used a learning rate of 6.75e\u22126, batch size of 128, dropout probability of 0.0 and weight decay of 1e\u22125. We train the model for 50 000 steps.\nRunaways Slaves in Britain To finetune PHD on the Runaways Slaves in Britain dataset, first trained the model on SQuAD using the hyperparameters mentioned above. Then, we finetuned the resulting model for an additional 1000 steps on the Runaways Slaves in Britain. The only hyperparameter we changed between the two runs is the dropout probability, which we increased to 0.2.\nA.2 Dataset Generation List of dataset augmentations To generate the synthetic dataset described in Section 4.1, we applied the following transformations to the rendered images: text bleed-through effect; addition of random horizontal and lines; salt and pepper noise; Gaussian blurring; water stains effect; \u201choles-inimage\" effect; colour jitters on image background; and random rotations.\nConverting the Caribbean Newspapers dataset into 368 \u00d7 368 scans We convert full newspaper pages into a collection of 368 \u00d7 368 pixels using the following process. First, we extract the layout of the page using the Python package Eynollah.8\n8https://github.com/qurator-spk/eynol lah\nThis package provides the location of every paragraph on the page, as well as their reading order. As newspapers tend to be multi-columned, we \u201clinearise\u201d the page into a single-column document. We crop each paragraph and resize it such that its width equals 368 pixels. We then concatenate all the resized paragraphs with respect to their reading order to generate a long, single-column document with a width of 368 pixels. Finally, we use a sliding window approach to split the linear page into 368 \u00d7 368 crops, applying a stride of 128 pixels. We reserve 5% of newspaper issues for validation, using the rest for training. See Fig 10 in App C for dataset examples."
        },
        {
            "heading": "B Historical GLUE Baselines",
            "text": "For all baselines below, we compute and average scores over 5 random initializations.\nOCR + BERT/PIXEL For each GLUE task, we first generate 5 epochs of noisy training data and run Tesseract on it to obtain noisy text datasets. Similarly, however without oversampling, we obtain noisy versions of our fixed validation sets. We then finetune BERT-base and PIXEL-base in the same way as Rust et al. (2023), with one main difference: the noisy OCR output prevents us from separating the first and second sentence in sentencelevel tasks. Therefore we treat each sentence pair as a single sequence and leave it for the models to identify sentence boundaries itself, similar to how PHD has to identify sentence boundaries in the images. We use the codebase and training setup from Rust et al. (2023).9\n9https://github.com/xplip/pixel\nCLIP We run linear probing on CLIP using an adaptation of OpenAI\u2019s official codebase.10 We first extract image features from the ViT-L/14 CLIP model and then train a logistic regression model with L-BFGS solver for all classification tasks and an ordinary least squares linear regression model for the regression tasks (only STS-B).\nDonut We finetune Donut-base using an adaptation of ClovaAI\u2019s official codebase.11 We frame each of the GLUE tasks as image-to-text tasks: the model receives the (noisy) input image and is trained to produce an output text sequence such as <s_glue><s_class><positive/> </s_class></s>. In this example, taken from SST-2, the < X > tags are new vocabulary items added to Donut and the label is an added vocabulary item for the positive sentiment class. All classification tasks in GLUE can be represented in this way. For STS-B, where the label is a floating point value denoting the similarity score between two sentences, we follow Raffel et al. (2020) to round and convert the floats into strings.12 We finetune with batch size 32 and learning rate between 1e\u22125 and 3e\u22125 for a maximum of 30 epochs or 15 000 steps on images resized to a resolution of 320 \u00d7 320 pixels.\nOCR-free BERT/PIXEL For GLUE, we take results reported in (Rust et al., 2021). For SQuAD, we take a BERT model finetuned on SQuAD-v2,13\n10https://github.com/openai/CLIP#linea r-probe-evaluation\n11https://github.com/clovaai/donut 12Code example in https://github.com/googl e-research/text-to-text-transfer-transfo rmer/blob/main/t5/data/preprocessors.py# L816-L855\n13from https://huggingface.co/deepset/ber t-base-cased-squad2.\nand evaluate it on the validation set of SQuAD-v2, after being balanced for the existence of an answer. For the Runaways Slaves in Britain dataset, we finetune a BERT-base-cased model14 on a manually transcribed version of the dataset. We use the default SQuAD-v2 hyperparameters reported in the official Huggingface repository for training on SQuAD-v2.15 We then evaluate the model on a balanced test set, containing 20% of the ads."
        },
        {
            "heading": "C Additional Material",
            "text": "Figure 9 additional examples from our artificially generated dataset. Figure 10 Sample scans from the real historical dataset, as described in Section 4.2. Figure 11 The process of generating the Visual SQuAD dataset. We first render the context as an image (a), generate a patch-level label mask highlighting the answer (b), add noise and concatenate the question (c). Figure 12 Additional examples of PHD\u2019s completions over test set samples. Figure 13 Dimensionality reduction of embedding calculated by our model on historical scans. We see that scans are clustered based on visual similarity and page structure. However, further investigation is required to determine whether scans are also clustered based on semantic similarity. Figure 14 Using PHD for semantic search. Figure 14a and is the target of the search (the concept we are looking for), while Figure 14b and are the retrieved scans. Figure 15 Additional examples of PHD\u2019s saliency maps for samples from the test set of the Runaways Slaves in Britain dataset. Figure 16 Examples of shipping ads Newspapers. Newspapers in the Caribbean region routinely reported on passenger and cargo ships porting and departing the islands. These ads are usually wellstructured and contain information such as relevant dates, the ship\u2019s captain, route, and cargo. Figure 17 Input samples for PIXEL. The images are rolled, i.e., the actual input resolution is 16 \u00d7 8464 pixels. The grid represents the 16 \u00d7 16 patches that the inputs are broken into. Figure 18 An example of a full newspaper page downloaded from the \u201cCaribbean project\u201d.\n14from https://huggingface.co/bert-bas e-cased\n15https://colab.research.google.com/gi thub/huggingface/notebooks/blob/master/e xamples/question_answering.ipynb"
        }
    ],
    "title": "PHD: Pixel-Based Language Modeling of Historical Documents",
    "year": 2023
}