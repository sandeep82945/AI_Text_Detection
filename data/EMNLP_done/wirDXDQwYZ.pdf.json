{
    "abstractText": "Generalized quantifiers (e.g., few, most) are used to indicate the proportions predicates are satisfied (for example, some apples are red). One way to interpret quantifier semantics is to explicitly bind these satisfactions with percentage scopes (e.g., 30%-40% of apples are red). This approach can be helpful for tasks like logic formalization and surface-form quantitative reasoning (Gordon and Schubert, 2010; Roy et al., 2015). However, it remains unclear if recent foundation models possess this ability, as they lack direct training signals. To explore this, we introduce QuRe, a crowdsourced dataset of human-annotated generalized quantifiers in Wikipedia sentences featuring percentage-equipped predicates. We explore quantifier comprehension in language models using PRESQUE, a framework that combines natural language inference and the Rational Speech Acts framework. Experimental results on the HVD dataset and QuRe illustrate that PRESQUE, employing pragmatic reasoning, performs 20% better than a literal reasoning baseline when predicting quantifier percentage scopes, with no additional training required1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yiyuan Li"
        },
        {
            "affiliations": [],
            "name": "Rakesh R. Menon"
        },
        {
            "affiliations": [],
            "name": "Sayan Ghosh"
        },
        {
            "affiliations": [],
            "name": "Shashank Srivastava"
        }
    ],
    "id": "SP:cc758f68484647acf2bac8f532348d8e7c7e7f29",
    "references": [
        {
            "authors": [
                "Rachid Alami."
            ],
            "title": "On human models for collaborative robots",
            "venue": "2013 International Conference on Collaboration Technologies and Systems (CTS), pages 191\u2013194.",
            "year": 2013
        },
        {
            "authors": [
                "Leon Bergen",
                "R. Levy",
                "Noah D. Goodman."
            ],
            "title": "Pragmatic reasoning through semantic inference",
            "venue": "Semantics and Pragmatics, 9.",
            "year": 2016
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Fausto Carcassi",
                "Jakub Szymanik."
            ],
            "title": "most\u2019 vs \u2018more than half\u2019: An alternatives explanation",
            "venue": "Proceedings of the Society for Computation in Linguistics 2021, pages 334\u2013343, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Ruixiang Cui",
                "Daniel Hershcovich",
                "Anders S\u00f8gaard."
            ],
            "title": "Generalized quantifiers as a source of error in multilingual NLU benchmarks",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Michael C. Frank",
                "Noah D. Goodman."
            ],
            "title": "Predicting pragmatic reasoning in language games",
            "venue": "Science, 336(6084):998\u2013998.",
            "year": 2012
        },
        {
            "authors": [
                "Noah D. Goodman",
                "Michael C. Frank."
            ],
            "title": "Pragmatic language interpretation as probabilistic inference",
            "venue": "Trends in Cognitive Sciences, 20(11):818\u2013829.",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Gordon",
                "Lenhart K. Schubert."
            ],
            "title": "Quantificational sharpening of commonsense knowledge",
            "venue": "AAAI Fall Symposium: Commonsense Knowledge.",
            "year": 2010
        },
        {
            "authors": [
                "Aur\u00e9lie Herbelot",
                "Eva Maria Vecchi."
            ],
            "title": "From concepts to models : Some issues in quantifying feature norms",
            "venue": "Linguistic Issues in Language Technology (LiLT).",
            "year": 2015
        },
        {
            "authors": [
                "Alexandra Horowitz",
                "Rose M. Schneider",
                "Michael C. Frank."
            ],
            "title": "The trouble with quantifiers: Exploring children\u2019s deficits in scalar implicature",
            "venue": "Child development, 89 6:e572\u2013e593.",
            "year": 2018
        },
        {
            "authors": [
                "Pratik Joshi",
                "Somak Aditya",
                "Aalok Sathe",
                "Monojit Choudhury."
            ],
            "title": "TaxiNLI: Taking a ride up the NLU hill",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 41\u201355, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Andrzej Wlodzimierz Mostowski."
            ],
            "title": "On a generalization of quantifiers",
            "venue": "Studies in logic and the foundations of mathematics, 93:311\u2013335.",
            "year": 1957
        },
        {
            "authors": [
                "Aakanksha Naik",
                "Abhilasha Ravichander",
                "Norman Sadeh",
                "Carolyn Rose",
                "Graham Neubig."
            ],
            "title": "Stress test evaluation for natural language inference",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340\u20132353,",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Haonan Chen",
                "Mohit Bansal."
            ],
            "title": "Combining fact extraction and verification with neural semantic matching networks",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI).",
            "year": 2019
        },
        {
            "authors": [
                "Sandro Pezzelle",
                "Raffaella Bernardi",
                "Manuela Piazza."
            ],
            "title": "Probing the mental representation of quantifiers",
            "venue": "Cognition, 181:117\u2013126.",
            "year": 2018
        },
        {
            "authors": [
                "Sandro Pezzelle",
                "Ionut-Teodor Sorodoc",
                "Raffaella Bernardi."
            ],
            "title": "Comparatives, quantifiers, proportions: a multi-task model for the learning of quantities from vision",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
            "year": 2018
        },
        {
            "authors": [
                "Steven T. Piantadosi",
                "Harry J. Tily",
                "Edward Gibson."
            ],
            "title": "The communicative function of ambiguity in language",
            "venue": "Cognition, 122:280\u2013291.",
            "year": 2012
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Abhilasha Ravichander",
                "Aakanksha Naik",
                "Carolyn Rose",
                "Eduard Hovy."
            ],
            "title": "EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Kyle Richardson",
                "Hai Hu",
                "Lawrence Moss",
                "Ashish Sabharwal."
            ],
            "title": "Probing natural language inference models through semantic fragments",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8713\u20138721.",
            "year": 2020
        },
        {
            "authors": [
                "Kyle Richardson",
                "Hai Hu",
                "Lawrence S Moss",
                "Ashish Sabharwal."
            ],
            "title": "Probing Natural Language Inference Models through Semantic Fragments",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI).",
            "year": 2019
        },
        {
            "authors": [
                "Subhro Roy",
                "Tim Vieira",
                "Dan Roth."
            ],
            "title": "Reasoning about quantities in natural language",
            "venue": "Transactions of the Association for Computational Linguistics, 3:1\u201313.",
            "year": 2015
        },
        {
            "authors": [
                "Stephanie Solt."
            ],
            "title": "On measurement and quantification: The case of most and more than half",
            "venue": "Language, 92:100 \u2013 65.",
            "year": 2016
        },
        {
            "authors": [
                "Shashank Srivastava",
                "Igor Labutov",
                "Tom Mitchell."
            ],
            "title": "Zero-shot learning of classifiers from natural language quantification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 306\u2013316.",
            "year": 2018
        },
        {
            "authors": [
                "Alberto Testoni",
                "Sandro Pezzelle",
                "Raffaella Bernardi."
            ],
            "title": "Quantifiers in a multimodal world: Hallucinating vision with language and sound",
            "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 105\u2013116, Min-",
            "year": 2019
        },
        {
            "authors": [
                "Shivin Thukral",
                "Kunal Kukreja",
                "Christian Kavouras."
            ],
            "title": "Probing language models for understanding of temporal expressions",
            "venue": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 396\u2013406, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Asso-",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Generalized quantifiers (Mostowski, 1957) are used to express relations between subsets of concepts or entities. For instance, the quantifier \u2018some\u2019 in the statement \u2018some apples are red\u2019 indicates that at least one apple is red. Quantifiers, being inherently fuzzy, are prevalent in both real-world communication and natural language processing (NLP) benchmarks (Joshi et al., 2020). Consequently, developing a formal framework for understanding quantifier semantics is essential to enhance the language understanding capabilities of NLP systems, particularly in facilitating natural human-AI language-based interactions, such as in human-robot collaborative tasks (Alami, 2013).\n1Code: https://github.com/Nativeatom/PRESQUE\nIn this work we present PRESQUE (Pragmatic REasoning for Semantics of QUantifiErs), a framework to model the semantics of quantifiers for textbased foundation models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), , through the lens of pragmatic reasoning. While foundation models have shown impressive performance on various text-based tasks (Bommasani et al., 2021; Wei et al., 2022), their ability to infer the semantic meanings of generalized quantifiers remains relatively unexplored.\nIn PRESQUE, we represent quantifier semantics in terms of percentage scopes, which indicate the proportion of cases where the associated predicate holds true. For example, in the sentence \u2018some apples are red\u2019, the quantifier \u2018some\u2019 could be associated with a percentage scope of 30-40%, indicating that 30-40% of all apples are red. Our framework consists of two components: (1) a natural language inference (NLI) component (Bowman et al., 2015) that models sentence-level semantics between a sentence containing a quantifier word and another sentence containing a percentage value, and (2) a rational speech act (RSA) component (Frank and Goodman, 2012) for pragmatic reasoning. Using these components, PRESQUE takes a sentence with a quantifier as input and outputs the corresponding percentage scope (further discussed in Section 2).\nAmbiguity, as highlighted by Piantadosi et al. (2012), is beneficial for efficient communication via language. Since the percentage values of quantifiers are not universally defined, humans often need to infer the exact percentage value, which is not explicitly conveyed in the utterance (Horowitz et al., 2018). Furthermore, the interpretation of quantifier semantics can be influenced by linguistic and social cues (Bergen et al., 2016). The pragmatic theory proposed by Grice (1975) emphasizes the role of communicative goals in interpreting the semantic meaning of natural language expressions, simplifying the required semantic theories (Bergen\net al., 2016). Lastly, NLI models are also shown to struggle with ambiguous premises (Thukral et al., 2021), quantifier inference (Richardson et al., 2020; Joshi et al., 2020), and quantitative reasoning (Naik et al., 2018; Ravichander et al., 2019), making a direct literal interpretation of generalized quantifiers less reliable.\nTo address these challenges, PRESQUE employs RSA, a Bayesian framework that follows a Gricean approach for modeling communication by reasoning about agent states (Grice, 1975). PRESQUE incorporates a literal speaker role, based on a foundation model fine-tuned on NLI datasets, and a pragmatic listener role, computed using Bayesian rules, to reason between the quantifier space and the space of percentage values.\nExisting datasets like HVD (Herbelot and Vecchi, 2015) and GQNLI (Cui et al., 2022) that investigate quantifier semantics either lack gold annotations of the percentage scopes for interpreting quantifier semantics, or are based on artificial setups using a small number of countable objects (Pezzelle et al., 2018b). Such fictional settings are not generalizable to broader and more complex real-world settings (e.g. describing concepts about a population using quantifiers). For a fair evaluation of the quantifier understanding capabilities acquired by foundation models through their pre-training, we should evaluate these models using text of similar style and content as the pre-training corpora. To address the aforementioned issues with current evaluation corpora for quantifier understanding, we crowd-source a dataset, QuRe (Quantifier Reasoning), which contains sentences containing quantifiers paired with annotations for quantifier semantics in terms of percentage scopes. Additionally, we characterize the ease of making quantifier predictions for different sentences in QuRe.\nUsing PRESQUE to evaluate the quantifier reasoning ability of foundation models on QuRe, we observe a 20% span-based F1 boost over the literal listener baseline at all specificity levels (Section 5.2). Our experiments highlight the improved quantifier understanding of foundation models when approached from a pragmatic perspective rather than relying on direct interpretation using textual understanding frameworks like NLI. Although our framework does not explicitly model mathematical concepts, it is noteworthy that the mean strengths of quantifiers in foundation models, as revealed by PRESQUE, echo observations of quantifier hierar-\nchies from previous works (Solt, 2016; Srivastava et al., 2018) that involve strong human priors, and findings from Pezzelle et al. (2018a), who associates quantifier usage with the counting behavior of human beings.\nIn summary, our contributions are two-fold: we develop PRESQUE based on pragmatic reasoning and NLI, and we crowd-source a dataset QuRe to support quantifier understanding investigation of foundation models. Our results on HVD and QuRe demonstrate that foundation models equipped with pragmatic reasoning (PRESQUE) can perform quantifier reasoning similar to humans."
        },
        {
            "heading": "2 Quantifier Semantics Understanding through RSA",
            "text": "We frame the task of quantifier understanding as the prediction of the percentage scope (e.g., 30%-50%) given a quantified sentence S\u0303q (e.g., Some apples are red.). Specifically, given an interval width \u03b2, we divide the percentage spectrum between 0 and 1 into evenly spaced intervals, denoted as W\u03b2 = {pi} (e.g., W\u03b2=0.05 = {0, 5%, 10%, ..., 95%, 100%}). The goal of a quantifier understanding model is to determine the percentage range in W\u03b2 where the associated predicate holds true (e.g., the proportion of red apples among all apples, 30%-50%).\nTo interpret quantifiers as percentage scopes, we develop PRESQUE, a framework that adopts the rational speech act (RSA) framework, with natural language inference (NLI) as the backbone for text understanding. The RSA framework consists of a speaker and a listener, where the listener infers the world state from the speaker\u2019s utterance by modeling the speaker\u2019s state of mind (Goodman and Frank, 2016). In PRESQUE, the world state corresponds to percentage values of predicates, while utterances correspond to quantifiers used with those predicates.\nGiven a premise p\u0303 (e.g., Some apples are red.) with quantifier q (some) and a hypothesis h\u0303 (e.g., 30% apples are red.) with a percentage value p (30%), we use the entailment score between the premise and hypothesis, obtained from an NLI model, to define the literal listener L0:\nL0(p|q) \u221d Entailment(p\u0303, h\u0303) (1)\nThe pragmatic listener L1, in the PRESQUE framework, interprets the semantics of the quanti-\nfier word as:\nL1(p|q) \u221d S0(q|p)P(p) (2)\nHere, S0 represents a literal speaker that maps the semantics of percentage values to quantifier words. Practically, we model the speaker by swapping the premise and hypothesis in L0:\nS0(q|p) \u221d Entailment(h\u0303, p\u0303) (3)\nThe prior P (p) in Eq. 2 can be expanded as:\nP(p) = \u2211 q\u2208U P(p|q)P(q) (4)\nHere, P(p|q) is computed similarly to L0, and P(q) represents the word frequency of q, obtained from the WORDFREQ dataset (Speer, 2022).\n3 QuRe: Quantifier Reasoning Dataset\nExisting datasets for quantifier understanding like HVD (Herbelot and Vecchi, 2015) are comprised of triples of the form \u27e8concept, feature, quantifier\u27e9 (e.g. \u27e8shrimp, is_white, some\u27e9) that denote how often a \u2018feature\u2019 is satisfied for a \u2018concept\u2019. Notably, these datasets do not provide annotated percentage scopes that can be used to decipher the semantics of the quantifiers, i.e., how often (in numerical terms) the \u2018feature\u2019 is satisfied for the \u2018concept\u2019 in the real world, and the supporting documents (e.g. knowledge-bases in Wikipedia or any publicly available corpus) about the percentage scope of those triples are not easily accessible. Therefore, the judgments are based on subjective observation and experience (e.g. the proportion of white shrimps.) and are hence inaccurate. To address this shortcoming in available resources for quantifier understanding, we contribute a dataset, QuRe, for evaluating the quantifier understanding capabilities of language models. QuRe consists of sentences (from Wikipedia) containing percentage mentions annotated with the quantifiers.\nTable 1 presents examples from QuRe. Of note, in addition to the quantifier annotation and percentage scopes, for each example in QuRe, we also provide specificity as additional metadata. Specificity measures the difficulty of deciphering the percentage scope of the quantifier from the sentence excluding the quantifier (i.e., if someone can deduce the percentage scope of a quantifier fully/partially from the sentence contents when the quantifier is absent; more details are provided later in Stage 4).\nThe annotations in QuRe are obtained through a mix of crowd-sourcing (using Amazon Mechanical Turk) and assistance from gpt-3.5-turbo (OpenAI, 2022). We describe the annotation procedure in more detail below.\nStage 1: Wikipedia sentences collection We utilize the \u27e8concept, feature, quantifier\u27e9 triples from the HVD dataset and convert them into sentences programmatically through templates (e.g. \u27e8shrimp, is_white, some\u27e9 \u2192 \u2018some shrimps are white\u2019). We then link these sentences to the most related Wikipedia entities2 using an off-the-shelf library3. For example, the related Wikipedia entities for the running example would be {Shrimp, Prawn, Indian prawn, etc.}. In practice, we find this setting links to more diverse entities than simply linking the concepts. We then use regular expressions to extract around 5,000 candidate sentences containing percentage values from the Wikipedia pages of these entities. For example, \u2018Among the prawn species entering the field F. indicus constitute around 36%\u201343%.\u2019 is one such sentence from the Wikipedia page of the entity Indian prawn, with the percentage mention underlined.\nStage 2: Sentence Filtering The candidate sentences are further filtered based on two criteria: (1) the length of the sentences should be between 10 and 40 tokens (space-tokenized), and (2) the percentage mentioned in the sentence should not indicate a comparative change (e.g., \u2018increased by 20%\u2019). To identify whether the sentence describes\n2Each Wikipedia entity is the title of a Wikipedia article. 3https://pypi.org/project/wikipedia/\na comparative change, we used regular expressions. However, capturing all possible variations of describing such comparative changes through regular expressions is cumbersome. Hence we employ GPT-3.5-turbo to annotate sentences that contain comparative changes. To validate the efficacy of GPT-3.5-turbo, we manually annotate a held-out set of 50 sentences based on our aforementioned filtering criteria. On this held-out set GPT-3.5-turbo achieves 0.76 F1. More details on the annotation usage of GPT-3.5-turbo in this stage are included in Appendix H. The filtered sentences are then paired up with all percentage mentions in the sentence and manually validated by the authors. Around half of the percentage mentions were deemed inappropriate for the quantifier understanding task and removed. We include examples, metadata, and the instruction used in Appendix A and J.\nStage 3: Percentage Expression Generation In many Wikipedia sentences, the percentage value is surrounded by texts like around, less than, more than, etc. to denote a percentage scope rather than the individual percentage value or a percentage range. We use GPT-3.5-turbo to obtain those percentage scopes and the instruction is included in Appendix I. The variations that we capture in this stage to obtain the percentage scopes are mentioned in Table 2.\nStage 4: Quantifier and Specificity Annotation We design two human annotation tasks. The first task is rephrasing a sentence, S\u0303p, with a target percentage mention (e.g. \u2018around 36%-43%\u2019 of \u2018...the field F. indicus constitute around 36%\u201343%\u2019 in the previous example) to S\u0303q with minimal semantic changes using a quantifier from U (e.g. Among the prawn species entering the field, F. indicus constitute a large amount.). This step ensures the seman-\ntics of the quantifier used in S\u0303q is associated to the percentage scope in S\u0303p.\nIn the second task, we measure specificity, or the difficulty of specifying the target percentage scope from S\u0303q without the quantifier q (e.g. removing a large amount from the previous S\u0303q). In our study, we discretize the specificity values into three distinct levels of difficulty: full/partial/indeterminable. Full means the target percentage scope can be fully determined by other contents in the sentence, like One in ten for 10%; partial means the percentage scope can be narrowed but not determined by the contents (e.g. an incomplete percentage breakdown), and indeterminable means there is no information in the content of the sentence to determine the percentage scope. This task aims to gauge the extent of information that the context contributes to the determination of the quantifier\u2019s percentage scope. For example, the specificity of the previous S\u0303q about prawn would be indeterminable since the rest of the sentence after removing a large amount does not provide information to determine the percentage scope of large. But with additional contents like \u2018... constitute a large amount (around one-third).\u2019, the specificity level would become partially. More examples are included in Appendix A.\nWe use majority voting (amongst three annotations) to choose the final annotated quantifier among all annotations for each sentence. The instruction used, examples, and example annotation interface are included in Appendix M. The set of quantifiers to select from is U = {all, generally, most, usually, some, likely, few, little, occasionally, none, seldom, tiny, small, moderate, large}, which largely comes from Srivastava et al. (2018), and is slightly extended based on preliminary annotator feedback. We leave the choice of nouns that are attached to adjective quantifiers (e.g. amount in small amount), like small and large, in sentences to the annotators.\nStatistics We have collected 744 S\u0303q sentences, of which 47% and 17% contain no and one percentage mention respectively and others contain more than one percentage mention. The average sentence length is 26.3 tokens. Each sentence is annotated by 3 annotators. The Fleiss\u2019 Kappa for quantifier choices and specificity are 0.37 and 0.80, meaning fair agreement in quantifier choices and substantial agreement in specificity levels. The distribution of quantifiers in QuRe is shown in Fig-\nure 1, where some is used in over 25% of sentences, followed by most, moderate, large and few. The most popular quantifiers for different percentage scopes are shown in Figure 2, where some is preferred in over 30% of the cases with target percentage values lower than 40%, and most is selected in over 40% of the cases with target percentage value greater than 60%. Overall, 17% of the sentences have target percentages fully specified, 32% partially specified, and 50% are indeterminable. We include examples across different specificity levels in Appendix A.\nWe also include the average strength of quantifiers under different grounding configurations in\nTable 3.4 We can see that the mean strengths are\n4For the definition of g and w, please refer to Appendix E.\nstable among configurations, and show interesting hierarchies: all (0.88) is higher than generally (0.73), and generally is higher than most (0.69). These patterns closely align with previous manual strength assignments like Srivastava et al. (2018), and Testoni et al. (2019)\u2019s quantifier collection from multimodal simulations. It also echoes Solt (2016)\u2019s finding that the strength of most is higher than more than half."
        },
        {
            "heading": "4 Experimental setup",
            "text": "For the experiment in HVD, we compute L(p|q) for PRESQUE and L0 among different foundation models and compare them with human interpretations. In QuRe, with the target percentage given in Section 3, we generate the percentage scope that S\u0303q satisfies. All percentage choices are selected from W\u03b2 , and experiments are run without training.\nPercentage Scope Generation With specific granularity g and window size w for the operators, a percentage expression in Section 3 is converted into a golden scope {pmin, pmax} \u2208 W\u03b2 (pmin \u2264 pmax). For example, if \u03b2 = 0.05, g = 0.01 and w = 2, the golden scope of \u223c 0.59 is [0.55, 0.65]. The full generation rules are in Appendix E.\nEvaluation Metrics For HVD, given a listener LM based on an NLI model M. LM(p|q) is computed by averaging the entailment scores over all S\u0303qs for all p values in W\u03b2 and normalize them to be a distribution. We can then compute the cross entropy between the human interpretation of quantifiers Ph from Section 5.1 and LM(p|q) to measure the similarity of quantifier interpretation between humans and M.\nCrossEntropy = \u2212 \u2211 q\u2208U \u2211 p\u2208W\u03b2 Ph(p|q) log LM(p|q) |U|\nFor S\u0303q in QuRe, we compute the following metrics,\nHIT@1 = I[argmax p\u2208W\u03b2 L(p|q) \u2208 sgolden]\nMRR = \u03b2/(Bm \u00b7 \u2211\np\u2032\u2208sgolden Rankp\u2032) CrossEntropy = \u2212 \u2211\np\u2032\u2208sgolden logP(p \u2032|q) where P(p\u2032|q)) = L(p\u2032|q)/ \u2211\np L(p|q) where Bm = pmax \u2212 pmin + \u03b2, p\u2032 \u2208 W\u03b2\nwhere I(\u00b7) is an indicator, sgolden is the gold scope [pmin, pmax], and Rankp\u2032 is the rank of p\u2032 in W\u03b2 by L(p|q). HIT@1 measures whether the top inference percentage lies in the gold scope. MRR and cross entropy measure the average rank and confidence of the gold scope. We also compute the span-based F1 score between the gold scope and the primary scope (Section 5.2) of the top K\npredictions (F1@K) under W\u03b2 , which is used in question answering (Rajpurkar et al., 2016). The metrics are averaged over the entire dataset, and L(p|q) is computed through Eq. 1 - Eq. 4."
        },
        {
            "heading": "5 Experiments and Results",
            "text": "We perform experiments to determine the percentage scope of quantifiers on two datasets: the HVD dataset, which includes predicates annotated with quantifiers but lacks percentage scope annotations, and the QuRe dataset, which provides annotations for both quantifiers and percentage scopes. As a baseline, we use the literal listener, L0."
        },
        {
            "heading": "5.1 Human Interpretation of Quantifiers",
            "text": "To quantitatively assess the similarity between quantifier understanding between foundation models and humans, we first collect interpretations Ph(p|q) from human participants. For this, we employ 25 Mechanical Turk annotators who are tasked with providing the percentage scope of quantifiers. To guide them, we provide an instruction (see Appendix K for details) and present them with five questions. Each question requires the annotator to indicate the strength of a given quantifier word in U by providing either a percentage scope or a single percentage value in W\u03b2=0.1, without resorting to online searching. The distribution of the annotators\u2019 choices, as shown in Figure 3, reveals that humans interpret different percentage peaks for different quantifier words. The percentage scope of few, some, most indicated by the selection ratio of\nmore than 10% are larger than those of no and all. Meanwhile, the percentage scope of some is leaning to few rather than most, where few and most have little scope overlap."
        },
        {
            "heading": "5.2 NLI Model\u2019s Interpretation of Quantifiers",
            "text": "We evaluate the quantifier reasoning ability of the \u2018large\u2019 (or \u2018xxlarge\u2019) variants of ALBERT (Lan et al., 2020), XLNet (Yang et al., 2019), BART (Lewis et al., 2020) and RoBERTa (Liu et al., 2019) that are fine-tuned on the NLI tasks (using Adversarial NLI (Nie et al., 2020), SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018), and NLI-style FEVER (Nie et al., 2019) datasets).5\nThe comparison of quantifier understanding using PRESQUE and L0 is presented in Table 4. The results show that PRESQUE achieves lower cross entropies compared to L0. Among the NLI models, RoBERTa performs the best within the PRESQUE framework, and therefore, it is chosen as the primary model for subsequent experiments. The L(p|q) scores of PRESQUE from RoBERTa, which are used to represent the model\u2019s interpretation of the percentage scopes of different quantifiers, are displayed in the lower half of Figure 3. In general, different quantifier words exhibit distinct percentage distributions. Similar to Section 5.1, the scopes of few, some, and most can be approximated as 0% - 30%, 10% - 50%, and 60% - 100%, respectively, with a cutoff criteria L(p|q) \u2265 0.1. These ranges align closely with the scopes determined by human evaluation (upper half of Figure 3). Further, the L(p|q) scores change in a smooth way as the percentages increase or decrease within the regions where L(p|q) \u2265 0.1. This suggests that the model can understand percentage values quite well.\n5In preliminary experiments, we found that foundation models without NLI fine-tuning performed worse on the quantifier prediction task.\nNext, we compare the results of PRESQUE with that of a literal listener baseline, L0 (Equation 1). As the percentage scope of a quantifier is measured by the consecutive percentage range among the top K-ranked percentage values, we compare the consecutiveness of L0 and PRESQUE, which is measured by the proportion of sentences with the entire top K choices being able to constitute a single consecutive range. For example e.g. {10%, 20%, 30%} constitutes a consecutive range from 10% to 30% while {10%, 30%, 50%} does not. Consecutiveness is based on the assumption that consecutive ranking of percentage values indicates better quantifier understanding as the semantic meaning of quantifier words does not leap between disjoint percentage values. To enlarge the possible ranges, we start by K = 3 and include the results in Figure 4, where PRESQUE has higher consecutiveness than L0, showing that PRESQUE has more consistent percentage inference behavior. Moreover, We select the primary scope by finding the consecutive scope (e.g. {10%-30%} and {10%, 30%, 50%} in the previous example) of the largest aggregated L(p|q) among all consecutive scopes.\nWe additionally compare the primary scopes between L0 and PRESQUE, through human preferences. For each quantifier word, we randomly sample 10 sentences where the top K inferences between L0 and PRESQUE differ for the same S\u0303q with\nK = 5. We then recruit 40 annotators from Amazon Mechanical Turk to select the more reasonable primary scope between L0 and PRESQUE given S\u0303q. We displayed the primary scopes for each S\u0303q in random order to avoid biases. Examples are included in Table 5 where PRESQUE generates smaller primary scopes for universal quantifiers like No and All, and larger primary scopes of other quantifiers which incorporate more vagueness. We leave the more general analysis in Appendix F.\nTable 6 provides a comparison of the top percentage predictions with the gold scopes from PRESQUE and L0 in QuRe. We observe that, in general, PRESQUE outperforms L0 in several aspects. Firstly, the topmost prediction value from PRESQUE appears more frequently within the gold scope, leading to a higher HIT@1 score. Additionally, the percentage values within the gold scope are ranked higher among the top predictions by PRESQUE, resulting in a higher Mean Reciprocal Rank (MRR). Furthermore, there is a larger overlap between the primary scopes of PRESQUE and the gold scopes, as indicated by a higher F1 score. Moreover, PRESQUE predicts better primary scopes on a distance-based metric designed to measure the distance between scopes, and we include the results in Appendix G. This finding aligns with the con-\nclusion of Carcassi and Szymanik (2021), which suggests that listeners aim to minimize the distance from the speaker\u2019s observation when the communication success is measured based on distance.\nQualitative Analysis. Examining examples generated by PRESQUE, we make several interesting observations. For fully determinable contexts, such as \u201c... only (2 out of the total 27) few school children were recognized...\" with a gold scope of 5%-10% (the true rate was 2 children out of 27 = 7%), PRESQUE provides a more accurate primary scope. In this case, PRESQUE predicted a scope of 0%-5%, while L0 predicted a scope of 0%. For partially determinable contexts, such as \u201c... calculating from less than few ... to 13%...\" (indicating a partially determinable percentage scope of less than 13%), with a gold scope of 5%-10%, PRESQUE often generates a broader scope than L0. In this case, PRESQUE predicts 0%-15%, which is more expansive than L0\u2019s prediction of 10%-15%. For some indeterminable sentences like \u201c... its alcohol content usually is very little.\" with a gold scope of 0%-5%, PRESQUE provides a primary scope of 0%-5%, while L0 predicts a significantly distant scope of 60%-70%. Appendix B provides a more comprehensive set of examples."
        },
        {
            "heading": "6 Conclusion",
            "text": "Generalized quantifiers are widely used for addressing satisfaction in natural language. However, the quantifier understanding abilities of foundation models are not well-studied or well-supported by the existing benchmarks based on factual context. In this work, we study quantifier understanding by proposing the PRESQUE framework that formulates the problem in a pragmatics reasoning perspective and the format of NLI. And we collect a dataset QuRe that includes human annotated quantifiers and percentage mentions for Wikipedia sentences. From the experimental results on the HVD dataset and our collected QuRe dataset, the PRESQUE achieves better performance compared to the direct interpretation of quantifier semantics by a literal listener model."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to thank the anonymous reviewers for their suggestions and feedback on the work. This work was supported in part by NSF grant DRL2112635. The views contained in this article are those of the authors and not of the funding agency. We also thank Daniel Fried for useful discussion and suggestions to this work.\nLimitations\nIn this work, we investigate the quantifier understanding abilities of several foundation models and collect a dataset QuRe that we expect will substantially benefit research on quantifier semantics. However, despite the value of our dataset and the promising results from the PRESQUE framework, our analysis and findings have some notable limitations. First, we note that our study and dataset still focus on a small part of the generalized quantifiers and likely do not cover the entire spectrum of quantifier semantics. Second, the sentences in our dataset all come from Wikipedia. Consequently, the performance of PRESQUE and the generalizability of our findings to other domains or languages remains an open question. Finally, assigning precise percentage scopes to quantifiers can be a challenging or even impossible task, since quantifier semantics are complex and depend on many factors beyond those analyzed here. In particular, these may subjectively depend on the domain, an annotator\u2019s background of knowledge or culture, comfort with the mathematics of percentages, and Bayesian\nvs Frequentist interpretations of percentage numbers, among many other factors. Thus, ambiguities and subjectivity are natural when determining a quantifier\u2019s scope. Our dataset and analysis largely skirt many of these complex issues.\nEthics and Broader Impact\nWe employ crowdsourcing through Amazon Mechanical Turk for (a) certain annotations of our dataset, QuRe, (b) understanding human interpretation of quantifier semantics, and (c) human evaluation of PRESQUE and baselines. In all our crowdsourcing tasks we do not collect any personally identifiable information about the turkers and all the turkers were paid above minimum wage, which is included in Appendix D. We released our crowdsourcing tasks to turkers in the USA and constrained the approval rate of the turkers to be above 98% to ensure good-faith turkers.\nBesides, the prevalence of quantifiers in naturally occurring corpora would inherit the generation behavior of models. PRESQUE, as one step towards revealing the quantifier understanding ability of foundation models, could be helpful in more accurately interpreting the meaning in model-generated text. It could also support automatic pipelines for knowledge-intensive reasoning that include quantifications, or logical reasoning in natural language."
        },
        {
            "heading": "C Discussion",
            "text": "The semantic understanding of a pragmatic listener is proportional to the product of two entailmentbased probabilities, where the premise and hypothesis are flipped with respect to each other (i.e., the premise used for S0(q|p) is the hypothesis for calculating P (p|q) in the prior (Eq. 4) To arrive at an intuitive understanding of why considering the flipped premise-hypothesis pairs, we analyze the sensitivity of NLI models (specifically, RoBERTa fine-tuned NLI model) towards percentage values (quantifier inference) and quantifier words (percentage inference) in premises. The distribution of average entailment scores over all the premises is shown in Figure 6. The upper part of the figure shows the result of percentage inference and the lower part shows the result of quantifier inference,\nwith two thresholds, 0.1 and 0.5. We can observe that the NLI model is more sensitive to the percentage values in the premise than quantifiers. The entailment scores of percentage inference is relatively low, which is led by high neutral scores, making it challenging to identify the percentage scope for each quantifier. For example, few, some and most don\u2019t have any percentage values that exceed even the lower threshold. The lower half of Figure 6, however, demonstrates more interpretable entailment distribution, where the percentage scopes of few, some and most can be interpreted as 0%-20%, 0%-90% and 60%-100% by the higher threshold. In short, NLI models are more sensitive to interpreting accurate numerical premises, which has also been observed that NLI performs better with accurate premises (Thukral et al., 2021; Richardson et al., 2019) where premises with quantifiers are less accurate than premises with percentage values."
        },
        {
            "heading": "D Annotator Recruitment",
            "text": "We use Amazon Mechanical Turk as the crowdsourcing platform for different annotation aspects of QuRe as described in previous paragraphs. To finalize our pool of turkers, we released a qualification task to test basic understanding of quantifier semantics. The annotators are selected to base on the United States, completed more than 1000 HITs with more than 98% approval rate. For annotator that participates the final QuRe collection in Section 3, they can make at most one annotation mistake in sentence rephrasing of the qualification task. The qualification task had a pass rate of 38% and we recruited 18 turkers for the main annotation tasks of QuRe. Annotators are paid about $ 7.30/hr on average. Besides, annotators recruited for human interpretation of quantifiers are paid about $ 9/hr on average. And the annotators recruited for human preference of percentage scopes are paid about $ 9.60/hr on average."
        },
        {
            "heading": "E Percentage Scope Generation Details",
            "text": "With granularity g, window size w, the grounded percentage scope can be determined in Table 10.\nFor HVD, \u03b2 is set to be 0.1. And in experiments on QuRe, \u03b2, w, g are set to be 0.05, 2, 0.01 respectively unless specified."
        },
        {
            "heading": "F Human Preference of HVD Examples",
            "text": "Figure 7 shows PRESQUE is in general preferred over L0 by the annotators, while the preferences\nmay differ for different quantifiers. The primary scopes of PRESQUE for no, some and all are more preferred than L0 by the annotators. Some and all have p < 0.05 in chi-squared test."
        },
        {
            "heading": "G Distance-based Scope Evaluation",
            "text": "To measure the primary scope of L(p|q) and the gold scope [pmin, pmax] in distance-based metrics. We compute the minimal scope distance (MSD) over the top K predictions (MSD@K). Specifically,\nMSD@K = \u2211\np\u2032\u2208TopK p\nI[p\u2032 /\u2208 sgolden] Bm min(pmin \u2212 p\u2032, p\u2032 \u2212 pmax)\nwhere Bm = pmax \u2212 pmin + \u03b2, p\u2032 \u2208 W\u03b2\nH Instruction for Sentence Filtering\nIn this task, you will determine whether a given sentence that has one or more quantifier values mentioned can have those quantifier values replaced by a natural language quantifier like \u2019some\u2019, \u2019most\u2019 or \u2019generally\u2019.\nExample sentences that meet the criteria are like \u2019It consists of about 80% water, soluble minerals (nearly 3% with half of the potassium) and polyphenols.\u2019 and sentences that don\u2019t meet the requirement are like \u2019180.1 million were rides on SEPTA\u2019s \u2019city transit\u2019 network. Ridership had decreased 13% from 2014 to 2019 due to many factors.\u2019 where the percentage value represents incremental percentage changes or comparisons (e.g. \u2019drop by 50%\u2019 or \u201920% higher\u2019, \u2019X% better\u2019) instead of absolute percentages.\nDo you think the following sentence meets the requirement? Answer in Yes or No:\nI Instruction for Percentage Scope Generation\nThe instruction for percentage scope generation is shown below.\nIn this task, you will give a sentence and a quantifier expression in the sentence, and you need to convert that quantifier expression into a mathematical expression. For example, for the sentence \u2019About 30% of homes are owned outright by their occupants. [30%]\u2019, you are given 30% in the bracket, and the corresponding mathematical expression is 0.3, where means approximate. Similarly, all the other mathematical operations supported include > meaning \u2019more than\u2019 (e.g. \u2019more than 80%\u2019 would be > 0.8), < meaning \u2019less than\u2019, \u2019-\u2019 meaning range (e.g. \u201920% to 50%\u2019 would be 0.2-0.5) and null meaning exact (e.g. \u2019takes up 20%\u2019 would be 0.2). Answering the expression itself is enough, don\u2019t repeat the sentence or use additional English words other than the operations. Also, try to avoid using \u2019<\u2019 and \u2019>\u2019 if you can formulate the range by using \u2019-\u2019. Now, please do the same conversion for the following sentence:\nJ Instruction for Sentence Topic Generation\nPlease use three or four labels to categorize a given sentence (starts with \"sentence\"), including the topics of the contents, split with semicolons. For example, Sentence: In fact, a 2006 survey found that trapping as a solution to beaver problems had a 79% failure rate within two years due to resettlement by new beavers. Labels: scientific study; animal; rate Sentence: Among individual countries, the proportion of urban residents living in slum areas in 2009 was highest in the Central African Republic (95.9%), Chad (89.3%), Niger (81.7%), and Mozambique (80.5%). The distribution of slums within a city varies throughout the world. Labels: population; ranking; countries. Now, please label the following sentence:\nK Instruction for Human Evaluation\nThe instruction for collecting human perception of quantifier words in Section 5.1 is displayed as\nThis form contains several natural language quantifiers. The users are expected to pick one/two numerical percentages from the provided list of percentages such that most accurately bound the range of the given quantifier to the best of his/her knowledge and online searching is not encouraged.\nUsers can use different (real or imaginative) statements as examples to help estimate the range, such as \u2018No water comes from the sky.\u2019 and \u2018Most sea cucumbers are scavengers.\u2019.\nThe users can select no more than 2 options to mark the lower and upper bound of the range, if they believe only one percentage would apply, they can select only 1 option.\nAn example of \u2018All\u2019 stands for\u2019 with a statement is \u2018All sugars are white.\u2019 The users are expected to select \u2018100%\u2019 or a range (based on the user\u2019s understanding) from all provided percentages as the range for \u2018All\u2019, and the paraphrase becomes \u2018A% to B% sugars are white.\u2019 (A and B are the selections and can be the same) which becomes the most appropriate paraphrase of \u2018All sugars are white\u2019. Note that the statement itself does not necessarily involve factuality (in fact, sugars can have various colors).\nThe instruction for collecting listener preference of L0 and L1 in Section 5.2 is displayed as\nThis form contains several statements (e.g. sugars are white) with natural language quantifiers (e.g. all). The users are expected to pick the more appropriate percentage range from the provided two options such that accurately bound the range of the given quantifier to the best of his/her knowledge and online searching is not encouraged.\nAn example statement with quantifier is \u2019All sugars are white.\u2019, and two example options are \u20180%-30%\u2019 and \u201890%-100%\u2019.\nIn this example, the users are expected to select \u201890%- 100%\u2019, which results in that \u201890%-100% sugars are white.\u2019 better describes \u2018All sugars are white.\u2019. Note that the statement itself does not necessarily involve factuality (in fact, sugars can have various colors)."
        },
        {
            "heading": "L Quantifier Understanding of GPT-3.5-turbo",
            "text": "Although we mainly focus on NLI models to develop PRESQUE, we also test the performance of QuRe on GPT-3.5-turbo using the following instruction.\nIn this task, you are given a sentence (starts with \u2018Sentence:\u2019) containing a predicate with a quantifier, and you need to provide a percentage scope that the predicate satisfies.\nFor example, if you are given \u201cSentence: Some apples are red.\u201d for the quantifier \u2018some\u2019, and you believe 37%-42% apples are red. Then the percentage scope for \u201csome apples are red\u201d would be 37%-42%.\nThe scope you can choose should be rounded in the granularity of 5 %. In the previous \u2018apples are red\u2019 example, your answer will be \"35%-45%\". Not that the percentage value cannot exceed 0% and 100%. You can also select one single percentage value for the scope.\nPlease provide a percentage scope for \u201csome\u201d in the following sentence.\nSentence:\nIn the example instruction shown above, the quantifier some would be replaced by the target quantifier that appeared in the sentence attached to the instruction. For example, for sentence Adult clams can get most of their nutrients from the algae and the rest from filter feeding. (gold scope 65%- 100%), the output of GPT-turbo-3.5 for quantifier most is 60%-80%.\nOverall, GPT-3.5-turbo achieves 0.28 F1 score of the quantifier understanding task on QuRe, under the same configuration in Appendix E, which is slightly higher than the F1@5 performance of PRESQUE. However, we are aware that text-totext models like GPT-3.5-turbo still suffer from hallucination and the output is unstable due to temperature-based sampling. Meanwhile, the PRESQUE in this work is agnostic to the backbone model choices and can be applied to any models that score the entailment relation between sentences."
        },
        {
            "heading": "M Annotation Task Interface",
            "text": "The instruction for the qualification task in collecting QuRe is included in Figure 9, and Figure 10 shows the example tasks annotators need to complete."
        }
    ],
    "title": "Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models",
    "year": 2023
}