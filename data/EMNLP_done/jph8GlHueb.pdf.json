{
    "abstractText": "The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for everincreasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of deployable high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned on any downstream task. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-performance high throughput MUX-PLMs that are competitive with vanilla PLMs while achieving 2x/5x inference speedup with only a 1\u2212 4% performance drop on a broad suite of tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vishvak Murahari"
        },
        {
            "affiliations": [],
            "name": "Ameet Deshpande"
        },
        {
            "affiliations": [],
            "name": "Carlos E. Jimenez"
        },
        {
            "affiliations": [],
            "name": "Izhak Shafran"
        },
        {
            "affiliations": [],
            "name": "Mingqiu Wang"
        },
        {
            "affiliations": [],
            "name": "Yuan Cao"
        },
        {
            "affiliations": [],
            "name": "Karthik Narasimhan"
        }
    ],
    "id": "SP:3f8c537a89f114a560fe8f082c1df54e08906b38",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Bernie Huang",
                "Candace Ross",
                "Vladimir Karpukhin",
                "Hu Xu",
                "Naman Goyal",
                "Dmytro Okhonko",
                "Mandar Joshi",
                "Gargi Ghosh",
                "Mike Lewis"
            ],
            "title": "Cm3: A causal masked multimodal model of the internet",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Ainslie",
                "Santiago Onta\u00f1\u00f3n",
                "Chris Alberti",
                "Philip Pham",
                "Anirudh Ravula",
                "Sumit Sanghai."
            ],
            "title": "ETC: encoding long and structured data in transformers",
            "venue": "CoRR, abs/2004.08483.",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Akam",
                "Dimitri M Kullmann."
            ],
            "title": "Oscillatory multiplexing of population codes for selective communication in the mammalian brain",
            "venue": "Nature Reviews Neuroscience, 15(2):111\u2013122.",
            "year": 2014
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Francisca Blumhagen",
                "Peixin Zhu",
                "Jennifer Shum",
                "YanPing Zhang Sch\u00e4rer",
                "Emre Yaksi",
                "Karl Deisseroth",
                "Rainer W Friedrich."
            ],
            "title": "Neuronal filtering of multiplexed odour representations",
            "venue": "Nature, 479(7374):493\u2013498.",
            "year": 2011
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis for pre-trained bert networks",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "William B Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Rainer W Friedrich",
                "Christopher J Habermann",
                "Gilles Laurent."
            ],
            "title": "Multiplexing using synchrony in the zebrafish olfactory bulb",
            "venue": "Nature neuroscience, 7(8):862\u2013871.",
            "year": 2004
        },
        {
            "authors": [
                "Mitchell Gordon",
                "Kevin Duh",
                "Nicholas Andrews."
            ],
            "title": "Compressing BERT: Studying the effects of weight pruning on transfer learning",
            "venue": "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 143\u2013155, Online. Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Stefan Gr\u00fcnewald",
                "Prisca Piccirilli",
                "Annemarie Friedrich."
            ],
            "title": "Coordinate constructions in english enhanced universal dependencies: Analysis and computational modeling",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Marton Havasi",
                "Rodolphe Jenatton",
                "Stanislav Fort",
                "Jeremiah Zhe Liu",
                "Jasper Snoek",
                "Balaji Lakshminarayanan",
                "Andrew Mingbo Dai",
                "Dustin Tran"
            ],
            "title": "Training independent subnetworks for robust",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "ArXiv, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Sungho Hong",
                "Mario Negrello",
                "Marc Junker",
                "Aleksandra Smilgin",
                "Peter Thier",
                "Erik De Schutter."
            ],
            "title": "Multiplexed coding by cerebellar purkinje neurons",
            "venue": "Elife, 5:e13810.",
            "year": 2016
        },
        {
            "authors": [
                "Ting Jiang",
                "Deqing Wang",
                "Fuzhen Zhuang"
            ],
            "title": "Pruning pre-trained language models without finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "TinyBERT: Distilling BERT for natural language understanding",
            "venue": "pages 4163\u20134174.",
            "year": 2020
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2020
        },
        {
            "authors": [
                "Fran\u00e7ois Lagunas",
                "Ella Charlaix",
                "Victor Sanh",
                "Alexander M Rush."
            ],
            "title": "Block pruning for faster transformers",
            "venue": "arXiv preprint arXiv:2109.04838.",
            "year": 2021
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "Thirteenth international conference on the principles of knowledge representation and reasoning.",
            "year": 2012
        },
        {
            "authors": [
                "Zhuang Liu",
                "Mingjie Sun",
                "Tinghui Zhou",
                "Gao Huang",
                "Trevor Darrell."
            ],
            "title": "Rethinking the value of network pruning",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Paul Michel",
                "Omer Levy",
                "Graham Neubig."
            ],
            "title": "Are sixteen heads really better than one? In Advances in Neural Information Processing Systems, volume 32",
            "venue": "Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Vishvak Murahari",
                "Carlos E Jimenez",
                "Runzhe Yang",
                "Karthik R Narasimhan."
            ],
            "title": "DataMUX: Data multiplexing for neural networks",
            "venue": "Thirty-Sixth Conference on Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Friederice Pirschel",
                "Jutta Kretzberg."
            ],
            "title": "Multiplexed population coding of stimulus properties by leech mechanosensory cells",
            "venue": "Journal of Neuroscience, 36(13):3636\u20133647.",
            "year": 2016
        },
        {
            "authors": [
                "Sai Prasanna",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "When BERT Plays the Lottery, All Tickets Are Winning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3208\u20133229, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan"
            ],
            "title": "Improving language understanding by generative pretraining",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre Ram\u00e9",
                "R\u00e9my Sun",
                "Matthieu Cord."
            ],
            "title": "Mixmo: Mixing multiple inputs for multiple outputs via deep subnetworks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 823\u2013833.",
            "year": 2021
        },
        {
            "authors": [
                "Ronald L Rivest",
                "Adi Shamir",
                "Leonard Adleman."
            ],
            "title": "A method for obtaining digital signatures and public-key cryptosystems",
            "venue": "Communications of the ACM, 21(2):120\u2013126.",
            "year": 1978
        },
        {
            "authors": [
                "Erik Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
            "venue": "CoNLL.",
            "year": 2003
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Alexander Rush."
            ],
            "title": "Movement pruning: Adaptive sparsity by fine-tuning",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 20378\u201320389. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhen Dong",
                "Jiayu Ye",
                "Linjian Ma",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W. Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Q-bert: Hessian based ultra low precision quantization of bert",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on empirical",
            "year": 2013
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Hongkun Yu",
                "Xiaodan Song",
                "Renjie Liu",
                "Yiming Yang",
                "Denny Zhou."
            ],
            "title": "MobileBERT: a compact task-agnostic bert for resource-limited devices",
            "venue": "pages 2158\u20132170.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam M. Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Elena Voita",
                "David Talbot",
                "Fedor Moiseev",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Ziheng Wang",
                "Jeremy Wohlwend",
                "Tao Lei."
            ],
            "title": "Structured pruning of large language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "Transactions of the Association for Computational Linguistics, 7:625\u2013641.",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "Structured pruning learns compact and accurate models",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2022
        },
        {
            "authors": [
                "Ziqing Yang",
                "Yiming Cui",
                "Zhigang Chen."
            ],
            "title": "TextPruner: A model pruning toolkit for pretrained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2022
        },
        {
            "authors": [
                "Yichun Yin",
                "Cheng Chen",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Qun Liu."
            ],
            "title": "AutoTinyBERT: Automatic hyper-parameter optimization for efficient pre-trained language models",
            "venue": "pages 5146\u20135157.",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8bert: Quantized 8bit bert",
            "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), pages 36\u201339.",
            "year": 2019
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Ariel Larey",
                "Guy Boudoukh",
                "Haihao Shen",
                "Moshe Wasserblat."
            ],
            "title": "Prune once for all: Sparse pre-trained language models",
            "venue": "arXiv preprint arXiv:2111.05754.",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "The IEEE International",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Language models like ChatGPT (OpenAI, 2023), PaLM (Chowdhery et al., 2022), T5 (Raffel et al., 2020), and CM3 (Aghajanyan et al., 2022), have seen unprecedented adoption in diverse sectors ranging from education and healthcare to manufacturing and marketing. The proficiency of these tools has led to unprecedented demand for these models, with users facing frequent outages and capacity limits. Additionally, ever-increasing model sizes and hardware shortages have constrained models\u2019 ability to handle a very high load of requests, thus limiting large-scale affordable access to these models. These trends bring into focus the need for high-throughput, high-performance, efficient, and environmentally responsible models\nthat can be deployed at scale to meet the quickly growing demand.\nMulti-input Multi-output architectures (MIMO) (Havasi et al., 2021; Ram\u00e9 et al., 2021; Murahari et al., 2022) are a promising hardware-agnostic and architecture-agnostic paradigm that perform inference for multiple inputs simultaneously at the cost of a single input. This efficiency paradigm is natively geared towards yielding high-throughput models, in addition to being complementary in approach and motivation to current efficiency methods such as pruning, quantization, and distillation. Interestingly, MIMO approaches are partly inspired by the human brain\u2019s extraordinary ability to process multiple inputs and propagate information at a high bandwidth with a few neural codes (Blumhagen et al., 2011; Akam and Kullmann, 2014; Pirschel and Kretzberg, 2016; Hong et al., 2016; Friedrich et al., 2004).\nMurahari et al. (2022) introduced data multiplexing, a MIMO technique that can enable a many-fold increase in throughput. The method compresses N different instances into a single \u201cmultiplexed\u201d hidden representation before decompressing it into N independent predictions. While they show the plausibility of MIMO training, their method leads to a significant drop in performance (20 \u2212 30% points) compared to state-of-the-art models.\nIn this work, we introduce MUX-PLMs, a class of high-throughput pre-trained language models trained in a MIMO fashion with data multiplexing to process multiple inputs (2-10) simultaneously with a forward pass over a single instance. MUX-PLMs offer up to 400% improvement in throughput over baseline pre-trained models while only being \u223c 4 points and \u223c 2 points worse than baseline pre-trained language models for text classification and token classification tasks, respectively. MUX-PLMs, like other pre-trained language models, provide general model initialization that can be fine-tuned for any downstream\ntask. We demonstrate the effectiveness and generality of our MUX-PLMs class of pre-trained models by training MUX-BERT and MUX-ELECTRA models, which are trained with pre-trained objectives adapted from BERT (Devlin et al., 2019) and ELECTRA (Clark et al., 2020) respectively, although in a MIMO fashion with data multiplexing.\nOur work is the first to introduce MIMO architectures to PLMs. To enable this, we first develop a new demultiplexing module, RSA-demux (Figure 2), that randomly initializes and learns private key vectors to recover the multiple outputs from a multiplexed representation. Secondly, we introduce a new Contextual Multiplexer module (Figure 3) that uses a cross-instance attention-based mechanism to aggregate context across the set of multiplexed instances, which seems to be particularly effective for token-level tasks. Thirdly, our three-stage training algorithm (Figure 1) enables stable and efficient training of MUX-PLMs.\nImportantly, MUX-PLMs are complementary to existing state-of-the-art model compression techniques. We hope our work validates MIMO architectures as a promising complementary direction to existing efficiency techniques. Consequently, we hope future research develops MIMO architectures in tandem with other efficiency approaches, leveraging the best of both paradigms. We will publicly release our models and code to promote opensource research on the next generation of MIMO\narchitectures for large language models."
        },
        {
            "heading": "2 Related Work",
            "text": "Efficient Inference with Transformers Recent methods in NLP rely heavily on transfer learning through Transformer-based (Vaswani et al., 2017) language models trained on large text corpora using self-supervised objectives, such as autoregressive (Radford and Narasimhan, 2018) or masked language modeling (Devlin et al., 2019). Prior analysis on pre-training language models has observed power-law scaling of model performance with respect to model size (Kaplan et al., 2020), leading the community to develop ever-larger language models. It is also generally recognized that pre-trained language models are significantly overparameterized; effectively learning a subnetwork that utilizes only a relatively small number of their total parameters (Voita et al., 2019; Michel et al., 2019; Gordon et al., 2020; Prasanna et al., 2020).\nThe ubiquity of pre-trained language models, their growing size, and over-parameterization has inspired extensive research on improving inference efficiency. This includes methods such as structured pruning (Liu et al., 2019; Wang et al., 2020; Lagunas et al., 2021; Xia et al., 2022; Yang et al., 2022), knowledge distillation (Hinton et al., 2015; Sanh et al., 2019; Sun et al., 2020; Jiao et al., 2020; Yin et al., 2021), quantization (Zafrir et al., 2019; Shen et al., 2020), and data multiplexing (Mura-\nhari et al., 2022). These approaches assume that PLMs are highly over-parametrized and attempt to approximate a large function by learning a smaller, compressed, version of the original model. Past work has also focused on unstructured pruning for both task finetuning (Chen et al., 2020; Sanh et al., 2020) and pre-trained (Zafrir et al., 2021; Jiang et al., 2022) language model settings, but don\u2019t increase model throughput due to hardware limits.\nMulti-input Multi-output Models While pruning, quantization, and distillation seek to reduce overparameterization by reducing models\u2019 representational capacity, other lines of work seek to exploit overparameterization in other ways. Multiinput Multi-output (MIMO) architectures (Havasi et al., 2021; Ram\u00e9 et al., 2021; Murahari et al., 2022) train models using mixed-instance representations, i.e. Zhang et al. (2018), in order to obtain predictions for multiple instances simultaneously. Unlike efficiency methods, Havasi et al. (2021) and Ram\u00e9 et al. (2021) try to obtain better performance by inducing multiple subnetworks in a single convolutional model to perform \u201censembling for free\u201d during inference. Data multiplexing, introduced in DataMUX (Murahari et al., 2022), aims to improve model efficiency by training Transformer models with mixed-instance representations to perform simultaneous inference for language tasks, thereby improving inference throughput many-fold. Currently, MIMO architectures have only been used in a limited setting, achieving middling performance. Our work training PLMs with a potent MIMO architecture, data multiplexing, dramatically improves inference throughput while preserving high accuracy for downstream tasks."
        },
        {
            "heading": "3 Methodology",
            "text": "We briefly introduce readers to the data multiplexing MIMO architecture (Murahari et al., 2022), which we denote T-MUX. We then detail our novel approach to train MUX-PLMs to yield highthroughput and performant language models."
        },
        {
            "heading": "3.1 T-MUX: Data multiplexing with Transformer",
            "text": "Data multiplexing allows parallel processing of multiple sequences with a single forward or backward pass through the model (M ) and requires two crucial components, multiplexer, and demultiplexer.\nMultiplexer The multiplexer module (MUX) combines an ordered set of multiple inputs \u2013 X1:N = ( x1, . . . ,xN ) into a single superimposed representation (xMUX). If xi \u2208 Rd, the multiplexer is a transformation (MUX : RN\u00d7d \u2192 Rd) such that xMUX = MUX ( X1:N ) .\nTo ensure MUX is an order-preserving transformation,T-MUX samples a vector (vi \u2208 Rd) from a standard multivariate Gaussian and applies the Hadamard product (element-wise multiplication) with the corresponding input representation (xi) before summing up vectors for all positions.\nxMUX = MUX ( X1:N ) = 1\nN N\u2211 i=1 xi \u2299 vi\nvi \u2208 Rd \u223c N (0, I)\n(1)\nThe model processes the multiplexed representation and emits a multiplexed hidden state \u2013 hMUX = M ( xMUX ) . To multiplex Transformers\u2019 sequenced\ninputs ( xi = ( xi1, . . . ,x i L )) of length L, T-MUX applies the same vi to all L positions of instance i.\nxMUX = MUX ( X1:N ) =(\n1\nN N\u2211 i=1 xi1 \u2299 vi, . . . , 1 N N\u2211 i=1 xiL \u2299 vi ) (2)\nDemultiplexer A prediction needs to be made for each instance in X1:N , and T-MUX\u2019s demultiplexer module (DeMUX) achieves this by separating the superimposed output (hMUX) into N output representations corresponding to the input (h1, . . . ,hN ).\nhi = DeMUX ( hMUX,pi ) hij = DeMUX ( hMUXj ,p i ) (3)\nThe vector pi \u2208 Rd is dynamically generated for each instance (i) with the help of a prefix that is added to the input and re-used for all positions in the instance. They add a prefixi to x\ni, represented by the following pattern, where \u03f5i is a special token, and pi is set to be the output corresponding to token i in the prefix.\nprefix1 = [\u03f51, \u03f5pad, . . . , \u03f5pad]\nprefix2 = [\u03f5pad, \u03f52, \u03f5pad, . . . , \u03f5pad]\n\u00b7 \u00b7 \u00b7 prefixN = [\u03f5pad, . . . , \u03f5pad, \u03f5N ]"
        },
        {
            "heading": "3.2 MUX-PLMs: Data multiplexing for high-throughput language models",
            "text": "We propose MUX-PLMs, a class of highthroughput pre-trained Transformer-based language models trained in a MIMO fashion with data multiplexing. To demonstrate the viability and the generality of this class of models, we pretrain Transformer models with objectives based on BERT and ELECTRA, to get MUX-BERT and MUX-ELECTRA respectively. MUX-PLMs are trained with our three stage training algorithm (Figure 1). Firstly, MUX-PLMs are trained with the token retrieval task in T-MUX, which is an auto-\nencoding setup to decode all the tokens in the multiplexed input. This simple auto-encoding task is critical to prime the model for MIMO-style data multiplexing. The MUX-PLMs are then pre-trained with standard pre-training objectives but adapted to MIMO-fashioned training with data multiplexing. MUX-PLMs show significant throughput improvement over standard pre-trained LMs while matching their downstream task accuracies. Finally, MUX-PLMs, like other pre-trained language models, provide general model initialization that can be fine-tuned for any downstream task.\nContextual multiplexer T-MUX\u2019s multiplexer multiplexes tokens independent of 1) tokens in the same position in other instances and 2) other tokens in the instance, which could lead to suboptimal representations. We, therefore, explore a contextual multiplexing scheme that aggregates context both from tokens in the same instance and tokens in the same position of other instances (Figure 3). We first use a single transformer layer TRANSctx to get contextual representations hictx = TRANSctx ( xi1, . . . ,x i L ) ) of length L. We apply a hadamard product with a multivariate gaussian vi to all L positions.\ngictx = h i ctx \u2299 vi (4)\nWe generate multiplexed representations, xMUX, by applying another transformer layer TRANSinst across encoded representations from N instances at each position from 1 to L. This is done by transposing gctx and applying TRANSinst.\nxMUX = TRANSinst ( g\u22a4ctx ) (5)\nRSA demultiplexer The demultiplexer in TMUX requires a prefix whose length scales lin-\nModel N GLUE Token \u2197 Mean (std) Max Mean (std) Max\nBERT 1 85.4 (0.0) 85.4 95.8 (0.0) 95.8 1.0\u00d7ELECTRA 82.1 (0.0) 82.1 95.3 (0.0) 95.3 1.0\u00d7\nT-MUX 2 60.4 (0.6) 61.8 81.4 (0.1) 81.5 1.9\u00d7 MUX-BERT\u2021 82.5 (0.6) 83.4 95.2 (0.1) 95.4 2.0\u00d7 MUX-ELEC\u2021 82.5 (0.4) 83.1 95.0 (0.0) 95.1 2.0\u00d7\nT-MUX 5 59.7 (0.6) 60.6 81.3 (0.2) 81.5 4.4\u00d7 MUX-BERT\u2021 80.3 (0.4) 80.9 93.6 (0.1) 93.6 4.9\u00d7 MUX-ELEC\u2021 79.8 (0.6) 80.5 93.4 (0.0) 93.5 4.9\u00d7\nT-MUX 10 58.1 (0.5) 59.1 79.7 (0.2) 80.0 8.4\u00d7 MUX-BERT\u2021 77.8 (0.6) 78.8 91.6 (0.1) 91.8 9.8\u00d7 MUX-ELEC\u2021 78.2 (0.6) 79.0 91.7 (0.1) 91.8 9.7\u00d7\nTable 1: Average GLUE and token-level classification scores for the BASE (L=12, H=768) configuration, across ELECTRA, BERT, and MUX-PLMs for N \u2208 {1, 2, 5, 10}. \u2021 indicates our models and \u2197 indicates throughput increase w.r.t. to a vanilla BERTBASE model. All models are evaluated on 5 seeds with mean and max scores reported.\nearly with the number of instances (N ), thus reducing the effective context length during pre-training, which degrades performance (Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020). Furthermore, it decreases throughput during inference for large N because the model must process an extra prefix of length N for each of the N instances. To address these issues, we draw inspiration from the RSA cryptosystem (Rivest et al., 1978) to randomly initialize and learn N (private) key vectors (k1, . . . ,kN , ki \u2208 Rd) which are keys that can be used to demultiplex the output representation (Figure 2).\nhi = DeMUX ( hMUX,ki ) hij = DeMUX ( hMUXj ,k i ) (6)\nAkin to RSA, vi and ki can be treated as the keys for multiplexing (encryption) and demultiplexing (decryption) while ensuring that, unlike T-MUX, the input sequence length does not change and thereby leading to an improvement in throughput. Importantly, this architecture ensures that the keys better transfer across the different stages of training as they are no longer conditioned on the input instances."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Datasets We pre-train all models on Wikipedia (Foundation) and Bookscorpus (Zhu et al., 2015). We evaluate on all datasets from the GLUE benchmark (Wang et al., 2018), which\nare CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QQP (qqp), STS-B (Cer et al., 2017), MNLI (Williams et al., 2018), QNLI (Wang et al., 2018), RTE (Wang et al., 2018), and WNLI (Levesque et al., 2012). We also evaluate on token classification tasks such as named entity recognition (Sang and Meulder, 2003) and POS tagging (Gr\u00fcnewald et al., 2021). We don\u2019t report average over WNLI and CoLA as these are the two smallest tasks in GLUE and we observe high variance across different seeds. All numbers are reported on the dev split. We report scores for all tasks in Appendix E.\nModels We experiment with ELECTRA and BERT pre-training objectives and present the pre-trained multiplexed models MUX-BERT and MUX-ELECTRA for N = 2, 5 and 10. To simplify training, we use a random generator to train MUX-ELECTRA models, presented as an ablation in Clark et al. (2020), instead of using a smaller masked LM. Except where otherwise noted, we do not use the contextual MUX module, but instead, use the RSA demultiplexing module. Refer to Appendix B and C for implementation details.\nBaselines We run experiments to compare our models against T-MUX, from Murahari et al. (2022) and baseline PLMs - ELECTRA and BERT, across three different model configurations (SMALL, BASE, and LARGE). We also provide a comparison to results reported in recent PLM pruning and distillation papers in Table 2.\nMulti-run evaluation We evaluate all models across 5 random seeds to reduce variance for smaller datasets which is caused by the randomized order in which we multiplex instances in the batch. We report both the average and maximum scores across seeds in Table 1 to understand the importance of ordering the multiplexed instances and report average across seeds for all other results."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 MUX-PLMs outperform PLMs and T-MUX",
            "text": "Table 1 shows that both MUX-BERT and MUXELECTRA outperform T-MUX at all levels of multiplexing (N ), with improvements between 12 and 20 points on GLUE and token-classification tasks respectively. Furthermore, MUX-PLMs\u2019 efficient RSA-inspired demultiplexing method allows\nit to achieve faster throughput than T-MUX, increasing it by over 16% for N = 10.\nMoreover, MUX-PLMs provide a significant boost in throughput (N times faster) when compared to PLMs, without a significant loss in performance. For example, MUX-ELECTRA (N = 2) is 0.4 points better and only 0.3 points worse than ELECTRA for GLUE and TOKEN tasks respectively, while being 2\u00d7 faster. Similarly, MUX-BERT (N = 2) is within 3 and 0.6 points of BERT for GLUE and TOKEN tasks respectively, while being significantly faster. We also observe that as N increases, MUX-PLMs\u2019 throughput is significantly better, though performance compared to PLMs can degrade. This is because a large N implies that MUX-PLMs must parallelly process more instances, thus having to share network parameters and activations with a larger number of instances, thus improving throughput and degrading performance. For example, the gap between ELECTRA and MUX-ELECTRA on TOKEN for N = 2 is 0.2 points and increases to 3.5 points for N = 10, which shows that N serves as a parameter to control the performance-throughput trade-off. We explore this further in Section 5.3 and Figure 4."
        },
        {
            "heading": "5.2 Comparing MUX-PLMs with other model compression methods",
            "text": "We compare our MUX-PLM models with other efficient learning methods, such as pruning and distillation, in Table 2. Contrary to other methods, our vanilla MUX-PLMs achieve competitive performance and significant throughput improvement\nwithout additional unlabeled and task-specific data, and can be easily fine-tuned on any downstream task without any architectural modifications (unlike pruning). For instance, when compared to DistilBERT, MUX-BERT (N = 2) does 1 point worse on QNLI and 2 points better on QQP while being equally fast and not requiring any additional unlabeled data.\nMore broadly, methods like CoFi, AutoTinyBERT, and MobileBERT show that combining qualitatively different efficiency paradigms is a promising approach towards efficient models. For example, CoFi combines structured pruning and knowledge distillation, and AutoTinyBERT combines knowledge distillation and neural architecture search. This places importance on discovering new efficiency paradigms that dovetail with existing ones, rather than making incremental changes to current techniques.\nGiven that MIMO architectures like data multiplexing propose a novel direction towards efficiency, we demonstrate its complementary nature by combining it with pruning. We perform structured pruning using CoFi (sparsity of 0.6) on our MUX-PLMs. The resulting pruned model is 2\u00d7 faster than the original MUX-PLM while being as performant. We believe that the best combination of efficiency techniques is largely hardware and use-case dependent and we hope that MIMO architectures evolve in tandem with other approaches."
        },
        {
            "heading": "5.3 Effect of varying model size",
            "text": "In this section, we show that our multiplexing techniques work on a host of model sizes and report results for MUX-BERT on three models sizes, SMALL, BASE, and LARGE for N = 2 (Table 3). We report results for other values of N in the appendix. MUX-BERT\u2019s performance is\nclose to that of BERT for all model sizes while having a significantly better throughput (the gap is less than 0.7 points for TOKEN tasks and 2.9 points for GLUE for close to twice the throughput). Multiplexing works effectively on all model sizes, with the drops with respect to BERT being 1.6 and 1.7 points on GLUE for SMALL and LARGE respectively. MUX-BERT\u2019s throughput is always \u2248 2\u00d7 that of BERT, which shows that a spectrum of MUX-PLM model sizes can be multiplexed during pre-training with competitive performance and with significantly higher throughput.\nPre-trained models typically have a performancecomputational efficiency trade-off, with larger models having better performance but worse computational efficiency. MUX-PLMs offers a similar trade-off, with large N leading to better throughput but lower performance. To understand this trade-off, we plot the performance and throughput of BERT and MUX-BERT for different model sizes and draw the pareto-optimal envelope (Figure 4). For any model on the envelope, no model\nhas both better accuracy and throughput. Users would only choose models on the envelope because for every model within the envelope, there always exists a model on the envelope which has both better performance and throughput. We note that all multiplexed models lie either on or very close to the Pareto frontier, for both TOKEN and GLUE tasks. This suggests that given an accuracy threshold, MUX-PLM models will usually be faster than PLMs. For instance, if we wanted the highest throughput model with a performance \u2265 77% on GLUE, the optimal BERT model is the SMALL configuration with a throughput of 2815 (in/s), but for the MUX-BERT model would be the N = 2 with the SMALL configuration, achieving a significantly higher throughput of 5539 (in/s)."
        },
        {
            "heading": "5.4 Ensembling MUX-PLMs",
            "text": "As opposed to feeding N different instances to MUX-PLMs to improve throughput, we consider an alternate setting where we feed the same instance N times and build an ensemble by averaging the N class logits to make a single prediction. We randomly permute the batch, after duplicating the instance N times, to prevent distribution shift. We use the BASE size models for N \u2208 {2, 5, 10} for both MUX-BERT and MUX-ELECTRA (Table 4). The ensemble model does significantly better than the non-ensemble variant on both MNLI and QQP for all values of N (e.g., 1.6 and 0.9 points on N = 5 MUX-BERT for the two tasks). We note that the improvement over the nonensemble variant (\u2206) is better for higher N , due to the larger ensemble size. This result shows that nonensemble variants are faster but perform slightly worse, while the ensemble variant performs better but is slower. A spectrum of models lie between\nthese two extremes, where only a fraction of the N multiplexed representations can be ensembled, allowing users to trade off performance and speed."
        },
        {
            "heading": "6 Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Ablation study",
            "text": "We analyze multiplexing and demultiplexing components of MUX-PLMs and report the results in Table 5. We consider two variants, one which uses the prefix demultiplexing proposed in T-MUX instead of our proposed RSA-DeMUX and another which uses Contextual multiplexing instead of Noncontextual. We note that Variant 1, which uses prefix demultiplexing, performs worse than our MUXBERT, other than for N = 2. In fact, Variant 1 does not converge for TOKEN tasks for N = 5 and N = 10 and performs 1.7 and 1.2 points worse on GLUE when compared to MUX-BERT.\nVariant 2 uses Contextual multiplexing which takes into account other tokens present in the instance and also tokens present in the same position of other instances. This variant performs better than Non-contextual for TOKEN tasks (almost over 1.7 points on TOKEN for N = 10) but performs worse for GLUE tasks. We believe that Contextual multiplexing\u2019s better performance in TOKEN is because the model needs to make a prediction for every single position in the instance, which requires it to efficiently multiplex all token positions in the output. However, for GLUE tasks, the model needs to make a prediction only for the [CLS] token, for which Non-contextual multiplexing suffices."
        },
        {
            "heading": "2 83.1 82.0 1.1 83.4 81.8 1.6",
            "text": ""
        },
        {
            "heading": "5 80.5 78.9 1.6 80.9 79.7 1.2",
            "text": ""
        },
        {
            "heading": "10 79.0 77.3 1.7 78.8 77.0 1.8",
            "text": ""
        },
        {
            "heading": "6.2 Effect of data sampling strategies during inference",
            "text": "During inference, our MUX-PLMs sample N instances uniformly at random from the evaluation set. However, other data-sampling strategies such as clustering similar instances based on wordoverlap could improve performance. We explore the effect of composition of N instances on the performance of MUX-PLMs in Table 6. For each model variant, we consider 5 random seeds which can be viewed as lottery tickets (Frankle and Carbin, 2018). Since the random seed controls the composition of N instances, we measure the difference (\u2206) between the best-performing ticket and the worst-performing ticket and average the performance for all the GLUE tasks. \u2206 is consistently greater than 1 point for all values of N for both MUX-ELECTRA and MUX-BERT, and illustrates the importance of the composition of N instances. An improved data sampling strategy could lead to improvements and we leave this to future work."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce MUX-PLMs, a class of highthroughput pre-trained language models trained with data multiplexing, a multi-input multi-output (MIMO) architecture. Our MUX-PLMs models, trained with novel MIMO modules, are competitive with state-of-the-art PLMs on several downstream tasks while achieving a many-fold increase in inference throughput. MUX-PLMs, similar to standard PLMs, can be fine-tuned on any downstream task to yield high-throughput, high-performance models. We hope our work inspires future research in MIMO architectures for PLMs as a complementary efficiency paradigm to existing approaches."
        },
        {
            "heading": "8 Limitations",
            "text": "Our MUX-PLMs class of high-throughput highperformance pre-trained models demonstrates the efficacy of the MIMO paradigm for language models. However, MUX-PLMs need MIMO-style training for both pre-training and fine-tuning. It would be more efficient to introduce MIMO-style training only during the fine-tuning stage as it would allow stakeholders to rapidly create high-throughput models from any off-the-shelve pre-trained model. We conducted initial experiments in this setting but faced issues getting models to converge and perform well. This setting is incredibly challenging as off-the-shelf pre-trained models have been trained to only process one instance at a time and introducing our novel multiplexing and demultiplexing modules would create a large distribution shift for the model. We hope that future work focuses on innovative solutions to address these limitations."
        },
        {
            "heading": "A Appendices",
            "text": ""
        },
        {
            "heading": "B Pre-training Details",
            "text": "We report all pre-training related hyper-parameters in Table 7. We primarily use the HuggingFace Transformers implementations for BERT and ELECTRA based models. All pre-training experiments were run on 8 A100 GPUs with distributed training. We run a small hyper-parameter search over over two learning rates. All pre-trained models are primed with the token retrieval task introduced in Murahari et al. (2022). We train on the Wikipedia and Bookscorpus datasets for up to 10000 training steps with a learning rate of 1e\u2212 4, and with a sequence length of 512.\nFor MUX-ELECTRA models, we don\u2019t train a generator as in the original ELECTRA work, but only use uniform-random token replacement. This is similar to what was used in ablations in ELECTRA (Clark et al., 2020). The generator randomly replaces 15% of tokens in the input with other tokens in the vocabulary."
        },
        {
            "heading": "C Fine-tuning Details",
            "text": "We report all the fine-tuning related hyper-parameters in Table 8. We run a small hyper-parameter search on the learning rate, batch size and number of training steps for different tasks. All models were trained with half-precision. We report numbers on the validation split. For GLUE tasks, we use the default metrics in Wang et al. (2018) and use F1 for the token-level tasks. All fine-tuning experiments were trained on 1 V100 GPU.\nSpeedup calculation For all models, we calculate throughput (samples/second) on a single V100 GPU and report throughput gains with respect to the BERTBASE model. We calculate throughput by averaging across 3 different trials (1 trial = 200 mini-batches) and use a batch size of 128 and a sequence length of 128 following prior work (Xia et al., 2022). We measure throughput for sequence-classification tasks on QQP and measure throughput for token-level classification tasks on named entity recognition."
        },
        {
            "heading": "D Analysis details",
            "text": "D.1 Ensembling results setup We find that multiplexing the same instance by duplicating the instance N times leads to worse performance. This is likely because this input configuration is very out of distribution from what the multiplexed models are trained on. To address this, we randomly permute the instances in the batch after duplicating the instances N times. This ensures that the input to the multiplexer lies in a similar distribution to what the model was trained on.\nD.2 Muxology setup To analyze the hidden states of pre-trained MUX-BERT models at different layers, we take the average absolute value of hidden states and every layer for both multiplexed and baseline models, across different configurations. To analyze the entropies of the attention distributions at different layers, we calculate the attention distribution across different attention heads for each position in the sequence length. To measure how peaky the attention distribution is likely to be, we calculate the entropies of the attention distributions at all positions and average across all the positions and across all the attention heads to get the average entropy for all layers. We conduct this analysis on WikiText-103 and average across all the samples in the evaluation split.\nE Task performance breakdown for all variants"
        }
    ],
    "title": "MUX-PLMs: Data Multiplexing for High-throughput Language Models",
    "year": 2023
}