{
    "abstractText": "Evaluating the performance of Grammatical Error Correction (GEC) systems is a challenging task due to its subjectivity. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. However, mainstream evaluation metrics, i.e., referencebased metrics, introduce bias into the multireference evaluation by extracting edits without considering the presence of multiple references. To overcome this issue, we propose ChunkLEvel Multi-reference Evaluation (CLEME), designed to evaluate GEC systems in the multireference evaluation setting. CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and references, thus eliminating the bias caused by inconsistent edit boundaries. Furthermore, we observe the consistent boundary could also act as the boundary of grammatical errors, based on which the F0.5 score is then computed following the correction independence assumption. We conduct experiments on six English reference sets based on the CoNLL-2014 shared task. Extensive experiments and detailed analyses demonstrate the correctness of our discovery and the effectiveness of CLEME. Further analysis reveals that CLEME is robust to evaluate GEC systems across reference sets with varying numbers of references and annotation styles 1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jingheng Ye"
        },
        {
            "affiliations": [],
            "name": "Yinghui Li"
        },
        {
            "affiliations": [],
            "name": "Qingyu Zhou"
        },
        {
            "affiliations": [],
            "name": "Yangning Li"
        },
        {
            "affiliations": [],
            "name": "Shirong Ma"
        },
        {
            "affiliations": [],
            "name": "Hai-Tao Zheng"
        },
        {
            "affiliations": [],
            "name": "Ying Shen"
        }
    ],
    "id": "SP:6fcc033a88bd40d38e1d40e26bf695bf5b77cec9",
    "references": [
        {
            "authors": [
                "Hiroki Asano",
                "Tomoya Mizumoto",
                "Kentaro Inui."
            ],
            "title": "Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "Ted Briscoe."
            ],
            "title": "Automatic annotation and evaluation of error types for grammatical error correction",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Bryant",
                "Hwee Tou Ng"
            ],
            "title": "How far are we from fully automatic high quality grammatical error correction",
            "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
            "year": 2015
        },
        {
            "authors": [
                "Christopher Bryant",
                "Zheng Yuan",
                "Muhammad Reza Qorib",
                "Hannan Cao",
                "Hwee Tou Ng",
                "Ted Briscoe."
            ],
            "title": "Grammatical error correction: A survey of the state of the art",
            "venue": "arXiv preprint arXiv:2211.05166.",
            "year": 2022
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Cameron Fordyce",
                "Philipp Koehn",
                "Christof Monz",
                "Josh Schroeder."
            ],
            "title": "Further meta-evaluation of machine translation",
            "venue": "Proceedings of the Third Workshop on Statistical Machine Translation, pages 70\u2013106, Columbus, Ohio. Associ-",
            "year": 2008
        },
        {
            "authors": [
                "Martin Chodorow",
                "Markus Dickinson",
                "Ross Israel",
                "Joel Tetreault."
            ],
            "title": "Problems in evaluating grammatical error detection systems",
            "venue": "Proceedings of COLING 2012, pages 611\u2013628, Mumbai, India. The COLING 2012 Organizing Committee.",
            "year": 2012
        },
        {
            "authors": [
                "Shamil Chollampatt",
                "Hwee Tou Ng."
            ],
            "title": "A reassessment of reference-based grammatical error correction metrics",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2730\u20132741, Santa Fe, New Mexico, USA. As-",
            "year": 2018
        },
        {
            "authors": [
                "Leshem Choshen",
                "Omri Abend."
            ],
            "title": "Automatic metric validation for grammatical error correction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1372\u20131382, Melbourne, Aus-",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and psychological measurement, 20(1):37\u201346.",
            "year": 1960
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Deutsch",
                "Rotem Dror",
                "Dan Roth."
            ],
            "title": "On the limitations of reference-free evaluations of generated text",
            "venue": "arXiv preprint arXiv:2210.12563.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Mariano Felice",
                "Ted Briscoe."
            ],
            "title": "Towards a standard evaluation method for grammatical error detection and correction",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2015
        },
        {
            "authors": [
                "Peiyuan Gong",
                "Xuebo Liu",
                "Heyan Huang",
                "Min Zhang."
            ],
            "title": "Revisiting grammatical error correction evaluation and beyond",
            "venue": "arXiv preprint arXiv:2211.01635.",
            "year": 2022
        },
        {
            "authors": [
                "Takumi Gotou",
                "Ryo Nagata",
                "Masato Mita",
                "Kazuaki Hanawa."
            ],
            "title": "Taking the correction difficulty into account in grammatical error correction evaluation",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 2085\u20132095,",
            "year": 2020
        },
        {
            "authors": [
                "Roman Grundkiewicz",
                "Marcin Junczys-Dowmunt",
                "Edward Gillian."
            ],
            "title": "Human evaluation of grammatical error correction systems",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461\u2013470.",
            "year": 2015
        },
        {
            "authors": [
                "Md Asadul Islam",
                "Enrico Magnani."
            ],
            "title": "Is this the end of the gold standard? a straightforward referenceless grammatical error correction metric",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3009\u20133015,",
            "year": 2021
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Sho Takase",
                "Ayana Niwa",
                "Naoaki Okazaki."
            ],
            "title": "Interpretability for language learners using example-based grammatical error correction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Yinghui Li",
                "Haojing Huang",
                "Shirong Ma",
                "Yong Jiang",
                "Yangning Li",
                "Feng Zhou",
                "Hai-Tao Zheng",
                "Qingyu Zhou."
            ],
            "title": "On the (in)effectiveness of large language models for chinese text correction",
            "venue": "CoRR, abs/2307.09007.",
            "year": 2023
        },
        {
            "authors": [
                "Yinghui Li",
                "Shirong Ma",
                "Qingyu Zhou",
                "Zhongli Li",
                "Li Yangning",
                "Shulin Huang",
                "Ruiyang Liu",
                "Chao Li",
                "Yunbo Cao",
                "Haitao Zheng."
            ],
            "title": "Learning from the dictionary: Heterogeneous knowledge guided fine-tuning for Chinese spell checking",
            "venue": "Findings",
            "year": 2022
        },
        {
            "authors": [
                "Yinghui Li",
                "Qingyu Zhou",
                "Yangning Li",
                "Zhongli Li",
                "Ruiyang Liu",
                "Rongyi Sun",
                "Zizhen Wang",
                "Chao Li",
                "Yunbo Cao",
                "Hai-Tao Zheng"
            ],
            "title": "2022b. The past mistake is the future wisdom: Error-driven contrastive probability optimization for Chinese spell checking",
            "year": 2022
        },
        {
            "authors": [
                "Zuchao Li",
                "Kevin Parnow",
                "Masao Utiyama",
                "Eiichiro Sumita",
                "Hai Zhao."
            ],
            "title": "MiSS: An assistant for multi-style simultaneous translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2021
        },
        {
            "authors": [
                "Shirong Ma",
                "Yinghui Li",
                "Haojing Huang",
                "Shulin Huang",
                "Yangning Li",
                "Hai-Tao Zheng",
                "Ying Shen."
            ],
            "title": "Progressive multi-task learning framework for chinese text error correction",
            "venue": "CoRR, abs/2306.17447.",
            "year": 2023
        },
        {
            "authors": [
                "Shirong Ma",
                "Yinghui Li",
                "Rongyi Sun",
                "Qingyu Zhou",
                "Shulin Huang",
                "Ding Zhang",
                "Li Yangning",
                "Ruiyang Liu",
                "Zhongli Li",
                "Yunbo Cao",
                "Haitao Zheng",
                "Ying Shen"
            ],
            "title": "Linguistic rules-based corpus generation for native Chinese grammatical error correc",
            "year": 2022
        },
        {
            "authors": [
                "Matou\u0161 Mach\u00e1\u010dek",
                "Ond\u0159ej Bojar."
            ],
            "title": "Results of the WMT13 metrics shared task",
            "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45\u201351, Sofia, Bulgaria. Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Koki Maeda",
                "Masahiro Kaneko",
                "Naoaki Okazaki."
            ],
            "title": "IMPARA: Impact-based metric for GEC using parallel data",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 3578\u20133588, Gyeongju, Republic of Korea. In-",
            "year": 2022
        },
        {
            "authors": [
                "Courtney Napoles",
                "Maria N\u0103dejde",
                "Joel Tetreault."
            ],
            "title": "Enabling robust grammatical error correction in new domains: Data sets, metrics, and analyses",
            "venue": "Transactions of the Association for Computational Linguistics, 7:551\u2013566.",
            "year": 2019
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Ground truth for grammatical error correction metrics",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-",
            "year": 2015
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Joel Tetreault."
            ],
            "title": "There\u2019s no comparison: Referenceless evaluation metrics in grammatical error correction",
            "venue": "Proceedings of the 2016 Conference on",
            "year": 2016
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Joel Tetreault."
            ],
            "title": "JFLEG: A fluency corpus and benchmark for grammatical error correction",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
            "year": 2017
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The CoNLL-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natu-",
            "year": 2014
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Courtney Napoles",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality",
            "venue": "Transactions of the Association for Computational Linguistics, 4:169\u2013182.",
            "year": 2016
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Matt Post",
                "Benjamin Van Durme."
            ],
            "title": "Efficient elicitation of annotations for human evaluation of machine translation",
            "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1\u201311, Baltimore, Maryland,",
            "year": 2014
        },
        {
            "authors": [
                "Jingheng Ye",
                "Yinghui Li",
                "Yangning Li",
                "Hai-Tao Zheng"
            ],
            "title": "2023a. Mixedit: Revisiting data augmentation and beyond for grammatical error correction",
            "year": 2023
        },
        {
            "authors": [
                "Jingheng Ye",
                "Yinghui Li",
                "Shirong Ma",
                "Rui Xie",
                "Wei Wu",
                "Hai-Tao Zheng."
            ],
            "title": "Focus is what you need for chinese grammatical error correction",
            "venue": "CoRR, abs/2210.12692.",
            "year": 2022
        },
        {
            "authors": [
                "Jingheng Ye",
                "Yinghui Li",
                "Haitao Zheng."
            ],
            "title": "System report for CCL23-eval task 7: THU KELab (sz) - exploring data augmentation and denoising for Chinese grammatical error correction",
            "venue": "Proceedings of the 22nd Chinese National Conference on",
            "year": 2023
        },
        {
            "authors": [
                "Ryoma Yoshimura",
                "Masahiro Kaneko",
                "Tomoyuki Kajiwara",
                "Mamoru Komachi."
            ],
            "title": "SOME: Reference-less sub-metrics optimized for manual evaluations of grammatical error correction",
            "venue": "Proceedings of the 28th International Conference",
            "year": 2020
        },
        {
            "authors": [
                "Ding Zhang",
                "Yinghui Li",
                "Qingyu Zhou",
                "Shirong Ma",
                "Yangning Li",
                "Yunbo Cao",
                "Hai-Tao Zheng."
            ],
            "title": "Contextual similarity is more valuable than character similarity: An empirical study for chinese spell checking",
            "venue": "ICASSP 2023 - 2023 IEEE Interna-",
            "year": 2023
        },
        {
            "authors": [
                "Yue Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Jiacheng Li",
                "Bo Zhang",
                "Chen Li",
                "Fei Huang",
                "Min Zhang."
            ],
            "title": "MuCGEC: a multi-reference multi-source evaluation dataset for Chinese grammatical error correction",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Grammatical Error Correction (GEC) is a task that involves making local substitutions to correct grammatical errors in a given ungrammatical text (Bryant et al., 2022; Ma et al., 2022; Ye et al., 2022; Ma et al., 2023). The practical value of GEC in daily life has led to increasing attention being paid to this task (Li et al., 2021, 2022a,b; Kaneko \u2217indicates equal contribution. \u2020Corresponding author: Hai-Tao Zheng. (E-mail: zheng.haitao@sz.tsinghua.edu.cn) 1All the source codes of CLEME are released at https:// github.com/THUKElab/CLEME.\net al., 2022; Li et al., 2023; Ye et al., 2023a,b; Zhang et al., 2023). However, it is intractable to evaluate GEC systems due to the highly subjective nature of the task and the low inter-annotator agreement (IAA) (Choshen and Abend, 2018). Therefore, most datasets improve compatibility by incorporating multiple references to guarantee a more realistic evaluation of the model performance.\nThere are two broad categories of GEC metrics: reference-based and reference-less. Referencebased metrics evaluate GEC systems by comparing their hypotheses and human-annotated references in terms of edits (Dahlmeier and Ng, 2012; Bryant et al., 2017) or n-grams (Napoles et al., 2015). Reference-less metrics are proposed to evaluate GEC systems without references. However, Deutsch et al. (2022) demonstrate that reference-less metrics are inherently biased and limited in their ability to evaluate generated text. Therefore, we focus on reference-based metrics, which can evaluate in an interpretable manner, thus providing useful insights for model analysis.\nFigure 1 illustrates how existing reference-based metrics, such as ERRANT, extract the edit and then compute the F0.5 score by comparing hypotheses and references. However, these metrics often fail to consider multiple references, which can result in bias during multi-reference evaluation. We argue that this bias arises because the current approach rewards equally good corrections unfairly. For in-\nstance, the ungrammatical phrase the technologies were is equally well-corrected by both Ref. 1 and Ref. 2. However, if a hypothesis aligns with Ref. 1\u2019s corrections (i.e., [the \u2192 \u03f5] and [were \u2192 have], TP=2), it will be rewarded less than the corrections of Ref. 2 (i.e., [the \u2192 \u03f5], [technologies \u2192 technology] and [were \u2192 has], TP=3).\nIn this paper, we propose Chunk-LEvel Multireference Evaluation (CLEME), which enables unbiased F0.5 scores for GEC multi-reference evaluation. Inspired by (Gotou et al., 2020), CLEME transforms the source, the hypothesis and all the references into chunk sequences with consistent boundaries, thereby eliminating the bias in GEC multi-reference evaluation.\nExisting metrics assume that corrections of grammatical errors are dependent. That is, whenever there is more than one reference for a source, the metrics try each reference in turn, and then the highest score is taken as the final score. However, we observe that grammatical errors corrections in terms of chunks can be considered approximately independent. For example, the ungrammatical phrases the technologies were and for shown in Figure 1 can be corrected independently, i.e., the correction of the technologies were has no bearing on the correction of for. Based on this observation, we compute F0.5 scores following the assumption that corrections of grammatical errors are independent. Specifically, we iterate through the chunks of a hypothesis and consider a chunk correct if it matches any of the corresponding chunks in the references. In this case, the hypothesis in Figure 1 would be rewarded 2TP, rather than 1TP and 1FP, which is the traditional case. To demonstrate the effectiveness and robustness of CLEME, we conduct experiments on six English reference sets with varying numbers of references and annotation styles, either calculating the F0.5 score at the corpus- or sentence-level.\nIn summary, our contributions are three folds:\n(1) We propose CLEME, a reference-based metric that evaluates GEC systems at the chunk-level, aiming to provide unbiased F0.5 scores for GEC multi-reference evaluation.\n(2) We observe that the corrections of grammatical errors in terms of chunks are approximately independent. Therefore, we propose to compute F0.5 scores based on the correction independence assumption.\n(3) Extensive experiments and human evaluation are conducted to confirm the effectiveness and robustness of our approach."
        },
        {
            "heading": "2 Preliminary Study",
            "text": ""
        },
        {
            "heading": "2.1 Consistent Boundaries",
            "text": "We determine consistent chunk-level boundaries by chunk partition process to debias the multireference evaluation, as depicted in Figure 2. We first extract the edit sets of the hypothesis and references, and then merge the overlapping edits into a chunk. It\u2019s worth noting that the source, hypothesis and references are all segmented into chunk sequences with the same number of chunks, regardless of the number of their tokens. This process is straightforward since we can locate and examine all possible corrections of an erroneous chunk. For example, the chunk by the can be corrected in two ways, i.e., with in Ref. 1 and through in Ref. 2. The resulting chunks fall into three categories: 1) unchanged chunks, which contain the same text segments as the source sentence, 2) corrected chunks, which consist of non-empty text segments different from the source sentence, and 3) dummy chunks are empty chunks."
        },
        {
            "heading": "2.2 Boundaries of Grammatical Errors",
            "text": "Figure 2 illustrates the merging of overlapping edits into either corrected or dummy chunks, which are then separated by unchanged chunks. This raises the question, are chunk boundaries the boundaries of grammatical errors?\nDataset. To answer the question, we conduct experiments on BN-10GEC (Bryant and Ng, 2015). The dataset comprises 1,312 source sentences that are identical to the CoNLL-2014 test data (Ng et al., 2014). Each source sentence is associated with 10 references annotated by 10 native English speakers, including two official annotators of CoNLL-2014, the first author of the paper, and seven freelancers recruited via an online recruitment website.\nExperiment Setup. For each source sentence, we sample 9 references and run the chunk partition process described in Section 2.1. The resulting chunk sequences are determined collectively by all 9 references. The edits of the remaining reference {e1, \u00b7 \u00b7 \u00b7 , eM} are then used to calculate the following three statistics: 1) The In-CorrectedChunk (ICC) ratio indicates the proportion of edits included by corrected/dummy chunks of the other\nreferences. An edit is included by a chunk if the interval of the edit falls within that of the chunk. 2) The In-Unchanged-Chunk (IUC) ratio gives the proportion of edits included by unchanged chunks of the other references. 3) The Cross-Chunk (CC) ratio computes the proportion of edits that extend beyond the original boundaries. These statistics are calculated as follows:\nICC = 1\nM M\u2211 i=1 f1(ei), (1)\nIUC = 1\nM M\u2211 i=1 f2(ei), (2)\nCC = 1\u2212 ICC \u2212 IUC, (3)\nwhere M is the number of edits from the remaining reference. If the edit ei is included in a corrected/dummy chunk, the function f1(ei) returns 1, otherwise 0. Likewise, if the edit ei is included in an unchanged chunk, the function f2(ei) returns 1, otherwise 0. We sample 9 different references for chunk partition in each run and repeatedly calculate the statistics using the remaining reference.\nResults. As shown in Table 1, the number of corrected and dummy chunks are less than that of edits since overlapping edits are merged into a chunk. A total of 90.66% edits are included by the corrected/dummy chunks, which suggests the grammatical errors to be corrected have been considered by the other references. However, only 7.74% edits are included by corrected chunks, indicating that these edits may be over-corrected since the other references believe no grammatical errors needed correction. Interestingly, 1.61% edits cross the chunk boundaries, suggesting that the chunk boundaries are stable enough to serve as the boundaries of grammatical errors to some extent. Additionally, human evaluation in Section 4.2 could be used as another argument to support this conclusion. Therefore, we have the following assumption.\nCorrection independence assumption: grammatical error corrections are independent.\nThat is, the correction of a grammatical error does not impact the correction of other grammatical errors. With this assumption, F0.5 scores can be calculated using an alternate method, which will be introduced in Section 3."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Chunk Evaluation",
            "text": "As shown in Figure 2, each chunk consists of edit operation(s), start index, end index, and correct tokens. Conventional reference-based metrics such as MaxMatch (M2) and ERRANT compute F0.5 scores based on the correction dependence assumption. They evaluate the performance for each reference separately and select the one that yields the best result for the source sentence. CLEMEdependent also computes F0.5 scores in this way by treating corrected/dummy chunks as edits. On the other hand, CLEME-independent is proposed to compute F0.5 scores based on the correction independence assumption. A corrected/dummy chunk from the hypothesis is considered correct if it matches one of the corresponding chunks from the references. It is worth noting that CLEME is able to fully inherit pre-classified errors from ERRANT, where each corrected/dummy chunk may consist of multiple error with different types."
        },
        {
            "heading": "3.2 Length Weighting",
            "text": "The average length of chunks is much longer than that of edits shown in Table 1, resulting in the unfairness of chunk evaluation if a longer chunk is rewarded equally with a shorter one. Therefore, we add length weighting to the chunk evaluation. The intuition of length weighting is to compensate for long chunk matching. The weights of True Positives (TPs), False Positives (FPs), and False Negatives (FNs) are computed as follows:2\nwTP = clip (\n\u03b11 1 + (\u03b11 \u2212 1) exp(\u2113\u2212 x) , cmin, cmax\n) , (4)\nwFP = clip (\n\u03b12 1 + (\u03b12 \u2212 1) exp(x\u2212 \u2113) , cmin, cmax\n) , (5)\n2We do not apply length weighting to TNs since it is unnecessary for F0.5 scores.\nwFN = clip (\n\u03b13 1 + (\u03b13 \u2212 1) exp(\u2113\u2212 x) , cmin, cmax\n) , (6)\nwhere \u03b11, \u03b12 and \u03b13 are scale factors for TPs, FPs and FNs respectively, x is the length of the chunk, \u2113 is the average length of chunks, and the function clip(v, a, b) clips the value v between a and b. The curves of length weighting are depicted in Figure 3. Formally, given a system corrected/dummy chunk set CH and a gold corrected/dummy chunk set CR, we apply length weighting on each chunk to compute precision, recall and F0.5 as follows:\nP =\n\u2211 c\u2208CH\u2229CR\nwTPc\u2211 c\u2208CH\u2229CR wTPc + \u2211 c\u2208CH\\CR wFPc , (7)\nR =\n\u2211 c\u2208CH\u2229CR\nwTPc\u2211 c\u2208CH\u2229CR wTPc + \u2211 c\u2208CR\\CH wFNc , (8)\nF\u03b2 = (1 + \u03b2 2) \u00b7 P \u00b7R\n(\u03b22 \u00b7 P ) +R, (9)\nwhere \u03b2 = 0.5 is usually used, which weighs precision twice as much as recall."
        },
        {
            "heading": "3.3 Corpus-level v.s. Sentence-level",
            "text": "We compute F0.5 scores of GEC systems at both corpus-level and sentence-level following (Gong et al., 2022). Corpus-level metrics compute an F0.5 score over the entire dataset. Sentence-level metrics compute an F0.5 score over each sentence of the dataset and evaluate GEC systems by us-\ning the average F0.5 score. CLEME-dependent and CLEME-independent are corpus-level metrics, and their sentence-level variants are respectively SentCLEME-dependent and SentCLEMEindependent. Both levels of the GEC metric are developed to provide more user-friendly options. Sentence-level metrics should be used if consistent evaluation weight for each sample is desired. This ensures that the evaluation result of each sample has the same influence on the final score. On the other hand, if harder samples containing more edits should have larger weight, then corpus-level metrics should be used instead."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Correlations with Human Judgments",
            "text": "Dataset. To verify the effectiveness of CLEME, we measure correlations between reference-based metrics and human judgments on multiple English reference sets, including CoNLL-2014 (Grundkiewicz et al., 2015), BN-10GEC (Bryant and Ng, 2015) and SN-8GEC (Sakaguchi et al., 2016). All the reference sets are based on CoNLL-2014 (Ng et al., 2014), consisting of 1,312 source sentences. SN-8GEC collected 8 references sets of annotations from both experts and non-experts, including 4 sets of minimal edits and 4 sets of fluency edits (2 by experts and 2 by non-experts). Reference sets statistics are reported in Appendix A.\nThe human judgments for the outputs of 13 GEC systems (including the unchanged source text) are presented by (Grundkiewicz et al., 2015), where eight native speaker were asked to rank the output of all the systems from best to worst. Two system ranking lists are generated using Expected Wins (EW) (Mach\u00e1c\u030cek and Bojar, 2013) and TrueSkill (TS) (Sakaguchi et al., 2014) respectively.\nExperiment Settings. Following (Gong et al., 2022; Chollampatt and Ng, 2018), we compute the Pearson \u03b3 and Spearman correlation coefficient \u03c1 between reference-based metrics and human judgments based on corpus-level ranking. We tune the hyperparameters on CoNLL-2014 and keep the hyperparameters on the other reference sets, in order to demonstrate the adaptability of our approach. The detailed hyperparameters of CLEME are reported in Appendix B.\nEvaluation Metrics. We compare our approach with the following reference-based metrics, includ-\ning corpus- and sentence-level variants 3:\n\u2022 GLEU and SentGLEU (Napoles et al., 2015) are n-gram based metrics, which reward hypothesis n-grams that overlap with the reference but not the source and penalize hypothesis n-grams that overlap with the source but not the reference.\n\u2022 M2 and SentM2 (Dahlmeier and Ng, 2012) dynamically extract the hypothesis edits with the maximum overlap of gold annotations.\n\u2022 ERRANAT and SentERRANT (Bryant et al., 2017) extract edits by utilizing a linguisticallyenhanced alignment algorithm.\n\u2022 PT-M2 and SentPT-M2 (Gong et al., 2022) are recently proposed reference and PLMbased GEC metric, which score edits using the knowledge of pre-trained language model.\nAdditionally, CLEME can evaluate GEC systems by accuracy scores, which is usually not implemented by conventional reference-based metrics. Please refer to Appendix C for the introduction and analyses of evaluating GEC systems by accuracy.\nResults. Table 2 reports the correlations between reference-based metrics and human judgments. For the corpus-level metrics, GLEU achieves the highest correlations on BN-10GEC and NE-fluency reference sets. However, GLEU suffers from negative correlations on NE-Minimal, which is caused by low-quality annotations 4 of NE-Minimal, indicating that GLEU may not be a robust metric, consistent with the findings of (Sakaguchi et al., 2016). ERRANT performs slightly better than M2 on most reference sets, while PT-M2 is a strong corpus-level metric, which achieves the highest or comparable correlations on all reference sets at the cost of more than 10\u00d7 running time than other reference-based metrics. Our proposed CLEME-dependent and CLEME-independent make better use of consistent chunk boundaries, thus performing slightly better than M2 and ERRANT on most reference sets. Notably, CLEME-independent achieves comparable performance to CLEME-dependent, showing the\n3We do not experiment with I-measure (Felice and Briscoe, 2015) due to its negative correlation and high computing complexity (Grundkiewicz et al., 2015). 4The phenomenon exists on all sentence-level metrics. We remove unchanged references from some reference sets to avoid it.\neffectiveness of computing F0.5 scores based on the correction independence assumption.\nThe majority of the sentence-level metrics outperform their corpus-level counterparts because they weigh samples equally, which is in line with the bias of human annotation. Despite the strong performance of PT-M2, SentPT-M2 achieves lower correlations on BN-10GEC, E-Fluency and NEFluency compared to other sentence-level metrics. It suggests that scoring edits using pre-trained language models may not generalize well to unseen reference sets for sentence-level metrics. Our approach aligns better with human judgments than existing reference-based metrics for most reference sets. Specifically, SentCLEME-dependent performs best on BN-10GEC and NE-Fluency, and performs on a par with the best metric on E-Fluency, indicating it is more suitable for fluent reference sets. This phenomenon aligns with our intuition since fluent editing is more likely to follow the correction dependence assumption. In contrast, SentCLEME-independent achieves higher correlations on E-Minimal and NE-Minimal, as we would expect from minimal editing that is more likely to follow the correction independence assumption. These results suggests that reference sets may have a preference towards one of the correction assump-\ntions. Additionally, our approach achieves higher correlations on (N)E-Fluency rather than (N)EMinimal, while SentM2 and SentERRANT perform worse on E-Fluency than E-Minimal. This is because CLEME evaluates GEC systems using longer chunks rather than scrappy edits, which could better reflect whether a grammatical error is fluently corrected. Overall, our approach achieves higher or comparable correlations on sentencelevel than existing reference-based methods."
        },
        {
            "heading": "4.2 Human Evaluation",
            "text": "Experiments have shown the effectiveness of evaluating GEC systems based on the correction independence assumption. In this section, we aim to demonstrate whether the correction independence assumption makes sense for humans. We define the correction independence of a pair of chunks as the irrelevance of the correction of one chunk to the correction of the other. A simple case is presented in Appendix E. To evaluate this assumption, we conduct human evaluation experiments on 1,000 sentences randomly sampled from BN10GEC (Bryant and Ng, 2015). Three annotators were asked to judge whether a pair of chunks is correction-independent.\nTable 3 reports the ratio of correction indepen-\nAnnotator Ratio of Correction Independence\ndence and Cohen\u2019s-\u03ba (Cohen, 1960) inter-annotator agreement (IAA) across the three annotators. Results show that more than 90% pairs of chunks are correction-independent for all the annotators, indicating that it is reasonable to evaluate GEC systems based on the correction independence assumption. Moreover, considering the subjectivity of GEC task, the IAA statistics show that it is relatively easy to judge whether a pair of chunks is correction-independent, compared with the previous study (Bryant and Ng, 2015) 5."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 False Negative",
            "text": "We observe that the number of false negatives (FNs) identified by CLEME is significantly lower than that of ERRANT. This difference can be attributed to the distinct definitions used by each system. While ERRANT considers FNs as edits in the reference that do not match those made in the hypothesis, CLEME identifies FNs as corrected/dummy chunks in the reference that do not match the chunks in the hypothesis. We argue the definition of ERRANT is problematic, as it tends to\n5Bryant and Ng (2015) attempted to compute IAA at the sentence level. Three raters were asked simply to decide whether 200 sentences were correct or not. The authors reported IAA of just 0.16, 0.4 and 0.23.\noverestimate FN counts in grammatical error correction (GEC) systems, which is evident from the examples presented in Table 4. On the other hand, CLEME\u2019s definition also includes true negatives (TNs), making it possible to calculate accuracy."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "We present ablation analyses of our approaches on BN-10GEC - we have similar findings on other reference sets. We report Pearson correlations \u03b3 using Expected Wins ranking. The trend is similar for Spearson correlations and TrueSkill ranking.\nNumber of References. Since CLEME is designed for multi-reference evaluation, it degrades to conventional reference-based metrics such as M2 and ERRANT when only one reference is available. Here we demonstrate how correlations change against an increasing number of available references. The results reported in Figure 4 indicate that the correlations of corpus-level metrics do not change significantly with the increasing number of available references. However, except for SentGLEU, correlations of sentence-level metric are consistently higher than corpus-level metrics, and steadily increase with more references. Therefore, we recommend evaluating GEC systems using sentence-level metrics rather than corpus-level\nmetrics for the multi-reference evaluation setting.\nParameter Sensitivity Analysis. The scale factors introduced in Section 3.2 dictate how much the weights of chunks change with their length. We report the corrections for various scale factors, as shown in Figure 5. The results demonstrate that CLEME is resilient to hyperparameter selection."
        },
        {
            "heading": "5.3 Case Study",
            "text": "Table 5 presents additional examples of CLEME. In the top group, chunk 2 and chunk 4 of the hypothesis respectively match those of Ref. 1 and Ref. 2. In this case, CLEME-dependent gives TP=1 and FP=1, while CLEME-independent gives TP=2. In the second group, the hypothesis exactly corrects the ungrammatical word firghtenning in chunk 4. However, it cannot be rewarded since the entire chunk is not corrected. In the bottom group, two given references have made extensive modifications, with an unchanged chunk young people. Evaluating hypotheses in terms of chunks is generally more challenging than fragmented edits, but it provides a more comprehensive diagnosis.\nEven though there are larger grammatical errors spanning a significant portion of a sentence, CLEME would not necessarily collapse, i.e., producing one single correction chunk spanning the entire sentence. If collapse happens, the quality of the reference set should be checked first. This is because that collapse happens only if the input sentences of chunk partition are completely different, resulting in a trivial chunk partition result, which is an extreme case that has not been observed in our\nexperiments."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Reference-based Metrics",
            "text": "Reference-based metrics score GEC systems under the guidance of manually written references. M2 scorer (Dahlmeier and Ng, 2012) determines an optimal edit sequence between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation. The performance of each system is then represented using the F0.5 score. However, optimality in terms of overlap does not guarantee optimality in GEC evaluation. Bryant et al. (2017) showed that M2 scorer exploits its dynamic edit boundary prediction to artificially maximize true positives and minimize false positives, thus producing slightly inflated scores. Therefore, (Bryant et al., 2017) proposed ERRANT, which improves edit extraction using a linguistically-enhanced alignment algorithm and merging rules, improving the alignment of tokens with similar linguistic properties. Despite its effectiveness, ERRANT is language-dependent and bias still exists in multi-reference evaluation. Inspired by BLEU (Papineni et al., 2002) in NMT, Napoles\net al. (2015) proposed GLEU, an n-gram based metric for GEC evaluation. To remedy the shortcoming that F0.5 is unable to differentiate a do-nothing system and a bad system unless TP > 0, I-measure (Felice and Briscoe, 2015) generates an exact (global optimal) alignment using a three-way alignment algorithm and computes weighted accuracy to score GEC systems in terms of relative textual improvement. The comparison of reference-based GEC metrics is shown in Table 6."
        },
        {
            "heading": "6.2 Reference-less Metrics",
            "text": "To avoid the prerequisite of references for GEC evaluation, recent works focus on scoring GEC systems without human-annotated references. Inspired by quality estimation in neural machine translation, Napoles et al. (2016) propose three Grammaticality-Based Metrics (GBMs), which are calculated by a benchmark GEC system or a pretrained ridge regression model. Asano et al. (2017) extend GBMs by introducing three assessment criteria for Grammaticality, Fluency and Meaning preservation (GFM). SOME (Yoshimura et al., 2020) further improves GFM by optimizing each Grammaticality, Fluency and Meaning preservation metric to more closely correlate with human judgements. Scribendi Score (Islam and Magnani, 2021) overcomes the limitations of SOME, requiring neither a benchmark GEC system nor fine-tuning. IMPARA (Maeda et al., 2022) comprises a quality estimator (QE) and similarity estimator (SE) based on BERT (Devlin et al., 2019), which evaluate the quality of GEC output and semantic similarity of two sentences, respectively.\nAlthough recent reference-less metrics may be highly consistent with human judgments, they always suffer from the lack of interpretability and robustness (Bryant et al., 2022), which are crucial factors for GEC evaluation. Additionally, evaluating GEC systems by leveraging pre-trained or fine-tuned language models could pose potential risks. Efficiency of reference-less metrics that rely on language models is also critical if they are used for GEC benchmarks."
        },
        {
            "heading": "6.3 Meta Evaluation Methods",
            "text": "It is intractable to determine the best GEC metric. A reasonable GEC metric should take into account multiple factors, including correlation with human judgments, interpretability and efficiency. Inspired by WMT human evaluation campaigns (CallisonBurch et al., 2008), 13 system outputs (includ-\ning the unchanged source) from the CoNLL-2014 shared task (Ng et al., 2014) were ranked based on human rankings collected by two ranking methods: Expected Wins (EW) and TrueSkill (TS) (Grundkiewicz et al., 2015). Sakaguchi et al. (2016) collected 8 (2\u00d7 2\u00d7 2) annotations (minimal and fluency, expert and non-expert, with two corrections each), revealing that GEC metrics work differently across reference sets. Napoles et al. (2019) explored how GEC metrics operate in new domains (formal and informal writing by native English speakers). They constructed a multi-reference GEC test set called GMEG-Data and a new ensemble metric GMEG-Metric."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper proposes CLEME, a reference-based GEC metric that aim to provide unbiased F0.5 scores for multi-reference evaluation. We explore evaluating GEC systems based on either the correction dependence assumption or the correction independence assumption. Several possible approaches can be suggested to further improve CLEME. For example, developing (1) a GEC metric that adaptively combines dependent and independent assumptions, and (2) a weighting strategy by utilizing the knowledge of pre-trained model. In the future, we would like to develop CLEME for all languages and admonstrate the effectiveness of CLEME on other languages. It is also worthwhile to explore accuracy-based metrics.\nLimitations\nAlthough CLEME can be extended to other languages, we have not tested its effectiveness in any language other than English. Furthermore, all the reference sets used in our experiments are based on the CoNLL-2014 shared task, a secondlanguage dataset. To demonstrate the robustness of our approaches, further experiments on evaluation datasets with multiple text domains are required. We believe that introducing the correction independence assumption perspective into GEC datasets of other languages and domains could lead to more in-depth analysis and exploration.\nWhile recent PLM-based metrics have shown superior correlations compared to reference-based metrics, including ours on some reference sets, our approach enables the evaluation of GEC systems in an interpretable manner, which is a significant advantage over reference-less metrics. We leave the\nexploration of incorporating the PLM\u2019s knowledge into CLEME for future work.\nEthics Statement\nIn this paper, we verify the effectiveness of our proposed approach using CoNLL-2014, BN-10GEC, and SN-8GEC reference sets, all of which are from publicly available datasets or resources on legitimate websites without sensitive data involved. All the baselines used in our experiments are also publicly available GEC metrics and we have cited the corresponding authors. We confirm that all datasets and baselines used in our experiments are consistent with their intended use.\nAdditionally, we conduct human evaluation experiments to show the rationality of correction independence assumption. To do so, three postgraduate students specializing in foreign linguistics and applied linguistics were employed as part-time annotators. Each annotator could complete the entire annotation process within approximately 6 working hours. All annotators were paid for their work, with an average salary of approximately $5 per hour."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research is supported by National Natural Science Foundation of China (Grant No.62276154), Research Center for Computer Network (Shenzhen) Ministry of Education, the Natural Science Foundation of Guangdong Province (Grant No. 2023A1515012914), Basic Research Fund of Shenzhen City (Grant No. JCYJ20210324120012033 and JSGG20210802154402007), the Major Key Project of PCL for Experiments and Applications (PCL2021A06), and Overseas Cooperation Research Fund of Tsinghua Shenzhen International Graduate School (HW2021008)."
        },
        {
            "heading": "A Statistics of Reference Sets",
            "text": "Table 7 presents the statistics of all reference sets involved in our experiments, including In-CorrectedChunk (ICC) ratio, Unchanged-Chunk (IUC) ratio and Cross-Chunk (CC) ratio. It is worth noting that all reference sets exhibit a low CC ratio with varying ICC and IUC ratios, indicating the rationality and feasibility of evaluating GEC systems following the correction independence assumption."
        },
        {
            "heading": "B Hyperparameters",
            "text": "The hyperparameters of our proposed CLEME consist of scale factors \u03b1 and thresholds. We tune the hyperparameters on CoNLL-2014 and keep them on the other reference sets to demonstrate the adaptability of our method. The hyperparameters of CLEME are listed in Table 8."
        },
        {
            "heading": "C Evaluate by Accuracy",
            "text": "Conventional reference-based metrics such as MaxMatch (M2) and ERRANT are unable to calculate accuracy because they do not define True Negatives (TNs) 6. In order to implement the computation of accuracy, CLEME defines TNs as hypothesis unchanged chunks that match the chunks of references. Similar to F0.5, accuracy can be computed based on correction dependence or independence assumptions in both corpus- and sentence-level, resulting in four new variants: 1) CLEME-dependent-acc, 2) CLEMEindependent-acc, 3) SentCLEME-dependentacc, and 4) SentCLEME-independent-acc. 6An exception is I-measure (Felice and Briscoe, 2015), which adopts an extended version of the Writer-Annotation-System evaluation scheme (Chodorow et al., 2012).\nThe results of human correlations are reported in Table 9. Accuracy-based metrics perform very differently at the corpus- and sentence-level, which is similar to the findings (Napoles et al., 2016, 2019). Surprisingly, two accuracy-based corpus-level metrics, i.e., CLEME-dependent-acc and CLEMEindependent-acc, result in negative correlations on all reference sets. However, their sentencelevel variants, i.e., SentCLEME-dependent-acc and SentCLEME-independent-acc, perform well and achieve the highest correlations on some reference sets. Regarding the disparity between the performance of accuracy-based metrics and F0.5-based metrics at the sentence level, one notable difference is their stability or robustness on reference sets with varying numbers of references and annotation styles. F0.5-based metrics are more robust to different reference sets, where SentCLEME(in)dependent achieves comparable correlations with the best metric on all reference sets. However, the performance of accuracy-based metrics lags far behind other metrics on some reference sets (BN10GEC, E-Minimal and NE-Fluency). A deeper investigation into this phenomenon is needed to understand the instability of accuracy-based metrics. We leave the exploration and further analysis of accuracy-based metric for future work."
        },
        {
            "heading": "D Detailed Analysis",
            "text": "Table 10 reports the detailed evaluation results of 13 systems on CoNLL-2014. The reason behind the lower TP and FP counts of CLEME as compared\nto ERRANT is attributed to the chunk partition process, where overlapping edits are merged into chunks. It is worth noting that the FN counts of CLEME are significantly lower than those of ERRANT because of their distinct definition. While ERRANT considers FNs as the edits of references that are not identical to hypotheses, CLEME defines them as the corrected/dummy chunks of references that do not exactly match the chunks of hypotheses. We believe that the definition of ERRANT could be problematic, as it has a tendency to overestimate the FN counts of GEC systems. This may result in an underestimated Recall rate in turn.\n-dependent v.s. -independent. Comparing the Precision and Recall of (Sent)CLEME-independent to those of (Sent)CLEME-dependent, it is observed that the former has a slightly higher value. This is because (Sent)CLEME-independent has the potential to overestimate the performance of GEC systems, whereas (Sent)CLEME-dependent could result in underestimating the same. It is noteworthy that both metrics provide an upper bound and lower bound for GEC performance, respectively.\nCorpus-level v.s. Sentence-level. The precision, recall, and F0.5 scores of sentence-level metrics are considerably higher than those of corpus-level variants. There might be several factors contributing to this difference, but one possible explanation is that precision and recall values get affected by a limited number of challenging samples that contain numerous corrected/dummy chunks."
        },
        {
            "heading": "E Correction Independence",
            "text": "We introduce the term correction independence to describe a pair of chunks where the correction of each chunk is not related to the correction of the other, as illustrated in Table 11. Specifically, chunk 2 and chunk 4 are considered correctiondependent because the correction of chunk 2 family do from Ref.9 must be matched with the correction of chunk 4 help then from Ref.9. However, chunk 6 is correction-independent with chunk 2 (or 4) since the correction of chunk 6 has no impact on the correction of chunk 2 (or 4)."
        },
        {
            "heading": "F More cases",
            "text": "We list more cases in Table 12, which involve JFLEG (Napoles et al., 2017) for English, and MuCGEC (Zhang et al., 2022) for Chinese."
        }
    ],
    "title": "CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction",
    "year": 2023
}