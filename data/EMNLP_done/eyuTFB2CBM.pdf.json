{
    "abstractText": "Modeling hypernym-hyponym (\"is-a\") relations is important for many natural language processing (NLP) tasks, such as classification, natural language inference and relation extraction. Existing work on is-a relation extraction is mostly in the English language environment. Due to the flexibility of language expression and the lack of high-quality Chinese annotation datasets, it is still a challenge to accurately identify such relations from Chinese unstructured texts. To tackle this problem, we propose a Knowledge Enhanced Prompt Learning (KEPL) method for Chinese hypernymhyponym relation extraction. Our model uses the Hearst-like patterns as the prior knowledge. By exploiting a Dynamic Adaptor to select the matching pattern for the text into the prompt, our method simultaneously embedding patterns and text. Additionally, we construct a Chinese hypernym-hyponym relation extraction dataset, which contains three typical scenarios, as Baidu Encyclopedia, news and We-media. The experimental results on the dataset demonstrate the efficiency and effectiveness of our proposed model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ningchen Ma"
        },
        {
            "affiliations": [],
            "name": "Dong Wang"
        },
        {
            "affiliations": [],
            "name": "Hongyun Bao"
        },
        {
            "affiliations": [],
            "name": "Lei He"
        },
        {
            "affiliations": [],
            "name": "Suncong Zheng"
        }
    ],
    "id": "SP:10bff7edc92ab78973ec31528f967e8e583cdae2",
    "references": [
        {
            "authors": [
                "Alain Auger",
                "Caroline Barri\u00e8re."
            ],
            "title": "Patternbased approaches to semantic relation extraction: A state-of-the-art",
            "venue": "14:1\u201319.",
            "year": 2008
        },
        {
            "authors": [
                "G\u00e1bor Berend",
                "M\u00e1rton Makrai",
                "P\u00e9ter F\u00f6ldi\u00e1k."
            ],
            "title": "300-sparsans at semeval-2018 task 9: Hypernymy as interaction of sparse attributes",
            "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation, pages 928\u2013934. Association for Compu-",
            "year": 2018
        },
        {
            "authors": [
                "Gabriel Bernier-Colborne",
                "Caroline Barri\u00e8re."
            ],
            "title": "Crim at semeval-2018 task 9: A hybrid approach to hypernym discovery",
            "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation, pages 725\u2013731. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Guido Boella",
                "Luigi Di Caro."
            ],
            "title": "Supervised learning of syntactic contexts for uncovering definitions and extracting hypernym relations in text databases",
            "venue": "Camille Salinesi, Moira C. Norrie, and \u00d3scar Pastor, editors, Advanced Information",
            "year": 2013
        },
        {
            "authors": [
                "Thomas Bott",
                "Dominik Schlechtweg",
                "Sabine Schulte im Walde."
            ],
            "title": "More than just frequency? demasking unsupervised hypernymy prediction methods",
            "venue": "Findings of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Jindong Chen",
                "Ao Wang",
                "Jiangjie Chen",
                "Yanghua Xiao",
                "Zhendong Chu",
                "Jingping Liu",
                "Jiaqing Liang",
                "Wei Wang"
            ],
            "title": "Cn-probase: A data-driven approach for large-scale chinese taxonomy construction",
            "year": 2019
        },
        {
            "authors": [
                "Sarthak Dash",
                "Md Faisal Mahbub Chowdhury",
                "Alfio Gliozzo",
                "Nandana Mihindukulasooriya",
                "Nicolas Rodolfo Fauceglia."
            ],
            "title": "Hypernym detection using strict partial order networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Sarthak Dash",
                "Md Faisal Mahbub Chowdhury",
                "Alfio Gliozzo",
                "Nandana Mihindukulasooriya",
                "Nicolas Rodolfo Fauceglia."
            ],
            "title": "Hypernym detection using strict partial order networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Ruiji Fu",
                "Jiang Guo",
                "Bing Qin",
                "Wanxiang Che",
                "Haifeng Wang",
                "Ting Liu."
            ],
            "title": "Learning semantic hierarchies via word embeddings",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2014
        },
        {
            "authors": [
                "Marti A. Hearst."
            ],
            "title": "Automatic acquisition of hyponyms from large text corpora",
            "venue": "COLING 1992 Volume 2: The 14th International Conference on Computational Linguistics.",
            "year": 1992
        },
        {
            "authors": [
                "William Held",
                "Nizar Habash."
            ],
            "title": "The effectiveness of simple hybrid systems for hypernym discovery",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3362\u20133367. Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Jin Huang",
                "Zhaochun Ren",
                "Wayne Xin Zhao",
                "Gaole He",
                "Ji-Rong Wen",
                "Daxiang Dong."
            ],
            "title": "Taxonomyaware multi-hop reasoning networks for sequential recommendation",
            "venue": "Proceedings of the Twelfth ACM International Conference on Web Search and",
            "year": 2019
        },
        {
            "authors": [
                "Tomas Kliegr",
                "Vojtech Svatek",
                "Krishna Chandramouli",
                "Jan Nemrava",
                "Ebroul Izquierdo"
            ],
            "title": "Wikipedia as the premiere source for targeted hypernym discovery",
            "year": 2008
        },
        {
            "authors": [
                "Jingye Li",
                "Hao Fei",
                "Jiang Liu",
                "Shengqiong Wu",
                "Meishan Zhang",
                "Chong Teng",
                "Donghong Ji",
                "Fei Li."
            ],
            "title": "Unified named entity recognition as wordword relation classification",
            "venue": "36(10):10965\u201310973.",
            "year": 2022
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu"
            ],
            "title": "Unified structure generation for universal information extraction",
            "year": 2022
        },
        {
            "authors": [
                "Xusheng Luo",
                "Luxin Liu",
                "Yonghua Yang",
                "Le Bo",
                "Yuanpeng Cao",
                "Jinhang Wu",
                "Qiang Li",
                "Keping Yang",
                "Kenny Q. Zhu"
            ],
            "title": "Alicoco: Alibaba e-commerce cognitive concept net",
            "year": 2020
        },
        {
            "authors": [
                "Yubo Ma",
                "Zehao Wang",
                "Yixin Cao",
                "Mukai Li",
                "Meiqi Chen",
                "Kun Wang",
                "Jing Shao"
            ],
            "title": "Prompt for extraction? paie: Prompting argument interaction for event argument extraction",
            "year": 2022
        },
        {
            "authors": [
                "Roberto Navigli",
                "Paola Velardi."
            ],
            "title": "Learning word-class lattices for definition and hypernym extraction",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318\u20131327. Association for Computa-",
            "year": 2010
        },
        {
            "authors": [
                "Stephen Roller",
                "Douwe Kiela",
                "Maximilian Nickel"
            ],
            "title": "2018. Hearst patterns revisited: Automatic hypernym detection from large text corpora",
            "year": 2018
        },
        {
            "authors": [
                "Ivan Sanchez",
                "Sebastian Riedel."
            ],
            "title": "How well can we predict hypernyms from word embeddings? a dataset-centric analysis",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2,",
            "year": 2017
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Exploiting cloze questions for few shot text classification and natural language inference",
            "year": 2021
        },
        {
            "authors": [
                "Julian Seitner",
                "Christian Bizer",
                "Kai Eckert",
                "Stefano Faralli",
                "Robert Meusel",
                "Heiko Paulheim",
                "Simone Paolo Ponzetto."
            ],
            "title": "A large database of hypernymy relations extracted from the web",
            "venue": "Proceedings of the Tenth International Conference",
            "year": 2016
        },
        {
            "authors": [
                "Rion Snow",
                "Daniel Jurafsky",
                "Andrew Ng."
            ],
            "title": "Learning syntactic patterns for automatic hypernym discovery",
            "venue": "Advances in Neural Information Processing Systems, volume 17. MIT Press.",
            "year": 2004
        },
        {
            "authors": [
                "hai Yu",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
            "year": 2021
        },
        {
            "authors": [
                "Yogarshi Vyas",
                "Marine Carpuat."
            ],
            "title": "Detecting asymmetric semantic relations in context: A casestudy on hypernymy detection",
            "venue": "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 33\u201343. Asso-",
            "year": 2017
        },
        {
            "authors": [
                "Chengyu Wang",
                "Yan Fan",
                "Xiaofeng He",
                "Aoying Zhou."
            ],
            "title": "Predicting hypernym\u2013hyponym relations for chinese taxonomy learning",
            "venue": "58:585\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Chengyu Wang",
                "Xiaofeng He."
            ],
            "title": "BiRRE: Learning bidirectional residual relation embeddings for supervised hypernymy detection",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3630\u20133640, On-",
            "year": 2020
        },
        {
            "authors": [
                "Chengyu Wang",
                "Xiaofeng He",
                "Aoying Zhou."
            ],
            "title": "A short survey on taxonomy learning from text corpora: Issues, resources and recent advances",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1190\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Josuke Yamane",
                "Tomoya Takatani",
                "Hitoshi Yamada",
                "Makoto Miwa",
                "Yutaka Sasaki."
            ],
            "title": "Distributional hypernym generation by jointly learning clusters and projections",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Compu-",
            "year": 2016
        },
        {
            "authors": [
                "Shuo Yang",
                "Lei Zou",
                "Zhongyuan Wang",
                "Jun Yan",
                "Ji-Rong Wen."
            ],
            "title": "Efficiently answering technical questions\u2014a knowledge graph approach",
            "venue": "31(1).",
            "year": 2017
        },
        {
            "authors": [
                "Changlong Yu",
                "Jialong Han",
                "Haisong Zhang",
                "Wilfred Ng."
            ],
            "title": "Hypernymy detection for lowresource languages via meta learning",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133656,",
            "year": 2020
        },
        {
            "authors": [
                "Wenhao Yu",
                "Lingfei Wu",
                "Yu Deng",
                "Qingkai Zeng",
                "Ruchi Mahindru",
                "Sinem Guven",
                "Meng Jiang"
            ],
            "title": "Technical question answering across tasks and domains",
            "year": 2021
        },
        {
            "authors": [
                "Hanqing Zhang",
                "Haolin Song",
                "Shaoyu Li",
                "Ming Zhou",
                "Dawei Song"
            ],
            "title": "2022a. A survey of controllable text generation using transformer-based pre-trained language models",
            "year": 2022
        },
        {
            "authors": [
                "Zhang",
                "Buzhou Tang",
                "Qingcai Chen"
            ],
            "title": "2022b. Cblue: A chinese biomedical language understanding evaluation benchmark",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Hypernym Discovery is a core work in taxonomy construction (Wang et al., 2019). Hypernym relation is a semantic relation that exists between a term (hyponym) and a more general or abstract term (hypernym). Due to its capacity for representing semantic relations, hypernym becomes an essential concept in modern natural-language research, and is a fundamental component in many natural language processing (NLP) tasks, such as question answering (Yang et al., 2017; Yu et al., 2021), taxonomy construction (Chen et al., 2019; Luo et al., 2020) and personalized recommendation (Huang et al., 2019).\n\u2217These authors contributed equally to this work. \u2020Corresponding Author\nTypical efforts in Hypernym Discovery can be roughly classified into two main types: rule-based methods and detection-based methods. Rulebased methods (Auger and Barri\u00e8re, 2008; Kliegr et al., 2008; Seitner et al., 2016; Snow et al., 2004; Wang et al., 2017) rely on predefined linguistic rules or patterns to extract hyponym-hypernym relations. These rule-based methods capture specific syntactic or semantic structures that indicate a hierarchical association between terms, however, rule-based methods lack sufficient capability to discover the hyponym-hypernym relation embedded in the semantic content of sentences. For example, in the sentence \u2019Jay Chou, an acclaimed singer, mesmerizes audiences with his soulful performances.\u2019 the rule-based approach might fail to recognize the hyponym-hypernym relation between \u2019Jay Chou\u2019 and \u2019singer\u2019.\nAnother effective way is to model this problem as a hypernym detection task (Dash et al., 2020a; Roller et al., 2018; Yamane et al., 2016). These methods handle the hypernym detection task in a pipelined manner, i.e., extracting the entities first and then recognizing hypernym relations. This separate framework makes each component more flexible, but it neglects the relevance between two sub-entities.\nIdentifying hyponym-hypernym relation by modeling the interaction of sparse attributes, many studies are dedicated to constructing English hypernym-hyponym relationship datasets (Berend et al., 2018; Bernier-Colborne and Barri\u00e8re, 2018). Meanwhile, Chinese language has unique linguistic characteristics and categories that need to be considered (Zhang et al., 2022b). The lack of such datasets has hampered the progress of Chinese hyponymy extraction research. Even though Chinese speakers account for a quarter of the world population, there has been no existing high-quality dataset for Chinese hypernym relation extraction.\nIn this paper, we propose the Knowledge En-\nhanced Prompt Learning (KEPL) method which utilizes both knowledge and latent semantics for end-to-end Chinese hypernym relation extraction. Specifically, our method employs a Dynamic Adaptor for Knowledge which can adaptively construct a unified representation for both the structured prior knowledge and the unstructured text context. To facilitate a more coherent integration of the structured prompts and unstructured text, we employ a mechanism that learns a unified representation of context through specific attention. For span selection, we use the focal loss to counteract the issue of sample imbalance, a commonly observed phenomenon in extractive tasks.\nThe lack of specific hypernym relation datasets also leads to deficiencies in current works (Chen et al., 2019; Luo et al., 2020). To address this challenge, we propose the CHR dataset that aims to improve the coverage and accuracy of hypernym relations in taxonomies. We believe that our dataset can contribute to the development of more accurate and comprehensive taxonomies.\nThe main contributions of our work are summarized as follows:\n1. To the best of our knowledge, there currently exists no commonly used dataset for Chinese hypernym-hyponym discovery. We construct a Chinese hypernym relation extraction dataset, which contains three typical scenarios, as follows as Baike, news and We-media. The proposed dataset with multiple data sources can well cover specific expressions in various corpus.\n2. We propose a novel framework, the Knowledge Enhanced Prompt Learning (KEPL) which\nleverages prior knowledge as prompts and transfers prior knowledge into an extraction task. Our framework learns a unified representation of context through specific attention, proving effective for various natural language processing tasks, including but not limited to taxonomy construction and semantic search.\n3. Our extensive experiments on the proposed dataset have shown that the KEPL framework achieves a 2.3% improvement in F1 over the best method, demonstrating the effectiveness of our approach. Further, we show the results of individually removing components from the trained KEPL on CHR dataset and proved the effectiveness of each component."
        },
        {
            "heading": "2 Related Work",
            "text": "Hypernym Dection Research into hypernym relation extraction has mainly used unsupervised methods, falling into two categories: patternbased and distributional approaches.\nThe pattern-based approach (Navigli and Velardi, 2010; Boella and Di Caro, 2013; Vyas and Carpuat, 2017; Bott et al., 2021), established by (Hearst, 1992; Wang and He, 2020), employs specific predefined linguistic patterns, such as \u2019isa\u2019 and \u2019including\u2019, to detect hypernym relations. While this approach is simple and widely applicable, it is constrained by its reliance on predefined patterns, sensitivity to sentence structure, and the necessity for manual resource curation. Distributional approaches like (Fu et al., 2014) use a distant supervision method for extracting hypernyms from various sources. Their models produce a list\nof hypernyms for a given entity. Subsequently, (Sanchez and Riedel, 2017) highlights the unique performance of the Baroni dataset in providing consistent results, attributing its effectiveness to its alignment with specific dimensions of hypernymy: generality and similarity\nAdditionally, hybrid methods (BernierColborne and Barri\u00e8re, 2018; Dash et al., 2020b; Yu et al., 2020) that amalgamate different techniques have been explored. For instance, (Held and Habash, 2019) proposed a method that merges hyperbolic embeddings with Hearst-like patterns, resulting in better performance on various benchmark datasets.\nPrompt learning Prompt-based learning, a novel paradigm in pretrained language models (Zhang et al., 2022a), restructures downstream tasks to better align with pre-training tasks, enhancing the model\u2019s performance. A notable application of this approach is demonstrated by (Schick and Sch\u00fctze, 2021), where classification problems are transformed into cloze tasks. This is achieved by creating relevant prompts with blanks and establishing a mapping from specific filled words to the corresponding predicted categories. This method effectively bridges the gap between the task and the model\u2019s training. Furthermore, (Ma et al., 2022) introduces a model named PAIE, which leverages prompts for Event Argument Extraction (EAE) at both sentence and document levels. This innovative use of prompts in EAE tasks showcases the versatility and efficiency of the prompt-based learning approach."
        },
        {
            "heading": "3 Data",
            "text": "We introduce the CHR (Chinese Hypernym Recognition) Dataset \u2013 an innovative resource that specifically addresses the current shortcomings in Chinese hypernym discovery. The key idea of constructing the CHR dataset is to enhance the quality and diversity across various domains, which are currently insufficient in existing resources."
        },
        {
            "heading": "3.1 Data source",
            "text": "To address the existing limitations in Chinese hypernym discovery, particularly the lack of diversity, we constructed the CHR dataset.\nOur dataset is constructed by incorporating data from three distinct sources: encyclopedic knowledge, We-Media public accounts and news.\nWe gather the Baike data from a variety of reputable Baidu Baike online encyclopedias. However, we strategically omitted entries that were too short, lacked contextual richness, or had content outside the scope of our study, such as stub entries and disambiguation pages. The We-Media data was gathered from a wide range of accounts as lifestyle, entertainment, technology, and education. Our scraper was programmed to regularly check these accounts for the latest articles and retrieve historical articles where possible. Advertisements and unrelated links were excluded from our dataset; we focused only on preserving the main content of the articles.\nOur dataset includes news data from multiple high-profile news platforms such as Xinhua News Agency, Tencent News, and Sina News. To eliminate domain bias, we selected articles from a broad spectrum of categories including politics, economics, sports, culture, and science, and we eliminated articles with insufficient text, duplicate articles, and those not pertaining to our research from the dataset."
        },
        {
            "heading": "3.2 Data construction",
            "text": "Pre-processing After gathering data from the various sources, we remove irrelevant features which contain HTML tags, special characters, and formatting. We also performed tokenization, converting sentences into tokens for further processing.\nSentences with a character count falling below the established threshold of 10 characters were disregarded. Those exhibiting an overuse of punctuation marks such as commas, colons, and periods were excised. Additionally, sentences embodying numerals or particular non-Chinese characters were filtered out.\nThe sentences that have passed through these filters are then manually labeled with the appropriate hypernym-hyponym relation by our team. We mark the entity position by determining the start entitystart and end entityend positions of the answer in the text, aiming for accurate identification of the relation scope. If a sentence does not contain explicit hypernym-hyponym entities, the corresponding positions are marked as NONE. Following the above extraction, the potential pairs were presented to a team of trained individuals for review.\nFinal Format We streamline and structure our data into a uniform and easily accessible format, facilitating subsequent analysis and modeling.\nWe use the data consisting of a set of (data, span1,span2), Which span1 represents the latent position of hypernym, span2 represents the latent position of the hyponym. An excerpt for one example: \u94fe\u63a5\u68c0\u9a8c\u5668\uff08link checker\uff09\u662f\u6d4b\u8bd5\u5e76\u62a5 \u544a\u7f51\u7ad9\u7684\u9875\u9762\u5185\u7684\u8d85\u6587\u672c\u94fe\u63a5\u6709\u6548\u6027\u7684\u7a0b\u5e8f. In this example,\u7a0b\u5e8f is the hypernym of the\u94fe\u63a5 \u68c0\u9a8c\u5668. The format of the output is set as <\u7a0b\u5e8f, \u94fe\u63a5\u68c0\u9a8c\u5668.>. We conducted a thorough examination of the data and implemented a meticulous annotation process to ensure the quality and reliability of the dataset, the detail can be seen in Table 1."
        },
        {
            "heading": "4 Method",
            "text": "In this section, we describe our Knowledge Enhanced Prompt Learning (KEPL) framework, specifically designed to extract informative hypernym arguments from text documents. The overall structure of KEPL is depicted in Figure 2."
        },
        {
            "heading": "4.1 KEPL framework",
            "text": "Preliminary We focus on the problem of hypernym discovery. Each document is represented as a set doc = {d1, . . . , dN} and is mapped to a set of spans, S = {S1, S2, . . . , Sk}, using a specific prompt Pmi. Here, each span consists of a pair Hu,Hd, where Hd denotes a hyponym and Hu represents a hypernym.\nKEPL Structure Our KEPL structure is based on a sequence-to-sequence pre-trained language model, primarily used for text generation. We first explain how we inject different knowledge into a given sentence (sec 4.2). Subsequently, we use an attention mechanism to learn a Unified Representation for Templates and Context, allowing us to capture valuable information from both structured prompts and unstructured text in Section 4.3. Lastly, we calculate the distribution of each token\nbeing selected as the start or end of each role feature in Section 4.4."
        },
        {
            "heading": "4.2 Dynamic Adaptor for Knowledge",
            "text": "In the proposed KEPL model, our aim is to construct a unified representation for both the structured prior knowledge and the unstructured text context. To achieve this, we use the Hearst method which facilitates the identification of hypernym (\"is-a\") relations within a sentence by exploiting certain lexico-syntactic patterns. By introducing this prior knowledge in the form of lexicosyntactic patterns, we aim to increase the model\u2019s capability to precisely identify hypernym and hyponym relations.\nFirstly, we represent each structured prior knowledge (the lexico-syntactic patterns) in the form of prompts, and use the corresponding prompt representations, Ep as follows:\nEp = Ldec (pmi ) (1)\nHere, pmi represents a set of structured prompts with Hearst patterns, and Ep denotes the corresponding embeddings obtained from the decoder module Ldec of the pre-trained language model.\nGiven prompt representation Ep and a set of prompt pmi, for different input si, we generate a scoring matrix w \u2208 RL\u00d7L to select the suitable template combination under specific semantics, this progress can be expressed by:\nEp\u2032 = exp\n( Wsi \u22a4 + b )\n\u2211M j=1 exp(Wsi \u22a4 + b) Ep (2)\nwhere Ep is the set of available prompts embeddings, Ep\u2032 \u2208 RM\u00d7L\u00d7H is a weighted representation that blends the adapted prompt embeddings, incorporating both the input sentence and the entire adjusted prompt set\u2019s semantic information, and W \u2208 RL\u00d7H is a learned weight matrix that is used to adapt the selected prompt to match the semantics of the input sentence si.\nThe Hearst method (Hearst, 1992) is an efficient method for identifying hypernym (is-a) relation from a given sentence with exploiting certain lexico-syntactic patterns. To attenuate the impaction from the specific prompt, we use the Hearst pattern as a representation of knowledge. We give examples of these prompts in Table 2."
        },
        {
            "heading": "4.3 Unified representation for templates and context",
            "text": "To facilitate a more coherent integration of the structured prompts and unstructured text, we employ an attention mechanism, we learn a unified representation of context\uff1a\nH(L)si = L (si) (3)\nwhere H(L)si \u2208 RL\u00d7H denotes the context representation. L now refers to the Language Model (LLM) and H is the dimension of the contexts. This progression in the approach is leveraged to effectively integrate context and prompts.\nTo capture useful information and uncertainty from each view (knowledge and context), we\nlearn a unified representation through specific attention. Specifically, as shown in Figure 2, we use semq, semv, Et to compute an attention output xatt \u2208 RL\u00d7H . This process can be expressed as follows:\nxatt = softmax\n( semq, semv T\n\u221a dk\n) Temp\u2032 (4)\nwhere semq, semv are the linear projection of the L(si), Temp\u2032 \u2208 RL\u00d7H represent the Ep\u2032 \u2208 RL\u00d7H through linear layer. This advancement is leveraged to facilitate enhanced interplay between templates and input information, promoting more effective integration of structured prompts and unstructured text."
        },
        {
            "heading": "4.4 Knowledge-guided extraction",
            "text": "In the process of knowledge-guided extraction, we aim to calculate the distribution of each token for the start or end of a given role based on the unified context-prompt representation. The logits reflect the unnormalized log probabilities of each token being the starting or ending positions of a target hypernym or hyponym. They are calculated via the linear projection from xatt as follows:\nlogitsstart|endi = x start|end att\u2032 \u00b7 Ep\u2032T (5)\nHere, xstart|end att\u2032\nare the linear projections of xatt that are computed separately for the start and end positions of a target feature (either hypernym or hyponym). xatt\u2032 \u2208 R1\u00d7H represents the linear projection of xatt, Ep\u2032T \u2208 RH\u00d7L, and logitsi \u2208 RL symbolizes the contextual token distributions. Noted that different span Spanpi will result in different corresponding xatt.\nThe logits are the scores assigned to each token in the input sentence and they measure the likelihood of each token being the start or end token of the span of interest. Once these logits are transformed into probabilities through a softmax operation, these values are utilized to determine the start and end positions for hypernym and hyponym spans in the text.\nWe observed sample imbalance in the extractive task, hence we utilize the focal loss function as follows:\nFL(pi) = \u2212 (1\u2212 pi)\u03b3 log (pi) (6)\npi = logitsi\u2211M j=1 logitsj\n(7)\nIn these equations, pi represents the predicted probability, and to avoid the exhaustive threshold tuning, we use the softmax function to compute these probabilities.\nThe connotation for starting and ending positions of the target span s\u0302k is illustrated as :\n(s\u0302k) = set (i, j) \u2208 C arg max scorek (i, j) (8)\nThe pair (i, j) that maximizes scorek(i, j), which gives us the span position of hypernym or hyponym in the sentence."
        },
        {
            "heading": "5 Experiment",
            "text": "In this section, we evaluate the efficacy of the proposed KEPL model. The evaluation experiments are presented in Section 5.1 and the results are discussed in Section 5.2. Furthermore, we assess the effect of varying the number of knowledge prompts in Section 5.3. An ablation study can be found in Section 5.4."
        },
        {
            "heading": "5.1 Evaluation and Settings",
            "text": "Dataset Our KEPL framework is further evaluated using the CHR Dataset, compiled from three diverse sources: We-Media, Baidu Encyclopedia,\nand various news outlets. This CHR dataset provides a more practical perspective on the applicability of our KEPL framework, as it encompasses a wide range of language styles and topics.\nEvaluation Our experiments have been employed on CHR datasets. We briefly describe each below. In order to evaluate the performance of our experiments on CHR datasets, we employ precision, recall and F1 as our primary evaluation indicators.\nBaselines We perform a comparison with the current state-of-the-art and also with some classical models to show the efficiency and effectiveness of the proposed KEPL model.\n\u2022 W2NER(Li et al., 2022) which discontinuous NER (8 English and 6 Chinese datasets) topperforming baselines\n\u2022 Ernie3(Sun et al., 2021) is a SOTA pretrained Chinese model outperforms the state-of-theart models on 54 Chinese NLP tasks\n\u2022 UIE(Lu et al., 2022) excels in Chinese information extraction tasks and is wildly used. .\nImplement Details The CHR Dataset was employed in our experiments, divided into training, validation, and test sets in a 0.7:0.15:0.15 ratio. The model was trained with a batch size of 32 and a learning rate of 0.0001 using the Adam optimizer. The models were trained for five epochs, with early stopping implemented to prevent overfitting."
        },
        {
            "heading": "5.2 Experiment Results",
            "text": "We report the results of different methods in Table 3. A more visual presentation of the direct results from our experiment can be appreciated in Table 4. It can be seen that our method outperforms all other methods in F1 score and achieves a 2.3% improvement in F1 over the best method UIE (Lu et al., 2022). It shows the effectiveness of our proposed method. Furthermore, from Table 1, we also can see, unlike other methods that predominantly rely on localized features or word-level relationships, KEPL models effectively account for the broader semantic context. This approach enhances the accuracy of inferring hypernym relationships, even in the absence of explicit markers for hierarchical relationships.\nInstance \u5965\u9a6c\u54c8\u6d77\u6ee9\u662f\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u8bfa\u66fc\u5e95\u767b\u9646\u6218\u5f79\u4e2d,\u56db\u4e2a\u4e3b\u8981\u767b\u9646\u70b9\u4e4b\u4e00\u7684\u4ee3\u53f7\nChatgpt <\u5965\u9a6c\u54c8\u6d77\u6ee9\uff0c\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u8bfa\u66fc\u5e95\u767b\u9646\u6218\u5f79 > <\u76df\u519b\u56db\u4e2a\u4e3b\u8981\u767b\u9646\u70b9\u4e4b\u4e00,\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u8bfa\u66fc\u5e95\u767b\u9646\u6218\u5f79 > Lexcial <\u5965\u9a6c\u54c8\u6d77\u6ee9,\u4e16\u754c\u5927\u6218\u8bfa\u66fc\u5e95\u767b\u9646\u6218\u5f79 > Pattern <\u5965\u9a6c\u54c8\u6d77\u6ee9,\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u8bfa\u66fc\u5e95\u767b\u9646\u6218\u5f79 >\nBaseline model <\u5965\u9a6c\u54c8\u6d77\u6ee9,\u4e3b\u8981\u767b\u9646\u70b9 > Our Method <\u5965\u9a6c\u54c8\u6d77\u6ee9,\u4ee3\u53f7 > Adaptor prompt \u5bf9\u8c61\u5c5e\u4e8e\u4e00\u79cd\u5206\u7c7b"
        },
        {
            "heading": "5.2.1 Results on CHR Dataset",
            "text": "The KEPL-Bart and KEPL-Ernie3 methods achieved higher rates when compared to other models such as W2NER, Ernie3, and UIE. This demonstrates the efficacy of the KEPL approach, particularly the effectiveness of incorporating Hearst-like patterns as prompts and embedding patterns and text simultaneously.\nThe performance enhancements of our KEPL models are largely attributed to the implementation of the Dynamic Adaptor for Knowledge and knowledge attention modules, which effectively mediate the interaction between structured prompts and unstructured text, contributing to a superior hypernym-hyponym extraction.\nThe evaluation results of different methods are shown in Table 5, from which we have several observations:\n1. Unstructured text lacks a predefined structure or explicit markers for hierarchical relationships, making it difficult to discern the underlying organization. Hierarchies are often implied through contextual cues, such as sentence structure, semantic relationships, or proximity of concepts, requiring sophisticated natural language processing techniques to identify and extract these relationships accurately.\n2. The other methods are unable to recognize the<Hypernym, Hyponym> appears in pairs, they predominantly rely on localized features or word-level relationships, thereby disregarding the encompassing semantic context required for accurate inference of hypernym relations."
        },
        {
            "heading": "5.2.2 Results on specific scene",
            "text": "We compare KEPL performance with the baseline model on each scene in Table 3, observing the results on the Baidu, We-Media, and News datasets, the performance of the KEPL models varies.\nCompared to other models, KEPL models show higher scores in almost all metrics across different datasets. This can be attributed to the Hearst pattern selector, which adapts to different sentence semantics, and the unified representation for templates and context, which effectively captures the\nuseful information from both the pattern and the text.\nThe results highlight the effectiveness of the KEPL approach in handling different text complexities and styles, thereby exhibiting promising potential for real-world hypernym-hyponym extraction tasks. Nevertheless, improvements could be made to better handle datasets with informal and context-specific language like the We-Media data."
        },
        {
            "heading": "5.3 Effect on Knowledge Number",
            "text": "In this section, we evaluate the effect of varying the number of knowledge prompts on the performance of our KEPL model in Table 6. We vary the number of prompts from 50 to 300 in increments of 50 and report the Recall (R), Precision (P), and F1 score in each case.\nAs seen in Table 6, there is a clear improvement in the KEPL model\u2019s performance as the number of prompts increases. This confirms our hypothesis that increasing the volume of knowledge prompts can enhance the model\u2019s effectiveness."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "In this section, we further provide more insights with qualitative analysis and error analysisto address the remaining challenges. In Table 7, we display the results of individually removing components from the trained KEPL model on the CHR dataset.\nIn the prompt w/o experiment, we use a Random matrix to replace the Dynamic Adaptor, which leads to a drop of 6.73% in F1 score, showing the importance of using the prompt as the connection of hypernym and hyponym. The prompt instructs the model to consider the hierarchical structure between concepts and to accurately identify hyponyms and hypernyms. This approach provides a structured framework for the model to leverage contextual cues, linguistic patterns, and semantic associations related to hyponymy and hypernymy,\nenabling it to capture and utilize the rich hierarchical information present in the text.\nIn the attn w/o experiment, we deliberately trained KEPL (Knowledge-Enhanced Prompt Learning) without knowledge-attention. This design choice resulted in a reduction in performance, primarily due to the model\u2019s compromised ability to effectively integrate context and prompts. Attention mechanisms play a crucial role in enabling the model to attend to relevant parts of the input and to appropriately align them with the given prompts. Therefore, the absence of attention mechanisms in our experiment negatively impacted the model\u2019s capacity to fully exploit contextual information and prompts, ultimately affecting its ability to accurately represent and extract hyponym-hypernym relationships from unstructured text.\nIn the Knowledge-guided extraction w/o experiment, we trained KEPL logits with a Linear MLP. We found that performance decreases by 4.97% in F1 score. This highlights the importance of incorporating prompts before directly merging them with the context, particularly in tasks related to acquiring hypernyms and hyponyms. The prompt serves as a guidance signal, providing explicit instructions to the model and aiding it in understanding the desired relationship or task. By incorporating prompts, the model gains a clearer direction and context-specific cues, which are crucial for accurately capturing and representing hierarchical relationships."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce Knowledge Enhanced Prompt Learning (KEPL) for extracting hypernym-hyponym relations in Chinese language. KEPL utilizes the concept of prompt learning to incorporate prior knowledge in the form of patterns into the model, which simultaneously embeds both the pattern and text.\nThe prompt in the framework uses Hearstlike patterns, specifically for extracting hypernymhyponym relations. Additionally, we have created a Chinese hypernym-hyponym relation extraction dataset, which includes three different types of scenarios: Wikipedia, news articles, and We-media. The results of our experiments using this dataset show that our proposed model is both efficient and effective.\nLimitations\nDomain Adaptability While the KEPL model has demonstrated effective performance in scenarios such as Baike, News, and We-media, its applicability in other domains or contexts is yet to be confirmed. For example, the model may require further tuning and optimization for specialized domains such as technology, law, or medicine.\nData Dependency The performance of the KEPL model is to a significant extent dependent on the quality and quantity of available data. In cases where data is scarce, particularly in certain domains or for specific tasks, the model may require larger datasets for efficient training."
        }
    ],
    "title": "KEPL: Knowledge Enhanced Prompt Learning for Chinese Hypernym-Hyponym Extraction",
    "year": 2023
}