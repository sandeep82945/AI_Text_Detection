{
    "abstractText": "With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenhui Shen"
        },
        {
            "affiliations": [],
            "name": "Liying Cheng"
        },
        {
            "affiliations": [],
            "name": "Xuan-Phi Nguyen"
        },
        {
            "affiliations": [],
            "name": "Yang You"
        },
        {
            "affiliations": [],
            "name": "Lidong Bing"
        }
    ],
    "id": "SP:717d66935a32e6e446fd9e1782fc818dd3164ea3",
    "references": [
        {
            "authors": [
                "Lidong Bing",
                "Piji Li",
                "Yi Liao",
                "Wai Lam",
                "Weiwei Guo",
                "Rebecca Passonneau."
            ],
            "title": "Abstractive multidocument summarization via phrase selection and merging",
            "venue": "Proceedings of ACL-IJCNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Moye Chen",
                "Wei Li",
                "Jiachen Liu",
                "Xinyan Xiao",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Sgsum: Transforming multi-document summarization into sub-graph selection",
            "venue": "Proceedings of EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Liying Cheng",
                "Xingxuan Li",
                "Lidong Bing"
            ],
            "title": "Is gpt-4 a good data analyst? arXiv preprint arXiv:2305.15038",
            "year": 2023
        },
        {
            "authors": [
                "Liying Cheng",
                "Dekun Wu",
                "Lidong Bing",
                "Yan Zhang",
                "Zhanming Jie",
                "Wei Lu",
                "Luo Si."
            ],
            "title": "Ent-desc: Entity description generation by exploring knowledge graph",
            "venue": "Proceedings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Cyril Chhun",
                "Pierre Colombo",
                "Fabian M. Suchanek",
                "Chlo\u00e9 Clavel."
            ],
            "title": "Of human criteria and automatic metrics: A benchmark of the evaluation of story generation",
            "venue": "Proceedings of Coling.",
            "year": 2022
        },
        {
            "authors": [
                "Cheng-Han Chiang",
                "Hung-yi Lee"
            ],
            "title": "Can large language models be an alternative to human evaluations",
            "venue": "In Proceedings of ACL",
            "year": 2023
        },
        {
            "authors": [
                "Israel Cohen",
                "Yiteng Huang",
                "Jingdong Chen",
                "Jacob Benesty",
                "Jacob Benesty",
                "Jingdong Chen",
                "Yiteng Huang",
                "Israel Cohen."
            ],
            "title": "Pearson correlation coefficient",
            "venue": "Noise reduction in speech processing.",
            "year": 2009
        },
        {
            "authors": [
                "Yue Dong",
                "Yikang Shen",
                "Eric Crawford",
                "Herke van Hoof",
                "Jackie Chi Kit Cheung."
            ],
            "title": "Banditsum: Extractive summarization as a contextual bandit",
            "venue": "Proceedings of EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Ori Ernst",
                "Avi Caciularu",
                "Ori Shapira",
                "Ramakanth Pasunuru",
                "Mohit Bansal",
                "Jacob Goldberger",
                "Ido Dagan."
            ],
            "title": "Proposition-level clustering for multidocument summarization",
            "venue": "Proceedings of NAACL.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "Summeval: Re-evaluating summarization evaluation",
            "venue": "TACL.",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Pal: Program-aided language models",
            "venue": "Proceedings of ICML.",
            "year": 2022
        },
        {
            "authors": [
                "Mingqi Gao",
                "Jie Ruan",
                "Renliang Sun",
                "Xunjian Yin",
                "Shiping Yang",
                "Xiaojun Wan."
            ],
            "title": "Human-like summarization evaluation with chatgpt",
            "venue": "arXiv preprint arXiv:2304.02554.",
            "year": 2023
        },
        {
            "authors": [
                "Shen Gao",
                "Xiuying Chen",
                "Piji Li",
                "Zhaochun Ren",
                "Lidong Bing",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Abstractive text summarization by incorporating reader comments",
            "venue": "Proceedings of AAAI.",
            "year": 2019
        },
        {
            "authors": [
                "Jian Guan",
                "Minlie Huang."
            ],
            "title": "UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation",
            "venue": "Proceedings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Junxian He",
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Nazneen Rajani",
                "Caiming Xiong."
            ],
            "title": "CTRLsum: Towards generic controllable text summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tomas Kocisky",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "NeurIPS.",
            "year": 2015
        },
        {
            "authors": [
                "MG Kendall."
            ],
            "title": "A new measure of rank correlation",
            "venue": "Biometrika.",
            "year": 1938
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Romain Paulus",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Improving abstraction in text summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Piji Li",
                "Wai Lam",
                "Lidong Bing",
                "Zihao Wang."
            ],
            "title": "Deep recurrent generative decoder for abstractive text summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Wei Li",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang",
                "Junping Du."
            ],
            "title": "Leveraging graph to improve abstractive multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out.",
            "year": 2004
        },
        {
            "authors": [
                "Peter J Liu",
                "Mohammad Saleh",
                "Etienne Pot",
                "Ben Goodrich",
                "Ryan Sepassi",
                "Lukasz Kaiser",
                "Noam Shazeer."
            ],
            "title": "Generating wikipedia by summarizing long sequences",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv 2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Hierarchical transformers for multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Liu",
                "Ansong Ni",
                "Linyong Nan",
                "Budhaditya Deb",
                "Chenguang Zhu",
                "Ahmed H Awadallah",
                "Dragomir Radev."
            ],
            "title": "Leveraging locality in abstractive text summarization",
            "venue": "Proceedings of EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Zheheng Luo",
                "Qianqian Xie",
                "Sophia Ananiadou."
            ],
            "title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "venue": "arXiv preprint arXiv:2303.15621.",
            "year": 2023
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Faithful chain-ofthought reasoning",
            "venue": "arXiv preprint arXiv:2301.13379.",
            "year": 2023
        },
        {
            "authors": [
                "Xueguang Ma",
                "Xinyu Zhang",
                "Ronak Pradeep",
                "Jimmy Lin."
            ],
            "title": "Zero-shot listwise document reranking with a large language model",
            "venue": "arXiv preprint arXiv:2305.02156.",
            "year": 2023
        },
        {
            "authors": [
                "Yubo Ma",
                "Yixin Cao",
                "YongChing Hong",
                "Aixin Sun"
            ],
            "title": "2023b. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559",
            "year": 2023
        },
        {
            "authors": [
                "Ani Nenkova."
            ],
            "title": "Summarization evaluation for text and speech: issues and approaches",
            "venue": "Ninth International Conference on Spoken Language Processing.",
            "year": 2006
        },
        {
            "authors": [
                "Ani Nenkova",
                "Kathleen McKeown."
            ],
            "title": "A survey of text summarization techniques",
            "venue": "Mining text data.",
            "year": 2012
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback. NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Karolina Owczarzak",
                "John Conroy",
                "Hoa Trang Dang",
                "Ani Nenkova."
            ],
            "title": "An assessment of the accuracy of automatic evaluation in summarization",
            "venue": "Proceedings of workshop on evaluation metrics and system comparison for automatic summarization.",
            "year": 2012
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics",
            "venue": "Proceedings of NAACL-HLT.",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of ACL.",
            "year": 2002
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Shiyue Zhang",
                "Peter Hase",
                "Mohit Bansal."
            ],
            "title": "Summarization programs: Interpretable abstractive summarization with neural modular trees",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Chenhui Shen",
                "Liying Cheng",
                "Lidong Bing",
                "Yang You",
                "Luo Si."
            ],
            "title": "Sentbs: Sentence-level beam search for controllable summarization",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Chenhui Shen",
                "Liying Cheng",
                "Xuan-Phi Nguyen",
                "Lidong Bing",
                "Yang You."
            ],
            "title": "A hierarchical encoding-decoding scheme for abstractive multidocument summarization",
            "venue": "Findings of EMNLP.",
            "year": 2023
        },
        {
            "authors": [
                "Chenhui Shen",
                "Liying Cheng",
                "Ran Zhou",
                "Lidong Bing",
                "Yang You",
                "Luo Si."
            ],
            "title": "Mred: A metareview dataset for structure-controllable text generation",
            "venue": "Findings of ACL.",
            "year": 2022
        },
        {
            "authors": [
                "C Spearman."
            ],
            "title": "The proof and measurement of association between two things",
            "venue": "American Journal of Psychology.",
            "year": 1987
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "venue": "arXiv preprint arXiv:2303.04048.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models. NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Welbl",
                "Amelia Glaese",
                "Jonathan Uesato",
                "Sumanth Dathathri",
                "John Mellor",
                "Lisa Anne Hendricks",
                "Kirsty Anderson",
                "Pushmeet Kohli",
                "Ben Coppin",
                "Po-Sen Huang."
            ],
            "title": "Challenges in detoxifying language models",
            "venue": "Findings of EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Ning Wu",
                "Ming Gong",
                "Linjun Shou",
                "Shining Liang",
                "Daxin Jiang."
            ],
            "title": "Large language models are diverse role-players for summarization evaluation",
            "venue": "arXiv preprint arXiv:2303.15078.",
            "year": 2023
        },
        {
            "authors": [
                "Wen Xiao",
                "Iz Beltagy",
                "Giuseppe Carenini",
                "Arman Cohan."
            ],
            "title": "Primera: Pyramid-based masked sentence pre-training for multi-document summarization",
            "venue": "Proceedings of ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Xu",
                "Greg Durrett."
            ],
            "title": "Dissecting generation modes for abstractive summarization models via ablation and attribution",
            "venue": "Proceedings of ACLIJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Yang",
                "Dayiheng Liu",
                "Wenqiang Lei",
                "Baosong Yang",
                "Mingfeng Xue",
                "Boxing Chen",
                "Jun Xie."
            ],
            "title": "Tailor: A soft-prompt-based approach to attribute-based controlled text generation",
            "venue": "Proceedings of ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "Proceedings of ICML.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Qingyu Zhou",
                "Nan Yang",
                "Furu Wei",
                "Shaohan Huang",
                "Ming Zhou",
                "Tiejun Zhao."
            ],
            "title": "Neural document summarization by jointly learning to score and select sentences",
            "venue": "Proceedings of ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593.",
            "year": 2019
        },
        {
            "authors": [
                "Fabbri"
            ],
            "title": "2021) has defined 4 evaluation dimensions",
            "year": 2021
        },
        {
            "authors": [
                "C C"
            ],
            "title": "Alternative prompts We also use ChatGPT to evaluate with the exact prompts from Wang et al. (2023a). We name these prompts \u201cStarEval",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The desire for inexpensive and fast automatic metrics has never stopped growing. In certain tasks like extractive summarization, where full source sentences are selected to appear in the summaries, simple n-gram overlap metrics against the \u201cgold\u201d summaries like ROUGE (Lin, 2004) or BLEU (Papineni et al., 2002) may work well because the correct answer space is narrow. However, for more open tasks like abstractive summarization, there are countless equally good summaries and the\n\u2217 Chenhui is under the Joint PhD Program between Alibaba and National University of Singapore.\n\u2020 Corresponding author. 1Our code and data are fully released at https://github\n.com/DAMO-NLP-SG/LLM_summeval.\n\u201cgold\u201d summaries become less important. Although many neural-based metrics such as BERTScore and BARTScore (Zhang et al., 2020b; Yuan et al., 2021), are advocated as more human-aligned, the evaluation criteria are also becoming increasingly complex. As a result, abstractive summarization may not be sufficiently evaluated with automatic metrics (Owczarzak et al., 2012; Nenkova, 2006), and often require extensive human evaluations as complements(Yang et al., 2023; Welbl et al., 2021). However, human evaluations often come with hefty costs and slow iteration cycles, while also being difficult to reproduce and standardize due to small sample sizes and potential human biases (Shen et al., 2022b; Liu et al., 2022).\nRecent large language models (LLMs) like ChatGPT and GPT-4 (OpenAI, 2023) have demonstrated outstanding capabilities in language comprehension and reasoning. This leads to a growing trend of employing LLMs as evaluators for complex language generation tasks by prompting them with carefully and elaborately crafted instructions (Chiang and Lee, 2023; Gao et al., 2023; Wang et al., 2023a; Wu et al., 2023; Luo et al., 2023; Liu et al., 2023). Despite the preliminary success suggested by such works, it is still inconclusive as to what degree of confidence we can trust the evaluation results produced by LLMs across different dimensions, despite their supposedly high average correlation with humans. It is also unclear if certain LLM-based metrics are more reliable than others, or if their reliability and fairness vary for different candidate systems.\nIn this work, we conduct extensive analysis to assess whether LLM evaluators can reliably replace human judges. Specifically, we incorporate two common human evaluation approaches with LLM evaluators, namely Likert-scale scoring (He et al., 2022; Shen et al., 2022b; Zhang et al., 2020a) and head-to-head (H2H) comparisons (Shen et al., 2022a; Li et al., 2020; Liu and Lapata, 2019). For\nLikert-scale scoring, we explore direct reason-thenscore (RTS) generation and a multiple-choice question (MCQ) method. The former instructs the LLM to provide reasoning before giving a score, while the latter simply prompts it to choose a specific score with a pre-determined description as the reason. For the Head-to-Head (H2H) comparison, we prompt LLM for a preference over the summaries from two compared candidate systems.\nOur experiments show that LLM evaluators, with RTS and MCQ, outperform existing automatic metrics (Lin, 2004; Yuan et al., 2021). However, they are not ready to be reliable alternatives for human evaluation yet. Specifically, (i) LLM evaluators struggle to distinguish candidates with close performances (\u00a7 4.2.1). (ii) LLM evaluators are candidate-dependent, meaning they do not exhibit highly consistent degrees of human alignment for different candidates (\u00a7 4.2.3). Thus, they may unfairly favor or disfavor an evaluated candidate. (iii) LLM evaluators are dimensiondependent, meaning they have varying degrees of evaluation capabilities for different dimensions like coherence and fluency (\u00a7 4.2.3). (iv) Lastly, as the quality of summaries improves with better candidates, LLM evaluators become unreliably less correlated with human judgments, according to our newly proposed meta-correlation metric (\u00a7 4.2.4).\nWhile we still call for a better automatic metric, in the meantime, we suggest a temporary solution in \u00a7 5 for abstractive summarization practitioners to use LLMs more reliably. Specifically, we advocate calculating the correlation between RTS and MCQ as a preliminary indicator of the reliability of the LLM for certain dimensions. If RTS and MCQ do not generally agree with each other, then further human evaluations are required."
        },
        {
            "heading": "2 Related Work",
            "text": "Summarization The summarization task involves generating a summary that contains concise and important (i.e., salient) contents of the original input article (Nenkova and McKeown, 2012). This task has been handled with 2 different approaches: extractive and abstractive. Unlike extractive summarization systems that directly extract salient phrases or sentences from the input article (Ernst et al., 2022; Chen et al., 2021; Zhou et al., 2018; Dong et al., 2018), abstractive summarization systems are expected to generate summaries using their own words and apply sentence fusion\nor paraphrasing techniques (Shen et al., 2023; Liu et al., 2022; Xiao et al., 2022; Lewis et al., 2020; Zhang et al., 2020a; Ziegler et al., 2019; Bing et al., 2015; Xu and Durrett, 2021). As such, abstractive summarization poses significantly more challenges for automatic and human evaluation pipelines (Saha et al., 2022; Pagnoni et al., 2021), because it is increasingly insufficient to use the provided \u201cgold\u201d summary as ground truth.\nHuman Evaluation Human evaluation can be conducted with different approaches. Some work (He et al., 2022; Shen et al., 2022b; Zhang et al., 2020a; Cheng et al., 2020; Gao et al., 2019; Liu et al., 2018; Li et al., 2017; Krys\u0301cin\u0301ski et al., 2018) employ a Likert scale to evaluate the summaries on discrete ranges, such as from 1 to 5. Meanwhile, many others suggest comparison approaches by asking human annotators to select the best summary out of 2 or more generated summaries from different systems (Shen et al., 2022a; Li et al., 2020; Liu and Lapata, 2019; Fan et al., 2018; Fabbri et al., 2019). Following this, we test LLM-based evaluators using both approaches with human-friendly instruction prompts.\nAutomatic Evaluation ROUGE (Lin, 2004) has been a common lexical overlap metric to evaluate summarization systems. Apparently, ROUGE is not sufficient for abstractive summarization, because the \u201cgold\u201d labels it relies on cannot comprehensively account for the complexity and variability of this task. In addition, the common usage of sentence fusion techniques and novel words for abstractive summarization may make ROUGE even less reliable. Zhang et al. (2020b) propose the neural-based BERTScore, which leverages the BERT word embeddings to compute the semantic similarity among tokens. Yuan et al. (2021) later introduce BARTScore, which uses BART (Lewis et al., 2020) to compute the probability of a summary given its input article. Nonetheless, these metrics may not reflect all of the complicated evaluation dimensions required for abstractive summarization mentioned earlier, nor do they have sufficiently high correlations with humans.\nLLM-based Evaluation There are many concurrent works that demonstrate the potential of LLMs to conduct complex human tasks (Chiang and Lee, 2023; Gao et al., 2023; Wang et al., 2023a; Wu et al., 2023; Luo et al., 2023; Liu et al., 2023; Cheng et al., 2023). The key advantage of instruction-\ntuned LLMs, like ChatGPT or GPT-4 (Ouyang et al., 2022; OpenAI, 2023), is that we can explicitly describe in natural language what our evaluation criteria and dimensions are and how to score the summaries, similar to how we would explain such tasks to a human expert. Chiang and Lee (2023) use LLMs for open-ended story evaluations, while Luo et al. (2023) apply ChatGPT specifically for evaluating the consistency of summaries. Wu et al. (2023) formulate LLMs as diverse roleplayers to evaluate summaries from the perspectives of different personas. Wang et al. (2023a) and Liu et al. (2023) also explore the LLM\u2019s evaluation potential in various dimensions for the natural language generation task. Our work differs from the above works in that besides investigating the LLMs\u2019 capability using different approaches across various dimensions for abstractive summarization, we further focus on the reliability of LLM across evaluated systems and dimensions."
        },
        {
            "heading": "3 LLM as a Zero-Shot Evaluator",
            "text": "We investigate an LLM\u2019s evaluation capabilities in the dimensions of coherence, consistency, fluency, and relevance respectively, as defined by Fabbri et al. (2021) (see Appendix A). Following common human evaluation approaches, we propose two methods for Likert-scale scoring, namely the reason-then-score method and the multiple-choice question method, as well as one method for headto-head comparisons. We describe each method in \u00a7 3.1 using the relevance dimension as an example (see more prompts and details in Appendix B). We further experiment with alternative phrasings for different methods in Appendix C.\nBesides exploring different evaluation methods, the stability of LLM-based evaluations across different summarization systems is equally important. Ideally, a stable LLM evaluator should perform equally well regardless of the evaluated systems, with a close (if not identical) degree of alignment with human judgments. In \u00a7 3.2, we propose a meta-correlation metric and explain how it can gauge the extent to which LLM evaluators\u2019 performances depend on the evaluated systems, which indicates how stable and reliable they may be with evaluating any future candidate systems."
        },
        {
            "heading": "3.1 Summary Evaluation Methods",
            "text": "Reason-then-Score (RTS) Given the success of chain-of-thought prompting (Kojima et al., 2022;\nWei et al., 2022), an intuitive method is to ask the LLM to evaluate a specific dimension by first generating the reasoning and then a corresponding score. Since the SummEval dataset (Fabbri et al., 2021) contains human scores on a Likert scale of 1 to 5, we also ask the LLM to score the summaries in the same range, as shown in Table 1.\nMCQ Scoring (MCQ) Nevertheless, previous works find that the reasoning generated by the LLM does not always make sense (Lyu et al., 2023;\nWang et al., 2023b; Gao et al., 2022). To avoid the misguidance of wrongly generated reasoning, we explore a more constrained MCQ method for the Likert-scale scoring. As shown in Table 2, instead of allowing the LLM to freely generate its thoughts, we dictate specific reasoning for each score.\nHead-to-Head Comparison (H2H) Lastly, some concurrent works also observe that ChatGPT can act as an effective ranking model (Ma et al., 2023a,b). We thus explore the head-to-head comparison approach for LLM-based evaluations. As shown in Table 3, we present 2 summaries (Summary #1 and #2) generated by different summarization systems on the same input article, then prompt the LLM to select the better summary, or to indicate a tie. Moreover, to avoid potential biases that arise from the summary IDs, we conduct each evaluation twice, presenting the same summary as either #1 or #2 respectively."
        },
        {
            "heading": "3.2 Stability of LLM Evaluators",
            "text": "To ensure fairness across all evaluated systems, we argue that it is crucial for LLMs to produce stable evaluations. That is, regardless of evaluated systems, the LLMs should maintain a consistent degree of alignment with human judgments. We investigate such stability in two ways.\nFirst, We categorize the summaries based on their originating summarization systems, and then\nexamine the correlation between the LLM and human evaluations for each system. Ideally, if an LLM is stable across systems, it should produce evaluations that are similarly correlated to human evaluations. Otherwise, if the correlations differ significantly across different candidates, then we may conclude that the LLM\u2019s evaluations are system-dependent.\nSecond, we define a meta-correlation metric to quantify the extent to which the LLM\u2019s performance is affected by the quality of the evaluated systems. Specifically, we use the average human score for each candidate as an indicator of its summarization quality (Qi), as shown in Equation (1):\nQi = 1\nN N\u2211 j=1 fhuman(gi,j) (1)\nwhere fhuman(\u00b7) indicates the human evaluation, gi,j represents the jth summary generated by the ith candidate system. Each candidate\u2019s quality is calculated as an average of N generated summaries (N = 100 for all systems). Next, we use the correlation Pi between LLM scores and human scores as an indicator of the LLM\u2019s evaluation performance for the ith candidate, as follows:\nPi = \u03c1([fLLM(gi,1), ..., fLLM(gi,N )],\n[fhuman(gi,1), ..., fhuman(gi,N )]) (2)\nwhere \u03c1 denotes the correlation metric (i.e., Spearman correlation, Pearson correlation, or Kendall\u2019s Tau2), and fLLM(\u00b7) indicates the LLM\u2019s evaluation for each summary gi,j . Finally, we calculate the meta-correlation3 M on a total of k candidates as:\nM = \u03c1([Q1, ..., Qk], [P1, ..., Pk]) (3)\nIdeally, an LLM should work well regardless of the quality of the evaluated systems, which means that M should be close to zero. On the other hand, a significant M would indicate an undesirable relationship between the LLM\u2019s evaluation capability and the quality of the evaluated systems, suggesting that the LLM evaluation is not stable, such that it may not evaluate each candidate system fairly using the same standards.\n2We use the scipy.stats.kendalltau package, which implements the tau-b variant that accounts for ties.\n3We use the same correlation metric for both Equation2 and 3. For instance, if Pi is obtained using Spearman correlation, then M is also calculated using Spearman correlation."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Setups",
            "text": "We use the ChatGPT \u201cgpt-3.5-turbo-0301\u201d snapshot (\u00a7 4.2) for all three methods. By using a fixed snapshot, we ensure all evaluations are conducted with the same LLM model. In addition, we evaluate with the GPT-4 \u201cgpt-4-0314\u201d snapshot (\u00a7 4.3) using the best evaluation method determined by ChatGPT to check for any potential improvement. Given that ChatGPT and GPT-4 are amongst the top performing LLMs, we use their performance to estimate the potential of LLMs as reliable evaluators. Additional results using three different-sized Llama 2 models (Touvron et al., 2023) are reported in Appendix D, which all performs worse. Similar to Luo et al. (2023) and Wu et al. (2023), we set the temperature to 0 and reset the dialogue history for each evaluation instance.\nDataset We use the SummEval benchmark dataset (Fabbri et al., 2021). This dataset contains expert human annotations for coherence, consistency, fluency, and relevance on the generation results from 12 abstractive systems (see details in Appendix table 21) on the CNN/DM dataset (Hermann et al., 2015). Each evaluated system generates summaries for the same 100 news articles, and each summary is scored by 3 expert annotators from 1 to 5. The annotations achieve with a high kappa coefficient of 0.713 (Fabbri et al., 2021). We further calculate the annotations\u2019 standard deviations across each evaluated system in Appendix Table 20. Given a step size of 1, the standard deviations are considered very small, thus suggesting that this dataset has a high level of human agreement. Following Chiang and Lee (2023), Chhun et al. (2022), and Guan and Huang (2020), we use the average human scores as the reference scores.\nBaselines We use ROUGE (Lin, 2004) F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L, BERTScore (Zhang et al., 2020b), BARTScore, BARTScore-CNN, and BARTScore-CNN-PARA (Yuan et al., 2021) as baseline metrics. The last two metrics use BART models fine-tuned on CNN/DM\u2019s training data, and are especially strong.\nPrompts We conduct evaluation following our prompt formats given in Table 1, 2, and 3. Following Fabbri et al. (2021), we re-use the definitions of the evaluation dimensions: (i) Coherence - the collective quality of all sentences, (ii) Consistency\n- the factual alignment between the summary and the summarized source, (iii) Fluency - the quality of individual sentences, and (iv) Relevance - the selection of important content from the source.\nMeasurements To compare all evaluation methods on equal ground with human evaluation, we use four different measurements. First, we count the number of correct preferences (#CP), which is the number of times each automatic metric has the same preference as the average human scores do over a set of compared system pairs (\u00a7 4.2.1). This can help measure the alignment of evaluation methods with humans at a granular level. To determine the preferred system by a particular metric, we assign a system 1 point if its generated summary is evaluated as better than that of the other system according to the metric, or assign both systems 0.5 for a tie. Then, we aggregate the different scores for the compared systems for all 100 test inputs, and the system with a higher score is considered the preferred system by that metric (see Appendix Table 22 for details).\nNext, we also use the Pearson correlation (Cohen et al., 2009), Spearman correlation (Spearman, 1987), and Kendall\u2019s Tau (Kendall, 1938) to measure the relationship between the scores of automatic evaluators and humans (\u00a7 4.2.2, 4.2.3, 4.2.4). While the Pearson score measures linear relationships, the other two measure the ordinal relationship that may be non-linear. Moreover, Kendall\u2019s Tau is less sensitive than Spearman correlation to outliers due to its paired counting of concordant and discordant pairs."
        },
        {
            "heading": "4.2 ChatGPT Evaluator",
            "text": "In this section, we examine the ChatGPT evaluator across many aspects, ranging from human correlation and stability across different systems."
        },
        {
            "heading": "4.2.1 Correct Preferences",
            "text": "The ultimate goal of evaluation is to determine if one candidate system is better than the other in a compared pair. The number of correct preferences (#CP) metric normalizes all evaluation methods into determining whether an evaluator can, as a human expert would, pick the same better system or determine a tie. We conduct such analysis with different pairs of summarization systems on the same input articles. Due to the limited budget for API calls, we only evaluate H2H on a challenge set, consisting of 11 candidate pairs with the closest\naverage performances according to human scores. However, for RTS, MCQ, and other baselines, we can easily calculate the #CP for all 66 possible pairs (see Appendix E).\nTable 5 reports the #CP for both the standard 66- pair full set (in brown) and the 11-pair challenge set (in black). As shown for the larger standard set, RTS unanimously obtains the largest #CP across all dimensions, with an average of 58.5 out of 66 candidate pairs (i.e. 88.6% accuracy).\nDespite the high overall accuracy, weaknesses of such evaluators are revealed as we dive into their performances in the 11-pair challenge set (black scores of Table 5), where the evaluated candidates are close matches. Specifically, BARTScore-CNNPara performs better than RTS in coherence and consistency, possibly because it is fine-tuned with same-domain summarization data. For fluency and relevance, ChatGPT-RTS still performs best among all evaluators. Nonetheless, its average accuracy\ndrops significantly to 63.6% (7 out of 11), which indicates LLM evaluators struggle to differentiate the closely matched candidate systems. In other words, LLM evaluators may only reliably compare candidates with a relatively large performance gap."
        },
        {
            "heading": "4.2.2 Correlations with Human",
            "text": "Table 4 reports that Spearman, Pearson correlations, and Kendall\u2019s Tau between scores of multiple automatic evaluators and humans with a total of 1200 summaries from all systems, across the four evaluation dimensions. As shown, ChatGPT RTS and MCQ demonstrate stronger correlations with humans than many automatic evaluators, such as ROUGE and BARTScore, with up to 0.2 gains in fluency. While RTS achieves higher correlations in the dimensions of consistency and relevance, MCQ has relatively strong correlations in the dimensions of coherence and fluency. Meanwhile, the specialized BARTScore-CNN family also shows competitive performance in coherence, most likely due to the fine-tuning process with CNN/DM."
        },
        {
            "heading": "4.2.3 Per-candidate Correlations",
            "text": "Next, we break down the human correlation of ChatGPT-RTS for each candidate system and measure the statistical spread for the correlations across all systems (see raw results in Appendix table 23). Ideally, a stable evaluator should exhibit the same human correlation across candidates and dimensions, and display flattened boxes in a line.\nHowever, as illustrated in Figure 1, the spread of correlations for different candidates is particularly wide, with up to 0.5 correlation difference in consistency. This means that the RTS evaluator exhibits a significantly varying degree of alignment with human judgment for different candidates. In other words, ChatGPT-RTS is candidate-dependent\nand one should not expect such LLM evaluators to have the same level of human alignment on a new summarization system. Similar trends can also be observed for MCQ (see Appendix table 24).\nIn addition, the medians across the four dimensions are also different. This indicates that the ChatGPT is also dimension-dependent and unstable. Given such varying performances across different dimensions, ChatGPT may not behave well with a newly introduced evaluation criterion."
        },
        {
            "heading": "4.2.4 Summary Quality vs Human Alignment",
            "text": "Using our proposed meta-correlation measurement in \u00a7 3.2, we analyze the relationship between summary quality and human correlation of LLM evaluators. We illustrate the meta-correlation in terms of\nKendall\u2019s Tau for both RTS and MCQ in Figure 2. As shown, both RTS and MCQ exhibit strong negative meta-correlation for consistency and fluency. This suggests that ChatGPT becomes less humanaligned with improving qualities of the evaluated systems.\nTo illustrate this phenomenon further, we scatter the paired coordinates of the summarization system quality (Qi, Equation (1)) and ChatGPT\u2019s evaluation performance (Pi, Equation (2)) in Figure 3. As shown, while the LLM evaluator is better humancorrelated with lower-quality candidates (< 3.5), it is less reliable when dealing with high-quality candidates (> 4.7) with much lower and inconsistent correlations.\nWe compare the meta-correlation for all evaluation metrics in Table 6. We can see that while the ROUGE metrics exhibit no significantly negative meta-correlation, the neural metrics all display significant meta-correlation in certain dimensions. One highly likely reason for this behavior is due to the varying biases inherent to the neural models, which would explain why ROUGE as a simple n-gram overlap metric doesn\u2019t exhibit significant negative meta-correlations. Interestingly, ROUGE2 even shows a strong positive meta-correlation on coherence (which is plausible, because bi-gram\noverlap performance may be more accurate as candidates produce more coherent texts).\nBoth the BARTScore variants and LLMs demonstrate the most negative meta-correlations. ChatGPT-RTS has the most negative metacorrelation in the dimensions of consistency and fluency, indicating that it may be the least reliable to evaluate high-quality systems on these dimensions. On the other hand, the BARTScore family may be unreliable in comparing systems with high qualities of coherence, consistency, and relevance.\nSo far, the observations discussed in \u00a7 4.2.3 and \u00a7 4.2.4 collectively suggest that LLM evaluators may not be a reliable standalone metric for challenging scenarios, and further human evaluation is required for conclusive decisions."
        },
        {
            "heading": "4.2.5 RTS and MCQ Scores",
            "text": "Lastly, we delve into the detailed scores generated by ChatGPT with either the RTS or MCQ method. Since both methods score the summaries in the same range of human scores of 1 to 5 (Fabbri et al., 2021), we can show a direct comparison of the average RTS and MCQ scores with human scores in Figure 4 (see more details in Appendix F). As shown, the RTS scores are much lower than the human scores across all dimensions, while MCQ scores are consistently higher and better match the human scores (except for relevance). In other words, while RTS is best aligned with humans according to \u00a7 4.2.1 and \u00a7 4.2.2, we cannot replace the human scores with RTS scores in absolute terms.\nThe discrepancy may be attributed to the unfaithful reasoning generated by LLMs (Lyu et al., 2023; Wang et al., 2023b; Gao et al., 2022). Our further investigation suggests that ChatGPT-RTS generates false or unrelated-to-dimension reasoning. Thus, it is possible that the much lower scores could be caused by ChatGPT penalizing the sum-\nmaries according to false premises (more examples in Appendix G). For instance, RTS may penalize the summary\u2019s repetitiveness in the consistency dimension or suppress fluency ratings for missing important details.4 On the other hand, the MCQ counterpart gives higher overall scores, most likely because the confined set of pre-defined reasons prevents such unrelated penalization, though not leading to better human alignment."
        },
        {
            "heading": "4.3 GPT-4 Evaluator",
            "text": "A natural question to ask is whether such aforementioned limitations are resolved with an stronger LLM. In this section, we conduct similar analyses on GPT-4 (OpenAI, 2023) with the RTS method. We present the GPT-4 results in the last rows of Table 4 and 5. The results suggest that a stronger LLM does not necessarily translate to a stronger LLM evaluator, although Table 4 does show that GPT-4 outperforms ChatGPT in terms of human correlation consistently across most dimensions.\nUnfortunately, GPT-4 still suffers from the same limitations as ChatGPT. It appears to be both candidate-dependent and dimension-dependent, as demonstrated by the large spreads with varying median values across dimensions in Figure 5 and the significantly negative meta-correlations out of 3 dimensions (Table 6). However, GPT-4 is less dimension-dependant as compared to ChatGPT, as the medians in the box plots in Figure 5 are more aligned than those in Figure 1.\nIn addition, there is a notable enhancement in the meta-correlation for consistency, which we attribute to a significant reduction in reported hallucinations with GPT-4 (OpenAI, 2023). It is possible that with much more instruction training to avoid hallucinations, GPT-4 is much better aligned with humans to detect inconsistencies (i.e. hallucina-\n4Fabbri et al. (2021) observe similar issues with crowdsourced non-expert annotators.\ntions) in summaries. Nevertheless, GPT-4 exhibits a much worse negative meta-correlation in the relevance dimension, which, interestingly, seems to reflect the challenges of maintaining both \u201ctruthfulness\u201d and \u201cinformativeness\u201d (Ouyang et al., 2022). This is because a model could be easily made more truthful if allowed to provide less relevant information (for instance, by refusing to answer the users\u2019 questions). It is possible that with reduced capability in the informativeness dimension, the model is less capable of differentiating the nuances of less relevant summaries when the summary quality is generally high. Nevertheless, we leave it to future work to determine whether GPT-4\u2019s more negative metacorrelation in the relevance dimension could be related to its stronger performance in consistency. We provide more details on the GPT-4 evaluator in Appendix H."
        },
        {
            "heading": "5 A Temporary Efficient Framework",
            "text": "Despite the aforementioned limitations, it may be hard to resist the temptation of using LLM evaluators given their superiority over other automatic metrics. In such a case, one should be able to tell when LLM evaluators are more likely to be unreliable and employ further human evaluation when necessary. To this end, we suggest combining the RTS and MCQ scores as a cost-efficient framework. Specifically, we calculate the correlation between RTS and MCQ scores for the ith candidate system as a reliability indicator:\nRi = \u03c1([fRTS(gi,1), ..., fRTS(gi,N )],\n[fMCQ(gi,1), ..., fMCQ(gi,N )]) (4)\nThen, we can loosely infer that up to a reliability tolerance r \u2208 (0, 1), the LLM evaluators (either RTS or MCQ) are reliable if Ri > r. In other words, given a candidate i, if RTS and MCQ agree with each other up to a certain degree of tolerance r, we may assume the evaluator is reliable enough to avoid invoking further human evaluation.\nTo validate this theory, we measure the correlations \u03c1(Ri, PRTSi ) or \u03c1(Ri, P MCQ i ), where\nP RTS/MCQ i is the performance of either method as defined in Equation (2). Given significantly large positive values of either \u03c1(Ri, PRTSi ) or \u03c1(Ri, P MCQ i ), we can then conclude that Ri can be used as a reliable indicator for the performance of the corresponding method.\nAs shown in Table 7, Ri demonstrates a significant correlation with PRTSi on both the consistency and fluency dimensions, and with PMCQi on the coherence and consistency dimensions. This means that if RTS and MCQ generally agree with each other on the candidate\u2019s performance on a particular dimension with high \u03c1(Ri, PRTSi ) (or \u03c1(Ri, P MCQ i )), RTS (or MCQ) is more likely to be human-aligned. Meanwhile, if RTS disagrees with MCQ (Ri < r), further human evaluators are required to provide a conclusive evaluation. We provide Ri values for ChatGPT on each evaluated system in Appendix Table 29."
        },
        {
            "heading": "6 Conclusion",
            "text": "We explore the potential of using LLMs with different prompting techniques as metrics for abstractive summarization systems. Our extensive analysis suggests that while LLMs like ChatGPT perform better than commonly used automatic metrics across different summarization systems and dimensions, they are still not ready to replace human evaluators because they are candidate- and dimension-dependent, and they do not align well with human when comparing high-quality candidates. Nonetheless, if an LLM evaluator is to be used, we suggest combining multiple evaluation methods as a preliminary indicator to determine whether the metric is likely to be unreliable and whether further human evaluation is required.\nLimitations\nPotential Human Bias. We benchmark the LLM evaluation results against the average of three human expert scores. Naturally, it is possible that these scores may exhibit potential biases of the human experts. Nevertheless, we wish to explore\nwhether LLM evaluators are aligned with human experts, and may naturally exhibit the same bias as a human would. In other words, we examine whether we can reliably replace human annotators with LLMs, instead of seeking a \u201cperfect\u201d solution that has absolutely zero bias.\nDataset Size. Given the constraints of the small size of the human-annotated SummEval dataset, we could only evaluate 100 summaries generated for each summarization system, with a total of 12 abstractive summarization systems. Since we have observed a significant correlation of LLM evaluations with humans for the consolidated 1200 summaries across all systems, it is possible that with a larger evaluation number, the per-system correlation could also be improved. In addition, given only 12 evaluated systems, our meta-correlation may still be subject to sample biases. We leave more investigations for the future once there are larger annotated datasets.\nPrompt tuning. Designing better prompt for LLMs are also ongoing research. Although it is possible that LLMs may act as better evaluators with better prompts, prompt tuning is not our focus. We seek to highlight the limitations of the investigated LLMs and have demonstrated that limitations such as negative meta-correlation are also found with a few other alternative prompts (see Appendix C).\nAvailability of Commercialized LLM We note that the \u201cgpt-3.5-turbo-0301\u201d snapshot is currently taken down5 by OpenAI and replaced with a newer snapshot, \u201cgpt-3.5-turbo-0613\u201d. This is also one disadvantage of using out-of-the-box commercialized LLM for summarization evaluations, as the exact checkpoints may not be stably available. As a result, future models may not be fairly compared against previously evaluated models using a different LLM checkpoint. Nevertheless, our paper only seeks to investigate the potential of LLM as an out-of-the-box evaluator, and the OpenAI models are currently one of the strongest. Eventually, we wish to raise awareness of some of the significant limitations found with these LLMs, which need to be resolved before LLMs can be used as direct replacements for human evaluations. In addition, we also note that the cost of evaluating only 100 sum-\n5We release all LLM generations involved in our experiments in https://github.com/DAMO-NLP-SG/LLM_summev al as JSON files.\nmaries for each system is relatively low (around 2 USD per system using ChatGPT). Since LLMs also conduct evaluations much faster than humans (around 2 minutes for LLMs versus 10 hours for human for 100 summaries), it may not pose significant barriers if one was to re-evaluate all compared systems on a single LLM.\nLimited Use of the Temporary Solution Unfortunately, our temporary efficient framework doesn\u2019t apply to the relevance dimension, where the Ri has no significant correlation with the performances of either RTS or MCQ. Moreover, the r value may be dataset-dependent, and it is hard to decide where to draw this line. We leave for future work of developing better methods to gauge the reliability of LLM evaluations."
        },
        {
            "heading": "Acknowledgements",
            "text": "Yang You is being sponsored by NUS startup grant (Presidential Young Professorship), Singapore MOE Tier-1 grant, ByteDance grant, ARCTIC grant, SMI grant and Alibaba grant."
        },
        {
            "heading": "A Evaluation Dimensions",
            "text": "Fabbri et al. (2021) has defined 4 evaluation dimensions as follows:\n1. Coherence: The collective quality of all sentences. The summary should be well-\nstructured and well-organized. The summary should not just be a heap of related information but should build from sentence to sentence to a coherent body of information about a topic.\n2. Consistency: The factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document.\n3. Fluency: The quality of individual sentences. Sentences in the summary should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.\n4. Relevance: Selection of important content from the source. The summary should include only important information from the source document.\nWe follow the above definitions for designing ChatGPT\u2019s evaluation prompts."
        },
        {
            "heading": "B Prompt Details and Design",
            "text": "We show the RTS prompts for relevance, consistency, fluency, and coherence in Table 1, Table 8, Table 9, and Table 10 respectively.\nWe show the MCQ prompts for relevance, consistency, fluency, and coherence in Table 2, Table 11, Table 12, and Table 13 respectively.\nWe show the H2H prompts for relevance, consistency, fluency, and coherence in Table 3, Table 14, Table 15, and Table 16 respectively.\nTo determine the exact definitions used in our prompts for each dimension, we re-use the first sentence from Fabbri et al. (2021)\u2019s definition. We then prompt the LLM to provide a definition for the evaluated dimension, such as \u201cdefine the word relevance in the context of summarization\u201d, then extract the key phrases generated that we believe to fit the definitions of Fabbri et al. (2021) to make up the full definition. We believe this approach may\nhelp the LLM to better evaluate the summaries according to definitions partially generated in its own language. Nevertheless, we didn\u2019t invest extensive efforts in prompt designs as this is not our key focus. We also demonstrate that our prompts have better evaluation results than two alternative prompts in Appendix C."
        },
        {
            "heading": "C Alternative prompts",
            "text": "We also use ChatGPT to evaluate with the exact prompts from Wang et al. (2023a). We name these prompts \u201cStarEval\u201d since they prompt the LLM to give one to five stars for the summary. In addition, we use ChatGPT to evaluate with alternative prompts for RTS and MCQ by using the full definition as shown in Appendix A instead of supplementing the definition with ChatGPT-generated phrases. We name these two prompts RTS2 and MCQ2 respectively.\nWe show the results of these alternative prompts in Table 17 and Table 18."
        },
        {
            "heading": "D Llama 2 Results",
            "text": "We report the results of using three different sizes of Llama 2 models as LLM evaluators in Table 19. As shown, while the smallest model (7B) exhibits very low correlations with human scores (and only significant on the consistency and relevance dimensions), the larger models (13B and 70B) demonstrate significant correlations with human scores on the full dataset level. However, even the bestperforming 70B model fails to outperform the human correlation of BARTScore, and is completely overwhelmed by the results of ChatGPT and GPT4. This suggests that the open-sourced Llama 2 models are not suitable to be used as zero-shot evaluators. Moreover, all Llama 2 models exhibit significant meta-correlations for at least one dimension."
        },
        {
            "heading": "M8 0.58 0.09 0.13 0.51",
            "text": ""
        },
        {
            "heading": "M9 0.65 0.15 0.34 0.54",
            "text": ""
        },
        {
            "heading": "M10 0.58 0.22 0.20 0.54",
            "text": ""
        },
        {
            "heading": "M11 0.59 0.34 0.40 0.49",
            "text": ""
        },
        {
            "heading": "M12 0.65 0.03 0.14 0.54",
            "text": ""
        },
        {
            "heading": "M13 0.62 0.07 0.14 0.51",
            "text": ""
        },
        {
            "heading": "M14 0.65 0.09 0.18 0.54",
            "text": ""
        },
        {
            "heading": "M15 0.60 0.04 0.13 0.53",
            "text": ""
        },
        {
            "heading": "M17 0.58 0.05 0.08 0.52",
            "text": ""
        },
        {
            "heading": "M20 0.48 0.29 0.24 0.49",
            "text": ""
        },
        {
            "heading": "M22 0.58 0.02 0.14 0.54",
            "text": ""
        },
        {
            "heading": "M23 0.58 0.05 0.12 0.54",
            "text": ""
        },
        {
            "heading": "E Challenging Pairs",
            "text": "To count the total correct pairs, we only evaluate the challenging pairs, which consist of summarization systems of consecutive performances according to average human scores across all dimensions. Thus, each pair contains 2 summarization systems with the smallest difference in terms of average performance.\nFor instance, as shown in Table 21, M22 has the best average human score of 4.57, followed by M23 of 4.55, then M17 of 4.52. We thus compare model pairs of \u201cM22-M23\u201d and \u201cM23-M17\u201d. The full challenge set is shown in Table 22.\nFor RTS, MCQ, and all other baseline metrics,\nwe simply need to compare the evaluated values across all systems, and each metric only needs to evaluate a total of 1200 summaries. However, for H2H, we need to evaluate a total of 6,600 summary pairs for the full standard set, and each pair needs to be evaluated twice with different summary positions (see \u00a7 3.1), resulting in a total of 13,200 LLM evaluations. Due to a limited budget, we thus only compare a challenge set of 11 pairs, reducing the total required LLM evaluations to 2,200."
        },
        {
            "heading": "F Average ChatGPT scores",
            "text": "We present the average ChatGPT evaluation scores for each model across all dimensions in Table 25. Generally, the same trend holds for the individual systems, that ChatGPT score systems much more\nconservatively with RTS, and becomes more optimistic with MCQ.\nG Incorrect Reasons in RTS\nWe illustrate some of the observed incorrect reasons generated with the RTS method by ChatGPT that do not correspond to the evaluated dimension in Table 26. We further provide incorrect reasons generated by GPT-4 in Table 27.\nWe haven\u2019t compiled the exact number of mismatched reasons for either LLM, but we do observe a non-trivial fraction of the evaluations containing unrelated-to-dimension reasoning during preliminary verification. For instance, on the consistency dimension, ChatGPT-RTS has 42% evaluations containing dimension-irrelevant reasoning for the M11 (avg. score of 2.36) model, and 10% for the M17 model (avg. score of 4.84)."
        },
        {
            "heading": "H GPT-4 Evaluator",
            "text": "We also look into the reasoning of GPT-4 and discover that it makes the same mistakes as ChatGPT by penalizing the summary for reasons unrelated to the evaluated dimension (see Table 27).\nAnother major difference is that GPT-4 tends to give overly generous scores. In one exceptionally extreme case, GPT-4 gives full scores for all generations by M12 in terms of consistency. Table 25 also shows the much higher average scores given by GPT-4 across all dimensions than those of ChatGPT-RTS."
        }
    ],
    "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
    "year": 2023
}