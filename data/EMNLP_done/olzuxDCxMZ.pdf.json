{
    "abstractText": "Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt, Bart Baesens Research Centre for Information Systems Engineering (LIRIS), KU Leuven IESEG School of Management, 3 Rue de la Digue, 59000 Lille, France University of Applied Sciences Darmstadt Department of Decision Analytics and Risk, University of Southampton {manon.reusens, philipp.borchert, jochen.deweerdt, bart.baesens}@kuleuven.be margot.mieskes@h-da.de",
    "authors": [
        {
            "affiliations": [],
            "name": "Manon Reusens"
        },
        {
            "affiliations": [],
            "name": "Philipp Borchert"
        },
        {
            "affiliations": [],
            "name": "Margot Mieskes"
        },
        {
            "affiliations": [],
            "name": "Jochen De Weerdt"
        },
        {
            "affiliations": [],
            "name": "Bart Baesens"
        }
    ],
    "id": "SP:6f96743980886340921f02a944cefec46e213a21",
    "references": [
        {
            "authors": [
                "Jaimeen Ahn",
                "Alice Oh."
            ],
            "title": "Mitigating languagedependent ethnic bias in BERT",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 533\u2013549, Online and Punta Cana, Dominican Republic. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Gilsinia Lopez",
                "Alexandra Olteanu",
                "Robert Sim",
                "Hanna Wallach."
            ],
            "title": "Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Zou",
                "Venkatesh Saligrama",
                "Adam Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "Proceedings of the 30th International Conference on Neural Information",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin"
            ],
            "title": "Multilingual bert readme document",
            "year": 2018
        },
        {
            "authors": [
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Analytical methods for interpretable ultradense word embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala",
                "Naoaki Okazaki"
            ],
            "title": "Gender bias in meta-embeddings",
            "year": 2022
        },
        {
            "authors": [
                "Anne Lauscher",
                "Tobias Lueken",
                "Goran Glava\u0161."
            ],
            "title": "Sustainable modular debiasing of language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4782\u20134797, Punta Cana, Dominican Republic. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Sharon Levy",
                "Neha Anna John",
                "Ling Liu",
                "Yogarshi Vyas",
                "Jie Ma",
                "Yoshinari Fujinuma",
                "Miguel Ballesteros",
                "Vittorio Castelli",
                "Dan Roth."
            ],
            "title": "Comparing biases and the impact of multilingual training across multiple languages",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Irene Mengze Li",
                "Emily Zheng",
                "Yao Chong Lim",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "Towards debiasing sentence representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Liang",
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Monolingual and multilingual reduction of gender bias in contextualized representations",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5082\u20135093,",
            "year": 2020
        },
        {
            "authors": [
                "Jind\u0159ich Libovick\u1ef3",
                "Rudolf Rosa",
                "Alexander Fraser"
            ],
            "title": "How language-neutral is multilingual bert? arXiv preprint arXiv:1911.03310",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Meade",
                "Elinor Poole-Dayan",
                "Siva Reddy."
            ],
            "title": "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel R. Bowman."
            ],
            "title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Aur\u00e9lie N\u00e9v\u00e9ol",
                "Yoann Dupont",
                "Julien Bezan\u00e7on",
                "Kar\u00ebn Fort."
            ],
            "title": "French CrowS-pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Slav Petrov"
            ],
            "title": "Measuring and reducing gendered",
            "year": 2020
        },
        {
            "authors": [
                "arXiv:2010.06032. Shijie Wu",
                "Mark Dredze"
            ],
            "title": "Are all languages",
            "year": 2020
        },
        {
            "authors": [
                "donez",
                "Kai-Wei Chang"
            ],
            "title": "2018a. Gender bias",
            "year": 2018
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Yichao Zhou",
                "Zeyu Li",
                "Wei Wang",
                "KaiWei Chang."
            ],
            "title": "Learning gender-neutral word embeddings",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847\u20134853, Brussels, Belgium. Associa-",
            "year": 2018
        },
        {
            "authors": [
                "Ran Zmigrod",
                "Sabrina J. Mielke",
                "Hanna Wallach",
                "Ryan Cotterell."
            ],
            "title": "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Race: (Meade"
            ],
            "title": "2022) black, caucasian, asian, african, caucasian, asian, black, white, asian, africa, america, asia, africa, america, china, africa, europe, asia",
            "venue": "Religion: (Liang et al., 2020a) jewish, christian,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "There has been a growing interest in addressing bias detection and mitigation in Natural Language Processing (NLP) due to their societal implications. Initially, research focused on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b), but recent studies found that pretrained language models also capture social biases present in training data (Meade et al., 2022). Hence, attention has shifted towards debiasing techniques that target sentence representations. These techniques include additional pretraining steps (Zhao et al., 2019; Webster et al., 2020; Zmigrod et al., 2019)\nand projection-based methods that assume a bias direction (Liang et al., 2020a; Ravfogel et al., 2020; Liang et al., 2020b).\nWhile debiasing techniques have been developed and evaluated for monolingual, and mostly English models, the effectiveness and transferability of these techniques to diverse languages within multilingual models remain largely unexplored (Stanczak and Augenstein, 2021; Sun et al., 2019). Our research aims to bridge this gap by examining the potential of debiasing techniques applied to one language to effectively mitigate bias in other languages within multilingual large language models. We examine English (EN), French (FR), German (DE), and Dutch (NL). Figure 1 illustrates an example sentence pair included in the English CrowS-Pairs dataset 1, where the unmodified and modified parts are highlighted in blue and yellow respectively. It shows the predicted probabilities of\n1This example assumes gender to be binary. We acknowledge that this fails to capture the full range of gender identities.\nthe modified part occurring given the unmodified part across different debiasing languages.\nThis study examines the cross-lingual transferability of debiasing techniques using mBERT. mBERT, trained on Wikipedia data from diverse languages, possesses the capability to process and generate text in various linguistic contexts. Despite balancing efforts, it still performs worse on low-resource languages (Wu and Dredze, 2020; Devlin, 2018). We investigate whether this performance disparity extends to gender, religious, and racial biases. Related studies demonstrate the effectiveness of cross-lingual debiasing for individual techniques and selected bias scopes (Liang et al., 2020b; Lauscher et al., 2021). We show how to reduce bias in mBERT across different languages by conducting a benchmark of state-of-the-art (SOTA) debiasing techniques and providing guidance on its implementation. To facilitate further research and reproducibility, we make the code and additional data available to the research community2.\nOur contributions can be summarized as follows: 1) We provide a benchmark of different SOTA debiasing techniques across multiple languages in a multilingual large language model. 2) We find that SentenceDebias is the most effective for crosslingual debiasing, reducing the bias in mBERT by 13%. 3) We provide implementation guidelines for debiasing multilingual models and highlight the differences in the cross-lingual transferability of different debiasing techniques. We find that most projection-based techniques applied to one language yield similar predictions across evaluation languages. We also recommend performing the techniques with an additional pretraining step on the lowest resource language within the multilingual model for optimal results."
        },
        {
            "heading": "2 Methodology",
            "text": "This section introduces the data, debiasing techniques, and experimental setup respectively."
        },
        {
            "heading": "2.1 CrowS-Pairs",
            "text": "CrowS-Pairs is a benchmark dataset comprising 1508 examples that address stereotypes associated with historically disadvantaged groups in the US, encompassing various types of bias, such as age and religion (Nangia et al., 2020). Following Meade et al. (2022), where different debiasing tech-\n2https://github.com/manon-reusens/ multilingual_bias\nniques were benchmarked and their effectiveness demonstrated on BERT for gender, race, and religion, we focus on these three types of bias. N\u00e9v\u00e9ol et al. (2022) translated the dataset in French. To the best of our knowledge, there are currently no peerreviewed variants of CrowS-Pairs available in other languages. Therefore, we used three samples of the full dataset and translated them into the respective language to evaluate our experiments.\nTo create an evaluation set for our experiments, we started from the English CrowS-Pairs dataset (Nangia et al., 2020). We randomly sampled N instances, where N \u2208 {20, 30, 40, 50}, and measured the performance differences on mBERT and BERT. Through three random seeds, we found that a sample size of 40 resulted in an average performance correlation of more than 75% with the full dataset for both models. Thus, we conclude that using 40 instances with three random seeds provides a representative dataset for our evaluation. Further details are shown in Appendix A. Subsequently, we included the translated samples from each language into our dataset, either the corresponding sentences from the French CrowS-Pairs or a translation."
        },
        {
            "heading": "2.2 Debiasing techniques",
            "text": "Next, the different debiasing techniques are explained. For more information on the attribute lists used, we refer to Appendix B.\nCounterfactual Data Augmentation (CDA) is a debiasing technique that trains the model on an augmented training set (Zhao et al., 2019; Webster et al., 2020; Zmigrod et al., 2019). First, the corpus is augmented by duplicating sentences that include words from a predefined attribute list. Next, counterfactual sentences are generated by swapping these attribute words with other variants in the list, for example, swapping he by she. We augment 10% of the Wikipedia corpus of the respective language and use an additional pretraining step to debias the model for three random seeds and average the results.\nDropout Regularization (DO) is introduced by Webster et al. (2020) as a debiasing technique by implementing an additional pretraining step. We execute this pretraining step while training on 10% of the Wikipedia corpus of the respective language using three random seeds and averaging the results.\nSentenceDebias (SenDeb) introduced by Liang et al. (2020a) is a projection-based debiasing technique extending debiasing word embeddings\n(Bolukbasi et al., 2016) to sentence representations. Attribute words from a predefined list are contextualized by retrieving their occurrences from a corpus and augmented with CDA. Next, the bias subspace is computed using the representations of these sentences through principal component analysis (PCA). The first K dimensions of PCA are assumed to define the bias subspace as they capture the principle directions of variation of the representations. We debias the last hidden state of the mBERT model and implement SenDeb using 2.5% of the Wikipedia text in the respective language.\nIterative Nullspace Projection (INLP) is a projection-based debiasing technique in which multiple linear classifiers are trained to predict biases, such as gender, that are to be removed from the sentence representations (Ravfogel et al., 2020). After training a single classifier, the representations are debiased by projecting them onto the learned linear classifier\u2019s weight matrix to gather the rowspace projection. We implement this technique using the 2.5% of the Wikipedia text in each language.\nDensRay (DR) is a projection-based debiasing technique first implemented by (Dufter and Sch\u00fctze, 2019) and adapted for contextualized word embeddings by (Liang et al., 2020b). This technique is similar to SenDeb, but the bias direction is calculated differently. This method aims to find an optimal orthogonal matrix so that the first K dimensions correlate well with the linguistic features in the rotated space. The second dimension is assumed to be orthogonal to the first one. The bias direction is considered to correspond to the eigenvector of the highest eigenvalue of the matrix. DR is only implemented for a binary bias type and using it for multiclass bias types requires modifying the technique. Therefore, we only apply it to the gender bias type. We implement DR debiasing the last hidden state of mBERT and using 2.5% of the Wikipedia text in the respective language."
        },
        {
            "heading": "2.3 Experimental setup",
            "text": "We debias mBERT using language X and evaluating it on language Y with X,Y \u2208 {EN,FR,DE,NL}. In essence, we debiased the model using one language and evaluated it on another, covering all language combinations in our experiments. We implement mBERT in its base configuration (uncased, 12 layers, 768 hidden size) and utilize the bias score as implemented in Meade et al. (2022). This metric evaluates the percentage\nof sentences where the model prefers the more biased sentence over the less biased sentence, with an optimal performance of 50%. All experiments are performed on P100 GPUs."
        },
        {
            "heading": "3 Results",
            "text": "Table 1 shows the performance of the different debiasing techniques when debiasing in English in terms of the absolute deviation of the ideal unbiased model. This is an average score for all bias types and models trained for the respective evaluation language. Base represents the score that is achieved by mBERT on the respective evaluation language dataset before debiasing. More results are shown in Appendices C and D.\nAs shown in Table 1, English is relatively unbiased compared to the other languages and shows a small bias increase after debiasing. This observation aligns with the findings of Ahn and Oh (2021), who propose mBERT as a debiasing technique. In cases where the initial bias score is already close to the optimal level, further debiasing can lead to overcompensation, consequently amplifying the total bias. We assume that an unbiased model should equally prioritize both biased and unbiased sentences. However, when debiasing techniques tend to overcorrect, they skew the balance towards favoring the prediction of unbiased sentences over biased ones. Addressing this challenge necessitates the adoption of specialized techniques to effectively mitigate any residual bias.\nThis phenomenon of overcompensation occurs in several underperforming techniques, as illustrated in Table 1. Notably, we find instances of overcompensation for gender when debiasing using INLP for French and using CDA for German, as well as for race when debiasing using DO for German. Another contributing factor to the poor performance of certain techniques within specific debiasing and evaluation language combinations lies in the inherent ineffectiveness of the debiasing method itself, exemplified by the cases of gender debiasing using CDA for French and religion debiasing using CDA for German. In Tables 5, 6, and 7, we find overcompensation for gender when debiasing with INLP in German and French, evaluating in German, debiasing with Sendeb and DR in French, and evaluating in French, as well as when debiasing in Dutch with INLP and evaluating in French. Moreover, overcompensation for race is also observed when debiasing with CDA in French\nand evaluating in German. Is cross-lingual transferability of debiasing techniques possible? Table 1 shows that crosslingual transfer is possible using English as debiasing language. Figure 2 confirms this, depicting the bias scores averaged over all debiasing techniques. As discussed, for English, these techniques increase the bias contained in the model due to its already close to optimal performance. For the other evaluation languages, we find better performance after debiasing. Therefore, we conclude that for these four languages, it is possible to debias the mBERT model to some extent using a different debiasing language, except when the bias contained in the model is already relatively low.\nTo shed some light on the insights that can be gathered from Figure 2, Table 2 offers an overview of the best- and worst-performing techniques per evaluation language. As shown, Dutch is found to be the best debiasing language for English. This is because this debiasing language has shown to overcompensate the gender bias category the least, therefore, resulting in the best performance. In general, we find that using the same debiasing language as evaluation language often results in an overcompensation of the bias, therefore turning around the bias direction. This means that the best-performing debiasing language is often not the same as the evaluation language. However, German is the exception. As this language already has the highest bias score for gender before debiasing, strong debiasing is beneficial and therefore does not result in overcompensation. Besides German being the best-performing debiasing language for German, it also shows the best performance for French because it achieves the best performance on all different evaluation sets. Moreover, it also shows less overcompensation for the gender bias present in the model than other languages such as Dutch.\nFrench is the worst-performing debiasing language for all evaluation languages except for Dutch, where it is the best-performing one. We find that when evaluating in French, the gender bias is over-\ncompensated. For English, both racial and gender bias are overcompensated. The German evaluation shows lower overall performance due to already two ineffective methods (INLP and CDA), which were also due to overcompensating racial bias. Finally, for Dutch, we find that debiasing with French overcompensates gender bias less than Dutch and, therefore, is the best-performing method. As Dutch has the second highest gender bias score before debiasing, it also benefits from strong debiasing and therefore both French and Dutch perform well.\nWe believe that these results are influenced by the fact that both German and French have a grammatical gender distinction, which may impact debiasing gender to a greater extent. This grammatical gender distinction is not embedded in English and Dutch. Moreover, as the religion category regularly shows limited bias decrease, we find that the performance in the gender and race category often determines whether a technique works well or not.\nHow are the different techniques affected by cross-lingual debiasing? Table 3 shows the overall percentage increase of the bias score per technique. From this, we conclude that SenDeb is the best-performing technique and reduces bias in mBERT on average by 13%. DO is the second bestperforming method reducing bias on average by 10%. However, Figure 3 shows that DO performs well for all debiasing languages except English, while SenDeb performs consistently well for all languages. The other techniques perform worse overall. Hence, we suggest using SenDeb as crosslingual debiasing technique for these languages.\nWhen zooming in on the projection-based techniques, i.e. INLP, SenDeb, and DR, a high performance variation is shown in Table 3 and Figure 3. While SenDeb offers consistent performance for all different debiasing languages, we see more variation and a lower bias decrease for INLP. This is due to the high variation in performance, resulting in a higher overall average. As INLP uses multiple linear classifiers to define the projection matrix, high variability is introduced. Since DR was only implemented for gender, no performance gains can be obtained from the other bias types, therefore resulting in a lower overall performance increase.\nTechniques using an additional pretraining step obtain the best results when debiasing in Dutch, as illustrated in Figure 4. Notably, Dutch is the lowest resource language out of these four languages during pretraining (Wu and Dredze, 2020). This additional pretraining step lets the model learn unbiased associations between words while becoming familiar with the lower-resource language resulting in lower overall bias. Therefore, we conclude that, for our set of languages, these techniques are most effective when applied to lowresource languages."
        },
        {
            "heading": "4 Related work",
            "text": "Significant research focuses on the cross-lingual performance of mBERT (Wu and Dredze, 2020; Pires et al., 2019; Libovicky\u0300 et al., 2019). Limited research focuses on the cross-lingual transferability of debiasing techniques in mBERT (Stanczak and Augenstein, 2021; Sun et al., 2019). Liang et al. (2020b) use DensRay in English to debias Chinese\nin mBERT for gender. Similarly, Zhao et al. (2020) analyze the cross-lingual transfer of gender bias mitigation using one method. Lauscher et al. (2021) also find that their proposed technique, ADELE, can transfer debiasing across six languages. Other studies analyze biases contained in multilingual language models. Kaneko et al. (2022) evaluate bias across multiple languages in masked language models using a new metric. Ahn and Oh (2021) study ethnic bias and its variability over languages proposing mBERT as debiasing technique. Finally, some studies also explore the cross-lingual transferability of downstream tasks (Levy et al., 2023)."
        },
        {
            "heading": "5 Conclusion",
            "text": "Most studies focus on debiasing techniques for large language models, but rarely explore their cross-lingual transferability. Therefore, we offer a benchmark for SOTA debiasing techniques on mBERT across multiple languages (EN, FR, DE, NL) and show that debiasing is transferable across languages, yielding promising results. We provide guidance for cross-lingual debiasing, highlighting SenDeb as the best-performing method, reducing bias in mBERT by 13%. Additionally, we find that, for the studied languages, debiasing with the lowest resource language is effective for techniques involving an additional training step (CDA and DO). This research is a first step into the cross-lingual transferability of debiasing techniques. Further studies should include languages from different cultures and other multilingual large language models to assess generalizability.\nLimitations\nA first limitation concerns the analysis focused on four closely related languages from a similar culture. A broader range of languages should be explored to ensure the generalizability of the findings. Our research was conducted employing a\nsingle multilingual model, mBERT. Extending this to other multilingual language models would provide valuable insights into the wider applicability of the results. Moreover, the evaluation of outcomes relied primarily on the CrowS-Pairs metric, although efforts were made to enhance the understanding by examining the absolute difference compared to the optimal model. Next, the consideration of gender was limited to binary classification, overlooking non-binary gender identities. This should be further addressed in future research. Furthermore, a comprehensive multilingual dataset to assess stereotypes across different languages is not available, and thus, the English CrowS-Pairs dataset was translated and corresponding sentences of the French dataset were used. Nevertheless, typical stereotypes for other languages were not adequately represented. Furthermore, the dataset used in the study exhibited certain flaws highlighted by Blodgett et al. (2021), such as the influence of selected names on predictions, which was observed to have a significant impact. This needs to be investigated further. Additionally, attribute lists for languages other than English were not available to the same extent. We tried to compile lists for French, Dutch, and German, excluding words with multiple meanings to minimize noise. However, our lists were not exhaustive, and therefore the omission of relevant attributes is possible. It is also worth noting that, in certain cases, the generic masculine form was considered the preferred answer, despite it being included in the attribute lists. Finally, the applicability of downstream tasks should be investigated (e.g. (Levy et al., 2023)). Hence, future research should encompass a wider language scope, include multiple models, address existing dataset flaws, and develop more comprehensive attribute lists for various languages.\nEthics Statement\nWe would like to address three key ethical considerations in this study that highlight ongoing challenges and complexities associated with mitigating bias in large language models. First, it is important to acknowledge that the gender bias examined in this paper is approached from a binary perspective. However, this does not capture the full range of gender identities present in reality. While we recognize this limitation, it was necessary to simplify the analysis for experimental purposes. In future research, we hope to address this limitation. Second, despite\nefforts to debias the multilingual large language model, it is important to note that not all forms of bias are completely mitigated. The applied debiasing techniques do lower the bias present in the model, however, there is still bias present in the model both within and outside the targeted bias types. Finally, we recognize that our evaluation datasets do not encompass all the different biases that might be present in the model. Therefore, even if a model would obtain a perfect score, it is still possible that other forms of bias persist."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was funded by the Statistics Flanders research cooperation agreement on Data Science for Official Statistics. The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center)."
        },
        {
            "heading": "A Correlation samples and full dataset",
            "text": "Table 4 shows the overall correlation between the performance of the respective model on the full dataset and on the sample over three random seeds. Full represents that we looked at all different bias types in the CrowS-Pairs dataset and (3) refers to the three bias types used in this study (gender, religion, and race). We used a threshold of 75% to decide on the sample size."
        },
        {
            "heading": "B Attribute Lists",
            "text": "The different lists of attribute words per language used for CDA, SenDeb, INLP, and DR are listed below. We started from the lists in (Meade et al., 2022; Zhao et al., 2018a; Liang et al., 2020a) and extended these to other languages. For the nonEnglish lists, we first translated the words in the English attribute list if a similar relation is contained in the translated words. Subsequently, we asked a native speaker of the respective language whether there are extra words that contain similar relations which do not exist in English. We added these to the list. However, note that these lists are not exhaustive, as many other words still exist that express the same relationship. Nevertheless, this list already provides a more extensive attribute list than by only translating the English attribute list.\nEnglish\nGender:(Zhao et al., 2018a) actor, actress, actors, actresses, airman, airwoman, airmen, airwomen, uncle, aunt, uncles, aunts, boy, girl, boys, girls, groom, bride, grooms, brides, brother, sister, brothers, sisters, businessman, businesswoman, businessmen, businesswomen, chairman, chairwoman, chairmen, chairwomen, dude, chick, dudes, chicks, dad, mom, dads, moms, daddy, mommy, daddies, mommies, son, daughter, sons, daughters, father, mother, fathers, mothers, male, female, males, females, guy, gal, guys, gals, gentleman, lady, gentlemen, ladies, grandson, granddaughter, grandsons, granddaughters, guy, girl, guys, girls, he, she, himself, herself, him, her, his, her, husband, wife, husbands, wives, king, queen, kings, queens, lord, lady, lords, ladies, sir, ma\u2019am, man, woman, men, women, sir, miss, mr., mrs., mr., ms., policeman, policewoman, prince, princess, princes, princesses, spokesman, spokeswoman, spokesmen, spokeswomen. Race: (Meade et al., 2022) black, caucasian, asian, african, caucasian, asian, black, white, asian, africa, america, asia, africa, america, china, africa, europe, asia. Religion: (Liang et al., 2020a) jewish, christian, muslim, jews, christians, muslims, torah, bible, quran, synagogue, church, mosque, rabbi, priest, imam, judaism, christianity, islam.\nFrench\nGender: acteur, actrice, acteurs, actrices, aviateur, aviatrice, aviateurs, aviatrices, oncle, tante, oncle, tantine, tonton, tata, tontons, tatas, oncles, tantes, oncles, tantines, gar\u00e7on, fille, gar\u00e7ons, filles, mari\u00e9, mari\u00e9e, \u00e9poux, \u00e9pouse, mari\u00e9s, mari\u00e9es, \u00e9poux, \u00e9pouses, brother, sister, fr\u00e8re, soeur, fr\u00e8res, soeurs, entrepreneur, entrepreneuse, entrepreneur, entrepreneure, entrepreneurs, entrepreneures, entrepreneurs, entrepreneuses, pr\u00e9sident, pr\u00e9sidente, pr\u00e9sidents, pr\u00e9sidentes, mec, meuf, gamin, gamine, mecs, meufs, p\u00e8re, m\u00e8re, p\u00e8res, m\u00e8res, papa, maman, papas, mamans, fils, fille, fils, filles, abb\u00e9, abbesse, abb\u00e9s, abbesses, masculin, f\u00e9minin, m\u00e2le, femelle, m\u00e2les, femelles, gars, fille, gars, filles, monsieur, dame, messieurs, dames, petit-fils, petite-fille, petit-fils, petites-filles, il, elle, lui-m\u00eame, elle-m\u00eame, lui, elle, mari, femme, maris, femmes, roi, reine, rois, reines, seigneur, seigneuresse, seigneurs, seigneuresses, monsieur, m\u2019dame, monsieur, madame, homme, femme, hommes, femmes, monsieur, mademoiselle, mr, mme, mr, mlle, policier, polici\u00e8re, prince,\nprincesse, princes, princesses, copain, copine, copains, copines, ami, amie, amis, amies, voisin, voisine, docteur, doctoresse, docteur, docteure, boulanger, boulang\u00e8re, h\u00e9ros, h\u00e9ro\u00efne, employ\u00e9, employ\u00e9e, employ\u00e9s, employ\u00e9es, chef, cheffe, chefs, cheffes, cousin, cousine, grand-p\u00eare, grand-m\u00eare, expert, experte, pompier, pompi\u00e8re, pompiers, pompi\u00e8res, agriculteur, agricultrice, agriculteurs, agricultrices, travailleur, travailleuse, infirmier, infirmi\u00e8re, infirmiers, infirmi\u00e8res, patron, patronne, patrons, patronnes. Race: noir, blanc, asiatique, black, blanc, asiatique, noir, caucasien, asiatique, africain, europ\u00e9en, asienne, africain, am\u00e9ricain, asiatique, afrique, am\u00e9rique, asie, afrique, am\u00e9rique, chine, afrique, europe, asie. Religion: juif, chr\u00e9tien, musulman, juifs, chr\u00e9tiens, musulmans, torah, bible, coran, synagogue, \u00e9glise, mosqu\u00e9e, rabbin, pr\u00eatre, imam, juda\u00efsme, christianisme, islam.\nGerman Gender: schauspieler, schauspielerin, koch, k\u00f6chin, lehrer, lehrerin, sch\u00fcler, sch\u00fclerin, student, studentin, pilot, pilotin, onkel, tante, junge, m\u00e4dchen, br\u00e4utigam, braut, bruder, schwester, gesch\u00e4ftsmann, gesch\u00e4ftsfrau, vorsitzender, vorsitzende, vater, mutter, papa, mama, sohn, tochter, mann, frau, kerl, m\u00e4del, herr, dame, enkel, enkelin, gro\u00dfvater, gro\u00dfmutter, cousin, cousine, er, sie, ihm, ihr, sein, ihr, seine, ihre, ehemann, ehefrau, feuerwehrmann, feuerwehrfrau, k\u00f6nig, k\u00f6nigin, f\u00fcrst, f\u00fcrstin, herzog, herzogin, mann, frau, m\u00e4nner, frauen, hr., fr., polizist, polizistin, prinz, prinzessin, sprecher, sprecherin, kollege, kollegin, mitarbeiter, mitarbeiterin, helfer, helferin, anwalt, anw\u00e4ltin, bauarbeiter, bauarbeiterin, krankenpfleger, krankenpflegerin, chef, chefin, vorgesetzter, vorgesetzte, s\u00e4nger, s\u00e4ngerin, kunde, kundin, besucher, besucherin, freund, freundin, arzt, \u00e4rztin, verk\u00e4ufer, verk\u00e4uferin, kanzler, kanzlerin, gesch\u00e4ftsleiter, gesch\u00e4ftsleiterin, pfleger, pflegerin, kellner, kellnerin. Race: dunkelh\u00e4utig, hellh\u00e4utig, asiatisch, afrikaner, europ\u00e4er, asiate, amerikaner, afrika, amerika, asien, china. Religion: j\u00fcdisch, christlich, muslimisch, jude, christ, muslim, torah, bibel, koran, synagoge, kirche, moschee, rabbiner, pfarrer, imam, judentum, christentum, islam.\nDutch Gender: acteur, actrice, acteurs, actrices,\noom, tante, ooms, tantes, nonkel, tante, nonkels, tantes, jongen, meisje, jongens, meisjes, bruidegom, bruid, bruidegommen, bruiden, broer, zus, broers, zussen, zakenman, zakenvrouw, zakenmannen, zakenvrouwen, kerel, griet, kerels, grieten, vader, moeder, vaders, moeders, papa, mama, papa\u2019s, mama\u2019s, zoon, dochter, zonen, dochters, man, vrouw, mannen, vrouwen, gast, wijf, gasten, wijven, heer, dame, heren, dames, kleinzoon, kleindochter, kleinzonen, kleindochters, vent, vrouw, venten, vrouwen, hij, zij, hemzelf, haarzelf, hem, haar, zijn, haar, mannelijk, vrouwelijk, vriend, vriendin, vrienden, vriendinnen, koning, koningin, koningen, koninginnen, heer, dame, heren, dames, meneer, mevrouw, jongeheer, jongedame, jongeheren, jongedames, jongeheer, juffrouw, jongeheren, juffrouwen, politieagent, politieagente, prins, prinses, prinsen, prinsessen, woordvoerder, woordvoerster, woordvoerders, woordvoersters, brandweerman, brandweervrouw, brandweermannen, brandweervrouwen, timmerman, timmervrouw, timmermannen, timmervrouwen, meester, juf, meesters, juffen, verpleger, verpleegster, verplegers, verpleegsters, bestuurder, bestuurster, bestuurders, bestuursters, kuisman, kuisvrouw, kuismannen, kuisvrouwen, kok, kokkin, kokken, kokkinnen, leraar, lerares, directeur, directrice, directeurs, directrices, secretaris, secretaresse, secretarissen, secretaressen, boer, boerin, boeren, boerinnen, held, heldin, gastheer, gastvrouw, gastheren, gastvrouwen, opa, oma, opa\u2019s, oma\u2019s, grootvader, grootmoeder, grootvaders, grootmoeders. Race: afrikaans, amerikaans, aziatisch, afrikaans, europees, aziatisch, zwart, blank, aziatisch, afrika, amerika, azi\u00eb, afrika, amerika, china, afrika, europa, azi\u00eb. Religion: joods, christen, moslim, joden, christenen, moslims, thora, bijbel, koran, synagoge, kerk, moskee, rabbijn, priester, imam, jodendom, christendom, islam."
        },
        {
            "heading": "C Averaged results",
            "text": "Tables 5, 6, and 7 show the different averaged results are the debiasing languages other than English, namely French, German, and Dutch."
        },
        {
            "heading": "D Breakdown results",
            "text": "In this section, a breakdown of the different scores per category is shown in terms of the bias metric established in (Nangia et al., 2020) in Tables 8, 9, 10, 11. For brevity, we employ the following\nabbreviations: Gender (G), Race (Ra), and Religion (Re)."
        }
    ],
    "title": "Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques",
    "year": 2023
}