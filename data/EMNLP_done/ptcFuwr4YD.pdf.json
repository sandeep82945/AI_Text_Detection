{
    "abstractText": "The steady increase in the utilization of Virtual Tutors (VT) over recent years has allowed for a more efficient, personalized, and interactive AI-based learning experiences. A vital aspect in these educational chatbots is summarizing the conversations between the VT and the students, as it is critical in consolidating learning points and monitoring progress. However, the approach to summarization should be tailored according to the perspective. Summarization from the VTs perspective should emphasize on its teaching efficiency and potential improvements. Conversely, student-oriented summaries should distill learning points, track progress, and suggest scope for improvements. Based on this hypothesis, in this work, we propose a new task of Multi-modal Perspective based Dialogue Summarization (MM-PerSumm), demonstrated in an educational setting. Towards this aim, we introduce a novel dataset, CIMA-Summ that summarizes educational dialogues from three unique perspectives: the Student, the Tutor, and a Generic viewpoint. In addition, we propose an Image and Perspective-guided Dialogue Summarization (IP-Summ) model which is a Seq2Seq language model incorporating (i) multi-modal learning from images and (ii) a perspective-based encoder that constructs a dialogue graph capturing the intentions and actions of both the VT and the student, enabling the summarization of a dialogue from diverse perspectives. Lastly, we conduct detailed analyses of our model\u2019s performance, highlighting the aspects that could lead to optimal modeling of IP-Summ.",
    "authors": [
        {
            "affiliations": [],
            "name": "Raghav Jain"
        },
        {
            "affiliations": [],
            "name": "Tulika Saha"
        },
        {
            "affiliations": [],
            "name": "Jhagrut Lalwani"
        },
        {
            "affiliations": [],
            "name": "Sriparna Saha"
        }
    ],
    "id": "SP:97ff5b338e5d93e99e42f9a9a3a5303a8885ff12",
    "references": [
        {
            "authors": [
                "Aditi Bhutoria."
            ],
            "title": "Personalized education and artificial intelligence in the united states, china, and india: A systematic review using a human-in-theloop model",
            "venue": "Computers and Education: Artificial Intelligence, 3:100068.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Caines",
                "Helen Yannakoudakis",
                "Helen Allen",
                "Pascual P\u00e9rez-Paredes",
                "Bill Byrne",
                "Paula Buttery."
            ],
            "title": "The teacher-student chatroom corpus version 2: more lessons, new annotation, automatic detection of sequence shifts",
            "venue": "Swedish Language Technology",
            "year": 2022
        },
        {
            "authors": [
                "Giuseppe Carenini",
                "Raymond T. Ng",
                "Xiaodong Zhou."
            ],
            "title": "Summarizing email conversations with clue words",
            "venue": "Proceedings of the 16th International Conference on World Wide Web, WWW \u201907, page 91\u2013100, New York, NY, USA. Association for Com-",
            "year": 2007
        },
        {
            "authors": [
                "Sabina Elkins",
                "Ekaterina Kochmar",
                "Jackie C.K. Cheung",
                "Iulian Serban"
            ],
            "title": "How useful are educational questions generated by large language models",
            "year": 2023
        },
        {
            "authors": [
                "jani Ramamurthy"
            ],
            "title": "Generating medical reports from patient-doctor conversations using sequence-tosequence models",
            "venue": "In Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,",
            "year": 2020
        },
        {
            "authors": [
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Bing Qin"
            ],
            "title": "A survey on dialogue summarization: Recent advances and new frontiers",
            "year": 2022
        },
        {
            "authors": [
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Libo Qin",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Language model as an annotator: Exploring DialoGPT for dialogue summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Anna Filighera",
                "Siddharth Parihar",
                "Tim Steuer",
                "Tobias Meuser",
                "Sebastian Ochs"
            ],
            "title": "Your answer is incorrect... would you like to know why? introducing a bilingual short answer feedback dataset",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Soumitra Ghosh",
                "Asif Ekbal",
                "Pushpak Bhattacharyya."
            ],
            "title": "Am i no good? towards detecting perceived burdensomeness and thwarted belongingness from suicide notes",
            "venue": "arXiv preprint arXiv:2206.06141.",
            "year": 2022
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2015
        },
        {
            "authors": [
                "Raghav Jain",
                "Tulika Saha",
                "Souhitya Chakraborty",
                "Sriparna Saha."
            ],
            "title": "Domain infused conversational response generation for tutoring based virtual agent",
            "venue": "2022 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Anirudh Joshi",
                "Namit Katariya",
                "Xavier Amatriain",
                "Anitha Kannan."
            ],
            "title": "Dr",
            "venue": "summarize: Global summarization of medical dialogue by exploiting local structures.",
            "year": 2020
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "year": 2017
        },
        {
            "authors": [
                "Rahul Kumar",
                "Sandeep Mathias",
                "Sriparna Saha",
                "Pushpak Bhattacharyya."
            ],
            "title": "Many hands make light work: Using essay traits to automatically score essays",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "page 10.",
            "year": 2004
        },
        {
            "authors": [
                "Chunyi Liu",
                "Peng Wang",
                "Jiang Xu",
                "Zang Li",
                "Jieping Ye."
            ],
            "title": "Automatic dialogue summary generation for customer service",
            "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery amp; Data Mining, KDD \u201919, page",
            "year": 2019
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Nancy F. Chen"
            ],
            "title": "Controllable neural dialogue summarization with personal named entity planning",
            "year": 2021
        },
        {
            "authors": [
                "Jakub Macina",
                "Nico Daheim",
                "Lingzhi Wang",
                "Tanmay Sinha",
                "Manu Kapur",
                "Iryna Gurevych",
                "Mrinmaya Sachan"
            ],
            "title": "Opportunities and challenges in neural dialog tutoring",
            "year": 2023
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Dexiong Chen",
                "Alexandre d\u2019Aspremont",
                "Julien Mairal"
            ],
            "title": "A trainable optimal transport embedding for feature aggregation and its relationship to attention",
            "year": 2021
        },
        {
            "authors": [
                "Sudipto Mukherjee",
                "Subhabrata Mukherjee",
                "Marcello Hasegawa",
                "Ahmed Hassan Awadallah",
                "Ryen White."
            ],
            "title": "Smart to-do: Automatic generation of to-do items from emails",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: A method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, page 311\u2013318, USA.",
            "year": 2002
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Virgile Rennard",
                "Guokan Shang",
                "Julie Hunter",
                "Michalis Vazirgiannis"
            ],
            "title": "Abstractive meeting summarization: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Tulika Saha",
                "Saraansh Chopra",
                "Sriparna Saha",
                "Pushpak Bhattacharyya",
                "Pankaj Kumar."
            ],
            "title": "A largescale dataset for motivational dialogue system: An application of natural language generation to mental health",
            "venue": "2021 International Joint Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Tulika Saha",
                "Vaibhav Gakhreja",
                "Anindya Sundar Das",
                "Souhitya Chakraborty",
                "Sriparna Saha."
            ],
            "title": "Towards motivational and empathetic response generation in online mental health support",
            "venue": "Proceedings of the 45th international ACM SIGIR conference on",
            "year": 2022
        },
        {
            "authors": [
                "Tulika Saha",
                "Saichethan Reddy",
                "Anindya Das",
                "Sriparna Saha",
                "Pushpak Bhattacharyya."
            ],
            "title": "A shoulder to cry on: towards a motivational virtual assistant for assuaging mental agony",
            "venue": "Proceedings of the 2022 conference of the North American chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Tulika Saha",
                "Saichethan Miriyala Reddy",
                "Sriparna Saha",
                "Pushpak Bhattacharyya."
            ],
            "title": "Mental health disorder identification from motivational conversations",
            "venue": "IEEE Transactions on Computational Social Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Tulika Saha",
                "Abhishek Tiwari",
                "Sriparna Saha."
            ],
            "title": "Trends and overview: The potential of conversational agents in digital health",
            "venue": "European Conference on Information Retrieval, pages 349\u2013356. Springer.",
            "year": 2023
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N. Kipf",
                "Peter Bloem",
                "Rianne van den Berg",
                "Ivan Titov",
                "Max Welling"
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "year": 2015
        },
        {
            "authors": [
                "Yan Song",
                "Yuanhe Tian",
                "Nan Wang",
                "Fei Xia."
            ],
            "title": "Summarizing medical conversations via identifying important utterances",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 717\u2013729, Barcelona, Spain (Online). In-",
            "year": 2020
        },
        {
            "authors": [
                "Katherine Stasaski",
                "Kimberly Kao",
                "Marti A. Hearst."
            ],
            "title": "CIMA: A large open access dialogue dataset for tutoring",
            "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201364, Seattle, WA, USA",
            "year": 2020
        },
        {
            "authors": [
                "Abhisek Tiwari",
                "Tulika Saha",
                "Sriparna Saha",
                "Pushpak Bhattacharyya",
                "Shemim Begum",
                "Minakshi Dhar",
                "Sarbajeet Tiwari."
            ],
            "title": "Symptoms are known by their companies: towards association guided disease diagnosis assistant",
            "venue": "BMC bioinformatics, 23(1):1\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "year": 2018
        },
        {
            "authors": [
                "Jun Wang"
            ],
            "title": "Essumm: Extractive speech summarization from untranscribed meeting",
            "year": 2022
        },
        {
            "authors": [
                "Sibo Wei",
                "Wenpeng Lu",
                "Xueping Peng",
                "Shoujin Wang",
                "Yi-Fei Wang",
                "Weiyu Zhang"
            ],
            "title": "Medical question summarization with entity-driven contrastive learning",
            "year": 2023
        },
        {
            "authors": [
                "Baosong Yang",
                "Jian Li",
                "Derek Wong",
                "Lidia S. Chao",
                "Xing Wang",
                "Zhaopeng Tu"
            ],
            "title": "Context-aware self-attention networks",
            "year": 2019
        },
        {
            "authors": [
                "Tiezheng Yu",
                "Wenliang Dai",
                "Zihan Liu",
                "Pascale Fung."
            ],
            "title": "Vision guided generative pre-trained language models for multimodal abstractive summarization",
            "venue": "arXiv preprint arXiv:2109.02401.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan."
            ],
            "title": "DIALOGPT : Largescale generative pre-training for conversational response generation",
            "venue": "Proceedings of the 58th An-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Artificial Intelligence (AI) is making significant strides in the field of education, paving the way for numerous innovative applications. The power of Natural Language Processing (NLP) and advanced machine learning techniques have given rise to Virtual Tutors (VTs) (Stasaski et al., 2020; Jain et al.,\n2022), which, unlike traditional methods, are accessible to anyone, anywhere, anytime1. VTs are revolutionizing the education sector, breaching the barriers of geographical boundaries, and democratizing learning on a global scale. They offer a tailored learning approach, adapting to each learner\u2019s pace, aptitude, and preferences, thereby catering to a diverse array of learning styles. While the implementation of AI in education are evident, extracting valuable insights from these virtual educational interactions remains a challenging task.\nSummarizing insights from VT carries significant implications for both the improvement of AIbased educational systems and the learning outcome of students. With a growing reliance on these AI-powered tools for teaching and learning, it becomes crucial to gauge their teaching efficiency, highlight potential areas of improvement, and assess their overall impact on learners\u2019 progress. For learners, summaries provide a distillation of key learning points, tracking their learning journey and suggesting areas for future study, which could be pivotal in informing their learning path. While dialogue summarization techniques (Feng et al., 2022) have seen extensive development in several domains, their application to educational interactions, particularly with VTs, has not been thor-\n1https://tech.ed.gov/files/2017/01/NETP17.pdf\noughly explored. Furthermore, the importance of summarizing these dialogues from multiple perspectives - those of the tutor, the student, and a neutral observer - is a distinct requirement in this domain (Jain et al., 2022). These perspectives can reveal diverse, valuable insights that can contribute to the evolution of virtual tutoring systems and the individual learning journey of students. Thus, this area holds vast potential for exploration and advancement in the field of AI-driven education.\nTo address this significant gap in the field, the contributions of this paper are : (i) Propose a new task of Multi-modal Perspective based Dialogue Summarization (MM-PerSumm) which is demonstrated in an educational setting; (ii) As a step towards this goal, we extend an existing education based conversational dataset, CIMA (Stasaski et al., 2020) by summarizing the dialogues from three different perspectives: that of the student, the tutor, and a generic viewpoint to create CIMA-Summ which serves as the foundation for our research; (iii) To address the task of MM-PerSumm, we propose Image and Perspective-guided Dialogue Summarization (IP-Summ) model which leverages a Seq2Seq language model with a perspective-based encoder. It is designed to construct a dialogue graph, capturing the intentions and actions of both tutors and students, thereby enabling the summarization of educational dialogues from diverse perspectives. By incorporating multi-modal learning in the form of image-based instruction as a contextual cue, our approach allows the IP-Summ to leverage both textual and visual information for more comprehensive and accurate summarization."
        },
        {
            "heading": "2 Related Works",
            "text": "In this section, we detail some relevant works in the context of Educational-NLP and Dialogue Summarization in general. Education and AI. With the advent of AI, tailored educational materials can be generated based on individual learning styles and abilities (Bhutoria, 2022). Automated feedback systems offer timely evaluations of assignments and homework (Kumar et al., 2022; Filighera et al., 2022). Educational question generation is another area that has witnessed significant advancements (Elkins et al., 2023). In the context of virtual tutoring, advancements in NLG have paved the way for interactive and immersive learning experiences. This led to the creation of AI-related datasets, such as CIMA (Stasaski et al., 2020) and the Teacher-Student Cha-\ntroom Corpus (Caines et al., 2022). Recently, a lot of attempts have been made to create a VT system utilizing these datasets (Jain et al., 2022; Macina et al., 2023). Dialogue Understanding. In the context of the Social Good theme, there have been an upsurge of research in the latest time focused on understanding dialogues. For example works in mental health conversations (Saha et al., 2022a,b,c, 2021), disease diagnosis assistant (Tiwari et al., 2022; Saha et al., 2023), education (Jain et al., 2022) etc. A broad spectrum of research has been done on dialogue summarization spanning different domains. One of the notable sectors includes meeting summarization, where advancements have been observed in both extractive and abstractive techniques (Wang, 2022; Rennard et al., 2023). Similarly, chat summarization has been a focus due to the explosion of messaging apps and business communication platforms (Gliwa et al., 2019; Feng et al., 2021). Email thread summarization (Carenini et al., 2007) is another domain that has seen significant research interest. The complexity of the task lies in identifying the salient information buried in long and often nested conversations (Mukherjee et al., 2020). The domain of customer service summarization (Liu et al., 2019) has a more specific goal, which is to extract customer issues. Finally, medical dialogue summarization (Joshi et al., 2020; Song et al., 2020; Enarvi et al., 2020; Wei et al., 2023) has seen substantial development, driven by the need for concise patient-doctor conversation records."
        },
        {
            "heading": "3 Dataset",
            "text": "To facilitate research in educational dialogue summarization, we create the CIMA-Summ dataset, which is based on an existing conversational dataset for educational tutoring."
        },
        {
            "heading": "3.1 Data Collection",
            "text": "This study builds upon the existing CIMA dataset (Stasaski et al., 2020), a valuable resource in the field of dialogue systems, particularly in the context of tutoring dialogues. This dataset was chosen because it is uniquely positioned in the intersection of dialogue systems and education while also being supplemented with a rich set of features like object images and intent-action labels. CIMA dataset contains one-to-one student-tutor conversations where the aim is to assist students in learning the Italian translation of an object and its characteristics. Each object discussed within the dialogue\nis also supplemented with a corresponding image. Each student utterance contains intent tags, namely, Guess, Question, Affirmation, or Other, while the Tutor\u2019s action are categorized as Question, Hint, Correction, Confirmation, and Other. However, in its original form, the CIMA dataset provides the intent-action labels only for the concluding student utterance and the corresponding gold tutor response for each dialogue. To overcome this limitation, we draw upon the silver action-intent labels from the extended-CIMA dataset (Jain et al., 2022)."
        },
        {
            "heading": "3.2 Data Annotation",
            "text": "A detailed annotation approach was followed which is discussed below. Annotation Guidelines. A clear set of annotation guidelines was established which were aimed at providing a structured way for annotators to distill key points from the dialogues and summarize them from three distinct perspectives: the student, the tutor, and an overall dialogue summary.\nStudent Perspective : Annotators were tasked with crafting a summary that encapsulates the key takeaways for the student from the dialogue. This includes the new knowledge gained, any misconceptions corrected, and the overall progress in understanding. This summary should reflect what the student has learned during the interaction.\nTutor Perspective : For the tutor perspective, annotators were instructed to focus on the teaching strategies employed by the tutor, the clarification of student doubts, and the overall guidance provided. The summary should highlight the tutor\u2019s effort in facilitating the student\u2019s learning.\nOverall Perspective : This is aimed at presenting a balanced view of the conversation both from the student\u2019s and tutor\u2019s perspective, highlighting key dialogic exchanges, instructional elements, and learning outcomes. More details on the annotation guidelines is presented in the Appendix section.\nAnnotation Process. We hired three annotators from the author\u2019s affiliation who were graduates in English Linguistics with substantial familiarity in the educational/acaedemic background for our annotation task. The annotators were first trained to summarise dialogues using a set of preannotated gold-standard samples from the CIMASumm dataset. These examples were annotated with three types of summaries (student perspective, tutor perspective, and overall dialogue summary) by two experienced researchers (from author\u2019s collaboration) specializing in educational dialogue sys-\ntems. The intention was to provide a diverse set of examples that demonstrated a range of topics and dialogue scenarios, giving the annotators an understanding of the depth and breadth of the task. Feedback sessions were arranged where the annotators interacted with the experienced researchers to discuss the evaluations and ways to enhance the quality of the summaries. More details on how the annotators were trained is detailed in the Appendix section. Finally, the main annotation task was conducted using the open-source platform Doccano2, deployed on a Heroku instance. Given the intricacy of the task and the need to ensure a high-quality annotation, we followed a structured schedule over a span of five days in a week (the details of which are mentioned in the Appendix section). This annotation process took approximately five weeks to complete. The final annotations demonstrated a high level of quality, with average fluency and adequacy scores of 4.98 and 4.87, respectively. Furthermore, the evaluators showed high inter-agreement levels, with unanimous scores of 94.7% for fluency and 89.1% for adequacy ratings. More details on the annotation quality assessment is detailed in the Appendix section.\n3.3 CIMA-Summarization : CIMA-Summ Dataset\nThe CIMA-Summ dataset comprises of 1134 dialogues with each dialogue annotated with three different perspective amounting to a total of 3402 parallel summaries accompanied with visual cues. A sample instance from the CIMA-Summ dataset is shown in Figure 2. Qualitative Analysis. Figure 3(a) shows the distribution of word count for three type of summaries. The median length of the student\u2019s summary is less than the tutor\u2019s summary, which in turn is less than the overall summary. Tutor might tend to use more technical language in the interaction to accurately convey the information, which can contribute to longer summaries compared to students, who may only focus on key points. Overall summary, encompassing both tutor\u2019s and student\u2019s perspectives, is bound to have the highest median length amongst the three. Figure 3(c) shows the distribution of similarity for pairs of summaries. It is evident that the similarity between the tutor and overall summaries is the highest. A possible reason for this could be that the tutor provides more detailed in-\n2https://github.com/doccano/doccano\nformation containing a higher level of complexity. Also, tutor and student summaries are least similar, possibly because students concentrate only on the most salient ideas while taking notes. Figure 3(b) represents a histogram of word count for these three type of summaries. For 98% of the conversations, tutor\u2019s summary is longer than the student\u2019s summary, reflecting a deeper understanding & comprehensive analysis of context by the tutor."
        },
        {
            "heading": "4 Proposed Methodology",
            "text": "In this section, we discuss the problem statement and proposed methodology in detail.\nProblem Description. Our proposed task Multimodal Perspective based Dialogue Summarization (MM-PerSumm) is defined as follows :\nInput : The input comprises of three entries: (1) The source dialogue D = {T0 < Ac0 >,S0 < In0 >, T1 < Ac1 >,S1 < In1 >, ...}, which is a conversation between a VT and a student where Ti and Aci represents the VTs utterance and its corresponding action tag. Si and Ini represents the student\u2019s utterance and its corresponding intent tag; (2) An associated visual image I , which is used by the tutor to facilitate the teaching process; (3) A specified perspective, P , which represents the viewpoint (either student, tutor, or a neutral observer) from which the dialogue is to be summarized.\nOutput : The output is a natural language sequence, Y , representing the summarized dialogue from the perspective, P . Given an instance of D, I , and P , Y can be manifested as a diverse summary\nconditioned on the perspective and image."
        },
        {
            "heading": "4.1 Image and Perspective-guided Dialogue",
            "text": "Summarization (IP-Summ) Model\nThis section presents the novel architecture of IPSumm, a perspective-guided, multimodal Seq2Seq model (Figure 4) that synthesizes dialogue and image context to enhance educational dialogue summarization. IP-Summ comprises of three distinct modules: (1) Global Context Encoder, (2) Perspective Context Encoder, and (3) Contextual Fusion Module discussed below."
        },
        {
            "heading": "4.1.1 Global Context Encoder",
            "text": "This integral component of our model serves as the primary mechanism for understanding the overall essence and the key points of the dialogue. It processes the complete multi-turn dialogue, D along with the corresponding image, I and generates a global context representation, ZG providing a highlevel understanding of the conversation.\nDialogue Context Encoder. Firstly, the source dialogue, D, is fed into a language encoder, Lenc(\u00b7). This language encoder is a function that maps the raw dialogue text into a high-dimensional space. The output of this operation is a vector representation, HD of the dialogue.\nImage Encoder. In parallel, we also feed the associated image, I into a vision encoder, Venc(\u00b7). This encoder is a function that processes the image and converts it into a high-dimensional vector representation, HI .\nDimensionality Reduction. Following their\ngeneration, these text and image representations undergo a dimensionality reduction procedure. This is facilitated through a nonlinear transformation, succeeded by a self-attention layer, as captured by the following equations:\nH \u2032 D = SoftMax(\nHDH T D\u221a\nd )HD (1)\nH \u2032 I = SoftMax(\nHIH T I\u221a\nd )HI (2)\nMulti-modal Fusion. Following this, the two vectors are concatenated and passed through an Optimal Transport-based Kernel Embedding (OTKE) layer (Mialon et al., 2021). This layer aims to foster cross-modal interaction between the textual and visual contexts. The OTKE layer facilitates this by mapping the feature vectors into a Reproducing Kernel Hilbert Space (RKHS) (Berlinet and Thomas-Agnan, 2011), subsequently implementing a weighted pooling mechanism. This mechanism leverages weights that are determined by the transport plan between the set and a learnable reference. This operation results in a single vector representation, ZG, which is given by:\nZG = OTKE([H \u2032 D : H \u2032 I ]) (3)\nThis vector, ZG encapsulates the global context, capturing both the textual and visual features of the dialogue-image set."
        },
        {
            "heading": "4.1.2 Perspective Context Encoder",
            "text": "Depending upon the specified perspective (i.e., student, tutor, or neutral observer), this encoder processes the dialogue and generates a perspectivespecific context representation. This ensures that the nuances, intentions, and actions associated with the chosen perspective are accurately captured and represented.\nPerspective driven Dialogue Graph Construction. The Dialogue Graph, denoted as G = (V,E), comprise of nodes, V representing utterances and edges, E illustrating the transition between these utterances. The methodology is intrinsically adaptable to the specified perspective, P (i.e., general, student-focused, or tutor-focused), allowing us to construct graphs tailored to different perspectives discussed as follows : \u2022 Node Creation : Independent of the perspective, P , every utterance within the dialogue, D is transformed into a node in G. Every node, v \u2208 V represents an utterance, (Si) or (Ti), where Si and Ti stand for student\u2019s and tutor\u2019s utterances, respectively. \u2022 Edge Creation : The formulation of edges in graph, G is intrinsically tied to the chosen perspective, P . Representing an edge from node i to node j as e{ij}, each edge is attributed with a label, l{ij}, reflective of the corresponding action or intent tag.\ni) General Perspective : For an edge between each consecutive pair of utterances in a dialogue\nof length N , we establish: e{ij} for all i, j such that j = i + 1 and i \u2208 {0, 1, ..., N \u2212 2}. The label, l{ij} is assigned as per equation 4. Additional edges, e{xy}, are interjected between nodes associated with identical action/intent tags.\nii) Student-focused Perspective : When our perspective, P is focused on the student, the edge creation process remains invariant, but the label assignment function prioritizes the student\u2019s intent tags (Equation 4). We also introduce an additional edge, e{xy} labeled \u2019same intent\u2019 for every pair of student utterances (x, y) that share the same intent.\niii) Tutor-focused Perspective : When P is tutor-focused, our label assignment function prioritizes the tutor\u2019s action tags ((Equation 4)). Under this perspective, an additional edge, e{xy} labeled \u2019same action\u2019 is introduced for each pair of tutor utterances (x, y) that share the same action tag.\nlij =  Aci if utt. i \u2208 T &P \u2208 \u2032T \u2032,\u2032Gen\u2032 Ini if utt. i \u2208 S &P \u2208 \u2032S\u2032,\u2032Gen\u2032 \u2032context\u2032 if utt. i \u2208 S &P =\u2032 T \u2032 \u2032context\u2032 if utt. i \u2208 T &P =\u2032 S\u2032\n(4) where context label indicates that the utterance provides contextual information to the student\u2019s/tutor\u2019s responses.\nLevi Graph Generation. Upon the completion of the directed graph, G generation, the next step is to construct the corresponding Levi graph, L(G) inspired from graph theory. A Levi graph is a representation that preserves the relational structure of the initial dialogue graph but further emphasizes the linkage between utterances sharing the same intent or action. Specifically, L(G) is obtained by introducing additional vertices corresponding to the edges of G. Each edge, eij in G becomes a vertex, vij in L(G), where i and j are utterances in the dialogue. Consequently, if two edges, eij and ekl in G share a node (j = k), they are connected in L(G) by an edge evijvkl . Further, we preserve the context by adding edges between new vertices corresponding to consecutive turns in the dialogue. If eij and ejk are consecutive turns in the dialogue in G, we add an edge, evijvjk in L(G), hence maintaining the sequential structure of the conversation. This transformation from G to L(G) allows for a more nuanced view of the dialogue\u2019s dynamics, highlighting both the sequential and structural aspects of the conversation, as it captures not only the utterance transitions but also the shared intents or actions across different\nturns. The Levi graph, L(G), with nodes denoted by V , is initially embedded using word2vec, producing node embeddings of dimension d. This graph is then subjected to several rounds of convolutions via a relational graph convolutional network (Schlichtkrull et al., 2017). After three convolution rounds, a max-pooling graph operation is used to generate graph embedding, HG. In parallel, a vector representation, HD of the dialogue is generated. Following this, both HD and HG are subjected to dimensionality reduction via a nonlinear transformation followed by a self-attention layer, resulting in H\n\u2032 G and H \u2032 D. Lastly, the OTKE layer infuses the\nperspective graph into the language representations of the dialogue, yielding the final vector, ZP . This vector encapsulates the dialogue\u2019s language representations along with perspective-specific nuances."
        },
        {
            "heading": "4.1.3 Contextual Fusion Module",
            "text": "The Contextual Fusion Module serves as an integrator, fusing the information from both the Global and Perspective Context Encoder. We utilize the OTKE fusion module to fuse the context representations, establishing a synergy between the global and perspective-specific contexts. The vectors, ZG and ZP from the Global and Perspective Context Encoder, respectively, undergoes the OTKE operation to create a fused representation, ZFinal, which ensures a more nuanced understanding of the dialogue by capturing and integrating diverse insights. The vector, ZFinal is then subjected to a multi-headed self-attention mechanism to yield Z\n\u2032 Final. This final vector, Z \u2032 Final is subsequently processed by a standard BART decoder, transforming the context-rich vector representation into a dialogue summary."
        },
        {
            "heading": "5 Experiments and Results",
            "text": "This section details the experimental setup followed by results and analysis of IP-Summ."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "The training was conducted for 20 epochs with a learning rate of 5 x 10\u22125, a batch size of 16, using the Adam optimizer, and an Adam epsilon value of 1 x 10\u22128. Our proposed model, as well as all the ablated models, are built on top of the BARTBase (Lewis et al., 2019) architecture. We split the 1,000 articles from CIMA Dataset into 80% for training, 10% for validation, and 10% for testing. The performance of the generative models was evaluated using several metrics, including aver-\nage BLEU score (Papineni et al., 2002), ROUGE-L score (Lin, 2004), and BERTScore F1 (Zhang et al., 2020a). Furthermore, we conduct a human evaluation of the most effective models. We evaluate the model\u2019s output on the following aspects: Fluency, Informativeness, and Relevance. To assess these models in terms of human evaluation, three independent human users from the authors\u2019 affiliation were asked to rate 100 simulated dialogues on a scale of 1 (worst), 3 (moderate), and 5 (best) based on the above-mentioned criteria. The final reported score is the average of the human-rated scores. In our study, we used the following models as baselines: (1) BART model (Lewis et al., 2019), (2) T5 model (Raffel et al., 2020), (3) DialoGPT model (Zhang et al., 2020b), (4) MultimodalBART (Yu et al., 2021), and (5) Con_Summ (Liu and Chen, 2021), which is the only work to date on perspective-based conditional dialogue summarization. Please refer to Appendix A.3 for details."
        },
        {
            "heading": "5.2 Results and Discussion",
            "text": "A careful review of Table 1 reveals that our proposed model, IP-Summ, excelled across all metrics and perspectives. This model embodies excellent capabilities in text summarization, as demonstrated by its impressive scores: a Rouge score of 0.46, a BLEU score of 0.3, and a BERTScore of 0.91 when assessed from a general viewpoint. IP-Summ\u2019s strong performance remains consistent when evaluated from the perspectives of both the student and the tutor. This implies the model\u2019s adaptability in understanding and summarizing various perspectives, a feature that can be credited to the utilization of the global and perspective context encoder.\nComparison with the Baselines. Delving into the baseline models, MultimodalBART emerged as the front-runner among this group. It scored the highest across all evaluation metrics - Rouge (0.38), BLEU (0.26), and BERTScore (0.81) - presenting a respectable benchmark for comparison. Among the\nbaseline models, BART and DialoGPT delivered relatively comparable results across BLEU and BERT-F1 scores, though BART displayed a slight edge in Rouge scoring. Conversely, T5 lagged in performance across all metrics.\nWith the help of ablation, we obtained crucial insights into the role of individual components within the IP-Summ model. When the ViT component was omitted, there was a significant reduction in the metric scores across all perspectives. This highlights the effectiveness of our proposed global context encoder, which is instrumental in integrating visual context into the dialogue. We observed a similar trend when the RGCN and Levi Graph Generator components were excluded from IP-Summ. Without the RGCN, there was a consistent decrease across all metrics and summaries, which underscores the importance of the dialogue graph encoder. The same trend was evident when we substituted the Levi graph with a standard graph, indicating that the Levi graph encapsulates richer information about the perspective compared to a standard dialogue graph. Additionally, a pattern emerges when we examine the scores across the student and tutor perspectives. For all models evaluated, including IP-Summ, the scores for the student perspective are consistently higher than those for the tutor perspective. This pattern is indicative of the relative complexity of generating tutor summaries compared to that of student summaries.\nEffect of different Components. In addition, we have illustrated the impact of various vision encoders, fusion mechanisms, and graph encoders on our model in Figure 5. (1) When implemented with the Vision Transformer (ViT) (Dosovitskiy et al., 2021), our model demonstrated superior performance across all perspectives compared to other vision models. This highlights the efficacy of transformer-based vision models in effectively modeling image features. The subsequent most\nsuccessful vision model was CLIP (Radford et al., 2021), trailed by VGG-19 (Simonyan and Zisserman, 2015), ResNet (He et al., 2015), and Swin (Liu et al., 2021) vision models. This sequence indicates the dominance of ViT and CLIP over more conventional image encoders. (2) The incorporation of the OTK fusion module into our model resulted in a consistently higher performance across all metrics compared to other fusion techniques, thus, illustrating the superiority of the OTK fusion mechanism. Following the OTK fusion module, the next most effective mechanism was context-aware self-attention (Yang et al., 2019), which was then succeeded by cross-modal attention and, finally, concatenation. This pattern demonstrates how the utilization of a more complex, sophisticated, and robust fusion mechanism can significantly enhance the results. (3) The use of the RGCN encoder led to improved performance in comparison to other graph encoders, such as GAT (Velic\u030ckovic\u0301 et al., 2018) and GCN (Kipf and Welling, 2017). This superior performance may be attributed to RGCN\u2019s ability to handle multi-relation graphs, making it an ideal choice for our Levi graph which contains multiple relations and nodes, unlike a standard dialogue graph.\nHuman Evaluation. Table 2 presents the results for BART, MultimodalBART, and IP-Summ. In terms of fluency, all models demonstrated admirable performance. IP-Summ led this category with a score of 4.3, followed closely by MultimodalBART (4.2) and BART (4). The metric\u2019s high scores across all models underscore their ef-\nfectiveness in language generation. However, the picture becomes more differentiated when considering the metric of informativeness. Here, the BART model, scoring 3.6, did not perform as well as its counterparts. The absence of a visual context component in BART is likely to be a factor, as this component plays a crucial role in generating summaries rich in key information. By contrast, the IP-Summ and MultimodalBART models, incorporating a visual context component, displayed enhanced performance in this area, achieving scores of 4.5 and 4.3, respectively. As for the metric of relevance, the IP-Summ model showed superior performance with a score of 4.3. The model\u2019s perspective encoder, which aligns the summary with the specific perspective, likely plays a significant role in this outcome. The scores achieved by MultimodalBART (3.9) and BART (3.3) in this area underline the importance of incorporating perspective-specific encoding mechanisms in models tasked with generating relevant summaries. To ensure the reliability of our human and automatic evaluations, we computed inter-annotator agreement for human judgments and statistical significance tests for metric improvements. For human evaluation, we calculated the rate of majority agreement across annotators on the acceptability of model responses. This yielded an inter-annotator agreement score of 72.3%, indicating reliable consensus. For automatic metrics, we conducted the statistical significance Welch\u2019s t-test at 5% significance to ensure that the improvement of our proposed model over the baselines is reliable. All the reported results are statistically significant."
        },
        {
            "heading": "Qualitative Analysis of Generated Summaries.",
            "text": "In Figure 6, we showcase two instances of our model-generated student, tutor, and generic summaries, alongside the ground truth summaries. The results demonstrate that the generic summaries generated by our model, IP-Summ, align well with the\nground truth summaries in both the given examples. However, there are instances where IP-Summ falters in other perspectives. For instance, in the first example, IP-Summ\u2019s student summary mistakenly incorporates information from the tutor about \"giving student examples.\" Additionally, we observe that IP-Summ\u2019s tutor summary covers all the core concepts but lacks fluency compared to the student summary, indicating the inherent difficulty in generating tutor summaries as opposed to student summaries. Figure 7 shows sample generated summaries (for the same instances in that of figure 6) from the baseline BART model. Some more analyses of the generated summaries are reported in the Appendix section."
        },
        {
            "heading": "6 Conclusion",
            "text": "The rapid adoption of VTs has brought forth novel challenges and opportunities in enhancing the learning experience amongst which dialogue summarization stands out as a pivotal area warranting dedicated attention. Our work addresses this crucial\naspect by proposing a new task of Multi-modal Perspective-based Dialogue Summarization (MMPerSumm) in an educational setting, which paves the way for a holistic understanding of the VTstudent exchanges. We introduce a novel dataset, CIMA-Summ, that features dialogue summaries from diverse perspectives, offering a multi-faceted view of the learning interaction. We further propose a model, Image and Perspective-guided Dialogue Summarization (IP-Summ), which effectively incorporates both image context and dialogue perspectives into the summarization process."
        },
        {
            "heading": "7 Acknowledgement",
            "text": "Dr. Sriparna Saha gratefully acknowledges the Young Faculty Research Fellowship (YFRF) Award, supported by Visvesvaraya Ph.D. Scheme for Electronics and IT, Ministry of Electronics and Information Technology (MeitY), Government of India, being implemented by Digital India Corporation (formerly Media Lab Asia) for carrying out this research."
        },
        {
            "heading": "Limitations",
            "text": "Despite its contributions, the present study acknowledges several limitations. The original CIMA dataset only includes 1134 conversations, which restricts us to generating summaries solely for these dialogues. This underlines the urgent need for more extensive and comprehensive datasets in a variety of educational settings. Furthermore, the specific application of our approach to language learning may not imply similar efficacy in other educational contexts, such as mathematical or science based learning. Therefore, further validation in these areas is essential to confirm the wider applicability of our proposed method. In addition, this study did not incorporate the potential use of Large Language Models (LLMs), which could offer additional insights and improvement of educational strategies. Future research should aim to overcome these limitations and further explore the potential of our approach."
        },
        {
            "heading": "Ethical Considerations",
            "text": "In the course of this research, we committed to the highest standards of ethical conduct. The data used for this research was derived from a pre-existing, anonymized, and publicly available dataset, ensuring the privacy and confidentiality of the individuals involved. In the creation of our unique dataset, CIMA-Summ, we employed annotators who were compensated in accordance with institutional norms. We took great care to ensure that their work was scheduled within normal working hours, thus, promoting a healthy work-life balance. Moreover, the dataset was constructed in a way to ensure it does not promote or favour any demographic, race, or gender, thus, striving to mitigate the risk of potential bias in language models trained over this. The design and implementation of our model were guided by a robust commitment to avoiding potential harm. The IP-Summ model is purely a tool for dialogue summarization, and it does not involve any predictive or prescriptive functionalities that could impact individual users negatively or unfairly. This model also doesn\u2019t require to store any personal information about students and tutors, thus, preserving the privacy of users."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Frequently Asked Questions (FAQs)",
            "text": "\u273d What was the reason for utilizing the BART-base model as the underlying language model for IP-Summ? \u27a0 We selected BART-Base as the underlying\nmodel for our proposed approach based on comprehensive experimentation. Our findings consistently demonstrated that BART-Base outperforms T5-Base across various metrics and perspectives. While both BART-Base and DialoGPT-Base exhibited comparable performance, we opted for BART-Base due to its encoderdecoder architecture, which enables the encoding of diverse information prior to generating summaries. Moreover, in order to accommodate our computing resources, we specifically focused on the Base versions of all models for our comparative analysis.\n\u273d What motivated us to utilize the CIMA dataset as the foundation dataset and expand it to incorporate summaries for dialogues? \u27a0 We specifically employed the CIMA\ndataset to construct the dataset for our proposed task, MM-PerSumm, for several compelling reasons. In comparison to other educational dialogue datasets (Caines et al., 2022), the CIMA dataset offers a wealth of contextual information pertaining to dialogues. Notably, it includes valuable features such as images, external knowledge, action tags, and intent tags. This comprehensive and multimodal nature of the CIMA dataset makes it exceptionally well-suited for developing a multimodal dialogue summarization dataset, thereby presenting a more relevant and impactful problem statement."
        },
        {
            "heading": "A.2 Annotation Details",
            "text": "Annotation Guidelines. Annotators were guided to ensure that the created summaries were concise, yet informative, capturing the essence of the dialogue without excessive detail avoiding any personal interpretation or embellishment. Th detailed\napproach ensures that the CIMA-Summ dataset offers rich, multi-perspective summaries that can serve as valuable resources for dialogue summarization in the education domain. A comprehensive summary of the annotation guidelines is presented in Table 3.\nAnnotation Quality Assessment. We assessed the quality of the summaries generated by the annotators following the approach described in Ghosh et al. (2022), which utilized two primary metrics : Fluency : It focuses on the linguistic correctness of the summaries using a 5-point Likert scale where a score of \u20191\u2019 indicated a summary riddled with grammatical errors while a score of \u20195\u2019 denoted a well-constructed summary free of grammatical mistakes. Adequacy : This measures the content of the summary using a 5-point Likert scale. A score of \u20191\u2019 suggested that the summary either misrepresented or missed the intent-action labels along with the dialogue\u2019s main points. A score of \u20195\u2019 was given to a summary that accurately captured and reflected the salient points of the dialogue and the intent-action labels.\nTraining of Annotators. Given the specialized nature of dialogue summarization in an educational context, it was critical to train our annotators thoroughly to ensure the quality and consistency of the dataset. The annotators needed to understand not just the dialogue and its nuances but also the unique perspectives of the student and the tutor. Hence, a rigorous annotation training was undertaken. We employed a four-phase training process to ensure the proficiency and competence of the annotators. To initiate the annotation training, a set of dialogues from the CIMA-Summ dataset was pre-annotated to provide gold-standard samples. These examples were annotated with three type of summaries (student perspective, tutor perspective, and overall dialogue summary) by two experienced researchers\n(from author\u2019s collaboration) specializing in educational dialogue systems and summarization. During each phase of training, the annotators were tasked with summarizing a selection of dialogues. They were reminded to follow the guidelines (Table 3) while generating the summaries. Post every phase, an evaluation of the summaries\u2019 fluency and adequacy was conducted. Feedback sessions were arranged after each phase where the annotators interacted with the experienced researchers to discuss the evaluations and ways to enhance the quality of the summaries. These iterative feedback discussions facilitated the development of their annotation skills, gradually improving the quality of the summaries. The annotation guidelines were also refined and updated after each phase, incorporating the insights obtained from these sessions. As a result of these iterative training phases, the quality of the summaries improved significantly from the first phase (fluency = 3.97, adequacy = 2.59) to the fourth phase (fluency = 4.91, adequacy = 4.81), indicating the effectiveness of our annotation training procedure.\nAnnotation Process. Each annotator was equipped with a secure account, permitting individual annotation and tracking of progress in the open-source platform Doccano3. Annotating dialogue summaries is a time-intensive task, requiring a balance between quality and efficiency. On average, our annotators needed 6-8 minutes to adequately summarize each dialogue instance, encapsulating necessary details and understanding the intent-action labels. Given the intricacy of the task, an honorarium of 10 INR per summarized dialogue instance was provided. The original dataset comprises 1134 dialogue instances, all of which were incorporated into the annotation process. We followed a structured schedule over a span of five days in a week as : Day 1 and Day 4: Each annotator was assigned 30 dialogues for summarization, split into three batches of 10 summaries, with a mandatory 20-minute break between each batch to ensure focused and efficient work. Day 2 and Day 5: Annotators were tasked with evaluating summaries produced by their peers, based on the quality assessment criteria outlined above. Day 3: Feedback sessions were held with the annotators to discuss potential improvements\n3https://github.com/doccano/doccano\nand address any challenges they faced during the annotation process."
        },
        {
            "heading": "A.3 Baseline Models",
            "text": "In this section, we will discuss the training process for each baseline model used in our study.\n\u2022 All three of these models \u2014 BART, T5, and DialoGPT \u2014 are unimodal, meaning they are trained to handle one modality of data, in this case, text. For the purpose of our study:\n\u2013 Tutor Perspective Training: The training dataset was structured such that the input consists of a conversation, and the output is the corresponding tutor summary.\n\u2013 Student Perspective Training: Following a similar structure, the input is the conversation, and the output is the student\u2019s summary.\n\u2013 Overall Perspective Training: For a holistic perspective, the training data once again had conversations as input with the overall summary as the output.\nThese models were then trained using the Maximum Likelihood Estimation (MLE) objective function.\n\u2022 Multimodal BART: The Multimodal BART extends the original BART model by incorporating both text and image modalities. The training process for each perspective is detailed below:\n\u2013 Tutor Perspective Training: The input is a combination of conversation text paired with an image, and the output is the corresponding tutor summary.\n\u2013 Student Perspective Training: Similar to the tutor perspective, the input pairs the conversation with an image, and the output is the student\u2019s summary.\n\u2013 Overall Perspective Training: The holistic view takes the conversation and image as input and provides the overall summary as output.\nAn intrinsic feature of this model is the crossmodal attention mechanism within the encoder. This mechanism fuses the representations of text and images, which is subsequently fed into the decoder for further processing. The training objective remains the Maximum Likelihood Estimation (MLE).\n\u2022 Con_Summ: It employs a single BART model adapted for the task of generating perspectivecontrolled summaries from input dialogues. The model requires a two-entry input: Source Input Dialogue: This is the primary conversation that needs to be summarized. Perspective Token: This token indicates the desired perspective (tutor, student, or overall) for which the summary is to be generated. Given these inputs, the output is a summary corresponding to the designated perspective token.\nA.4 CIMA-Summ Dataset Figure 8 illustrates another example instance from our proposed CIMA-Summ dataset."
        },
        {
            "heading": "A.5 Experimental Section",
            "text": "The models were trained on a Tyrone machine equipped with an Intel Xeon W-2155 processor and an 11 GB Nvidia 1080Ti GPU. All the models were implemented using Scikit-Learn and PyTorch."
        },
        {
            "heading": "A.6 Analysis and Discussion",
            "text": ""
        },
        {
            "heading": "Linguistic Analysis of Generated Summaries.",
            "text": "Figure 9 presents the distribution of length for 3 types of generated summaries. The generated summaries have relatively shorter length compared to\nthe ground truth of the complete dataset as shown in Figure 3(a). The generated summaries have a more condensed length distribution compared to the ground truth of the complete dataset i.e. generated summaries show less variability in summary lengths across all three categories (Student, Tutor, and Overall). In Figure 10, we can notice an overall trend where the number of English and corresponding Italian word occurrences are highest in the Overall predicted summary followed by Tutor and Student generated summaries. A similar trend was noticed in the word count distribution for the annotated CIMA-Summ dataset. Also, there are no occurrences of few words in Tutor\u2019s summaries like\n\u2019green\u2019 & it\u2019s Italian translation \u2019verde\u2019, \u2019bed\u2019 & it\u2019s Italian translation \u2019letto\u2019, etc. While they do appear in student summaries indicating that the student\u2019s were able to figure out these Italian words without tutor\u2019s aid and tutor only focused on improving areas were student needed help and correction."
        },
        {
            "heading": "Qualitative Analysis of Generated Summaries.",
            "text": "In Figure 6, we showcase two instances of our model-generated student, tutor, and generic summaries, alongside the ground truth summaries. The results demonstrate that the generic summaries generated by our model, IP-Summ, align well with the ground truth summaries in both the given examples. However, there are instances where IP-Summ falters in other perspectives. For instance, in the first example, IP-Summ\u2019s student summary mistakenly incorporates information from the tutor about \"giving student examples.\" Additionally, we observe that IP-Summ\u2019s tutor summary covers all the core concepts but lacks fluency compared to the student summary, indicating the inherent difficulty in generating tutor summaries as opposed to student summaries. Figure 7 shows sample generated summaries (for the same instances in that of figure 6) from the baseline BART model."
        }
    ],
    "title": "Can you Summarize my learnings? Towards Multi-modal Perspective-based Educational Dialogue Summarization",
    "year": 2023
}