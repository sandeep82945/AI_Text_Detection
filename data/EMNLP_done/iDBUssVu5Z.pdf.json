{
    "abstractText": "Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact transfer, which seeks to transfer the factual content of a source text between topics without modifying its style. We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors. To address these issues, we design ModQGA, a framework that minimally modifies a source text with a novel combination of end-to-end question generation and specificity-aware question answering. Through experiments on four existing datasets adapted for text fact transfer, we show that ModQGA can accurately transfer factual content without sacrificing the style of the source text.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Nishant Balepur"
        },
        {
            "affiliations": [],
            "name": "Jie Huang"
        },
        {
            "affiliations": [],
            "name": "Kevin Chen-Chuan Chang"
        }
    ],
    "id": "SP:b6ce413140c0baaba3cab68c6d7d329b3ad91873",
    "references": [
        {
            "authors": [
                "Fadi Abu Sheikha",
                "Diana Inkpen."
            ],
            "title": "Generation of formal and informal sentences",
            "venue": "Proceedings of the 13th European Workshop on Natural Language Generation, pages 187\u2013193, Nancy, France. Association for Computational Linguistics.",
            "year": 2011
        },
        {
            "authors": [
                "Ali Amin-Nejad",
                "Julia Ive",
                "Sumithra Velupillai."
            ],
            "title": "Exploring transformer text generation for medical dataset augmentation",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4699\u20134708, Marseille, France. European",
            "year": 2020
        },
        {
            "authors": [
                "Chenxin An",
                "Ming Zhong",
                "Zhichao Geng",
                "Jianqiang Yang",
                "Xipeng Qiu."
            ],
            "title": "Retrievalsum: A retrieval enhanced framework for abstractive summarization",
            "venue": "CoRR, abs/2109.07943.",
            "year": 2021
        },
        {
            "authors": [
                "Nishant Balepur",
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Expository text generation: Imitate, retrieve, paraphrase",
            "venue": "arXiv preprint arXiv:2305.03276.",
            "year": 2023
        },
        {
            "authors": [
                "Markus Bayer",
                "Marc-Andr\u00e9 Kaufhold",
                "Bj\u00f6rn Buchhold",
                "Marcel Keller",
                "J\u00f6rg Dallmeyer",
                "Christian Reuter."
            ],
            "title": "Data augmentation in natural language processing: a novel text generation approach for long and short text classifiers",
            "venue": "International journal of",
            "year": 2023
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "CoRR, abs/2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Bhavya Bhavya",
                "Jinjun Xiong",
                "ChengXiang Zhai."
            ],
            "title": "Analogy generation by prompting large language models: A case study of InstructGPT",
            "venue": "Proceedings of the 15th International Conference on Natural Language Generation, pages 298\u2013312,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Yixin Cao",
                "Ruihao Shui",
                "Liangming Pan",
                "Min-Yen Kan",
                "Zhiyuan Liu",
                "Tat-Seng Chua."
            ],
            "title": "Expertise style transfer: A new task towards better communication between experts and laymen",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Ziqiang Cao",
                "Wenjie Li",
                "Sujian Li",
                "Furu Wei."
            ],
            "title": "Retrieve, rerank and rewrite: Soft template based neural summarization",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 152\u2013161.",
            "year": 2018
        },
        {
            "authors": [
                "Asli Celikyilmaz",
                "Elizabeth Clark",
                "Jianfeng Gao."
            ],
            "title": "Evaluation of text generation: A survey",
            "venue": "arXiv preprint arXiv:2006.14799.",
            "year": 2020
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Rui Xu",
                "Ziquan Fu",
                "Wei Shi",
                "Zhongqiao Li",
                "Xinbo Zhang",
                "Changzhi Sun",
                "Lei Li",
                "Yanghua Xiao",
                "Hao Zhou."
            ],
            "title": "E-KAR: A benchmark for rationalizing natural language analogical reasoning",
            "venue": "Findings of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Mingda Chen",
                "Qingming Tang",
                "Sam Wiseman",
                "Kevin Gimpel."
            ],
            "title": "Controllable paraphrase generation with a syntactic exemplar",
            "venue": "Proceedings of",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Pengfei Liu",
                "Hiroaki Hayashi",
                "Zhengbao Jiang",
                "Graham Neubig."
            ],
            "title": "GSum: A general framework for guided neural abstractive summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Hady Elsahar",
                "Pavlos Vougiouklis",
                "Arslen Remaci",
                "Christophe Gravier",
                "Jonathon Hare",
                "Frederique Laforest",
                "Elena Simperl."
            ],
            "title": "T-rex: A large scale alignment of natural language with knowledge base triples",
            "venue": "Proceedings of the Eleventh International",
            "year": 2018
        },
        {
            "authors": [
                "Zhenxin Fu",
                "Xiaoye Tan",
                "Nanyun Peng",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Style transfer in text: Exploration and evaluation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).",
            "year": 2018
        },
        {
            "authors": [
                "Claire Gardent",
                "Anastasia Shimorina",
                "Shashi Narayan",
                "Laura Perez-Beltrachini."
            ],
            "title": "The webnlg challenge: Generating text from rdf data",
            "venue": "Proceedings of the 10th International Conference on Natural Language Generation, pages 124\u2013133.",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Graefe"
            ],
            "title": "2016. Guide to automated journalism",
            "year": 2016
        },
        {
            "authors": [
                "Kilem Li Gwet."
            ],
            "title": "Computing inter-rater reliability and its variance in the presence of high agreement",
            "venue": "British Journal of Mathematical and Statistical Psychology, 61(1):29\u201348.",
            "year": 2008
        },
        {
            "authors": [
                "Chris Hokamp",
                "Qun Liu."
            ],
            "title": "Lexically constrained decoding for sequence generation using grid beam search",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1535\u20131546,",
            "year": 2017
        },
        {
            "authors": [
                "Zhiqiang Hu",
                "Roy Ka-Wei Lee",
                "Charu C. Aggarwal",
                "Aston Zhang."
            ],
            "title": "Text style transfer: A review and experimental evaluation",
            "venue": "SIGKDD Explor. Newsl., 24(1):14\u201345.",
            "year": 2022
        },
        {
            "authors": [
                "Zhiting Hu",
                "Zichao Yang",
                "Xiaodan Liang",
                "Ruslan Salakhutdinov",
                "Eric P. Xing."
            ],
            "title": "Toward controlled generation of text",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning",
            "year": 2017
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang",
                "Jinjun Xiong",
                "Wen-mei Hwu"
            ],
            "title": "Can language models be specific? how? arXiv preprint arXiv:2210.05159",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv., 55(12).",
            "year": 2023
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Zhiting Hu",
                "Olga Vechtomova",
                "Rada Mihalcea."
            ],
            "title": "Deep Learning for Text Style Transfer: A Survey",
            "venue": "Computational Linguistics, 48(1):155\u2013205.",
            "year": 2022
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Lisa Orii",
                "Peter Szolovits."
            ],
            "title": "Hooks in the headline: Learning to generate headlines with controlled styles",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5082\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Tomas Mikolov."
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-",
            "year": 2017
        },
        {
            "authors": [
                "Eleni Kaldoudi",
                "Nikolas Dovrolis",
                "Stathis Th. Konstantinidis",
                "Panagiotis D. Bamidis."
            ],
            "title": "Depicting educational content repurposing context and inheritance",
            "venue": "IEEE Transactions on Information Technology in Biomedicine, 15(1):164\u2013170.",
            "year": 2011
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020a. BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Juncen Li",
                "Robin Jia",
                "He He",
                "Percy Liang."
            ],
            "title": "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Shuai Lin",
                "Wentao Wang",
                "Zichao Yang",
                "Xiaodan Liang",
                "Frank F. Xu",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Datato-text generation with style imitation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1589\u20131598, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Yuning Mao",
                "Xiang Ren",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Constrained abstractive summarization: Preserving factual consistency with constrained generation",
            "venue": "arXiv preprint arXiv:2010.12723.",
            "year": 2020
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Pro-",
            "year": 2013
        },
        {
            "authors": [
                "Samraj Moorjani",
                "Adit Krishnan",
                "Hari Sundaram",
                "Ewa Maslowska",
                "Aravind Sankar."
            ],
            "title": "Audiencecentric natural language generation via style infusion",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1919\u20131932, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Shashi Narayan",
                "Yao Zhao",
                "Joshua Maynez",
                "Gon\u00e7alo Sim\u00f5es",
                "Vitaly Nikolaev",
                "Ryan McDonald."
            ],
            "title": "Planning with learned entity prompts for abstractive summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1475\u20131492.",
            "year": 2021
        },
        {
            "authors": [
                "Phuong Nguyen",
                "Tung Le",
                "Thanh-Le Ha",
                "Thai Dang",
                "Khanh Tran",
                "Kim Anh Nguyen",
                "Nguyen Le Minh."
            ],
            "title": "Improving neural machine translation by efficiently incorporating syntactic templates",
            "venue": "Advances and Trends in Artificial Intelligence. Theory",
            "year": 2022
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "Ms marco: A human generated machine reading comprehension dataset",
            "venue": "choice, 2640:660.",
            "year": 2016
        },
        {
            "authors": [
                "Dave Orr."
            ],
            "title": "50,000 lessons on how to read: a relation extraction corpus",
            "venue": "Online: Google Research Blog, 11.",
            "year": 2013
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: A method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, page 311\u2013318, USA.",
            "year": 2002
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick S.H. Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Shrimai Prabhumoye",
                "Yulia Tsvetkov",
                "Ruslan Salakhutdinov",
                "Alan W Black."
            ],
            "title": "Style transfer through back-translation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866\u2013876,",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
            "venue": "arXiv e-prints, page arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Sudha Rao",
                "Joel Tetreault."
            ],
            "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Shang",
                "Chong Feng",
                "Tianfu Zhang",
                "Da Xu."
            ],
            "title": "Guiding neural machine translation with retrieved translation template",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20137. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Anshumali Shrivastava",
                "Ping Li."
            ],
            "title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "Akhilesh Sudhakar",
                "Bhargav Upadhyay",
                "Arjun Maheswaran."
            ],
            "title": "transforming\u201d delete, retrieve, generate approach for controlled text style transfer",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Asahi Ushio",
                "Luis Espinosa Anke",
                "Steven Schockaert",
                "Jose Camacho-Collados"
            ],
            "title": "BERT is to NLP what AlexNet is to CV: Can pre-trained language models identify analogies",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Dingmin Wang",
                "Ziyao Chen",
                "Wanwei He",
                "Li Zhong",
                "Yunzhe Tao",
                "Min Yang."
            ],
            "title": "A template-guided hybrid pointer network for knowledge-based taskoriented dialogue systems",
            "venue": "Proceedings of the 1st Workshop on Document-grounded Dialogue and",
            "year": 2021
        },
        {
            "authors": [
                "Shuohang Wang",
                "Yichong Xu",
                "Yuwei Fang",
                "Yang Liu",
                "Siqi Sun",
                "Ruochen Xu",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Yunli Wang",
                "Yu Wu",
                "Lili Mou",
                "Zhoujun Li",
                "Wenhan Chao."
            ],
            "title": "Harnessing pre-trained neural networks with rules for formality style transfer",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Bolin Wei",
                "Yongmin Li",
                "Ge Li",
                "Xin Xia",
                "Zhi Jin."
            ],
            "title": "Retrieve and refine: exemplar-based neural comment generation",
            "venue": "Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, pages 349\u2013360.",
            "year": 2020
        },
        {
            "authors": [
                "Matthew West",
                "Geoffrey L Herman",
                "Craig Zilles."
            ],
            "title": "Prairielearn: Mastery-based online problem solving with adaptive scoring and recommendations driven by machine learning",
            "venue": "2015 ASEE Annual Conference & Exposition, pages 26\u20131238.",
            "year": 2015
        },
        {
            "authors": [
                "Colin Cherry."
            ],
            "title": "Paraphrasing for style",
            "venue": "Pro-",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text style transfer aims to control the stylistic attributes of text, such as sentiment or formality, without affecting its factual content (Jin et al., 2022; Hu et al., 2022). This task has several applications, including personalizing dialogue agents (Rao and Tetreault, 2018; Zheng et al., 2020), increasing persuasiveness in marketing or news (Jin et al., 2020; Moorjani et al., 2022), or simplifying educational resources (Wang et al., 2019; Cao et al., 2020).\nWhile text style transfer models can adeptly alter stylistic elements, they do not address all text modification needs, especially those centered on factual modifications. Specifically, there exist several applications that require the transfer of factual content between topics without altering style, such as adapting past news articles for current events\n1Code is available at https://github.com/nbalepur/ text-fact-transfer.\n(Graefe, 2016) and repurposing educational materials for new subjects (Kaldoudi et al., 2011), which are outside the scope of text style transfer. Further, studying methods to transfer facts while preserving style could be useful for augmenting datasets, i.e., expanding training sets with new, factual training examples in a similar style (Amin-Nejad et al., 2020; Bayer et al., 2023), or evaluating the factual accuracy of text generation models (Celikyilmaz et al., 2020; Ji et al., 2023).\nTo address these needs, we propose the task of text fact transfer, which aims to modify the factual content of a source text while preserving its style. We define factual content as topic-specific entities that convey knowledge and style as how the factual content is phrased and organized, as well as its level of specificity2. As shown in Figure 1 (top), given as inputs a source text, source topic, target topic, and corpus of facts for the target topic, we seek to generate a target text that matches the style of the source text and contains factual content specific to the target topic. Thus, while text style transfer aims to modify subjective, stylistic aspects of text, text\n2Depending on the setting, this definition of style may need to be modified. For example, in educational repurposing, it may be infeasible to keep the phrasing consistent, as different subjects may need to be discussed and phrased differently.\nfact transfer controls the objective, factual content.\nOne approach for text fact transfer (on parallel corpora) is to train/prompt seq2seq or large LMs (Lewis et al., 2020a; Brown et al., 2020). However, there are two inherent challenges to text fact transfer that cannot be overcome by directly applying these models. First, the generated text must not deviate from the wording of the source text, but LMs may not always succeed in this regard (Balepur et al., 2023). For example in Figure 1, GPT-3.5 states that Nelson Mandela \u201cwas a member of\u201d the ANC, which is inconsistent with the phrasing of \u201cbelongs to\u201d present in the source text.\nSecond, along with being accurate, the factual content must align with the specificity of the source text to best maintain its style. However, LMs have been shown to hallucinate (Ji et al., 2023) and struggle to control the specificity of their outputs (Huang et al., 2022). For example, as seen in Figure 1, the seq2seq LM states that \u201cNelson Mandela belongs to Rhodesia.\u201d Although the leader has some links to Rhodesia, it is inaccurate to state that he belongs there. Further, the source text contains the political party of Joseph Stalin (i.e., Communist Party), so the target text should contain a political party (i.e., ANC) rather than a country, which is less specific.\nFurther, these challenges become more complex if supervised text fact transfer is infeasible. For example, when adapting past news for current events or augmenting datasets, it could take ample time and effort to construct parallel corpora and train a supervised model. In such cases, 0-shot models, while harder to develop, are preferred, as they can adapt to domains without extra training. Hence, we must study 0-shot and supervised text fact transfer models to ensure adaptability in downstream tasks.\nTo address these challenges of text fact transfer, we extend the concept of minimal alterations for text style transfer proposed by Li et al. (2018) and seek to execute the two-step process of: (1) locating factual entities in the source text; and (2) solely transferring these entities between topics. To perform step one, we note that factual entities are inherently question-driven, and thus any entity in the source text that must be transferred can answer a question. For example in Figure 1, the factual entity \u201cCommunist Party\u201d answers the question \u201cWhat is Joseph Stalin\u2019s party?\u201d. To perform step two, we find that transferring entities between topics is challenging, but transferring questions that can retrieve said entities is simple. For example, transferring\n\u201cCommunist Party\u201d to \u201cANC\u201d directly is difficult, but we can easily transfer \u201cWhat is Joseph Stalin\u2019s party?\u201d to \u201cWhat is Nelson Mandela\u2019s party?\u201d by replacing the source topic (Joseph Stalin) with the target topic (Nelson Mandela), returning a question that can be used to retrieve the entity \u201cANC.\u201d\nExploiting these findings, we design ModQGA, a model that minimally modifies the source text with a combination of Question Generation (QG) and Answering (QA). As shown in Figure 2, ModQGA first uses end-to-end QG to jointly produce entities from the source text and questions that can be answered by said entities. Next, these questions are transferred to pertain to the target topic. ModQGA then uses specificity-aware QA to retrieve an answer from the corpus for each transferred question, while matching the specificity of the source text entities. Finally, these answers are filled into the source text. Solely modifying factual entities allows for the preservation of the phrasing of the source text, while the focused approach of transferring entities with specificity-aware QA promotes factuality and matched specificity, as shown in Figure 1. Further, we can train the QG and QA models of ModQGA on external QA datasets (Rajpurkar et al., 2016), resulting in a 0-shot model that can be applied to diverse domains without extra training.\nWe showcase the strength of ModQGA for text fact transfer by creating four parallel corpora from existing datasets, spanning expository text generation (Balepur et al., 2023) and relationship triples (Elsahar et al., 2018; Gardent et al., 2017). Hence, our initial study of text fact transfer focuses on the adaptation of expository texts and relationship triples, leaving applications such as repurposing news articles and dataset augmentation for future research. Using these datasets, we design a 0-shot and supervised version of ModQGA and in our experiments, find that both models outperform their respective baselines in style preservation and factuality on a majority of datasets.\nOur contributions can be summarized as follows:\n1) We propose the task of text fact transfer, which aims to alter factual content while preserving style. 2) To solve our task, we design ModQGA, which minimally modifies a source text with an ensemble of end-to-end QG and specificity-aware QA. We qualitatively assess the latter, which shows at least some ability to control the specificity of its answer. 3) We adapt four datasets for text fact transfer. 4) Through experiments on our four datasets, we\ndemonstrate that ModQGA generates factual text that is stylistically consistent with the source text."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Text Style Transfer",
            "text": "Text style transfer aims to modify the style of text without inherently affecting its content (Fu et al., 2018; Jin et al., 2022; Hu et al., 2022). The concept of style can take many forms, including formality (Wang et al., 2019; Zhang et al., 2020), sentiment (Prabhumoye et al., 2018; Yang et al., 2018), and authorship (Xu et al., 2012). Text fact transfer is the counterpart to text style transfer, as we focus on transferring the factual content of text between topics without affecting its underlying style. Hence, our task emphasizes generating new, factual text, which is not the main focus of style transfer tasks.\nSeveral methods have been developed for text style transfer, such as training neural models on parallel corpora (Rao and Tetreault, 2018; Xu et al., 2019), latently disentangling content and style (Hu et al., 2017; Shen et al., 2017), or prototype editing (Li et al., 2018; Sudhakar et al., 2019; Abu Sheikha and Inkpen, 2011). ModQGA is most similar to the Delete-Retrieve-Generate model (Li et al., 2018), which extracts attribute markers, transfers attributes across styles, and generates an output. We apply a similar technique for text fact transfer, but notably use a novel combination of end-to-end question generation and specificity-aware question answering, which has not been explored in prior work."
        },
        {
            "heading": "2.2 Stylistic Exemplars",
            "text": "Recent work has studied models that leverage stylistic exemplars to guide stylistic choices in text generation (Cao et al., 2018; Wei et al., 2020). Such exemplars improve the fluency of seq2seq models in various tasks, including summarization (Dou et al., 2021; An et al., 2021), machine translation (Shang et al., 2021; Nguyen et al., 2022), dialogue generation (Zheng et al., 2020; Wang et al., 2021), and question answering (Wang et al., 2022).\nMore relevant to text fact transfer are tasks that require strictly adhering to the style of an exemplar. Chen et al. (2019) propose the task of controllable paraphrase generation, which aims to combine the semantics from one sentence and the syntax from a second sentence. Lin et al. (2020) introduce \u201cstyle imitation\u201d and perform data-to-text generation while strictly maintaining the style of an ex-\nemplar. Apart from a lack of focus on transferring factual content, these works differ from our task as they do not leverage a factual corpus.\nThe task most similar to ours is expository text generation (ETG) (Balepur et al., 2023), which seeks to generate factual text from a corpus in a consistent style. However, ETG dictates that this style is learned from examples of outputs in the same domain, while text fact transfer adheres to the style of a single source text. Hence, an ETG model is domain-specific, while a single text fact transfer model (e.g., 0-shot ModQGA) could be used in several domains. Further, the IRP model proposed by Balepur et al. (2023) for ETG combines content planning, retrieval, and rephrasing, while ModQGA modifies a source text with question generation and answering, and our model tackles the additional problem of controlling specificity (\u00a73.3)."
        },
        {
            "heading": "2.3 Analogy Completion",
            "text": "The concept of transferring entities between topics is similar to analogy completion (Ushio et al., 2021; Bhavya et al., 2022; Chen et al., 2022), which aims to select a word that parallels an input query-word pair (e.g., \u201cParis:France, Lima:[MASK]\u201d). While analogy completion could be used for factual entity transfer, this is only one aspect of text fact transfer. Further, our task is fundamentally a text generation task, while analogy completion is typically used to assess how models internally capture relations."
        },
        {
            "heading": "3 Methodology",
            "text": "Given a source text Ds, source topic ts, and target topic tt, text fact transfer aims to produce a target text Dt that matches the style of Ds and modifies the entities related to the source topic ts with entities related to the target topic tt. To serve as ground truth information for tt, we also provide a corpus of factual sentences C related to the target topic tt.\nAs illustrated in Figure 2, the backbone of ModQGA consists of two key modules: (i) An end-toend question generator p(Qs|Ds, ts) that produces question/entity pairs (q, e) \u2208 Qs, where each q can be answered by e using the source text Ds; and (ii) A specificity-aware question answering model p(\u27e8ai, aj\u27e9|c, q, e) that extracts an answer span \u27e8ai, aj\u27e9 from the context c (where c \u2286 C), which answers question q and matches the specificity of the entity e. After training these models, ModQGA performs text fact transfer via: 1) endto-end question generation with p(Qs|Ds, ts); 2)\nquestion transferring; 3) question answering with p(\u27e8ai, aj\u27e9|c, q, e); and 4) source text infilling. We will describe each of these steps followed by how they are combined for the full ModQGA model."
        },
        {
            "heading": "3.1 End-to-End Question Generation",
            "text": "Our approach to text fact transfer is rooted in the observation that all entities in the source text that need to be transferred can be viewed as an answer to a question. For example, given the source text \u201cIbuprofen is used to relieve pain,\u201d transferring between the topics Ibuprofen and Melatonin may result in the text \u201cMelatonin is used to promote sleep.\u201d The part of the source text that needs to be transferred (apart from the known transfer of \u201cIbuprofen\u201d to \u201cMelatonin\u201d) is \u201cto relieve pain,\u201d which can answer the question \u201cWhy is Ibuprofen used?\u201d. This question-answer paradigm helps us guide the modification process in text fact transfer.\nHence, to identify entities that need to be transferred and the questions that can be answered by said entities, we train an end-to-end question generation model that jointly generates entities and their questions from a context. To do so, we leverage the SQuAD-V2 dataset (Rajpurkar et al., 2016). We train BART-large (Lewis et al., 2020a) to minimize the loss \u03bbqg of token prediction of question q and answer (entity) e, surrounded by <|question|> and <|answer|> tokens (represented as \u27e8q \u00b7 e\u27e9), conditioned on the context c and topic t:\n\u03bbqg = \u2212 |\u27e8q\u00b7e\u27e9|\u2211 i=1 log p(\u27e8q \u00b7e\u27e9i|c, t, \u27e8q \u00b7e\u27e91, ..., \u27e8q \u00b7e\u27e9i\u22121). (1)\nIf a generated question q contains the source topic ts, q can be simply transferred to the target topic tt by replacing ts with tt. To elicit this desirable\nproperty, we only keep SQuAD entries where the topic is a substring of the question.3 Thus, all training questions contain the topic tt, teaching the model to produce tt in the output during inference.\nTo ensure all factual entities in the source text are detected, we use nucleus sampling to generate n question/entity pairs Qs = {(qj , ej)}nj=1 with each sentence of the source text Ds as the context c and source topic ts as the topic t. Thus, each unique factual entity from the source text may be mapped to multiple questions, which are ensembled by the specificity-aware question answering model (\u00a73.3).\nAs a final post-processing step, we discard pairs in Qs with an entity that does not appear in the source text, as this entity is hallucinated. Further, if an entity is a substring of another entity in Qs, we discard the substring (shorter) entity."
        },
        {
            "heading": "3.2 Question Transferring",
            "text": "Each question qj found in the question/entity pairs (qj , ej) \u2208 Qs pertain to the source topic ts, but we require a transferred question q\u2032j that pertains to the target topic tt. Since qj will contain the substring ts, we can simply replace ts with tt to obtain q\u2032t.\nThrough testing on our validation sets, we also find that generic queries can outperform specific queries when retrieving contexts for question answering. For instance, we find that the generic query \u201cWhat is the hub of the airport?\u201d outperforms the specific query \u201cWhat is the hub of Cathay Pacific Airport?\u201d. We find this occurs because the inclusion of the topic tt in the query distracts the retriever when searching C for contexts in QA, as it is more biased towards facts that contain the to-\n3We found that performing lexically constrained token decoding (Hokamp and Liu, 2017) with topic tt led to a similar outcome, but this resulted in higher GPU memory usage.\nkens in tt, even when said facts are not relevant to the query.4 We show the benefit of generic queries experimentally with ablation studies (\u00a75.3).\nTo obtain a generic question q\u2032\u2032j , we take the intersecting tokens of qj and q\u2032j , which eliminates the topic-specific tokens found in ts and tt. Combining the specific and generic questions, we obtain a set of transferred questions and their corresponding source entities Qt = {(q\u2032j , ej)}nj=1\u222a{(q\u2032\u2032j , ej)}nj=1."
        },
        {
            "heading": "3.3 Specificity-Aware Question Answering",
            "text": "After creating the transferred question-entity pairs Qt, we faithfully answer each transferred question by retrieving a context from the factual source C followed by extractive question answering (QA). However, an off-the-shelf QA model cannot be used for our task, as it fails to consider the specificity of the answer we require (Huang et al., 2022). To extract transferred entities that are stylistically aligned with the source text, we seek answers with the same level of specificity as the entities they are replacing. For example, at one step in ModQGA, we may obtain the question \u201cWhere is Stanford located?\u201d derived from the source entity \u201crural\u201d. While \u201cCalifornia,\u201d \u201cPalo Alto,\u201d and \u201csuburban\u201d are all valid answers, \u201csuburban\u201d is the best choice, as it shares the same level of specificity as \u201crural,\u201d and thus best matches the style of the source text.\nTo create a dataset with these specifications, we again modify the SQuAD-V2 dataset (Rajpurkar et al., 2016). The dataset already provides questions, contexts, and answer spans, but we still require guidance to match the specificity levels of the answers. We find that one way to obtain specificity guidance of an answer is through the skip-gram assumption (Mikolov et al., 2013)\u2014similar words are discussed in similar contexts. For example, we intuit that because \u201crural\u201d and \u201csuburban\u201d are used in the same context (e.g., \u201cthe location is [suburban/rural]\u201d), they have similar specificity levels. Hence, we obtain specificity guidance for each answer in the SQuAD dataset by replacing every word in the answer with a random top-20 skip-gram synonym via fastText embeddings (Joulin et al., 2017).\nWe use BERT-large (Devlin et al., 2018) to train our specificity-aware QA model p(\u27e8ai, aj\u27e9|c, q, e). We minimize \u03bbqa, the sum of \u03bbi, the cross-entropy loss of the predicted start index ai and \u03bbj , the loss of the predicted end index aj , conditioned on the\n4We note that the specific query can find the facts with top-k retrieval for large k, but our QA model is trained to use fewer contexts (k = 5), hence our need for generic queries.\ncontext c, question q, and specificity guidance e:\n\u03bbi = \u2212 N\u2211 z=1 log p(ai|c, q, e)I(z = i), (2) \u03bbj = \u2212 N\u2211 z=1 log p(aj |c, q, e)I(z = j), (3)\n\u03bbqa = \u03bbi + \u03bbj , (4)\nwhere I is the indicator function and N is the number of tokens in the input sequence.\nFor each transferred question/source text entity pair (q, e) \u2208 Qt, we first use Contriever (Izacard et al., 2022) to obtain the context c, i.e., the top-k most relevant facts to q in C via maximum innerproduct search (Shrivastava and Li, 2014). Next, the question q, entity e (specificity guidance), and context c are fed through the specificity-aware QA model p(\u27e8ai, aj\u27e9|c, q, e). We record the predicted answer a = \u27e8ai, aj\u27e9 with the highest likelihood (sum of start and end likelihoods) under length m.\nWe map each unique entity e in Qt to the answer a with the highest total likelihood. This process returns a map E with each source text entity e as the key and its transferred entity a as the value."
        },
        {
            "heading": "3.4 Source Text Infilling",
            "text": "Lastly, we infill the source text Ds, replacing each entity e with its mapped entity a in E . We describe zero-shot and supervised infilling methods below: Zero-shot: Given that each entity e appears in the source text Ds, we replace every occurrence of e with a and ts with tt to create the target text Dt. Supervised: We train the LED language model (Beltagy et al., 2020) to generate the target text Dt using the source topic ts, source text Ds, target topic tt, corpus C, and each transferred entity a (surrounded by <|answer|> tokens). This process is similar to keyword-guided text generation techniques (Mao et al., 2020; Narayan et al., 2021). Overall, the supervised version of ModQGA allows the model to have more flexibility during infilling.\nAlthough ModQGA is designed primarily as a 0-shot text fact transfer model, using custom components trained on external SQuAD datasets, altering the infilling process allows us to fairly compare our model with supervised baselines (\u00a74.2)."
        },
        {
            "heading": "3.5 The ModQGA Framework",
            "text": "In Algorithm 1, we use the above components to design ModQGA. First, ModQGA performs endto-end question generation with the BART model\nAlgorithm 1 ModQGA 1: procedure MODQGA(Ds, ts, tt, C, n, m, k) 2: Qs \u2190 {},Qt \u2190 {}, E \u2190 [map : E \u2192 (A,S)] 3: while |Qs| < n do 4: Qs \u2190 Qs \u222a E2E-QG(Ds, ts) 5: Qt \u2190 Qt \u222a QUESTIONTRANSFER(Qs, ts, tt) 6: for (q, e) \u2208 Qt do 7: c\u2190 CONTRIEVER(q, k, C) 8: a, score\u2190 SA-QA(q, e, c,m) 9: a\u2032, score\u2032 \u2190 E(e) \u25b7 Lookup e in E map 10: if score > score\u2032 then 11: E(e)\u2190 (a, score) \u25b7 Update best answer 12: Dt \u2190 INFILL(Ds, E) 13: return Dt\np(Qs|Ds, ts), to generate n question/entity pairs Qs covering the factual content of the source text Ds. ModQGA then transfers the questions in Qs from the source topic ts to the target topic tt to create Qt, which has specific and generic questions. For each transferred question q and source entity e in Qt, ModQGA performs specificity-aware QA with the BERT model p(\u27e8ai, aj\u27e9|c, q, e). We build the map E , containing each source entity e mapped to the answer a with the highest likelihood, to represent its transferred entity. Last, using E , ModQGA infills the source text Ds to create the target text Dt, either in a 0-shot or supervised manner."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We provide a detailed setup in Appendix A."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We adapt the following tasks and datasets to construct parallel corpora for text fact transfer: 1) Expository Text Generation (ETG) uses topicrelated sentences to create multi-sentence factual texts in a consistent style (Balepur et al., 2023). We adapt the U.S. News and Medline datasets, spanning college and medical domains. We use the output text as the target text and retrieve/create training examples for the source text (see Appendix A.1). We use the document titles for the source/target topics, and the provided corpus for C. 2) Relationship triples have a subject x, predicate y, and relation r between x and y. We adapt the t-REX (Elsahar et al., 2018) and Google (Orr, 2013; Petroni et al., 2019) relationship triple datasets. t-REX contains open-domain relations, while Google contains biographical relations. The open-domain nature of t-REX allows us to assess the adaptability of each baseline. We obtain triples that share a relation r (i.e., \u27e8x1, r, y1\u27e9 and\n\u27e8x2, r, y2\u27e9) and use x1 \u00b7 r \u00b7 y1 as the source text and x2 \u00b7 r \u00b7 y2 as the target text (\u00b7 denotes concatenation). We use x1 and x2 as the source and target topics. For t-REX, we use the Wikipedia texts in the dataset for C, and for Google, we scrape sentences from the top-7 web pages queried with x2."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare zero-shot ModQGA (0-shot ModQGA) with the following zero-shot baselines: 1) 0-Shot GPT: We use a zero-shot prompt (Appendix A.3) instructing GPT-3.5 to create the target text using the source text, source topic, and target topic. This model uses its internal knowledge. 2) 0-Shot GPT+Retr: We add the top-5 retrieved facts from C as an extra input to 0-Shot GPT. 3) SourceCopy: We trivially copy the source text as the predicted output for the target text.\nWhen parallel data exists in text style transfer, seq2seq models are typically used (Jin et al., 2022). Thus, for our parallel text fact transfer setting, we compare supervised ModQGA (ModQGA-Sup) with the following supervised seq2seq models: 1) z-Shot GPT: We construct a z-shot prompt for GPT-3.5 to generate the target text with the source text, source topic, and target topic as inputs. This model relies on its internal knowledge. 2) z-Shot GPT+Retr: We add the top-5 retrieved facts from C as an extra input to z-Shot GPT. 3) LED: LED (Beltagy et al., 2020) is a seq2seq LM based on the Longformer. LED produces the target text using the source text, source topic, target topic, and corpus as inputs. This model is ModQGA-Sup without the transferred entities as inputs. 4) BART+Retr: Similar to RAG (Lewis et al., 2020b), we retrieve the top-25 facts from C and train BART to generate the target text using the source text, source/target topics, and retrieved facts.\nAll GPT-3.5 models are gpt-3.5-turbo with a temperature of 0.2. Models that perform retrieval use the same Contriever setup (Izacard et al., 2022) as ModQGA. The input query used is the source text Ds with every occurrence of ts replaced with tt. We found that this query outperforms solely the target topic tt, as it provides the Contriever context as to which information to search for (see Table 6)."
        },
        {
            "heading": "4.3 Quantitative Metrics",
            "text": "We measure the output similarity of the predicted and target texts with ROUGE-1/2 (R1/R2) and\nBLEU (Lin, 2004; Papineni et al., 2002), serving as proxies for style preservation of the source text.\nTo evaluate factuality, we adopt three metrics: 1) Halluc calculates the average percentage of tokens that are extrinsically hallucinated, meaning that they do not appear in the corpus C or source text Ds; 2) FactCC (Kryscinski et al., 2020) is a classifier that predicts if any factual errors exist between a source text and claim. We use the true output as the source and each sentence of the generated text as the claim, and report the proportion of sentences with no factual errors; 3) NLI-Ent uses textual entailment to predict whether a claim is entailed by a source (Maynez et al., 2020). We train a DistilBERT (Sanh et al., 2019) classifier on the MNLI dataset (Williams et al., 2018) (accuracy of 0.82) and report the proportion of sentences in the generated text that are entailed by the true output.\nAll metrics are reported from a single run."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Quantitative Performance",
            "text": "In Table 1, we see that 0-shot ModQGA excels at text fact transfer, achieving the strongest results in 22/24 metrics. This is impressive given that ModQGA has significantly less parameters than GPT-3.5 (0.8B vs 175B). We also note that 0-shot ModQGA outperforms ModQGA-Sup on opendomain t-REX, showing that our 0-shot model is more adaptable than its supervised version, but is surpassed by BART+Retr, opening the door to research in 0-shot text fact transfer to close this gap.\nIn Table 2, we find that ModQGA-Sup outperforms baselines on three datasets (17/18 metrics on U.S. News/Medline/Google), and achieves the second strongest results on t-REX. Further, ModQGA-\nSup surpasses LED in 23/24 metrics, meaning that our extra input of transferred entities is valuable for improving the style and factuality of seq2seq models in text fact transfer. These findings suggest that our strategy of identifying entities, transferring entities between topics, and infilling, can outperform generating text solely in a seq2seq manner.\nFinally, we note that GPT-3.5 fails to produce factual text, obtaining much lower factuality scores that are not always improved by using the corpus C. The LLM also struggles to adhere to the style of the source text, shown by the lower output similarity scores and larger length ratios. Thus, text fact transfer highlights the limitations of GPT-3.5 with preserving factuality and style, meaning that our task could benchmark these capabilities of LLMs."
        },
        {
            "heading": "5.2 Human Evaluation",
            "text": "We invite two computer science and engineering students to evaluate 50 generated outputs from U.S. News and Google on style (i.e. which output best matches the source text style) and factuality (i.e. which output is more factual). Following best practices, we use a pairwise comparative evaluation (Lewis et al., 2020b). To study the issues of 0-shot LLMs, we compare 0-shot ModQGA and 0-Shot GPT+Retr, and to study if the extra inputs of transferred entities aid seq2seq models, we compare ModQGA-Sup and LED.\nIn Table 3, the evaluator ratings indicate that 0- shot ModQGA better preserved style compared to 0-Shot GPT+Retr in over 55% of cases on both datasets and was more factual on U.S. News in 47% of cases, highlighting that ModQGA is a preferred choice for the challenging task of 0-shot text fact transfer. Further, evaluators indicated that ModQGA-Sup outperformed LED in factuality in 46% of cases on U.S. News, once again suggesting that our transferred entities can improve the factual\naccuracy of seq2seq models. These findings parallel our quantitative results (\u00a75.1), reinforcing that ModQGA can effectively transfer factual content without sacrificing the style of the source text."
        },
        {
            "heading": "5.3 Ablation Studies",
            "text": "We conduct an ablation study (Table 4, full results Appendix 8) and note that the use of generic questions and specificity guidance improve the output similarity and factuality of 0-shot ModQGA. We find the specificity result to be noteworthy, as it means controlling specificity can enhance the performance of 0-shot text fact transfer frameworks."
        },
        {
            "heading": "5.4 Specificity-Aware QA Analysis",
            "text": "In Figure 3, we assess the abilities of our specificityaware QA model. Overall, we find that the model does use the specificity of the entity guidance, having the ability to provide a regional descriptor (\u201cresidential\u201d), city (\u201cTallahassee\u201d), city and state (\u201cTallahassee, Florida\u201d), and city descriptor (\u201cThe State Capital\u201d). This suggests that our model has at least some ability to control the specificity of its answers.\nDespite these strengths, our QA model may still err. Specifically, the model may identify a part of the context that matches the specificity of the entity, even though it does not correctly answer the question (e.g., \u201cNorth\u201d comes from the context \u201cNorth side of campus,\u201d but the correct answer is \u201cSouth\u201d). Further, the model may be biased towards answers\nthat match the length of the entity, even if the specificity is not matched (e.g., predicting \u201cTallahassee\u201d instead of \u201cFlorida\u201d). Finally, if the provided entity is drastically unrelated to the question (e.g., \u201c200\u201d), so will the answer (e.g., \u201c185\u201d). Controlling specificity is a difficult task (Huang et al., 2022), but we believe our specificity-aware QA model reveals a potential direction to address this problem."
        },
        {
            "heading": "5.5 Sample Outputs",
            "text": "In Appendix B.3, we present examples of target texts generated by ModQGA and other baselines."
        },
        {
            "heading": "6 Conclusion",
            "text": "We propose the task of text fact transfer and develop ModQGA to overcome the difficulty of LMs to perform our task. ModQGA leverages a novel combination of end-to-end question generation and specificity-aware question answering to perform text fact transfer. Through experiments on four datasets, including human evaluation, we find that 0-shot and supervised ModQGA excel in style preservation and factuality on a majority of datasets. We conduct an ablation study to reveal the strengths of our design choices of ModQGA. Finally, we perform a qualitative analysis of our specificity-aware question answering model, which shows at least some ability to control the specificity of its answers."
        },
        {
            "heading": "7 Limitations",
            "text": "One limitation of 0-shot ModQGA is that it has a slower inference time compared to the 0-Shot GPT models. Although our model shows improvements in factuality and style over the GPT models, we acknowledge that it is important to ensure our framework is computationally efficient. The slowest part of ModQGA is the ensembling of multiple questions during question answering. Hence, we believe future research could improve upon ModQGA by identifying a subset of generated questions that are likely to produce high-quality answers, and only using this subset in ModQGA. This could make contributions to an interesting research area of high-quality question identification.\nFurther, we assume that the factual corpora used in our tasks are error-free and do not contain contradictions. Hence, we did not assess how any text fact transfer framework would perform if placed in a setting with misinformation. This could be an interesting future setting for text fact transfer, as\nany model to solve the task would now have to incorporate the extra step of fact verification, making the task more similar to its downstream use case."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "The goal of text fact transfer is to transfer the factual content of a source text while preserving its original style, which we accomplish by designing ModQGA. As mentioned in the introduction, some downstream applications of text fact transfer could include automatically generating news for current events by leveraging a previous news article for a similar event or repurposing existing educational materials for new subjects. However, as with all text generation frameworks, a model like ModQGA which is designed for text fact transfer could still hallucinate factual errors. Hence, to avoid the spread of misinformation and inaccurate factual content, ample considerations and thorough evaluations must be made before leveraging a text fact transfer framework in downstream applications."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "We thank the anonymous reviewers for their feedback. This material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizon Network, grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies."
        },
        {
            "heading": "A Experimental Setup",
            "text": "A.1 Datasets\nThe U.S. News dataset in ETG already follows a very consistent style that can be adapted for text fact transfer. Hence, we take each output as the target text, and match it with a source text by selecting a random example from the training set. We match descriptions for public colleges with other random descriptions for public colleges, and the same for private colleges, as there are slight differences in the style between these descriptions when describing tuition. The last sentence of the public college descriptions follow the form \u201cThe in-state tuition is X; the out-of-state tuition is Y\u201d, while the last sentence of the private college descriptions follow the form \u201cThe tuition is X\u201d.\nFor the Medline dataset in ETG, retrieving a similar example for the source text cannot be done in the same way, as the style is less consistent. Hence, we first convert each output document to a consistent style by performing question answering using the output as the context with the following questions: 1) \u201cWhat is [topic] used to treat?\u201d; 2) \u201cWhat class of medications does [topic] fall into?\u201d; 3) \u201cHow does [topic] work?\u201d. Using these answers, we construct a document through the template: [Topic] is used to treat [(1)]. It belongs to a class of medications called [(2)]. It works by [(3)]. For question answering, we leverage the RoBERTa-Base model trained on SQuAD5. To ensure all collected outputs fall into this template, we discard documents which provide a negative logit score to any of the three questions. We then use the same process as U.S. News to match source and target texts.\nThe Google and t-REX datasets do not require any modifications, as we simply pair source texts and target texts by finding relation triples that share a relation. To obtain the factual corpora C for each target topic tt in Google, we web scrape using the query \u201ctt Wikipeida.\u201d We keep only alphanumeric characters and punctuation, and decode the text with unidecode. We qualitatively analyzed a sample of corpora and did not find any personal identifiable information. To be safe, we use the Presidio6 analyzer provided by Microsoft and remove all sentences with the following de-\n5https://huggingface.co/deepset/ roberta-base-SQuAD2\n6https://microsoft.github.io/presidio/ analyzer/\ntected entities (prediction score > 0.3): \u201cPHONE NUMBER\u201d, \u201cCRYPTO\u201d, \u201cEMAIL ADDRESS\u201d, \u201cIBAN CODE\u201d, \u201cIP ADDRESS\u201d, \u201cMEDICAL LICENSE\u201d, \u201cUS BANK NUMBER\u201d, \u201cUS DRIVER LICENSE\u201d, \u201cUS ITIN\u201d, \u201cUS PASSPORT\u201d, \u201cUS SSN\u201d.\nWe provide summary statistics of each dataset in Table 5. All datasets are in English.\nA.2 Training Setup\nThe question generation model of ModQGA is trained with BART Large (406M), using a batch size of 8, learning rate of 2e-5, weight decay of 0.01, 500 warmup steps, 8 gradient accumulation steps, and 3 training epochs. The question answering model of ModQGA is trained with BERT Large (340M) using the same parameters. We select answers spans with a maximum length m equal to two times the length of the entity specificity guidance. We generate n = 10 sequences in end-to-end question generation with nucleus decoding (topp = 0.75). During retrieval, we select k = 5 texts.\nThe infilling for ModQGA-Sup and LED are implemented with the same LED model (Beltagy et al., 2020) (149M), using a batch size of 1, learning rate of 5e-5, and 1500 warmup steps. We train each model for 15 epochs and after training, load the model with the lowest validation loss with respect to each epoch. We use a maximum input size of 16384 to encode the input corpus, a maximum output length of 256 for U.S. News and Medline, and a maximum output length of 64 for Google and t-REX. The training time for this model was around 10 hours on each dataset. The BART model in BART+Retr is trained with the same parameters and similar model size (140M) as the LED model, but instead using a maximum input size of 1024. Using the same strategy, we train the model for 10 epochs and after training, load the model with the lowest validation loss with respect to each epoch. We ensured that the validation loss of each seq2seq model converged on our datasets.\nAll GPT-3.5 models are gpt-3.5-turbo (175B) with a temperature of 0.2. For U.S. News and Medline, we set the maximum output length to 256, and for Google and t-REX, we set the maximum output length to 64. The Retriever used by all baselines is the Contriever model (Izacard et al., 2022) fine-tuned on MS-MARCO (Nguyen et al., 2016), which is based on BERT (110M). The input query is the source text with every occurrence of\nthe source topic replaced with the target topic. In Table 6, we show that this setup outperforms solely using the target topic as the query.\nWe retrieve k = 25 texts for the BART+Retr model, and k = 5 texts for the GPT models We found that retrieving more than k = 5 texts would limit the number in-context examples that we could provide to GPT-3.5, and we found that these incontext examples were essential to improve the performance of the GPT+Retr models (See Appendix A.3, which also contains the prompts used for each GPT model).\nHyperparameters were manually selected (no search) by assessing validation loss. All models were trained on a single NVIDIA A40 GPU. R1, R2, and BLEU were calculated using the huggingface Evaluate library.7\nA.3 GPT Prompts and Considerations\nWe provide a preliminary analysis to study how prompt size affects the few-shot GPT-3.5 models for text fact transfer on the Google dataset in Table 7. Interestingly, we find that increasing the size of the in-context examples from 3 to 10 worsens the performance of the GPT-3.5 models that do not use retrieval, but also increases the performance of the GPT-3.5 models that do use retrieval. This could indicate that LLMs are highly sensitive to the in-context examples for text fact transfer.\nWe provide the prompt used for the 0-shot GPT3.5 models in Figure 4 and the prompt used for the z-shot GPT-3.5 models in Figure 5. When creating the prompt for the 0-shot model, we tested slight variations of the prompt shown in Figure 4 on the validation sets and ultimately found the one shown to work the best.\nGiven the sensitivity of 0-shot GPT-3.5, we acknowledge that there likely exists a prompt that could boost the performance of this model. However, looking at Tables 1 and 2, we observe that 0- shot ModQGA consistently outperforms the z-shot GPT-3.5 models on all datasets except for Medline. Given this outcome and that z-shot GPT-3.5 is expected to outperform 0-shot GPT-3.5 regardless of the prompt, we believe that, at the very least, 0- shot ModQGA will outperform the 0-shot GPT-3.5 models across varied prompt formats on all datasets except Medline.\n7https://huggingface.co/docs/evaluate/index"
        },
        {
            "heading": "B Results",
            "text": "B.1 Full Ablation We display the full ablation results in Table 8. We find that the use of a specificity aware question answering model and ensembling of generic questions consistently improve the factuality and style of 0-shot ModQGA.\nB.2 Human Evaluation We build the human evaluation interface using PrairieLearn (West et al., 2015). Instructions given to the annotators are shown in Figure 6, and a screenshot from the interface is shown in Figure 7. The model outputs were randomized in each comparison. We use Gwet\u2019s AC2 (Gwet, 2008) to measure annotator agreement, given the presence of high agreement in our evaluation (e.g., over 90% of supervised models annotated as having equal style). We compute a value of 0.71, indicating good agreement.\nB.3 Sample Outputs We provide examples of outputs produced by ModQGA (0-shot and supervised) along with their respective baselines in Tables 9, 10, 11 for U.S. News, Medline, and Google, respectively."
        }
    ],
    "title": "Text Fact Transfer",
    "year": 2023
}