{
    "abstractText": "Empathetic response generation aims to generate empathetic responses by understanding the speaker\u2019s emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotionsemantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses. Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhou Yang"
        },
        {
            "affiliations": [],
            "name": "Zhaochun Ren"
        },
        {
            "affiliations": [],
            "name": "Yufeng Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaofei Zhu"
        },
        {
            "affiliations": [],
            "name": "Zhihao Chen"
        },
        {
            "affiliations": [],
            "name": "Tiecheng Cai"
        },
        {
            "affiliations": [],
            "name": "Yunbing Wu"
        },
        {
            "affiliations": [],
            "name": "Yisong Su"
        },
        {
            "affiliations": [],
            "name": "Sibo Ju"
        },
        {
            "affiliations": [],
            "name": "Xiangwen Liao"
        }
    ],
    "id": "SP:cab0df7365f2f485d762d6c320568479ab25b8e7",
    "references": [
        {
            "authors": [
                "Danqi Chen",
                "Christopher D Manning."
            ],
            "title": "A fast and accurate dependency parser using neural networks",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740\u2013750.",
            "year": 2014
        },
        {
            "authors": [
                "Ren\u00e9 Dirven."
            ],
            "title": "Emotions as cause and the cause of emotions",
            "venue": "The language of emotions, pages 55\u201384.",
            "year": 1997
        },
        {
            "authors": [
                "Timothy Dozat",
                "Christopher D Manning."
            ],
            "title": "Deep biaffine attention for neural dependency parsing",
            "venue": "arXiv preprint arXiv:1611.01734.",
            "year": 2016
        },
        {
            "authors": [
                "Joseph L Fleiss",
                "Jacob Cohen."
            ],
            "title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
            "venue": "Educational and psychological measurement, 33(3):613\u2013619.",
            "year": 1973
        },
        {
            "authors": [
                "Ad Foolen",
                "Ulrike M L\u00fcdtke",
                "Timothy P Racine",
                "Jordan Zlatev."
            ],
            "title": "Moving ourselves, moving others: Motion and emotion in intersubjectivity, consciousness and language",
            "venue": "John Benjamins Publishing Company.",
            "year": 2012
        },
        {
            "authors": [
                "Jun Gao",
                "Yuhan Liu",
                "Haolin Deng",
                "Wei Wang",
                "Yu Cao",
                "Jiachen Du",
                "Ruifeng Xu."
            ],
            "title": "Improving empathetic response generation by recognizing emotion cause in conversations",
            "venue": "Findings of EMNLP, pages 807\u2013819.",
            "year": 2021
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Byeongchang Kim",
                "Gunhee Kim."
            ],
            "title": "Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Wongyu Kim",
                "Youbin Ahn",
                "Donghyun Kim",
                "KyongHo Lee."
            ],
            "title": "Emp-rft: Empathetic response generation via recognizing feature transitions between utterances",
            "venue": "arXiv preprint arXiv:2205.03112.",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv:abs/1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Eliyahu Kiperwasser",
                "Yoav Goldberg."
            ],
            "title": "Simple and accurate dependency parsing using bidirectional lstm feature representations",
            "venue": "Transactions of the Association for Computational Linguistics, 4:313\u2013327.",
            "year": 2016
        },
        {
            "authors": [
                "Adhiguna Kuncoro",
                "Miguel Ballesteros",
                "Lingpeng Kong",
                "Chris Dyer",
                "Graham Neubig",
                "Noah A Smith"
            ],
            "title": "What do recurrent neural network grammars learn about syntax? arXiv preprint arXiv:1611.05774",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "arXiv:abs/1510.03055.",
            "year": 2015
        },
        {
            "authors": [
                "Qintong Li",
                "Hongshen Chen",
                "Zhaochun Ren",
                "Pengjie Ren",
                "Zhaopeng Tu",
                "Zhumin Chen."
            ],
            "title": "Empdg: Multiresolution interactive empathetic dialogue generation",
            "venue": "arXiv:abs/1911.08698.",
            "year": 2020
        },
        {
            "authors": [
                "Qintong Li",
                "Piji Li",
                "Zhaochun Ren",
                "Pengjie Ren",
                "Zhumin Chen."
            ],
            "title": "Knowledge bridging for empathetic dialogue generation",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Ying Zhang",
                "Yufeng Chen",
                "Jinan Xu",
                "Jie Zhou."
            ],
            "title": "Infusing multisource knowledge with heterogeneous graph neural network for emotional conversation generation",
            "venue": "AAAI, volume 35, pages 13343\u201313352.",
            "year": 2021
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Jamin Shin",
                "Peng Xu",
                "Pascale Fung."
            ],
            "title": "Moel: Mixture of empathetic listeners",
            "venue": "EMNLP-IJCNLP, page 121\u2013132.",
            "year": 2019
        },
        {
            "authors": [
                "Siyang Liu",
                "Chujie Zheng",
                "Orianna Demasi",
                "Sahand Sabour",
                "Yu Li",
                "Zhou Yu",
                "Yong Jiang",
                "Minlie Huang."
            ],
            "title": "Towards emotional support dialog systems",
            "venue": "arXiv:abs/2106.01144.",
            "year": 2021
        },
        {
            "authors": [
                "Navonil Majumder",
                "Pengfei Hong",
                "Shanshan Peng",
                "Jiankun Lu",
                "Deepanway Ghosal",
                "Alexander Gelbukh",
                "Rada Mihalcea",
                "Soujanya Poria."
            ],
            "title": "Mime: Mimicking emotions for empathetic response generation",
            "venue": "EMNLP, page 8968\u20138979.",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "Saif Mohammad."
            ],
            "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
            "venue": "ACL, pages 174\u2013184.",
            "year": 2018
        },
        {
            "authors": [
                "Saif M Mohammad",
                "Peter D Turney."
            ],
            "title": "Crowdsourcing a word\u2013emotion association lexicon",
            "venue": "Computational intelligence, 29(3):436\u2013465.",
            "year": 2013
        },
        {
            "authors": [
                "Meredith Osmond."
            ],
            "title": "The prepositions we use in the construal of emotion: Why do we say fed up with but sick and tired of",
            "venue": "The language of emotions: Conceptualization, expression, and theoretical foundation, 21:111.",
            "year": 1997
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "G\u00fcnter Radden."
            ],
            "title": "The conceptualisation of emotional causality by means of prepositional phrases",
            "venue": "Speaking of emotions: Conceptualisation and expression, pages 273\u2013294.",
            "year": 1998
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Eric Michael Smith",
                "Margaret Li",
                "Y-Lan Boureau."
            ],
            "title": "Towards empathetic opendomain conversation models: A new benchmark and dataset",
            "venue": "ACL, page 5370\u20135381.",
            "year": 2019
        },
        {
            "authors": [
                "Sahand Sabour",
                "Chujie Zheng",
                "Minlie Huang."
            ],
            "title": "Cem: Commonsense-aware empathetic response generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, Virginia, USA. AAAI Press.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS, page 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Liuping Wang",
                "Dakuo Wang",
                "Feng Tian",
                "Zhenhui Peng",
                "Xiangmin Fan",
                "Zhan Zhang",
                "Mo Yu",
                "Xiaojuan Ma",
                "Hongan Wang."
            ],
            "title": "Cass: Towards building a social-support chatbot for online health community",
            "venue": "PACMHCI, volume 5, pages 1\u201331.",
            "year": 2021
        },
        {
            "authors": [
                "Chujie Zheng",
                "Yong Liu",
                "Wei Chen",
                "Yongcai Leng",
                "Minlie Huang."
            ],
            "title": "CoMAE: A multi-factor hierarchical framework for empathetic response generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 813\u2013824, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Peixiang Zhong",
                "Di Wang",
                "Pengfei Li",
                "Chen Zhang",
                "Hao Wang",
                "Chunyan Miao."
            ],
            "title": "Care: Commonsense-aware emotional response generation with latent concepts",
            "venue": "AAAI, volume 35, pages 14577\u201314585.",
            "year": 2021
        },
        {
            "authors": [
                "Peixiang Zhong",
                "Chen Zhang",
                "Hao Wang",
                "Yong Liu",
                "Chunyan Miao."
            ],
            "title": "Towards persona-based empathetic conversational models",
            "venue": "arXiv:abs/2004.12316.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotionsemantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses. Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression.1"
        },
        {
            "heading": "1 Introduction",
            "text": "Attracting an increasing amount of attention, empathetic response generation aims to generate empa-\n\u2217Corresponding author. 1Our code is available at https:\n//github.com/zhouzhouyang520/ EmpatheticDialogueGeneration_ESCM\nthetic responses by perceiving the speaker\u2019s emotional feelings (Rashkin et al., 2019; Zhong et al., 2021, 2020; Liang et al., 2021; Zheng et al., 2021; Liu et al., 2021; Wang et al., 2021).\nEarly methods perceive the speaker\u2019s feelings by understanding the holistically semantics and emotions expressed in the language of the context (Rashkin et al., 2019; Lin et al., 2019; Majumder et al., 2020). These methods are prone to generate trivial and uninformed responses, ascribed to the neglect of nuances of human emotion in dialogues (Li et al., 2020). To address this issue, recent methods detect emotional words in the language of the communicators and build them as static vectors to perceive subtle emotions (Li et al., 2020; Sabour et al., 2022; Kim et al., 2021; Gao et al., 2021; Li et al., 2022; Kim et al., 2022).\nHowever, according to linguistic re-\nsearch (Foolen et al., 2012; Dirven, 1997; Osmond, 1997; Radden, 1998), focusing on emotional words while ignoring their characteristics in the expression process leads to emotional misunderstandings and the neglect of words with important semantic information. As an important theory of emotional expression in linguistics, the conceptualization of emotions (Foolen et al., 2012) suggests that emotional words have two important characteristics in the expression process: variability and relevance. Variability is that the affection of emotional words changes dynamically during the expression process. For example, \u201cwell\u201d generally carries a positive meaning, but in sentence 1 (shown in Figure 1), it is used as an interjection to express a neutral emotion. Using static vectors (such as Embedding (Pennington et al., 2014; Mikolov et al., 2013) or VAD (Mohammad, 2018)) to represent this dynamic emotion, previous methods are prone to misunderstand this sentence as positive.\nRelevance refers to the grammatical correlations between emotional words and words carrying semantic meaning, which plays an important role in understanding emotions and semantics. For example, as shown in Figure 1, sentence 2 expresses the \u201cexciting\u201d emotion due to the victory of \u201cteam\u201d. \u201cTeam\u201d is the primary subject described in the sentence, carrying key semantic information. Through the \u201c[ADJ]-CCOMP-[NOUN]\u201d correlation, the emotional word \u201cexciting\u201d directly modifies \u201cteam\u201d. Compared to previous work that did not consider such correlations, the model is more likely to identify key semantic words that are directly associated with emotional words through these types of syntax-meaningful relationships. Therefore, focusing on the variability and relevance of emotional words can promote the correct recognition of emotions and the detection of important semantics.\nTherefore, we propose a dynamical EmotionSemantic Correlation Model (ESCM) for empathetic dialogue generation. ESCM dynamically constructs emotion-semantic vectors through the interaction of context and emotions. By encoding emotion-semantic vectors, the model dynamically adjusts emotions and semantics in the context to capture the variability of emotional words. To reflect the correlations between emotions and semantics clearly, we introduce a dependency tree. Based on the dynamic emotion-semantic represen-\ntation and the dependency tree, ESCM proposes a dynamic correlation graph convolutional network to guide the model to capture the correlations between emotions and semantics clearly. By learning dynamic emotion-semantic representations and their correlations, ESCM accurately understands the emotions of the dialogue and captures important semantics to generate more empathetic responses.\nWe conduct experiments on the EMPATHETICDIALOGUES dataset (Rashkin et al., 2019). The results show that the ESCM model accurately understands the dialogue and generates grammatically fluent and informative empathetic responses. Furthermore, we extract and statistically analyze the common correlation structures in dialogues from the Empathetic-Dialogue dataset. The results indicate that the correlations between emotion and semantics are frequently and extensively utilized in expressing emotions during conversations. Additionally, the results of our analysis of correlation structures are consistent with linguistic conclusions (Foolen et al., 2012).\nTo sum up, our contributions are as follows:\n\u2022 We introduce the expressive characteristics of emotions in linguistics, including the variability of emotions and the correlations between emotions and semantics, to enhance the understanding of the meaning in conversations.\n\u2022 We propose the ESCM model, which constructs dynamic emotion-semantic vectors to adjust the dynamics of emotions, and leverages a dependency tree-based dynamic correlation graph convolutional network to learn correlations, in order to generate empathetic responses.\n\u2022 Experiments on the EMPATHETICDIALOGUE dataset demonstrate the effectiveness of ESCM. Furthermore, additional statistical and analytical experiments show that the correlations in dialogue are consistent with psychological research."
        },
        {
            "heading": "2 Related Work",
            "text": "Empathetic response generation refers to empathetically responding by perceiving emotional feelings in the language of the speaker (Rashkin et al., 2019).\nEarly approaches explore the overall emotions of the conversation. Rashkin et al. (2019) intro-\nduce emotion representation generated by a pretrained emotion classifier to learn and express specific types of emotions in the conversation. However, emotions expressed in responses are often diverse rather than specific (Lin et al., 2019). Therefore, Lin et al. (2019) utilize multiple professional emotion listeners to express various appropriate emotions. Majumder et al. (2020) group multiple conversation emotions by polarity and simulate the speaker\u2019s emotions to generate empathetic responses.\nThese methods focus on the overall emotions of the conversation and ignore nuanced emotions (Li et al., 2020). To capture nuanced emotions, Li et al. (2020) extract emotional words through the NRC Emotion Lexicons (Mohammad and Turney, 2013) and integrate them into the model. Gao et al. (2021) and Kim et al. (2021) introduce emotional cause detection models to capture emotional words and perceive nuanced emotions. Li et al. (2022) enhance emotional representation in the context with additional knowledge, which helps detect emotional words. Sabour et al. 2022 use commonsense reasoning knowledge to infer nuanced emotions in the conversation. Kim et al. 2022 employ pre-trained models to detect word-level emotion and keywords to detect the nuanced emotion in dialogues. These methods detect emotional words with nuanced emotions in the language of the conversation and use static vectors such as word embeddings or VAD to represent emotional words.\nOverall, early approaches ignore emotional words with nuanced emotions. Recent methods ignore two major characteristics of emotional words in linguistic expression: variability and correlation. Unlike these methods, we consider the two characteristics of emotional words and propose a dynamic emotion-semantic correlation model to better understand the conversation."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "Given a dialogue context D = [U1, U2, ..., UM ] of two interlocutors, our model needs to accurately perceive the emotions and semantics in the dialogue context and generate empathetic responses Y = [y1, y2, ...,yj , yN ]. Here, Ui = [wi1, w i 2, ..., w i mi] represents the i-th utterance containing mi words. Y is a response containing N words."
        },
        {
            "heading": "3.2 Overview",
            "text": "We propose ESCM, which takes into account the dynamic emotions and semantics in the dialogue and their correlations. The proposed model is a transformer-based model with encoder-decoder architecture. To accurately perceive the content of the dialogue, we mainly reconstruct the encoder. As shown in Figure 2, ESCM mainly consists of three parts: (1) a context encoder (Section 3.3), which is a standard encoder structure and is used to understand the semantics of the dialogue; (2) a dynamic correlation encoding module (Section 3.4), which includes the construction of dynamic correlation vectors and the encoding of a dynamic correlation graph convolutional network. It learns the correlations between emotions and semantics; (3) emotion and response predicting module (Section 3.5), which completes the functions of emotion prediction and response generation."
        },
        {
            "heading": "3.3 Context Encoder",
            "text": "As with previous methods (Li et al., 2020, 2022; Sabour et al., 2022), we concatenate the utterances of the dialogue context and prepend [CLS] as the whole sequence token to form the context input C = = [CLS] \u2295 U1 \u2295 U2 \u2295 ... \u2295 UM . Here, \u2295 denotes the concatenation symbol. To input the context C into the model, we convert C into context word embeddings Ec. Then, we sum up the word embeddings Ec, position embeddings, and state embeddings to form the semantic embeddings E\u0303c. The state embeddings are used to distinguish between speaker or responder types and are randomly initialized. To understand the semantics of the dialogue, we feed the semantic embeddings E\u0303c into the context encoder Encctx to obtain the context semantic representation Hk:\nHctx = Encctx(E\u0303c) (1)\nwhere Hctx \u2208 RL\u00d7d, L is the length of the context sequence, and d represents the hidden size of the encoder."
        },
        {
            "heading": "3.4 Dynamic Correlation Encoding Module",
            "text": "Dynamic correlation encoding consists of two submodules: (1) Dynamic emotion-semantic vectors. It brings the model the ability to flexibly adjust emotions and semantics, making the representation of context more reasonable. (2) Dynamic correlation graph convolutional network. This module\nuses emotions, semantics, part-of-speech, and dependency types to guide the model to discover and aggregate words with strong correlations, in order to more accurately understand the emotions and semantics of the conversation.\nDynamic Emotion-Semantic Vectors. Context has a significant impact on words, and understanding words without context may lead to errors. For example, without context, the word \u201cwell\u201d is generally considered to have a positive emotion, but in sentence 1 (shown in Figure 1), it functions as an interjection to enhance the tone. This usage does not indicate a positive meaning. Therefore, it is necessary to dynamically adjust emotions and semantics to adapt to the context.\nRegarding semantics, we utilize weighted adjustments of context word embeddings with semantics.\nEds = wsEc + bs (2)\nwhere ws and bs are trainable parameters, and Eds \u2208 RL\u00d7ds . ds is the hidden size of the dynamic semantic vector Eds.\nRegarding emotions, we interact context word embeddings Ec with emotion embeddings Ee to obtain dynamic emotion vectors Ede. Emotion embeddings refer to emotion categories represented in word form, which are converted into vector embed-\ndings.\nEdot = (wcEc + bc) \u00b7 (weEe + be)T (3) Ede = wceEdot + bce (4)\nwhere wc, bc, we, be, wce, bce are trainable parameters. Edot, Ede \u2208 RL\u00d7de , and de is the number of emotional categories.\nWe then feed the combined emotion and semantic vectors into the encoder to learn emotionsemantic representations. In this way, the model can take into account both emotion and semantics simultaneously during training to more comprehensively understand words in context.\nVdes = Ede \u2295 Eds (5) Hdes = Encdes(Vdes) (6)\nwhere Vdes, Encdes, and Hdes refer to the dynamic vectors, encoder, and representations for emotionsemantics, respectively. And Vdes, Hdes \u2208 RL\u00d7(ds+de).\nDynamic Correlation Graph Convolutional Network. The next problem is how to focus on and learn grammatical correlations. Dependency trees clearly reflect the grammatical dependency relationships between related words (Kuncoro et al., 2016; Kiperwasser and Goldberg, 2016; Chen and Manning, 2014; Dozat and Manning, 2016). Therefore,\nwe use dependency trees to reflect the correlations between words. However, words with correlations in the dependency tree are not always important for understanding emotions and semantics. For example, \u201cexciting\u201d and \u201cteam\u201d are more important, while \u201cexciting\u201d and \u201cis\u201d are relatively unimportant. To distinguish these correlations, we consider multiple aspects of correlation guidance, including dynamic emotion-semantic representation, part-ofspeech of related words, and the dependency types between them.\nWe list two examples to illustrate the validity of the above correlation guidance. In sentence 2 (shown in Figure 1), the part-of-speech (ADJ and NOUN)2 and the dependency type (CCOMP)3 indicate that \u201cexciting\u201d is closely related to the key semantic \u201cteam\u201d. Conversely, the part-of-speech (ADJ and AUX)4 and dependency type (COP)5 indicate that the correlation between \u201cexciting\u201d and \u201cis\u201d is trivial and unimportant.\nTherefore, we consider the above multiple aspects and concatenate the emotion-semantic representation, part-of-speech, and dependency type to form the guiding vector Vqk.\nVqk = Hdes \u2295 Vp \u2295 Vr (7)\nwhere Vp, Vr respectively denote part-of-speech embeddings , dependency type embeddings, which are randomly initialized. And Vp, Vr \u2208 RL\u00d7L\u00d7(dpr), Vqk \u2208 RL\u00d7L\u00d7(ds+de+2dpr). dpr is the embedding size of Vp or Vr.\nBy assigning probabilities to each correlated neighbor, we aggregate the neighboring nodes that are associated in the dependency tree. Subsequently, we obtain the correlation representation Hcor.\npi,j = ai,j \u00b7 exp(Vqk[i] \u00b7 Vqk[j])\u2211L j=1 ai,j \u00b7 exp(Vqk[i] \u00b7 Vqk[j]) (8)\nHcor = ReLU( L\u2211\nj=1\npi,j(WvVdes[j] + bv)) (9)\nwhere pi,j \u2208 RL\u00d7L\u00d71, Hcor \u2208 RL\u00d7(ds+de). ai,j is the value of the adjacency matrix about the dependency tree. When node i and node j have a direct relationship in the dependency tree, ai,j is 1, otherwise it is 0. Vqk[i] and Vqk[j] represent the\n2ADJ: adjective, NOUN: noun 3CCOMP: clausal complement 4AUX: auxiliary 5COP: copula\nguiding vectors of node i and node j, respectively. Wv and bv are trainable parameters, and ReLU is the ReLU activation function."
        },
        {
            "heading": "3.5 Emotion and Response Predicting",
            "text": "Based on the context semantics and the correlations between emotions and semantics, we predict the emotions of the conversation and generate empathetic responses.\nEmotion Predicting. To understand the context semantics and capture important correlations, we use two aggregation networks with the same structure but different parameters to process the context semantic representation Hctx (Eq. 1) and correlation representation Hcor (Eq. 9). We take the processing of the context representation as an example.\nWe first calculate the weights of the words in the context semantic representation Hctx and sum them up according to their weights to obtain the hidden layer representation H2.\nH1a = Tanh(w 1 aHctx + b 1 a) (10)\nPs = Softmax(w 1 sH 1 a + b 1 s) (11)\nH2 = L\u2211\nj=1\nPs[j] \u00b7Hctx[j] (12)\nwhere H1a \u2208 RL\u00d7d, Ps \u2208 RL\u00d71, H2 \u2208 Rd. w1a, b 1 a, w 1 s , b 1 s are learnable parameters, and Tanh is the tanh activation function. Then we feed the hidden layer representation into a non-linear layer to learn and generate context semantic emotion probabilities P ectx.\nH2a = Tanh(w 2 aH 2 + b2a) (13)\nP ectx = Softmax(w 2 sH 2 a + b 2 s) (14)\nwhere H2a \u2208 Rd, P ectx \u2208 Rde , w2a, b2a, w2s , b2s are learnable parameters.\nSimilarly, we use the same structure to construct an aggregation attention network about the correlations and obtain emotion probabilities P ecor \u2208 Rde . We add the two types of emotion probabilities together as the overall emotion probability Pe \u2208 Rde .\nPe = P e ctx + P e cor (15)\nTo ensure that important information in semantics and correlations is not affected by each other, we set loss functions for them separately. We employ log-likelihood loss to optimize the parameters\nduring the training phase based on the emotion category and the ground truth label.\nLectx = \u2212log(P ectx(e\u2217)) (16) Lecor = \u2212log(P ecor(e\u2217)) (17)\nResponse Predicting. Similarly, our decoder generates responses based on the context semantics and the correlations between emotion and semantics. This module takes the context semantic representation Hctx and the correlation representation Hcor as input and predicts the next word at each time step t. Similar to (Li et al., 2022), we use a point generator network to capture key vocabulary in the context and correlations.\nH = Hctx \u2295Hcor (18) P (yt|y < t, C) = Dec(Ey<t, H) (19)\nwhere H \u2208 RL\u00d7(d+ds+de), P (yt|y < t, C) \u2208 RV . V is the length of the vocabulary. Dec represents a decoder with a pointer network.\nSubsequently, we use cross-entropy as generation loss.\nLgen(yt) = \u2212 T\u2211 t=1 log(P (yt|y < t, C)) (20)\nTotal Loss. Finally, we add the loss Lgen(yt) and two emotion losses Lectx and Lecor together to obtain the total loss L. We optimize the training parameters in the model using the total loss.\nL = Lgen(yt) + Lectx + Lecor (21)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Baselines",
            "text": "We compare recent state-of-the-art baselines with our model.\nTransformer (Vaswani et al., 2017) is a vanilla Seq2Seq model, including both encoder and decoder;\nEmoPrend-1 (Rashkin et al., 2019) is a Transformer-based model that enhances empathy by incorporating emotion labels from a pre-trained emotion classifier;\nMoEL (Lin et al., 2019) is also a Transformerbased model that softly combines various emotions with multiple decoders to generate empathetic responses;\nMIME (Majumder et al., 2020) is a Transformerbased model, which consider polarity-based emotion clusters and emotional mimicry to generate appropriate responses;\nEmpDG (Li et al., 2020) emphasizes the importance of user feedback and multi-resolution emotions. It uses a generative adversarial network to train the model and generate empathetic responses;\nKEMP (Li et al., 2022) employs ConceptNet as extra knowledge to enrich the representation of implicit emotions and captures these emotions to generate appropriate responses;\nCEM (Sabour et al., 2022) takes into account both the emotional and cognitive aspects of empathy. By incorporating reasoning knowledge, it enhances the ability to perceive and express emotions."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We conduct experiments on the EMPATHETICDIALOGUES (Rashkin et al., 2019) dataset. In the dataset, the number of emotions is de=32. In the model, we use Glove (Pennington et al., 2014) as the initialization vector for word embedding, with a dimension of d=300. We set the dimension of the dynamic emotion vector to ds=10. At the same time, the dimensions of the part-of-speech embedding and dependency type embedding are both set to dpr=50. We use Biaffine Parser (Dozat and Manning, 2016) to obtain dependency relationships. For the multi-head attention networks in our model, we use a 1-layer network and set the number of heads to 2. Subsequently, we set the batch size to 16 and use the Adam optimizer (Kingma and Ba, 2014) to optimize the parameters. After training for 13500 rounds on an NVIDIA Tesla T4 GPU, the model converged."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "As with previous work, we employ both automatic and manual metrics to evaluate the performance of the model.\nAutomatic Evaluation Metrics. Following (Li et al., 2022; Sabour et al., 2022), we use the following automatic metrics in our experiments: Perplexity (PPL), Accuracy (Acc), Dist-1, and Dist-2. PPL measures language fluency, which is of higher quality when the score is lower. Acc assesses the accuracy of emotion perception. Dist-1 and Dist2 (Li et al., 2015) measure response diversity at single and double granularity, respectively.\nHuman Evaluation Metrics. Previous work scores models\u2019 responses on a scale of 1 to 5 to assess their quality (Li et al., 2020, 2022). This type of assessment is prone to inconsistent results due to differences in individual criteria (Sabour\net al., 2022). Therefore, we adopt the A/B test strategy (Lin et al., 2019; Majumder et al., 2020). Given two responses generated by the models for the same conversation, three professional crowdsourcers are required to assign ESCM a score of 1 on Win when the response generated by ESCM is better than the compared model. Correspondingly, when ESCM is better than or equal to the compared model, the crowdsourcers will add points for ESCM or Tie. Furthermore, three aspects are considered for evaluating models: empathy, relevance, and fluency. Empathy evaluates whether the responses show the right types of emotions; Relevance measures whether the reply is consistent with the theme and semantics of the context; Fluency assesses the response\u2019s readability and grammatical accuracy."
        },
        {
            "heading": "5 Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "Automatic Evaluation Results. Table 1 shows the main results of the automatic evaluation for all models. We find that early models (EmoPrend-1,\nMoEL, and MIME) are not as effective as models that focus on subtle emotions (EmpDG, KEMP, CEM). Additionally, we find that ESCM outperforms the baselines in all metrics. In terms of diversity, ESCM significantly outperforms the baselines. This suggests that focusing on the correlations between emotions and semantics helps the model capture key semantics and express informative responses. ESCM also outperforms the baselines in terms of emotion accuracy, indicating the effectiveness of the dynamic emotion-semantic vectors. Furthermore, ESCM achieves the best fluency, which indicates that the model combines emotions and semantics to express more natural language.\nHuman Evaluation Results. As shown in Table 2, ESCM outperforms the three strongest baselines in terms of empathy, relevance, and fluency. The superiority in empathy indicates that the model accurately understands and expresses emotions. The significant improvement in relevance suggests that the model captures and expresses key semantics. The superiority in fluency indicates that the model expresses more fluent responses by better understanding the context."
        },
        {
            "heading": "5.2 Ablation Studies",
            "text": "To verify the effectiveness of each component, the following experiments are conducted:\n(1) w/o DESV: Dynamic emotion-semantic representations Hdes (in Eq. 6) are replaced by context semantic representations Hctx (in Eq. 1);\n(2) w/o DCGCN: No dynamic correlation graph convolutional network (in Eqs. 7 - 9);\n(3) w/o Vr/Vp/Vdes: Without the guidance of vectors Vr/Vp/Vdes (in Eq. 7) in the dynamic correlation graph convolutional network.\nThe results of the ablation experiments are shown in Table 7. To verify the effectiveness of the dynamic emotion-semantic vectors, we remove DESV . The results show a significant decrease in emotional accuracy and diversity. The drop in\nemotion accuracy suggests that dynamic emotionsemantic vectors play an important role in capturing emotions. The changes in diversity metrics demonstrate the crucial role of dynamic adjustment of emotions and semantics in precise understanding and informative expression in conversations.\nWe remove DCGCN and its various guiding vectors to verify the effectiveness of the dynamic correlation graph convolutional network. After removing DCGCN , we find a significant decrease in emotional accuracy and diversity. This indicates that correlations have a significant impact on the perception of emotions and semantics, and they play an important role in expressing informative responses. We further explore the role of various guiding vectors. When part-of-speech Vp or dependency types Vr are removed, the emotional accuracy decreases significantly. This indicates that ESCM can aggregate effective information related to emotions based on part-of-speech or dependency types. After removing part-of-speech, dependency types, and emotion-semantic vectors respectively, the diversity decreases significantly. This indicates that these features affect the aggregation of semantic information.\nFurthermore, we find that removing any module improves the fluency of responses but decreases diversity. This is mainly due to the fact that the ablation models express responses with more fluent yet less information, such as trivial sentences."
        },
        {
            "heading": "5.3 Correlation Analysis",
            "text": "To further explore the correlations, we conduct a statistical analysis of the correlations in the EMPATHETIC-DIALOGUES dataset (see appendix A for details). We extract 1138 correlations from the dialogue dataset, which are used a total of 151242 times in the dialogue. This indicates that the correlation structure is frequently and repeatedly used in the dialogue. In addition, we find that emotions are mainly expressed through three parts of speech: adjectives, nouns, and verbs. This is consistent with linguistic research (Foolen et al., 2012) on the parts of speech of emotional words. At the same time, we also find that the \u201cpreposition + noun\u201d structure is frequently used, which is also consistent with linguistic research (Foolen et al., 2012). In each emotion type of empathetic dialogue, the frequently used correlation structures are similar, but the frequency of use may differ."
        },
        {
            "heading": "5.4 Time and Resource Consumption",
            "text": "To further demonstrate the efficacy of the model, we conduct analytical experiments on time and resource consumption.\nTime Consumption. As shown in Table 4, we calculate the time for our model and baselines to converge during training. The results show that our model does not have significantly higher average per-iteration time compared to the baselines. This is mainly because: although the intermediate variable Vqk intuitively has higher dimensionality, in practice the dialogue context length L is short, so the impact on time is not substantial. Meanwhile, since our model can better understand the dialogue, it requires fewer iterations. Therefore, the total training time is actually significantly less than the baselines.\nResource Consumption. As shown in Table 5, we compute the GPU memory consumption of the models. Due to the higher dimensionality of the intermediate variable dpr in our model, our model requires slightly more resources than the baselines (see last row). However, even when reducing the GPU consumption of our model to be comparable to the baselines, our model still significantly outperforms the baselines overall (see penultimate row)."
        },
        {
            "heading": "5.5 Case Study",
            "text": "As shown in Table 7, we select two strongest baselines and compare them with ESCM through sample analysis. In the first case, The speaker is annoyed due to the fact that \u201cthe younger brother threw food onto the table\u201d. The baselines do not accurately understand the emotional expression \u201cannoyed\u201d and the event it described. However, ESCM understands and expresses \u201cannoying\u201d correctly, and gives a response indicating disgust towards the event. This indicates that ESCM is able to capture key semantics through correlations. In the second case, the speaker expresses the emotion of \u201cproud\u201d using the word \u201camazed\u201d with a surprised emotion. The baselines do not understand the emotions and semantics involved, while ESCM accurately understands the emotions and expresses an empathetic response. This demonstrates the effectiveness of building dynamic emotion-semantics."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "This paper proposes ESCM, which introduces two characteristics of emotions in the linguistic expression process: variability and the correlations\nbetween emotions and semantics. The proposed model constructs a dynamic emotion-semantic vector to reflect variability and uses a dependency treebased dynamic correlation graph convolutional network to learn correlations. Both automatic and manual metrics demonstrate the effectiveness of the model. Furthermore, we conduct statistical analysis experiments. The results show that correlations are frequently used in the dialogue. Additionally, we find that the correlation structures in the dialogue are consistent with linguistic research.\nTo further investigate the correlation between emotion and semantics, we will take into account pre-trained knowledge, multilinguality, personalization, and other factors in future work.\nLimitations\nThe limitations of our work are as follows: (1) Pretrained models have become the mainstream nowadays. To further explore the impact of pre-trained models, we constructed a pre-trained ESCM model. Since the word coverage of the EMPATHETICDIALOGUES dataset in the vocabulary of pretrained models is only 51.8%, we only surpassed the baseline Emp-RFT (Kim et al., 2022) on two metrics (Acc: 42.44 (42.08), Dist-2: 9.91 (4.48)). In the future, we will further explore the impact of pre-trained models on correlations. (2) The correlations between emotions and semantics discussed in this paper are only applicable to English. However, different languages may have different types of correlations (Foolen et al., 2012). Therefore, we will investigate the correlations between emotions and semantics in multilingual contexts in the future. (3) Intuitively, there are individual differences in the expression of emotions. Due to data limitations, we did not consider this personalized factor. We will involve more research on correlations and personalization in the future.\nEthical Considerations\nThe potential ethical implications of our work are as follows: (1) Dataset: EMPATHETICDIALOGUES is an open-source, publicly available dataset for empathetic response generation. In the dataset, the original provider has filtered information about personal privacy and ethical implications (Rashkin et al., 2019). (2) Models: Our baselines are also open source, and they have no permission issues. Since our model is trained on a healthy\ndataset, it does not generate discriminatory, abusive, or biased responses to users."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to express our sincere gratitude to the reviewers for their diligent evaluation and constructive feedback, which helped improve the quality of this paper. In addition, we appreciate the insightful discussions and comments from the authors, which stimulated valuable thinking around this work. Their diverse perspectives and experience shared through feedback have contributed immensely to the development of this research. This work was supported by National Natural Science Foundation of China (No.61976054)."
        },
        {
            "heading": "A Appendix",
            "text": "Frequency Phenomenon. As shown in Figure 3, we list the correlation statistical results. (1) As\nshown in \u201cAll Correlation\u201d in Figure 3 (the highest red line), for the overall correlations in the dataset, the top 10% of frequently used correlations account for more than 80% of the total number of correlations. That is, CfCtotal =86.17%, where Cf is the number of frequently used correlations in the top 10%, and Ctotal is the total number of correlations used in the dataset.\n(2) As shown in \u201cOthers: 32 Emotion Correlations\u201d in Figure 3, for each type of emotional empathetic dialogue, the top 20% of frequently used correlations account for 80% of the total number of correlations. That is,\nCef Cetotal\nis approximately 80%. For example, the number of empathetic dialogues expressing \u201cjoyful\u201d emotion is Cjoyfultotal =6083, and the number of frequently used associations in the top 20% is Cjoyfulf =4945, so C joyful f /C joyful total is approximately 81.13%. Part-of-Speech Phenomenon. As shown in Table 7, we list examples of the most commonly used correlation structures. Taking \u201cNOUN-amod-ADJb\u201d as an example, \u201cNOUN-amod-ADJ\u201d represents a noun and an adjective linked together by the \u201camod\u201d dependency type. \u201cb\u201d refers to the emotion word as the second word, and \u201cf\u201d indicates that the emotion word is the first word. Specifically, \u201cROOT\u201d refers to the root node of the dependency tree. In the frequently used correlations, emotions are mainly expressed through three parts of speech: adjectives, nouns, and verbs, which is consistent with linguistic research (Foolen et al., 2012) on emotion words.\nCorrelation Structure Phenomenon. As shown in Table 7, we list the most commonly used\ncorrelation structures for each type of emotion. We also find that the \u201cpreposition + noun\u201d method is frequently used, which is consistent with linguistic research (Foolen et al., 2012).\nAs shown in Table 8, we list the most commonly used correlations in various emotional dialogues, with the numbers in parentheses indicating the probability of their usage. In each type of emotional empathetic dialogue, the frequently used correlation structures are similar, but the frequency of use may vary."
        }
    ],
    "title": "Exploiting Emotion-Semantic Correlations for Empathetic Response Generation",
    "year": 2023
}