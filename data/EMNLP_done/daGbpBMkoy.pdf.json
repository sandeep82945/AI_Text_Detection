{
    "abstractText": "To create a captivating story, a writer often plans a sequence of logically coherent events and ingeniously manipulates the narrative order to generate flashback in place. However, existing storytelling systems suffer from both insufficient understanding of event correlations and inadequate awareness of event temporal order (e.g., go to hospital <after> get ill), making it challenging to generate high-quality events that balance the logic and narrative order of story. In this paper, we propose a narrative order aware framework BPOT (Bidirectional Pretraining Model with Optimal Transport Reward) for story generation, which presents a bidirectional pretrained model to encode event correlations and pairwise event order. We also design a reinforcement learning algorithm with novel optimal transport reward to further improve the quality of generated events in the fine-tuning stage. Specifically, a narrative order aware event sequence model is pretrained with the joint learning objectives of event blank infilling and pairwise order prediction. Then, reinforcement learning with novel optimal transport reward is designed to further improve the generated event quality in the fine-tuning stage. The novel optimal transport reward captures the mappings between the generated events and the sentences in the story, effectively measuring the quality of generated events. Both automatic and manual evaluation results demonstrate the superiority of our framework in generating logically coherent stories with flashbacks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhicong Lu"
        },
        {
            "affiliations": [],
            "name": "Li Jin"
        },
        {
            "affiliations": [],
            "name": "Guangluan Xu"
        },
        {
            "affiliations": [],
            "name": "Linmei Hu"
        },
        {
            "affiliations": [],
            "name": "Nayu Liu"
        },
        {
            "affiliations": [],
            "name": "Xiaoyu Li"
        },
        {
            "affiliations": [],
            "name": "Xian Sun"
        },
        {
            "affiliations": [],
            "name": "Zequn Zhang"
        },
        {
            "affiliations": [],
            "name": "Kaiwen Wei"
        }
    ],
    "id": "SP:b1acff6df84b338367bd42f8d80812ebe4a084a8",
    "references": [
        {
            "authors": [
                "Gang Chen",
                "Yang Liu",
                "Huanbo Luan",
                "Meng Zhang",
                "Qun Liu",
                "Maosong Sun"
            ],
            "title": "Learning to generate explainable plots for neural story generation",
            "year": 2020
        },
        {
            "authors": [
                "Xiuying Chen",
                "Mingzhe Li",
                "Shen Gao",
                "Zhangming Chan",
                "Dongyan Zhao",
                "Xin Gao",
                "Xiangliang Zhang",
                "Rui Yan."
            ],
            "title": "Follow the timeline! generating an abstractive and extractive timeline summary in chronological order",
            "venue": "ACM Transactions on Informa-",
            "year": 2023
        },
        {
            "authors": [
                "Marco Cuturi."
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Advances in neural information processing systems, 26.",
            "year": 2013
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 889\u2013 898.",
            "year": 2018
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Strategies for structuring story generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2650\u2013 2660.",
            "year": 2019
        },
        {
            "authors": [
                "Matt Gardner",
                "Joel Grus",
                "Mark Neumann",
                "Oyvind Tafjord",
                "Pradeep Dasigi",
                "Nelson Liu",
                "Matthew Peters",
                "Michael Schmitz",
                "Luke Zettlemoyer."
            ],
            "title": "Allennlp: A deep semantic natural language processing platform",
            "venue": "Proceedings of Workshop for NLP",
            "year": 2018
        },
        {
            "authors": [
                "Seraphina Goldfarb-Tarrant",
                "Tuhin Chakrabarty",
                "Ralph Weischedel",
                "Nanyun Peng."
            ],
            "title": "Content planning for neural story generation with aristotelian rescoring",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Fei Huang",
                "Zhihao Zhao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A knowledge-enhanced pretraining model for commonsense story generation",
            "venue": "Transactions of the Association for Computational Linguistics, 8:93\u2013108.",
            "year": 2020
        },
        {
            "authors": [
                "Rujun Han",
                "Hong Chen",
                "Yufei Tian",
                "Nanyun Peng."
            ],
            "title": "Go back in time: Generating flashbacks in stories with event temporal prompts",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Rujun Han",
                "Qiang Ning",
                "Nanyun Peng."
            ],
            "title": "Joint event and temporal relation extraction with shared representations and structured prediction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
            "year": 2019
        },
        {
            "authors": [
                "Rujun Han",
                "Xiang Ren",
                "Nanyun Peng."
            ],
            "title": "Econet: effective continual pretraining of language models for event temporal reasoning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5367\u20135380.",
            "year": 2020
        },
        {
            "authors": [
                "Qingbao Huang",
                "Linzhang Mo",
                "Pijian Li",
                "Yi Cai",
                "Qingguang Liu",
                "Jielong Wei",
                "Qing Li",
                "Ho-fung Leung."
            ],
            "title": "Story ending generation with multi-level graph convolutional networks over dependency trees",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Leonid Vitalevich Kantorovich."
            ],
            "title": "On a problem of monge",
            "venue": "Journal of Mathematical Sciences, 4(133):1383\u20131383.",
            "year": 2006
        },
        {
            "authors": [
                "Xiangzhe Kong",
                "Jialiang Huang",
                "Ziquan Tung",
                "Jian Guan",
                "Minlie Huang."
            ],
            "title": "Stylized story generation with style-guided planning",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 2430\u20132436.",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Shih-Ting Lin",
                "Nathanael Chambers",
                "Greg Durrett."
            ],
            "title": "Conditional generation of temporally-ordered event sequences",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2020
        },
        {
            "authors": [
                "Nayu Liu",
                "Kaiwen Wei",
                "Xian Sun",
                "Hongfeng Yu",
                "Fanglong Yao",
                "Li Jin",
                "Guo Zhi",
                "Guangluan Xu."
            ],
            "title": "Assist non-native viewers: Multimodal cross-lingual summarization for how2 videos",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natu-",
            "year": 2022
        },
        {
            "authors": [
                "Huanru Henry Mao",
                "Bodhisattwa Prasad Majumder",
                "Julian McAuley",
                "Garrison W Cottrell."
            ],
            "title": "Improving neural story generation by targeted common sense grounding",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Chris Martens",
                "Joao F Ferreira",
                "Anne-Gwenn Bosser",
                "Marc Cavazza."
            ],
            "title": "Generative story worlds as linear logic programs",
            "venue": "Intelligent Narrative Technologies, 7:51.",
            "year": 2014
        },
        {
            "authors": [
                "Lara Martin",
                "Prithviraj Ammanabrolu",
                "Xinyu Wang",
                "William Hancock",
                "Shruti Singh",
                "Brent Harrison",
                "Mark Riedl."
            ],
            "title": "Event representations for automated story generation with deep neural nets",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2018
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Qiang Ning",
                "Hao Wu",
                "Haoruo Peng",
                "Dan Roth."
            ],
            "title": "Improving temporal relation extraction with a globally acquired statistical resource",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Rafael P\u00c9rez \u00dd P\u00c9rez",
                "Mike Sharples."
            ],
            "title": "Mexica: A computer model of a cognitive account of creative writing",
            "venue": "Journal of Experimental & Theoretical Artificial Intelligence, 13(2):119\u2013139.",
            "year": 2001
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Zhihong Shao",
                "Minlie Huang",
                "Jiangtao Wen",
                "Wenfei Xu",
                "Xiaoyan Zhu."
            ],
            "title": "Long and diverse text generation with planning-based hierarchical variational model",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour."
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in neural information processing systems, 12.",
            "year": 1999
        },
        {
            "authors": [
                "Bowen Tan",
                "Zichao Yang",
                "Maruan AI-Shedivat",
                "Eric P Xing",
                "Zhiting Hu."
            ],
            "title": "Progressive generation of long text with pretrained language models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Xian Sun",
                "Zequn Zhang",
                "Li Jin",
                "Jingyuan Zhang",
                "Jianwei Lv",
                "Zhi Guo."
            ],
            "title": "Implicit event argument extraction with argument-argument relational knowledge",
            "venue": "IEEE Transactions on Knowledge and Data Engineering.",
            "year": 2022
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Xian Sun",
                "Zequn Zhang",
                "Jingyuan Zhang",
                "Guo Zhi",
                "Li Jin"
            ],
            "title": "Trigger is not sufficient: Exploiting frame-aware knowledge for implicit event",
            "year": 2021
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Yiran Yang",
                "Li Jin",
                "Xian Sun",
                "Zequn Zhang",
                "Jingyuan Zhang",
                "Xiao Li",
                "Linhao Zhang",
                "Jintao Liu",
                "Guo Zhi."
            ],
            "title": "Guide the manyto-one assignment: Open information extraction via iou-aware optimal transport",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "Yujia Xie",
                "Xiangfeng Wang",
                "Ruijia Wang",
                "Hongyuan Zha."
            ],
            "title": "A fast proximal point method for computing exact wasserstein distance",
            "venue": "Uncertainty in artificial intelligence, pages 433\u2013453. Proceedings of Machine Learning Research (PMLR).",
            "year": 2020
        },
        {
            "authors": [
                "Yuqiang Xie",
                "Yue Hu",
                "Yunpeng Li",
                "Guanqun Bi",
                "Luxi Xing",
                "Wei Peng."
            ],
            "title": "Psychology-guided controllable story generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6480\u20136492.",
            "year": 2022
        },
        {
            "authors": [
                "Jingjing Xu",
                "Xuancheng Ren",
                "Yi Zhang",
                "Qi Zeng",
                "Xiaoyan Cai",
                "Xu Sun."
            ],
            "title": "A skeleton-based model for promoting coherence among sentences in narrative story generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural",
            "year": 2018
        },
        {
            "authors": [
                "Peng Xu",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Raul Puri",
                "Pascale Fung",
                "Anima Anandkumar",
                "Bryan Catanzaro."
            ],
            "title": "Megatron-cntrl: Controllable story generation with external knowledge using large-scale language models",
            "venue": "Proceedings of the 2020 Con-",
            "year": 2020
        },
        {
            "authors": [
                "Lili Yao",
                "Nanyun Peng",
                "Ralph Weischedel",
                "Kevin Knight",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Planand-write: Towards better automatic storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378\u20137385.",
            "year": 2019
        },
        {
            "authors": [
                "Zhexin Zhang",
                "Jiaxin Wen",
                "Jian Guan",
                "Minlie Huang."
            ],
            "title": "Persona-guided planning for controlling the protagonist\u2019s persona in story generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Bo Zhou",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao",
                "Jiexin Xu",
                "Xiaojian Jiang",
                "Qiuxia Li."
            ],
            "title": "Generating temporally-ordered event sequences via event optimal transport",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Yucheng Zhou",
                "Xiubo Geng",
                "Tao Shen",
                "Guodong Long",
                "Daxin Jiang"
            ],
            "title": "Eventbert: A pre-trained",
            "year": 2022
        },
        {
            "authors": [
                "Han"
            ],
            "title": "Pretraining data For pretraining data, we utilize the event sequences provided by Lin et al. (2020). It is originally used to pretrain with the tasks of temporal event ordering and event infilling",
            "year": 2022
        },
        {
            "authors": [
                "Han"
            ],
            "title": "2019). Then we feed their event triggers and contexts into three ECONET (Han et al., 2020) with different random seeds to get pairwise event",
            "year": 2020
        },
        {
            "authors": [
                "Bart: (Lewis"
            ],
            "title": "2020) It is a typical seq2seq model for natural language generation",
            "venue": "We finetune it on both datasets. We directly serve the first sentence of story on ROCStories and prompt of story on WritingPrompts",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As the fundamental element of numerous literary genres, stories have played a significant role in various social domains, such as literature, education, and entertainment. However, manually creating high-quality stories is often time-consuming and laborious. Therefore, developing an AI system that\n\u2020Corresponding author.\ncan generate stories like human writer is meaningful. Story generation aims to generate a series of events and organize them into a reasonable story given limited information (Huang et al., 2021).\nRecently, a significant process has been made in story generation. Similar to human writers, Martin et al. (2018); Yao et al. (2019); Xu et al. (2018) introduced a planning-based method, namely planning sketch prior to writing story. To improve the controllability of story generation (Kong et al., 2021; Xie et al., 2022), different attributes (e.g., style, psychology) were added to sketch. Besides, Guan et al. (2020); Xu et al. (2020) leveraged external knowledge bases to generate commonsense stories. Although existing methods can generate fluent stories related to the desired attribute, they pay little attention to event correlations, resulting in insufficient understanding of event correlations. This makes it difficult to maintain logical coherence as a whole story (Zhou et al., 2022c). Moreover, they rarely consider the crucial role of narrative order in story generation. For example in Table 1,\nall storytelling systems exhibit logical incoherence and only the last system reverses narrative order in place to generate flashback.\nFlashback is a common writing technique that describes the past events to provide context for the current narrative. It arouses reader\u2019s interest and deepens their understanding of the narratives. To generate flashback in story, Han et al. (2022) proposed temporal prompt. As shown in Figure 1, <after> hints at the pairwise event order, which triggers model to reverse the narrative order to describe the past events for flashback generation (\"catch a neighborhood thief\" occurs later than \"had stolen a wallet\"). However, they face the problems of inefficient prompt and deteriorating logic in some cases (shown in section 4.5). It is probably because model always unidirectionally reasons event from order, lacking the process of inversely inferring order from pairwise events, resulting in inadequate awareness of event temporal order. Additionally, when generating high-quality events in story, it is intrinsically more challenging to balance the logic and narrative order than merely focusing on logic.\nIn this paper, we propose a narrative order aware framework BPOT for story generation, which presents a bidirectional pretrained model to encode event correlations and pairwise event order. We also design a reinforcement learning (RL) algorithm with novel optimal transport (OT) reward to further improve the quality of generated events in the fine-tuning stage. Specifically, a narrative order aware event sequence model is pretrained with the joint learning objectives of event blank infilling and pairwise order prediction. Then, we employ OT to capture the mappings between the generated events and the sentences in the story. Based on the mappings, we construct a novel reward to effectively measure the quality of generated events, further improving the generated event quality under the optimization of the RL algorithm. In summary, our main contributions include:\n1) We propose a narrative order aware framework BPOT for story generation, which pretrains an event sequence model with the joint learning objectives of event blank infilling and pairwise order prediction to encode the event correlations and pairwise event order.\n2) We design an RL algorithm with novel OT reward to further improve the quality of generated event in the fine-tuning stage. The novel OT reward effectively measures the generated event quality\nbased on the mappings captured by OT. 3) Extensive evaluation results demonstrate the superiority of our framework in generating logically coherent stories with flashbacks."
        },
        {
            "heading": "2 Preliminaries",
            "text": "This section provides a description of the sketch used in this work, followed by an introduction to the basic concept of OT."
        },
        {
            "heading": "2.1 Sketch Details",
            "text": "We follow the line of the planning-based method (Martin et al., 2018; Yao et al., 2019) for story generation. Our sketch S is composed of alternating splicing of event e and temporal prompt t, where t \u2208 {<before>, <after>, <vague>}. It hints at the pairwise event order, thereby manipulating the narrative order. Note that <vague> represents arbitrary event order. Formally, an event e is a structure (a0, v, a1), which is extracted from the corresponding sentence s in story Y by semantic role labeling (SRL) tools (Gardner et al., 2018). v is the trigger (Wei et al., 2021) describing the event, a0 and a1 are its relevant arguments (Wei et al., 2022). We only consider one event in each sentence for simplicity. Let ei,n = (a0i,n, vi,n, a 1 i,n) denotes the n-th event in i-th sketch Si, ti,n denotes the event order between the n-th event and the (n+1)-th event in Si. Then a sketch with m events can be represented as Si = {ei,1, ti,1, ..., ei,m, <eoe>}, where <eoe> refers to event ending. A sketch with five events is shown at the bottom of Figure 3."
        },
        {
            "heading": "2.2 Optimal Transport",
            "text": "OT has recently been introduced to numerous tasks in NLP (Wei et al., 2023). Its original objective is to find the OT maps between two data distributions with minimum cost (Kantorovich, 2006). Formally, given two complete metric spaces X and Y, let u(x) and v(y) denote two discrete probability distributions on X and Y respectively, where\u2211n\ni=1 u(xi) = \u2211m\nj=1 v(yj) = 1. Then the OT maps T \u2217 between u(x) and v(y) is obtained by solving the optimization problem (1):\nT \u2217 = argmin T \u2211 i,j=1 tij \u00b7 c(xi, yj)\ns.t. \u2211 j=1\ntij = u(xi) \u2200i \u2208 {1, ..., n}\u2211 i=1 tij = v(yj) \u2200j \u2208 {1, ...,m} T \u2208 Rn\u00d7m+\n(1)\nwhere c is the cost function. To effectively solve (1), researchers have proposed some approximation algorithms, such as Sinkhorn (Cuturi, 2013), IPOT (Xie et al., 2020). In this work, we adopt IPOT."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we detail our proposed narrative order aware framework BPOT for story generation, which presents a bidirectional pretrained model to encode event correlations and pairwise event order. As shown in the upper of Figure 3, an RL algorithm with novel OT reward is designed to further improve the generated event quality. Particularly, a narrative order aware event sequence model is pretrained with the joint learning objectives of event blank infilling and pairwise order prediction as shown in Figure 2. In the fine-tuning stage, we employ OT to capture the mappings between the generated events and the sentences in the story. Based on the mappings, a novel reward is constructed to effectively measure the quality of generated events, further improving the generated event quality under the RL optimization."
        },
        {
            "heading": "3.1 Vanilla Pipeline Generation",
            "text": "The vanilla pipeline generation of our framework is shown at the bottom of Figure 3. Following the line of the planning-based method, we plan sketch prior to writing story. Firstly, golden sketch S\u0302i is extracted from golden story Y\u0302i as mentioned in section 2.1. Temporal prompts are obtained by identifying the pairwise event order between sentences by ECONET (Han et al., 2020) in advance. Then all events are masked in S\u0302i except for the input event to obtain input xti. Last, we require sketch model to generate sketch Si given on xti and story model to unfold Si into story Y\u0302i. The pretrained BART-base (Lewis et al., 2020) is served as both sketch model and story model. Formally, Let \u03b1 and \u03b8 denote the parameters of sketch model and story model respectively, per sample loss for two models during training can be expressed as (2), (3):\nL\u03b1 = \u2212logp(S\u0302i|xti) = \u2212 |S\u0302i|\u2211 k=1 logp(S\u0302i,k|xti, S\u0302i,<k) (2) L\u03b8 = \u2212logp(Y\u0302i|Si) = \u2212 |Y\u0302i|\u2211 k=1 logp(Y\u0302i,k|Si, Y\u0302i,<k) (3)\nWe use Si rather than S\u0302i to reduce the discrepancy between training and inference."
        },
        {
            "heading": "3.2 Bidirectional Pretraining",
            "text": "Intuitively, the quality of sketch greatly influences the generated story. However, due to suffering from both insufficient understanding of event correlations and inadequate awareness of event temporal order, it is challenging for sketch model to generate high-quality events that balance the logic and narrative order of story. Therefore, we present a joint pretraining of event blank infilling and pairwise order prediction to encode event correlations and pairwise event order as shown in Figure 2. Event Blank Infilling demands model to reason blank event from the given pairwise event order. It requires the generated event to not only be reasonable in event correlations but also conform to a specific event order. Concretely, for each event e\u0302i in S\u0302i, we mask it with a probability of 0.25 to obtain Sei and keep all temporal prompts t\u0302 visible. If no events are selected for masking, we randomly choose one to mask. By doing so, the model is able to pay more attention to understanding event correlations conditioned on a specific order. Pairwise Order Prediction requires model to reason order from the given pairwise events within the generated paradigm. Concretely, we mask all temporal prompts t\u0302 in S\u0302i, where <eoe> is also masked to facilitate model to understand the story ending. To prevent excessive deviation from the sketch generation, we additionally mask an event to obtain Sti . If an event is chosen to mask, the corresponding temporal prompt or <eoe> will be restored. Since only one event is masked, model reasons pairwise event order in most cases, which facilitates model to have a better awareness of event temporal order.\nTo jointly encode event correlations and event temporal order, we first obtain Sei and S t i from S\u0302i for each sample. Then, we execute different learning objectives in parallel and combine their loss in a varying ratio to jointly guide optimization. Per sample loss Lp can be represented as (4):\nLp = \u2212 |S\u0302i|\u2211 k=1 (\u03b31logp(S\u0302i,k|Sei , S\u0302i,<k)+\n\u03b32logp(S\u0302i,k|Sti , S\u0302i,<k))\n(4)\nwhere \u03b31 and \u03b32 are weight factors, varying with the pretraining process."
        },
        {
            "heading": "3.3 Optimal Transport Reward",
            "text": "An apparent problem with the planning-based method is sketch model can\u2019t adjust with story model as the sketch generation is nondifferentiable. This leads to sketch model not knowing how the generated sketch affects the final story. To overcome this barrier, we design an RL algorithm with novel OT reward, further improving the generated event quality.\nWe briefly introduce how to utilize RL to optimize sketch model. Particularly, a reward R\u0302 is constructed based on the feedback from story model. Later, the policy gradient (Sutton et al., 1999) is adopted to optimize sketch model through maximizing the expected reward E\u03b1[R\u0302i] in (5). The gradient of sketch model can be represented as (6) according to the policy gradient theorem, which can be approximated with the sampling techniques.\nE\u03b1[R\u0302i] = E[R\u0302i \u00b7 log(p(S\u0302i|xti, \u03b1)] (5) \u2207J(\u03b1) = E[R\u0302i \u00b7 \u2207logp(S\u0302i|xti, \u03b1)] (6)\nTherefore, the core idea is to design an effective reward R\u0302i that guides sketch model to generate better sketch. A naive approach is regarding negative sentence loss as the corresponding event reward. Because low-quality event in sketch will make it harder for story model to reconstruct the original sentence in story, leading to higher sentence loss and smaller event reward. However, the mappings between each event ei in sketch and the sentence sj in story modeled by the naive approach are oneto-one as the reward for ei is only determined by si. But they should be one-to-many as an event may contribute multiple sentences. To overcome this barrier, we design a novel OT reward.\nAs shown in the upper of Figure 3, we extract the event semantics He = {hei}lei=1 and sentence semantics Hs = {hsj}lsj=1 by averaging the representations of the included tokens from the last hidden state of sketch model and story model. Here, le and ls represent the number of events in sketch and sentences in story respectively. The sentence loss Ls = {Lsk}lsk=1 is also computed by adding up the loss of the included tokens. Later, we view the process of unfolding sketch into story as moving the event semantic distribution to sentence semantic distribution, and employ OT to capture the mappings between them. Concretely, the cost matrix C \u2208 Rle\u00d7ls is first calculated based on the simi-\nAlgorithm 1: Training And Optimization Input: Dtrain = {xti, Y\u0302i} N i=1, \u03b7 weight factor for OT loss, \u03b1, \u03b8 parameters of sketch model and story model. 1 pretrain sketch model based on (4) 2 for sample P = {xti, Y\u0302i} \u2208 D\ntrain do 3 Compute loss L\u03b1, L\u03b8 by (2), (3). 4 Extract He, Hs, Ls from the output 5 Compute cost matrix C by (7) 6 Compute transport matrix T by IPOT 7 Compute compact reward Ri and OT Loss Lot by (8), (9) 8 Expand Ri to get final reward R\u0302i 9 \u2207J(\u03b1) = R\u0302i \u00b7 \u2207logP (S\u0302i|xti, \u03b1)\n10 \u2207\u03b1 = \u2207J(\u03b1) + \u03b7\u2207Lot 11 \u2207\u03b8 = \u2207L\u03b8 + \u03b7\u2207Lot 12 optimize the entire model based on \u2207\u03b1, \u2207\u03b8 13 end\nlarity between events and sentences measured by RBF kernel as (7).\ncij = 1\u2212 exp( \u2212||hei \u2212 hsj ||2\n2\u03b22 ) (7)\nIt means the closer the semantics of hei and hsj , the smaller the transportation cost between them. Then, IPOT is adopted to compute the transport matrix (maps) T \u2208 Rle\u00d7ls based on C, He and Hs. Tij can be understood as the semantic transportation or contribution from ei to sj . After obtaining the mappings T , we construct the compact reward R = {ri}lei=1 and compute the OT loss Lot as below:\nRi = \u2212TLs (8) Lot = trace(T TC) (9)\nwhere ri indicates the reward for ei. Last, we expand each ri to make its dimension consistent with the number of tokens contained in the corresponding event to get the final reward R\u0302. In this way, ri is not only determined by the sentence loss si, but also by other sentence loss. The reward intensity of sj to ei depends on the semantic contribution (Tij) from ei to sj captured by OT. Consequently, each event reward comprehensively considers the feedback of all sentences, thus effectively measuring the quality of events in sketch. Furthermore, sketch model can perceive the generated event quality and understand event correlations through comparing the reward of different events, thereby adjusting itself accordingly. It facilitates sketch model to improve the quality of generated events. The entire flow is shown in Algorithm 1."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we first introduce the datasets, the compared models, and the evaluation measures in the experiments. Then, we show the experimental results and provide a detailed analysis."
        },
        {
            "heading": "4.1 Datasets",
            "text": "To verify the effectiveness of our framework in generating both short and long stories, we choose ROCStories (ROC) (Mostafazadeh et al., 2016) and WritingPrompts (WP) (Fan et al., 2018) as benchmark datasets. For pretraining data, we adopt the event sequences provided by Lin et al. (2020) and further process it as follows. For each pairwise events, the event triggers and contexts are fed into ECONET (Han et al., 2020) to obtain their pairwise event order. Last, we repeat the operations in section 3.1 and obtain a total of 100k event sequences for pretraining. More details about the datasets and pretraining data are shown in appendix A.1."
        },
        {
            "heading": "4.2 Compared Models",
            "text": "Baselines. (1) Bart (Lewis et al., 2020) is a typical seq2seq model for natural language generation. We directly fine-tuned Bart-base on both datasets. (2) Bart (planning) adopted the planning-based method based on (1). (3) TemporalBart (Lin et al., 2020) is pretrained with temporal event ordering and event infilling tasks. We utilize its pre-trained weights to initialize our sketch model. (4) Megatron (Xu et al., 2020) fine-tuned GPT2 (Radford et al., 2019) and leveraged external knowledge bases to enrich the generated stories on ROCStories. (5) ContentPlanning (Goldfarb-Tarrant et al., 2020) adopted the planning-based Bart-large model on WritingPrompts. (6) Flashback (Han et al., 2022) utilized temporal prompt to generate flashback in place, which is the strongest baseline for our framework. We also compare with its three variants for fair, which are only RL, only Pretrained and the integration of RL and Pretrained (PR), respectively. For RL, it regarded the negative story loss as the final reward R\u0302. For Pretrained, it adopted an autoregressive manner at pretraining stage. Note that Flashback and its variants all served event arguments as mask unit. (7) ChatGPT is selected as its powerful ability in natural language generation. More details about the baselines are in Appendix A.3. Ablation Variants. In addition to the baselines, we present the ablation variants of our framework. (1) Vanilla performs vanilla pipeline generation as shown in the bottom of Figure 3. (excluding bidirectional pretraining) (2) BP adopts bidirectional pretraining of event blank infilling and event temporal order based on (1). (3) OTRL attaches the RL algorithm with OT reward based on (1)."
        },
        {
            "heading": "4.3 Evaluation Measures",
            "text": "Automatic Evaluation We use the following automatic metrics to evaluate stories. PPL represents the model\u2019s perplexity of the stories. Repeat-n (R-n) (Shao et al., 2019) reflects the redundancy of the stories by computing the ratio of the stories that repeat at least one n-gram. Distinct-n (D-n) measures the diversity of the stories by computing the ratio of n-gram types to all generated n-grams. Tks is the average length of the stories. We also report standard BLEU-3 (B-3) (Papineni et al., 2002) and ROUGEL(RL) (Lin, 2004). Manual Evaluation We conduct a manual evaluation on ROCStories to verify whether the storytelling system can generate the high-quality events that balance the logic and narrative order in generating stories with flashbacks. Specifically, we randomly choose 100 stories that have <after> in test sets. For each story, we obtain six versions which are generated by the ablation variants of our framework and three strong baselines. Then, the annotators are required to evaluate stories on three aspects: Narrative order, Coherence and Overall. For Narrative order, we ask them to label all pairwise event orders in each story. Then the ratio of each narrative order is calculated, and we further compute the entropy to represent Narrative Order Diversity (NOD). Meanwhile, Narrative Order Accuracy (NOA) is measured by calculating the ratio of the annotated results that are consistent with the given temporal prompts. For Coherence, we have the annotators rate each story (1-5) according to the inter-sentence logic and whether the generated story deviates from the given input. For Overall, the annotators are required to rank\nstories based on the overall quality of the story and their preferences. Also, they can line up two stories together if they are fairly similar in quality. For each set of stories, we have 5 workers to annotate it. Appendix A.4 shows more details on automatic and manual evaluation."
        },
        {
            "heading": "4.4 Experimental Results",
            "text": "Automatic Evaluation Results are reported in Table 2. BPOT surpasses other baselines in all metrics except for repetition, which indicates the generated stories are more fluent and diverse, overlapping more with the reference stories. For repetition, our model outperforms other baselines under similar configurations (e.g., RL vs OTRL) on ROCStories, while showing slightly worse results on WritingPrompts, probably because of the much longer generated stories. Concretely, when compared to the strongest baselines (Flashback and its variants), Vanilla performs worse than Flashback on ROCStroies. It could be that Flashback serves event arguments as mask unit, which helps learn the dependencies between event arguments. Instead, Vanilla is better than Flashback on WritingPompts, especially in Tks. It may be because the stories in WritingPrompts are much longer. Thus, the dependencies between event arguments are complex\nand hard to learn, while serving event as mask unit makes it easier to learn event correlations, thereby generating longer stories. When attaching OT reward, OTRL outperforms all variants of Flashback. Besides, although our pretraining data is about 0.1 of Pretrained (1 million), BP performs better, especially in diversity and repetition. BP achieves a growth of 1.66 and 2.1 in Distinction and a decline of 1.66 and 0.4 in Repetition on two datasets, while Pretrained are \u22120.63, \u22120.93 and \u22123.05, \u22122.2 respectively. The superior performance on two datasets demonstrates the effectiveness of bidirectional pretraining and OT reward. Moreover, since the experimental settings of Megatron (Xu et al., 2020) and ContentPlanning (Goldfarb-Tarrant et al., 2020) are different from other baselines, we conducted separate experiments to compare with them. The results are shown in Table 3. Megatron achieves better results in diversity as it served GPT2 as the backbone and leveraged external knowledge bases to insert novel words into stories. Content Planning outperforms ours in some metrics because it used more training data and designed a series of classifiers to refine sketch. Manual Evaluation Results are shown in Table 4. More detailed statistics on the annotated results of\nnarrative order are shown in Figure 4. We can find that the dominant narrative order of the generated story is straightforward (<before>), which is consistent with human writing habits and the findings in Ning et al. (2018). BPOT performs best in NOD (1.115) and NOA (0.958). It proves that inversely inferring pairwise order through bidirectional pretraining facilitates model to be more better aware of <after>, thus reversing narrative order to generate flashback (highest after ratio 24% and highest after accuracy 88.18%). Moreover, OTRL greatly boosts the coherence and overall quality. We believe that the reason is that OT reward effectively measures the quality of the generated events. Therefore, sketch model can adjust with reward to generate higher-quality events. Besides, the bidirectional pretraining also improves the coherence and overall quality. We believe that it is because event correlations encoded by bidirectional pretraining facilitate sketch model to plan reasonable event logic, thus alleviating the logical incoherence. Particularly, although ChatGPT performs better in coherence and overall due to its huge number of parameters and rich training corpus, it is difficult for ChatGPT to break the dominant narrative order <before> and\nfollow <after> to generate flashback. It is reflected in the poor performance of NOD (0.474), lowest after ratio 8.25% and NOA (0.745) , lowest after accuracy 16.36%. Analysis of BP and OTRL To further verify the effectiveness of BP and OTRL, we conduct comparative experiments on ROCStories. As shown in Figure 5 and Table 5, we compare various pretraining strategies and rewards under the same experimental settings (serving event as mask unit). We can find that BP achieves the lowest PPL during pretraining stage. After combing with OTRL, BPOT outperforms other control groups in all metrics. It proves that the bidirectional reasoning is superior to unidirectional reasoning and the autoregressive manner. Besides, when compared to RL and Naive-RL, OTRL performs best. It demonstrates the effectiveness of measuring the event quality in fine-grained way and further considering the oneto-many mappings between the generated events and the sentences in the story. Both comparative experiments verify the superiority of bidirectional pretraining and OT reward."
        },
        {
            "heading": "4.5 Case Study",
            "text": "Two cases are shown in Figure 7. We find that ChatGPT is almost able to generate reasonable event logic. However, it fails to follow the <after> and reverse narrative order to generate flashback in place. Moreover, for simple situations with one <after>,\nPR sometimes generates correct flashback. But it is faced with logical incoherence. Seeing the upper example, \"terry rushed to the hospital\" follows \"it was a car accident\". But the accident is not related to terry so that there is no need for him to rush to the hospital. Instead, BPOT generates the correct flashback (\"had attacked terry\u2019s house.\" occurs in the past and explains why \"hearing a loud bang\") and maintains the reasonable event logic. Besides, for complex situations with multiple <after>, PR faces the problems of inefficient prompt and deteriorating logic. As shown in the bottom example, it fails to generate flashback with second <after> and the conflicting relationship with that man has shifted from my friend to me. In contrast, BPOT generates two correct flashbacks (\"had a boyfriend who was cheating on her\" occurs in the past and explains \"broke up\", \"cheated on her\" occurs in the past and explains \"asked her why\"). Both cases demonstrate the effectiveness of our framework in generating logically coherent stories with flashbacks. More cases are shown in Appendix."
        },
        {
            "heading": "5 Related Work",
            "text": "Story Generation was first approached by symbolic and logical planning (P\u00c9rez and Sharples, 2001; Martens et al., 2014). Recently, significant progress has been made in applying deep neural networks in story generation. Fan et al. (2018); Mao et al. (2019) reused a seq2seq model to translate the prompt into a story. Xu et al. (2018); Yao et al. (2019); Martin et al. (2018) introduced the planning-based method, namely planning sketch prior to writing story. Then, numerous works were devoted to designing the format of sketch (Fan et al., 2019; Chen et al., 2020) and enriching sketch with external knowledge bases (Tan et al., 2021; Xu et al., 2020). Moreover, researchers explored controllable story generation through fine-grained control of sketch, such as writing style (Kong et al., 2021), protagonist\u2019s personality (Zhang et al., 2022). However, they faced the problem of logical incoherence and rarely considered the crucial role of narrative order in story generation. Although Han et al. (2022) proposed the temporal prompt to generate flashback, it faced the problems of inefficient prompt and deteriorating logic in some cases. In this work, we focus on generating logically coherent stories with flashbacks. Event Correlations and Temporal Order have been proven useful in many event-related tasks\n(Chen et al., 2023; Liu et al., 2022). Recent works paid attention to utilizing pretraining to encode event correlations and event temporal order. Han et al. (2020) learned to identify the temporal relationship between events through masking event triggers and temporal indicators. Lin et al. (2020); Zhou et al. (2022a) explored temporal event ordering and event infilling tasks for mining temporal knowledge. Zhou et al. (2022b,c) adopted event-level pretraining with contrastive learning to capture event correlations. However, they rarely involve mutual reasoning between pairwise event and event order. In this work, we jointly encode event correlations and pairwise event order through the bidirectional reasoning between event and order."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this paper, we propose a narrative order aware framework BPOT for story generation, which presents a bidirectional pretrained model to encode event correlations and pairwise event order. We also design an RL algorithm with novel OT reward to further improve the generated event quality in the fine-tuning stage. Both automatic and manual evaluation results demonstrate the superiority of our framework in generating logically coherent stories with flashbacks. In the future, we will explore how to control the narrative order of long texts (paragraphs) or other narrative modalities (video).\nLimitations\nThe performance of our proposed framework is related to the used pretrained language model (PLM). Applying our proposed framework to stronger PLM may lead to further improvements. Besides, the temporal prompts are obtained by ECONET through majority voting with different random seeds. This inevitably introduces some noise in the data, possibly affecting the final performance."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was funded by the National Natural Science Foundation of China (62206267). We sincerely thank Shiyao Yan, Changyuan Tian, and Wei Jia for their constructive collaboration."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Details on Datasets and Pretraining data\nROCStories (Mostafazadeh et al., 2016) It contains 98,162 five-sentence short stories. The average length of the story is about 42 words. Following (Xu et al., 2020; Han et al., 2022) we split the data into 88,344/4.908/4,909 for train/validation/test sets. WritingPrompts (Fan et al., 2018) It contains 30,335 pairs of prompts and stories. The average length of the story is over 700 words. Thus, the stories in WritingPrompts are much longer than those in ROCStories. Following (Han et al., 2022), we select stories with a maximum of 500 words, leading to a total number of 96,488 training and 5,784 validation prompt-story pairs, respectively. For the test set, we utilize the 1000 prompt-story pairs provided by the compared baseline (GoldfarbTarrant et al., 2020). The temporal prompts in both datasets are provided by Han et al. (2022). Pretraining data For pretraining data, we utilize the event sequences provided by Lin et al. (2020). It is originally used to pretrain with the tasks of temporal event ordering and event infilling. Thus, the event sequences are full of the knowledge of event correlations and event temporal order. We further process it as follows. For each event sequence, we segment all its pairwise adjacent events. For each pairwise event, we separately restore their original sentences and detect their event triggers by Han et al. (2019). Then we feed their event triggers and contexts into three ECONET (Han et al., 2020) with different random seeds to get pairwise event order. We only adopt the result when the event orders judged by the three models are the same. Otherwise, we label its pairwise event order as <vague>, which means arbitrary event order. Note that there may be multiple results of semantic role labeling for a sentence in datasets, we choose the results whose event trigger is the ROOT node in the syntactic analysis as our event. By doing so,\nwe finally obtain a total of 100k event sequences as our golden sketches for pretraining.\nA.2 Implementation Details\nUnless specifically mentioned, we use the pretrained Bart-base * as our sketch model and story model. The initial vocabulary of BART contains 50265 tokens. If the compared model which could be baselines or ablation variants of our framework adopts the planning-based method, we add an event ending (<eoe>) token. If it inserts temporal prompts into sketch, we add three additional narrative order tokens (<before>, <after>, <vague>). On ROCStories, we serve the event in the first sentence of each story as the input event. On WritingPrompts, we serve the prompt as the input event. For ROCStories, the hyper-parameters are learning rate: 5e\u22125; batch size: 10; \u03b2: 0.7; gradient accumulation: 1. To fairly compare with (Han et al., 2022), we use three random seeds (5, 9998, 20016) and report the average performance of all models evaluated on ROCStories. For WritingPrompts, the hyper-parameters are learning rate 2e\u22125; batch size: 64; \u03b2:0.7, gradient accumulation: 8. For both datasets, we fine-tune on a single Tesla V100 GPU with 32G memory. When compared with the baseline ContentPlanning (Goldfarb-Tarrant et al., 2020), we fine-tune Bart-large on a single A100 GPU with 80G memory. The training time for per epoch on ROCStories and WritingPrompts is 2.5- 3.5 hours and 24 hours respectively. We train our framework and its ablation variants for 10 epochs and save the model with the best evaluation perplexity. For pretraining, hyper-parameters are learning rate 1e\u22125; batch size: 10; gradient accumulation: 10. We also pretrain 10 epochs and save the model with the best evaluation perplexity. The weight factor \u03b71, \u03b72 during pretraining are computed as follows:\n\u03b71 = 3\u2212 2 \u2217 (global step/total step) (10) \u03b72 = 1 + 2 \u2217 (global step/total step) (11)\nwhere the global step represents the current number of iterations that have been updated and the total step represents the total number of iterations to be updated. It means that the influence of event blank infilling will gradually increase during pretraining, while the influence of pairwise order prediction will gradually decrease. It is because we hope\n*https://huggingface.co/facebook/bart-base/tree/main\nmodel can focus more on pairwise order prediction in the early stage of pretraining, then utilizing the awareness of event temporal order to generate better events in event blank infilling.\nA.3 Details on Baselines\nBart: (Lewis et al., 2020) It is a typical seq2seq model for natural language generation. We finetune it on both datasets. We directly serve the first sentence of story on ROCStories and prompt of story on WritingPrompts as the input, and the entire story as labels to fine-tune the model. Bart (planning): It adopts the planning-based method based on Bart. We first use sketch model to generate sketch and then use story model to generate story given on the generated sketch. The detailed process is consistent with the vanilla pipeline generation at the bottom of Figure 3 except that we replace all temporal prompts with <eoe>. TemporalBart: (Lin et al., 2020) It is designed with the tasks of temporal event ordering and event infilling. We pick it as it implicitly encodes event correlations and event temporal order. Note that the temporal prompt is not used during pretraining. Concretely, we use its pre-trained weights to initialize the sketch model and then adopt the workflow of Flashback (Han et al., 2022) to complete the subsequent fine-tuning. Megatron: (Xu et al., 2020) It used GPT2(Radford et al., 2019) and leveraged external knowledge bases to generate the commonsense stories. We picked it as it outperforms previous systems (Guan et al., 2020) on ROCStories. Due to its operation of delexicalization that replaces names and entities with [MALE], [FEMALE], and [NEUTRAL], when compared with it, we strive to recover its original entities and name. ContentPlanning: (Goldfarb-Tarrant et al., 2020). It adopted the planning-based method on WritingPrompts. As reported in Han et al. (2022), our final training data is about two-thirds of theirs. Moreover, they do not adopt end-to-end training as they additionally design a series of classifiers to refine the sketch. Flashback: (Han et al., 2022). It first considered the crucial role of narrative order in story generation and proposed temporal prompt to generate flashback. It is the strongest baseline for our work. The detailed process is consistent with the bottom of Figure 3 except it served event arguments as mask unit. Concretely, each masked\nevent in input is represented as \"<mask> ; <mask> ; <mask> ; <Temporal Prompt>\" rather than \"<mask><Temporal Prompt>\" in our framework. We serve event as mask unit as we deem that it facilitates model to directly learn event correlations, especially in long stories. The results of automatic evaluation on WritingPrompts also demonstrate it. To further verify the effectiveness of our methods, we compare with its three variants. The only RL simply regards the negative story loss as reward R\u0302, which means that it returns an identical reward for all events in the generated sketch. The Pretrained used 1 million event sequences which are ten times for our pretraining data. Moreover, it adopts autoregerssive manner during pretrianing stage. The autoregressive manner represents that it unidirectionally reasons all event arguments from left to right. The PR is the integration of RL and Pretrained, which means that it uses the pre-trained weights to initialize sketch model and combines with RL in the fine-tuning stage. ChatGPT: we pick it because of its powerful ability in NLP. We only compare with it on ROCStories. Because the stories in WritingPrompts are much longer and contain dialogue or short phrases without events, which makes it hard for annotators to judge the order of pairwise event. Referring to the tutorial \u2020, we design the instruction for ChatGPT as shown in Figure 6. Note that we prompt ChatGPT to generate stories no more than 48 words as the average length of the story on ROCStories is about 42. If the length is not constrained, the stories generated by ChatGPT are much longer than 42 words, which is adverse to fair comparison. Moreover, although we hint the story should be five-sentence, ChatGPT sometimes generates more than five sentences. In this case, we only choose the first five sentences as the final story.\nA.4 Details on Evaluation Measures\nAutomatic Evaluation Measures: We find that our models can achieve nearly 0 in Repeat-3 on ROCStories and in Repeat-4 on WritingPrompts, which is in line with the findings in Han et al. (2022). Therefore, we report Repeat-2 on ROCStories and Repeat-3 on WritingPrompts. For Tks, we only report on WritingPrompts. Because the stories on ROCStories are relatively short so that the generated stories are almost the same length through full\n\u2020https://platform.openai.com/docs/guides/completion/promptdesign\ntraining. For WritingPrompts, it is hard for models to generate such long stories in training sets due to the limitation of the model\u2019s capacity. Therefore, the Tks can reflect the performance of the model to some degree. Manual Evaluation Measures: We randomly sample 100 stories from the test sets of ROCStories. For each story, we get 6 versions which are generated by different storytelling systems. Each set of stories is evaluated by 5 annotators on three aspects: Narrative Order, Coherence, Overall. It should be noted that all annotators have linguistic backgrounds and the order of storytelling systems in each set is shuffled. The specific instructions are shown in Figure 7 and Figure 9. Specifically, we clearly introduce the definition of the task and the evaluation method for each metric. Moreover, we provide an example with the detailed explanation for the corresponding task. For each set of stories, we require four annotators to rate coherence and rank overall. Then, we average their values and obtain the final results reported in the manual evaluations. We also ask one annotator to label the pairwise event order to get the narrative order.\nA.5 Comparison of Reward\nThe comparison of various rewards is shown in Figure 8. The RL in (Han et al., 2022) is constructed at the story-level, which means that all\nevents in the generated sketch receive the identical reward. The reward is regarded as the negative story loss. Therefore, it cannot measure the quality of the generated events because there is no difference between their rewards. The Naive-RL is constructed at the event-level, which means that different events in the generated sketch receive the different rewards. The reward is regarded as the negative sentence loss. Therefore, the sketch model can perceive the quality of generated event through comparing the reward of different events. However, the mappings between the generated events and the sentences in the story are modeled by one-toone. But they actually should be one-to-many as an event may influence multiple sentences in story. Therefore, it can\u2019t effectively measure the event quality as the mappings are inaccurate. The OTRL is constructed at the event-level. It also means that different events in the generated sketch receive the different rewards. The reward is regarded as the weighted sum of sentence loss. The weights are the mappings between the generated events and the sentences in the story, which are captured by OT."
        }
    ],
    "title": "Narrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward",
    "year": 2023
}