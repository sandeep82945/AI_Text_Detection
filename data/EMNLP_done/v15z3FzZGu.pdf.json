{
    "abstractText": "Adapters are widely popular parameterefficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rishabh Bhardwaj"
        },
        {
            "affiliations": [],
            "name": "Tushar Vaidya"
        },
        {
            "affiliations": [],
            "name": "Soujanya PoriaDeCLaRe"
        }
    ],
    "id": "SP:3b3c86892761b3f555fcf0139fce1ed62fc9605d",
    "references": [
        {
            "authors": [
                "Motasem Alfarra",
                "Adel Bibi",
                "Hasan Hammoud",
                "Mohamed Gaafar",
                "Bernard Ghanem."
            ],
            "title": "On the decision boundaries of neural networks: A tropical geometry perspective",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
            "year": 2022
        },
        {
            "authors": [
                "Rishabh Bhardwaj",
                "Amrita Saha",
                "Steven CH Hoi."
            ],
            "title": "Vector-quantized input-contextualized soft prompts for natural language understanding",
            "venue": "arXiv preprint arXiv:2205.11024.",
            "year": 2022
        },
        {
            "authors": [
                "Rishabh Bhardwaj",
                "Tushar Vaidya",
                "Soujanya Poria"
            ],
            "title": "2022b. KNOT: Knowledge distillation using optimal",
            "year": 2022
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326.",
            "year": 2015
        },
        {
            "authors": [
                "Yu Cheng",
                "Duo Wang",
                "Pan Zhou",
                "Tao Zhang."
            ],
            "title": "A survey of model compression and acceleration for deep neural networks",
            "venue": "arXiv preprint arXiv:1710.09282.",
            "year": 2017
        },
        {
            "authors": [
                "Narsingh Deo."
            ],
            "title": "Graph theory with applications to engineering and computer science",
            "venue": "Courier Dover Publications.",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin."
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635.",
            "year": 2018
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Yerlan Idelbayev",
                "Miguel A Carreira-Perpin\u00e1n."
            ],
            "title": "Low-rank compression of neural nets: Learning the rank of each layer",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8049\u20138059.",
            "year": 2020
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Xin Li",
                "Dan Roth."
            ],
            "title": "Learning question classifiers",
            "venue": "COLING 2002: The 19th International Conference on Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Christos Louizos",
                "Max Welling",
                "Diederik P Kingma."
            ],
            "title": "Learning sparse neural networks through l_0 regularization",
            "venue": "arXiv preprint arXiv:1712.01312.",
            "year": 2017
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the ACL.",
            "year": 2005
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "Adapterfusion: Non-destructive task composition for transfer learning",
            "venue": "arXiv preprint arXiv:2005.00247.",
            "year": 2020
        },
        {
            "authors": [
                "Andreas R\u00fcckl\u00e9",
                "Gregor Geigle",
                "Max Glockner",
                "Tilman Beck",
                "Jonas Pfeiffer",
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Adapterdrop: On the efficiency of adapters in transformers",
            "venue": "arXiv preprint arXiv:2010.11918.",
            "year": 2020
        },
        {
            "authors": [
                "Abigail See",
                "Minh-Thang Luong",
                "Christopher D Manning."
            ],
            "title": "Compression of neural machine translation models via pruning",
            "venue": "arXiv preprint arXiv:1606.09274.",
            "year": 2016
        },
        {
            "authors": [
                "Jiarun Wu",
                "Qingliang Chen",
                "Zeguan Xiao",
                "Yuliang Gu",
                "Mengsi Sun."
            ],
            "title": "Pruning adatperfusion with lottery ticket hypothesis",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1632\u20131646, Seattle, United States. Association",
            "year": 2022
        },
        {
            "authors": [
                "Liwen Zhang",
                "Gregory Naitzat",
                "Lek-Heng Lim."
            ],
            "title": "Tropical geometry of deep neural networks",
            "venue": "International Conference on Machine Learning, pages 5824\u20135832. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "farra"
            ],
            "title": "2022), and ognoting the shifts, one",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the increase in network sizes, we are observing an ever-increasing space and computational demand for models needed to solve a given task. To tackle this, model compression (Cheng et al., 2017) techniques are becoming continuously popular which retain the most important learning from the full model while reducing the size of the network either by pruning or distillation.\nTransfer learning approaches, such as adapters (Houlsby et al., 2019), are a parameter-efficient alternative to full model fine-tuning which obviates the need to maintain a task-specific copy of the base language model (LM). Adapters insert simple modules in between layers of an LM to adapt the pre-trained representation for a given downstream NLP task. However, there is a lack of research in pruning adapter modules to further enhance their parameter efficiency. We hypothesize that adapter weights can be pruned significantly by not compromising the performance observed with unpruned states, this motivates the proposed approach.\nIn this work, we propose a novel approach to pruning adapter layers without any iterative finetuning of the model parameters on downstream tasks. Using tropical algebra, we study the (duals of) hypersurfaces generated by adapter modules in the high-dimensional space. As a pruning objective, we aim to minimize the magnitude of adapter weights while constraining the change in hypersurface geometry to be small.\nRelated works include adapters pruning using lottery ticket hypothesis (Wu et al., 2022; Frankle and Carbin, 2018) that performs iterative pruning\u2014 a few gradient steps, prune, and reset the parameters to initial weights. R\u00fcckl\u00e9 et al. (2020) drops adapter from lower transformer layers. While these works are interesting, we provide a more concrete angle to prune adapter layers\u2014prune by preserving the hypersurface geometry. We extend an insightful analysis of tropical geometry of neural networks (Zhang et al., 2018; Alfarra et al., 2022) to adapters."
        },
        {
            "heading": "2 Background",
            "text": "Adapter Operations. We use the adapter setup proposed by Pfeiffer et al. (2020) that inserts small modules after FFN add and layer norm sub-layer.\nh\u2190 h+ f(hWd)Wu (1)\nIt consists of down-projection Wd \u2208 Rd\u00d7r, upprojection Wu \u2208 Rr\u00d7d, a ReLU activation function f(\u00b7), where typically r < d.\nTropical Arithmetic. Tropical algebra is a variant of classical algebra where basic arithmetic operations are redefined. The tropical sum \u2295 of two numbers represents their maximum and the tropical product \u2299 represents a classical addition1. Thus,\nx\u2295 y = max {x, y} x\u2299 y = x+ y\n1The tropical addition can be defined as a \u2295 b = min{a, b} or max{a, b}, we focus on the latter as we analyze a ReLU-based adapter network.\nFor e.g., 2 \u2295 5 = 5 and 2 \u2299 5 = 7. Axioms and order of arithmetic operations in tropical algebra follow the classical, thus addition is commutative and multiplication is distributive over addition. We relegate detailed discussions about tropical algebra, polynomials, and hypersurfaces to the Appendix A.\nNotations used: Henceforth, we denote Wd, Wu, h by A, B, and x, respectively; B+:=max{B,0}; B\u2212:=max{\u2212B,0}; bi denotes ith row of B; bi+:=max{bi,0}, bi\u2212:=max{\u2212bi,0}; Diag[u] arranges u in a diagonal matrix; ||G||1,1:=\u03a3dk=1||G(i, :)||1; || \u00b7 ||F denotes Frobenius Norm."
        },
        {
            "heading": "3 Tropical Adapter Pruning",
            "text": "Given a frozen language model adapted to a specific task using adapter layers, we divide our approach into two steps: 1) Finding adapter weights PT that are crucial to preserving the tropical adapter hypersurface by solving a simple optimization problem; 2) Pruning of adapter weights with least magnitudes that do not lie in PT . Next, we describe step-1 which is core to the pruning method:\nA bottleneck adapter block can be expressed by f(x) = Bmax{Ax,0}. Since f(x) in itself is not a tropical polynomial and thus does not form a tropical surface, we rewrite it in terms of the difference between two tropical polynomials f(x) = H(x)\u2212Q(x), following the analysis of tropical rational function by Alfarra et al. (2022). Thus we focus on a relatively lenient problem i.e. identifying weights that preserve tropical hypersurfaces defined by H(x) and Q(x). Let H(x) and Q(x) be the respective hypersurfaces, one can choose a sparse set of A\u0302, B\u0302 that belongs to the set of matrices obtained by solving the following optimization problem\nmin A\u0302,B\u0302\nd(H(x), H\u0302(x)) + d(Q(x), Q\u0302(x))\nWhere d(\u00b7) defines the distance between two geometric objects; H\u0302 and Q\u0302 are hypersurfaces obtained by substituting A and B with A\u0302 and B\u0302 in f(x). In place of preserving the orientation ofH(x) and Q(x), we aim to preserve the orientation of their respective dual objects denoted by \u03b4(H(x)) and \u03b4(Q(x)). Thus,\nmin A\u0302,B\u0302\nd ( \u03b4(H(x)), \u03b4(H\u0302(x)) ) +d ( \u03b4(Q(x)), \u03b4(Q\u0302(x)) )\nWithout the loss of generality, we assume downprojection is bias-free2, \u03b4(\u00b7) can be expressed in terms of generator matrices G of zonotopes obtained from A and B. To find sparse A\u0302, B\u0302, we introduce sparse regularization terms in the optimization function. Thus, finding adapter weights that preserve the hypersurface geometry can be cast as the following optimization problem:\nmin A\u0302,B\u0302\n1\n2 \u2223\u2223\u2223\u2223\u2223\u2223G\u03021 \u2212G1\u2223\u2223\u2223\u2223\u2223\u22232 F + 1 2 \u2223\u2223\u2223\u2223\u2223\u2223G\u03022 \u2212G2\u2223\u2223\u2223\u2223\u2223\u22232 F\n+ \u03bb1 \u2223\u2223\u2223\u2223\u2223\u2223G\u03021\u2223\u2223\u2223\u2223\u2223\u2223 1,1 + \u03bb2 \u2223\u2223\u2223\u2223\u2223\u2223G\u03022\u2223\u2223\u2223\u2223\u2223\u2223 1,1 (2)\nwhere G1 = Diag[bi+]A;G2 = Diag[bi\u2212]A,\nG\u03021 = Diag[b\u0302 i+]A\u0302; G\u03022 = Diag[b\u0302 i\u2212]A\u0302,\nWe provide a derivation of the above function in Appendix B. It is important to note that in the pruning phase, we do not iteratively fine-tune adapter or LM parameters on the downstream task.\nAlgorithm 1: Tropical Adapter Pruning Initialize: T , \u03b7 \u03bb1, \u03bb2; Return: A\u0302, B\u0302; B\u0302+ \u2190 B+, B\u0302\u2212 \u2190 B\u2212; for t in 1,. . . , T do\nfor i in 1,. . . , r do if t is even then\nG\u0302i1 = Diag[b\u0302 i+]A\u0302; loss1 = ||G\u0302i1 \u2212Gi1||2F ; loss2 = ||G\u03021||1,1; \u2113 = 0.5 \u2217 loss1 + \u03bb1 \u2217 loss2;\nelse G\u0302i2 = Diag[b\u0302\ni\u2212]A\u0302; loss1 = ||G\u0302i2 \u2212Gi2||2F ; loss2 = ||G\u03022||1,1; \u2113 = 0.5 \u2217 loss1 + \u03bb2 \u2217 loss2;\nend < check convergence of combined loss > A\u0302\u2190 A\u0302\u2212 \u03b7 \u2217 \u2202\n\u2202(A\u0302) \u2113;\nB\u0302\u2190 B\u0302\u2212 \u03b7 \u2217 \u2202 \u2202(B\u0302) \u2113.\nend end\n2We merge bias term b with the down-projection matrix, thus x\u2190 [x, 1] and A\u2190 [A;b].\nGiven an adapter module, Algorithm1 finds the minimizers A\u0302 and B\u0302 by performing gradient descent-based updates3 over two loss terms expressed in terms of generators G1 and G2. T , r denote the maximum gradient steps and the number of rows in A and columns in B. \u03b7 \u2208 R+ is step size and \u03bb1, \u03bb2 \u2208 R+ indicate the importance of pruning over the shift in generators. We employ layer-wise pruning of the network without any iterative fine-tuning on downstream tasks. We find p% parameters with the smallest magnitude in {A,B} and {A\u0302, B\u0302} separately, denoted by PS and PT . We denote tropical adapter pruning by Tropical that prunes only those parameters in PT which are also present in the set PS . The final percentage of pruned parameters decreases to p\u0302%. We compare the approach with the baseline that prunes p\u0302% of the smallest magnitude parameters from the layer. We denote this setting by Standard. Combined chooses one of Tropical or Standard whichever gives better results on the development set. We omit the comparison with AdapterDrop\n3Not to confuse with gradient descent used to learn model parameters. Here, it is used to solve the optimization problem in Equation (2).\nmethod as even at 50% pruning, the method shows a significant drop in the performance. Standard inherently tests the validity of magnitude-based pruning via lottery ticket hypothesis (Wu et al., 2022) but without iterative retraining of adapter parameters. We do not iteratively fine-tune adapter parameters on the downstream task. The proposed method is agnostic to downstream tasks, models, and the learning algorithm used to train it. Thus, the framework is related to but not directly comparable to model L0 sparsification (Louizos et al., 2017) and low-rank compression (Idelbayev and Carreira-Perpin\u00e1n, 2020)."
        },
        {
            "heading": "4 Experiments",
            "text": "We set up a RoBERTa-base (Liu et al., 2019) with one adapter module inserted in each layer after add and layer norm sub-layer. We follow the adapter configuration from Pfeiffer et al. (2020). For pruning analysis, we consider three tasks\u2014Emotion Recognition in Conversations (ERC), Natural Language Inference (NLI), and Text Classification (TC). For ERC, we use MELD, the task is to classify the emotion of an utterance given past utterances.\nKeeping the current utterance first, we append the past seven utterances in reverse order (Bhardwaj et al., 2022b). For NLI, we use SNLI dataset (Bowman et al., 2015). We append the premise and hypothesis separated by the special token <s>. For TC task, we use three datasets: IMDB (Maas et al., 2011), Rotten Tomatoes RT (Pang and Lee, 2005), and TREC (Li and Roth, 2002). Separately, we pretrain adapters on downstream tasks with batch size 32, LR of 0.001, and 1000 steps with evaluation at every 100 steps using a development set. The evaluation metric for ERC is macro F1 and accuracy for all the other tasks. We set pruning percentage p \u2208 {98%, 96%, . . . , 2%}. Table 1 shows the test performance of networks with the percentage of adapter parameters retained, i.e., (100\u2212 p\u0302)%, this is represented in black-bold fonts. We observe that both Standard and Tropical can be effective in pruning more than 60% of the adapter parameters with a small drop in performance with respect to the full module performance (FM). Moreover, we notice Tropical outperforms Standard in eight out of nine pruned model states on MELD, six out of nine on SNLI, eight out of nine pruned adapter states on RT and IMDB, and six out of nine states on Trec. Across the 45 combinations of tasks and pruning fractions, except for two settings, we observe tropical geometry-based combined approach outperforms the other two, denoted in red font.\nNext, we study tropical pruning in different scenarios\u2014class-bind, class-uniform, and nodewise (See et al., 2016). In class-blind (CB), all the parameters of adapters are considered for pruning p% of the smallest magnitude weights and biases. In class-uniform (CU), we prune p% of the smallest magnitude parameters of each adapter layer separately. We also refer to it as layer-wise pruning. In node-wise pruning, we prune p% of the node-wise\nparameters (considering both weights and biases). As shown in Table 2, in the Standard settings S-CN/ S-CU/ S-CB, we observe layer-wise S-CU pruning works best in four out of six different fractions of parameters retained. In the Tropical pruning settings T-CN/ T-CU/ T-CB, layer-wise pruning T-CU performs best amongst all the considered pruning fractions. Moreover, T-CU works best under each pruning fraction category.\nFigure 1 shows the Objective function in Equation (2) quickly converges to the minimum. This observation corroborates the claim of convexity by (Alfarra et al., 2022). The plot in Figure 2 shows the change in zonotope structure before and after optimization on SNLI. The black polytope is obtained from generators A, B and the red polytope shows the polytope obtained after optimization, i.e., zonotope obtained from A\u0302. B\u0302. We observe the optimization preserves the geometry of zonotopes while enforcing the rows of the down-projection matrices to be as much sparse as possible, i.e., many points in the zonotope come close to zero, keeping necessary boundary points to preserve the geometry. These zonotopes are dual to adapter hypersurfaces, thus preserving one structure enforces the other\u2019s orientation to remain preserved. Hence, one can prune adapters yet maintain their characteristic properties."
        },
        {
            "heading": "5 Conclusion",
            "text": "We proposed a novel approach for adapter pruning by studying their tropical characteristics. We formulated it as an optimization problem that aims to identify row-sparse projection matrices while minimizing the distance between tropical hypersurfaces before and after pruning. We demonstrated the advantages of tropical characterization on five NLP datasets reformulated as classification."
        },
        {
            "heading": "6 Limitations",
            "text": "As our focus is on adapter-based architectures, the proposed approach can not be directly adapted to other parameter-efficient approaches such as soft prompt tuning (Lester et al., 2021; Bhardwaj et al., 2022a) which do not have explicit dense connections and activation. Another limitation comes from ReLU activation function. Since it fits in min-max (Tropical) algebra, we could reformulate the problem in terms of tropical polynomials. However, for other non-linear activation functions such as Tanh, one has to reformulate and likely resort to approximations as there is no straightforward way to cast them in a tropical algebraic expression."
        },
        {
            "heading": "Acknowledgement",
            "text": "We thank the anonymous reviewers for their constructive feedback. This project is supported by the AcRF MoE Tier-2 grant (Project no. T2MOE2008, and Grantor reference no. MOE-T2EP202200017) titled: \u201cCSK-NLP: Leveraging Commonsense Knowledge for NLP\u201d, and the SRG grant id: T1SRIS19149 titled \u201cAn Affective Multimodal Dialogue System\u201d."
        },
        {
            "heading": "A Tropical Algebra and Geometry",
            "text": "To motivate our approach we first provide background on tropical algebra and geometry.\nTropical Arithmetic. Tropical algebra is a variant of classical algebra where basic arithmetic operations are redefined. The tropical sum \u2295 of two numbers represents their maximum and the tropical product \u2299 represents a classical addition4. Thus,\nx\u2295 y = max {x, y} x\u2299 y = x+ y\nFor instance, 2 \u2295 5 = 5 and 2 \u2299 5 = 7. Axioms and order of arithmetic operations in tropical algebra follows the classical, thus addition is commutative and multiplication is distributive over addition:\nx\u2295 y = y \u2295 z (commutative) x\u2299 (y \u2295 z) = x\u2299 y \u2295 x\u2299 z (distributive)\nFrom these properties, it can be inferred that \u2212\u221e is the additive identity as \u2212\u221e \u2295 x = x and 0 is multiplicative identity 0\u2299 x = x. Elements under the tropical arithmetic in the space of real numbers (with \u2212\u221e) are said to form a semiring T denoted by a triplet (R \u222a {\u2212\u221e},\u2295,\u2299).\nTropical Power and Monomial. For any variable x \u2208 T, the tropical power can be defined as x\u2299a = a.x, where a \u2208 N (a natural number). For simplicity of notations, we will write xa in place of x\u2299a. A tropical monomial is expressed in the form\nc x\u03b1 := c\u2299 xa11 \u2299 x a2 2 \u2299 . . .\u2299 x ad n\nwhere c \u2208 R\u222a{\u2212\u221e} and ai \u2208 N. For convenience, we will write tropical monomial by c x\u03b1 where x=(x1, . . . , xd) \u2208 Td and \u03b1=(a1, . . . , ad) \u2208 Nd.\nTropical Polynomial. A d-variable tropical polynomial f(x) can be represented by a finite sum of tropical monomials\nf(x) = c1x \u03b11 \u2295 c2x\u03b12 \u2295 . . .\u2295 cnx\u03b1n\nwhere the ai \u0338= aj when i \u0338= j, coefficients ci \u2208 R \u222a {\u2212\u221e}, \u03b1i = (ai1, ai2, . . . , aid) \u2208 Nd and exponents ai are integers. Ignoring\u2212\u221e for ease, it is important to note that p has a mapping Rd \u2192 R, both x and \u03b1 are d-dimensional vectors.\n4The tropical addition can be defined as a \u2295 b = min{a, b} or max{a, b}, we focus on the latter as we analyze a ReLU-based adapter network.\nTropical powers, monomials and polynomials are basic building blocks of the algorithm we propose for adapter pruning.\nA.1 Tropical Hypersurfaces.\nTropical hypersurfaces are analogues to classical algebraic surfaces and key objects for us to study for adapter pruning. Given a tropical polynomial f(x) = c1x\n\u03b11 \u2295 . . .\u2295 cnx\u03b1n , its tropical hypersurface is a set of points where p is attained by two or more constituting monomials, thus\nF(p) := {x \u2208 Rd : cix\u03b1i = cjx\u03b1j , for some \u03b1i \u0338= \u03b1j}.\nHere we mention a few provable facts\u2014F divides the domain of p into convex regions (or cells). Polynomial p is non-linear at x if and only if x lies onF . Similar to algebraic polynomials, we can identify Newton polytopes associated to tropical polynomials.\nNewton Polytopes. For a given polynomial f(x) = c1x\n\u03b11 \u2295 . . . \u2295 cnx\u03b1n , its newton polytope is defined by the convex hull of the exponents \u03b1i \u2208 Nd. The points \u03b1i and polytope lies in a d-dimensional plane (Rd). Thus\n\u2206(p) := ConvHull({\u03b1i \u2208 Rd : ci \u0338= \u2212\u221e}ni=1)\nThe tropical polynomial p determines the dual subdivision \u03b4(p) of newton polytope. The tropical hypersurface F(p) is dual graph to this \u03b4(p), i.e., vertices of F(p) are regions of \u03b4(p) and edges represent two adjacent regions in \u03b4(p)5. Each vertex in \u03b4(p) corresponds to one \"cell\" in Rb where p\n5Reader can read more about dual graphs in (Deo, 2017)\nis linear. Since \u03b4(p) is in one-to-one correspondence with the tropical hypersurface, we study the adapter characteristics\u2014underlying hypersurfaces F(p))\u2014by studying the orientation of the primal graph \u03b4(p). To determine \u03b4(p), we use the fact that when the model is bias-free, \u03b4(p) = \u2206(p) (Zhang et al., 2018; Alfarra et al., 2022). Figure 3 provides an illustration of F(p) adn \u03b4(p) for a specific p.\nZonotopes. The zonotope formed by v1, . . . ,vm \u2208 Rn is defined as Z(v1, . . . ,vm) := { \u2211m\ni=1 \u03bbivi, 0 \u2264 \u03bbi \u2264 1}.\nMinkowski sum. Given two sets P1 and P2 in Rd, the Minkowski is defined as\nP1+\u0303P2 := {v1 + v2 : v1 \u2208 P1,v2 \u2208 P2}\nProperty-1. The Minkowski sum of two polytopes is the convex hull of their vertex sets. Let, V(P ) be the vertex sets of a polytope P , then\nP1+\u0303P2 = ConvHull ( V(P1)+\u0303V(P2) ) Under bias-free assumption,\nProperty-2. Let p1 and p2 be the tropical polynomials, then\n\u03b4(p1 \u2299 p2) = \u03b4(p1)+\u0303\u03b4(p2)"
        },
        {
            "heading": "B Pruning Objective",
            "text": "B.1 Notations Used We denote Wd, Wu, h by A, B, and x, respectively; B+:=max{B,0}; B\u2212:=max{\u2212B,0}; bi denotes ith row of B; bi+:=max{bi,0}, bi\u2212:=max{\u2212bi,0}; Diag[u] arranges u in a diagonal matrix; ||G||1,1:=\u03a3dk=1||G(i, :)||1; || \u00b7 ||F denotes Frobenius Norm.\nB.2 Derivation of Pruning Objective Let f(x) = Bmax{Ax, 0}, then\nf(x) = (B+ \u2212B\u2212) ( max{A+x,A\u2212x} \u2212A\u2212x ) = [ B+max{A+x,A\u2212x}+B\u2212A\u2212x\n] \u2212 [ B\u2212max{A+x,A\u2212x}+B+A\u2212x\n] Thus we define H(x) and Q(x) as\nH(x) := [ B+max{A+x,A\u2212x}+B\u2212A\u2212x ] Q(x) := [ B\u2212max{A+x,A\u2212x}+B+A\u2212x ]\nThus, f(x) = H(x) \u2212 Q(x). Let f i denote the first output from adapter block, bi = B[i, :] (i.e. ith row of B). We useH and Q to denote tropical hypersurfaces of H and Q at node i.\nH =\n[ p\u2299\nj=1\n( xa + j \u2295 xa \u2212 j )bi+j ]\u2299 [ p\u2299 j=1 ( xa \u2212 j )bi\u2212j ]\nComputing dual subdivision \u03b4(H) = [ +\u0303\np j=1 ( bi+ConvHull(a+j , a \u2212 j ) )] +\u0303 [ +\u0303\np j=1 ( bi\u2212j a \u2212 j )] (P-1)\n= [ +\u0303\np j=1 ( bi+ConvHull(a+j \u2212 a \u2212 j , 0) )] +\u0303 [ +\u0303\np j=1 ( bi\u2212j a \u2212 j )] + shift (P-2)\n= [ +\u0303\np j=1 ( bi+ConvHull(aj , 0) )] + shift\nSimilarly, we compute dual subdivision of qi\n\u03b4(Q) = [ +\u0303\np j=1 ( bi\u2212ConvHull(aj , 0) )] + shift\nNote that convex hull of aj and 0 is a line segment. Thus, \u03b4(hi) defines a Minkowski sum over line segments which is a zonotope. Following Alfarra et al. (2022), and ognoting the shifts, one can straightaway obtain zonotope generators G1 and G2 for \u03b4(H) and \u03b4(Q), respectibely."
        }
    ],
    "title": "Adapter Pruning using Tropical Characterization",
    "year": 2023
}