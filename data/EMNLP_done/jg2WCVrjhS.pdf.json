{
    "abstractText": "Transformers have become the gold standard for many natural language processing tasks and, in particular, for multi-hop question answering (MHQA). This task includes processing a long document and reasoning over the multiple parts of it. The landscape of MHQA approaches can be classified into two primary categories. The first group focuses on extracting supporting evidence, thereby constraining the QA model\u2019s context to predicted facts. Conversely, the second group relies on the attention mechanism of the long input encoding model to facilitate multi-hop reasoning. However, attention-based token representations lack explicit global contextual information to connect reasoning steps. To address these issues, we propose GEMFormer, a two-stage method that first collects relevant information over the entire document to the memory and then combines it with local context to solve the task1. Our experimental results show that fine-tuning a pre-trained model with memory-augmented input, including the most certain global elements, improves the model\u2019s performance on three MHQA datasets compared to the baseline. We also found that the global explicit memory contains information from supporting facts required for the correct answer.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alsu Sagirova"
        },
        {
            "affiliations": [],
            "name": "Mikhail Burtsev"
        }
    ],
    "id": "SP:2dd1e566e8b25cd05d70ad321ee85c4590a744f0",
    "references": [
        {
            "authors": [
                "Joshua Ainslie",
                "Santiago Ontanon",
                "Chris Alberti",
                "Vaclav Cvicek",
                "Zachary Fisher",
                "Philip Pham",
                "Anirudh Ravula",
                "Sumit Sanghai",
                "Qifan Wang",
                "Li Yang."
            ],
            "title": "ETC: Encoding long and structured inputs in transformers",
            "venue": "Proceedings of the 2020 Conference",
            "year": 2020
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "G P Shrivatsa Bhargav",
                "Michael Glass",
                "Dinesh Garg",
                "Shirish Shevade",
                "Saswati Dana",
                "Dinesh Khandelwal",
                "L Venkata Subramaniam",
                "Alfio Gliozzo."
            ],
            "title": "Translucent answer predictions in multi-hop reading comprehension",
            "venue": "Proceedings of the AAAI Conference",
            "year": 2020
        },
        {
            "authors": [
                "Aydar Bulatov",
                "Yuri Kuratov",
                "Mikhail S Burtsev."
            ],
            "title": "Recurrent memory transformer",
            "venue": "NeurIPS 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Mikhail S. Burtsev",
                "Grigory V. Sapunov."
            ],
            "title": "Memory transformer",
            "venue": "ArXiv, abs/2006.11527.",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Campos",
                "V\u00edtor Mangaravite",
                "Arian Pasquali",
                "Al\u00edpio Jorge",
                "C\u00e9lia Nunes",
                "Adam Jatowt."
            ],
            "title": "Yake! keyword extraction from single documents using multiple local features",
            "venue": "Information Sciences, 509:257\u2013289.",
            "year": 2020
        },
        {
            "authors": [
                "Sarath Chandar",
                "Sungjin Ahn",
                "Hugo Larochelle",
                "Pascal Vincent",
                "Gerald Tesauro",
                "Yoshua Bengio"
            ],
            "title": "Hierarchical memory networks",
            "year": 2016
        },
        {
            "authors": [
                "Christopher Clark",
                "Matt Gardner."
            ],
            "title": "Simple and effective multi-paragraph reading comprehension",
            "venue": "In",
            "year": 2018
        },
        {
            "authors": [
                "Ruiliu Fu",
                "Han Wang",
                "Xuejun Zhang",
                "Jun Zhou",
                "Yonghong Yan."
            ],
            "title": "Decomposing complex questions makes multi-hop QA easier and more interpretable",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 169\u2013180,",
            "year": 2021
        },
        {
            "authors": [
                "Yori Zwols",
                "Georg Ostrovski",
                "Adam Cain",
                "Helen King",
                "Christopher Summerfield",
                "Phil Blunsom",
                "Koray Kavukcuoglu",
                "Demis Hassabis."
            ],
            "title": "Hybrid computing using a neural network with dynamic external memory",
            "venue": "Nature, 538(7626):471\u2013476.",
            "year": 2016
        },
        {
            "authors": [
                "Dirk Groeneveld",
                "Tushar Khot",
                "Mausam",
                "Ashish Sabharwal."
            ],
            "title": "A simple yet strong pipeline for HotpotQA",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8839\u20138845, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Ankit Gupta",
                "Jonathan Berant."
            ],
            "title": "Gmat: Global memory augmentation for transformers",
            "venue": "ArXiv, abs/2006.03274.",
            "year": 2020
        },
        {
            "authors": [
                "Xanh Ho",
                "Anh-Khoa Duong Nguyen",
                "Saku Sugawara",
                "Akiko Aizawa."
            ],
            "title": "Constructing a multihop QA dataset for comprehensive evaluation of reasoning steps",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Omar Khattab",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Baleen: Robust multi-hop reasoning at scale via condensed retrieval",
            "venue": "CoRR, abs/2101.00436.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyang Lin",
                "Yuxin Wang",
                "Xiangyang Liu",
                "Xipeng Qiu."
            ],
            "title": "A survey of transformers",
            "venue": "AI Open, 3:111\u2013132.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Vaibhav Mavi",
                "Anubhav Jangra",
                "Adam Jatowt."
            ],
            "title": "A survey on multi-hop question answering and generation",
            "venue": "ArXiv, abs/2204.09140.",
            "year": 2022
        },
        {
            "authors": [
                "Kosuke Nishida",
                "Kyosuke Nishida",
                "Masaaki Nagata",
                "Atsushi Otsuka",
                "Itsumi Saito",
                "Hisako Asano",
                "Junji Tomita."
            ],
            "title": "Answering while summarizing: Multi-task learning for multi-hop QA with evidence extraction",
            "venue": "Proceedings of the 57th Annual Meet-",
            "year": 2019
        },
        {
            "authors": [
                "Kosuke Nishida",
                "Kyosuke Nishida",
                "Itsumi Saito",
                "Sen Yoshida."
            ],
            "title": "Towards interpretable and reliable reading comprehension: A pipeline model with unanswerability prediction",
            "venue": "CoRR, abs/2111.09029.",
            "year": 2021
        },
        {
            "authors": [
                "Lin Qiu",
                "Yunxuan Xiao",
                "Yanru Qu",
                "Hao Zhou",
                "Lei Li",
                "Weinan Zhang",
                "Yong Yu."
            ],
            "title": "Dynamically fused graph network for multi-hop reasoning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Alsu Sagirova",
                "Mikhail Burtsev."
            ],
            "title": "Complexity of symbolic representation in working memory of transformer correlates with the complexity of a task",
            "venue": "Cognitive Systems Research, 75:16\u201324.",
            "year": 2022
        },
        {
            "authors": [
                "Alsu Sagirova",
                "Mikhail Burtsev."
            ],
            "title": "Extending transformer decoder with working memory for sequence to sequence tasks",
            "venue": "Advances in Neural Computation, Machine Learning, and Cognitive Research V, pages 253\u2013260, Cham. Springer Interna-",
            "year": 2022
        },
        {
            "authors": [
                "Artyom Sorokin",
                "Nazar Buzun",
                "Leonid Pugachev",
                "Mikhail Burtsev"
            ],
            "title": "Explain my surprise: Learning efficient long-term memory by predicting uncertain outcomes",
            "year": 2022
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason Weston",
                "Rob Fergus"
            ],
            "title": "End-to-end memory networks",
            "year": 2015
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "MuSiQue: Multihop questions via single-hop question composition",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Ming Tu",
                "Kevin Huang",
                "Guangtao Wang",
                "Jing Huang",
                "Xiaodong He",
                "Bowen Zhou."
            ],
            "title": "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
            "venue": "CoRR, abs/1911.00484.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "Siyuan Wang",
                "Zhongyu Wei",
                "Zhihao Fan",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Locate then ask: Interpretable stepwise reasoning for multi-hop question answering",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyang Wu",
                "Zhenzhong Lan",
                "Kun Qian",
                "Jing Gu",
                "Alborz Geramifard",
                "Zhou Yu."
            ],
            "title": "Memformer: A memory-augmented transformer for sequence modeling",
            "venue": "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 308\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Yury Zemlyanskiy",
                "Joshua Ainslie",
                "Michiel de Jong",
                "Philip Pham",
                "Ilya Eckstein",
                "Fei Sha."
            ],
            "title": "ReadTwice: Reading very large documents with memories",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Wenting Zhao",
                "Justin T. Chiu",
                "Claire Cardie",
                "Alexander M. Rush"
            ],
            "title": "Hop, union, generate: Explainable multi-hop reasoning without rationale supervision",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Transformer (Vaswani et al., 2017) and its variants (Lin et al., 2022) have become one of the most popular solutions for various NLP tasks. Particularly, Transformers are applied to solve the multi-hop question answering (MHQA) tasks (Tu et al., 2019; Zemlyanskiy et al., 2021; Khattab et al., 2021) that require reasoning over multiple parts of the long document to answer the question.\nThe problem of reasoning in MHQA has attracted a lot of research recently (Mavi et al., 2022). One group of methods focuses on the usage of subnetworks or dedicated modules to extract evidence\n1https://github.com/Aloriosa/GEMFormer\nfrom a long document and then solve the questionanswering (QA) task based on the detected evidence facts (Nishida et al., 2021, 2019; Tu et al., 2019; Bhargav et al., 2020; Zhao et al., 2023). The resulting performance of such models highly depends on the evidence extraction method quality, limiting the QA model context to a small number of pre-selected facts. Another group of methods addresses the general long document encoding problem by sparsifying the attention patterns to enlarge the maximal input sequence length (Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020). Despite the merits of these models, their attention-based token representations combine local and global information in the same vector. The high-level contextual features are spread over a long sequence which makes it harder to access them. To address the described problems, we propose GEMFormer (Global Explicit Memory Transformer). GEMFormer is a method for augmenting the pre-trained language model with memory to store global information relevant to the task. It processes long input concatenated with a memory sequence consisting of tokens from input that are important to solve the task. Token importance is defined by the language model uncertainty."
        },
        {
            "heading": "2 Related work",
            "text": "Augmentation of the neural network model with memory provides additional space to store relevant information that can be used to improve model performance and reduce computational costs. Early examples of memory-augmented neural network architectures such as RNN and LSTM (Hochreiter and Schmidhuber, 1997) used hidden states as internal memory. Graves et al. (2014) and Graves et al. (2016) introduced the external type of memory, where a separate network manipulated memory. With the growing popularity of the attention mechanism, attention was adopted for model-memory interaction (Weston et al., 2015; Sukhbaatar et al.,\n2015; Chandar et al., 2016). A number of recent papers used memory to store global input representations. Gupta and Berant (2020); Zemlyanskiy et al. (2021); Wu et al. (2022) employed memory to encode and retrieve the compressed view of the long input. Memory (Burtsev and Sapunov, 2020) and Recurrent Memory (Bulatov et al., 2022) Transformers used memory to store non-local representations of the processed sequence. The interpretable working memory in the Transformer decoder (Sagirova and Burtsev, 2022a,b) was presented to store contextual information that was not explicitly stated in the input. Sorokin et al. (2022) summarized representations of events from the distant past into memory to improve the model predictions. GEMFormer adapts the idea of interpretable memory by storing tokens from the input sequence. This augmentation of memory enriches long inputs with essential global contextual information important for the task."
        },
        {
            "heading": "3 Global Explicit Memory",
            "text": "We implemented GEMFormer with a RoBERTabase (Liu et al., 2019) model as a backbone. Global explicit memory is a sequence of document tokens that are important for correct reasoning and answer prediction. The model\u2019s uncertainty is used to measure the importance of inputs. Given an input sequence x = [t1, t2, . . . , tm] of m tokens and a vocabulary of size n, we first obtain a vector p = Softmax(LM_RoBERTa(x)) of token candidates\u2019 probabilities using RoBERTa with language modeling (LM) head. Then we calculate the entropy H = \u2212 1n \u2211n j=1 pj log pj for each input position. In our experiments, we tested two\nmemory population conditions for entropy:\nHighest H = arg top k [H(ti), ti \u2208 x], Low H = [ti \u2208 x : H(ti) < \u03b8],\n(1)\nwhere \u03b8 is a model uncertainty threshold and k is a selected memory size (see details in Section 4). The general motivation behind the entropy-based memory population is the following. The model is fed with a question and a context, and the entropy is computed for each contextual token. This entropy is conditional with respect to the question and token-surrounding context. Such entropy value measures how much entropy a token has remaining if we have already learned the question and the context. In other words, how easily the token can be predicted given the question and the context. Taking into account that a document is a collection of question-relevant and distractor paragraphs, the entropy of a task-relevant token should be lower than the entropy of the irrelevant one.\nThe GEMFormer architecture is depicted in Figure 1. To fit RoBERTa maximum sequence length limit, the contextual document is split into segments and each segment is concatenated to the question. Input processing consists of two stages: 1) document comprehension and memory population, and 2) task predictions generation using memory-enhanced inputs. In the first stage, question-context segments are passed through the RoBERTa model with LM head to obtain crossdictionary distributions for entropy estimation. Context tokens with little independent semantic content, such as special separators, punctuation marks, and stop words, are not considered for the memory population (see details in Appendix B). Global memory is then collected from the remaining tokens from the document based on the chosen\nentropy conditions (Eq. 1). In the second stage, question and global memory tokens are concatenated with each segment for the MHQA task training. The model\u2019s weights for the first stage are updated every epoch with the weights of the model trained to solve the target task in the second stage.\nWe evaluated GEMFormer on three English MHQA datasets: HotpotQA distractor setting (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020) and MuSiQue-Ans (Trivedi et al., 2022). Further in the paper, we will refer to them as HP, 2W, and MSQ respectively. Data preprocessing, training, and inference details are described in Appendix B."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "As baselines in our experiments we used a RoBERTa fine-tuned on the task without memory and a RoBERTa fine-tuned with memory bank of 200 YAKE! (Campos et al., 2020) keyword tokens from the contextual document (see Table 1). We started memory experiments from an assumption that tokens for which a language model is the most uncertain might be the most useful in the global context. However, the distribution of entropy of a task-tuned baseline model over the document tokens (see Appendix E Fig. 5) showed that the majority of the context tokens have high entropy, and locations of low entropy tokens are closely aligned with answers and supporting facts within the document. This observation points to the hypothesis that low entropy tokens might be helpful for generation because they overlap with the context with information relevant to the answer. Also, the global memory of low entropy tokens tends to guide the answer model to focus on such tokens. To ensure the correctness of this observation, we examined if the low entropy tokens are not rare entities for which the model has less amount information about. Figure 6 in Appendix E shows that the\nthe pre-trained model associates newly appeared tokens (<t>, </t>, [/sent] tokens) with a notably high entropy. Moreover, comparison of rare tokens entropy distributions to the overall context entropy distributions (see Table 8 in Appendix D) confirmed that rare tokens tend to have higher entropy and are infrequent in global memory. During fine-tuning, the entropy of rare tokens related to the question becomes lower compared to irrelevant ones. This allows the model to preferentially store in the memory more relevant entities. Besides, the model with a global memory filled with top-200 highest entropy tokens (Highest H in Equation 1 and Table 1) produced no improvement over the baselines.\nWe hypothesize that a model combining a tuned RoBERTa encoder with a frozen LM head for uncertainty estimation tends to detect parts of a text that are essential for the answer by reducing their uncertainty (see also Appendix B). To test this hypothesis, we used a variable-size memory consisting of tokens with entropy lower than a given threshold (Low H in Equation 1). The fine-tuned model with constant entropy threshold \u03b8\u0302 outperformed the highest performing baseline memoryfree model by 1.6 joint F1 points for HP, 1.59 and 0.58 points for 2W answer and supporting evidence prediction, and 1.25 F1 points for MSQ supporting paragraphs prediction. We also tested a dynamical entropy threshold as a value of the fifth percentile of the document token entropy assuming that this might help to better cover the full supporting evidence (Low (H < 5th%) in Table 1). Such a memory selection rule showed the best answer F1 for MSQ and led to a slight improvement compared to the task-tuned RoBERTa baseline but was weaker than fixed \u03b8\u0302 for HP and 2W.\nWe also evaluated the ChatGPT2 (gpt-3.5turbo-0613 checkpoint) with memory generated\n2https://openai.com/blog/chatgpt\nby GEMFormer Low (H < \u03b8\u0302). The results are presented in Table 2. We tested the question-only inputs to assess the general QA ability of the model and the two variants of input document representation (the ChatGPT prompts for our experiments are listed in Appendix C). In the first case, a doc-\nument sentences were stored in a vector database and ranked by relevance to the question via the document-based QA pipeline from the Langchain library. The retrieved most relevant sentences were concatenated to the question and used for answer prediction. As a result, the ChatGPT performance was highly dependent on the effectiveness of the evidence retrieval process. If the retriever failed to accurately extract the evidence, the subsequent memory augmentation did not rectify this shortcoming. In the second case, we fed a model with a concatenation of a question and a contextual document with and without global memory augmentation. Although the answer F1 scores were significantly improved compared to the retrieval setting, they still fell short of the fine-tuned RoBERTa scores. However, in this setting, we observed notable improvements in answer F1 scores (+6 F1 for HP, +7.94 F1 for 2W, and +5.47 F1 for MSQ), signifying the efficacy of memory augmentation in enhancing answer generation.\nAblation study To verify the proposed memory augmentation, we conducted an ablation study on HP. Results are shown in Table 3. To test the importance of the question input for memory filling, we trained No Q/Doc only model with memory selection rule H < 0.3 and stage 1 entropies calculated from the document-only context. The resulting joint F1 degraded by 0.68 points indicating how question affects the memory quality. With No fine-tune configuration we checked if the second stage MHQA task tuning affects memory content\nor general-purpose LM can be used for that as well. We tuned the model on stage 2 but froze the pretrained RoBERTa for memory uncertainty estimation. This led to the 3 joint F1 points decrease. Thus, updating the model for uncertainty estimation is critical to make memory useful. Indeed, we\nmeasured average per token entropy during training of the GEMFormer Low (H < 0.3) model and found the growing difference in uncertainty between evidence and distractor facts (Fig. 2). The Random memory ablation was to fill memory with tokens selected randomly from the document. It showed that global memory with the arbitraryselected content can not only fail to improve predictions but can actually degrade the model performance. We also provide the comparison of our best-\nperforming GEMFormer Low (H < \u03b8\u0302) model with existing base-sized MHQA models in Appendix A.\nMemory analysis The results shown above demonstrate that global memory of supporting facts improves MHQA performance, and it is natural to expect that a larger memory size should give better results. We analyzed the dev set predictions of GEMFormer Low (H < 0.3) trained on HP with three different random seeds (Table 4) and found\nthat instances, where the model utilized larger average memory sizes, corresponded to higher joint F1 scores, thereby reinforcing our hypothesis. Specifically, a five-fold expansion in memory is linked to an increase of 1.32 points in the joint F1 score. Notably, runs with larger memory sizes indicate a higher coverage of evidence, albeit it does not exceed 30%. The average size of supporting facts\nis 83 tokens which means that only about half of the memory is occupied by useful information with the rest of the memory slots filled with tokens from distractor paragraphs. This occupation is the same for correct and wrong answer predictions.\nCounterintuitively, samples with larger memory sizes for the same model have lower answer prediction quality (Fig. 3). This could potentially be attributed to the distracting influence of irrelevant tokens stored within the memory. The larger mem-\nory size should have more unrelated tokens so the training should optimize the trade-off between enlarging memory for better evidence coverage and decreasing it to remove noise. This is supported by the results of the deeper analysis in Fig. 4. Samples with the memory containing more tokens from evidence and less irrelevant ones tend to have better\nscores (Fig. 4a). And the amount of evidence stored in memory has almost no effect for low coverage (up to 30%) and leads to the answer F1 decrease for higher coverage values (Fig. 4b)."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this study, we demonstrated how utilizing uncertainty-based global explicit memory can enhance the model performance on MHQA tasks. Our findings indicate that utilizing low entropy context tokens can aid the model in MHQA reasoning, but only when the entropy estimation model is specifically fine-tuned to the target task. Experiments show that higher-performing models use larger memory sizes with better coverage of supporting facts.\nLimitations\nThere are several limitations to this work. First, the global explicit memory augmentation of the input sequence may increase the training time by shortening the context chunk lengths. Second, the current implementation of memory token selection results in storing a significant fraction of irrelevant tokens which interferes with the calculation of correct predictions. We will work on methods to improve the relevance of information stored in memory.\nEthics Statement\nThis work is a fundamental research work that focuses on technical improvement, thus we have not applied additional filtering techniques to the textual data we used, beyond what has been performed on\nthe original datasets. The textual data we used may have information naming or uniquely identifying individual people or offensive content that we have not been able to identify, as those are out of the focus of this work."
        },
        {
            "heading": "Acknowledgements",
            "text": "AS was supported by a grant for research centers in the field of artificial intelligence, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730321P5Q0002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.\nAuthors are thankful to SberDevices for granting access to additional computational resources."
        },
        {
            "heading": "A Comparison with other MHQA methods",
            "text": "We also compared our best-performing GEMFormer Low (H < \u03b8\u0302) model with existing basesized MHQA models in Table 5. The proposed model shows the best results in MSQ answer prediction and 2W supporting evidence prediction. On HotpotQA our method achieves answer F1 comparable to ETC, BigBird, and ReadTwice and joint F1 higher than Longformer with the smaller-sized model."
        },
        {
            "heading": "B Datasets and Training Details",
            "text": "All datasets\u2019 contexts are based on Wikipedia. 2WikiMultiHopQA also has additional evidence relations tuples that were not used in our experiments.\nModel Size HotpotQA 2WikiMHQA MuSiQue Ans | Sup | Jnt Ans | Sup Ans | Sup\nGEMFormer (H < \u03b8\u0302) 125M 75.13|83.8 |64.77 67.14|95.67 31.56|63.85 IRC (2021) 110M 72.9 |79.8 | \u2014 \u2014 | \u2014 \u2014 | \u2014 *DFGN (2019) 110M 69.23| \u2014 | \u2014 38.49|57.79 \u2014 | \u2014 StepReasoner (2022) 110M \u2014 | \u2014 | \u2014 73.03|91.21 \u2014 | \u2014 RAG-Small (2023) 140M 62.8 |49.0 | \u2014 \u2014 | \u2014 24.2 | \u2014 HUG-Small (2023) 140M 66.8 |67.1 | \u2014 \u2014 | \u2014 25.1 | \u2014 SAE (2019) 110M 74.81|85.27|66.45 \u2014 | \u2014 \u2014 | \u2014 Longformer-base (2020)149M \u2014 | \u2014 |64.4 \u2014 | \u2014 \u2014 | \u2014 ETC (2020) 166M 75.1 |86.9 | \u2014 \u2014 | \u2014 \u2014 | \u2014 BigBird-ITC (2020) 132M 75.7 |86.8 |67.7 \u2014 | \u2014 \u2014 | \u2014 BigBird-ETC (2020) 132M 75.5 |87.1 |67.8 \u2014 | \u2014 \u2014 | \u2014 ReadTwice (2021) 140M 75.9 | \u2014 | \u2014 \u2014 | \u2014 \u2014 | \u2014\nTable 5: Comparison with existing methods. Related works\u2019 F1 scores are taken from the corresponding papers. The mark * means scores taken from Fu et al. (2021).\nThe number of context paragraphs, the number of reasoning hops, sources, and sizes of train and dev sets for each dataset used in our experiments are presented in Table 6.\nPreprocessing and Objective HotpotQA and 2WikiMultiHopQA have questions with yes/no answers and context-span answers. Also, both datasets have golden targets for supporting evidence sentences and paragraphs. We prepared HotpotQA and 2WikiMultiHopQA inputs and targets following the Longformer (Beltagy et al., 2020) MHQA approach. To prepare the input sequence, we added special tokens to indicate the question start and end, paragraph title start and end, and sentence end. The special tokens were added to the RoBERTa vocabulary and randomly initialized before fine-tuning. The training was carried out in a multi-task way to predict question types (yes/no/span), answer spans, relevant paragraphs, and evidence sentences jointly. We used the following loss function proposed by the Longformer paper authors:\nL = \u03b11CEqtype + \u03b12or_CEspan+\n\u03b13CEpara + \u03b14CEsent, (2)\nwhere or_CEspan is the noisy labels handling loss function (Clark and Gardner, 2018) to account for all possible occurrences of answer span and the\nrest are cross-entropy losses for the classification of question types, supporting paragraphs, and evidence sentences. Weightenings \u03b1i were used to keep each loss term in a similar range6. For HotpotQA we used \u03b11 = 10, \u03b12,3,4 = 1 to reproduce the RoBERTa baseline scores from the Longformer paper. We also tested \u03b11 of 1, 2 and 5 because of the lower values of the question type loss compared to the other loss terms during training but \u03b11 = 10 showed the best results. For 2WikiMultiHopQA we did not observe the significant differences between loss terms values and used unit values for all alphas.\nMuSiQue preprocessing and training were carried out following the Trivedi et al. (2022) End2End model setting. The dataset has no yes/no type of questions, only document spans. The evidence is presented with golden supporting paragraphs. So we added a paragraph start indicator token to the RoBERTa vocabulary for supporting evidence prediction. It was randomly initialized before fine-tuning. The answer span targets were selected as the last occurrence of the span in the supporting paragraphs. The multi-task training objective was a sum of an answer span cross-entropy loss CEspan and supporting paragraphs binary crossentropy loss BCEpara:\nL = CEspan +BCEpara. (3)\nTraining and Hyperparameters For all datasets, the long input sequence was split into segments to be processed by the pre-trained RoBERTa model with 512 tokens input length. To avoid partitioning answer spans, we applied 20 token length overlaps between consecutive input chunks. The maximum memory size was limited to 200 tokens to balance the lengths of memory and context in the model input. On the memory population stage, before we collected the information from the context to the memory, we filtered out special tokens related to the supporting evidence prediction, all possible punctuation-related tokens, and stop words7 to reduce the amount of noninformative memory content.\nTo start training we used the public RoBERTabase checkpoint8 (125M parameters) and the Hug-\n6https://github.com/allenai/longformer/issues /143#issuecomment-733894862\n7https://github.com/nltk/nltk/wiki/Frequently -Asked-Questions-(Stackoverflow-Edition)#how-t o-remove-stopwords-with-nltk\n8https://huggingface.co/roberta-base\ngingface Transformers RoBERTa model implementation9. The HotpotQA and 2WikiMultiHopQA training and evaluation consisted of four subtasks: question type prediction using the question classification head over the first [CLS] token, answer span prediction, and evidence paragraphs and sentences prediction. The MuSiQue training and evaluation consisted of two subtasks: answer span prediction and supporting paragraphs prediction. To predict supporting evidence, we used two-layer feedforward networks with GeLU activation between layers applied over the paragraph title end and sentence end tokens for HotpotQA and 2WikiMultiHopQA and over the paragraph start tokens for MuSiQue. For the fixed entropy thresholdbased memory experiments, we tested a number of threshold values and reported results with the best \u03b8\u0302 choices to detect supporting evidence-related tokens.\nDuring inference, the given model checkpoint and pre-trained LM head were used to generate a global memory. To compute evaluation metrics, we collected predictions across document segments and took the most probable answer as the final prediction. For HotpotQA we ensured that predicted evidence came from the two most probable paragraphs according to the dataset setup (Groeneveld et al., 2020).\nTraining hyperparameters are listed in Table 7. All models were trained on 8 NVIDIA A100 GPUs. The linear decay schedule without a warmup, with 1000 steps and with 10% of training steps linear warmup and constant schedule with 1000 steps warmup were tested. Linear warmup with linear decay scheduler that performed best on all datasets was used in our experiments. We also tested learning rates of 2e-5, 3e-5, and 5e-5 and epochs of 3, 5, and 7 for hyperparameter search.\nUncertainty Estimation Procedure GEMFormer stage 1 includes predicting token\n9https://huggingface.co/docs/transformers/mod el_doc/roberta\ndistributions with the task-tuned RoBERTa encoder followed by a pre-trained LM head. By this, we combined the reasoning skills of the tuned encoder and the language structure knowledge of the LM head. The RoBERTa pre-training data contains Wikipedia corpus (Liu et al., 2019) and the datasets we used are also based on Wikipedia. This assures that pre-trained RoBERTa LM head weights possess information about the language distribution of the context documents in the training set. The frozen LM head is able to map the output representations of the encoder while preserving the uncertainty of predictions. As the encoder is progressively fine-tuned for MHQA task, its output representations become more certain for tokens related to the answer and supporting facts. During our primary experiments, we also tested the LM head jointly fine-tuned with the encoder model. It led to the same results as a baseline and did not yield any significant differences in uncertainty values for answer and supporting facts compared to other parts of the text. This observation can be attributed to the inherent nature of the LM task, which aims to predict all tokens in a sequence without any specific focus on the MHQA reasoning."
        },
        {
            "heading": "C ChatGPT evaluation prompts",
            "text": "The prompt for experiments with the retrieved context was the following: System: You are a world-class algorithm to answer questions in a specific format. Human: Answer the question using the following context <context>. Question: <question>. Tips: Make sure to answer in the correct format.\nTo combine the question and the full contextual document in the model input (Question+Context in Table 2) we used the following prompt: System: You are a world-class algorithm to answer questions based on the given text. Store the answer and the supporting evidence from the text in the following structure: {\u2019answer\u2019: \u2019answer string\u2019, \u2019supporting evidence\u2019: [\u2019list of supporting evidence sentences\u2019]}. Human: Text: <context> Question: <question>.\nFinally, the memory-augmented input (Question+Memory+Context in Table 2) prompt was the following: System: You are a world-class algorithm to answer questions based on the given text and memory.\nUse the provided memory to detect question-related information in text. Store the answer and the supporting evidence from the text in the following structure: {\u2019answer\u2019: \u2019answer string\u2019, \u2019supporting evidence\u2019: [\u2019list of supporting evidence sentences\u2019]}. Human: Memory: <mem> Text: <context> Question: <question>."
        },
        {
            "heading": "D Rare tokens entropy",
            "text": "We collected rare (occurring in a context less than 5 times) tokens from each contextual document of the validation set for each dataset to compare the characteristics of the entropy distributions of rare tokens to the overall context entropy distributions. The descriptive statistics averaged over the validation set samples are presented in Table 8. As a result, we can see that the mean, median, mode and maximum values of rare tokens distributions match overall context distributions up to two decimal places for all three datasets. Also, both distribution types have skewness coefficients lower than \u22121 for each dataset, which means the distributions are skewed to the left. This observation confirms that rare tokens most frequently have high entropy values. We also calculated how many global memory tokens are rare: for HotpotQA 61 \u00b1 21% of memory tokens are rare ones, for 2WikiMHQA \u2013 9\u00b1 9%, and for MuSiQue \u2013 16\u00b1 15%. These observations imply that, in practice, rare tokens tend to have high entropy relative to the overall context entropy, and for two out of three datasets rare entities are infrequent in global memory. The rare tokens analysis suggests that during fine-tuning, the entropy of question-related rare tokens decreases compared to the entropy of irrelevant tokens."
        },
        {
            "heading": "E Entropy over document distribution heatmaps",
            "text": "In this section, we listed per token entropy heatmaps for one validation context example. Heatmaps illustrate that the pre-trained model\u2019s uncertainty is almost uniformly low except for newly-added tokens (see Fig. 6). Looking at the entropy difference after the first epoch and before fine-tuning, we can see how the uncertainty of title and sentence indicator tokens decreases, and the supporting facts have tokens with unchanged uncertainty, while the entropy of the rest of the document increases (see Fig. 7)."
        }
    ],
    "title": "Uncertainty Guided Global Memory Improves Multi-Hop Question Answering",
    "year": 2023
}