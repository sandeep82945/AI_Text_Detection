{
    "abstractText": "Developing cultural adaptation methods is important, which can improve the model performance on the low-resource ones and provide more equitable opportunities for everyone to benefit from advanced technology. Past methods primarily focused on multilingual and multimodal capabilities, and the improvement of multicultural competence is still an unexplored problem. This is largely due to the difficulty of data scarcity and expensive annotation. In this paper, we navigate this uncharted territory by leveraging high-resource cultures to facilitate comprehension of low-resource ones. We first introduce an annotation-free method for cultural-concept adaptation and construct a concept mapping set. To facilitate the model\u2019s comprehension of cultural-concept mappings, we propose a new multimodal data augmentation called CultureMixup. This approach employs a three-tier code-switching strategy on textual sentences. Additionally, it uses a cultural concept-based mixup method for the images. This combination effectively generates new data instances across culture, phrase, word, and image levels. For visually grounded reasoning across languages and cultures, experimental results on five languages show that our method consistently improves performance for four existing multilingual and multimodal models on both zero-shot and few-shot settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhi Li"
        },
        {
            "affiliations": [],
            "name": "Yin Zhang"
        }
    ],
    "id": "SP:2f5299d992732e785b030273ccf8792ebb346142",
    "references": [
        {
            "authors": [
                "Anurag Acharya",
                "Kartik Talamadupula",
                "Mark A Finlayson."
            ],
            "title": "Towards an atlas of cultural commonsense for machine reasoning",
            "venue": "arXiv preprint arXiv:2009.05664.",
            "year": 2020
        },
        {
            "authors": [
                "Arnav Arora",
                "Lucie-Aim\u00e9e Kaffee",
                "Isabelle Augenstein."
            ],
            "title": "Probing pre-trained language models for cross-cultural differences in values",
            "venue": "arXiv preprint arXiv:2203.13722.",
            "year": 2022
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Ryan Cotterell",
                "Naoaki Okazaki",
                "Desmond Elliott."
            ],
            "title": "Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language berts",
            "venue": "arXiv preprint arXiv:2011.15124.",
            "year": 2020
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Fangyu Liu",
                "Jonas Pfeiffer",
                "Siva Reddy",
                "Desmond Elliott",
                "Edoardo Maria Ponti",
                "Ivan Vulic."
            ],
            "title": "IGLUE: A benchmark for transfer learning across modalities, tasks, and languages",
            "venue": "CoRR, abs/2201.11732.",
            "year": 2022
        },
        {
            "authors": [
                "Yong Cao",
                "Li Zhou",
                "Seolhwa Lee",
                "Laura Cabello",
                "Min Chen",
                "Daniel Hershcovich."
            ],
            "title": "Assessing cross-cultural alignment between chatgpt and human societies: An empirical study",
            "venue": "arXiv preprint arXiv:2303.17466.",
            "year": 2023
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "UNITER: universal image-text representation learning",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, Au-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoshuai Hao",
                "Yi Zhu",
                "Srikar Appalaraju",
                "Aston Zhang",
                "Wanqian Zhang",
                "Bo Li",
                "Mu Li."
            ],
            "title": "Mixgen: A new multi-modal data augmentation",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 379\u2013389.",
            "year": 2023
        },
        {
            "authors": [
                "Shreya Havaldar",
                "Sunny Rai",
                "Bhumika Singhal",
                "Langchen Liu Sharath Chandra Guntuku",
                "Lyle Ungar."
            ],
            "title": "Multilingual language models are not multicultural: A case study in emotion",
            "venue": "arXiv preprint arXiv:2307.01370.",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415.",
            "year": 2016
        },
        {
            "authors": [
                "ders S\u00f8gaard"
            ],
            "title": "Challenges and strategies in cross-cultural nlp. ArXiv, abs/2203.10020",
            "year": 2022
        },
        {
            "authors": [
                "Aashi Jain",
                "Mandy Guo",
                "Krishna Srinivasan",
                "Ting Chen",
                "Sneha Kudugunta",
                "Chao Jia",
                "Yinfei Yang",
                "Jason Baldridge."
            ],
            "title": "MURAL: multimodal, multitask retrieval across languages",
            "venue": "CoRR, abs/2109.05125.",
            "year": 2021
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen-tse Huang",
                "Xing Wang",
                "Zhaopeng Tu."
            ],
            "title": "Is chatgpt a good translator? a preliminary study",
            "venue": "arXiv preprint arXiv:2301.08745.",
            "year": 2023
        },
        {
            "authors": [
                "Humair Raj Khan",
                "Deepak Gupta",
                "Asif Ekbal."
            ],
            "title": "Towards developing a multilingual and code-mixed visual question answering system by knowledge distillation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Austin C. Kozlowski",
                "Matt Taddy",
                "James A. Evans."
            ],
            "title": "The geometry of culture: Analyzing meaning through word embeddings",
            "venue": "CoRR, abs/1803.09288.",
            "year": 2018
        },
        {
            "authors": [
                "Dohyeon Lee",
                "Jaeseong Lee",
                "Gyewon Lee",
                "ByungGon Chun",
                "Seung-won Hwang."
            ],
            "title": "SCOPA: soft code-switching and pairwise alignment for zeroshot cross-lingual transfer",
            "venue": "CIKM \u201921: The 30th ACM International Conference on Information and",
            "year": 2021
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Ming Gong",
                "Daxin Jiang."
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11336\u201311344.",
            "year": 2020
        },
        {
            "authors": [
                "Fangyu Liu",
                "Emanuele Bugliarello",
                "Edoardo Maria Ponti",
                "Siva Reddy",
                "Nigel Collier",
                "Desmond Elliott."
            ],
            "title": "Visually grounded reasoning across languages and cultures",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Nan Duan"
            ],
            "title": "2021a. M3p: Learning universal rep",
            "year": 2021
        },
        {
            "authors": [
                "Nan Duan"
            ],
            "title": "2021b. M3P: learning universal repre",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Fraser"
            ],
            "title": "Adapting entities across",
            "year": 2021
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February",
            "year": 2017
        },
        {
            "authors": [
                "Alane Suhr",
                "Stephanie Zhou",
                "Ally Zhang",
                "Iris Zhang",
                "Huajun Bai",
                "Yoav Artzi."
            ],
            "title": "A corpus for reasoning about natural language grounded in photographs",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
            "year": 2019
        },
        {
            "authors": [
                "Jimin Sun",
                "Hwijeen Ahn",
                "Chan Young Park",
                "Yulia Tsvetkov",
                "David R Mortensen."
            ],
            "title": "Crosscultural similarity features for cross-lingual transfer learning of pragmatically motivated tasks",
            "venue": "arXiv preprint arXiv:2006.09336.",
            "year": 2020
        },
        {
            "authors": [
                "Samson Tan",
                "Shafiq R. Joty."
            ],
            "title": "Code-mixing on sesame street: Dawn of the adversarial polyglots",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Shashanka Venkataramanan",
                "Ewa Kijak",
                "Laurent Amsaleg",
                "Yannis Avrithis."
            ],
            "title": "Alignmixup: Improving representations by interpolating aligned features",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting palm for translation: Assessing strategies and performance",
            "venue": "arXiv preprint arXiv:2211.09102.",
            "year": 2022
        },
        {
            "authors": [
                "David Vilares",
                "Carlos G\u00f3mez-Rodr\u00edguez."
            ],
            "title": "Grounding the semantics of part-of-day nouns worldwide using Twitter",
            "venue": "Proceedings of the Second Workshop on Computational Modeling of People\u2019s Opinions, Personality, and Emotions in Social Me-",
            "year": 2018
        },
        {
            "authors": [
                "Devesh Walawalkar",
                "Zhiqiang Shen",
                "Zechun Liu",
                "Marios Savvides."
            ],
            "title": "Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification",
            "venue": "arXiv preprint arXiv:2003.13048.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Yang",
                "Bojie Hu",
                "Ambyera Han",
                "Shen Huang",
                "Qi Ju."
            ],
            "title": "CSP: code-switching pre-training for neural machine translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Yang",
                "Bojie Hu",
                "Ambyera Han",
                "Shen Huang",
                "Qi Ju."
            ],
            "title": "CSP:code-switching pre-training for neural machine translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2624\u20132636,",
            "year": 2020
        },
        {
            "authors": [
                "Da Yin",
                "Liunian Harold Li",
                "Ziniu Hu",
                "Nanyun Peng",
                "Kai-Wei Chang."
            ],
            "title": "Broaden the vision: Geodiverse visual commonsense reasoning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2115\u20132129,",
            "year": 2021
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo."
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "Proceedings of the IEEE/CVF international conference on com-",
            "year": 2019
        },
        {
            "authors": [
                "Yan Zeng",
                "Wangchunshu Zhou",
                "Ao Luo",
                "Xinsong Zhang."
            ],
            "title": "Cross-view language modeling: Towards unified cross-lingual cross-modal pre-training",
            "venue": "arXiv preprint arXiv:2206.00621.",
            "year": 2022
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "arXiv preprint arXiv:1710.09412.",
            "year": 2017
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Ruidan He",
                "Haiyun Peng",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Cross-lingual aspectbased sentiment analysis with aspect term codeswitching",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Mingyang Zhou",
                "Luowei Zhou",
                "Shuohang Wang",
                "Yu Cheng",
                "Linjie Li",
                "Zhou Yu",
                "Jingjing Liu."
            ],
            "title": "UC2: universal cross-lingual cross-modal vision-andlanguage pre-training",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Rohit Girdhar",
                "Armand Joulin",
                "Philipp Kr\u00e4henb\u00fchl",
                "Ishan Misra."
            ],
            "title": "Detecting twenty-thousand classes using image-level supervision",
            "venue": "European Conference on Computer Vision, pages 350\u2013368. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2021a) and (Bugliarello et al., 2022), we also add a special feature [IMG] that encodes the entire image, {[IMG",
            "year": 2022
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2021a), each language test dataset can be divided into multiple topics based on the categories of cultural concepts. For example, the topic \u201cSpeech and language",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The computer vision (CV) and natural language processing (NLP) communities have witnessed significant strides in multilingual and multimodal research in recent years. For instance, XLMs (Conneau and Lample, 2019) have substantially surpassed the previous benchmarks in cross-lingual tasks. The UC2 model (Zhou et al., 2021) learns to represent input tokens in a context-aware manner by leveraging both linguistic and visual content.\n\u2217Corresponding author: Yin Zhang.\nHowever, despite these advancements, the multicultural element is often neglected. The development of cultural adaptation methods is critical as they enhance model performance for low-resource languages and democratize the benefits of advanced technology. Hershcovich et al. (2022) underscores two major hurdles in cross-cultural NLP: cultural concepts and common sense. Our focus is primarily on the former: models trained on high-resource languages and images struggle to comprehend lowresource cultural concepts.\nA number of previous works (Vilares and G\u00f3mez-Rodr\u00edguez, 2018; Acharya et al., 2020; Yin et al., 2021; Liu et al., 2021a; Cao et al., 2023) have delved into cultural topics, largely focusing on cultural differences or evaluating the cross-cultural competency of computational models instead of enhancing them. The primary reason is the complexity of improving cross-cultural abilities, as lowresource languages and their cultural concepts are inherently scarce, exacerbating the data scarcity issue. Moreover, annotating cross-cultural data and establishing links between concepts across cultures is an expensive process given the limited number of annotators well-versed in various countries\u2019 cultures.\nTo overcome this challenge, we initially propose an annotation-free method for cultural-concept adaptation, which constructs a concept mapping set. An instance of cultural-concept adaptation involves the Chinese concept Erhu 1, which has no corresponding English translation. Explaining\nit to English-speaking individuals unfamiliar with Chinese culture would likely involve likening the Erhu to a Chinese violin. This is an example of cultural adaptation. Leveraging the relationships of hypernyms, hyponyms, and synonyms from publicly accessible semantic dictionaries, our method maps source cultural concepts to their corresponding target concepts, thereby eliminating the need for costly manual annotation.\nTo support the model\u2019s understanding of culturalconcept mappings, we subsequently introduce a novel cultural concept-based multimodal data augmentation technique. This technique features a three-tier code-switching strategy on textual sentences and a cultural concept-based mixup method for images 2. By training the model on both original and augmented data, we manage to significantly boost the model\u2019s performance on visually grounded reasoning tasks across languages and cultures. This improvement is reflected by a minimum increase of 2 points over existing multilingual multimodal models. Furthermore, our method can be adapted to improve specific languages or cultural topics by modifying the sampling distribution, thereby mitigating model bias.\nOur contributions are encapsulated as follows:\n\u2022 Leveraging web resources, we propose an annotation-free cultural adaptation method. By utilizing relationships of hypernyms, hyponyms, and synonyms from openly accessible semantic dictionaries, we construct a cultural adaptation graph that facilitates mapping between source and target cultural concepts.\n\u2022 To combat data scarcity and foster the model\u2019s understanding of cultural adaptation mappings, we introduce a novel cultural conceptbased multimodal data augmentation technique, generating new data instances at the concept, phrase, word, and image levels.\n\u2022 Key results for the task of visually grounded reasoning across languages and cultures reveal that our methods consistently and significantly outperform baseline measures. Additionally, our technique can be tailored to enhance specific languages or cultural topics by adjusting the sampling distribution, thus reducing model bias.\n1\u4e8c\u80e1 (Erhu) is a two-stringed bowed musical instrument, which may also be called a Southern Fiddle and is sometimes known in the Western world as the Chinese violin."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Cultural Research",
            "text": "Human language and visual content are intimately entwined with their respective cultures, evolving together, mirroring, and reciprocally influencing them. Culture is typically tied to a specific geographic region or locality, with distinct cultures characterizing different countries. The extant literature on culture tends to concentrate on three key aspects: examining cultural differences or similarities (Vilares and G\u00f3mez-Rodr\u00edguez, 2018; Acharya et al., 2020; Kozlowski et al., 2018; Sun et al., 2020), developing cross-cultural benchmarks (Peskov et al., 2021; Yin et al., 2021; Liu et al., 2021a), and evaluating the cross-cultural competence of computational models (Nguyen et al., 2022; Arora et al., 2022; Cao et al., 2023). For instance, Liu et al. (2021a) outline the multifaceted challenges involved in reasoning visually across languages and cultures, encompassing cross-modal, cross-lingual, and cross-cultural aspects. In contrast to the majority of prior research focusing predominantly on analysis and evaluation, our work confronts the issue directly by enhancing the model\u2019s adaptability to low-resource cultural concepts from both a visual and textual standpoint."
        },
        {
            "heading": "2.2 Code-switching and Mixup Methods",
            "text": "Code-switching is a common occurrence in multilingual communities, wherein the lexicon and morphemes of two or more languages are interchangeably utilized in oral or written communication. Training models using code-switched data encourages the alignment of source and target language representations by blending their contextual information. This approach has been used to challenge multilingual models (Tan and Joty, 2021), enhance Neural Machine Translation (NMT) tasks (Yang et al., 2020a; Liu et al., 2021b; Yang et al., 2020b), and further cross-lingual tasks (Qin et al., 2020; Zhang et al., 2021; Lee et al., 2021). In this study, we broaden the conventional perception of code-switching, transitioning it from a solely linguistic phenomenon to a cultural one.\nWhile code-switching operates on sentences, mixup methods are utilized in a variety of con-\n2Code-switching is a widespread phenomenon in multilingual communities, characterized by switching words and morphemes from two or more languages in speech or writing. The switched elements usually bear semantic similarity to the originals.\ntexts, such as mixup (Zhang et al., 2017), cutmix (Yun et al., 2019), attentive cutmix (Walawalkar et al., 2020), and alignmixup (Venkataramanan et al., 2022). Hao et al. (2023) introduced a joint data augmentation method, which generates new image-text pairs while maintaining semantic coherence through image interpolation and text concatenation. In contrast to these approaches, we substitute the target portion of the image with one that corresponds to a low-resource cultural concept."
        },
        {
            "heading": "2.3 Multilingual Multimodal Models",
            "text": "Most multimodal models based on transformer architecture (Ni et al., 2021b; Zhou et al., 2021; Jain et al., 2021; Liu et al., 2021a; Khan et al., 2021; Song et al., 2021; Zeng et al., 2022) are pre-trained using self-supervised objectives. While these methodologies advance the multilingual and multimodal capabilities of models, they often overlook cross-cultural aptitude. We propose a cultural concept adaptation approach to improve model performance across different cultures. By extending the code-switching mechanism to cultural concepts during fine-tuning, our method can help mitigate biases towards language and culture that may arise from imbalanced pre-training resource distribution, an issue that is challenging to address using selfsupervised pre-training objectives alone."
        },
        {
            "heading": "3 Method",
            "text": "To overcome data scarcity and costly labeling, we initially propose an annotation-free method for\ncultural-concept adaptation, which constructs a concept mapping set. To support the model\u2019s understanding of cultural-concept mappings, we subsequently introduce a novel cultural concept-based multimodal data augmentation technique. By training the model on both original and augmented data, we significantly boost the model\u2019s performance."
        },
        {
            "heading": "3.1 Cultural Adaptation",
            "text": "To illuminate the research methodology, we now formally define the central components:\nDefinition 1 (Cultural Adaptation Set) Consider a low-resource cultural concept x and a high-resource cultural concepts set Y = {y1, . . . , yn}. The cultural adaptation set of x : Yx = {yk, . . . , ym} is a subset of Y such that each y \u2208 Yx shares similar category attributes3 to x.\nProblem Statement Given x \u2208 X , where X is a set of cultural concepts, how can we identify the cultural adaptation set Yx?\nProposed Solution We denote Ho, S, and He as functions to query hyponyms, synonyms, and hypernyms respectively from publicly accessible semantic networks such as Conceptnet (Speer et al., 2017) and Wordnet (Miller, 1995). We construct a cultural adaptation graph Gx by applying a composite function F(x), which is defined as follows:\nF (x) = Ho(. . . (Ho(S(He(. . . (He(x))))))) (1) 3For example, the \u2019violin\u2019 and \u2019erhu\u2019 are both classified\nunder the category of orchestral instruments.\nDefinition 2 (Cultural Adaptation Graph) A cultural adaptation graph Gx = (V,E), where V denotes the set of vertices and E the set of edges, is a directed graph created by the iterative application of the Hypon, Syn, and Hyper functions starting from the cultural concept x. Each vertex vi \u2208 V represents a cultural concept linked to x through a path of hyponym, synonym, or hypernym relationships, while each directed edge eij \u2208 E depicts one of these relationships {Ho, S,He} between the cultural concepts vi and vj .\nGiven a cultural adaptation graph Gx = (V,E), each leaf node vi in Gx potentially represents a cultural adaptation of the cultural concept x. An example of this process is illustrated in Figure 1, showing the cultural adaptation of \"\u4e8c\u80e1(Erhu)\". The \u2019Erhu\u2019 has a hypernym \"\u5f13\u5f26\u4e50\u5668 4\", synonymous in English with \"Bowed string instrument\". This term has various hyponyms, including the \u2019violin\u2019 and \u2019cello\u2019, making them potential cultural adaptations of the \u2019Erhu\u2019. Similarly, \"\u4e50\u5668\", the hypernym of \"\u5f13\u5f26\u4e50\u5668\", and by extension a secondorder hypernym of \u2019Erhu\u2019, translates to \"Musical Instrument\" in English. Thus, other instruments like the \u2019saxophone\u2019 and \u2019oboe\u2019 may also serve as potential cultural adaptations of the \u2019Erhu\u2019. In fact, every leaf node in the cultural adaptation graph could potentially represent a cultural adaptation of \u2019Erhu\u2019, where shorter path distances indicate higher accuracy. For instance, \u2019violin\u2019 and \u2019cello\u2019 would provide more accurate cultural adaptations than \u2019saxophone\u2019 and \u2019drum\u2019. A simple iterative traversal algorithm can yield all leaf nodes and their respective path distances from \u2019Erhu\u2019. These\ncultural adaptation sets can serve as a kind of multicultural resource."
        },
        {
            "heading": "3.2 Culture Mixup",
            "text": "To mitigate the issue of data scarcity and prompt the model to comprehend the mapping of cultural adaptation, we propose a cultural concept-based multimodal data augmentation named CultureMixup. It includes a three-tier code-switching strategy on the textual sentences and a cultural concept-based mixup method on the image and thereby generates new data instances from the culture, phrase, word, and image levels. We initially gather data from online resources to construct a set X of cultural concepts, each paired with corresponding images. Refer to the appendix A for further details. Subsequently, we select a cultural concept x \u2208 X for cultural adaptation according to a predefined probability distribution. The sampling of an adaptation yk \u2208 Yx from the high-resource cultural adaptation set is inversely proportional to its path distance in the cultural adaptation graph Gx (as defined in Equation 1), i.e., P (yk|x) \u221d 1d(x,yk) . The adapted concept yk is then used to retrieve data instances from the training dataset. Each data instance comprises both textual sentences and images."
        },
        {
            "heading": "3.2.1 Three-tier Code-switching on Text",
            "text": "We apply a three-tier code-switching strategy to the textual sentences, as displayed in Figure 2.\n4\u5f13\u5f26\u4e50\u5668 is bowed string instruments in English, which are a subcategory of string instruments that are played by a bow rubbing the strings. The bow rubbing the string causes vibration which the instrument emits as sound.\nCultural-level We extend the conventional understanding of code-switching from a purely linguistic phenomenon to a cultural one. The culturally adapted concept yk in the sentence is replaced by the original low-resource cultural concept x. For instance, in Figure 2, the word \"violin\" is codeswitched with its Chinese cultural adaptation \"\u4e8c \u80e1\".\nPhrase-level For the phrase-level codeswitching, we collect and identify high-frequency phrases pertinent to reasoning and translate them into different languages. If a sentence contains these selected phrases, they are code-switched based on a given probability. For example, in Figure 2, the phrase \"on the left\" is code-switched by \"kushoto\" in Swahili.\nWord-level At the word level, each word in a sentence undergoes code-switching in a different language based on a specified probability. For example, in Figure 2, the words \"image\" and \"player\" are code-switched by \"resim\" in Turkish and \"peserta\" in Indonesian, respectively."
        },
        {
            "heading": "3.2.2 Concept-Based Mixup for Images",
            "text": "For the image-level cultural adaptation, we first apply an object detection algorithm to the images. We identify visual elements that correspond to highresource cultural concepts yk, and replace them with visual elements associated with the original low-resource cultural concept x. The specific process is as follows:\n1. Visual Element Detection: We use the highresource concept as the vocabulary term and deploy the object detection model to locate the corresponding visual elements in the image.\n2. Bounding Box Extraction: Based on the object detection results, we extract the object that needs to be replaced from the original high-resource image using bounding boxes.\n3. Resizing: We adjust the size of the lowresource image element, which is collected together with concepts themselves, to match the size of the bounding box.\n4. Pasting: The resized new low-resource image is then pasted into the corresponding location of the original image.\n5. Smoothing and Harmonization: Finally, smoothing and harmonization techniques are\napplied to seamlessly integrate the new object into the original image and make it more compatible with the background.\nThis introduces cultural diversity at the visual level, thereby enriching the data instances for training."
        },
        {
            "heading": "3.3 Reduce Model Bias",
            "text": "Pretrained multilingual multimodal models often demonstrate disparate performance across various languages and cultural contexts in test datasets, a discrepancy likely attributable to uneven resource distribution during pretraining. These imbalances pose challenges that are not readily addressed by self-supervised pretraining objectives. Our method offers a mechanism to ameliorate these language and cultural biases by manipulating the sampling distribution. Essentially, we can enhance model performance on specific language or cultural topics in a controllable manner. For instance, if the model is anticipated to be applied in a Turkish context, the sampling probability for Turkish can be increased during data augmentation. Likewise, if the model will be deployed in scenarios involving traditional Chinese musical instruments, like the Erhu, we can elevate the sampling probability of these specific Chinese musical concepts. In summary, our approach provides a statistically significant, fine-grained performance boost for the model over predefined language or cultural categories."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We collect and evaluate cultural concepts from diverse cultures (Indonesian, Swahili, Tamil, Turkish, Chinese) using crowd-sourced workers. The process involves gathering a wide range of cultural concepts manually and validating them through a majority vote. For a detailed methodology, please refer to the appendix. This approach, while costeffective, emphasizes accuracy by requiring a significant consensus among evaluators and prioritizing manual collection to capture cultural nuances. This dataset is publicly available 5. In evaluating the resulting cultural adaptation graphs, about 84% aligned with human judgment, confirming the method\u2019s effectiveness. For the less accurate graphs, issues were primarily due to translation limitations for certain cultural concepts\u2019 hypernyms.\n5https://github.com/zhilizju/Culture-mixup\nStrategies like restricting the use of higher-order hypernyms and implementing an exponential decay of sampling probability for concept inclusion were employed to enhance accuracy and authenticity, ensuring the graphs\u2019 overall quality and reliability. See the appendix B for details.\nWe use the Detic model (Zhou et al., 2022) available at Facebook Research\u2019s GitHub repository for object detection. It employs the CLIP (Radford et al., 2021) classifier and offers open-vocabulary capabilities. With this model, we can freely configure the vocabulary, allowing us to detect specific cultural concepts within images. If you want a more precise result, it is recommended to use the segment anything model(Kirillov et al., 2023), but it may require some manual clicking.\nFollowing Liu et al. (2021a), we employ NLVR2 in English for training and MaRVL for testing. MaRVL (Liu et al., 2021a) is a visually grounded reasoning dataset similar to NLVR2 (Suhr et al.,\n2019). It not only spans five typologically diverse languages (Chinese, Tamil, Swahili, Indonesian, and Turkish) but also adopts different basic-level concepts across different cultures. Thus, the challenges are multi-faceted, including cross-modal, cross-lingual, and cross-cultural aspects. In this task, given two images (Ileft and Iright) and a description D, the model needs to assess the validity of the description given the images, which can be cast as a classification problem. See the appendix C for sampled examples and detailed descriptions."
        },
        {
            "heading": "4.2 Baseline",
            "text": "We assess the efficacy of our approach using four existing multilingual multimodal pretrained models that have been released so far:\n\u2022 mUNITER (Liu et al., 2021a), which extends the UNITER architecture (Chen et al., 2020) to support multilingual functionality.\n\u2022 xUNITER (Liu et al., 2021a), which is identical to mUNITER, except for its initialization: while mUNITER is initialized from mBERT, xUNITER originates from XLM-R.\n\u2022 M 3P (Ni et al., 2021b), which introduces pretraining tasks using multimodal codeswitching.\n\u2022 UC 2 (Zhou et al., 2021), which uses masked region-to-token modeling and visual translation language modeling as pretraining tasks.\nTo provide a fair comparison with baselines (Liu et al., 2021a; Bugliarello et al., 2022), we adopt nearly identical experimental setups and hyperparameters except that we finetune models on the origin and augmented NLVR2 (Suhr et al., 2019) dataset. Despite augmenting the dataset, we maintain the same total number of training steps by reducing the training epochs. For more detailed information about settings and the implementation of the model, please refer to the appendix D. Our code is based on VOLTA (Bugliarello et al., 2020)."
        },
        {
            "heading": "4.3 Results",
            "text": "In this part, we mainly discuss four parts with experimental results. (1) What proportion is appropriate for the augmented data? (2) The zero-shot and few-shot performance of models with our proposed methods.(3) Ablation studies. (4) Controllability and reduce model bias."
        },
        {
            "heading": "4.3.1 Proportion",
            "text": "We conduct two groups of experiments on mUNITER in a zero-shot setting: one examines the impact of the proportion of augmented data, while the other investigates the effect of the total volume of augmented data on model performance while keeping the proportion constant. In Table 2, x : y implies that the total dataset is x + y times larger than the original English training data. Furthermore, xx+y of the total dataset is the original English training data (essentially replicating the original dataset x times), and yx+y is the codeswitched generated data. To maintain the total number of training steps, we reduce the training epochs and train the models for 20x+y epochs.\nFor the first group of experiments, we establish five different ratios x : y=1 : 1, 2 : 1, 3 : 1, 4 : 1, 5 : 1. Results in Table 2 suggest that as the volume of original data increases, the model\u2019s performance on the English test set consistently improves. However, performance in other languages and cultures initially increases then decreases. This reveals a trade-off: while a substantial proportion of English data aids the model\u2019s task comprehension, an appropriate amount of augmented data from other cultures facilitates the model\u2019s transfer ability. A ratio of roughly 3:1 yields optimal results. We further investigate this by holding the scale constant and incrementally increasing the English and augmented datasets.\nIn order to examine the influence of the total volume of augmented data on model performance\nwhile keeping the proportion constant, we establish three different ratios x : y=3 : 1, 9 : 3, and 15 : 5. As indicated in Table 2, the performance of the model improves with an increase in the volume of augmented data. Taking into account the results from these two sets of experiments, we decide to amplify the dataset to twenty times the original size and choose a ratio of x : y = 15 : 5 for our subsequent zero-shot and few-shot experiments. Although we do not contend that the ratio of x : y = 15 : 5 is optimal, we assert that this choice is adequate to demonstrate the effectiveness of our approach."
        },
        {
            "heading": "4.3.2 Zero-shot and Few-shot",
            "text": "As previously mentioned, we employ a configuration of x : y = 15 : 5 to augment the dataset to twenty times its original size. This involves replicating the original English NLVR2 dataset (Suhr et al., 2019) 15 times and generating a augmented dataset five times larger. Consequently, the final dataset is 20 times the size of the original English NLVR2 dataset. To maintain a consistent number of training steps with Liu et al. (2021a); Bugliarello et al. (2022), who trained models for 20 epochs, we train our models on this expanded dataset for a single epoch. We present both the best results and statistical mean for our method. \u2019Translatetest\u2019 refers to the setup in which the multilingual MaRVL datasets are translated into English. The results of Liu et al. (2021a); Bugliarello et al. (2022) are used directly as zero-shot benchmarks.\nTable 3 displays the zero-shot performance of the four models and demonstrates that our method consistently and statistically surpasses the baselines. Our approach considerably diminishes the disparity between the performance in the translation test and the transfer performance, validating the effectiveness of our code-switching method. We observe that, compared with the baselines, our\nmethod enhances the M3P and mUNITER scores by about 3 \u223c 4 points, while UC 2 and xUNITER gain only about 2 \u223c 3 points. This disparity might stem from the fact that UC 2 and xUNITER have acquired better-aligned representations during the pre-training phase.\nTable 4 displays the results of few-shot performance on three languages, demonstrating that our method also achieves the state-of-the-art performance in the few-shot setting. Nevertheless, similar to Liu et al. (2021a); Bugliarello et al. (2022), our results corroborate the finding that unlike textonly multilingual tasks, where even a handful of examples in the target language can substantially enhance model performance, this phenomenon is largely absent in multimodal multilingual settings (Bugliarello et al., 2022). As the number of shots increases, the model\u2019s performance remains largely unchanged or shows slight growth. We attribute this to two main factors. Firstly, the inherent complexity of the task, and secondly, within the same language, data samples embodying diverse cultural concepts may vary significantly. The model may overfit to data samples associated with specific cultural concepts, a phenomenon that warrants further investigation in future studies."
        },
        {
            "heading": "4.3.3 Ablation Study",
            "text": "To examine the impact of our cultural conceptbased multimodal data augmentation approach, we conducted an ablation study on mUNITER, maintaining the 15:5 setting for consistency. The results, as presented in Table 5, represent the statistical mean of various random seeds and underscore that each component of our method significantly contributes to enhancing the model\u2019s performance."
        },
        {
            "heading": "4.3.4 Reduce Model Bias",
            "text": "Our method can be utilized to mitigate model bias on specific target languages or cultural topics by\nadjusting the sampling distribution. For instance, most models exhibit relatively lower scores in Swahili compared to other languages, with UC 2 being particularly affected. To address this, we specifically focus on improving UC 2 \u2019s performance in Swahili. By sampling, we obtain 1k and 10k examples of Swahili-English code-switched data, which we then merge with the original English dataset to create a new training dataset. As Figure 3 illustrates, our method can significantly enhance the performance of UC 2 in Swahili. We also enhance the model\u2019s performance on the topic \"Speech and language\" in Chinese. For further details, please refer to appendix D."
        },
        {
            "heading": "5 Conclusion",
            "text": "To attack the difficulties of data annotation and scarcity, we propose an annotation-free cultural adaptation method and design a novel cultural concept-based multi-modal data augmentation to generate the new data example. By training the model on the augmented dataset, key results indicate that our methods consistently and statistically outperform the baselines. In the future, we plan to apply our method to more downstream tasks related to culture. Employing curriculum learning and designing more refined training strategies according to the difficulty of different languages and cultural concepts is also worth exploring. At the same time, how to further extend our method to make it more applicable to multi-modal models based on autore-\ngressive generation, such as GPT-4-V 6, is also highly worthwhile to explore.\nLimitations\nOur approach principally amplifies the conceptual adaptability of models to low-resource cultures. Nevertheless, cultural differences are complex and multidimensional, encompassing not only conceptual elements but also elements of common sense. The comprehensive acquisition of such common sense across diverse cultures is a vital yet challenging endeavor. Therefore, our community still has a considerable path to tread in order to fully enhance the multicultural competencies of AI models. Simultaneously, we only conducted experiments on multi-modal models based on masked language modeling. Further investigation is needed to determine the effectiveness on multi-modal models based on autoregressive generation."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by Zhejiang Provincial Natural Science Foundation of China under Grant No. LZ23F020009, the NSFC project (No. 62072399), MoE Engineering Research Center of Digital Library, China Research Centre on Data and Knowledge for Engineering Sciences and Technology, and the Fundamental Research Funds for the Central Universities. We would like to express our sincere gratitude to the anonymous reviewers for their invaluable feedback and constructive comments, which significantly contributed to the improvement of this paper."
        },
        {
            "heading": "A Collecting the Cultural Concepts",
            "text": "First, we need to collect the cultural concepts of different countries. In detail, we choose a diverse set of cultures and languages following: Indonesian, Swahili, Tamil, Turkish, Chinese. Each culture contains about ten chapters: festival, music, religion and belief, animal and plant, food, clothing, building, agriculture, tool, and sport. Each chapter contains several to dozens of cultural concepts. The collection of cultural concepts is primarily carried out manually. Importantly, this approach doesn\u2019t necessitate cross-cultural specialists; rather, it employs crowd-sourced workers familiar with the respective culture, making it a relatively costeffective and simple process. The procedure is twofold:\n1. Diverse Candidate Collection: For each culture under consideration, we involve five crowd-sourced workers and require them to explore a minimum of three types of online resources. (1) Wikipedia. For example, the article \"Culture of Turkey\" on Wikipedia lists many cultural concepts including foods, festivals, architecture, and so on. (2) Official websites. Most countries provide official websites to introduce their culture. (3) Search engines. Some websites retrieved by search engines such as travel guides will introduce the local culture. They collect as many cultural concepts as possible for each category. The collected data from each worker is then aggregated.\n2. Voting for Filtering: Next, an additional 10 crowd-sourced workers from the respective country or region assess whether each gathered concept genuinely belongs to the local culture. If seven or more evaluators agree, the concept is added to the final \u2019culture concept set.\u2019\nThis method ensures quality in several ways:\n\u2022 Inclusion of Multiple Evaluators: By involving multiple people from the respective culture in both the collection and the validation process, we minimize individual bias and enhance the dataset\u2019s reliability.\n\u2022 Threshold for Inclusion: The use of a voting system provides a safety net against inaccuracies and biases. If a concept is included, it\u2019s\nbecause a significant majority (at least 7 out of 10) of the evaluators from that culture have vouched for its relevance.\n\u2022 Manual Over Automatic: While automated methods may miss nuances or make errors, manual collection engages those who understand the cultural intricacies best\u2014the people from that culture.\nHence, our approach offers a robust, yet economical way of collecting high-quality cultural concepts."
        },
        {
            "heading": "B Human Evaluation of Cultural Adaptation Graphs",
            "text": "The evaluation of the generated cultural adaptation graphs yielded quite reassuring results:\n\u2022 High Human Agreement: Approximately 84% of the generated cultural adaptation graphs were in alignment with human judgment. This high rate of agreement underscores the validity of our approach in capturing culturally meaningful concepts.\n\u2022 Addressing Inaccuracies: For the remaining graphs that were less accurate, the primary issue often lay in the lack of direct English translations for the low-resource cultural concepts\u2019 hypernyms. To address this, the method would need to consider higher-order hypernyms, making the entire path too distant and potentially distorting cultural similarity according to human evaluators.\nTo counteract these issues, we implemented the following strategies:\n\u2022 Hypernym Limitation: As described in the paper, we restrict the method to considering at most three-order hypernyms during the construction of the cultural adaptation graph. If a suitable translation or hypernym cannot be identified within this constraint, the concept is discarded.\n\u2022 Exponential Decay of Sampling Probability: The paper also outlines that the sampling probability for including concepts in the cultural adaptation set decays exponentially with path distance. This mechanism serves to mitigate possible inaccuracies by giving greater weight to more closely related concepts.\nThese measures are designed to ensure that the cultural adaptation graph generated is of high quality, both in terms of capturing authentic cultural elements and in conforming to human judgment. Therefore, while the graph is not perfect, it is constructed with numerous safeguards to ensure its utility and accuracy."
        },
        {
            "heading": "C NLVR2 and MaRVL Dataset",
            "text": "Liu et al. (2021a) points that most of the synsets employed by NLVR2 (Suhr et al., 2019) and ImageNet (Deng et al., 2009) are only present in 30 or fewer languages and they contain overly specific concepts that belong to leaf nodes in WordNet. Given the biases in ImageNet-derived or inspired datasets, they define a protocol to collect data that is driven by native speakers of a language, consisting of concepts arising from their lived experiences. As a consequence, the descriptions are written in five languages: Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish, and the concepts are selected to be culturally relevant. Both multilingual and monolingual models perform comparably well in English (NLVR2). When these models are evaluated on the languages in MaRVL, however, the performance of zero-shot multilingual baselines dramatically drops a lot, floating just above the chance level. Further analysis shows that there are two sources of difficulty that make MaRVL challenging: 1) crosscultural transfer (out-of-distribution concepts with respect to English datasets) and 2) cross-lingual transfer."
        },
        {
            "heading": "D The Implementation in Details",
            "text": "D.1 Experimental Setting\nWe provide a detailed experimental setting. First, we collate and merge data from three web resources and build an initial dataset containing cultural concepts of different countries. Refer to appendix A for detail. For cultural adaptation, \u201d/r/IsA\u201d in Conceptnet can be used to query hypernyms in target languages, and \u201d/r/Synonym\u201d is employed to query synonyms in English. And we use the hyponyms() function of Wordnet to query hyponyms of English concepts. We only consider three-order hypernym at most. And the sampling probability decays exponentially with path distance. For phrase level, we count n-gram phrases. n range from 2 to 5. We select phrases related to reasoning and translate them to other languages via Google Translate. For word level, we employ the \u201d/r/Synonym\u201d in Conceptnet.\nD.2 Model Implementation\nMost multilingual multimodal pre-trained models architecture consists of a stack of Transformer layers are similar to BERT. Their inputs are the concatenation of language and vision embeddings. The language inputs are tokenized and surrounded by two special tokens {[CLS], t1, ..., tn, [SEP ]}. The language embeddings are then obtained by index token ids just like the original BERT. The vision input consists of a set of visual features produced by a well-trained object detector. Following (Liu et al., 2021a) and (Bugliarello et al., 2022), we also add a special feature [IMG] that encodes the entire image, {[IMG], v1, ..., vm}. Each feature is embedded using a BERT-like embedding layer by using its bounding box coordinates as the input position. Then these language and vision embeddings will be fed into a BERT-like encoder and get hl[IMG], h l [CLS], h r [IMG], h r [CLS] representations at the last layer. We follow Liu et al. (2021a) and employ the cross-entropy loss function. We apply a two-layer MLP with a GeLU activation function (Hendrycks and Gimpel, 2016) on top of the image\u2013text representation. The probability that they are both correct is predicted by a softmax over two classes (representing true and false labels):\nP(C | Il, Ir, D) = softmax ( MLP ([ hl[IMG] \u2299 h l [CLS]\nhr[IMG] \u2299 h r [CLS] ])) (2)"
        },
        {
            "heading": "E Error Case Analysis",
            "text": "E.1 Performance Across Languages As demonstrated in Table3 of the paper, performance diverges significantly across languages. This discrepancy is due largely to the imbalanced distribution of pre-training data, linguistic idiosyncrasies, and cultural distinctions. Furthermore, the ease of language transfer from English to other languages varies considerably.\nE.2 Within-Language Category Differences An intra-language examination reveals that performance fluctuates across different categories. For instance, the model demonstrates heightened efficacy in the \"animal and plant\" and \"building\" categories relative to others. Section 4.3.4 elaborates on this phenomenon, attributing it primarily to the distribution of pre-training data.\nE.3 Instance-Level Analysis We sampled a subset of the test data, and when we conducted a detailed error investigation at the individual instance level, we discovered several illuminating patterns:\n\u2022 Poor Spatial Awareness: The model predominantly falters in spatial reasoning tasks. Notably, descriptions formatted as \"one image..., another image...\" are more challenging for the model than those specifying \"left image...,\nright image...\", with performance improving statistically across languages in the latter case (58.4% vs 53.8%).\n\u2022 Limited Textual Reasoning: Performance enhances when the description is condensed into a single sentence (\"in both images...\") rather than dispersed across two (\"one image..., another image...\"), with a performance leap to 60.4%.\n\u2022 False Bias: The models exhibit a pronounced propensity to predict \u2019false\u2019 in the test set, constituting 57% of the model\u2019s predictions, contrasted with 43% of \u2019true\u2019 predictions. This imbalance, especially conspicuous given the actual 50-50 distribution of \u2019true\u2019 and \u2019false\u2019 labels in the training and test sets, stems predominantly from the model\u2019s unfamiliarity with certain cultural concepts and images, prompting a \u2019false\u2019 prediction.\nThese analyses collectively provide a comprehensive insight into the model\u2019s limitations, illuminating potential avenues for refinement."
        },
        {
            "heading": "F Reduce Model Bias",
            "text": "According to Liu et al. (2021a), each language test dataset can be divided into multiple topics based on the categories of cultural concepts. For example, the topic \u201cSpeech and language\u201d of the Chinese test dataset in MARVL contains many traditional Chinese musical instruments, such as Erhu. Similar to adjusting the sampling probability of language, we can generate Chinese-English code-switched by adding the sampling probability of assigned cultural concepts and phrases of Chinese. Assuming models need to improve their reasoning ability on datasets about traditional Chinese musical instruments, all we need is to improve the sampling probability of concepts about this topic in our collected cultural concepts set. Figure 5 shows that as we add more related data, the model performance is becoming better on corresponding topics."
        },
        {
            "heading": "G LLM for Cultural Concept Adaptation",
            "text": "This section discusses the potential of using large language models (LLMs) like ChatGPT for cultural concept adaptation. Despite their proficiency in explaining various cultural concepts, several factors discourage their exclusive use as a baseline or expert system.\n\u2022 Language Resource Limitations: Prior studies (Jiao et al., 2023; Vilar et al., 2022; Cao et al., 2023; Havaldar et al., 2023) and our case studies on GPT-3.5\u2019s initial version indicate subpar performance in certain low-resource languages, such as Swahili (sw), despite proficiency in languages like Chinese (zh).\n\u2022 Cost-Effectiveness: Dictionary lookups are generally more efficient and economical than operating large language models.\n\u2022 Reproducibility & Hallucinations: Inherent weaknesses in LLMs include instability based on prompts and difficulty ensuring reproducibility. Verifying outputs is particularly challenging when hallucinations occur.\n\u2022 Dataset Compatibility: Our downstream dataset, NLVR2, revolves around a WordNet subset, facilitating queries. Relying on ChatGPT would necessitate additional output processing.\n\u2022 Leveraging Existing Dictionary Resources: Our method can fully exploit existing dictionary resources for extremely low-resource languages, like Navajo, through platforms such as ConceptNet.\nG.1 Evaluating Recent Models Recently, we evaluated the latest GPT-3.5 and GPT4 models for cultural adaptation capabilities. Using a chain of prompt approach to construct a cultural adaptation graph yielded promising results, particularly with GPT-4. Nonetheless, these models also encounter challenges, such as potential hallucinations, and necessitate meticulous prompt design for diverse cultural adaptations. We believe that the integration of dictionary-based cultural adaptation methods with large language models represents a promising hybrid approach. For instance, a dictionary-based method could produce a broad set of cultural adaptations, subsequently filtered for relevance by a large language model. Incorporating LLMs to devise our cultural adaptation graph is indeed promising and under consideration for future exploration."
        }
    ],
    "title": "Cultural Concept Adaptation on Multimodal Reasoning",
    "year": 2023
}