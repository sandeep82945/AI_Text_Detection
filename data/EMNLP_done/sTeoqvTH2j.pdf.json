{
    "abstractText": "In this paper, we propose a hierarchical contrastive learning framework, HiCL, which considers local segment-level and global sequence-level relationships to improve training efficiency and effectiveness. Traditional methods typically encode a sequence in its entirety for contrast with others, often neglecting local representation learning, leading to challenges in generalizing to shorter texts. Conversely, HiCL improves its effectiveness by dividing the sequence into several segments and employing both local and global contrastive learning to model segment-level and sequencelevel relationships. Further, considering the quadratic time complexity of transformers over input tokens, HiCL boosts training efficiency by first encoding short segments and then aggregating them to obtain the sequence representation. Extensive experiments show that HiCL enhances the prior top-performing SNCSE model across seven extensively evaluated STS tasks, with an average increase of +0.2% observed on BERTlarge and +0.44% on RoBERTalarge. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhuofeng Wu"
        },
        {
            "affiliations": [],
            "name": "Chaowei Xiao"
        },
        {
            "affiliations": [],
            "name": "VG Vinod Vydiswaran"
        }
    ],
    "id": "SP:3e6700a557b92ad4e7241dc9b24785c2f016d2c7",
    "references": [
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2014 task 10: Multilingual semantic textual similarity",
            "venue": "Proceedings of the 8th Interna-",
            "year": 2014
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
            "venue": "Proceedings of the",
            "year": 2016
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "SemEval-2012 task 6: A pilot on semantic textual similarity",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the",
            "year": 2012
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor GonzalezAgirre",
                "Weiwei Guo."
            ],
            "title": "SEM 2013 shared task: Semantic textual similarity",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Confer-",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Kevin Swersky",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "Big selfsupervised models are strong semi-supervised learners",
            "venue": "arXiv preprint arXiv:2006.10029.",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He."
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297.",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He."
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "2021 IEEE/CVF",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He."
            ],
            "title": "An empirical study of training self-supervised visual transformers",
            "venue": "arXiv preprint arXiv:2104.02057.",
            "year": 2021
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Rumen Dangovski",
                "Hongyin Luo",
                "Yang Zhang",
                "Shiyu Chang",
                "Marin Soljacic",
                "ShangWen Li",
                "Scott Yih",
                "Yoon Kim",
                "James Glass."
            ],
            "title": "DiffCSE: Difference-based contrastive learning for sentence embeddings",
            "venue": "Proceedings of the 2022",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "Senteval: An evaluation toolkit for universal sentence representations",
            "venue": "arXiv preprint arXiv:1803.05449.",
            "year": 2018
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Hongchao Fang",
                "Sicheng Wang",
                "Meng Zhou",
                "Jiayuan Ding",
                "Pengtao Xie."
            ],
            "title": "Cert: Contrastive self-supervised learning for language understanding",
            "venue": "arXiv preprint arXiv:2005.12766.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "John Giorgi",
                "Osvald Nitski",
                "Bo Wang",
                "Gary Bader."
            ],
            "title": "DeCLUTR: Deep contrastive learning for unsupervised textual representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Valko."
            ],
            "title": "Bootstrap your own latent - a new approach to self-supervised learning",
            "venue": "Advances in",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick."
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738.",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Norman Mu",
                "Ekin D. Cubuk",
                "Barret Zoph",
                "Justin Gilmer",
                "Balaji Lakshminarayanan."
            ],
            "title": "AugMix: A simple data processing method to improve robustness and uncertainty",
            "venue": "Proceedings of the International Conference on Learning Represen-",
            "year": 2020
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.",
            "year": 2004
        },
        {
            "authors": [
                "Ting Jiang",
                "Shaohan Huang",
                "Zihan Zhang",
                "Deqing Wang",
                "Fuzhen Zhuang",
                "Furu Wei",
                "Haizhen Huang",
                "Liangjie Zhang",
                "Qi Zhang."
            ],
            "title": "Promptbert: Improving bert sentence embeddings with prompts",
            "venue": "arXiv preprint arXiv:2201.04337.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Hierarchical transformers for multi-document summarization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070\u2013 5081, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli."
            ],
            "title": "A SICK cure for the evaluation of compositional distributional semantic models",
            "venue": "Proceedings of the Ninth International Conference",
            "year": 2014
        },
        {
            "authors": [
                "Piotr Nawrot",
                "Szymon Tworkowski",
                "Micha\u0142 Tyrolski",
                "Lukasz Kaiser",
                "Yuhuai Wu",
                "Christian Szegedy",
                "Henryk Michalewski."
            ],
            "title": "Hierarchical transformers are more efficient language models",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278,",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann",
            "year": 2005
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Robinson",
                "Ching-Yao Chuang",
                "Suvrit Sra",
                "Stefanie Jegelka."
            ],
            "title": "Contrastive learning with hard negative samples",
            "venue": "arXiv preprint arXiv:2010.04592.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research, 15(56):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Ellen M Voorhees",
                "Dawn M Tice."
            ],
            "title": "Building a question answering test collection",
            "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207.",
            "year": 2000
        },
        {
            "authors": [
                "Hao Wang",
                "Yangguang Li",
                "Zhen Huang",
                "Yong Dou",
                "Lingpeng Kong",
                "Jing Shao."
            ],
            "title": "Sncse: Contrastive learning for unsupervised sentence embedding with soft negative samples",
            "venue": "arXiv preprint arXiv:2201.05979.",
            "year": 2022
        },
        {
            "authors": [
                "Janyce Wiebe",
                "Theresa Wilson",
                "Claire Cardie."
            ],
            "title": "Annotating expressions of opinions and emotions in language",
            "venue": "Language resources and evaluation, 39(2):165\u2013210.",
            "year": 2005
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Zijia Lin",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "Infocse: Information-aggregated contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2210.06432.",
            "year": 2022
        },
        {
            "authors": [
                "Xing Wu",
                "Chaochen Gao",
                "Liangjun Zang",
                "Jizhong Han",
                "Zhongyuan Wang",
                "Songlin Hu."
            ],
            "title": "ESimCSE: Enhanced sample building method for contrastive learning of unsupervised sentence embedding",
            "venue": "Proceedings of the 29th International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X Yu",
                "Dahua Lin."
            ],
            "title": "Unsupervised feature learning via nonparametric instance discrimination",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3733\u20133742.",
            "year": 2018
        },
        {
            "authors": [
                "Zhuofeng Wu",
                "Sinong Wang",
                "Jiatao Gu",
                "Rui Hou",
                "Yuxiao Dong",
                "V.G.Vinod Vydiswaran",
                "Hao Ma."
            ],
            "title": "IDPG: An instance-dependent prompt generation method",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Zhuofeng Wu",
                "Sinong Wang",
                "Jiatao Gu",
                "Madian Khabsa",
                "Fei Sun",
                "Hao Ma."
            ],
            "title": "Clear: Contrastive learning for sentence representation",
            "venue": "arXiv preprint arXiv:2012.15466.",
            "year": 2020
        },
        {
            "authors": [
                "Liu Yang",
                "Mingyang Zhang",
                "Cheng Li",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical encoder for long-form document matching",
            "venue": "Proceedings of the 29th ACM International Conference",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Kun Zhou",
                "Beichen Zhang",
                "Xin Zhao",
                "Ji-Rong Wen."
            ],
            "title": "Debiased contrastive learning of unsupervised sentence representations",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6120\u2013",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Current machine learning systems benefit greatly from large amounts of labeled data. However, obtaining such labeled data is expensive through annotation in supervised learning. To address this issue, self-supervised learning, where supervisory labels are defined from the data itself, has been proposed. Among them, contrastive learning (Chen et al., 2020a,b,c, 2021; He et al., 2020; Grill et al., 2020; Chen and He, 2021) has become one of the most popular self-supervised learning methods due to its impressive performance across various domains. The training target of contrastive learning is to learn a representation of the data that maximizes the similarity between positive examples and minimizes the similarity between negative examples.\n1Our code will be released at https://github.com/ CSerxy/HiCL.\nTo achieve better performance, existing methods mainly focus on designing better positive examples (Hendrycks et al., 2020; Fang et al., 2020; Wu et al., 2020; Giorgi et al., 2021; Gao et al., 2021b) or investigating the role of the negative examples (Robinson et al., 2020; Zhou et al., 2022; Wang et al., 2022).\nDespite the success, existing methods augment data at the level of the full sequence (Gao et al., 2021b; Wang et al., 2022). Such methods require calculating the entire sequence representation, leading to a high computational cost. Additionally, it also makes the task of distinguishing positive examples from negative ones too easy, which doesn\u2019t lead to learning meaningful representations. Similarly, methods like CLEAR (Wu et al., 2020) demonstrated that pre-training with sequence-level na\u00efve augmentation can cause the model to converge too quickly, resulting in poor generalization.\nIn contrast, Zhang et al. (2019) considered modeling in-sequence (or local) relationships for language understanding. They divide the sequence into smaller segments to learn intrinsic and underlying relationships within the sequence. Since this method is effective in modeling long sequences by not truncating the input and avoiding loss of information, it achieves promising results. Given this success, a natural question arises: is it possible to design an effective and efficient contrastive learning framework by considering the local segmentlevel and global sequence-level relationships?\nTo answer the question, in this paper, we propose a hierarchical contrastive learning framework, HiCL, which not only considers global relationships but also values local relationships, as illustrated in Figure 1. Specifically, given a sequence (i.e., sentence), HiCL first divides it into smaller segments and encodes each segment to calculate local segment representation respectively. It then aggregates the local segment representations belonging to the same sequence to get the global\nsequence representation. Having obtained local and global representations, HiCL deploys a hierarchical contrastive learning strategy involving both segment-level and sequence-level contrastive learning to derive an enhanced representation. For local contrastive learning, each segment is fed into the model twice to form the positive pair, with segments from differing sequences serving as the negative examples. For global contrastive learning, HiCL aligns with mainstream baselines to construct positive/negative pairs.\nWe have carried out extensive experiments on seven STS tasks using well-representative models BERT and RoBERTa as our backbones. We assess the method\u2019s generalization capability against three baselines: SimCSE, ESimCSE, and SNCSE. As a result, we improve the current state-of-the-art model SNCSE over seven STS tasks and achieve new state-of-the-art results. Multiple initializations and varied training corpora confirmed the robustness of our HiCL method.\nOur contributions are summarized below:\n\u2022 To the best of our knowledge, we are the first to explore the relationship between local and global representation for contrastive learning in NLP. \u2022 We theoretically demonstrate that the encoding efficiency of our proposed method is much faster than prior contrastive training paradigms. \u2022 We empirically verify that the proposed training paradigm enhances the performance of current state-of-the-art methods for sentence embeddings on seven STS tasks."
        },
        {
            "heading": "2 Preliminaries: Contrastive Learning",
            "text": "In this paper, we primarily follow SimCLR\u2019s framework (Chen et al., 2020a) as our basic contrastive framework and describe it below. The general training objective of contrastive learning (Oord et al., 2018) is to distinguish similar pairs from dissimilar pairs, where similar pairs are constructed using pre-defined data augmentation techniques and dissimilar pairs are other examples in the same batch. Specifically, for an arbitrary example xi in a batch B, the InfoNCE loss Lg brings the representation hi closer to positive instance representation h+i and away from negative ones h\u2212j\u2208B\\i. If hi, h + i , h \u2212 j\u2208B\\i are the representation vectors from the encoder, \u03c4 is a temperature scale factor (set to 0.05 following SimCSE), and sim(u, v) = uT v/\u2225u\u2225\u2225v\u2225 denotes the cosine similarity, Lg is computed as:\nLg = \u2212 log esim(hi,h\n+ i )/\u03c4\nesim(hi,h + i )/\u03c4 + \u2211 j\u2208B\\i esim(hi,hj)/\u03c4 ,\n(1) Benefiting from human-defined data augmentations, it can generate numerous positive and negative examples for training without the need for explicit supervision, which is arguably the key reason why self-supervised learning can be effective.\nPositive instance Designing effective data augmentations to generate positive examples is a key challenge in contrastive learning. Various methods such as back-translation, span sampling, word deletion, reordering, and synonym substitution have been explored for language understanding tasks in prior works such as CERT (Fang et al., 2020), DeCLUTR (Giorgi et al., 2021), and CLEAR (Wu et al., 2020). Different from previous approaches that augment data at the discrete text level, SimCSE (Gao et al., 2021b) first applied dropout (Srivastava et al., 2014) twice to obtain two intermediate representations for a positive pair. Specifically, given a Transformers model, E\u03b8 (parameterized by \u03b8), (Vaswani et al., 2017) and a training instance xi, hi = E\u03b8,p(xi) and h+i = E\u03b8,p+(xi) are the positive pair that can be used in Eq. 1, where p and p+ are different dropout masks. This method has been shown to significantly improve sentence embedding performance on seven STS tasks, making it a standard comparison method in this field.\nNegative instance Negative instance selection is another important aspect of contrastive learning. SimCLR simply uses all other examples in the same batch as negatives. However, in DCLR (Zhou et al., 2022), around half of in-batch negatives were similar to SimCSE\u2019s training corpus (with a cosine similarity above 0.7). To address this issue, SNCSE (Wang et al., 2022) introduced the use of negated sentences as \u201csoft\u201d negative samples (e.g., by adding not to the original sentence). Additionally, instead of using the [CLS] token\u2019s vector representation, SNCSE incorporates a manually designed prompt: \u201cThe sentence of xi means [MASK]\u201d and takes the [MASK] token\u2019s vector to represent the full sentence xi. This approach has been shown to improve performance compared to using the [CLS] token\u2019s vector. In this paper, we compare against SNCSE as another key baseline, not only because it is the current state-of-the-art\non evaluation tasks, but also because it effectively combines contrastive learning with techniques like prompt tuning (Gao et al., 2021a; Wu et al., 2022c).\nMomentum Contrast The momentum contrast framework differs from SimCLR by expanding the negative pool through the inclusion of recent instances, effectively increasing the batch size without causing out-of-memory issues. ESimCSE (Wu et al., 2022b) proposes a repetition operation to generate positive instances and utilizes momentum contrast to update the model. We include it as a baseline for comparison to assess the ability of our model to adapt to momentum contrast."
        },
        {
            "heading": "3 Hierarchical Contrastive Learning",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "Figure 1 shows an overview of HiCL. Our primary goal is to incorporate additional underlying (local) information in traditional, unsupervised text contrastive learning. Two objectives are combined to achieve this goal.\nGiven a set of sequences {seq1, seq2, . . . , seqn} in a batch B, we slice each seqi into segments {segi,1, segi,2, . . . , segi,li} of slicing length L, where n is the batch size, and li = 1 + \u230a(|seqi| \u2212 1)/L\u230b is the number of\nsegments that can be sliced in seqi. The slicing is performed using a queue rule: every consecutive L tokens (with no overlap) group as one segment, and the remaining tokens with length no greater than L form a separate segment. In other words, |segi,j | = L,\u2200j \u2208 [1, li); |segi,li | \u2208 [1, L]; and seqi = concat[segi,1, . . . , segi,li ].\nUnlike traditional contrastive learning, which encodes the input sequence seqi directly, we encode each sub-sequence segi,j using the same encoder and obtain its representation: hi,j = E\u03b8(segi,j), where E\u03b8 is a Transformer (Vaswani et al., 2017), parameterized by \u03b8. We aggregate the hi,j representations to obtain the whole sequence representation hi by weighted average pooling, where the weight of each segment segi,j is proportional to its length |segi,j |: hi = \u2211 j hi,j \u00d7 wi,j , where wi,j = |segi,j |\u2211 k |segi,k|\n. In Section 5.1, we explore other pooling methods, such as unweighted average pooling, and find that weighted pooling is the most effective. According to Table 2, most (99.696%) input instances can be divided into three or fewer segments. Therefore, we do not add an extra transformer layer to get the sequence representation from these segments, as they are relatively short.\nTo use HiCL with SNCSE, we slice the input se-\nquence in the same way, but add the prompt to each segment instead of the entire sequence. We also apply the same method to the negated sentences."
        },
        {
            "heading": "3.2 Training Objectives",
            "text": "Local contrastive Previous studies have highlighted the benefits of local contrastive learning for unsupervised models (Wu et al., 2020; Giorgi et al., 2021). By enabling the model to focus on short sentences, local contrastive learning allows the model to better match the sentence length distribution, as longer sentences are less common. Building on the work of Gao et al. (2021b), we use dropout as a minimum data augmentation technique. We feed each segment segi,j twice to the encoder using different dropout masks p and p+. This results in positive pairs hi,j = E\u03b8,p(segi,j) and h + i,j = E\u03b8,p+(segi,j) for loss computation. As mentioned in Section 1, defining negatives for segments can be challenging. Using segments from the same sequence as negatives carries the risk of introducing correlations, but treating them as positive pairs is not ideal either. We chose not to use segments from the same sequence as either positive or negative pairs and we will show that this approach is better than the other alternatives in Section 5.2. Hence, for segment segi,j , we only consider as negatives, segments from other sequences {segk,\u2217, k \u2208 {B \\ i}}. The local contrastive Ll is formalized as:\nLl = \u2212 log esim(hi,j ,h\n+ i,j)/\u03c4\nesim(hi,j ,h + i,j)/\u03c4 + \u2211 k \u0338=i esim(hi,j ,hk,\u2217)/\u03c4\n(2)\nGlobal contrastive The global contrastive objective is the same as that used by most baselines, which tries to pull a sequence\u2019s representation hi closer to its positive h+i while keeping it away from in-batch negatives h\u2212j\u2208B\\i, as defined by the global contrastive loss Lg in Eq. 1.\nOverall objective The overall objective is a combination of local and global contrastive loss, L = \u03b1Ll + (1 \u2212 \u03b1)Lg, where weight \u03b1 \u2208 {0.01, 0.05, 0.15} is tuned for backbone models. Our adoption of a hybrid loss, with a lower weighting assigned to the local contrastive objective, is motivated by the potential influence of the hard truncation process applied to the sequences. This process can result in information loss and atypical sentence beginnings that may undermine the effectiveness of the local contrastive loss. Meanwhile, a\nstandalone global contrastive loss is equally inadequate, as it omits local observation. We conduct an analysis in Section 5.4 to discuss the intricate relationship between two objectives."
        },
        {
            "heading": "3.3 Encoding Time Complexity Analysis",
            "text": "According to our slicing rule, all front segments segi,j<li in sequence seqi have length L and the last segment segi,li has length |segi,li | \u2208 [1, L]. Hence, the encoding time complexity for HiCL is O(L2\u00d7 (li\u2212 1)+ |segli |\n2), while the conventional methods take:\nO(|seqi|2) = O((L\u00d7 (li \u2212 1) + |segli |) 2)\n> (li \u2212 1)\u00d7O(L2(li \u2212 1) + |segli | 2)\nwhich is (li\u22121) times more than that for HiCL. The longer the training corpus, the higher the benefit using HiCL. This suggests that our approach has a variety of use cases, particularly in pre-training, due to its efficient encoding process.\nThe practical training time is detailed in Appendix A.1. In short, we are faster than baselines when maintaining the same sequence truncation size 512. For example, SimCSE-RoBERTalarge takes 354.5 minutes for training, while our method only costs 152 minutes."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Evaluation tasks Following prior works (Gao et al., 2021b; Wang et al., 2022), we mainly evaluate on seven standard semantic textual similarity datasets: STS12\u201316 (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer et al., 2017), and SICK-Relatedness (Marelli et al., 2014). When evaluating using the SentEval tookit2, we adopt SimCSE\u2019s evaluation setting without any additional regressor and use Spearman\u2019s correlation as the evaluation matrix. Each method calculates the similarity, ranging from 0 to 5, between sentence pairs in the datasets. In our ablation study, we extend our evaluation to encompass two lengthy datasets (i.e., Yelp (Zhang et al., 2015) and IMDB (Maas et al., 2011)) and seven transfer tasks (Conneau and Kiela, 2018). Due to space consideration, we have provided detailed results for the seven transfer learning tasks in Appendix A.2.\n2https://github.com/facebookresearch/SentEval\nImplementing competing methods We re-train three previous state-of-the-art unsupervised sentence embedding methods on STS tasks \u2013 SimCSE (Gao et al., 2021b), ESimCSE (Wu et al., 2022b), and SNCSE (Wang et al., 2022) \u2013 with our novel training paradigm, HiCL. We employ the official implementations of these models and adhere to the unsupervised setting utilized by SimCSE. This involves training the models on a dataset comprising 1 million English Wikipedia sentences for a single epoch. More detailed information can be found in Appendix A.3. We also carefully tune the weight \u03b1 of local contrastive along with the learning rate in Appendix A.4."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Table 1 presents the results of adding HiCL to various baselines on the seven STS tasks. We observe that (i) HiCL consistently improves the performance of baselines in the large model setting. Specifically, HiCL improves the average scores by +0.25%, +1.36%, and +0.44% on the RoBERTalarge variants of SimCSE, ESimCSE,\nand SNCSE baselines, respectively. (ii) HiCL with SNCSE-RoBERTalarge as the backbone has achieved a new state-of-the-art score of 81.79. It is worth mentioning that SNCSE originally reported a score of 81.77 based on full precision (fp32), but we achieved a better result with half precision (fp16). (iii) HiCL enhances the performance across nine backbone models, yet marginally underperforms (\u2264 0.1) on three models employing the -base architecture.\nAll prior studies in the field have followed a common practice of employing the same random seed for a single run to ensure fair comparisons. We have rigorously adhered to this convention when presenting our findings in Table 1. However, to further assess the robustness of our method, we have extended our investigation by conducting multiple runs with varying random seeds. As shown in Table 15, we generally observe consistency between the multi-run results and the one-run results. The nine backbone models on which HiCL demonstrated superior performance under the default random seed continue to be outperformed by HiCL."
        },
        {
            "heading": "5 Intrinsic Study and Discussion",
            "text": ""
        },
        {
            "heading": "5.1 Local Information Aggregation",
            "text": "Forming the representation for the entire sequence from segment representations is crucial to our task. We experiment with both weighted average pooling (weighted by the number of tokens in each segment) and unweighted average pooling. Since most sequences are divided into three or fewer segments (as shown in Table 2), we did not include an additional layer (either DNN or Transformer) to model relationships with \u2264 3 inputs. Therefore, we opted to not consider aggregating through a deep neural layer, even though this approach might work in scenarios where the training sequences are longer. Results in Table 3 indicate that under three different backbones, weighted pooling is a better strategy for extracting the global representation from local segments."
        },
        {
            "heading": "5.2 Relationships between Segments from Same Sequence",
            "text": "When optimizing the local contrastive objective, an alternative approach is to follow the traditional training paradigm, which treats all other segments as negatives. However, since different parts of a sequence might describe the same thing, segments from the same sequence could be more similar compared to a random segment. As a result, establishing the relationship between segments from the same sequence presents a challenge. We explore\nwith three different versions: 1) considering them as positive pairs, 2) treating them as negative pairs, and 3) categorizing them as neither positive nor negative pairs. The results in Table 4 of SimCSE and SNCSE indicate that the optimal approach is to treat them as neither positive nor negative - a conclusion that aligns with our expectations.\nAn outlier to this trend is observed in the results for ESimCSE. We postulate that this anomaly arises due to ESimCSE\u2019s use of token duplication to create positive examples, which increases the similarity of segments within the same sequence. Consequently, this outcome is not entirely unexpected. Given that ESimCSE\u2019s case is special, we believe that, in general, it is most appropriate to label them as neither positive nor negative."
        },
        {
            "heading": "5.3 Optimal Slicing Length",
            "text": "To verify the impact of slicing length on performance, we vary the slicing length from 16 to 40 for HiCL in SimCSE-RoBERTalarge and SNCSE-RoBERTalarge settings (not counting prompts). We were unable to process longer lengths due to memory limitations. From the results in Figure 2, we find that using short slicing length can negatively impact model performance. As we showed in Section 3.3, the longer the slicing length, the slower it is to encode the whole input sequence, because longer slicing lengths mean fewer segments. Therefore, we recommend setting the default slicing length as 32, as it provides a good balance between performance and efficiency.\nWe acknowledge that using a non-fixed slicing strategy, such as truncation by punctuation, could potentially enhance performance. Nevertheless, in pursuit of maximizing encoding efficiency, we have opted for a fixed-length truncation approach, leaving the investigation of alternative strategies to future work."
        },
        {
            "heading": "5.4 How Hierarchical Training Helps?",
            "text": "One might argue that our proposed method could benefit from the truncated portions of the training\nTable 5: Performance on seven STS tasks for methods trained on wiki-103 and 1 million English Wikipedia sentences. Each method is evaluated on full test sets by Spearman\u2019s correlation, \u201call\u201d setting.\nMethod STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.\nSimCSE-RoBERTalarge 71.72 84.33 76.45 84.16 80.36 81.90 72.27 78.74 + HiCL 72.45 84.69 76.98 84.79 81.64 82.43 71.54 79.22\nFigure 2: Comparison between different slicing lengths over three backbone models by Spearman\u2019s correlation.\ncorpus - the part exceeding the length limitations of baselines and thus, unprocessable by them. To address this concern, we reconstruct the training corpus in such a way that any sequence longer than the optimal slicing length (32) is divided and stored as several independent sequences, each of which fits within the length limit. This allows the baseline models to retain the same information as HiCL. The aforementioned models are indeed using a local-only loss, given they implement contrastive loss on the segmented data. Table 6 shows that HiCL continues to exceed the performance of single segment-level contrastive learning models, indicating its superior performance is not solely reliant on the reduced data. The lower performance exhibited by these models effectively emphasizes the significance of incorporating a hybrid loss.\nThe intriguing insight from above observations is that the omitted data does not improve the performance of the baseline models; in fact, it hinders performance. We hypothesize that this is due to a hard cut by length resulting in some segments beginning with unusual tokens, making it more difficult for the encoder to accurately represent their meaning.\nWe further verify this hypothesis by doing an ablation study on HiCL with various values of \u03b1 in the overall objective. Recall that our training objective is a combination of local contrastive and global contrastive loss. HiCL model with \u03b1 = 0 is identical to the baselines, except that it incorporates the omitted information. As shown in Table 7, training with just global contrastive loss with complete information yields poorer results. Similarly, when \u03b1 = 1, the HiCL model focuses solely on local contrastive loss, but also performs poorly, which indicates that global contrastive loss is an essential component in learning sentence representation.\nIt\u2019s crucial to clarify that our approach isn\u2019t just a variant of the baseline with a larger batch. As Table 2 indicates, in 99.9% of instances, the training data can be divided into four or fewer segments. Comparing SimCSE-BERTbase with a batch size quadruple that of our method (as shown in Table 10), it\u2019s evident that SimCSE at a batch size of 256 trails our model\u2019s performance with a batch size of 64 (74.00 vs. 76.35). Simply amplifying the batch size for baselines also leads to computational issues. For example, SimCSE encounters an \"Out of Memory\" error with a batch size of 1024, a problem our model, with a batch size of 256, avoids. Therefore, our approach is distinct and offers benefits beyond merely adjusting batch sizes."
        },
        {
            "heading": "5.5 HiCL on Longer Training Corpus",
            "text": "To further verify the effectiveness of HiCL, we add a longer corpus, WikiText-103, along with the original 1 million training data. WikiText-103 is a dataset that contains 103 million tokens from 28,475 articles. We adopt a smaller batch size of 64 to avoid out-of-memory issue. Other training\ndetails followed the instructions in Section 4.1. As shown in Table 5, HiCL shows more improvement (+0.48%) compared to the version only trained on short corpus (+0.25%). This indicates that HiCL is more suitable for pre-training scenarios, particularly when the training corpus is relatively long."
        },
        {
            "heading": "5.6 HiCL on Longer Test Datasets",
            "text": "The datasets that were widely evaluated, such as STS12-16, STS-B, and SICK-R, primarily consist of short examples. However, given our interest in understanding how HiCL performs on longer and more complex tasks, we further conduct evaluations on the Yelp (Zhang et al., 2015) and IMDB (Maas et al., 2011) datasets. Table 8 provides an overview of these two datasets.\nSpecifically, we test with SimCSE-BERTbase backbone model and follow the evaluation settings outlined in SimCSE, refraining from any further fine-tuning on Yelp and IMDB. The results are compelling, with our proposed method consistently outperforming SimCSE, achieving a performance gain of +1.97% on Yelp and +2.27% on IMDB."
        },
        {
            "heading": "5.7 A Variant of HiCL",
            "text": "As we discussed in Section 5.2, the best approach to treat relationships between in-sequence segments is to consider them as neither positive nor negative. However, this would result in losing information about them belonging to the same sequence. To overcome this, we consider modeling the relationship between sequences and segments. Since each segment originates from a sequence, they inherently contain an entailment relationship \u2013 the sequence entails the segment. We refer to this variant as HiCLv2. Additional details are provided in Appendix A.6.\nAs shown in Table 9, explicitly modeling this sequence-segment relationship does not help the model. We think that it is probably because this objective forces the representation of each sequence to be closer to segments from the same sequence. When the representation of each sequence is a weighted average pooling of segments, it pulls segments from the same sequence closer, which is\nanother way of regarding them as positive. As seen in the results in Section 5.2, treating segments from the same sequence as positive would negatively impact the performance of SimCSE and SNCSE backbones. Thus, it is not surprising that HiCLv2 failed to show as much improvement as HiCL."
        },
        {
            "heading": "5.8 Baseline Reproduction",
            "text": "One might wonder why there are notable discrepancies between our reproduced baselines and the numbers reported in the original papers. For instance, our SimCSE-BERTbase achieved a score of 74.00, while the original paper reported 76.25. Indeed, this difference comes from the different hyperparameters we adopt.\nDifferent baselines adopt various configurations such as batch size, training precision (fp16 or fp32), and other factors. Recognizing that these factors significantly influence the final results, our aim is to assess different baselines under consistent conditions. To clarify, it would be misleading to evaluate, say, SimCSE-BERTbase with a batch size of 64 while assessing SNCSE-BERTbase with a batch size of 256. Such discrepancies could obscure the true reasons behind performance gaps. Therefore, we use a unified batch size of 256 for base models and 128 for large models.\nTo eliminate concerns about whether the proposed method can still work at baseline\u2019s optimal hyperparameters, we reassess the SimCSE-BERTbase model in Table 10. Regardless of whether we use SimCSE\u2019s optimal settings or our uniform configuration, our method consistently outperforms the baseline.\nLastly, we want to mention that some baselines actually benefit from our standardized setup. For example, our reproduction of SimCSE-RoBERTabase saw an increase, going from the originally reported 76.57 to 77.09."
        },
        {
            "heading": "6 Related Work",
            "text": "Contrastive learning The recent ideas on contrastive learning originate from computer vision, where data augmentation techniques such as AUGMIX (Hendrycks et al., 2020) and mechanisms like end-to-end (Chen et al., 2020a), memory bank (Wu et al., 2018), and momentum (He et al., 2020) have been tested for computing and memory efficiency. In NLP, since the success of SimCSE (Gao et al., 2021b), considerable progress has been made towards unsupervised sentence representation. This includes exploring text data augmentation techniques such as word repetition in ESimCSE (Wu et al., 2022b) for positive pair generation, randomly generated Gaussian noises in DCLR (Zhou et al., 2022), and negation of input in SNCSE (Wang et al., 2022) for generating negatives. Other approaches design auxiliary networks to assist contrastive objective (Chuang et al., 2022; Wu et al., 2022a). Recently, a combination of prompt tuning with contrastive learning has been developed in PromptBERT (Jiang et al., 2022) and SNCSE (Wang et al., 2022). With successful design of negative sample generation and utilization of prompt, SNCSE resulted in the state-of-the-art performance on the broadly-evaluated seven STS tasks. Our novel training paradigm, HiCL, is compatible with previous works and can be easily integrated with them. In our experiments, we re-train SimCSE, ESimCSE, and SNCSE with HiCL, showing a consistent improvement in all models.\nHierarchical training The concept of hierarchical training has been proposed for long-text processing. Self-attention models like Transformer (Vaswani et al., 2017) are limited by their input length, and truncation is their default method of processing text longer than the limit, which leads to information loss. To combat this issue, researchers have either designed hierarchical Transformers (Liu and Lapata, 2019; Nawrot et al., 2022) or adapted long inputs to fit existing Transform-\ners (Zhang et al., 2019; Yang et al., 2020). Both solutions divide the long sequence into smaller parts, making full use of the whole input for increased robustness compared to those using partial information. Additionally, hierarchical training is usually more time efficient. SBERT (Reimers and Gurevych, 2019) employs a similar idea of hierarchical training. Instead of following traditional fine-tuning methods that concatenate two sentences into one for encoding in sentence-pair downstream tasks, SBERT found that separating the sentences and encoding them independently can drastically improve sentence embeddings. To our knowledge, we are the first to apply this hierarchical training technique to textual contrastive learning."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce HiCL, the first hierarchical contrastive learning framework, highlighting the importance of incorporating local contrastive loss into the prior training paradigm. We delve into the optimal methodology for navigating the relationship between segments from same sequence in the computation of local contrastive loss. Despite the extra time required for slicing sequences into segments, HiCL significantly accelerates the encoding time of traditional contrastive learning models, especially for long input sequences. Moreover, HiCL excels in seamlessly integrating with various existing contrastive learning frameworks, enhancing their performance irrespective of their distinctive data augmentation techniques or foundational architectures. We employ SimCSE, ESimCSE, and SNCSE as case studies across seven STS tasks to demonstrate its scalability. Notably, our implementation with the SNCSE backbone model achieves the new state-of-the-art performance. This makes our hierarchical contrastive learning method a promising approach for further research in this area."
        },
        {
            "heading": "Acknowledgment",
            "text": "The work is partly supported by the Endowment of Basic Sciences at the University of Michigan Medical School and by Dissertation Fellowships from the School of Information. Chaowei Xiao is supported by the U.S. Department of Homeland Security under Grant Award Number, 17STQAC0000106-00.\nWe thank Fei Sun, Tianyu Gao, Simone Conia, Pratyush Maini, Amrith Setlur, Zhenhao Zhang, Jiazhao Li, members of the University of Michigan\u2019s NLP4Health research group, and all anonymous reviewers for helpful discussion and valuable feedback."
        },
        {
            "heading": "Limitations",
            "text": "We acknowledge following limitations of our work: 1. Despite demonstrating the suitability of HiCL for pre-training scenarios in Section 5.5, we were not able to fully pre-train our model from scratch. Further research is needed to verify the effectiveness of HiCL in this context. 2. Some baselines (e.g., SimCSE) have shown that training a contrastive learning objective on human-labeled NLI datasets can lead to improved performance. We did not investigate this supervised setting."
        },
        {
            "heading": "Ethics Statement",
            "text": "Data All of the training data and evaluation benchmarks used in this paper are publicly available and do not pose any privacy concerns.\nAI Writing Assistance We have utilized the ChatGPT to polish our original content, rather than generate new ideas or suggestions. Despite the fact that EMNLP 2023 has not clearly stated whether the use of generative language models needs to be disclosed, we believe it is best to be transparent and explicitly disclose this."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Training Time",
            "text": "The practical training time can be complex, as the actual encoding time does not strictly follow a quadratic rule. However, our method demonstrates advantages in terms of efficiency when maintaining the same sequence truncation size. For example, while SimCSE-RoBERTa-large takes approximately 354.5 minutes for training, our method achieves the same task in just 152 minutes. The acceleration in time stems from two primary factors: 1) Savings in encoding time, as discussed in Section 3.3, and 2) The capability of HiCL to handle significantly larger batch sizes, enabling parallel processing for further acceleration. Please note that these advantages in efficiency are discussed specifically in the context of training with identical information, where the same truncation size is maintained. The original SimCSE, which adopts a truncation size of 32, is indeed faster than our approach as it does not need process truncated information."
        },
        {
            "heading": "A.2 Transfer Learning",
            "text": "We also evaluate competing methods on several transfer tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST-2 (Socher et al., 2013), TREC (Voorhees and Tice, 2000), MRPC (Dolan and Brockett, 2005).\nA comparison between SimCSE-BERTbase (using its reported optimized hyperparameters and without adding MLM loss) and our model can be found in Table 11. While our approach may not demonstrate enhancements across all tasks, it does register improvements in 5 out of the 7 tasks, with each exceeding a 0.4% increase. The aggregate improvement stands at 0.39%."
        },
        {
            "heading": "A.3 Experimental setup",
            "text": "Different baselines utilize varying training setups, including batch size, training precision (fp16 or fp32), and other factors. In order to maximally en-\nsure a fair comparison, we unify the training setup across all competing methods and strictly follow each baseline\u2019s hyperparameter tuning process to re-tune the optimal hyperparameters accordingly.\nSpecifically, we employ a batch size of 256 for base models and 128 for large models. All base models are trained using full precision (fp32), while large models are trained using half precision (fp16) to mitigate potential memory issues with certain models. It is worth noting that disparities in performance between our re-run models and the original baselines may arise due to our intentional parameter adjustments to facilitate a direct comparison with other baselines.\nFollowing the procedures used by SimCSE and SNCSE, we evaluate the model every 125 training steps on the development set of STS-B and select the best checkpoint for the final evaluation on the test sets."
        },
        {
            "heading": "A.4 Hyperparameter tuning of HiCL",
            "text": "We first tune the weight of local contrastive \u03b1 \u2208 {0.01, 0.05, 0.15} along with learning rate 5e-6 for large models and 1e-5 for base models.3 After fixing an optimized \u03b1, we proceed to tune the learning rate using the suggested values for each model, including 5e-6 as an additional option. Specifically, we tune the learning rate to be one of the following values: {5e-6, 1e-5, 3e-5, 5e-5} for SimCSE models, {5e-6, 1e-5, 3e-5} for ESimCSE models, and {5e-6, 1e-5} for SNCSE models. The optmized hyperparameters of HiCL are listed in Table 13.\n3We find that the results on the SICK-R development set were more consistent with the results on the seven STS test sets. Once an optimized checkpoint was identified through STS dev set, we fine-tuned the hyperparameters using the SICK-R development set.\nFor HiCLv2, we use the optimized values in HiCL and only tune for \u03b2 \u2208 {1e-5, 3e-5, 1e-4, 3e-4}. Optimized \u03b2 is shown in Table 14."
        },
        {
            "heading": "A.5 Multiple Runs",
            "text": "In addition to the conventional practice of comparing models under the same random seed, we test the generalizability of our proposed method by using different random seeds, as presented in Table 15. An inherent challenge is the decision whether to retune the hyperparameters, considering the optimized ones under one initialization may vary under a different one. With the aim of investigating whether the optimized hyperparameters can be effectively transferred across various random seeds, we have opted not to retune the hyperparameters. 4\nHiCL improves performance across the identical nine backbone models as shown in Table 1, thereby demonstrating its robust generalization capabilities. The average scores in the multi-run setting are uniformly lower than those in the one-run setting, possibly due to the lack of sufficient hyperparameter tuning. This lack of tuning may also result in a reduced performance gap between HiCL and the baseline models."
        },
        {
            "heading": "A.6 A Variant of HiCL",
            "text": "Sequence-segment entailment. The sequencesegment entailment objective is designed as an alternative way to model the relationship between\n4We note that both SimCSE-BERT-based models exhibit a substantial standard deviation, complicating the assessment of the models\u2019 performance. As a consequence, we only make minor adjustments to the learning rate for these two models for both baseline and HiCL method.\nsegments from the same sequence. Intuitively, segments from the same sequence are likely to be more similar to each other than to a random segment. However, this is not always the case, as segments from the same sequence can have contrary meaning. Modeling this relationship is difficult because it has both a high degree of correlation, yet no clear relationship between segments. To tackle this problem, we instead focus on modeling the relationship between a sequence and its segments. This is more straightforward, as we know that segi,j comes from seqi, and therefore they naturally form an entailment relationship. By doing this, we also retain the information about whether two segments come from the same sequence. Figure 3 provides an overview of this variant framework.\nWe employ a third contrastive objective to model the entailment relationship. Specifically, a segment segi,j is entailed by sequence seqi but should not be entailed by seqk, \u2200k \u0338= i. Therefore, we treat segi,j and seqi as a positive pair, and all other sequences in the batch as negative pairs with segi,j . We optimize the following InfoNCE loss function:\nLe = \u2212 log esim(hi,j ,hi)/\u03c4 esim(hi,j ,hi)/\u03c4 + \u2211 k \u0338=i esim(hi,j ,hk)/\u03c4\n(3)\nOverall objective of HiCLv2 The overall objective of HiCLv2 is a combination of local contrastive, global contrastive, and entailment loss, given by L = \u03b1Ll + \u03b2Le +(1\u2212\u03b1\u2212 \u03b2)Lg, where \u03b1 and \u03b2 are the weights."
        },
        {
            "heading": "A.7 Computing Infrastructure",
            "text": "All models were trained on a single 48GB memory NVIDIA A40 GPU for one epoch, using the same initialization, i.e., the same random seed as used by all baselines, for one run. The server has the following configuration: Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz x86-64 with CentOS 7 Linux operating system. PyTorch 1.7.1 is used as the programming framework."
        },
        {
            "heading": "A.8 Summation or Averaging in Contrasting Negatives",
            "text": "In Eq. 2, we contrast a segment hi,j with segments from different sequences hk,\u2217 where k \u0338= i, which is more like picking the correct pair out of a very big batch. We investigate an alternative way which includes a weighted factor when computing similarities between hi,j and hk,\u2217. Specifically,\u2211 k \u0338=i wk \u00d7 esim(hi,j ,hk,\u2217)/\u03c4 assumes contrasting over a fixed small batch instead of using an extended big batch, where wk = 1v , v is the number of segments that sequence k is divided into. In essence, this design would allocate varying weights to different segments in computation: segments derived from shorter sequences would receive a higher weight, while those from longer sequences would be assigned a lower weight. A critical query here is: should we differentiate the importance of segments based on their originating sequence length? To answer this question, we compare this new design (named averaging) with our previous version in Eq. 2 (named summation).\nWe benchmark the averaging loss against summation using three different models: SimCSE, ESimCSE, and SNCSE, all on the RoBERTalarge backbone. To control for initialization effects, we\u2019ve provided results from three-run evaluations on seven STS datasets in Table 16. It is clear that the summation loss consistently outperforms the averaging loss, which proves that varying the importance of segments based on source length may not yield beneficial outcomes."
        }
    ],
    "title": "HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings",
    "year": 2023
}