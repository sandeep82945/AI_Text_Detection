{
    "abstractText": "Rhetoric, both spoken and written, involves not only content but also style. One common stylistic tool is parallelism: the juxtaposition of phrases which have the same sequence of linguistic (e.g., phonological, syntactic, semantic) features. Despite the ubiquity of parallelism, the field of natural language processing has seldom investigated it, missing a chance to better understand the nature of the structure, meaning, and intent that humans convey. To address this, we introduce the task of rhetorical parallelism detection. We construct a formal definition of it; we provide one new Latin dataset and one adapted Chinese dataset for it; we establish a family of metrics to evaluate performance on it; and, lastly, we create baseline systems and novel sequence labeling schemes to capture it. On our strictest metric, we attain F1 scores of 0.40 and 0.43 on our Latin and Chinese datasets, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Stephen Bothwell"
        },
        {
            "affiliations": [],
            "name": "Justin DeBenedetto"
        },
        {
            "affiliations": [],
            "name": "Theresa Crnkovich"
        },
        {
            "affiliations": [],
            "name": "Hildegund M\u00fcller"
        },
        {
            "affiliations": [],
            "name": "David Chiang"
        }
    ],
    "id": "SP:53a5ff10fdd4fd6230f676b46e16d1c619b608e9",
    "references": [
        {
            "authors": [
                "Tosin Adewumi",
                "Roshanak Vadoodi",
                "Aparajita Tripathy",
                "Konstantina Nikolaido",
                "Foteini Liwicki",
                "Marcus Liwicki."
            ],
            "title": "Potential idiomatic expression (PIE)-English: Corpus for classes of idioms",
            "venue": "Proceedings of the Thirteenth Language Resources",
            "year": 2022
        },
        {
            "authors": [
                "Mohammed Alliheedi."
            ],
            "title": "Multi-document Summarization System Using Rhetorical Information",
            "venue": "Master\u2019s thesis, University of Waterloo, Waterloo, Ontario, Canada.",
            "year": 2012
        },
        {
            "authors": [
                "Ron Artstein",
                "Massimo Poesio"
            ],
            "title": "Survey article: Inter-coder agreement for computational lin",
            "year": 2008
        },
        {
            "authors": [
                "Saint Augustine of Hippo."
            ],
            "title": "De doctrina Christiana",
            "venue": "Oxford Early Christian texts. Clarendon, Oxford.",
            "year": 1995
        },
        {
            "authors": [
                "Saint Augustine of Hippo."
            ],
            "title": "Sermones: Part 1, volume 12 of Saint Augustine: Opera Omnia - Corpus Augustinianum Gissense",
            "venue": "Electronic Edition. InteLex Corp., Charlottesville, Virginia, U.S.A.",
            "year": 2000
        },
        {
            "authors": [
                "Saint Augustine of Hippo."
            ],
            "title": "Sermones: Part 2, volume 13 of Saint Augustine: Opera Omnia - Corpus Augustinianum Gissense",
            "venue": "Electronic Edition. InteLex Corp., Charlottesville, Virginia, U.S.A.",
            "year": 2000
        },
        {
            "authors": [
                "Jimmy Lei Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E. Hinton."
            ],
            "title": "Layer normalization",
            "venue": "arXiv:1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "David Bamman",
                "Patrick J. Burns."
            ],
            "title": "Latin BERT: A contextual language model for classical philology",
            "venue": "ArXiv:2009.10053.",
            "year": 2020
        },
        {
            "authors": [
                "David Bamman",
                "Gregory Crane."
            ],
            "title": "Measuring historical word sense variation",
            "venue": "Proceedings of the 11th Annual International ACM/IEEE Joint Conference on Digital Libraries, JCDL \u201911, pages 1\u201310, New York, NY, USA. Association for Computing",
            "year": 2011
        },
        {
            "authors": [
                "David Bamman",
                "David Smith."
            ],
            "title": "Extracting two thousand years of Latin from a million book library",
            "venue": "Journal on Computing and Cultural Heritage, 5(1).",
            "year": 2012
        },
        {
            "authors": [
                "Beata Beigman Klebanov",
                "Nitin Madnani."
            ],
            "title": "Automated evaluation of writing \u2013 50 years and counting",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7796\u20137810, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "James Bergstra",
                "Yoshua Bengio."
            ],
            "title": "Random search for hyper-parameter optimization",
            "venue": "J. Mach. Learn. Res., 13:281\u2013305.",
            "year": 2012
        },
        {
            "authors": [
                "Patrick J. Burns",
                "James A. Brofos",
                "Kyle Li",
                "Pramit Chaudhuri",
                "Joseph P. Dexter."
            ],
            "title": "Profiling of intertextuality in Latin literature using word embeddings",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jean Carletta."
            ],
            "title": "Assessing agreement on classification tasks: The kappa statistic",
            "venue": "Computational Linguistics, 22(2):249\u2013254.",
            "year": 1996
        },
        {
            "authors": [
                "Noe Casas",
                "Marta R. Costa-juss\u00e0",
                "Jos\u00e9 A.R. Fonollosa."
            ],
            "title": "Combining subword representations into word-level representations in the Transformer architecture",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Wanxiang Che",
                "Zhenghua Li",
                "Ting Liu."
            ],
            "title": "LTP: A Chinese language technology platform",
            "venue": "Coling 2010: Demonstrations, pages 13\u201316, Beijing, China. Coling 2010 Organizing Committee.",
            "year": 2010
        },
        {
            "authors": [
                "Stanley F. Chen",
                "Joshua Goodman."
            ],
            "title": "An empirical study of smoothing techniques for language modeling",
            "venue": "Technical Report TR-10-98, Harvard University, Cambridge, Massachusetts.",
            "year": 1998
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and Psychological Measurement, 20(1):37\u201346.",
            "year": 1960
        },
        {
            "authors": [
                "Silvia Corbara",
                "Alejandro Moreo",
                "Fabrizio Sebastiani."
            ],
            "title": "Syllabic quantity patterns as rhythmic features for Latin authorship attribution",
            "venue": "Journal of the Association for Information Science and Technology, 74(1):128\u2013141.",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pretrained models for Chinese natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for Chinese BERT",
            "venue": "IEEE Transactions on Audio, Speech and Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Yange Dai",
                "Wei Song",
                "Xianjun Liu",
                "Lizhen Liu",
                "Xinlei Zhao."
            ],
            "title": "Recognition of parallelism sentence based on recurrent neural network",
            "venue": "2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS), pages 148\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Janez Dem\u0161ar."
            ],
            "title": "Statistical comparisons of classifiers over multiple data sets",
            "venue": "Journal of Machine Learning Research, 7(1):1\u201330.",
            "year": 2006
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Marie Dubremetz",
                "Joakim Nivre."
            ],
            "title": "Rhetorical figure detection: The case of chiasmus",
            "venue": "Proceedings of the Fourth Workshop on Computational Linguistics for Literature, pages 23\u201331, Denver, Colorado, USA. Association for Computational Linguis-",
            "year": 2015
        },
        {
            "authors": [
                "Marie Dubremetz",
                "Joakim Nivre."
            ],
            "title": "Rhetorical figure detection: Chiasmus, epanaphora, epiphora",
            "venue": "Frontiers in Digital Humanities, 5.",
            "year": 2018
        },
        {
            "authors": [
                "Jenny Rose Finkel",
                "Christopher D. Manning."
            ],
            "title": "Nested named entity recognition",
            "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 141\u2013150, Singapore. Association for Computational Linguistics.",
            "year": 2009
        },
        {
            "authors": [
                "Christopher Forstall",
                "Walter Scheirer."
            ],
            "title": "Features from frequency: Authorship and stylistic analysis using repetitive sound",
            "venue": "Journal of the Chicago Colloquium on Digital Humanities and Computer Science, 1(2).",
            "year": 2010
        },
        {
            "authors": [
                "Milton Friedman."
            ],
            "title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance",
            "venue": "Journal of the American Statistical Association, 32(200):675\u2013701.",
            "year": 1937
        },
        {
            "authors": [
                "Milton Friedman."
            ],
            "title": "A comparison of alternative tests of significance for the problem of < rankings",
            "venue": "The Annals of Mathematical Statistics, 11(1):86\u201392.",
            "year": 1940
        },
        {
            "authors": [
                "Akansha Gautam",
                "Koteswar Rao Jerripothula."
            ],
            "title": "SGG: Spinbot, Grammarly and GloVe based fake news detection",
            "venue": "2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM), pages 174\u2013182.",
            "year": 2020
        },
        {
            "authors": [
                "Jakub Jan Gawryjolek."
            ],
            "title": "Automated annotation and visualization of rhetorical figures",
            "venue": "Master\u2019s thesis, University of Waterloo, Waterloo, Ontario, Canada.",
            "year": 2009
        },
        {
            "authors": [
                "Kallirroi Georgila."
            ],
            "title": "Using integer linear programming for detecting speech disfluencies",
            "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
            "year": 2009
        },
        {
            "authors": [
                "Jiefu Gong",
                "Xiao Hu",
                "Wei Song",
                "Ruiji Fu",
                "Zhichao Sheng",
                "Bo Zhu",
                "Shijin Wang",
                "Ting Liu."
            ],
            "title": "IFlyEA: A Chinese essay assessment system with automated rating, review generation, and recommendation",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Marie Gu\u00e9gan",
                "Nicolas Hernandez"
            ],
            "title": "Recognizing textual parallelisms with edit distance and",
            "year": 2006
        },
        {
            "authors": [
                "Robert Guthrie"
            ],
            "title": "DeepLearningForNLPInPytorch: An IPython Notebook tutorial on deep learning for natural language processing, including structure prediction",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "Gaussian error linear units (GELUs)",
            "venue": "arXiv:1606.08415.",
            "year": 2016
        },
        {
            "authors": [
                "Julian Hough",
                "David Schlangen."
            ],
            "title": "Recurrent neural networks for incremental disfluency detection",
            "venue": "Proc. Interspeech 2015, pages 849\u2013853.",
            "year": 2015
        },
        {
            "authors": [
                "George Hripcsak",
                "Adam S. Rothschild."
            ],
            "title": "Agreement, the f-measure, and reliability in information retrieval",
            "venue": "Journal of the American Medical Informatics Association : JAMIA, 12(3):296\u2013298.",
            "year": 2005
        },
        {
            "authors": [
                "Zhiheng Huang",
                "Wei Xu",
                "Kai Yu."
            ],
            "title": "Bidirectional LSTM-CRF models for sequence tagging",
            "venue": "CoRR, abs/1508.01991.",
            "year": 2015
        },
        {
            "authors": [
                "Matthew Lee Jockers."
            ],
            "title": "Macroanalysis: Digital Methods and Literary History",
            "venue": "Topics in the Digital Humanities. University of Illinois Press, Urbana.",
            "year": 2013
        },
        {
            "authors": [
                "Kyle P. Johnson",
                "Patrick J. Burns",
                "John Stewart",
                "Todd Cook",
                "Cl\u00e9ment Besnier",
                "William J.B. Mattingly."
            ],
            "title": "The Classical Language Toolkit: An NLP framework for pre-modern languages",
            "venue": "Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Omer Levy",
                "Luke Zettlemoyer",
                "Daniel Weld."
            ],
            "title": "BERT for coreference resolution: Baselines and analysis",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Jakub Kabala."
            ],
            "title": "Computational authorship attribution in medieval Latin corpora: The case of the Monk of Lido (ca",
            "venue": "1101\u201308) and Gallus Anonymous (ca. 1113\u201317). Language Resources and Evaluation, 54(1):25\u201356.",
            "year": 2020
        },
        {
            "authors": [
                "Zixuan Ke",
                "Vincent Ng."
            ],
            "title": "Automated essay scoring: A survey of the state of the art",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 6300\u20136308. International Joint Conferences on Ar-",
            "year": 2019
        },
        {
            "authors": [
                "Mike Kestemont",
                "Justin Stover",
                "Moshe Koppel",
                "Folgert Karsdorp",
                "Walter Daelemans."
            ],
            "title": "Authenticating the writings of Julius Caesar",
            "venue": "Expert Systems with Applications, 63:86\u201396.",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Klaus Krippendorff."
            ],
            "title": "Reliability, fourth edition, chapter 12",
            "venue": "SAGE Publications, Inc., Thousand Oaks.",
            "year": 2019
        },
        {
            "authors": [
                "H.W. Kuhn."
            ],
            "title": "The Hungarian method for the assignment problem",
            "venue": "Naval Research Logistics Quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando Pereira."
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "Departmental Papers (CIS).",
            "year": 2001
        },
        {
            "authors": [
                "Guillaume Lample",
                "Miguel Ballesteros",
                "Sandeep Subramanian",
                "Kazuya Kawakami",
                "Chris Dyer."
            ],
            "title": "Neural architectures for named entity recognition",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "J. Richard Landis",
                "Gary G. Koch."
            ],
            "title": "The measurement of observer agreement for categorical data",
            "venue": "Biometrics, 33(1):159\u2013174.",
            "year": 1977
        },
        {
            "authors": [
                "Guanghui Liu",
                "Lijun Fu",
                "Bo Yu",
                "Li Cui."
            ],
            "title": "Automatic recognition of parallel sentence based on sentences-interaction CNN and its application",
            "venue": "2022 7th International Conference on Computer and Communication Systems (ICCCS), pages 245\u2013250.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoqiang Luo."
            ],
            "title": "On coreference resolution performance metrics",
            "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 25\u201332, Vancouver, British Columbia, Canada.",
            "year": 2005
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Eduard Hovy."
            ],
            "title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064\u20131074, Berlin, Ger-",
            "year": 2016
        },
        {
            "authors": [
                "Marie W. Meteer",
                "Ann A. Taylor"
            ],
            "title": "Dysfluency annotation stylebook for the Switchboard corpus",
            "year": 1995
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop",
            "year": 2013
        },
        {
            "authors": [
                "Franco Moretti."
            ],
            "title": "Conjectures on world literature",
            "venue": "New Left Review, 1(1):54\u201368.",
            "year": 2000
        },
        {
            "authors": [
                "James Munkres."
            ],
            "title": "Algorithms for the assignment and transportation problems",
            "venue": "Journal of the Society for Industrial and Applied Mathematics, 5(1):32\u201338.",
            "year": 1957
        },
        {
            "authors": [
                "Peter Bjorn Nemenyi."
            ],
            "title": "Distribution-Free Multiple Comparisons",
            "venue": "Ph.D. thesis, Princeton University, Princeton, New Jersey, USA.",
            "year": 1963
        },
        {
            "authors": [
                "Hermann Ney",
                "Ute Essen",
                "Reinhard Kneser."
            ],
            "title": "On structuring probabilistic dependences in stochastic language modelling",
            "venue": "Computer Speech & Language, 8(1):1\u201338.",
            "year": 1994
        },
        {
            "authors": [
                "Toan Q. Nguyen",
                "Julian Salazar."
            ],
            "title": "Transformers without tears: Improving the normalization of self-attention",
            "venue": "Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong. Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Mari Ostendorf",
                "Sangyun Hahn."
            ],
            "title": "A sequential repetition model for improved disfluency detection",
            "venue": "Proc. Interspeech 2013, pages 2624\u20132628.",
            "year": 2013
        },
        {
            "authors": [
                "Ellis B. Page."
            ],
            "title": "The imminence of",
            "venue": ". . grading essays by computer. The Phi Delta Kappan, 47(5):238\u2013243.",
            "year": 1966
        },
        {
            "authors": [
                "Razvan Pascanu",
                "Tomas Mikolov",
                "Yoshua Bengio."
            ],
            "title": "On the difficulty of training recurrent neural networks",
            "venue": "Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages",
            "year": 2013
        },
        {
            "authors": [
                "jani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "PyTorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Lance Ramshaw",
                "Mitch Marcus."
            ],
            "title": "Text chunking using transformation-based learning",
            "venue": "Third Workshop on Very Large Corpora, pages 82\u201394.",
            "year": 1995
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Elizabeth Shriberg."
            ],
            "title": "Preliminaries to a Theory of Speech Disfluencies",
            "venue": "Ph.D. thesis, University of California, Berkeley, Berkeley, California, USA.",
            "year": 1994
        },
        {
            "authors": [
                "Wei Song",
                "Tong Liu",
                "Ruiji Fu",
                "Lizhen Liu",
                "Hanshi Wang",
                "Ting Liu."
            ],
            "title": "Learning to identify sentence parallelism in student essays",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Pa-",
            "year": 2016
        },
        {
            "authors": [
                "F\u00e1bio Souza",
                "Rodrigo Frassetto Nogueira",
                "Roberto de Alencar Lotufo."
            ],
            "title": "Portuguese named entity recognition using BERT-CRF",
            "venue": "arXiv:1909.10649.",
            "year": 2019
        },
        {
            "authors": [
                "Pontus Stenetorp",
                "Sampo Pyysalo",
                "Goran Topi\u0107",
                "Tomoko Ohta",
                "Sophia Ananiadou",
                "Jun\u2019ichi Tsujii"
            ],
            "title": "BRAT: A web-based tool for NLP-assisted text annotation",
            "venue": "In Proceedings of the Demonstrations Session at EACL",
            "year": 2012
        },
        {
            "authors": [
                "Gaius Suetonius Tranquillus."
            ],
            "title": "De Vita Caesarum Libri VIII, volume 1 of Opera",
            "venue": "B. G. Teubner.",
            "year": 1993
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Yonatan Belinkov",
                "Stuart Shieber",
                "Sebastian Gehrmann."
            ],
            "title": "LSTM networks can perform dynamic counting",
            "venue": "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 44\u201354, Florence. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Yonatan Belinkov",
                "Stuart M. Shieber."
            ],
            "title": "On evaluating the generalization of LSTM models in formal languages",
            "venue": "Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 277\u2013286.",
            "year": 2019
        },
        {
            "authors": [
                "Chenhao Tan",
                "Hao Peng",
                "Noah A. Smith."
            ],
            "title": "You are no Jack Kennedy\u201d: On media selection of highlights from presidential debates",
            "venue": "Proceedings of the 2018 World Wide Web Conference, WWW \u201918, pages 945\u2013954, Republic and Canton of Geneva,",
            "year": 2018
        },
        {
            "authors": [
                "Maksim Terpilowski."
            ],
            "title": "scikit-posthocs: Pairwise multiple comparison tests in Python",
            "venue": "The Journal of Open Source Software, 4(36):1169.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Samy Bengio",
                "Eugene Brevdo",
                "Francois Chollet",
                "Aidan N. Gomez",
                "Stephan Gouws",
                "Llion Jones",
                "\u0141ukasz Kaiser",
                "Nal Kalchbrenner",
                "Niki Parmar",
                "Ryan Sepassi",
                "Noam Shazeer",
                "Jakob Uszkoreit"
            ],
            "title": "Tensor2Tensor for neural machine",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Paul van"
            ],
            "title": "Mulbregt, and SciPy 1.0 Contributors",
            "venue": "Nature Methods,",
            "year": 2020
        },
        {
            "authors": [
                "Jue Wang",
                "Lidan Shou",
                "Ke Chen",
                "Gang Chen."
            ],
            "title": "Pyramid: A layered model for nested named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5918\u20135928, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Qiang Wang",
                "Bei Li",
                "Tong Xiao",
                "Jingbo Zhu",
                "Changliang Li",
                "Derek F. Wong",
                "Lidia S. Chao."
            ],
            "title": "Learning deep Transformer models for machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Gail Weiss",
                "Yoav Goldberg",
                "Eran Yahav."
            ],
            "title": "On the practical computational power of finite precision RNNs for language recognition",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
            "year": 2018
        },
        {
            "authors": [
                "Guowei Xu",
                "Wenbiao Ding",
                "Weiping Fu",
                "Zhongqin Wu",
                "Zitao Liu."
            ],
            "title": "Robust learning for text classification with multi-source noise simulation and hard example mining",
            "venue": "Machine Learning and Knowledge Discovery in Databases. Applied Data Science",
            "year": 2021
        },
        {
            "authors": [
                "Vicky Zayats",
                "Mari Ostendorf."
            ],
            "title": "Giving attention to the unexpected: Using prosody innovations in disfluency detection",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2019
        },
        {
            "authors": [
                "Vicky Zayats",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi"
            ],
            "title": "Disfluency detection using a bidi",
            "year": 2016
        },
        {
            "authors": [
                "Victoria Zayats",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-domain disfluency and repair detection",
            "venue": "Proc. Interspeech 2014, pages 2907\u2013 2911.",
            "year": 2014
        },
        {
            "authors": [
                "Dawei Zhu",
                "Qiusi Zhan",
                "Zhejian Zhou",
                "Yifan Song",
                "Jiebin Zhang",
                "Sujian Li."
            ],
            "title": "ConFiguRe: Exploring discourse-level Chinese figures of speech",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 3374\u20133385,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Ueni, uidi, uici,1 or, \u201cI came, I saw, I conquered\u201d \u2013 why is this saying so memorable? One reason is the high degree of parallelism it exhibits in its morphology (each verb is first-person, singular, perfect tense), phonology (each verb starts with /w/ and ends with /i:/), and prosody (each verb has two long syllables with accent on the first). In similar sayings (as in Fig. 1), syntax and semantics also contribute. The related elements generally occur in either the same order or in an inverted order.\nParallelism is organically employed in many argumentative structures (e.g., \u201con the one hand . . . on the other hand . . .\u201d), making parallelism a routine rhetorical figure. The rhetorical impact of\n1Popularly attributed to Julius Caesar (Suetonius Tranquillus, 1993). In Classical Latin, u and v were graphical variants of one letter; as a consonant, it was pronounced /w/. Following common practice, we write it as u. We also write i and j as i.\nthese structures is apparent even for audiences not schooled in classical rhetoric. Recognition of parallelism, then, is important for grasping the structure, meaning, and intent that humans wish to convey; thus, the computational modeling of parallelism is both an interesting and practical problem.\nHowever, because parallelism can occur at so many levels linguistically\u2014often with no lexical overlap\u2014modeling parallelism computationally is difficult. As a first foray into studying the problem of rhetorical parallelism detection (RPD) and enabling others to study it, we present in this paper multiple public datasets, metrics, and models for it.\nFor one dataset, we turn to the Latin sermons of Augustine of Hippo (354\u2013430). Augustine had been trained as a rhetorician and had taught the craft of secular rhetoric before his conversion to Christianity. However, upon becoming a preacher, he consciously replaced the adorned style of late ancient rhetoric with a style streamlined toward speaking well with diverse North African congregations. Parallelism frequently marked this style.\nAugustine did not just employ parallelism stylistically, however; he also theorized about it. In his work De Doctrina Christiana, Augustine described three rhetorical styles, or genera dicendi (\u201cways of\nspeaking\u201d) (Augustine of Hippo, 1995); from these, he characterized the genus temperatum (\u201cmoderate style\u201d) by highly-parallel passages.2 Thus, his sermons are ideal for studying parallelism. In addition, his theory implies that parallelism detection may be useful for automatic stylistic analysis. Already, it has been used to analyze discourse structure (Gu\u00e9gan and Hernandez, 2006), summarize documents (Alliheedi, 2012), identify idioms (Adewumi et al., 2022), evaluate student writing (Song et al., 2016), study political speech (Tan et al., 2018) and detect fake news (Gautam and Jerripothula, 2020).\nParallelism detection may also have broader NLP applications. In syntactic parsing, in the sentence I saw a man with a mustache and a woman with a telescope, the reading where telescope modifies saw is all but ruled out because it would violate parallelism. Thus, modeling parallelism could assist in syntactic disambiguation. Parallelism is also vital in disfluency detection, as speakers tend to maintain prior syntactic context when amending verbal errors (Zayats and Ostendorf, 2019).\nToward such ends, we establish the task of rhetorical parallelism detection (RPD, Section 2). We create one dataset from Augustine\u2019s sermons and adapt another consisting of Chinese student essays (Section 4), establish evaluation metrics (Section 5), and investigate baseline models (Section 6). By automatically learning and exploiting relationships among linguistic features, we achieve roughly a 40% F1 score on both datasets\u2019 test sets (Section 7)."
        },
        {
            "heading": "2 Task Definition",
            "text": "Moving away from the sentence-level conceptions of Gu\u00e9gan and Hernandez (2006) and Song et al. (2016), we formalize the task of rhetorical parallelism detection (RPD). Broadly, we deem sequences to be parallel if they meet two conditions:\n1. Locality: Parallel structures should be within temporal proximity of one another for two related reasons. First, structures that are close by are more likely to be intentional rather than incidental in the mind of the speaker/author. Second, for parallel structures to be rhetorically effective, they must be consecutive enough to be recalled by the listener/reader.\n2. Linguistic Correspondence: Parallel structures should have some linguistic features in common, which could include features of\n2See chapters 4.20.40; 20.44; 21.47f.\nphonology, morphology, syntax, semantics, or even style. The features could be the same (e.g., the repeated sounds in ueni, uidi, uici) or diametrically opposed (e.g., the opposite meanings in a time to be born and a time to die). These features typically occur in the same order or the opposite order.\nSuppose that we have a document w, which is a sequence of tokens F1 . . . F=. A span of w is a pair (8, 9), where 1 \u2264 8 \u2264 9 \u2264 =, whose contents are the tokens F8 . . . F 9 . We say that two spans (81, 91) and (82, 92) are overlapping if they have at least one token in common\u2014that is, 81 \u2264 82 \u2264 91 or 82 \u2264 81 \u2264 92. Meanwhile, they are nested if 81 \u2264 82 \u2264 92 \u2264 91 or 82 \u2264 81 \u2264 91 \u2264 92.\nA parallelism is a set of two or more nonoverlapping spans whose contents are parallel. We call these spans the branches of a parallelism. An example of a complete parallelism with three branches is given in Fig. 1. RPD, then, is the task of identifying all of the parallelisms in a text.\nLet % be a set of parallelisms. Then % falls into one of three categories:\n1. % is flat if all the branches of every parallelism in % are pairwise non-overlapping.\n2. % is nested if some pair of its branches nest and no pair of its branches overlap without also being nested.\n3. % is overlapping if it contains some pair of branches which overlap.\nOur Augustinian Sermon Parallelism (ASP) dataset (see Section 4) is nested, and the Paibi Student Essay (PSE) dataset (Song et al., 2016) is flat.3 Although our baseline models only predict flat sets of parallelisms, our evaluations include nested ones."
        },
        {
            "heading": "3 Related Work",
            "text": "In this section, we connect two subareas of NLP to RPD in order to display their relation to and influence on its development."
        },
        {
            "heading": "3.1 Automated writing evaluation",
            "text": "Arguably, the main research area that has explored RPD is automated writing evaluation (AWE). Since its inception (Page, 1966), AWE has aimed to reduce instructor burden by swiftly scoring essays;\n3Song et al.\u2019s dataset was not originally named but has been given a name through its inclusion in this work.\nhowever, especially with the involvement of neural networks, it seems to be limited in its pedagogical benefit because of its inability to give sufficient feedback (Ke and Ng, 2019; Beigman Klebanov and Madnani, 2020).\nConcerning RPD, Song et al. explored parallel structure as a critical rhetorical device for evaluating writing quality (Song et al., 2016). Construing parallelism detection as a sentence pair classification problem, they achieved 72% 1 for getting entire parallelisms correct on their dataset built from Chinese mock exam essays via a random forest classifier with hand-engineered features. Subsequent work used RNNs (Dai et al., 2018) and CNNs with some custom features (Liu et al., 2022). Such work has also been applied in the IFlyEA assessment system to provide students with stylistic feedback (Gong et al., 2021).\nCompared to that work, our formalization of RPD permits a token-level (as opposed to sentencelevel) granularity for parallel structure. In line with this, we approach RPD in terms of sequence labeling instead of classification. Moreover, we provide the first (to our knowledge) public release of tokenlevel parallelism detection data (see Section 4)."
        },
        {
            "heading": "3.2 Disfluency detection",
            "text": "RPD closely resembles and is notably inspired by disfluency detection (DD). DD\u2019s objective is generally to detect three components of a speech error: the reparandum (the speech to be replaced), the interregnum (optional intervening speech), and the repair (optional replacing speech) (Shriberg, 1994). Because the reparandum and repair often relate in their syntactic and semantic roles, they (as with parallelisms) share many linguistic features.\nDD has frequently been framed as a sequence labeling task. Some previous models use schemes which adapt BIO tagging (Georgila, 2009; Ostendorf and Hahn, 2013; Zayats et al., 2014, 2016), while others (Hough and Schlangen, 2015) have proposed novel tagging schemes on the basis of prior theory (Shriberg, 1994) and the Switchboard corpus\u2019s notation (Meteer and Taylor, 1995), augmenting tags with numerical links to indicate relationships between reparanda and repairs.\nWe employ elements of both types of tagging schemes: we directly adapt BIO tags to parallelisms, and, like Hough and Schlangen (2015), we also augment tags with numerical links to indicate relationships between branches (see Section 6.1)."
        },
        {
            "heading": "4 Datasets",
            "text": "This paper presents two datasets for RPD: the Augustinian Sermon Parallelism (ASP) and Paibi Student Essay (PSE) datasets. We first describe steps taken for the individual datasets in Sections 4.1 and 4.2 before discussing shared preprocessing steps and data analyses in Sections 4.3 and 4.4."
        },
        {
            "heading": "4.1 Augustinian Sermon Parallelism Dataset",
            "text": "The ASP dataset consists of 80 sermons from the corpus of Augustine of Hippo (Augustine of Hippo, 2000a,b).4 Our fourth author, an expert classicist and Augustine scholar, labeled these sermons for parallel structure using our annotation scheme. This scheme involves labeling branches and linking them to form parallelisms. We further distinguish between synchystic (same order) and chiastic (in-\n4The ASP dataset is freely available at https://github. com/Mythologos/Augustinian-Sermon-Parallelisms.\nverted order) parallelisms.5 For more details on our annotation scheme, see Appendix A; for details on a verification of the dataset\u2019s quality through an inter-annotator agreement study, see Appendix C.\nAn example of Augustine\u2019s use of parallelism is presented in Fig. 2(a). In this subordinate clause, Augustine builds up a five-way parallelism. This parallelism not only boasts shared morphology and syntax through a series of subject-verb clauses, but it also presents stylistic parallelism. Each clause uses the rhetorical device of personification, presenting an object or abstract idea as an agent, and this device is paralleled to emphasize it. As this example shows, Augustine frequently layers linguistic features to compose parallelisms.\nTo preprocess the ASP data, we remove all capitalization, following previous work on Latin word embeddings (Bamman and Burns, 2020; Burns et al., 2021). We also employ the LatinWordTokenizer from the CLTK library to tokenize our data (Johnson et al., 2021)."
        },
        {
            "heading": "4.2 Paibi Student Essay Dataset",
            "text": "The PSE dataset was created by Song et al. (2016) to improve automatic essay evaluation.6 They collected a set of mock exam essays from Chinese senior high school students. Two annotators then marked parallel structure in these essays. Annotators labeled parallelism on the sentence rather than span level. On this task, they achieved an interannotator agreement of 0.71 according to the ^ statistic (Carletta, 1996) over a set of 30 essays.\nIn Chinese, a\u6392\u6bd4 (pa\u030cib\u0131\u030c, parallelism) is some-\n5The terms synchystic and chiastic are used by analogy with the traditional rhetorical terms synchysis and chiasmus.\n6The PSE dataset and its PSE-I variant are freely available at https://github.com/Mythologos/ Paibi-Student-Essays.\ntimes defined as having at least three branches (although the PSE dataset has many examples of twobranch parallelisms). One such three-way parallelism is given in Fig. 2(b), where both lexical repetition of the comparative adverb\u66f4 (g\u00e8ng) and syntactic parallelism help the sentence\u2019s three clauses to convey its message in a spirited manner.\nWe obtained 409 of the original 544 essays from the authors. The essays are annotated for inter-paragraph parallelisms, intra-paragraph parallelisms, and intra-sentence parallelisms. To adapt them for our use, we first excluded inter-paragraph parallelisms, as these do not satisfy our criterion of locality. Then, for each intra-sentence parallelism that had been annotated, our annotators marked the locations of the parallel branches. For more details about this annotation process, see Appendix A. We call this version of the dataset PSE-I, or the version only having parallelisms inside paragraphs. We retained the tokenization generated by the Language Technology Platform (LTP) (Che et al., 2010)."
        },
        {
            "heading": "4.3 Data collection and preprocessing",
            "text": "Both datasets had annotations collected for them via the BRAT annotation tool (Stenetorp et al., 2012). In applying the parallelism annotations to the data, both datasets excluded punctuation from parallelisms. This was done to reduce annotator noise brought about by the inconsistent membership of punctuation in parallelisms.\nAfter preprocessing each dataset, we split both into training, validation, optimization, and test sets. To do so, we attempted to keep the ratio of tokens inside parallelisms to tokens outside parallelisms as even as possible across each set, assuring that the sets were distributed similarly. For more details on our data splitting, see Appendix E."
        },
        {
            "heading": "4.4 Dataset statistics",
            "text": "To summarize our datasets, we provide a numerical and statistical overview in Tables 1 and 2.\nOne factor we wanted to examine was the type of similarity that parallel branches exhibit; are their similarities surface-level or more linguistically complex? To measure this, we compute the normalized lexical overlap (NLO) of all pairs of related branches. Treating each branch as a multiset of tokens, we apply the formula\nNLO(11, 12) = |11 \u2229 12 | |11 \u222a 12 |\nwhere 11, 12 are related branches. This value is 1 if 11 = 12, and it is 0 if 11 \u2229 12 = \u2205.\nIn Fig. 3, we depict a histogram of the ASP dataset\u2019s NLO across all related branch pairs. (The PSE-I dataset\u2019s histogram is similar, but it peaks between 0.2 and 0.3 and has few pairs between 0 and 0.1.) This histogram shows that the vast majority of related branch pairs are frequently lexically dissimilar from one another; most are below 0.6 NLO, showing that it is very common to have at least half the words in a parallelism be different from one another. Thus, as our task definition asserted, parallelisms exploit many linguistic features to produce an effect; in turn, any method for RPD should try to capture these relationships."
        },
        {
            "heading": "5 Evaluation Metrics",
            "text": "Next, we describe a general family of evaluation metrics for RPD and specify one instance of it.\nSuppose that we have a document w, as above, and a reference set G and a hypothesis set H of parallelisms in w. An evaluation metric for RPD should check both that the branches in H have the same spans as those in G and that the branches are grouped into parallelisms in H as they are in G.\nGiven these criteria, we follow prior work in coreference resolution (CR) to establish our metrics. Coreference resolution involves detecting an entity of interest and linking all mentions related to that entity together; in a similar manner, our task links a set of spans together. As in the Constrained Entity-Alignment F-Measure metrics (Luo, 2005), we express our metrics as a bipartite matching between G and H. We use the Kuhn-Munkres (also known as \u201cHungarian\u201d) algorithm (Kuhn, 1955; Munkres, 1957) to maximally align parallelisms between G and H. To do this, we must define two functions: score and size.\nThe function score(?1, ?2), where ?1 and ?2 are parallelisms, determines how well ?1 matches ?2, and the function size(?), where ? is a parallelism, bounds the score that ? can merit with another parallelism. These functions must satisfy size(?) > 0 and 0 \u2264 score(?1, ?2) \u2264 min{size(?1), size(?2)}.\nWith those in place, we can find the bipartite matching with maximum weight <,\n< = max \" \u2211 (?1, ?2) \u2208\" score(?1, ?2)\nwhere the maximization is over matchings \" between G and H. Having <, we define metrics in the likeness of precision (%), recall ('), and the F1-score ( 1) as follows:\n% = <\u2211\n?\u2208H size(?)\n' = <\u2211\n?\u2208G size(?)\n1 = 2\n1/% + 1/' .\nThe simplest choice of size and score is\nsize(?) = 1 score(?1, ?2) = {\n1 if ?1 = ?2 0 if ?1 \u2260 ?2.\nWe call this the exact parallelism match (EPM) metric, where each parallelism in H must perfectly match one in G to gain credit.\nAs an example of this metric, consider the passage given in Fig. 4. Suppose that the depicted parallelism\u2014call it ?1\u2014is the only parallelism in G. Furthermore, suppose that the model proposes two hypotheses, H = {?1, ?2}, where ?2 is a parallelism that does not overlap with ?1. Then\nscore(?1, ?1) = 1 score(?1, ?2) = 0\nFrom this, we derive the maximally-weighted matching, \" = {(?1, ?1)}, and < = 1. Then\n% = 1 2\n' = 1 1\n1 = 2 3 .\nAppendix B introduces some alternative choices of score and size that give partial credit for imperfectly-matching hypothesized parallelisms.7"
        },
        {
            "heading": "6 Models",
            "text": "In this section, we propose some models for automatic RPD as a starting point for future research. Our models here treat RPD as a sequence labeling problem, using several novel variants of BIO tagging (Ramshaw and Marcus, 1995).8"
        },
        {
            "heading": "6.1 Tagging schemes",
            "text": "Branches are substrings, like named entities in NER, and parallelisms are sets of related substrings, like disfluencies in DD. Variants of BIO tagging have been successful for these sequence labeling tasks (Zayats et al., 2016; Reimers and Gurevych, 2017), so they are a natural choice for RPD as well. In this scheme, the first word of a branch is tagged B; the other words of a branch are tagged I; words outside of branches are tagged O.\nHowever, BIO tagging does not indicate which branches are parallel with which. In each parallelism, for each branch 1 except the first, we augment the B tags with a link, which is a number that indicates the previous branch with which 1 is parallel. We propose two linking schemes.\nSuppose we have consecutive parallel branches 11 = (81, 91) and 12 = (82, 92) with 91 < 82. A token distance link, akin to links used in DD (Hough and Schlangen, 2015), is 91 \u2212 82; it is the (negative)\n7We accompany this paper with the pyrallelism library. It implements each of our metrics and provides sample formats for evaluating RPD. It is available on PyPI and at https: //github.com/Mythologos/pyrallelism.\n8Our main modeling and results repository is available at https://github.com/Mythologos/Intro-RPD. Our models are implemented in PyTorch (Paszke et al., 2019) and are initially based on code by Robert Guthrie (2017).\nnumber of word-to-word hops to get from 12\u2019s start to 11\u2019s end. A branch distance link is the (negative) number of branch-to-branch hops to get from 12 to 11. If % contains the interlocking parallelisms ?1 = {(1, 3), (7, 9)} and ?2 = {(4, 6), (10, 12)}, then the token distance between (1, 3) and (7, 9) is \u22124, while the branch distance is \u22122.\nIt is important to form tag sequences that help the model to learn better, diverging decisions from the dominant majority class O. Therefore, in addition to adding links, we include three additional tag types, each of which is exhibited in Fig. 4.\nFirst, the M tag replaces O tags that occur for tokens that occur in the middle of consecutive branches in a parallelism. Lines 2 and 3 of Fig. 4 exhibit this: four non-branch tags become M instead of O because they are between related branches. This shift obstructs a model from predicting singlebranch parallelisms by providing a separate pathway to seek linked branches, as O\u2192 B\u2113 (where \u2113 stands for a link) can never occur in the data.\nSecond, the E tag replaces branch-ending I tags to indicate the end of a branch. Adding this tag removes the I\u2192 O transition. This change may encourage a model to be more sensitive to a branch\u2019s endpoint; because most parallelisms are more than two tokens long, branches largely must transition from B to either I or E, and E must eventually be selected before returning to O.\nThird, the J tag replaces I tags in non-initial branches, where an initial branch is the first branch of a parallelism that occurs in a document. When paired with M tags, J tags help to promote a sequence of transitions which do not include O as a likely candidate until two branches have concluded. While the second example of Figure 4 displays this, Figure 5 shows this behavior across ASP\u2019s entire training set. Treating a sequence of tags as a linear chain, we tally all node transition pairs. When compared to the BIO chain of tags (left), we can see that I\u2192 O no longer exists in the BIOMJ chain (right). Only non-initial branch tokens (i.e., B_ or J) can transition to O. Through its supported transitions, the altered tagset actively enforces the constraint that a parallelism must have at least two branches.\nWith these three new tags, we form eight tagsets\u2014BIO, BIOE, BIOJ, BIOM, BIOJE, BIOME, BIOMJ, and BIOMJE\u2014and apply each link type to them, forming sixteen tagging schemes. These tagging schemes constitute all parallelisms in each dataset: each level of nesting is an addi-\ntional layer, or stratum (pl. strata), of tags."
        },
        {
            "heading": "6.2 Architecture",
            "text": "Inspired by prior success in sequence labeling for named entity recognition, we employ a conditional random field (CRF) output layer (Lafferty et al., 2001) in combination with various neural networks (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). Our general architecture (Fig. 6) proceeds in five steps. It embeds tokens (words or subwords) as vectors; it blends token-level embeddings into word-level embeddings; it encodes the sequence of embeddings to incorporate more contextual and task-specific information; it maps encodings to tag space with a linear layer; it uses a CRF to compute a distribution over tag sequences.\nEach model examined in Section 7.2 is a variation on this paradigm. We tested a total of six models. Following work in NER, we selected a BiLSTM-CRF baseline (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). We also tried exchanging the BiLSTM for a Transformer\n(Vaswani et al., 2017) as an encoder layer. We tried three embedding options. The first option was to learn embeddings from scratch. The second option was to use frozen word2vec embeddings (Mikolov et al., 2013). We selected a 300- dimensional embedding built from Latin data lemmatized by CLTK\u2019s LatinBackoffLemmatizer (Johnson et al., 2021) trained by Burns et al. (2021). The third option was to employ frozen embeddings from a BERT model\u2014namely, Latin BERT (Bamman and Burns, 2020) for the ASP dataset and Chinese BERT with whole word masking (Cui et al., 2020, 2021) for the PSE-I dataset.\nFor both the Transformer encoder and the BERT embeddings, we applied WordPiece tokenization (Wu et al., 2016) by reusing the tokenizer previously trained for Latin BERT (Bamman and Burns, 2020) with the tensor2tensor library (Vaswani et al., 2018). We employed operations (termed blending functions) to combine subword representations into word representations. The choice of blending function did not heavily impact our\nresults, so we defer discussion of them to Appendix D.3. For other models, we did not use subwords, and no blending function was needed.\nTo reduce the average sequence length seen by our models, each sequence presented is a section rather than a whole document. For the ASP dataset, each sermon\u2019s division into sections was imposed by later editors of the texts. Meanwhile, for the PSE-I dataset, we split the data based on paragraph divisions. When sections remain longer than BERT\u2019s 512-token limit, we split sections into 512- token chunks and concatenate the output sequences after the embedding layer. As noted in previous CR work (Joshi et al., 2019), this approach is superior to merging overlapping chunks.\nWe discuss other design choices in Appendix D."
        },
        {
            "heading": "7 Experiments",
            "text": "In this section, we describe our experiments (Section 7.1 and present our results (Section 7.2)."
        },
        {
            "heading": "7.1 Experimental design",
            "text": "For each dataset, we performed hyperparameter searches over several model architecture and tagging scheme combinations. We judged these as the elements which would likely primarily govern task performance. For the ASP dataset, we trained six possible architectures described in Section 6.2 with each of the sixteen tagging schemes described in Section 6.1. Meanwhile, for the PSE-I dataset, we ran the three BERT-based architectures with each of the same sixteen tagging schemes. The hyperparameters for each trial were chosen using random search (Bergstra and Bengio, 2012) described in Appendix F. Eight trials per configuration were\ntrained for the ASP and PSE-I datasets, totaling 768 and 384 experiments, respectively.\nEach trial\u2019s model trained for up to 200 epochs using Adam (Kingma and Ba, 2015) and gradient clipping with an L2 norm of 1 (Pascanu et al., 2013). We stopped training early if the model\u2019s F1 score did not improve for 25 epochs on the validation set using the maximum branch-aware word overlap (MBAWO) metric (defined in Appendix B).9 Each trial was assessed on the optimization set. We denoted the trial with the highest MBAWO score on this set per setting as that setting\u2019s best run. We evaluated each best run on the test set."
        },
        {
            "heading": "7.2 Results",
            "text": "To compare the performance across models, we determined the best result for each model, as shown in Table 3. \u201cBest\u201d is defined as the experiment with the highest F1 score on the test set across all attempted settings. We also highlight a few factors which generally improved performance: the embeddings, encoders, and tagsets selected.10\nIn terms of embeddings, BERT embeddings vastly improved performance. As Table 5 depicts, BERT-based models exceeded every other nonBERT model by at least 0.2 F1 on the ASP dataset. We attribute their superiority to the contextual representations produced by BERT. This is supported by the fact that both Burns et al.\u2019s embeddings and Latin BERT\u2019s embeddings were mainly trained on the same Internet Archive dataset (Bamman and Crane, 2011; Bamman and Smith, 2012).\nIn terms of tagging schemes, we wanted to know whether any schemes performed significantly better than the others. We applied the Friedman rankedsums test (Friedman, 1937, 1940). Although this test is usually used to compare multiple classifiers across multiple datasets (Dem\u0161ar, 2006), we instead took results across a single dataset (ASP) and sampling procedure (our hyperparameter search process) and considered each collection of best model F1 scores as a sample. Because the Friedman test is nonparametric, we circumvent issues arising from the fact that model performance already causes F1 scores to differ heavily.\nWith ? < 0.05, we find the differences between samples to be significant (? < 0.001). We then\n9We opted for MBAWO instead of EPM because we were concerned that the stricter EPM metric would overly penalize incremental progress in capturing correct tokens.\n10For an error analysis and a full catalogue of our results and best model hyperparameters, see Appendices G to I.\nused a post-hoc Nemenyi test (Nemenyi, 1963) via the scikit-posthocs library (Terpilowski, 2019) to determine which tagging scheme pairs achieve significantly different results. With ? < 0.05, only one pair of tagging schemes significantly differs: BIOMJ-TD and BIOJE-BD\u2014the best and worst schemes, according to the average ranks of each scheme\u2019s F1 scores (as presented in Table 4). Given our low sample count, we suspect that further samples might show the superiority of certain schemes. With this in mind, we tentatively recommend any M-based tagging schemes, especially in combination with either of the E or J tags, for use.\nIn terms of encoders, BiLSTMs generally outperformed Transformers. Although non-BiLSTM models achieved peak performance in Table 3, the average performance by BiLSTMs was consistently higher. Table 5 depicts this regardless of the type of embedding used: each BiLSTM model performs\non average better than its Transformer (or encoderless) variant. One possible reason for BiLSTMs\u2019 superiority may be that the subtask of predicting distance-based links benefits from LSTMs\u2019 ability to count (Weiss et al., 2018; Suzgun et al., 2019b,a)."
        },
        {
            "heading": "8 Conclusions and Future Work",
            "text": "In this paper, we introduced the task of rhetorical parallelism detection. We described two datasets, an evaluation framework, and several baseline models for the task. Our baselines achieved fairly good performance, and we found that BERT embeddings, a BiLSTM-based encoder, and an M-inclusive tagging scheme were valuable modeling components.\nWe identify a few directions for future work. The models described here have a closed tagset, so they cannot predict links for distances not seen in the training data; modeling links in other ways might be more effective. Our models only predict flat parallelisms; approaches for nested NER (Finkel and Manning, 2009; Wang et al., 2020) may be a viable direction for extending the Encoder-CRF paradigm toward this end. Finally, applying this work\u2019s methods to detect grammatical parallelism might enhance tasks like syntactic parsing, automated essay evaluation, or disfluency detection."
        },
        {
            "heading": "9 Limitations",
            "text": "This work introduces a novel task in the form of rhetorical parallelism detection (RPD). Because it is novel, it is innately exploratory. Although it establishes approaches for building and organizing datasets, for evaluating performance at various granularities, and for constructing models to capture parallel structure, it does not do all these perfectly. Thus, it should not be the last word on the topic. In the following paragraphs, we highlight elements of this work which could be improved.\nFirst, this work puts forth two datasets for RPD: the Augustinian Sermon Parallelism (ASP) and the Paibi Student Essay (PSE) (Song et al., 2016) datasets. We annotated both datasets\u2014the former from scratch and the latter on the basis of prior annotations. As is noted in Appendix C, our annotation scheme was not perfect.\n\u2022 For the ASP dataset, we achieved a 0.4124 EPM F1 score (although higher branch-based and word-based scores) between our two annotators on bootstrapping experiments. Our scores indicate moderate agreement, but the meaningful disagreement implies that our guidelines could be sharpened.\n\u2022 For the PSE-I dataset\u2014the version of the PSE dataset including our annotations\u2014we did not perform an inter-annotator agreement study. We did not consider it necessary because we were already overlapping with prior annotators\u2019 conclusions, but one could argue that the sentence- and span-level annotation tasks differ enough to warrant separate studies.\nSecond, regarding the ASP dataset, we acknowledge that the use of Latin as a foundation for this task limits its immediate applicability to modern NLP contexts. We believe that the ASP dataset is readily applicable to tasks such as authorship attribution and authorship authentication, which both have precedent in Latin (Forstall and Scheirer, 2010; Kestemont et al., 2016; Kabala, 2020; Corbara et al., 2023) and frequently employ stylistic features in their approaches. Moreover, we believe that ASP can aid distant reading approaches (Moretti, 2000; Jockers, 2013) in permitting the annotation of large corpora with stylistic annotations.\nOn the other hand, the inclusion of the PSE dataset for RPD provides a modern language for which this task is available. Moreover, late into this work, we became aware of a vein of literature\nthat could either add to the available Chinese data based on an interest in parallelism detection as a sentence-level or clause-level subtask (Xu et al., 2021; Zhu et al., 2022) as well as work performing narrower, more restricted versions of word-level parallelism detection (Gawryjolek, 2009) or chiasmus detection (Dubremetz and Nivre, 2015, 2018), in English. Future work can potentially expand RPD to the datasets provided by these works.\nThird, some modeling decisions were made to suit the needs of our problem and provide baselines without exploring the larger space of possibilities. Namely, as described in Section 6.2 and Appendix D.3, we use a blending function to combine subwords into words so that the appropriate number of tags would be produced. However, we could have also eschewed the CRF altogether and attempted to follow BERT in selecting every word\u2019s first tag as its representative tag (Devlin et al., 2019). Future work could further and more deeply investigate the search space of model architectures to improve performance on this task."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Wei Song for working with us to not only provide data used in previous work but also to prepare it for public release. We thank Meng Jiang and Juliana, Joshua, and Joseph Chiang for refining the annotations on the PSE dataset. We thank John Lalor for his suggestions for our inter-annotator agreement study. Finally, we also thank our anonymous reviewers and Brian DuSell, Darcey Riley, Ken Sible, Aarohi Srivastava, and Chihiro Taguchi for their comments and suggestions.\nThis research was supported in part by an FRSP grant from the University of Notre Dame."
        },
        {
            "heading": "A Annotation Procedures",
            "text": "In Section 2, we described two major guidelines which directed our study of parallel structures. Here, we list the specific criteria which were used by our annotators to locate parallelisms. Moreover, we describe our annotation scheme in more detail. We begin by enumerating general criteria in Appendix A.1 before discussing specific applications of these criteria in Appendix A.2.\nA.1 General Guidelines\nIn general, branches of parallelisms were tagged from the first identical (or similar) word to the last identical (or similar) word; as such, each branch was maximally represented. Branches were detected, paired, and combined based upon whether they exhibited at least two of these criteria:\n\u2022 They contained identical number of words (or were within approximately two words of one another).\n\u2022 They had identical syntactic structure.\n\u2022 They had two or more pairs of words of an identical grammatical form in identical order.\n\u2022 They had two or more pairs of words that are lexically identical, are synonyms, are cognates, or are antonyms.\n\u2022 They had at least two words that have a phonetically similar ending (e.g., rhyme).\n\u2022 They were a short distance from one another (i.e., they had few intervening words). The acceptable margin was again around two words.\nA.2 Specific Applications\nAs mentioned earlier, we used the BRAT annotation tool (Stenetorp et al., 2012) to form our datasets.\nFor the ASP dataset, we developed an annotation scheme consisting of BRAT\u2019s entities and relationships. An entity could be a ParallelArm or one of ChiasmA and ChiasmB. A relationship could be Parallel, linking two ParallelArm entities. This relationship indicates a synchystic parallel structure. A relationship could also be Chiasm, linking a ChiasmA and a ChiasmB and labeling a chiastic structure. Our two annotators, both for the initial data collection and the inter-annotator agreement study (see Appendix C), could nest annotations; however, they could not overlap them.\nMeanwhile, for the PSE-I dataset, we made a few changes. Because we were mostly interested in annotating sentences that were already marked as parallel in the dataset, we did away with the distinction between synchystic and chiastic parallelisms. Instead, all branches of a parallelism were tagged as Branch entities and connected with Parallel relationships. Annotators were also allowed to use a dummy entity type to tag sentences if they were deemed not to be parallel, but this was never done. The five annotators were told to focus on marked sentences in BRAT, but they were allowed to connect these sentences to unmarked sentences in the same paragraph if they noticed a parallelism."
        },
        {
            "heading": "B Additional Metrics",
            "text": "In Section 5, we constructed a framework for defining parallelism-related metrics. We defined one such metric for exact parallelism matches between parallelisms. However, the score and size functions allow for finer granularity. Below, we define three additional metrics which examine parallelisms in terms of their branches, their words-perbranch, and their words altogether.\nFirst, we describe the maximum parallel branch match (MPBM) metric. MPBM focuses on branches. It retains a sense of parallel structure by requiring that at least two branches are shared between the hypothesis and reference parallelisms to produce any score. The functions of MPBM are:\nsize(?) = |? | score(?1, ?2) = hasParBranch(?1, ?2) \u00b7 |?1 \u2229 ?2 |\nwhere\nhasParBranch(@, A) = I[|@ \u2229 A | \u2265 2]\nNext, we proceed to metrics which examine parallelisms in terms of their words (which we\nuse interchangeably with tokens below). Let ? be a parallelism. We define words(?) =\u22c3 (8, 9) \u2208?{8, . . . , 9} as the set of all token positions in those branches; this formulation dissolves distinctions between branches for a parallelism. We also define branchedWords(?) = {{8, . . . , 9} : (8, 9) \u2208 ?} as the set of all token positions in branches divided up into a set per branch. This formulation preserves branch-level distinctions.\nThe first word-level metric is the maximum branch-aware word overlap (MBAWO) metric. The objective of this metric is similar to MPBM: if at least two distinct branches are matched in a parallelism, then such a matching should obtain some score. However, this metric is less strict than MPBM in that only words from two distinct branches need to match. In short, if at least two branches in a hypothesis parallelism have words that match with at least two branches in a reference parallelism, then the match merits a nonzero score based on the number of tokens matched.\nTo compute this metric, we defineW(?1, ?2) = branchedWords(?1) \u00d7branchedWords(?2); that is, W pairs all branches of one parallelism with all branches of another parallelism in terms of their word indices. Then we define score and size as:\nsize(?) = |words(?) | score(?1, ?2) = max\n, \u2208W(?1, ?2) hasParWord(,) \u00b7\u2211\n(?1, ?2) \u2208, |?1 \u2229 ?2 |\nwhere\nhasParWord(,) = I[wordMatches(,) \u2265 2] wordMatches(,) = \u2211 (?1, ?2) \u2208, I[|?1 \u2229 ?2 | > 0]\nMBAWO calculates an internal maximallyweighted bipartite matching for token overlap; however, it only values those matches which correspond to parallel structure in the reference data. It enforces that through the function wordMatches.\nFinally, we define the maximum word overlap (MWO) metric. MWO is at the opposite extreme of EPM; it does not necessarily control for the presence of parallel structure. Even so, its ability to measure a model\u2019s capacity to find any parallel words may be useful. Its functions are as follows:\nsize(?) = |words(?) | score(?1, ?2) = |words(?1) \u2229 words(?2) |\nC Inter-Annotator Agreement Study\nTo provide some measure of the quality of our data, we performed an inter-annotator agreement study on the ASP dataset. Due to constraints on time and expertise, we recruited our third author, an experienced classicist and Latin teacher, to revisit eight sermons using our annotation scheme. To address the limited annotated data, we computed a bootstrapped estimate for agreement across the whole dataset. After obtaining an initial computation of inter-annotator agreement, we ran 1,000 trials to obtain a 95% confidence interval for our agreement scores. The number of items sampled was equivalent to the total number of matchings made in the initial computation.\nBecause the space of possible spans and links between spans is prohibitively large, we could not use more traditional inter-annotator agreement metrics such as Cohen\u2019s ^ (Cohen, 1960) or Krippendorff\u2019s U (Krippendorff, 2019). Instead, we used our own metrics as defined in Section 5 and Appendix B. However, following the conclusions of Hripcsak and Rothschild that the F1 measure converges to ^ as the number of negative pairs grows increasingly large, we can treat our F1 scores in a similar manner to chance-corrected agreement statistics.\nAs a final preprocessing step before matching annotations, we corrected them with two strategies to reduce noise. We felt that these strategies were warranted after discussion unearthed two vague points in our guidelines. We first describe our issues with the guidelines below, and then we clarify the annotation cleaning done afterward.\n1. Conjunctions: It was unclear whether conjunctions meaningfully contributed to a parallelism. For some parallelisms, they may have been syntactically necessary\u2014but did that mean that they should be part of the parallelism? We decided that conjunctions should be included if every branch of the parallelism has one; otherwise, they should be omitted. This allows polysyndeton to be recognized while avoiding the requisite but incidental connections created by some syntactic structure.\n2. Interlocking Parallelisms: Second, it was difficult to determine the best way to partition parallel structure into branches. This was especially the case with longer parallelisms that involved multiple clauses. Take Fig. 7\nas an example. Without clear rules, there are multiple ways to produce parallel structure:\n\u2022 Interpretation 1 considers each clause its own branch.\n\u2022 Interpretation 2 considers each pair of clauses, which juxtapose inanis (\u201cwithout, lacking, empty\u201d) and plenus (\u201cfilled, full\u201d), as a single parallelism; moreover, it also recognizes the similar syntactic relationships in the individual clauses.\n\u2022 Interpretation 3 only annotates the first stratum of the prior interpretation.\nUltimately, we ruled that the third interpretation was best. It maximally captured the parallel structure in these clauses. Although it is true that each pair of clauses has its own relationships (e.g., same number of words, same syntactic structure), these relationships also contribute to the larger structure they are contained within, as each inanis and plenus takes a noun of the same case (ablative) as its object. Thus, these connections are still captured in the third interpretation. We only advise differ-\ning from a maximal interpretation if the parallelisms intentionally use different features.\nTo address these issues in our original annotations, we automatically altered the annotations on two bases. First, if all branches of a parallelism were preceded by a conjunction, then conjunctions were included in all branches; otherwise, no conjunctions were permitted, and any present were removed from annotations. Second, we collapsed interlocking parallelisms into larger branches if they were adjacent. If these larger branches were part of a nested parallelism, and such branches produced a parallelism on an earlier stratum, we removed these larger branches entirely. Although these alterations did cause gains in annotation agreement, they were all between roughly 0.02 to 0.03 F1.\nWe present our inter-annotator agreement results in Table 6. Across the board, these scores fall below traditional standards (i.e., above 0.67 at the very least) for good chance-corrected reliability in content analysis (Carletta, 1996). However, as noted by Artstein and Poesio, acceptable inter-annotator agreement values remain disputed; the earlier work by Landis and Koch denotes the range between 0.4 and 0.6 as moderate agreement and the range\nbetween 0.6 and 0.8 as substantial agreement. Regardless of the exact standard that we follow, we do believe that the disagreement displayed by this inter-annotator agreement study is meaningful and in part due to some vagueness in our original guidelines. We have already updated our guidelines to provide greater specifications for future annotators, and we are currently working on improving our data with these guidelines. We hope to release an enhanced version of our dataset in future work."
        },
        {
            "heading": "D Model Idiosyncrasies",
            "text": "In Section 6.2, we go over our general EncoderCRF model architecture, and we describe some of the major components of our models. Due to space considerations, some details were omitted from that discussion. We discuss such details here.\nD.1 Training <UNK> in BiLSTM-CRFs\nIn Encoder-CRF models which apply a word-level vocabulary, one necessary issue concerns the handling of unknown words. Usually, such models maintain an <UNK> token for such cases. However, if the <UNK> token is not seen during training, the model will not know how to use it and may confuse it with other tokens seen during training.\nVarious strategies have been employed to address this problem. Some decide that all words seen less than : times, where : is some preset value, become <UNK> tokens (Reimers and Gurevych, 2017). Others select singletons\u2014words which only appear once in the training data\u2014and alter them to become <UNK> during training with a predetermined fixed probability (Lample et al., 2016). We follow the latter strategy, but we augment it so that we can adapt the probability to the dataset at hand and can avoid choosing an arbitrary value.\nWe propose a singleton replacement probability analogous to Kneser-Ney smoothing (Ney et al., 1994). We compute the frequency of each type in the dataset. Then, we tally the number of types which occur exactly once, =1, and the number of types which occur exactly twice, =2. Here, we use notation from previous work where = represents the number of word types and the subscripts represent a conditional frequency on that count (Chen and Goodman, 1998). Finally, for the singleton replacement probability ?A , we use the equation:\n?A = =1\n=1 + 2=2 (1)\nThe Kneser-Ney replacement probability for the ASP dataset, given the current data splits, is approximately 0.6529.\nD.2 Transformer Modifications\nWe apply the Transformer encoder in some of our models with a slight change from the usual implementation. In line with previous work which sees minor improvements in model performance (Chen et al., 2018; Wang et al., 2019; Nguyen and Salazar, 2019), we swap the traditional order of layer normalization and skip connections. Applying the notation of Nguyen and Salazar, we use PreNorm in the incorporation of the residual connection:\nx;+1 = LayerNorm(x; + ; (x;))\nwhere ; is the layer\u2019s index, represents the layer itself, and LayerNorm represents the layer normalization operation (Ba et al., 2016).\nD.3 Subword Blending\nAs previously mentioned, we use WordPiece tokenization with our models which either have a Transformer encoder or Latin BERT embeddings. Using subword-level tokens presents an issue for a word-level tagging problem: in what way should the model process subwords to generate word-level tags? The original BERT paper, which tackles NER, does not use a CRF and instead lets the tag classification of each word\u2019s first subword represent the whole word (Devlin et al., 2019).\nWe take inspiration from BERT\u2019s methods and others\u2019 approaches to handling subword-to-word relationships (Souza et al., 2019; Casas et al., 2020) by putting a blending layer into the architecture. As mentioned in Section 6.2, the blending layer\u2019s objective is to combine subword-level encodings into word-level encodings. We define three variations:\n\u2022 Take-First: In this approach, we mimic BERT and select the first subword of each word as its representative. (Note that, when convenient, we abbreviate this variant as \u201ctf\u201d.)\n\u2022 Mean: In this approach, we take the mean of all subword encodings per word. This is akin to a subword-aware mean pooling layer.\n\u2022 Sum: In this approach, we sum all subword encodings for a word to represent it. This representation may allow for a model to recognize the accumulation of multiple subwords."
        },
        {
            "heading": "E Data Splitting Approach",
            "text": "As noted in Section 4, parallelisms are not evenly distributed over sections. Sections are also not evenly distributed over documents. Because we performed evaluation in terms of documents, we wanted to apply a simple heuristic to guarantee that the data splits would fairly distribute tags.\nTo do this, we used a straightforward approach to optimize split creation. To apply the approach, we supplied it with a set of ratios which govern the quantity and relative size of the splits. We began by taking each file in the dataset and computing\ncounts of its outer and inner tags.11 A tag is an outer tag if it is O or M; otherwise, it is an inner tag. Then, we attempted to place a file in each split. If placing that file in a split minimized the meansquared error (MSE) among the splits, then the file was placed in that split. To be specific, we averaged the MSE for inner and outer tags to compute the final MSE. We repeated this process for every file.\nAs a further heuristic, we sorted the files after their inner and outer tag counts were computed. We did so by the number of inner tags from maximum\n11As in the main paper, we created strata of tags for each dataset equal to the maximum nesting depth of a branch.\nto minimum, breaking ties by the number of outer tags. This was in line with our objective to balance out the parallelisms and branches seen across splits: the ordering allowed for files which have a greater effect on MSE to be placed first, thereby letting files with less tags smooth out error gradually.\nTo verify our procedure, we provide the resulting mean-squared error for our splits. We also used Welch\u2019s C-test for this purpose. For each pair of splits created, we ran this test twice with per-file counts of the inner and outer tags. In this case, we do not want the test to report significance; rather, in showing that the difference between the distributions is insignificant (e.g., ? \u2265 0.05), we are showing that the splits are divided up such that they could have been drawn from the same distribution. We use the SciPy implementation of Welch\u2019s C-test for this purpose (Virtanen et al., 2020)."
        },
        {
            "heading": "F Hyperparameter Search Spaces",
            "text": "For our hyperparameter search experiments in Section 7.2, we performed a random hyperparameter search (Bergstra and Bengio, 2012) of eight trials per setting. However, because we are proposing a new task, we did not have prior literature on successful hyperparameters which we could draw upon in order to define hyperparameter spaces. Thus, to perform these experiments, we created our own.\nThese hyperparameter search experiments were governed by three main elements:\n1. The set of hyperparameters which are selected for variation. Although many hyperparameters could be varied, only some were chosen to direct the search toward hyperparameters which were suspected to be meaningful.\n2. The space itself from which hyperparameters were drawn, as this determines all possible values which the hyperparameter can take.\n3. The set of constraints which are imposed upon the random trials. Naturally, we want to prevent any trial from being duplicated; however, there are other inductive biases which may guide our search.\nIn the subsections below, we expound upon each of these elements and provide tables which summarize pertinent information regarding them.\nF.1 Selected Hyperparameters\nTable 11 provides information about what hyperparameters are varied per model. Due to space considerations, we use a number of abbreviations; these are supplied in Table 10.\nF.2 Designated Hyperparameter Spaces\nTable 12 lists the hyperparameter spaces which we used during the random search process. While some of the above hyperparameters are relatively self-explanatory in their functionality, we provide an explanation for those which may be less clear or otherwise warrant further explanation:\n\u2022 Activation Function: The PyTorch (Paszke et al., 2019) implementation of the Transformer (Vaswani et al., 2017) provides integrations for both ReLU and GeLU (Hendrycks and Gimpel, 2016) as activation functions. We uphold that support in our modifications (as presented in Appendix D.2) to see whether they have any major effects on our results.\n\u2022 Dimensionalities: For all embedding and hidden dimensionality spaces, we let the search space be in powers of two and at every value averaged between adjacent powers of two. We used this approach to explore a decent portion of the search space without leaving big gaps between larger powers. Furthermore, we differed the spaces for BiLSTMs and Transformers to adapt more to precedent while still allowing some exploration. In particular, we took inspiration from some hyperparameter LSTM searches in NER (Reimers and Gurevych, 2017) and the general application of the Transformer (Vaswani et al., 2017).\n\u2022 Depth: In initial trials, we found that high depth values prevented the models from learning meaningful information. Thus, instead of setting a maximum of 6 encoder layers as per the initial Transformer implementation (Vaswani et al., 2017), we lowered it to 4.\n\u2022 Learning Rates: In the NER literature, a variety of learning rates have been used (alongside a variety of optimizers). Values from .1 (Huang et al., 2015) to .015 (Ma and Hovy, 2016) to .01 (Lample et al., 2016) to a variety of manually-tuned others (Reimers and Gurevych, 2017) have been proposed. Due to the differences in our task, however, we examine a wider range on the basis of those\nvalues. Previous experience in using Adam (Kingma and Ba, 2015) has seen models struggle with high learning rates, hence the choice of varying between lower ones.\nFor all such hyperparameter search spaces, we sampled from each uniformly.\nF.3 Hyperparameter Trial Constraints\nAlthough we could theoretically generate any combination of hyperparameters in our randomlysought trials, it is not necessarily the case that any combination will be fruitful. With a wide enough search space, it is possible that random search will avoid discovering regions of the search space where performance is approximately maximal. As a re-\nsult, we direct the random search process in some cases by forcing trials to meet a set of constraints.\nThe main constraint we supplied was for the BiLSTM-based models. This constraint was the word-level dimensionality compression constraint, which assured that HS \u2264 IS. The hidden state is intended here to compress and learn how to use word embedding information. This constraint is applied implicitly for the model with BERT embeddings and the BiLSTM encoder, as every dimensionality in the search space was at or below 768."
        },
        {
            "heading": "G Error Analysis",
            "text": "The main body of this paper, and especially Section 7.2, highlights major hyperparameters for our\nEncoder-CRF architecture which improve performance. In this section, we attempt to provide some insight about where our Encoder-CRF models could still improve by describing the kinds of errors that these models make. We focus on the ASP dataset in this section, as this dataset was designed for word-level parallelism detection.\nTo elucidate relevant and constructive model behavior, we defined a set of categories for this error analysis. That set of categories can be divided into a three-by-three grid on the basis of three error granularities and three error types. Following our metrics, these granularities are at the levels of parallelisms, branches, and words. Meanwhile, two of our error types derive from classification error types: false positive (FP) and false negative (FN).\nOur third error type, false mixture (FM), is a combination of the FP and FN which applies to branches and words. Because a model can both predict a branch or word that a parallelism does not contain (false positive) and omit a branch or word that a parallelism contains (false negative), both error types are simultaneously possible for these granularities. Conversely, a parallelism cannot both be entirely missing and entirely distinct from reference data; thus, the granularity-type combination of parallelism and false mixture is not possible. This leaves us with a set of eight categories.\nWe went over ASP\u2019s validation set with this annotation scheme, comparing hypothesized parallelisms from the model with the highest-scoring F1 score from Section 7.2 and reference parallelisms. We added a category for every matched hypothesis parallelism and reference parallelism. In other\nwords, a hypothesis and reference are only categorized once if they have some overlap; otherwise, they are categorized independently. We provide a more detailed, category-by-category overview of this scheme in our main code repository.\nWe present the results of our error analysis categorization in Table 13. From this table, we see that the predominant cause of error comes from the highest level of granularity: parallelisms. Conversely, we see that other kinds of error are quite infrequent. As a result, we can surmise that these Encoder-CRF models are relatively all-or-nothing in their sequence labeling approach. They are proficient at getting entire parallelisms correct, and largely down to the boundaries, when they detect a parallelism. However, their ability to confidently detect them in full needs more work.\nTo illustrate some of these error categories, we provide a set of three examples in Fig. 8. Fig. 8(a) presents the most common case: the false negative parallelism. Fig. 8(b), in turn, presents its false positive version. Finally, Fig. 8(c) shows what a false mixture looks like.\nStarting with Fig. 8(a), we find a two-branch parallelism from ASP. Both branches contain two clauses spanning four words, are headed by subjects of the same case and number, and apply verbal forms in their first clauses with the same lemma (promitto). Augustine plays up contrasts by juxtaposing human beings and God as well as verbs for belief (crederes) and doubt (dubitas). The moods used to define the conditional structures of each branch, being contrafactual and factual, respectively, further highlight this contrast. In short, these\nclauses contain many distinct cues as to their parallel nature. In spite of this, our model did not detect this pair. One possibility is that variations on token order due to function words (si, et) and commas caused the model to ignore the branch pair.\nNext, Fig. 8(b) shows a predicted parallelism that the reference data did not contain. On the one hand, there are aspects of this branch pair that make them seem parallel. The clauses juxtapose heaven (caeli, de caelo) and earth (terreno, terrae) in the same grammatical roles. Both also use verbs conjugated in the same manner, and each takes a direct object in the same case (creatorem, conditorem) which are synonymous. On the other hand, the word order in these clauses calls their parallel nature into question; there is no clear alignment between the word orders in each clause (e.g., the direct objects are the second and fifth words, respectively). As a result, it seems understandable both why the model was fooled by this sentence as well as why it was not annotated in the first place.\nFinally, we provide an example of a branch-level false mixture in Fig. 8(c). In this branch-level false mixture, the model agrees with the reference parallelism on its second branch. However, it both ignores the reference\u2019s first branch and adds an additional branch after the second. The reference contains a relatively simple parallelism consisting\nof two three-word branches centering around two prepositional phrases that share the word suos. The pair are followed by a third clause that is not part of the parallelism due to its rather elaborate content and lack of suos (although, it contains the related form se). To speculate about the model\u2019s mistake, it may have been drawn by lexical connections between the second branch and the final clause (i.e., per with per, suos with se, and christianos with christi). However, because of the final clause\u2019s additional complexity, it then only selected a couple words to pair with the second branch."
        },
        {
            "heading": "H Exhaustive Experimental Results",
            "text": "In this section, we display box plots which give an overview of all F1 scores attained from each of the best models across all configurations explored during the hyperparameter search and all metrics. All box plots are created from 16 data points, as each model architecture variation (i.e., embeddingencoder combination) was examined across sixteen distinct tagging schemes. The ASP dataset\u2019s plots are presented in Figs. 9 to 12. Meanwhile, the PSEI dataset\u2019s plots are presented in Figs. 13 to 16. We also provide tabular data and CSV data for all our results, additionally including precision and recall for all metrics, in our main code repository."
        },
        {
            "heading": "I Exemplary Model Hyperparameter Tables",
            "text": "In this section, we provide sets of tables which describe the hyperparameters used for each best trial catalogued in Appendix H. For the ASP dataset, we organized the results into three tables\u2014Tables 14 to 16\u2014based upon what hyperparameters were varied. For the PSE-I dataset, we organized them similarly in Tables 17 and 18."
        }
    ],
    "title": "Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines",
    "year": 2023
}