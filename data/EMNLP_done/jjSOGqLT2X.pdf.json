{
    "abstractText": "Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Videohelpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention model with two novel methods: Frame attention loss and Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/videohelpful-MMT.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yihang Li"
        },
        {
            "affiliations": [],
            "name": "Shuichiro Shimizu"
        },
        {
            "affiliations": [],
            "name": "Chenhui Chu"
        },
        {
            "affiliations": [],
            "name": "Sadao Kurohashi"
        },
        {
            "affiliations": [],
            "name": "Wei Li"
        }
    ],
    "id": "SP:7e8b24d3807e3fa26030a8b89e88f36eab6af70c",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Basura Fernando",
                "Mark Johnson",
                "Stephen Gould."
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "Computer Vision\u2013ECCV 2016, pages 382\u2013398.",
            "year": 2016
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine",
            "year": 2005
        },
        {
            "authors": [
                "Lo\u00efc Barrault",
                "Fethi Bougares",
                "Lucia Specia",
                "Chiraag Lala",
                "Desmond Elliott",
                "Stella Frank."
            ],
            "title": "Findings of the third shared task on multimodal machine translation",
            "venue": "Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages",
            "year": 2018
        },
        {
            "authors": [
                "Ozan Caglayan",
                "Pranava Madhyastha",
                "Lucia Specia",
                "Lo\u00efc Barrault."
            ],
            "title": "Probing the need for visual context in multimodal machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Joao Carreira",
                "Andrew Zisserman."
            ],
            "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
            "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308.",
            "year": 2017
        },
        {
            "authors": [
                "Zhigang Dai",
                "Bolun Cai",
                "Yugeng Lin",
                "Junying Chen."
            ],
            "title": "Up-detr: Unsupervised pre-training for object detection with transformers",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1601\u20131610.",
            "year": 2021
        },
        {
            "authors": [
                "Desmond Elliott."
            ],
            "title": "Adversarial evaluation of multimodal machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2974\u20132978, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Lo\u00efc Barrault",
                "Fethi Bougares",
                "Lucia Specia."
            ],
            "title": "Findings of the second shared task on multimodal machine translation and multilingual image description",
            "venue": "Proceedings of the Second Conference on Machine",
            "year": 2017
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Khalil Sima\u2019an",
                "Lucia Specia"
            ],
            "title": "Multi30K: Multilingual EnglishGerman image descriptions",
            "venue": "In Proceedings of the 5th Workshop on Vision and Language,",
            "year": 2016
        },
        {
            "authors": [
                "Qingkai Fang",
                "Yang Feng."
            ],
            "title": "Neural machine translation with phrase-level universal visual representations",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5687\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Weiqi Gu",
                "Haiyue Song",
                "Chenhui Chu",
                "Sadao Kurohashi."
            ],
            "title": "Video-guided machine translation with spatial hierarchical attention network",
            "venue": "Proceedings of ACL-IJCNLP-SRW, pages 87\u201392.",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "year": 2015
        },
        {
            "authors": [
                "John Hughes."
            ],
            "title": "krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff\u2019s Alpha Coefficient",
            "venue": "The R Journal, 13(1):413\u2013425.",
            "year": 2021
        },
        {
            "authors": [
                "Liyan Kang",
                "Luyang huang",
                "Ningxin Peng",
                "Peihao Zhu",
                "Zewei Sun",
                "Shanbo Cheng",
                "Mingxuan Wang",
                "Degen Huang",
                "Jinsong Su."
            ],
            "title": "Bigvideo: A largescale video subtitle translation dataset for multimodal machine translation",
            "venue": "ArXiv, abs/2305.18326.",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR (Poster).",
            "year": 2015
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Statistical significance tests for machine translation evaluation",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388\u2013395, Barcelona, Spain. Association for Computational Lin-",
            "year": 2004
        },
        {
            "authors": [
                "Klaus Krippendorff"
            ],
            "title": "Computing krippendorff\u2019s alpha-reliability",
            "year": 2011
        },
        {
            "authors": [
                "Bei Li",
                "Chuanhao Lv",
                "Zefan Zhou",
                "Tao Zhou",
                "Tong Xiao",
                "Anxiang Ma",
                "JingBo Zhu."
            ],
            "title": "On vision features in multimodal machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Yihang Li",
                "Shuichiro Shimizu",
                "Weiqi Gu",
                "Chenhui Chu",
                "Sadao Kurohashi."
            ],
            "title": "VISA: An ambiguous subtitles dataset for visual scene-aware machine translation",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Li",
                "Rui Wang",
                "Kehai Chen",
                "Masso Utiyama",
                "Eiichiro Sumita",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Data-dependent gaussian prior objective for language generation",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Huan Lin",
                "Fandong Meng",
                "Jinsong Su",
                "Yongjing Yin",
                "Zhengyuan Yang",
                "Yubin Ge",
                "Jie Zhou",
                "Jiebo Luo."
            ],
            "title": "Dynamic context-guided capsule network for multimodal machine translation",
            "venue": "Proceedings of the 28th ACM International",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Lison",
                "J\u00f6rg Tiedemann."
            ],
            "title": "OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 923\u2013929, Por-",
            "year": 2016
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Ming Zhong",
                "Yang Chen",
                "Wen Lei",
                "Nan Duan",
                "Tianrui Li."
            ],
            "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning",
            "venue": "Neurocomputing, 508:293\u2013 304.",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Manning",
                "Mihai Surdeanu",
                "John Bauer",
                "Jenny Finkel",
                "Steven Bethard",
                "David McClosky."
            ],
            "title": "The Stanford CoreNLP natural language processing toolkit",
            "venue": "Proceedings of 52nd Annual Meeting of the Association for",
            "year": 2014
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of NAACL-HLT 2019: Demonstrations.",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadel-",
            "year": 2002
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Belgium, Brussels. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun."
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Ramon Sanabria",
                "Ozan Caglayan",
                "Shruti Palaskar",
                "Desmond Elliott",
                "Lo\u00efc Barrault",
                "Lucia Specia",
                "Florian Metze."
            ],
            "title": "How2: A large-scale dataset for multimodal language understanding",
            "venue": "NeurIPS.",
            "year": 2018
        },
        {
            "authors": [
                "Lucia Specia",
                "Stella Frank",
                "Khalil Sima\u2019an",
                "Desmond Elliott"
            ],
            "title": "A shared task on multimodal machine translation and crosslingual image description",
            "venue": "In Proceedings of the First Conference on Machine Translation: Volume",
            "year": 2016
        },
        {
            "authors": [
                "Yuanhang Su",
                "Kai Fan",
                "Nguyen Bach",
                "C.-C. Jay Kuo",
                "Fei Huang."
            ],
            "title": "Unsupervised multi-modal neural machine translation",
            "venue": "Proceedings of CVPR, pages 10482\u201310491.",
            "year": 2019
        },
        {
            "authors": [
                "Umut Sulubacak",
                "Ozan Caglayan",
                "Stig-Arne Gr\u00f6nroos",
                "Aku Rouhe",
                "Desmond Elliott",
                "Lucia Specia",
                "J\u00f6rg Tiedemann."
            ],
            "title": "Multimodal machine translation through visuals and speech",
            "venue": "Machine Translation, 34(2):97\u2013147.",
            "year": 2020
        },
        {
            "authors": [
                "Umut Sulubacak",
                "Ozan Caglayan",
                "Stig-Arne Gr\u00f6nroos",
                "Aku Rouhe",
                "Desmond Elliott",
                "Lucia Specia",
                "J\u00f6rg Tiedemann."
            ],
            "title": "Multimodal machine translation through visuals and speech",
            "venue": "Machine Translation, 34(2):97\u2013147.",
            "year": 2020
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "CoRR, abs/1512.00567.",
            "year": 2015
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann",
                "Yves Scherrer."
            ],
            "title": "Neural machine translation with extended context",
            "venue": "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82\u201392, Copenhagen, Denmark. Association for Computational Linguis-",
            "year": 2017
        },
        {
            "authors": [
                "Arseny Tolmachev",
                "Daisuke Kawahara",
                "Sadao Kurohashi."
            ],
            "title": "Juman++: A morphological analysis toolkit for scriptio continua",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates,",
            "year": 2017
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Xin Wang",
                "Jiawei Wu",
                "Junkun Chen",
                "Lei Li",
                "YuanFang Wang",
                "William Yang Wang."
            ],
            "title": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer",
            "year": 2019
        },
        {
            "authors": [
                "Yonghui Wu",
                "Mike Schuster",
                "Zhifeng Chen",
                "Quoc V Le",
                "Mohammad Norouzi",
                "Wolfgang Macherey",
                "Maxim Krikun",
                "Yuan Cao",
                "Qin Gao",
                "Klaus Macherey"
            ],
            "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine",
            "year": 2016
        },
        {
            "authors": [
                "Zhiyong Wu",
                "Lingpeng Kong",
                "Wei Bi",
                "Xiang Li",
                "Ben Kao."
            ],
            "title": "Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of",
            "year": 2021
        },
        {
            "authors": [
                "Pengcheng Yang",
                "Boxing Chen",
                "Pei Zhang",
                "Xu Sun."
            ],
            "title": "Visual agreement regularized training for multi-modal machine translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9418\u20139425.",
            "year": 2020
        },
        {
            "authors": [
                "Yongjing Yin",
                "Fandong Meng",
                "Jinsong Su",
                "Chulun Zhou",
                "Zhengyuan Yang",
                "Jie Zhou",
                "Jiebo Luo."
            ],
            "title": "A novel graph-based multi-modal fusion encoder for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Kehai Chen",
                "Rui Wang",
                "Masao Utiyama",
                "Eiichiro Sumita",
                "Zuchao Li",
                "Hai Zhao."
            ],
            "title": "Neural machine translation with universal visual representation",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yuting Zhao",
                "Mamoru Komachi",
                "Tomoyuki Kajiwara",
                "Chenhui Chu."
            ],
            "title": "Double attention-based multimodal neural machine translation with semantic image regions",
            "venue": "Proceedings of the 22nd Annual Conference of the European Association for",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016) models relying on text data have achieved state-of-the-art performance. However, in many cases, the text is insufficient to provide the information needed for appropriate translation, especially when the source text is ambiguous. In this work, \u201cAmbiguous\u201d refers not only to ambiguity in a narrow sense caused by\nfactors such as polysemy but also to different possible translations caused by factors such as emotion, politeness, and omission that needs multimodal information for disambiguation. For a source text, if there are multiple possible translations and some of them are more appropriate than others under certain visual scenes, we call it ambiguous. Multimodal machine translation (MMT) (Specia et al., 2016; Sulubacak et al., 2020a) uses visual data as auxiliary information to tackle the ambiguity problem. The contextual information in the visual data helps to resolve the ambiguity in the source text data.\nPrevious MMT studies have mainly focused on the image-guided machine translation (IMT) task (Elliott et al., 2016; Zhao et al., 2020; Li et al., 2022a), where, given an image and a source sentence, the goal is to enhance the quality of translation by leveraging their semantic correspondence to the image. Resolving ambiguities through visual cues is one of the main motivations behind this task. Compared with images, videos contain ordered sequences of frames and can provide richer visual features such as motion features. Recently, some studies have started to focus on the video-guided machine translation (VMT) task (Wang et al., 2019; Gu et al., 2021).\nVMT faces the problem of data scarcity. The How2 (Sanabria et al., 2018) and VATEX (Wang et al., 2019) datasets are recent efforts to allevi-\nate the problem. In addition, previous datasets are limited to subtitles of instructional videos or video captions that describe the video clips. It has been shown that caption MMT essentially does not require visual information due to the lack of language ambiguity in captions (Caglayan et al., 2019). At the same time, the subtitles of the instructional videos are similar to captions and also lack ambiguity. The VISA (Li et al., 2022b) dataset has made efforts to solve this problem. VISA contains parallel subtitles and corresponding video clips collected from movies or TV episodes in which the source subtitles are ambiguous. However, it has a limitation that the videos do not necessarily contribute to disambiguation.\nTo address the problems of previous VMT datasets, we construct a new large-scale VMT dataset EVA (Extensive training set and Videohelpful evaluation set for Ambiguous subtitles translation) for VMT research. EVA has an extensive training set, and a video-helpful evaluation set in which source subtitles are ambiguous and videos are guaranteed to be helpful for disambiguation. In total, EVA contains 852k Ja-En parallel subtitle pairs, 520k Zh-En parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes, where each pair of parallel subtitles has a corresponding video clip. Subtitles from movies and TV episodes are essentially dialogues and short (7.34 English words in our case), which makes subtitles have many possible interpretations and thus makes videos helpful.\nTo train a VMT model that can disambiguate translations, it is necessary for the training set to contain possible translation patterns. To achieve this, a simple yet effective way is to make the training set as large as possible. The training set of EVA contains 848k Ja-En parallel subtitle pairs, 517k Zh-En parallel subtitle pairs, and corresponding video clips, which are collected from 763 movies and 1, 361 TV episodes with a total length of 3, 791 hours. The training set is much larger than existing VMT datasets and may cover more subtitle translation patterns.\nFor testing VMT models, it is inappropriate to use general data because many translations do not require visual information. Therefore, we select video-helpful data to construct an evaluation set that contains 4, 276 Ja-En parallel subtitle pairs, 2, 940 Zh-En parallel subtitle pairs, and corresponding video clips. Considering the definition of am-\nbiguity and the need for the training set containing possible translation patterns, we collect videohelpful data using translation sets, which are sets of parallel subtitles that have the same source subtitles but different target subtitles. An example of a translation set is shown in Figure 1. Each pair of parallel subtitles belongs to a video clip. As a translation task, the video clip can help us translate the source subtitle. The first video clip shows two women escaping, suggesting a \u201cgo\u201d translation, while the last video clip portrays a scene involving an army, suggesting a \u201cforward\u201d translation. We construct the dataset by collecting parallel subtitles and corresponding video clips, constructing an evaluation set with translation sets and crowdsourcing, and then using the remaining part as the training set.\nFurthermore, we propose an MMT model SAFA (Selective Attention model with Frame attention loss and Ambiguity augmentation) for the VMT task. Based on the selective attention model proposed (Li et al., 2022a) for the IMT task, we 1) use CLIP4Clip (Luo et al., 2022) model to extract video features, 2) propose frame attention loss to make the model focus more on the central frames where the subtitles occur, and 3) propose ambiguity augmentation to make the model put more weights on the possibly-ambiguous data. Experiments on EVA show that SAFA achieves 15.41 BLEU score and 35.86 METEOR score for Ja-En translation, and 27.62 BLEU score and 48.74 METEOR score for Zh-En translation, which are significantly better than existing MMT models. Furthermore, our proposed methods significantly improve MT performance when incorporating videos, with relative improvements of 9.99% and 4.94% in BLEU scores for Ja-En and Zh-En translations, respectively.\nIn summary, our contributions are three-fold:\n\u2022 We construct EVA, a large-scale parallel subtitles and video clips dataset, to promote VMT research.\n\u2022 We propose the SAFA model with frame attention loss and ambiguity augmentation.\n\u2022 We conduct substantial experiments on the EVA dataset with SAFA to set a benchmark of the dataset."
        },
        {
            "heading": "2 Related Work",
            "text": "Multimodal Machine Translation. MMT involves drawing information from multiple modal-\nities, assuming that they should contain useful alternative views of the input data (Sulubacak et al., 2020b). Previous studies mainly focus on IMT using images as a visual modality to help machine translation (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018; Su et al., 2019; Yang et al., 2020; Li et al., 2022a). The usefulness of the visual modality has recently been disputed under specific datasets or task conditions (Elliott, 2018; Caglayan et al., 2019; Wu et al., 2021). However, using images in captions translation is theoretically helpful for handling grammatical characteristics and resolving ambiguities when translating between dissimilar languages (Sulubacak et al., 2020b). VMT is a MMT task similar to IMT but focuses on video clips rather than images associated with the textual input. Existing VMT models mainly focus on video caption translation and use video motion features extracted with the I3D (Carreira and Zisserman, 2017) model to help translation (Wang et al., 2019; Gu et al., 2021). The hierarchical attention network (Gu et al., 2021) has been proposed further to combine motion features, and object features (Ren et al., 2015).\nVMT Datasets. The scarcity of datasets is one of the largest obstacles to the advancement of VMT. Recent efforts to compile freely accessible data for VMT, such as the How2 (Sanabria et al., 2018), VATEX (Wang et al., 2019) and VISA (Li et al., 2022b) datasets, have begun to alleviate this bottleneck. Table 1 shows the statistics of existing VMT datasets and EVA. EVA is the largest VMT dataset in video hours and a number of video clips and parallel sentences. Very recently, the BigVideo (Kang et al., 2023) dataset consisting of 4.5 million Zh-En sentence pairs and 9, 981 hours of YouTube videos is proposed to facilitate the study of MMT. We consider this work contemporaneous to our study. The evaluation set of BigVideo is annotated by professional speakers in both Chinese and English to enhance the quality, while we design a languageindependent pipeline with translation sets to ensure the scalability of the evaluation set and reusability\nof the pipeline."
        },
        {
            "heading": "3 Dataset",
            "text": "In this section, we outline the construction of EVA, which is a VMT dataset combined with parallel subtitles and corresponding video clips. The dataset contains a large-scale training set and a small videohelpful evaluation set. For the former, we make it large enough to cover different translation patterns. For the latter, we ensure that the source subtitles are ambiguous and the video clips can help disambiguate the source subtitles."
        },
        {
            "heading": "3.1 Pipeline",
            "text": "To construct the dataset, we first collect a large number of parallel subtitles and one-by-one corresponding video clips, where each pair of parallel subtitles matches a video clip. For these data, we select some video-helpful data to construct the evaluation set and use the remaining data as the training set. To select video-helpful data, we collect translation sets from the parallel subtitles, select ambiguous translation sets, and do crowdsourcing to further select video-helpful data. We construct the evaluation set with the goal of reaching a specific number instead of covering all video-helpful data. In this way, we can both get a video-helpful evaluation set and keep the most ambiguous data in the training set. As the dataset construction pipeline is language-independent, it can be extended to other language pairs. The pipeline is shown in Figure 2."
        },
        {
            "heading": "3.1.1 Collect parallel subtitles and video clips",
            "text": "We collect parallel subtitles and corresponding video clips following the method in (Li et al., 2022b). On the one hand, the collected data can be used to construct an extensive training set. On the other hand, the collected data can be used to further extract the video-helpful evaluation set.\nRegarding the parallel subtitles, we collect Japanese\u2013English and Zh-En parallel subtitles from the OpenSubtitles(Lison and Tiedemann, 2016) dataset. OpenSubtitles is a subtitles dataset compiled from an extensive database of film and TV subtitles which includes a total of 1, 689 bitexts spanning 2.6 billion sentences across 60 languages. From OpenSubtitles, we can also collect subtitle timestamps and the Internet Movie Database (IMDb) ids of the video sources.\nTo collect corresponding video clips, we fix subtitle timestamps and crop video clips according to these timestamps. More details can be found in Appendix B.1. Based on accurate timestamps, we crop 10-second 25-fps video clips for parallel subtitles following (Wang et al., 2019; Li et al., 2022b). From the midpoint of each subtitle\u2019s period, each video clip takes 5 seconds before and after, respectively. The audios of most videos are in English, which is the same as the translation target language and may interfere with translation. Therefore, we only keep the video content of video clips and remove the audio content.\nAs a result, we collected 852, 440 Ja-En parallel subtitles, 519, 673 Zh-En parallel subtitles, and corresponding video clips. This way, we can construct a VMT training set much more extensive than existing VMT datasets."
        },
        {
            "heading": "3.1.2 Collect translation sets",
            "text": "After collecting parallel subtitles and corresponding video clips, we can collect translation sets from the parallel subtitles. Translation sets are sets of parallel subtitles that have the completely same source subtitles but different target subtitles (i.e., translations). Therefore, a large number of parallel subtitles is a prerequisite for collecting translation sets. Note that the parallel subtitles with similar but not completely the same source subtitles are not collected into the translation sets and therefore retained in the training set. In this step, we collected 26, 533 Ja-En translation sets and 17, 642 Zh-En translation sets."
        },
        {
            "heading": "3.1.3 Select ambiguous translation sets",
            "text": "Then we select ambiguous translation sets with sentence similarity. An ambiguous translation set contains a source subtitle and two target subtitles with different meanings. As shown in Figure 3, we select ambiguous translation sets considering two points. On the one hand, we ensure the target subtitles have different meanings. On the other hand, we ensure the target subtitles are parallel with the source subtitle. Subtitles in the OpenSubtitles dataset are provided by volunteers and therefore contain some non-parallel subtitles. If we only focus on the first point, we might often select the non-parallel target subtitles because they tend to have a totally different meaning from other target parallel subtitles.\nWe propose a method to balance the two points discussed above. The main idea is as the following. We first keep the parallel subtitles similarity as high as possible. Then we select the most different target subtitles pair. If they are different enough, we combine them and the source subtitle into an ambiguous translation set and discard the remaining data of the translation set. Otherwise, we relax the restriction on parallel subtitles\u2019 similarity and repeat the process above. More details can be found in Appendix B.3. As a result, we collected 10, 594 Ja-En ambiguous translation sets and 7, 102 Zh-En ambiguous translation sets."
        },
        {
            "heading": "3.1.4 Select video-helpful data via crowdsourcing",
            "text": "At last, we do crowdsourcing to further select videohelpful data. In order to determine whether the video can help disambiguate the source subtitles, the best way should be to distribute tasks to workers who can understand both the source and target languages. However, in practice, it is hard to find such workers. So we designed a scheme that only requires workers to be able to understand the target language.\nIn each task, we show two target subtitles from the same ambiguous translation set and one video\nclip that belongs to one of the two subtitles. Then we ask workers if there is any subtitle strongly related to the video content. And we give workers four choices: (1) none of them; (2) only the first subtitle; (3) only the second subtitle; (4) both of them. In this way, if the video content is only strongly related to the corresponding subtitle, we may estimate that according to the video content, the source subtitle can only be translated to the corresponding subtitle instead of the other one.\nWe did crowdsourcing on Amazon Mechanical Turk. We distribute each task to three workers. Suppose at least two workers agree that the video content is only strongly related to one of the two subtitles, and the subtitle is the corresponding subtitle; in that case, we regard the video clip and corresponding parallel subtitles as video-helpful data. Other techniques to improve crowdsourcing quality can be found in Appendix B.4. As a result, we constructed a video-helpful evaluation set containing 4, 276 Ja-En parallel subtitle pairs, 2, 940 Zh-En parallel subtitle pairs, and corresponding video clips. And we equally divide this set into a validation set and a test set.\nWe calculated the inter-annotator reliability to evaluate the crowdsourcing results. Krippendorff\u2019s alpha (Krippendorff, 2011) is 0.681 and 0.728 for Ja-En and Zh-En. Both of them achieve the substantial agreement (Hughes, 2021)."
        },
        {
            "heading": "3.2 Dataset analyses",
            "text": "Table 2 shows the splits of EVA. The evaluation set contains many kinds of ambiguities. We checked 50 samples. The most frequent causes are omission, emotion, and polysemy, with approximate proportions of 30%, 30%, and 20%. For the remaining, it is difficult to define the causes of ambiguity. Sometimes, instances of ambiguity arise from a combination of various factors, thereby challenging precise classification. For example, \u201c\u653e\u305b!\"\u201d can be translated as both \u201cLet me go!\u201d and \u201cDrop it!.\u201d This ambiguity could arise due to the polysemy \u201c\u653e\u305b!\u201d or it could stem from the omission of the object. And sometimes, it is difficult to tell whether\na subtitle is ambiguous by just checking the source subtitles, and the ambiguity can be easier understood by comparing the different target subtitles. For example, \u201c\u805e\u3053\u3048 (hear)\u307e\u3059\u304b (can) ?\u201d is usually translated into \u201c Can you hear me?\u201d. However, if a person is on the phone and this is his first sentence, \u201cIs there anyone?\u201d is more natural."
        },
        {
            "heading": "4 Model",
            "text": "We present a new model, SAFA, for VMT based on the EVA dataset. Previous VMT models mainly focus on caption translation based on the existing video caption datasets. In contrast, our model mainly focuses on subtitle translation. We design SAFA based on a selective attention model, propose a frame attention loss to make the model focus on the central frames where the subtitles occur, and use ambiguity augmentation to make the model put more weight on the possibly-ambiguous data. The model overview is shown in Figure 4."
        },
        {
            "heading": "4.1 Selective attention model",
            "text": "There are few existing VMT models, and most of them are based on LSTM or GRU models (Wang et al., 2019; Gu et al., 2021). The selective attention model is an IMT model recently proposed in (Li et al., 2022a), which is based on the Transformer model and has the advantage of simplicity and efficiency. We replace the image feature extraction models with video feature extraction models to fit the VMT task. The model mainly consists of the following five modules.\nText transformer encoder. The text transformer encoder follows the transformer encoderdecoder paradigm(Vaswani et al., 2017). The input is the source text, and the output is the text representation.\nVideo feature extraction model. The video feature extraction model\u2019s input is the video frames, and the output is the video feature.\nSelective attention. Given the text representation Htext and the video feature Hvideo, the selective attention mechanism is a single-head attention network to correlate words with video frames, where the query, key, and value are Htext, Hvideo and Hvideo, respectively. The selective attention output Hvideoattn can be defined as:\nHvideoattn = Softmax\n( QKT\u221a\ndk\n) V (1)\nwhere dk is a scaling factor.\nGated fusion The gated fusion mechanism is a popular technique for fusing representations from different sources (Wu et al., 2021; Fang and Feng, 2022; Lin et al., 2020; Yin et al., 2020). The fused output is a weighted sum between the text representation and the selective attention output, in which the weight is controlled with the gate \u03bb. The gate \u03bb \u2208 [0, 1] and the fused output can be defined as:\n\u03bb = Sigmoid ( UHtext + V Hvideoattn ) (2)\nHout = (1\u2212 \u03bb) \u00b7Htext + \u03bb \u00b7Hvideoattn (3)\nwhere U and V are trainable variables. Then, the fused output Hout is fed into the decoder.\nTransformer decoder The transformer decoder also follows the transformer encoder-decoder paradigm. The difference is that the cross-attention block uses the fused output instead of the text representation as key and value.\nThe loss function LO is cross entropy loss with label smoothing (Szegedy et al., 2015)."
        },
        {
            "heading": "4.2 Frame attention loss",
            "text": "Unlike captions which describe the entire video clip, subtitles only occur for a few seconds in a video clip. Generally, the frames close to the subtitles are more associated with the subtitles and thus provide more information associated with the subtitles. Therefore, we hope the model can pay more attention to the frames close to the subtitles. In EVA, the subtitles occur in the center of the video clips because we have aligned the subtitles to the\nvideos. Inspired by (Li et al., 2020), we propose a frame attention loss that uses Gaussian distribution to guide the attention on video features.\nFor a random variable X \u223c N ( \u00b5, \u03c32 ) in which \u00b5 = 1 and \u03c3 = 1, we define fX(x) as the probability density function of X . We define z = [z1, z2, . . . zM ] in which z1 to zM are M points equally spaced from \u2212a to a and M is the number of frames in a video feature. Then the frame attention loss is defined as the Kullback\u2013Leibler (KL) divergence between the frame attention defined in Eq. (1) and the Gaussian distribution with the softmax temperature (Hinton et al., 2015) mechanism:\nLG = KL ( Softmax ( QKT\u221a\ndk\n) \u2225 Softmaxt (fX(z)) ) (4)\nwhere KL() is the KL divergence function and Softmaxt is the softmax temperature mechanism:\nSoftmaxt (x) = exp (x/T )\u2211 j exp (xj/T )\n(5)\nwhere T \u2208 (0,\u221e) is a temperature parameter. When T gets smaller, the distribution tends to a Kronecker distribution (and is equivalent to a onehot target vector), and the model will pay more attention to the central frames; when T gets larger, the distribution tends to a uniform distribution, and the model will pay equal attention to all the frames. As we can substantially change the uniformity of the distribution by adjusting T , we fix a = 3 to reduce the number of hyperparameters.\nWith the frame attention loss, the loss function can be defined as:\nL = LO + \u03b3LG (6)\nwhere \u03b3 is a hyperparameter."
        },
        {
            "heading": "4.3 Ambiguity augmentation",
            "text": "In VMT, the video can help with translation only when the source subtitles are ambiguous. Therefore, we hope the model puts more weight on the data with ambiguous source subtitles.\nFor a VMT dataset X = {x1, x2, . . . xN}, xi is data consisting of a pair of parallel subtitles and a corresponding video clip. We divide the dataset into possibly-ambiguous dataset Xa = {xa1, xa2, . . . xaP } and possibly-unambiguous dataset Xu = { xu1 , x u 2 , . . . x u Q } according to whether the source subtitle of xi is in a translation set or not, where P and Q are the numbers of possibly-ambiguous data and possiblyunambiguous data respectively. The loss function of the selective attention model is defined as:\nLO = 1 N N\u2211 i=1 Lxi (7)\nwhere N is the number of data and Lxi is the loss of the data xi. Then, with ambiguity augmentation, the loss function is defined as:\nL = w 1 P P\u2211 i=1 Lxui + 1 Q Q\u2211 i=1 Lxai (8)\nwhere w > 1 is a weight to increase the loss of possibly-ambiguous data, making the model put more weight on the possibly-ambiguous data."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Settings",
            "text": "We conducted experiments with the Transformer configuration following (Li et al., 2022a). We train the Transformer from scratch without using external text data. For the video feature extraction model, we used CLIP4Clip (Luo et al., 2022). More details can be found in Appendix C.\nWe adopted BLEU (Papineni et al., 2002; Post, 2018) and METEOR (Banerjee and Lavie, 2005) as the evaluation metrics. Subtitles are essentially dialogues that are often short. Therefore we introduced the METEOR score in addition to BLEU.\nFor experiments in Sections 5.2 and 5.3, we repeated the experiment five times for each setting, discarded the maximum and minimum scores, and then took the average of the remaining three scores as a result. For experiments in Section 5.4, we used\nthe results of single experiments following the Spatial HAN model (Gu et al., 2021). Moreover, we reported the statistical significance of BLEU using bootstrap resampling (Koehn, 2004) over a merger of three test translation results."
        },
        {
            "heading": "5.2 Compare with SOTA",
            "text": "So far, there are very few VMT models. Because the Spatial HAN model (Wang et al., 2019) is not available, we use the publicly available VMT model proposed in VATEX (Wang et al., 2019) as a baseline. Existing VMT models (Wang et al., 2019; Gu et al., 2021) are mainly based on LSTM or GRU NMT model, while our model is based on the Transformer NMT model. As shown in Table 3, even the text-only model alone, which is the standard text-only Transformer model, performs much better than the previous VMT models. Therefore we focus on the comparison between our model and the text-only model. Compared to the text-only model, SAFA has comparable parameters while demonstrating significant performance gains. Specifically, for Zh-En translation, the achieves 9.99% improvement in BLEU score (absolute: 1.40) and 2.75% improvement in METEOR score (absolute: 0.96). Furthermore, for JaEn translation, the achieves 4.94% improvement in BLEU score (absolute: 1.30) and 3.28% improvement in METEOR score (absolute: 1.55).\nFurthermore, we did a context translation experiment following (Tiedemann and Scherrer, 2017) to check whether local contextual information can help disambiguate the source subtitles. We combine each subtitle with the two subtitles before and after it to construct context data and train the text-only model on context data. As shown in Table 3, the text-only model using context data performs similarly to using single subtitle data. The reason why context does not enhance translation performance may be the lack of speaker identity. The results indicate that visual information is more helpful than local contextual information in this translation task."
        },
        {
            "heading": "5.3 Effectiveness of the two proposed methods",
            "text": "In the SAFA block of Table 3, we conducted ablation studies by removing the frame attention loss (w/o Frame Attn), ambiguity augmentation (w/o Ambi Aug), and both (w/o Both). We can see that w/o Frame Attn decreases the performance more than w/o Ambi Aug, and w/o Both further decreases significantly. SAFA w/o Both (i.e., the\nselective attention model) is characterized by its straightforwardness, but it does not account for the significance of the central frames where subtitles occur, nor does it allocate additional attention to ambiguous data. Therefore, it cannot take full advantage of the video information."
        },
        {
            "heading": "5.4 Effectiveness of SAFA on other VMT datasets",
            "text": "Considering the How2 dataset is an instruction subtitle dataset with long videos while VATEX is a caption dataset, we conducted additional experiments on the VISA (Li et al., 2022b) dataset that is also a subtitle dataset to test the model\u2019s generalization ability across datasets. Table 4 shows the results. While the Spatial HAN model is not available, we obtained the translation results on VISA for comparison.\nIt\u2019s worth noting that the Spatial HAN model is based on the winning model in the VMT Challenge competition,1 which is a GRU model. In contrast, the VATEX model is based on LSTM. As a result, there is a significant performance gap between the Spatial HAN and VATEX models. Because VISA is relatively small and thus more appropriate for models based on GRU instead of Transformer, our text-only model performed similarly to the Spatial HAN model (Gu et al., 2021) model. We can see that the SAFA model performs significantly better than other models, although the improvements\n1https://competitions.codalab.org/competitions/24384\nare not as large as that on EVA. On the one hand, due to the size of the VISA dataset, it only contains a small number of translation sets and a small number of possibly-ambiguous data. We think that the performance improvement of the model mainly comes from the frame attention loss method. On the other hand, the videos in VISA\u2019s evaluation set do not necessarily contribute to disambiguation."
        },
        {
            "heading": "5.5 Case Study",
            "text": "Finally, we compare several real cases to see how visual information helps translation. We choose text-only and SAFA translation results. Figure 5 shows four qualitative examples in which the VMT model uses visual information to improve the translation. Generally, as subtitles from movies and TV episodes are usually short, video can help promote the interpretation of the subtitles. In the first example of omission ambiguity, the man speaks to the woman while looking at her. Therefore, we can infer that the subject pronoun \u201cyou\u201d is omitted in the source caption rather than an object or scene. In the second example, two men in suits are talking. Considering politeness, the address should be added. And considering the gender, the address should be \u201csir.\u201d In the third example, the video shows the door is closed, so the VMT model did a more generative translation. In the last example of emotion ambiguity, the woman shows a surprised\nexpression, and it is clear that the woman is not talking to someone else according to other frames. Therefore, the source subtitle should be translated as an expression of surprise rather than a statement that another person is lying. In this case, the VMT model does not translate correctly, but its translation contains emotional information. Appendix D shows the frame attention of the examples."
        },
        {
            "heading": "6 Conclusion",
            "text": "The paper introduced a new VMT dataset called EVA, which contains an extensive training set and a video-helpful evaluation set in which videos are guaranteed to be helpful for disambiguating source subtitles. In addition, we proposed a novel VMT model called SAFA that incorporates selective attention with frame attention loss and ambiguity augmentation. Experiments on EVA demonstrated that visual information and the proposed methods can boost translation performance, and SAFA performs significantly better than previous VMT models. We hope that this work will inspire further research in this field."
        },
        {
            "heading": "7 Limitations",
            "text": "The main limitations of our dataset are the following. First, the subtitles are from the OpenSubtitles dataset and provided by volunteers, which ensures the size of the dataset but results in the dataset containing some low-quality parallel subtitles. We used methods such as cross-lingual similarity to filter high-quality parallel subtitles when constructing the evaluation set but could not wholly remove lowquality subtitles. Second, each task in the crowdsourcing is only distributed to three workers due to financial constraints. If we distribute the tasks to more workers and expand the crowdsourcing scale, it is possible to obtain a higher-quality evaluation set."
        },
        {
            "heading": "8 Ethical Statements",
            "text": "We aim to facilitate MMT, so our dataset and codes will be publicly released. The subtitles utilized in this study are collected from publicly available datasets, while the videos are extracted from movies and TV episodes. We only use 10-second video clips associated with subtitles to address copyright concerns, with the audio removed. We will require all users to provide their academic affiliation as a condition to access the data. Besides, we may ask users intending to access our data to\nprovide a self-declaration that the data is to be used solely for research purposes."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "This work was supported by Google Research Scholar Award and JSPS KAKENHI Grant Number JP23H03454."
        },
        {
            "heading": "A Other Experiments",
            "text": "A.1 Compare with IMT method To compare the performance against image-based methods, we conducted IMT experiments using the Selective Attention model. In this setting, we extracted the central frame from each video clip and obtained the DETR (Dai et al., 2021) image feature of these frames to guide translations. For Ja-En translation, the BLEU score and METEOR score are 14.62 and 34.93, respectively. For Zh-En translation, the BLEU score and METEOR score are 26.36 and 47.58, respectively. Compared with the results in Table 3, the IMT method has better performance than the text-only method but not as good as the VMT method using CLIP4Clip features (w/o Both). Although the central frames are strongly associated with the subtitles, they cannot adequately capture the contextual information necessary for interpreting the subtitles. Sometimes the central frame may solely display the speaker\u2019s face, while relevant information, such as the object referred to in the subtitle, may appear before or after it.\nA.2 Effect on randomly divided evaluation set To check the performance of the model on a test set with the original distribution, we conducted experiments on the Ja-En part of EVA with a randomly divided training set and evaluation set instead of using a video-helpful evaluation set. The size of each set was equivalent to that of EVA. The BLEU and METEOR scores of the text-only model are 13.64 and 41.58, respectively, while those of SAFA are 13.77 and 41.44, respectively. We can see that the two models have similar performance. Many samples in the randomly divided test set do not require disambiguation. Therefore, videos can not significantly help the model improve its performance.\nA.3 Experiments on other VMT evaluation set\nWe conducted experiments on VISA\u2019s training set and EVA\u2019s evaluation set to check the necessity of the EVA\u2019s training set. The BLEU and METEOR scores of the SAFA model are 6.77 and 25.17, respectively. Both are significantly lower than the SAFA results in Table 3. The results indicate that EVA\u2019s training set is more helpful.\nA.4 Other evaluation scores\nWe calculated the CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) scores for the main results. The results are shown in Table 5. Since we focus on the subtitles translation task instead of the video caption generation task, we add the results as a reference."
        },
        {
            "heading": "B Additional Details for Dataset",
            "text": "B.1 Fix subtitle timestamps\nThe subtitle timestamps collected from the OpenSubtitles dataset are provided by volunteers and may not match the video. Therefore we need to align subtitles to videos to fix the timestamps. In practice, we use alass2 to align subtitles to videos. Alass can perform subtitles alignment in two ways. One is to align subtitle files with incorrect timestamps to subtitle files with correct timestamps, such as those extracted from videos. The other is to align the incorrect subtitle file with the corresponding video using voice activity detection (VAD). We combine the two methods to do alignment and manually check the results. In this way, we make sure that the timestamps of the subtitles correspond exactly to the time when the subtitles appear.\n2https://github.com/kaegi/alass\nB.2 Size distribution of translation sets\nWe examined the size distribution of translation sets in the Ja-En and Zh-En parts, separately. The result is shown in Figure 6.\nB.3 Select ambiguous translation sets\nWe use sent-BERT (Reimers and Gurevych, 2019) to calculate the target subtitles similarity and use the cross-lingual version of sent-BERT to calculate the parallel subtitles similarity. Sent-BERT is one of the best methods to evaluate semantic similarity.\nIn each translation set, for parallel subtitles with similarities higher than a threshold Tp, we select an ambiguous translation set if and only if the similarity between the two most different target subtitles is lower than a threshold Tt. Otherwise, we lower the Tp, and repeat the process above. Finally, we collect all the ambiguous translation sets under each Tp as a result. Note that we select at most one ambiguous translation set containing two target subtitles from each translation set. Therefore the remaining data of translation sets are retained in the training set.\nWe set Tt and Tp separately. To set the threshold Tt, we do experiments on 100 randomly selected translation sets. We generate a ground truth by manually checking whether the source subtitle of each translation set is ambiguous and calculate the precision and recall under different Tt. As we do crowdsourcing later to collect final results, recall is more important than precision in this step. We set Tt = 0.3 with recall 0.56 and precision 0.38. Similarly, we do experiments with 200 randomly selected parallel subtitles to set Tp. When we only keep parallel subtitles with sentence similarity higher than 0.3, recall is 1.00 while precision is 0.90. We relax Tp from 0.8 to 0.3 with 0.1 interval in sequence.\nB.4 Crowdsourcing\nThe crowdsourcing interface is shown in Figure 7. In the instructions, we tell workers that the subtitles occur in the center of the video clips. Because the language of most video clips is English, the workers may choose the subtitle according to the character\u2019s lip movements. Therefore we especially tell the workers to choose based on the content of the videos rather than the characters\u2019 lip movement. Moreover, we state that if the video only shows some people talking with each other and nothing special, it should not be regarded as strongly related to any subtitle.\nWe use qualification tests to further improve the quality of crowdsourcing. Specifically, we set qualification test tasks to check whether workers can answer them correctly. Then we only distribute large-scale tasks to workers who are good at answering this kind of task."
        },
        {
            "heading": "C Detailed Experimental Setup",
            "text": "The Transformer model consists of 4 encoder and decoder layers. The input/output layer dimension is 128, and the inner feed-forward layer dimension is 256. There are 4 heads in the multi-head selfattention mechanism. We set the dropout as 0.3 and the label smoothing as 0.1.\nWe searched for the hyperparameters separately for the frame attention loss method, the ambiguity augmentation method, and the combination method (SAFA). For the SAFA model, we set T = 1, \u03b3 = 0.5, and w = 2. For the frame attention loss method only, we set T = 1 and \u03b3 = 1. For the ambiguity augmentation method only, we set w = 2.\nOur implementation was based on Fairseq (Ott et al., 2019). For training, we used Adam Optimizer (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22128. We adopted the same learning rate schedule as (Vaswani et al., 2017), where the learning rate first increased linearly for warmup = 2, 000 steps from 1e\u22127 to 5e\u22123. After the warmup, the learning rate decayed proportionally to the inverse square root of the current step. Each training batch contained 16, 000 tokens. We also adopted the early-stop training strategy (Zhang et al., 2020) to avoid the overfitting issue.\nWe used Juman++ (Tolmachev et al., 2018), Stanford CoreNLP (Manning et al., 2014), and Moses (Koehn et al., 2007) to tokenize Japanese, Chinese, and English subtitles, respectively. On\nthe EVA dataset, we mapped tokens appearing less than three times to unknown. As a result, in JaEn translation, the Japanese vocabulary contains 38, 516 tokens while the English vocabulary contains 35, 540 tokens. In Zh-En translation, the Chinese vocabulary contains 34, 860 tokens while the English vocabulary contains 27, 028 tokens. On the VISA dataset, we used all tokens to build the vocabulary. As a result, the Japanese vocabulary contains 17, 676 tokens while the English vocabulary contains 17, 732 tokens. Therefore the number of model parameters in Tables 3 and 4 are different."
        },
        {
            "heading": "D Frame Attention Analysis",
            "text": "We show some examples of frame attention. We show four examples from the case study section (Section 5.5) and one additional example. The examples are shown in Figures 8, 9, 10, 11, and 12. In the source sentence, each token has its own attention on different frames. In the first two examples, most frames can help disambiguate the source subtitles. The model pays more attention to the first half of the video clips. In the third example, the token \u201c\u9589\u3081\u308d (close)\u201d pay more attention to the fourth and fifth frames, which contain the door. Therefore, the model translates the source subtitle to \u201c Close the door\u201d instead of closing other things. In the fourth example, both tokens pay much attention to the third and fifth frames. Especially the fifth frame shows the surprised expression of the woman. Therefore, the source subtitle is translated as an expression of surprise. In the last example,\nas the man says the sentence to a dog and he does not point to a special position, the source subtitle should be translated to \u201cget away.\u201d In this example, the model pays more attention to the fourth, fifth, and sixth frames containing the dog.\nThe ambiguity of the first three examples is caused by omission, and the fourth is caused by emotion. The ambiguity of the last example is not caused by omission or emotion, and we approximately classify it as an ambiguity caused by the polysemy \u201c\u3044\u3051 (go).\u201d"
        }
    ],
    "title": "Video-Helpful Multimodal Machine Translation",
    "year": 2023
}