{
    "abstractText": "Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to \u201challucinate\u201d content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusionin-Decoder-based (Izacard and Grave, 2021b) reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yunxiang Zhang"
        },
        {
            "affiliations": [],
            "name": "Muhammad Khalifa"
        },
        {
            "affiliations": [],
            "name": "Lajanugen Logeswaran"
        },
        {
            "affiliations": [],
            "name": "Moontae Lee"
        },
        {
            "affiliations": [],
            "name": "Honglak Lee"
        },
        {
            "affiliations": [],
            "name": "Lu Wang"
        }
    ],
    "id": "SP:797962b585e819536688e85e20f1e58c3846fe1a",
    "references": [
        {
            "authors": [
                "aoyu Feng",
                "Vlad Fienber",
                "Markus Freitag",
                "Xavier Garcia",
                "Sebastian Gehrmann",
                "Lucas Gonzalez"
            ],
            "title": "Palm 2 technical report",
            "year": 2023
        },
        {
            "authors": [
                "Akari Asai",
                "Matt Gardner",
                "Hannaneh Hajishirzi."
            ],
            "title": "Evidentiality-guided generation for knowledge-intensive NLP tasks",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang."
            ],
            "title": "Semantic parsing on freebase from question-answer pairs",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October",
            "year": 2013
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading wikipedia to answer opendomain questions",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -",
            "year": 2017
        },
        {
            "authors": [
                "Hung-Ting Chen",
                "Michael J.Q. Zhang",
                "Eunsol Choi."
            ],
            "title": "Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick S.H. Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv., 55(12).",
            "year": 2023
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Jun Araki",
                "Haibo Ding",
                "Graham Neubig."
            ],
            "title": "How can we know When language models know? on the calibration of language models for question answering",
            "venue": "Trans. Assoc. Comput. Linguistics, 9:962\u2013977.",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S. Weld",
                "Luke Zettlemoyer."
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL",
            "year": 2017
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick S.H. Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "Variants of the hungarian method for assignment problems",
            "venue": "Naval research logistics quarterly, 3(4):253\u2013258.",
            "year": 1956
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Trans. Assoc. Comput. Linguistics, 7:452\u2013 466.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August",
            "year": 2019
        },
        {
            "authors": [
                "Kyungjae Lee",
                "Seung-won Hwang",
                "Sang-eun Han",
                "Dohyeon Lee"
            ],
            "title": "Robustifying multi-hop QA",
            "year": 2021
        },
        {
            "authors": [
                "Nayeon Lee",
                "Wei Ping",
                "Peng Xu",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Bryan Catanzaro."
            ],
            "title": "Factuality enhanced language models for open-ended text generation",
            "venue": "CoRR, abs/2206.04624.",
            "year": 2022
        },
        {
            "authors": [
                "Daliang Li",
                "Ankit Singh Rawat",
                "Manzil Zaheer",
                "Xin Wang",
                "Michal Lukasik",
                "Andreas Veit",
                "Felix X. Yu",
                "Sanjiv Kumar."
            ],
            "title": "Large language models with controllable working memory",
            "venue": "CoRR, abs/2211.05110.",
            "year": 2022
        },
        {
            "authors": [
                "Junlong Li",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Self-prompting large language models for opendomain QA",
            "venue": "CoRR, abs/2212.08635.",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Skyler Hallinan",
                "Ximing Lu",
                "Pengfei He",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Yejin Choi."
            ],
            "title": "Rainier: Reinforced knowledge introspector for commonsense question answering",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Generated knowledge prompting for commonsense reasoning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Shayne Longpre",
                "Kartik Perisetla",
                "Anthony Chen",
                "Nikhil Ramesh",
                "Chris DuBois",
                "Sameer Singh."
            ],
            "title": "Entity-based knowledge conflicts in question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Kaixin Ma",
                "Hao Cheng",
                "Yu Zhang",
                "Xiaodong Liu",
                "Eric Nyberg",
                "Jianfeng Gao."
            ],
            "title": "Chain-of-skills: A configurable model for open-domain question answering",
            "venue": "CoRR, abs/2305.03130.",
            "year": 2023
        },
        {
            "authors": [
                "Lucie Charlotte Magister",
                "Jonathan Mallinson",
                "Jakub Ad\u00e1mek",
                "Eric Malmi",
                "Aliaksei Severyn."
            ],
            "title": "Teaching small language models to reason",
            "venue": "CoRR, abs/2212.08410.",
            "year": 2022
        },
        {
            "authors": [
                "Alex Mallen",
                "Akari Asai",
                "Victor Zhong",
                "Rajarshi Das",
                "Hannaneh Hajishirzi",
                "Daniel Khashabi."
            ],
            "title": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "venue": "CoRR, abs/2212.10511.",
            "year": 2022
        },
        {
            "authors": [
                "Alex Mallen",
                "Akari Asai",
                "Victor Zhong",
                "Rajarshi Das",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
            "venue": "Proceedings of the 61st Annual Meeting of",
            "year": 2023
        },
        {
            "authors": [
                "Ella Neeman",
                "Roee Aharoni",
                "Or Honovich",
                "Leshem Choshen",
                "Idan Szpektor",
                "Omri Abend."
            ],
            "title": "Disentqa: Disentangling parametric and contextual knowledge with counterfactual question answering",
            "venue": "CoRR, abs/2211.05655.",
            "year": 2022
        },
        {
            "authors": [
                "Barlas Oguz",
                "Xilun Chen",
                "Vladimir Karpukhin",
                "Stan Peshterliev",
                "Dmytro Okhonko",
                "Michael Sejr Schlichtkrull",
                "Sonal Gupta",
                "Yashar Mehdad",
                "Scott Yih"
            ],
            "title": "Unik-qa: Unified representations of structured and unstructured knowledge",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Liangming Pan",
                "Wenhu Chen",
                "Min-Yen Kan",
                "William Yang Wang."
            ],
            "title": "Contraqa: Question answering under contradicting contexts",
            "venue": "CoRR, abs/2110.07803.",
            "year": 2021
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feed",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick S.H. Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard",
                "Vassilis Plachouras",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel"
            ],
            "title": "KILT: a benchmark",
            "year": 2021
        },
        {
            "authors": [
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Denny Zhou"
            ],
            "title": "2022. Recitation-augmented language",
            "year": 2022
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena D. Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi."
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "CoRR,",
            "year": 2019
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Xiang Lorraine Li",
                "Srini Iyer",
                "Jingfei Du",
                "Patrick S.H. Lewis",
                "William Yang Wang",
                "Yashar Mehdad",
                "Scott Yih",
                "Sebastian Riedel",
                "Douwe Kiela",
                "Barlas Oguz."
            ],
            "title": "Answering complex opendomain questions with multi-hop dense retrieval",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William W. Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Em-",
            "year": 2018
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "11th International Con-",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Yu",
                "Chenguang Zhu",
                "Zaitang Li",
                "Zhiting Hu",
                "Qingyun Wang",
                "Heng Ji",
                "Meng Jiang."
            ],
            "title": "A survey of knowledge-enhanced text generation",
            "venue": "ACM Comput. Surv., 54(11s):227:1\u2013227:38.",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "CoRR, abs/2210.03493.",
            "year": 2022
        },
        {
            "authors": [
                "Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen."
            ],
            "title": "A survey of large language models",
            "venue": "CoRR, abs/2303.18223.",
            "year": 2023
        },
        {
            "authors": [
                "G(Q",
                "PL). Yu"
            ],
            "title": "2023) also explore a simple approach (Figure 2a) of directly merging passages from both sources together: A",
            "year": 2023
        },
        {
            "authors": [
                "B COMBO"
            ],
            "title": "Dataset Details We use three single-hop QA datasets (NaturalQuestions (Kwiatkowski et al., 2019)",
            "venue": "TriviaQA (Joshi et al.,",
            "year": 2013
        },
        {
            "authors": [
                "Asai"
            ],
            "title": "Gb for consistency label mining",
            "year": 2022
        },
        {
            "authors": [
                "Rhona Mitra"
            ],
            "title": "Charlotte in the fourth season of \"The Strain\" TV Series. in 2018, Mitra was cast as Mercy Graves in The CW television series .",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Open-domain question answering (QA) typically requires models to consolidate and reason over information from external knowledge sources (Chen et al., 2017; Petroni et al., 2021). A common retrieve-then-read framework (Izacard and Grave, 2021a,b; Izacard et al., 2022; Karpukhin et al., 2020) would first fetch relevant passages from an external corpus and pass them to a reader model to arrive at an answer. Retrieving from reliable\n\u2217Correspondence to yunxiang@umich.edu 1Our code is publicly available at https://github.com/\nyunx-z/COMBO."
        },
        {
            "heading": "NaturalQuestions (Single-hop QA)",
            "text": "1. Gail Matthius is an American actress \u2026 She also co-anchored the Weekend Update segment of the show \"Saturday Night Live\" \u2026 2. Robert Davi \u2026 is perhaps best known for his roles in movies such as \u2026 \"The Goonies\u201d \u2026 where he played the character Nicholas Andre.\nsources, e.g., Wikipedia, enjoys the benefits of being factual, but may suffer from incomplete knowledge coverage and contain irrelevant information.\nLarge Language Models (LLMs) have been shown to store a wide range of knowledge in their parameters, which can serve as an alternative information source (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Ouyang et al., 2022). Capitalizing on the success of leveraging LLMs\u2019 parametric knowledge for natural language understanding tasks, a new paradigm known as generatethen-read (Yu et al., 2023) has emerged. It prompts an LLM to generate contextual passages for a question in lieu of retrieval from static corpora. Compared to retrieved passages, LLM-generated texts are more relevant to the question, as its generative nature implicitly optimizes for content relevance. Yet, they frequently contain factual errors due to hallucinations (Ji et al., 2023; Peng et al., 2023).\nThis work aims to address how to combine retrieved and parametric knowledge to get the best of both worlds for open-domain QA. Specifically,\nwe want to maintain the factuality of retrieved knowledge while leveraging the relevance of LLM knowledge. Yu et al. (2023) have explored a simple merging approach by separately encoding the two types of passages into a Fusion-in-Decoder (Izacard and Grave, 2021b) reader (Figure 2a). While this setting outperforms using either source alone, the direct merging approach ignores inconsistent facts between the sources, which can easily confuse the reader model. Figure 1 shows an example of knowledge conflicts where LLM-generated passage contains fabrication, contradicting the retrieved knowledge.2\nTo address this challenge, we propose a novel Compatibility-Oriented knowledge Merging for Better Open-domain (COMBO) QA framework. Intuitively, an answer tends to be correct if it is supported by information from both sources. Therefore, we promote the model\u2019s usage of factual and relevant information by combining retrieved and generated knowledge into compatible pairs, which are then fed into a reader module.\nTo date, there has been no gold-standard annotation for training a compatibility scorer for passage pairs. To this end, we introduce a novel method that automatically mines silver labels of compatibility at scale, without the need for human annotation or dataset-specific heuristics. Specifically, we estimate the silver compatibility by checking whether the prediction correctness of a QA model would flip if one or both passages from a target pair were to be removed from the input. Afterward, we train discriminators on silver labels to compute passage pairs\u2019 compatibility scores.\nLastly, we benchmark COMBO in a fullysupervised setting on four popular open-domain QA datasets, including both single-hop (NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), WebQuestion (Berant et al., 2013)) and multi-hop questions (HotpotQA (Yang et al., 2018)). Using state-of-theart retrievers, DPR (Karpukhin et al., 2020) and MDR (Xiong et al., 2021), as well as performant LLMs, e.g., InstructGPT (Ouyang et al., 2022) and ChatGPT, COMBO outperforms competitive baselines on all testbeds except HotpotQA by up to +1.9 exact match score.\nTo summarize, the main contributions of our work are three-fold:\n2Figure 8 in Appendix A shows another example of knowledge conflict on HotpotQA (Yang et al., 2018).\n\u2022 We introduce COMBO, a novel compatibilityoriented framework for merging LLM knowledge and retrieved knowledge in open-domain QA.\n\u2022 We automatically mine silver labels for training compatibility scorers without human annotations.\n\u2022 We demonstrate the effectiveness of our framework with extensive experiments and analyses over four open-domain QA datasets."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "Parametric and Retrieved Knowledge for QA.",
            "text": "QA models commonly have access to two knowledge sources: parametric knowledge stored in language models and retrieved knowledge from external corpora. Previous work (Chen et al., 2022; Longpre et al., 2021; Pan et al., 2021) focuses on analyzing knowledge conflicts in simulated settings by perturbing retrieved contexts, e.g., replacing entities with incorrect counterparts. These studies reveal that reader models overly rely on the parametric knowledge and demonstrate limited abilities of resolving conflicting information. In contrast, we propose solutions to repress models from using generated knowledge that contradicts with retrieved contexts. Other works (Li et al., 2022a; Mallen et al., 2022; Neeman et al., 2022) leverage predefined rules to guide the models to choose between parametric vs. retrieved knowledge sources under conflicts. Instead of relying on dataset-specific heuristics, we introduce a generalizable approach that teaches QA models to prioritize compatible information over conflicting ones.\nAugmentation with Large Language Model Outputs. LLMs, such as InstructGPT (Ouyang et al., 2022) or PaLM (Chowdhery et al., 2022), store rich world knowledge in their model weights and demonstrate impressive performance on opendomain QA tasks with prompting techniques. Recent studies further boost their performance by eliciting supporting evidence before answer prediction. The types of supporting evidence include encyclopedia knowledge (Li et al., 2022b; Sun et al., 2022; Yu et al., 2023), commonsense knowledge (Liu et al., 2022a,b; Shwartz et al., 2020; Wang et al., 2022; West et al., 2022) and chain-of-thought reasoning steps (Magister et al., 2022; Trivedi et al., 2022; Wei et al., 2022; Zhang et al., 2022). Most\nxx\nR P 1\nLLM Psg 5Q Retrieved Psg 3 LLM Psg 3Q\nLLM Psg 4Q\nRetrieved Psg 4\nRetrieved Psg 5\nAFiD\n2. Pairwise Compatibility Matching 3. Predicting with Passage Pairs\n1. Compatibility Scoring\n\ud83c\udf54\nx xx\n \ud83c\udf54 LP \ud835\udc56 RP j\nConsistency Discriminator\nEvidentiality Discriminator\nLP 1\nLP 2\nLP 3\nLP 4\nLP 5\nR P 2\nR P 3\nR P 4\nR P 5\n\u2026 \u2026\nLLM Psg 1Q\nRetrieved Psg 1Q\nLLM Psg 2Q\nLLM Psg 5Q\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nRetrieved Psg 2Q\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nRetrieved Psg 5Q\nAFiD\n(a) Direct Merging (Yu et al., 2023)\nxx\nR P 1\nLLM Psg 5Q Retrieved Psg 3\nLLM Psg 3Q\nLLM Psg 4Q\nRetrieved Psg 4\nRetrieved Psg 5\nAFiD\n2. Pairwise Compatibility Matching 3. Predicting with Passage Pairs\n1. Compatibility Scoring\n\ud83c\udf54\nx xx\n \ud83c\udf54 LP \ud835\udc56 RP j\nConsistency Discriminator\nEvidentiality Discriminator\nLP 1\nLP 2\nLP 3\nLP 4\nLP 5\nR P 2\nR P 3\nR P 4\nR P 5\n\u2026 \u2026\nLLM Psg 1Q\nRetrieved Psg 1Q\nLLM Psg 2Q\nLLM Psg 5Q\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nRetrieved Psg 2Q\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nRetrieved Psg 5Q\nAFiD\n(b) COMBO (Ours)\nFigure 2: Overview of COMBO framework (right) that uses both LLM-generated passages and retrieved passages for open-domain QA. 1. We compute pairwise compatibility scores, which quantify the extent to which the two passages support each other regarding evidence related to the question. 2. We then match them into pairs by maximizing their overall compatibility (matched pairs are highlighted in red boxes). 3. Finally, passage pairs are sorted by their compatibility and fed to a FiD-based (Izacard and Grave, 2021b) reader model to produce an answer.\nrelevant to our work, Yu et al. (2023) directly augment retrieval passages with LLM-generated texts, but they do not take knowledge conflicts into account, which could mislead the model and degrade the prediction. Our work promotes the model\u2019s usage of factual evidence over hallucinating information in LLM outputs (Creswell and Shanahan, 2022; OpenAI, 2023; Peng et al., 2023; Zhao et al., 2023) with the help of retrieval knowledge.\nEvidentiality-guided QA. Traditionally, the field of Open-Domain QA has been dominated by retrieve-then-read models (Izacard et al., 2021; Izacard and Grave, 2021a; Karpukhin et al., 2020; Xiong et al., 2021). The performance of the readers largely depends on the evidentiality of the retrieved passages\u2014whether they contain the correct evidence to support the answer. Recent work (Asai et al., 2022; Fajcik et al., 2021; Lee et al., 2021) augments QA performance by adding a (silver) evidentiality signal of each retrieved passage to the training of reader models. However, their method is insufficient to work with LLM-generated passages that contain hallucination errors. We further incorporate the evidentiality of LLM-generated passages and leverage both sources to highlight the correct evidence for the reader."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we present COMBO, which combines both the retrieved passages and the LLM\u2019s parametric knowledge to improve open-domain QA performance over the existing retrieved-then-\nread and generate-then-read counterparts.3 The core idea is to feed paired passages\u2014one from an LLM, one from a retriever\u2014into a FiD-based reader model. The passages are paired in such a way that they provide consistent evidence to the question for the reader to identify both the factual and relevant information. Specifically, for each question, N retrieved passages and M LLMgenerated passages are given to COMBO. The answer is produced in three steps shown in Figure 2b.\n1. The compatibility scores of all possible passage pairs are computed according to an evidentiality discriminator and a consistency discriminator (\u00a73.1), trained using silver labeled data (\u00a73.2).\n2. A compatibility-oriented matching strategy selects passage pairs to maximize overall compatibility while balancing the usage of all passages (\u00a73.3).\n3. A FiD-based reader handles passage pairs, sorted by compatibility scores, to generate the final answer."
        },
        {
            "heading": "3.1 Defining Compatibility",
            "text": "We first describe two assumptions, based on a preliminary analysis provided in Appendix A.4, which lay the basis for the compatibility formulation.4\n3We provide a brief introduction to the retrieved-then-read and generate-then-read QA frameworks in Appendix A.2.\n4For simplicity, we illustrate our method under the singlehop QA setting in the main paper. Appendix A.3 shows how to adapt it to the multi-hop setting with minimal modifications.\nAssumption 1 Retrieved passages are faithful to real-world facts, i.e., being factual, regardless of its relevance to the question.\nAssumption 2 LLM-generated passages contain relevant evidence to the question, i.e., being plausible, irrespective of its factuality.\nFigure 1 shows an example of Assumption 2, where a plausible answer (\u201cAlexandra Breckenridge\u201d) is supported by the LLM-generated text despite its inconsistency with the retrieved passage.\nWith these two assumptions, when an answer can be evinced by both the retrieved passage and the LLM-generated passage, it is likely that the retrieved knowledge contains the relevant evidence whereas the generated knowledge is factual. Moreover, the answer tends to be correct in such situations. Therefore, we define the compatibility of a pair of passages as follows.\nDefinition 1 A passage pair is COMPATIBLE if both passages contain the proper evidence to support answering the question correctly.\nWith this concept, we hope to match two passages, one from each source, into compatible pairs. The pairs will facilitate a reader model by promoting the correct evidence when knowledge conflicts exist.\nCompatibility Formulation. To illustrate how we compute passage compatibility, we formulate the concept of compatibility with mathematical notations. Given a question Q, we use lpi to denote the i-th LLM-generated passage for Q, and rpj for the j-th retrieved passage. lpi \u22a8 Q means lpi contains the correct evidence for Q, and similarly for rpj \u22a8 Q. Following Definition 1, given Q, for a pair of LLM-generated and retrieved passages lpi and rpj , we formulate the compatibility score cQi,j as P (lpi \u22a8 Q, rpj \u22a8 Q), which could be further factorized into two components:5\ncompatibility score\ufe37 \ufe38\ufe38 \ufe37 P (lpi \u22a8 Q, rpj \u22a8 Q) = P (rpj \u22a8 Q)\ufe38 \ufe37\ufe37 \ufe38 evidentiality score \u00b7P (lpi \u22a8 Q | rpj \u22a8 Q)\ufe38 \ufe37\ufe37 \ufe38 consistency score . (1)\nP (rpj \u22a8 Q) measures whether the retrieved passage contains the correct evidence (i.e., evidential-\n5Notice that we do not factorize the compatibility score the other way round (i.e., P (lpi \u22a8 Q) \u00b7 P (rpj \u22a8 Q | lpi \u22a8 Q)). This is because lpi always contains plausible evidence to the question (Assumption 2) so the discriminator cannot measure the evidentiality of lpi on its own (P (lpi \u22a8 Q)). Instead, we use a retrieved factual passage (Assumption 1) to determine whether the lpi is hallucinating or not.\nity). P (lpi \u22a8 Q | rpj \u22a8 Q) characterizes the consistency between the retrieved and LLM-generated knowledge.\nBased on the decomposition in Equation 1, we train two binary discriminators accordingly: an evidentiality discriminator DE for modeling P (rpj \u22a8 Q) and a consistency discriminator DC for modeling P (lpi \u22a8 Q | rpj \u22a8 Q). Concretely, DE takes a question Q and the j-th retrieved passage rpj as input, and predicts whether rpj is evidential (positive) or non-evidential (negative). If rpj is evidential, DC will take Q, the i-th LLM-generated passage lpi, and rpj as input, and predicts whether (lpi, rpj) is consistent (positive) or conflicting (negative), i.e., the retrieved passage contains the correct evidence while the LLM-generated passage does not. As a result, a passage pair is considered compatible if it is both evidential and consistent."
        },
        {
            "heading": "3.2 Mining Silver Labels",
            "text": "We do not have human-annotated evidentiality and consistency labels in most existing datasets. To avoid costly manual annotation, we propose an approach to automatically mine silver labels for training the two discriminators. To collect silver evidentiality labels for training DE , we largely follow the leave-one-out generation approach by Asai et al. (2022). Briefly, a passage is deemed EVIDENTIAL if the prediction by a given QA model\nchanges from correct to incorrect after removing it from a pool of retrieved passages. We explain the details in Appendix C.1.\nWe then extend this idea to mine consistency labels of passage pairs for training DC , as demonstrated in Figures 3 and 9 (for negative sample, in Appendix C.2). Intuitively, if the prediction changes from correct to incorrect after removing either passage from the target pair (e.g., IV\u21d2II for an LLM-generated passage and I\u21d2II for a retrieved passage, as in Figure 3), both passages supposedly contain the correct evidence and they are therefore a CONSISTENT pair. Concretely, for each dataset, we first finetune a base reader model initialized from Fusion-in-Decoder (Izacard and Grave, 2021b) with top-N retrieved passages PR = {rp1, rp2, ..., rpN} as input.6 We then label the consistency of (lpi, rpj) based on the correctness of predictions with four types of inputs: I. all the retrieved passages (Q,PR) II. all the retrieved passages except the target retrieved passage (Q,PR\\{rpj}) III. all the retrieved passages plus the target LLM-generated passage (Q,PR \u222a {lpi}) IV. all the retrieved passages without the target retrieved passage and with the target LLM-generated passage (Q,PR \u222a {lpi}\\{rpj}). We consider (lpi, rpj) as consistent if the model\u2019s prediction is incorrect with II and all correct with I, III, and IV. Conversely, (lpi, rpj) is labeled as CONFLICTING if the prediction is correct with I and all incorrect with II, III, IV.7"
        },
        {
            "heading": "3.3 Matching Retrieved and LLM-generated Passages into Pairs",
            "text": "To signal the reader model with the connections between the two knowledge sources (compatible vs. conflicting), we adapt FiD to encode passage pairs as input. Each pair of passages is concatenated with the question, and processed independently from other pairs by the encoder. We add special tokens: \u201cquestion:\u201d, \u201cgenerated passage:\u201d and \u201cretrieved passage:\u201d before the question and the passages, respectively. In this way, the encoder could perform self-attention (Vaswani et al., 2017) across the retrieved and LLM-generated passages. We keep the relative order of LLM-generated and retrieved passage fixed across all input pairs, by always putting lpi in front of rpj (Figure 2b). This\n6In our experiments, we set N = 10 across all datasets. 7To speed up the consistency label mining process, we only need to obtain model predictions for III and IV if I is correct and II is incorrect (i.e., rpj is evidential).\ntrains the model to rely on the factual evidence that is always located in the latter passage when facing conflicting information.\nTo create passage pairs with a balance of factuality and coverage of supporting evidence, we design a compatibility-guided optimal matching strategy with the following two goals. First, for simplicity, we include all passages during the matching process, leaving it to future work to filter incompatible passage pairs. Second, we aim to maximize the compatibility of matched passage pairs, to minimize the adverse impact of knowledge conflicts on the reader model.\nTo do so, we first use the two discriminators together to compute scores for evaluating the compatibility of all possible pairwise combinations of the retrieved and LLM-generated passages of a question (i.e., {(lpi, rpj) | i \u2208 {1, 2, ...,M}, j \u2208 {1, 2, ..., N}}). This results in a 2-dimensional matrix shown in Figure 2b, where each element represents the compatibility score for a passage pair. A subset of all possible M \u00d7 N pairs (red boxes highlighted in the matrix of Figure 2b) are then selected as input to the reader model. The selection is solved as a bipartite graph maximum-weighted matching problem, with details in below paragraph. The algorithm requires that compatible pairs score higher than conflicting and non-evidential ones, so it can achieve the maximum overall compatibility of matched passage pairs while balancing the usage of all passages. To this end, we devise a simple heuristic for compatibility scoring, dubbed as evidentiality-cutoff :\ncQi,j = P (lpi \u22a8 Q | rpj \u22a8 Q) \u00b7 1{P (rpj\u22a8Q)>0.5} (2) where 1 is an indicator function. It binarizes the decision from DE to score non-evidential pairs as zero and prioritize compatible pairs over conflicting counterparts.\nCompatibility-guided Optimal Matching We view the compatibility-guided matching process as a maximum-weighted matching problem on a bipartite graph. Concretely, we treat each passage as a node and there are edges connecting LLM-passage-nodes with retrieved-passage-nodes, whose weights are the corresponding compatibility score. This results in a complete bipartite graph G = (PL,PR;E) where E = {(lpi, rpj) | i \u2208 {1, 2, ...,M}, j \u2208 {1, 2, ..., N}}. The weight function w : E \u2192 [0, 1] assigns compatibility score of\nEquation 2 as edge weight: w((lpi, rpj)) = c Q i,j . We want to find a perfect matching M of maximum weight where the weight of matching M is given by w(M) = \u2211 e\u2208Mw(e). This problem is typically solved by the Hungarian algorithm (Kuhn, 1955, 1956) in polynomial time. This matching is optimal in the sense that it covers all passages while maximizing the sum of their compatibility scores."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "We experiment with four open-domain QA datasets: NaturalQuestions Open (Kwiatkowski et al., 2019), TriviaQA unfiltered (Joshi et al., 2017), WebQuestion (Berant et al., 2013) for single-hop QA and HotpotQA full wiki setting (Yang et al., 2018) for multi-hop QA. We provide brief introductions and statistics for each dataset in Appendix B. We use Exact Match (EM) (Rajpurkar et al., 2016) as our major evaluation metric across all datasets. We run each experiment three times with different random seeds and report the average.8\nWe focus on the fully-supervised setting and obtain retrieved and LLM-generated passages for each question in the dataset. For retrieved passages, we use retrieval results by DPR (Karpukhin et al., 2020) for single-hop QA and MDR (Xiong et al., 2021) for multi-hop QA, where no gold-standard passages are used in this study. We do not further finetune the retriever. For LLM-generated passages, we use the ones provided by Yu et al. (2023)9 for the three single-hop QA datasets, which are based on InstructGPT. Since there are no available resources for LLM-generated passages on multi-hop QA, we obtain them by calling ChatGPT\u2019s API (gpt-35-turbo). We choose ChatGPT because it is a performant LLM with reasonable costs.\nWe employ FiD (Izacard and Grave, 2021b) as our reader model. For the discriminators DE and DC , we use RoBERTa-large (Liu et al., 2019) for single-hop QA and DeBERTa-large (He et al., 2021) for multi-hop QA. DeBERTa features relative positional embedding, making it possible to adapt to longer input for passages of multi-hop QA.\nIn our experiments, we train separate discriminators DE and DC for each dataset. However, given the limited amount of silver labels mined from the small-scale dataset WebQuestions, we warm up\n8For results of standard deviation under EM and F1 scores, please see Table 6 in Appendix D.1.\n9https://github.com/wyu97/GenRead.git\nthe training of its discriminators with data from the other two single-hop QA datasets. Note that the additional warm-up data is only used for training the discriminators but not the reader model, so as to ensure a fair comparison with the baselines. More details of the prompts for LLMs, model implementation and hyperparameters are included in Appendix C.2."
        },
        {
            "heading": "4.2 Results and Analysis",
            "text": "Comparison with Baselines. Table 1 shows experimental results on the four open-domain QA datasets. All methods use 10 retrieved and/or 10 LLM-generated passages for each question Q and FiD-large (770M) as the reader model. Yu et al. (2023) presents a strong baseline of directly merging two knowledge sources. With the improved coverage of world knowledge, it beats the retrievalonly model by an average of 7.1 EM over the three single-hop QA datasets. With the same reader model and the same set of input passages, our COMBO framework further improves over Direct Merging by an average of 1.3 EM scores across the single-hop QA datasets.\nWe also compare our proposed algorithm to a Random Matching baseline, which randomly divides passages into pairs. Our improvement over Random Matching indicates that compatible pairs of LLM and retrieved passages are essential for model to make better predictions. Appendix D.1 also shows the results of an oracle setting assuming access to ground-truth answers for matching and the state-of-the-art results reported in existing literature. Since our approach does not modify retriever or reader architecture, it can be easily extended to\ncapitalize on larger models, more retrieved passages and additional knowledge sources (e.g., tables and KBs), as already leveraged in state-of-theart systems. We also note that any potential performance improvements are somewhat constrained by the percentage of the hallucinated (generated) and evidential (retrieved) passages provided. In the extreme scenario where all generated passages are devoid of hallucinations or all retrieved passages lack correct evidence, our method would default to Random Matching, as the compatibility matching score would be uniform.\nDifferent from single-hop QA, we do not observe improvements on HotpotQA. However, minor improvement is observed on the subset (73%) of bridge questions that require the retriever or LLM to find a bridge entity that connects two pieces of evidence together to reveal the answer. It shows that our method is not directly generalizable to the more challenging multi-hop setting, and thus calls for a more fine-grained modeling of the compatibility between hops of evidence. We note that for the rest of comparison questions in HotpotQA (e.g., \u201cwho is younger, Keith Bostic or Jerry Glanville?\u201d), models could make the right prediction even if the LLM-generated passages contain hallucinated evidence (see Appendix D.2 for an example). This explains why our compatibilityoriented framework appears to be less effective on this subset.10\nAblation Study. We create several variants to validate the effectiveness of different components of our COMBO framework, by removing or replacing each component individually. Results are shown in Table 2. First, we remove the evidentiality discriminator DE and only use the probability given by the\n10We are unable to show results of multiple methods on the hidden test set of HotpotQA, as the official leaderboard of HotpotQA (https://hotpotqa.github.io/) allows submitting predictions only once. Therefore, we show results on the dev set instead.\nconsistency discriminator DC as the compatibility score for ranking passage pairs (w/o evidentiality discriminator) . The performance drop highlights the importance of having an evidentiality discriminator to filter out irrelevant (i.e., non-evidential) retrieved passages before computing the passage consistency and thus echoes our compatibility decomposition motivated by Equation 1.\nSecond, we remove the pairwise formulation by linearizing the matched pairs into a sequence of single passages as input (w/o pairwise input). See Figure 4 in Appendix C.2 for an illustration of the linearized inputs. The performance loss validates the usefulness of the encoder\u2019s self-attention between tokens of LLM-generated and retrieved passages, which allows the reader module to jointly reason over passage pairs as aggregated evidence.\nThird, we randomly shuffle the order of input passage pairs instead of ranking by their compatibility scores (w/o sorting pairs). The score worsens, as this model variant fails to learn to prioritize compatible information. Further analysis of model\u2019s behavior in Appendix D.4 verifies that our model could attribute higher attention scores to compatible pairs than others when making predictions.\nFor the fourth ablation experiment (w/o fixed (lpi, rpj) order), we randomly shuffle the order of lpi and rpj within each passage pair, instead of always putting lpi in front of rpj (Figure 4, Ap-\nConflicting Rate Subset% Retrieved Psg. Only Direct Merging COMBO\nare shaded with darker orange. Overall, COMBO\u2019s improvement over Direct Merging is greater when the conflicting rate is higher, suggesting the robustness of our method to knowledge conflicts.\npendix C.2). We find that fixing their order allows the model to learn to rely on the factual evidence in the latter retrieved passage when facing conflicting information.\nWe also explore other ablations to show the contributions of several heuristics employed in our system, including evidentiality-cutoff (Equation 2) and optimal matching (Section 3.3). We replace Equation 2 with Equation 1 for producing compatibility scores (w/o evidentiality-cutoff ). Equation 1 directly multiplies the two probabilities given by the two discriminators together. It demonstrates that models benefit from a binarized decision by DE and we hypothesize that this is because the raw predicted probability is often not well calibrated (i.e., predicted probabilities do not correlate well with the probabilities of correctness (Guo et al., 2017; Jiang et al., 2021)).\nAdditionally, we replace the optimal matching (maximum weighted matching) with a greedy strategy (w/o optimal matching). Specifically, we first match compatible passage pairs, followed by conflicting and non-evidential pairs. When finding the k-th pair to match, we only consider passages not appearing in previous k\u22121 pairs. The performance drop shows the optimal matching could bring together more compatible pairs and thus better guide the model\u2019s predictions.\nImpact of Conflicting Contexts on the Reader Model. Here we aim to answer a question: How well can the reader model (i.e., FiD) pinpoint the correct answer when varying degrees of knowledge conflicts exist between LLM-generated texts and retrieved passages? Table 3 shows the performance of different methods when facing different rates of\nconflicting knowledge. We measure conflict rate as follows:\nconflicting_rate = NA \u00b7 (M \u2212MA)\nN \u00b7M . (3)\nNA refers to the number of retrieved passages that contain the gold answer string A and M \u2212 MA means the number of LLM-generated passages that do not contain the gold answer string. The conflicting rate indicates the percentage of conflicting pairs (i.e., rpi contains the answer while lpj does not) over all possible pairs. Table 3 suggests that when there is minimal conflicts between the two sources, both Direct Merging and COMBO can significantly improve over the Retrieved-passageonly model. However, when the conflict rate is high, only COMBO can maintain a consistent improvement over the Retrieved-passage-only baseline, suggesting its robustness.\nScaling with Number of Passages. We further evaluate the performance of COMBO with respect to different numbers of LLM-generated passages (M ) and retrieved passages (N ). Figure 5 shows the results for NaturalQuestions. Given a larger number of passages, we switch our reader model from FiD-large to FiD-base due to GPU memory limits. When M is smaller than N , we simply duplicate the LLM-generated passages to match the number of retrieved passages, so that every passage is included in the matching results (consistent with Section 3.3). We observe that given more passages from both sources, our framework generally achieves greater performance gain over"
        },
        {
            "heading": "Retrieved Passage 7",
            "text": ""
        },
        {
            "heading": "Retrieved Passage 1",
            "text": "the direct merging approach. Additional analysis in Appendix D.3 shows that more knowledge conflicts arise from the increased number of input passages, again highlighting the importance of compatibilityguided knowledge merging. Because we train our discriminators only on the top 10 passages, they may not generalize well when applied to the top 50 passages. This is possibly why our method seems less effective when provided with 50 retrieved passages as input.\nCase Study of Matching Results. Figure 6 shows an example from NaturalQuestions where the COMBO matching results of passages help rectify the prediction of the Direct Merging model. In the first compatible pair, both passages contain correct evidence that supports the ground-truth answer \u201cDon Shula\u201d. The last one is an incompatible pair where the LLM-generated passages frequently contain a hallucinating error (\u201cGeorge Halas\u201d) that misleads the Direct Merging model to make a wrong prediction. Since we prioritize compatible pairs over incompatible pairs, the reader model could leverage this inductive bias by paying more attention to the higher-ranked passages."
        },
        {
            "heading": "Human Evaluation of Compatibility Labels.",
            "text": "Figure 7 shows the confusion matrix of the compatibly labels predicted by our discriminators on 150 examples from the dev set of NaturalQuestions. Specifically, we randomly select 25 questions and sample 2 compatible pairs, 2 conflicting pairs and 2 non-evidential pairs (if applicable) for each question. The authors manually analyze whether the passages actually contain the correct evidence. We find that our discriminators yield an overall accuracy of 78% on this 3-way classification task. It shows that our discriminators are capable of providing signals of passage compatibility to the reader model for making well-informed decisions."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this work, we study the problem of merging retrieved and LLM-generated knowledge for opendomain QA. We tackle the challenge of knowledge conflicts caused by LLM\u2019s hallucination with a compatibility-oriented knowledge merging framework (COMBO). Specifically, we match LLMgenerated and retrieved passages for a given question into pairs based on their compatibility and perform information fusion on the encoder side of the FiD-based reader by feeding matched pairs as input. Experiments on four open-domain QA datasets demonstrate the empirical success of COMBO.\nIn the future, we plan to extend our framework to few-shot QA settings by employing an LLM with in-context learning to compute the compatibility scores instead of training a discriminator from scratch, thereby eliminating the necessity for (weakly) supervised training. But it would also require carefully examining whether and which LLM is able to discern the knowledge conflicts between its own parametric memory and retrieved evidence."
        },
        {
            "heading": "Limitations",
            "text": "1. We only evaluate our knowledge merging framework on the tasks of open-domain QA. It would be interesting to apply our method to other knowledge-intensive NLP tasks such as fact checking (Thorne et al., 2018) and knowledge-enhanced text generation (Dinan et al., 2019; Yu et al., 2022).\n2. Under the conditions of our experiment, we did not manipulate the distribution of questions or input passages in any way. However, we anticipate that in more controlled settings, such as PopQA (Mallen et al., 2023) where questions about infrequent entities are examined in more detail, our method could yield more dramatic performance improvements. At present, LLMs appear more susceptible to hallucinating knowledge pertaining to less frequent entities. We eagerly anticipate investigating this aspect further in our future work.\n3. For each dataset, we only conduct experiments with generated passages from a single LLM (either InstructGPT or ChatGPT) and retrieved passages from a single retriever (either DPR or MDR). Our experiments are limited to a generative reader model which is based on the widely-used Fusion-in-Decoder (Izacard and Grave, 2021b) architecture.\n4. Regarding computational overheads of COMBO, we leverage the same set of input passages and same reader model as direct merging. During the training of the FiD-based reader model, according to our empirical observations on NaturalQuestions, it only introduces minimal computational overheads of approximately 23% more GPU memory and 27% more training time, compared to direct merging. Practitioners who can afford to train the previous direct merging model should also be able to train our framework.\n5. Our proposed approach for silver label mining could result in a limited amount of labels on a small dataset, which could not provide sufficient data to train a dataset-specific discriminator. It thus calls for the need for training a unified discriminator that works for various datasets and tasks."
        },
        {
            "heading": "Ethics Statement",
            "text": "Large language models are known to have racial and gender biases and may generate harmful content such as fabricated facts and toxic response (OpenAI, 2023). Since we leverage generated contextual passages from LLMs to enhance QA performance, our models may inherit these biases during generation. Although we aim to promote the usage of factual information by combining generated knowledge with retrieved knowledge from an external trustworthy corpus, we do not fully eliminate hallucinations from LLMs. Interested practitioners should carefully address these risks before deploying our framework in certain domains such as politics, finance, and healthcare."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by LG AI Research and computational resources and services provided by Advanced Research Computing (ARC), a division of Information and Technology Services (ITS) at the University of Michigan, Ann Arbor. Additionally, we would like to thank Wenhao Yu for his invaluable assistance in clarifying aspects of their published work and generously sharing code and data to facilitate the reproducibility of the results presented in this paper. We also appreciate the anonymous reviewers for their valuable suggestions. We thank the members of the LAUNCH group at the University of Michigan for their discussions and suggestions."
        },
        {
            "heading": "A Preliminaries",
            "text": ""
        },
        {
            "heading": "A.1 Analysis of Reader\u2019s Predictions under Knowledge Conflicts",
            "text": "We are interested in the question: does FiD-based reader model prefer LLM-generated passages over retrieved ones for making predictions under knowledge conflicts? We compare the outputs of two models: one is fed with retrieved passages only, and the other takes the concatenation of retrieved passage and generated passage as input (i.e., Direct Merging shown in Figure 2a). We focus on the examples from dev set of NaturalQuestions where knowledge conflicts probably exist. Here we select questions whose retrieved passages contain correct answer while LLM-generated passages do not. Specifically, we obtain a subset of questions with more then 20% of conflicting passage pairs (i.e., conflicting rate defined in Equation 3). On this subset, the Retrieved-passage-only model is not affected by knowledge conflicts since it does not have access to LLM-generated passages.\nWe find that on the examples initially predicted as correct by the Retrieved-passage-only method, models are fooled by adding LLM-generated passages on 16% of them (i.e., Direct Merging method gives the wrong predictions). The performance drop shows that there is still room for improvement for FiD-based reader to resolve conflicts on its own, thus calling for the need of our compatibilityoriented QA framework.\nAdditionally, we observe that 65% of predictions are sub-spans of at least one of the generated passages. This shows that the reader favors LLM passages more than retrieved passages when conflict exists. LLM passages often appear to be more relevant to the question and describe the plausible evidence in a way more specific to question. We hypothesize that the reader model find it easier to extract answer from LLM-generated passages so it tend to rely on them more for predictions."
        },
        {
            "heading": "A.2 Retrieve/Generate-then-read Open-domain QA Framework",
            "text": "The task of open-domain QA is typically solved by a retrieve-then-read framework (Karpukhin et al., 2020) consisting of two modules: 1) a retriever R that fetches top-N relevant passages PR = {rp1, rp2, ..., rpN} to the question Q from a large corpus \u2126R such as Wikipedia and 2) a generative reader G that generates the answer A conditioned on retrieved passages PR, denoted\nNaturalQuestions (Single-hop QA) Question: Who plays Charlotte in \u201cThe Strain\u201d season 4? Correct Answer: Rhona Mitra \u221a Prediction: Alexandra Breckenridge \u00d7\nHotpotQA (Multi-hop QA) Question: Gail Matthius co-anchored the Weekend Update segment of \"Saturday Night Live\" with the actor who played the villain Nicholas Andre in what movie? Answer: Dumb and Dumber \u221a Prediction: The Goonies \u00d7\nRetrieved Passage 1. (Gail Matthius) \u2026 co-anchored the Weekend Update segment with Charles Rocket in 1981. 2. (Charles Rocket) \u2026 He was best known for his role as the villain Nicholas Andre in the film \u201cDumb and Dumber\u201d \u2026\n1. Gail Matthius is an American actress \u2026 She also co-anchored the Weekend Update segment of the show \"Saturday Night Live\" \u2026 2. Robert Davi \u2026 is perhaps best known for his roles in movies such as \u2026 \"The Goonies\u201d \u2026 where he played the character Nicholas Andre.\nLLM-generated Passage\nLLM-generated Passage The Strain is an American horror drama television series \u2026. Charlotte \u2026 is played by Alexandra Breckenridge in season 4.\nRetrieved Passage (Rhona Mitra) \u2026 she played Charlott in the fourth season of \u201cThe Strain\u201d TV Series \u2026\nFigure 8: A Fusion-in-Decoder-based (Izacard and Grave, 2021b) QA model is misled by the knowledge conflict between retrieved passage and hallucinating LLM-generated passage on an example from HotpotQA (Yang et al., 2018).\nas A = G(Q,PR). We use Fusion-in-Decoder (FiD) (Izacard and Grave, 2021b), a state-of-theart retrieval-augmented generation model, as our reader model. FiD is a sequence-to-sequence architecture based on the pretrained T5 models (Raffel et al., 2020). It first encodes separately each passage ri appended to the question, resulting in representation pj = T5-encoder(Q, rpj). Then the decoder attends to the concatenation of representations of all passages and generates the answer: A = T5-decoder(p1,p2, ...,pN ). The crossattention mechanism of the decoder enables FiD to perform evidence aggregation, i.e., assigning different attention scores to each passage.\nThe recently proposed generate-then-read pipeline (Yu et al., 2023) simply substitutes the retriever R with an LLM L that generates top-M contextual passages PL = {lp1, lp2, ..., lpM} expressing knowledge stored in its parameters. Then these LLM-generated passages are handled by the reader to give the final answer: A = G(Q,PL). Yu et al. (2023) also explore a simple approach (Figure 2a) of directly merging passages from both sources together: A = G(Q,PL \u222a PR), regardless of knowledge conflicts between PL and PR."
        },
        {
            "heading": "A.3 Extension of Compatibility Definition from Single-hop to Multi-hop QA",
            "text": "Different from the single-hop QA setting, our retrieved item under the multi-hop QA setting is actually a passage chain rcj = (rp1j , rp 2 j , ..., rp k j ) instead of a single passage rpj , following the common practice proposed by Xiong et al. (2021).\nSpecifically, in HotpotQA (Yang et al., 2018), all questions are supposed to be 2-hop, so a passage chain rcj is essentially an ordered pair of passages (rp1j , rp 2 j ) that provides sufficient evidence for answering Q. An example of the retrieved passage chain is shown in Figure 8. We modify the assumptions and definitions for the multi-hop setting accordingly, as shown in Table 4. Specifically, one could simply replace rpj with rcj and lpi with lci throughout this paper to obtain the same conclusion. The modifications of definitions from the single-hop to the multi-hop setting are minimal yet reasonable, allowing for a unified concept formulation and system implementation to handle both single-hop and multi-hop QA.\nA.4 Validating Assumptions behind the Compatibility Definition\nIn Section 3.1, we make two assumptions for computing passage compatibility. Assumption 1 (\u201cretrieved passages are factual\u201d) is widely adopted in the existing literature (Ji et al., 2023; Lee et al., 2022; Thorne et al., 2018) when retrieving from Wikipedia corpus. For Assumption 2 (\u201cLLM-generated passages contain relevant evidence to the question\u201d), the authors manually annotate 100 random examples from the NaturalQuestions (Kwiatkowski et al., 2019) dataset to verify the plausibility of LLM\u2019s generations. For each example, we first determine whether it contains a plausible answer. If yes, we further compare it to the ground-truth answer or search on Wikipedia to label it as hallucinating or not. We show examples of annotations in Table 5. We find that 94% of them contain plausible answers, thus supporting our assumption. Of all the passages that contain plausible answers, 51% of the answers are\nincorrect, meaning that these passages suffer from the hallucinations of LLM and pose a risk to the reliability of QA readers. This highlights the challenge of knowledge merging and the importance of our compatibility-oriented matching framework COMBO."
        },
        {
            "heading": "B Dataset Details",
            "text": "We use three single-hop QA datasets (NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), WebQuestions (Berant et al., 2013)) and one multi-hop (HotpotQA (Yang et al., 2018)) QA dataset as the testbeds for evaluating our method. NaturalQuestions consists of real-world information-seeking queries issued to the Google search engine and their corresponding long answers (gold evidence passage) and short answers (one or more entities). We use the open-domain version created by Lee et al. (2019) which only keeps questions and short answers with less than five tokens. TriviaQA includes questions from trivia and quizleague websites. WebQuestions contains questions from Google Suggest API that queries entities in Freebase. HotpotQA contains questions requiring reasoning over multiple Wikipedia documents to answer. We focus on the fullwiki setting where QA systems need to retrieve relevant evidence from the whole Wikipedia corpus. For train / dev / test splits, we use the same setting as Karpukhin et al. (2020) for NaturalQuestions, TriviaQA, and WebQuestions, and the official leaderboard version11 for HotpotQA.\n11https://hotpotqa.github.io/\nC Implementation Details"
        },
        {
            "heading": "C.1 Silver Evidentiality Label Mining",
            "text": "We use the same reader Gb for consistency label mining in Section 3.2. Inspired by Asai et al. (2022), we consider rpj as evidential for Q if 1) the prediction based on top-N retrieved passages Gb(Q,PR) is correct and 2) Gb(Q,PR\\{rpj}), the prediction based top-N retrieved passages except rpj , is incorrect. rpj is thus believed to contain the correct evidence since removing itself makes the predictions change from right to wrong. Similarly, we consider rpj as non-evidential for Q if Gb(Q,PR) is incorrect and Gb(Q,PR\\{rpj}) is correct, which indicates adding rpj to the rest of input passages misleads the model."
        },
        {
            "heading": "C.2 Model Implementations and Hyperparameters",
            "text": "For LLM-generated passages, we use the ones provided by Yu et al. (2023)12 for the three singlehop QA datasets. They prompt InstructGPT (text-davinci-002) (Ouyang et al., 2022) with the prompt \u201cProvide a background document from Wikipedia to answer the given question. \\n\\n {question} \\n\\n\u201d and use sampling to generate 20 documents for each question. Additionally, we prompt ChatGPT (gpt-3.5-turbo) to generate contextual passages for HotpotQA. We use the prompt \u201cYou are an assistant designed to provide a chain of two 100-word documents from Wikipedia\n12https://github.com/wyu97/GenRead.git\nQuestion: When did Italy host the FIFA World Cup? Answer: 1934\nQ RP 1 RP 2 RP 3\nQ RP 2 RP 3\nLP 1Q RP 1 RP 2 RP 3\nLP 1Q RP 2 RP 3\nFinetuned Base Reader 1984 \u00d7\n1934 \u221a\n1934 \u221a\n1934 \u221a\n(FIFA World Cup hosts) \u2026 the only remaining candidate Italy to take the hosting job for the 1934 World Cup. The decision was ratified \u2026\nRetrieved Passage 1\n(Italy national football team) \u2026 Italy is one of the most successful national teams in the history of the World Cup, having won four titles (1934, \u2026\nRetrieved Passage 2\n(1990 FIFA World Cup) \u2026 The vote to choose the hosts of the 1990 tournament was held on 19 May 1984 in Z\u00fcrich, Switzerland. \u2026\nRetrieved Passage 3\n\u2026 The 1934 FIFA World Cup was the second FIFA World Cup, and was held in Italy from 27 May to 10 June 1934.\nLLM-generated Passage 1\nRP 1 LP 1 Consistent\nI.\nII.\nIII.\nIV.\nthat can be combined together to answer the user\u2019s question. Here\u2019s an example of your output format: Document 1: \"\"\\n\\n Document 2: \"\"\\n\\n {question}\u201d. We then parse the output to get the generated passage chains. It costs around 400 US dollars to generate 10 passage chains per question with ChatGPT for HotpotQA.\nFor both the evidentiality discriminator DE and consistency discriminator DC , we initialize the model from the pretrained RoBERTa-large 13 for single-hop QA and DeBERTa-large14 for multihop QA from huggingface (Wolf et al., 2019). We set the max sequence length of RoBERTa as 512 and DeBERTa as 1024 since each passage chain of HotpotQA contains two passages. The training is performed on two A40 GPUs and the batch size is 16 per GPU. Peak learning rate is set to 1e-5 for RoBERTa and 5e-6 for DeBERTa. We use the Adam optimizer and linear schedule of learning rate. We train the model for 7 epochs and select the best epoch checkpoint based on the F1 score of the silver dev set, which is mined from the dev set of the original corresponding dataset. We apply sample weights in the cross-entropy loss function to handle imbalanced classes in the silver training data.\nWe employ FiD as the backbone architecture for our reader model in this paper. We train the models for 100k steps using four A40 GPUs with 46 GB memory and save checkpoints every 10k steps. The best checkpoint is selected based on the EM score on the dev set. For pairwise input, we set the max input length per passage pair as 400 for single-hop QA and 1000 for multi-hop QA. For single-passage input, we set the max input length per passage as 200 for single-hop QA and 500 for multi-hop QA. Per GPU batch size is 1 and we set the gradient\n13https://huggingface.co/roberta-large 14https://huggingface.co/microsoft/\ndeberta-v3-large\naccumulation step to 16 to imitate a large batch size. The learning rate is 1e-4 with 2000 warmup steps and a linear scheduler. Adam is the optimizer and the dropout probability is set to 0.1."
        },
        {
            "heading": "D Additional Experimental Results and Analysis",
            "text": ""
        },
        {
            "heading": "D.1 More Results",
            "text": "We also compare our proposed compatibilityguided optimal matching algorithm to an oracle setup for passage matching, in addition to the Random Matching baseline mentioned in Table 1. We first match passages into pairs if both of them contain the ground truth answer string and randomly pair the rest. We call it same-answer matching. Although an answer-containing passage does not necessarily contain the correct evidence (Asai et al., 2022), this setting still gives us a rough estimate of the upper bound of the model\u2019s performance. Results are shown in Table 7. We show that our method is close to the upper bound, indicating that our compatibility discriminators are good at identifying passages that contain the correct evidence.\nTable 7 also includes state-of-the-art results on these datasets. It should be noted that state-of-\nthe-art systems for these datasets leverage readers with much larger size (up to 340B) (Izacard et al., 2022; Anil et al., 2023) or incorporate more input retrieved passages (Ma et al., 2023) and additional knowledge in different formats (e.g., Tables, KBs) (Oguz et al., 2022). Therefore, it is not an apple-to-apple comparison between our models and theirs. However, knowledge merging can in principle benefit a wide range of open-domain QA system so our contributions should be complementary to other state-of-the-art retrievers and reader modules."
        },
        {
            "heading": "D.2 Comparison Questions from HotpotQA are Less Sensitive to Hallucinations in LLM-generated Passages",
            "text": "A comparison question is one of the two types of question in HotpotQA, testing models\u2019 ability to compare two entities on some shared properties (Yang et al., 2018). This type of question is less sensitive to hallucinations in LLM-generated passages than the bridge question. For example, for the comparison question \u201cwho is younger, Keith Bostic or Jerry Glanville?\u201d, the ChatGPT-generated passage chain is \u201cDocument 1: Keith Bostic was born on January 8, 1956 ... \\n \\n Document 2: Jerry Glanville was born on October 14, 1941 ...\u201d. It contains hallucination errors since Keith Bostic was actually born on January 17, 1961. Nevertheless, the reader model could still make the correct prediction (\u201cKeith Bostic\u201d) based on the fabricated evidence."
        },
        {
            "heading": "D.3 Distribution of Pairwise Relationships",
            "text": "In Figure 10, we show the distribution of the relationships of passage pairs (compatible vs. conflicting vs. non-evidential) determined by our evidentiality and consistency discriminators. We observe a trend that as we increase the number of input LLM passages and retrieved passages, the percentage of compatible pairs decreases, which shows that more knowledge conflicts arise. This could account for the larger improvements of our COMBO framework over direct merging with more passages as input, which is shown in Figure 5."
        },
        {
            "heading": "D.4 Analyzing Attention Distributions Over Different Types of Passage Pairs",
            "text": "In Figure 11, we show the distribution of attention scores for each type of passage pair across different numbers of input LLM-generated and retrieved passages. Following Izacard and Grave (2021a), the attention score for a passage pair is obtained by averaging the pre-normalized attention score of all tokens in the passage pair, all layers and all heads of the decoder. We find that COMBO model generally assigns higher attention scores to compatible pairs than incompatible ones. This indicates that it learns to prioritize compatible information when making its predictions, which mitigates the harmful impact of hallucination errors."
        }
    ],
    "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
    "year": 2023
}