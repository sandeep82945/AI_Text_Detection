{
    "abstractText": "Understanding the prevalence and dynamics of justice partisanship and ideology in the US Supreme Court is critical in studying jurisdiction. Most research quantifies partisanship based on voting behavior, and oral arguments in the courtroom \u2014 the last essential procedure before the final case outcome \u2014 have not been well studied for this purpose. To address this gap, we present a framework for analyzing the language of justices in the courtroom for partisan signals, and study how partisanship in speech aligns with voting patterns. Our results show that the affiliated party of justices can be predicted reliably from their oral contributions. We further show a strong correlation between language partisanship and voting ideology.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Biaoyan Fang"
        },
        {
            "affiliations": [],
            "name": "Trevor Cohn"
        },
        {
            "affiliations": [],
            "name": "Timothy Baldwin"
        },
        {
            "affiliations": [],
            "name": "Lea Frermann"
        },
        {
            "affiliations": [],
            "name": "\u2660MBZUAI \u2663CSIRO"
        }
    ],
    "id": "SP:9e1bd528d44a994b18f9e3c52d2422c753105046",
    "references": [
        {
            "authors": [
                "Michael A Bailey."
            ],
            "title": "Is today\u2019s court the most conservative in sixty years? Challenges and opportunities in measuring judicial preferences",
            "venue": "The Journal of Politics, 75(3):821\u2013834.",
            "year": 2013
        },
        {
            "authors": [
                "Lawrence Baum",
                "Neal Devins."
            ],
            "title": "The company they keep: How partisan divisions came to the Supreme Court",
            "venue": "Oxford University Press.",
            "year": 2019
        },
        {
            "authors": [
                "Noah Bergam",
                "Emily Allaway",
                "Kathleen Mckeown."
            ],
            "title": "Legal and political stance detection of SCOTUS language",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2022, pages 265\u2013 275, Abu Dhabi, United Arab Emirates (Hybrid).",
            "year": 2022
        },
        {
            "authors": [
                "Adam Bonica",
                "Maya Sen."
            ],
            "title": "Estimating judicial ideology",
            "venue": "Journal of Economic Perspectives, 35(1):97\u2013118.",
            "year": 2021
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos."
            ],
            "title": "LEGAL-BERT: The muppets straight out of law school",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan P. Chang",
                "Caleb Chiam",
                "Liye Fu",
                "Andrew Wang",
                "Justine Zhang",
                "Cristian DanescuNiculescu-Mizil."
            ],
            "title": "ConvoKit: A toolkit for the analysis of conversations",
            "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Dis-",
            "year": 2020
        },
        {
            "authors": [
                "Tom S Clark."
            ],
            "title": "Measuring ideological polarization on the United States Supreme Court",
            "venue": "Political Research Quarterly, 62(1):146\u2013157.",
            "year": 2009
        },
        {
            "authors": [
                "Pieter Delobelle",
                "Ewoenam Tokpo",
                "Toon Calders",
                "Bettina Berendt."
            ],
            "title": "Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Nikhil Garg",
                "Rob Voigt",
                "James Zou",
                "Jesse Shapiro",
                "Matthew Gentzkow",
                "Dan Jurafsky"
            ],
            "title": "Analyzing polarization in social media",
            "year": 2019
        },
        {
            "authors": [
                "Neal Devins",
                "Lawrence Baum."
            ],
            "title": "Split definitive: How party polarization turned the Supreme Court into a partisan court",
            "venue": "The Supreme Court Review, 2016(1):301\u2013365.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Diermeier",
                "Jean-Fran\u00e7ois Godbout",
                "Bei Yu",
                "Stefan Kaufmann."
            ],
            "title": "Language and ideology in Congress",
            "venue": "British Journal of Political Science, 42(1):31\u201355.",
            "year": 2012
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Michael Horning",
                "Eugenia H Rho."
            ],
            "title": "Same Words, Different Meanings: Semantic Polarization in Broadcast Media Language Forecasts Polarity in Online Public Discourse",
            "venue": "Proceedings of the International AAAI Conference on Web and",
            "year": 2023
        },
        {
            "authors": [
                "Ryan D Doerfler",
                "Samuel Moyn."
            ],
            "title": "Democratizing the Supreme Court",
            "venue": "Cal. L. Rev., 109:1703.",
            "year": 2021
        },
        {
            "authors": [
                "Sujan Dutta",
                "Beibei Li",
                "Daniel S Nagin",
                "Ashiqur R KhudaBukhsh."
            ],
            "title": "A Murder and Protests, the Capitol Riot, and the Chauvin Trial: Estimating Disparate News Media Stance",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artifi-",
            "year": 2022
        },
        {
            "authors": [
                "Lee Epstein",
                "William M Landes",
                "Richard A Posner."
            ],
            "title": "Inferring the winning party in the Supreme Court from the pattern of questioning at oral argument",
            "venue": "The Journal of Legal Studies, 39(2):433\u2013467.",
            "year": 2010
        },
        {
            "authors": [
                "Lee Epstein",
                "Andrew D Martin",
                "Kevin M Quinn",
                "Jeffrey A Segal."
            ],
            "title": "Ideological drift among Supreme Court justices: Who, when, and how important",
            "venue": "Nw. UL Rev., 101:1483.",
            "year": 2007
        },
        {
            "authors": [
                "Lee Epstein",
                "Kevin Quinn",
                "Andrew D Martin",
                "Jeffrey A Segal."
            ],
            "title": "On the perils of drawing inferences about Supreme Court justices from their first few years of service",
            "venue": "Judicature, 91:168.",
            "year": 2007
        },
        {
            "authors": [
                "Lee Epstein",
                "Keren Weinshall."
            ],
            "title": "The Strategic Analysis of Judicial Behavior: A Comparative Perspective",
            "venue": "Cambridge University Press.",
            "year": 2021
        },
        {
            "authors": [
                "Biaoyan Fang",
                "Trevor Cohn",
                "Timothy Baldwin",
                "Lea Frermann."
            ],
            "title": "It\u2019s not only What You Say, It\u2019s also Who It\u2019s Said to: Counterfactual Analysis of Interactive Behavior in the Courtroom",
            "venue": "Proceedings of The 13th International Joint Conference on Natu-",
            "year": 2023
        },
        {
            "authors": [
                "Biaoyan Fang",
                "Trevor Cohn",
                "Timothy Baldwin",
                "Lea Frermann."
            ],
            "title": "Super-SCOTUS: A multi-sourced dataset for the Supreme Court of the US",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2023, Singapore. Association for Compu-",
            "year": 2023
        },
        {
            "authors": [
                "Amir Feder",
                "Katherine A. Keith",
                "Emaad Manzoor",
                "Reid Pryzant",
                "Dhanya Sridhar",
                "Zach Wood-Doughty",
                "Jacob Eisenstein",
                "Justin Grimmer",
                "Roi Reichart",
                "Margaret E. Roberts",
                "Brandon M. Stewart",
                "Victor Veitch",
                "Diyi Yang"
            ],
            "title": "Causal inference in natural lan",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Gentzkow",
                "Jesse M Shapiro",
                "Matt Taddy."
            ],
            "title": "Measuring group differences in highdimensional choices: method and application to congressional speech",
            "venue": "Econometrica, 87(4):1307\u20131340.",
            "year": 2019
        },
        {
            "authors": [
                "Rodney S Haddow",
                "Thomas Richard Klassen."
            ],
            "title": "Partisanship, globalization, and Canadian labour market policy: four provinces in comparative perspective, volume 25",
            "venue": "University of Toronto Press.",
            "year": 2006
        },
        {
            "authors": [
                "Renfen Hu",
                "Shen Li",
                "Shichen Liang."
            ],
            "title": "Diachronic sense modeling with deep contextualized word embeddings: An ecological view",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3899\u20133908, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Tonja Jacobi",
                "Matthew Sag."
            ],
            "title": "The new oral argument: justices as advocates",
            "venue": "Notre Dame L. Rev., 94:1161.",
            "year": 2018
        },
        {
            "authors": [
                "Sharon E Jarvis."
            ],
            "title": "Partisan patterns in presidential campaign speeches, 1948\u20132000",
            "venue": "Communication Quarterly, 52(4):403\u2013419.",
            "year": 2004
        },
        {
            "authors": [
                "Jacob Jensen",
                "Suresh Naidu",
                "Ethan Kaplan",
                "Laurence Wilse-Samson",
                "David Gergen",
                "Michael Zuckerman",
                "Arthur Spirling"
            ],
            "title": "Political polarization and the dynamics of political language: Evidence from 130 years of partisan speech [with comments",
            "year": 2012
        },
        {
            "authors": [
                "George Lakoff."
            ],
            "title": "Moral politics: How liberals and conservatives think",
            "venue": "University of Chicago Press.",
            "year": 2010
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Robert N Lupton",
                "Steven M Smallpage",
                "Adam M Enders."
            ],
            "title": "Values and political predispositions in the age of polarization: Examining the relationship between partisanship and ideology in the United States, 1988\u20132012",
            "venue": "British Journal of Political Sci-",
            "year": 2020
        },
        {
            "authors": [
                "Andrew D Martin",
                "Kevin M Quinn."
            ],
            "title": "Dynamic ideal point estimation via Markov chain Monte Carlo for the US Supreme Court, 1953\u20131999",
            "venue": "Political Analysis, 10(2):134\u2013153.",
            "year": 2002
        },
        {
            "authors": [
                "Andrew D Martin",
                "Kevin M Quinn."
            ],
            "title": "Assessing preference change on the US Supreme Court",
            "venue": "The Journal of Law, Economics, & Organization, 23(2):365\u2013385.",
            "year": 2007
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Dana Patton",
                "Joseph L Smith."
            ],
            "title": "Lawyer, interrupted: Gender bias in oral arguments at the US Supreme Court",
            "venue": "Journal of Law and Courts, 5(2):337\u2013361.",
            "year": 2017
        },
        {
            "authors": [
                "USA TODAY Richard Wolf."
            ],
            "title": "The 21 most famous Supreme Court decisions",
            "venue": "Accessed on June 17th, 2023.",
            "year": 2015
        },
        {
            "authors": [
                "Michael D Robinson",
                "Ryan L Boyd",
                "Adam K Fetterman",
                "Michelle R Persich."
            ],
            "title": "The mind versus the body in political (and nonpolitical) discourse: Linguistic evidence for an ideological signature in US politics",
            "venue": "Journal of Language and Social Psychology,",
            "year": 2017
        },
        {
            "authors": [
                "Charles R Shipan."
            ],
            "title": "Partisanship, ideology, and Senate voting on Supreme Court nominees",
            "venue": "Journal of Empirical Legal Studies, 5(1):55\u201376.",
            "year": 2008
        },
        {
            "authors": [
                "Yanchuan Sim",
                "Bryan Routledge",
                "Noah A. Smith."
            ],
            "title": "Friends with motives: Using text to infer influence on SCOTUS",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1724\u20131733, Austin, Texas.",
            "year": 2016
        },
        {
            "authors": [
                "Adam F Simon",
                "Jennifer Jerit."
            ],
            "title": "Toward a theory relating political discourse, media, and public opinion",
            "venue": "Journal of Communication, 57(2):254\u2013271.",
            "year": 2007
        },
        {
            "authors": [
                "Matt Thomas",
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts",
            "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327\u2013335,",
            "year": 2006
        },
        {
            "authors": [
                "Dietrich Trautmann",
                "Alina Petrova",
                "Frank Schilder."
            ],
            "title": "Legal prompt engineering for multilingual legal judgement prediction",
            "venue": "arXiv preprint arXiv:2212.02199.",
            "year": 2022
        },
        {
            "authors": [
                "Keyon Vafa",
                "Suresh Naidu",
                "David Blei."
            ],
            "title": "Textbased ideal points",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5345\u20135357, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Mike West",
                "Jeff Harrison."
            ],
            "title": "Bayesian forecasting and dynamic models",
            "venue": "Springer Science & Business Media.",
            "year": 2006
        },
        {
            "authors": [
                "David Alistair Yalof."
            ],
            "title": "Pursuit of justices: Presidential politics and the selection of Supreme Court nominees",
            "venue": "University of Chicago Press.",
            "year": 2001
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The study of partisanship and ideology has been an important topic in understanding the US legal system (Jacobi and Sag, 2018; Devins and Baum, 2017; Bonica and Sen, 2021; Doerfler and Moyn, 2021). Most research has focused on justice voting patterns (Bonica and Sen, 2021; Martin and Quinn, 2002, 2007; Epstein et al., 2007a; Bailey, 2013) and behavior in court, e.g. the frequency of interruptions (Epstein et al., 2010) or questions (Epstein and Weinshall, 2021). Despite their core role in legal determination, the content in terms of whether oral arguments portray partisan values has received less attention (Bergam et al., 2022).\nIn political discourse in particular, word choice is nuanced to convey specific messages (Lakoff, 2010; Jarvis, 2004; Robinson et al., 2017; Jensen et al., 2012; Dutta et al., 2022). For instance, Republicans tend to prefer the term baby over fetus to emphasize their belief that human rights begin at conception (Simon and Jerit, 2007), reflecting their\n\u2217Now at Google DeepMind. 1The dataset and code are available from:\nhttps://github.com/biaoyanf/SCOTUS-partisanship.\nbeliefs and partisanship (Gentzkow et al., 2019; Demszky et al., 2019; Thomas et al., 2006; Bergam et al., 2022; Vafa et al., 2020).\nIn this paper, we ask to what extent do oral arguments reveal justice partisanship. We propose a framework to analyze partisanship in justices\u2019 oral arguments in the Supreme Court of the United States (SCOTUS). We classify justices as Democrat or Republican based on the political party of their nominating president, following common practice in the law literature (Devins and Baum, 2017; Yalof, 2001). We cast language-based partisanship prediction as a classification task and show that our models can reliably predict partisanship, suggesting that justices project their affiliations in court arguments.\nWe go on to derive language-based partisanship scores from our models, and ask How does language-based partisanship align with established measures of voting-based ideology? We show that our scores correlate well with established voting-based measures (Figure 1). Equipped with this layer of validation, we move on to more nuanced analyses of language-based partisanship: (a) of the overall court, (b) of individual justices, and\n(c) of landmark cases over time."
        },
        {
            "heading": "2 Methodology",
            "text": "Data We use a subset of the Super-SCOTUS dataset (Fang et al., 2023b), which contains transcripts of SCOTUS oral arguments from 1955\u2013 2019 (Chang et al., 2020).2 We filter out cases where hearings stretch over more than one year. We additionally remove non-linguistic indicators, e.g., [laughter], and mask all person names with a BERT NER model3 (Devlin et al., 2019) in order to focus our models on linguistic indicators rather than mentions of individuals with potential party affiliations.4\nWe concatenate all turns from an individual justice in a single court hearing, to derive multiple instances with a maximum of 510 tokens and a sliding window of 255 tokens. To retain only informative instances, we further remove those that have less than 50 tokens. We finally randomly separate the resulting instances into 10 folds for cross-validation. Partisanship and polarization are dynamic phenomena, responding to shifts in topics and political landscapes. We equip our models with a notion of time by adding a special [year] tag to each instance, flagging its year of origin.\nWe obtain the reference party affiliation for each justice as the party (Republican or Democrat) of the President who nominated the justice, which correlates strongly with the self-reported party of the justice and has been shown to be associated with the interest of the nomination party (Devins and Baum, 2017; Shipan, 2008; Yalof, 2001). The statistics of the resulting dataset are shown in Table 1.\n2https://convokit.cornell.edu/documentation/supreme.html 3https://huggingface.co/dslim/bert-base-NER 4See Appendix A for data construction details.\nClassification To understand to what extent their language reveals a justice\u2019s party affiliation (Democrat vs. Republican), we formulate partisanship prediction as a binary classification task, and fine-tune the BERT-base model5 (Devlin et al., 2019) to predict the affiliated party of a justice from their oral contributions in a single court hearing. For each iteration of cross-validation, we fine-tune BERT on the training fold for 30 epochs, with a batch size of 16, and a dropout rate of 0.3, and select the best model based on the Macro-F1 score on the development fold."
        },
        {
            "heading": "3 Affiliated Party Prediction",
            "text": "We first evaluate our framework intrinsically, asking To what extent do oral arguments reveal justices\u2019 party affiliation? by predicting justices\u2019 affiliated party from their court contributions.\nTable 2 shows affiliated party prediction performance of our classifiers.6 The large boost for the BERT models over random and majority baselines, as well as the high overall performance, shows that the wording of the oral arguments reveals justices\u2019 party affiliation as a proxy for their partisanship. Furthermore, models benefit from temporal tags (+year), indicating that relevant linguistic signals drift over time.\nTo further investigate the impact of temporal lan-\n5https://huggingface.co/bert-base-cased 6Distributions of gold vs. predicted labels over one fold\nare provided in Appendix B.\nguage shift (Hu et al., 2019; Ding et al., 2023) on our oral partisanship prediction task, we train and test our model with a chronological data split. We split the data into non-overlapping temporal spans, with the training covering all cases from 1955\u2013 2000 (N = 5336), development set (2001\u20132009, N = 671) and test set (2010\u20132019, N = 656). The results in Table 3 show that, compared to the random split (Table 2), BERT performs worse on the development and test sets. In line with prior work, this suggests that characteristics of partisan language change over time. Incorporating language drift into predictive models is a fertile area for future work.\nA natural question is whether our model indeed captures partisanship, or rather language idiosyncracies of individual justices. To test this, we use our trained models to predict speaker identities. Specifically, we extract the final layer embedding of the [CLS] token and train logistic regression (LR) and support vector machine (SVM) models for speaker prediction (i.e., a 35-way justice classification task). We compare against off-the-shelf BERT-base-cased embeddings, with no fine-tuning. We use instances with year tags as input, based on the best-performing setup in Table 2.7 Our fine-tuned model performs worse than off-the-shelf BERT embeddings on the task of speaker prediction (Table 4). As such, we conclude that our finetuned model representations abstract away from individual speaker characteristics to representations that indeed capture properties indicative of party affiliation."
        },
        {
            "heading": "4 Justices\u2019 language reflects their voting",
            "text": "We next ask How does language-based partisanship align with established measures of votingbased ideology? To do so, we derive a partisanship score from our predicted party affiliation probabilities. We then relate our language-based ideology\n7Appendix C shows that patterns are consistent for inputs without year tags.\nscores to established measures of ideology obtained from justices\u2019 voting behavior (Martin and Quinn, 2002, 2007; Bonica and Sen, 2021). While partisanship and ideology are not identical, a strong correlation exists, particularly in the two-party system of the United States (Baum and Devins, 2019; Devins and Baum, 2017; Lupton et al., 2020).\nHaving shown that our BERT models capture partisanship from court arguments (Section 3) and with a grounded assumption that partisanship is reflected in (reasonably) local linguistic choice (Jarvis, 2004; Haddow and Klassen, 2006), we obtain a language partisanship score of a justice by averaging the predicted party probability for each of the justices\u2019 instances (Section 2). For example, to obtain the oral partisanship score of a given justice in a given year, we average the predicted party probability of all related instances from the given justice in the given year. Similarly, the language partisanship score for a case (year) is calculated by averaging the predicted party probability of all instances from all justices in that case (year), respectively.\nWe use MQ scores8 (Martin and Quinn, 2002), a widely used and validated measure of the ideology of a court or individual justices, derived from voting outcomes. MQ scores are estimated with a dynamic Bayesian item response model (West and Harrison, 2006), which infers the latent \u2018ideal point\u2019 of a justice, i.e., their ideological standpoint on a unidimensional scale (Liberal \u2014 Conservative) based on their observed voting behavior and a prior encouraging a smooth change in ideology over time.\nCourt-level partisanship Figure 1 compares the overall SCOTUS ideology based on voting behavior (MQ scores, orange) and language (blue) over time, from 1955 to 2019. We observe a strong correlation of both measures across time (Pearson\u2019s r = 0.611, p = 6.5e\u20138), indicating that partisanship in oral arguments reflects the voting behavior in court. Similar observations have been made for more overtly-partisan domains such as political discourse and votes in the US Congress (Gentzkow et al., 2019; Diermeier et al., 2012).\nJustice-level partisanship We further investigate language partisanship and voting ideology at the individual justice level. Justices in recent years have shown clearer alignment with party affilia-\n8http://mqscores.lsa.umich.edu/measures.php\ntions in terms of their voting patterns (Devins and Baum, 2017), and we ask whether this is reflected in their language, too. We present a case study of the eight most recently appointed justices in our data set.9\nFigure 2 shows that Republicanaffiliated/conservative justices (Roberts, Saalito, Gorsuch, Kavanaugh) are consistently separated by the neutral (thick, dashed) line from the Democrat/liberal justices (Ginsburg, Breyer, Kagan, Sotomayor). This holds for both language-based partisanship scores (solid lines) and MQ scores (dashed lines). For Democratic justices, MQ scores and language partisanship are aligned in tendency over their tenure, with a minimum of Pearson\u2019s r = 0.6 across justices. We do not observe such strong evidence in Republican justices, indicating that conservative justices project their values less directly in their speech. Language-based partisanship estimates tend to be more extreme than voting ideology. Notable examples are Roberts (Republican) and Breyer (Democrat), who is known to be a pragmatist whose decisions are often guided by real-life consequences regardless of party-lines,10 possibly explaining the disparity between partisanship and MQ scores.\nEpstein et al. (2007b) observed that, over their tenure, justices drift away from their first-year preferences, but with no certainty in what direction they will move. To study if the same observation holds\n9We excluded the most-recently appointed justice Clarence Thomas due to data sparsity.The same comparison for all justices is provided in Appendix D.\n10https://www.oyez.org/justices/stephen_g_breyer\nfor language partisanship, we compare in Figure 3 how our eight most recent justices\u2019 voting ideology and language partisanship have drifted between their first year of tenure (small dots) vs. their entire tenure to date (large dots). The error bars indicate the standard deviation of partisanship scores (MQ on the x- and language-based on the y-axis) over all years of service. For most justices, the overall trend exceeds the SD interval, suggesting that it goes beyond the typical year-to-year fluctuation. As before, MQ and language scores align well (justices clustering in the bottom left and top right quadrants). Compared to their first year, Democrat-appointed justices (blue) statistically significantly tend to become more liberal in both MQ scores and language partisanship over their tenure. The relative shift in language partisanship for Republican-appointed justices (red) is less pronounced, in terms of both language and MQ scores.\nCase-level partisanship In high-profile cases, voting behaviors have more distinctively lined up along party lines in the more recent years (Devins and Baum, 2017). Correspondingly, we might expect the gap between the average language partisanship in Democrat (blue) vs. Republican-affiliated (red) justices to increase over time. We test this by looking at 14 high-profile cases between 1962 and 2014 (Richard Wolf, 2015).11 As shown in Fig-\n11Full details of the cases are provided in Appendix E.\nure 4, language partisanship indeed becomes more polarized. Particularly, the language of Democrats, which in the 1960s occasionally crossed the neutral line to the Conservative side, has become more liberal over time. Additionally, the gap in language partisanship between the two groups of justices has increased, again confirming that justices\u2019 final voting behavior is reflected in their spoken court arguments."
        },
        {
            "heading": "5 Discussion",
            "text": "Partisanship as affiliation along the liberal\u2013 conservative political spectrum is a fundamental axis in the political discourse. Automatically identifying partisan language through NLP techniques enables large-scale analysis of political and public discourse and a better understanding of divisions and polarization. While prior work has mostly focused on explicitly partisan text, e.g., congress speeches (Jensen et al., 2012) or news outlets (Dutta et al., 2022), we study supreme court language, which, by definition, should be politically neutral, and ask does the language of justices in court reveal their party affiliation? We proposed an analysis framework showing that BERT-based classifiers can reliably predict the affiliated party of justices based on linguistic signals in their SCOTUS arguments.\nIn line with research which has shown a correlation between conversational content and final voting in political discourse (Lupton et al., 2020; Vafa et al., 2020; Bonica and Sen, 2021; Bergam et al., 2022), we further asked to what extent lan-\nguage partisanship aligns with voting ideology. We derived language-based partisanship scores from our validated models and compared them with MQ scores, an established ideology measure derived from voting patterns. We showed a strong correlation between language partisanship and voting ideology at the overall court, individual justice, and important case level over time (Section 4), indicating that the oral arguments of justices do encode their political leanings. Moreover, our work reveals nuanced differences in linguistic and voting behaviors, e.g., their respective tendency shift over justices\u2019 tenure (Figure 3), which further demonstrates the importance of studying SCOTUS from various perspectives.\nMore broadly, we test the extent to which language representations from large language models capture nuanced socio-cultural phenomena, by comparing predictions against corresponding behavioral data sets. Here we focus on partisanship, building on related approaches for stance (Bergam et al., 2022) and counterfactual approaches to investigate the effect of social speaker attributes like gender and seniority on language use in the courtroom (Fang et al., 2023a).\nOur proposed framework enriches the partisanship and ideology analysis with an additional dimension to voting behavior, the spoken text, where we show a strong correlation between voting and spoken language. We hope that this could further spur analyses from other dimensions, such as partisanship of advocates (Patton and Smith, 2017), and amicus curiae (Sim et al., 2016), and take important parts of the legal process, e.g., further opinionwriting (Clark, 2009), into consideration.\nLimitations\nOur work focuses on the Supreme Court of the United States, due to a wealth of available resources and prior research to ground or results. Future work should extend our framework to other political corpora, e.g., congressional records (Gentzkow et al., 2019) and the federal circuit,12 as well as languages and their associated political/legal systems.\nWe did not exhaustively search for the best language representation, and other language models, e.g. RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), or BART (Lewis et al., 2020) may\n12https://cafc.uscourts.gov/home/oral-argument/listen-tooral-arguments/\nlead to improved performance. Also, experiments with additional domain-specific language models, e.g., LEGAL-BERT (Chalkidis et al., 2020), and prompt engineering (Trautmann et al., 2022) may achieve a further increase in performance.\nThe temporal drift of language itself (e.g. different talking styles in court over time), as well as focus of topics (e.g. digital security only emerged in the past few decades), could be confounders in our partisanship analysis. Although we mitigate their impact by randomly splitting instances per year across folds, we acknowledge that these confounders are worthy of further exploration. Our use case suggests itself as a testbed for causal modeling approaches that control for confounders explicitly (Feder et al., 2022).\nWe acknowledge that it\u2019s hard to obtain reliable features, i.e., important words that impact prediction results, out of large language models. Our analysis framework could be further enhanced by inspecting the highly-associated words for the parties.\nEthics Statement\nOur models predict party affiliation and explicitly not voting outcomes. We neither attempt nor are able to do voting prediction based on our analysis framework, noting the severe ethical concerns it would raise (including, but not limited to, associating individuals with predicted professional behavior of potentially low quality, that may reflect detrimentally on their reputation).\nOur analysis framework aims to understand how oral court arguments reveal the affiliated party of justices in the Supreme Court of the US. Although our study covers the analysis of individual justices, we make no presumptions of the values or beliefs of justices beyond what is in the public domain, nor do we target individual justices. All our analyses aggregate predictions over several cases per justice and we do not aim to predict or overly rely on individual votes or contributions of individuals. This research aids in understanding the US legal system, and we strongly advise against over-interpretation of the results in terms of behaviors of individual justices.\nOur case study focuses on a subset of historical oral arguments from SCOTUS. Although it covers most publicly-available cases, this does not reflect the full history of SCOTUS, nor represent the current state of the court.\nIt has been shown that pretrained language models are biased (Delobelle et al., 2022; Nadeem et al., 2021). Although our evaluation validates our results against external sources, we acknowledge a possible impact of biases in language representations (e.g., resulting from a prevalence of liberal or conservative sources in the pre-training corpora)."
        },
        {
            "heading": "A Data Construction",
            "text": "We removed all nonlinguistic information from the transcripts, including indicators of cross-talk (e.g. [voice overlap], [interruption]), nonverbal expressions (e.g. [laughter], [sighs], [applause]), and procedural markers (e.g. [luncheon], [recess]). The full list is made available as part of the code repository."
        },
        {
            "heading": "B Distribution of Gold and Predicted Affiliated Party",
            "text": "Figures 5 shows the label distribution of gold vs. predicted for affiliated party prediction over one randomly-selected fold."
        },
        {
            "heading": "C Detailed Experiment on Justice Identification Prediction",
            "text": "Table 5 provides a detailed experiment on justice identification prediction with and without year tags on instances."
        },
        {
            "heading": "D Comparison of Voting-based and Language-based Partisanship at Individual Justice Level",
            "text": "Figure 6 shows the comparison of voting-based (MQ scores; Martin and Quinn (2007) and language partisanship measures of all individual justices over their tenure.\nFigure 7 shows the Comparison of all justice partisanship in their first year and over their tenure."
        },
        {
            "heading": "E High Profile SCOTUS Cases",
            "text": "Table 6 lists the details of the high-profile SCOTUS cases analyzed in Section 4, Figure 4."
        }
    ],
    "title": "More than Votes? Voting and Language based Partisanship in the US Supreme Court",
    "year": 2023
}