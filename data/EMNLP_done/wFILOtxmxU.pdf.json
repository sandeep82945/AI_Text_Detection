{
    "abstractText": "Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with tokenlevel retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrievalbased mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect code. Computationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose kNN-TRANX, a token-level retrieval augmented code generation method. kNN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate kNN-TRANX on two public datasets and the experimental results confirm the effectiveness of our approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiangyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Yu Zhou"
        },
        {
            "affiliations": [],
            "name": "Guang Yang"
        },
        {
            "affiliations": [],
            "name": "Taolue Chen"
        }
    ],
    "id": "SP:091c97c204079478498143cfd4d3c0ce08d7c600",
    "references": [
        {
            "authors": [
                "Wasi Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang."
            ],
            "title": "Unified pre-training for program understanding and generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Nathana\u00ebl Beau",
                "Benoit Crabb\u00e9."
            ],
            "title": "The impact of lexical and grammatical processing on generating code from natural language",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2204\u20132214, Dublin, Ireland. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "CoRR, abs/2207.10397.",
            "year": 2022
        },
        {
            "authors": [
                "Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "CoRR, abs/2107.03374.",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Hsiu Dai",
                "Zhirui Zhang",
                "Qiuzhi Liu",
                "Qu Cui",
                "WeiHong Li",
                "Yichao Du",
                "Tong Xu."
            ],
            "title": "Simple and scalable nearest neighbor machine translation",
            "venue": "ArXiv, abs/2302.12188.",
            "year": 2023
        },
        {
            "authors": [
                "M. Amin Farajian",
                "Marco Turchi",
                "Matteo Negri",
                "Marcello Federico."
            ],
            "title": "Multi-domain neural machine translation through unsupervised adaptation",
            "venue": "Proceedings of the Second Conference on Machine Translation, pages 127\u2013137, Copenhagen, Denmark.",
            "year": 2017
        },
        {
            "authors": [
                "Zhangyin Feng",
                "Daya Guo",
                "Duyu Tang",
                "Nan Duan",
                "Xiaocheng Feng",
                "Ming Gong",
                "Linjun Shou",
                "Bing Qin",
                "Ting Liu",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "CodeBERT: A pre-trained model for programming and natural languages",
            "venue": "Findings of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Daya Guo",
                "Shuai Lu",
                "Nan Duan",
                "Yanlin Wang",
                "Ming Zhou",
                "Jian Yin."
            ],
            "title": "UniXcoder: Unified crossmodal pre-training for code representation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Shirley Anugrah Hayati",
                "Raphael Olivier",
                "Pravalika Avvaru",
                "Pengcheng Yin",
                "Anthony Tomasic",
                "Graham Neubig."
            ],
            "title": "Retrieval-based neural code generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Hui Jiang",
                "Ziyao Lu",
                "Fandong Meng",
                "Chulun Zhou",
                "Jie Zhou",
                "Degen Huang",
                "Jinsong Su."
            ],
            "title": "Towards robust k-nearest-neighbor machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2019
        },
        {
            "authors": [
                "Nora Kassner",
                "Hinrich Sch\u00fctze."
            ],
            "title": "BERT-kNN: Adding a kNN search component to pretrained language models for better QA",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3424\u20133430, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Angela Fan",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Nearest neighbor machine translation",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161",
            "year": 2023
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem."
            ],
            "title": "Learning without forgetting",
            "venue": "Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pages",
            "year": 2016
        },
        {
            "authors": [
                "Yuxian Meng",
                "Xiaoya Li",
                "Xiayu Zheng",
                "Fei Wu",
                "Xiaofei Sun",
                "Tianwei Zhang",
                "Jiwei Li."
            ],
            "title": "Fast nearest neighbor machine translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 555\u2013565, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Sajad Norouzi",
                "Keyi Tang",
                "Yanshuai Cao."
            ],
            "title": "Code generation from natural language with less prior knowledge and more monolingual data",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Yusuke Oda",
                "Hiroyuki Fudaba",
                "Graham Neubig",
                "Hideaki Hata",
                "Sakriani Sakti",
                "Tomoki Toda",
                "Satoshi Nakamura."
            ],
            "title": "Learning to generate pseudo-code from source code using statistical machine translation",
            "venue": "2015 30th IEEE/ACM Interna-",
            "year": 2015
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Md Rizwan Parvez",
                "Wasi Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang."
            ],
            "title": "Retrieval augmented code generation and summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719\u20132734, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Poesia",
                "Oleksandr Polozov",
                "Vu Le",
                "Ashish Tiwari",
                "Gustavo Soares",
                "Christopher Meek",
                "Sumit Gulwani."
            ],
            "title": "Synchromesh: Reliable code generation from pre-trained language models",
            "venue": "arXiv preprint arXiv:2201.11227.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Ren",
                "Daya Guo",
                "Shuai Lu",
                "Long Zhou",
                "Shujie Liu",
                "Duyu Tang",
                "Neel Sundaresan",
                "Ming Zhou",
                "Ambrosio Blanco",
                "Shuai Ma."
            ],
            "title": "Codebleu: a method for automatic evaluation of code synthesis",
            "venue": "CoRR, abs/2009.10297.",
            "year": 2020
        },
        {
            "authors": [
                "Kurt Shuster",
                "Spencer Poff",
                "Moya Chen",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Retrieval augmentation reduces hallucination in conversation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Daniel C. Wang",
                "Andrew W. Appel",
                "Jeffrey L. Korn",
                "Christopher S. Serra."
            ],
            "title": "The zephyr abstract syntax description language",
            "venue": "Proceedings of the Conference on Domain-Specific Languages, DSL\u201997, Santa Barbara, California, USA, October",
            "year": 1997
        },
        {
            "authors": [
                "Yue Wang",
                "Hung Le",
                "Akhilesh Deepak Gotmare",
                "Nghi DQ Bui",
                "Junnan Li",
                "Steven CH Hoi."
            ],
            "title": "Codet5+: Open code large language models for code understanding and generation",
            "venue": "arXiv preprint arXiv:2305.07922.",
            "year": 2023
        },
        {
            "authors": [
                "Yue Wang",
                "Weishi Wang",
                "Shafiq Joty",
                "Steven C.H. Hoi."
            ],
            "title": "CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Frank F. Xu",
                "Zhengbao Jiang",
                "Pengcheng Yin",
                "Bogdan Vasilescu",
                "Graham Neubig."
            ],
            "title": "Incorporating external knowledge through pre-training for natural language to code generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Compu-",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Bowen Deng",
                "Edgar Chen",
                "Bogdan Vasilescu",
                "Graham Neubig."
            ],
            "title": "Learning to mine parallel natural language/source code corpora from stack overflow",
            "venue": "Proceedings of the 40th International Conference on Software Engineering:",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig."
            ],
            "title": "TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstra-",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig."
            ],
            "title": "Reranking for neural semantic parsing",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4553\u20134559, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Xin Zheng",
                "Zhirui Zhang",
                "Junliang Guo",
                "Shujian Huang",
                "Boxing Chen",
                "Weihua Luo",
                "Jiajun Chen."
            ],
            "title": "Adaptive nearest neighbor machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Zhu",
                "Qianfeng Zhao",
                "Yunzhe Lv",
                "Shujian Huang",
                "Siheng Zhao",
                "Sizhe Liu",
                "Jiajun Chen."
            ],
            "title": "knn-box: A unified framework for nearest neighbor generation",
            "venue": "arXiv preprint arXiv:2302.13574.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Neural code generation aims to map the input natural language (NL) to code snippets using deep learning. Due to its great potential to streamline software development, it has garnered significant attentions from both natural language processing and software engineering communities. Various methods have been explored to facilitate code generation (Yin and Neubig, 2018; Wang et al., 2021; Guo et al., 2022). Recent progress in neural machine translation (NMT) shows that the non-parametric knearest-neighbour machine translation (kNN-MT) approach may significantly boost the performance of standard NMT models (Khandelwal et al., 2021) and other text generation models (Kassner and Sch\u00fctze, 2020; Shuster et al., 2021) by equipping\n\u2217Corresponding author.\nthe models with a token-level retriever. In particular, this neural-retrieval-in-the-loop approach facilitates the integration of external knowledge into the pre-trained model and provides a simple yet effective method to update the model by switching the retrieval datastore, without fine-tuning the model parameters.\nCan such neural-retrieval-in-the-loop approach benefit neural code generation? Our preliminary experiments reveal three main issues (cf. the example in Table 1) if it is adopted outright. Firstly, the model performance may be negatively affected by the noise in the retrieved knowledge. For example, \"all\" does not match the intention of the description, but it is recognized as the target token by the retriever, resulting in the generation of incorrect code. Secondly, the code generated by kNN-MT cannot guarantee syntactic correctness, as demonstrated by the mismatching parentheses in the given example. Thirdly, the token-level retrieval method requires similarity search of the entire datastore at each time step of inference, which hinders the deployment of such approach.\nIn this paper, we propose a novel code generation approach, i.e. kNN-TRANX, to overcome the limitations of the neural-retrieval-in-the-loop paradigm for code generation tasks. The basic idea is to integrate symbolic knowledge to ensure the quality of the generated code and expedite the retrieval process. To achieve this, we leverage the sequence-to-tree (seq2tree) model to generate abstract syntax tree (AST), which is a hierarchical tree-like structure used to represent the code, rather than generate target code snippet directly. This enables us to use AST construction rules to guarantee\nthe syntactic correctness of the generated code and filter out retrieval noise.\nWe design kNN-TRANX as a two-step process (cf. Figure 2). In the first step, we construct two separated datastores, i.e., the syntactic datastore and the semantic datastore, based on the type of AST nodes. This allows us to determine the type of the next node to be predicted according to the grammar rules and query a specific datastore. In the second step, we utilize syntactic rules to filter out irrelevant knowledge and convert the similarity retrieval results of the current target token into a probability distribution, i.e., the kNN probability. This probability, together with probability from the neural network, yields the probability of the action to be used for AST generation via a learnable confidence parameter. It can help to minimize retrieval noise and dynamically exploit combinations of the two probabilities, resulting in improved code generation performance.\nTo evaluate the effectiveness of kNN-TRANX, we perform experiments on two publicly available code generation datasets (i.e., CoNaLa and Django). The experimental results show a 27.6% improvement in the exact match metric on the CoNaLa dataset and a 4.2% improvement in the BLEU metric on the Django dataset, surpassing five state-of-the-art models under comparison. Additionally, we conduct an experiment on model canonical incremental adaptation, which updates kNN-TRANX by switching the datastore. The experimental results demonstrate that our model can achieve performance comparable to fully finetuned models and reduce the trainable parameters by 7,000 times."
        },
        {
            "heading": "2 Background",
            "text": "In this section, we provide an overview of the kNNMT paradigm and the seq2tree model.\n2.1 kNN-MT\nThe kNN-MT paradigm (Khandelwal et al., 2021) is a translation mechanism that enhances the quality of model generation by incorporating an additional translation retriever. This allows NMT models to benefit from the retrieved knowledge. The paradigm comprises two main parts, namely, datastore building and model inferring. Datastore Building. The datastore consists of a set of key-value pairs, where the key is the decoder hidden state and the value is the corresponding\ntarget token. Formally, given a bilingual sentence pair (x, y) from the training corpus (X ,Y), a pretrained NMT model fNMT (\u00b7) generates the i-th context representation hi = fNMT (x, y<i), then the datastore D is constructed as follows.\nD = (K, V ) = \u22c3\n(x,y)\u2208(X ,Y)\n{(hi, yi), \u2200yi \u2208 y}\nModel Inferring. During inference, at time step i, given the already generated token y\u0302<i and the contextual information h\u0302i, the kNN-MT model generates yi by retrieving the datastore, which can be calculated as\npkNN(yi | x, y<i) \u221d \u2211\n(hj ,yj)\n1yi=yjexp ( \u2212dj T ) (1)\nwhere T is the temperature and dj indicates the l2 distance between query h\u0302i and the retrieved key hj ."
        },
        {
            "heading": "2.2 Seq2tree Model",
            "text": "The purpose of the seq2tree code generation models is to generate ASTs instead of directly outputting code snippets. Compared to the sequenceto-sequence (seq2seq) models, the seq2tree models ensure the syntactic correctness of the generated code. Among the seq2tree models, BertranX (Beau and Crabb\u00e9, 2022) was recently proposed and represented the state-of-the-art architecture. BertranX employs BERT to process the input natural language and features a grammar-based decoder.\nBertranX describes ASTs using sequences of actions based on ASDL (Wang et al., 1997) grammar, which gives concise notations for describing the abstract syntax of programming languages (cf. Figure 1 as an example). With ASDL, BertranX\ndefines two distinct types of actions that generate ASTs, i.e., PREDRULE and GENERATE. The first type is used for initiating the generation of a new node from its parent node, which we mark as syntactic nodes in this paper; the second type on the other hand, is used to produce terminal or primitive symbols that we mark as semantic nodes.\n3 kNN-TRANX\nThe workflow of kNN-TRANX is depicted in Figure 2. It consists of two main components: datastore building and model inferring."
        },
        {
            "heading": "3.1 Datastore Building",
            "text": "Given a pre-trained seq2tree model and the training dataset, we first parse code snippets to ASTs and generate all instances in the training corpus. This process allows us to capture and store the decoder representations along with their corresponding target tokens as key-value pairs. The actions that constitute an AST can be categorized into two groups: rules and primitives. These categories align with the actions of GENERATE and PREDRULE, respectively. As shown in Figure 3, the two types of nodes have significant differences in terms of type and quantity. Combining them into the same datastore could potentially reduce retrieval accuracy. Therefore, we employ separated datastores for each node type, referred to as the syntactic and semantic datastores respectively. Nodes representing the structural information (e.g., Expr and Call) are put into the syntactic datastore, while nodes representing the semantic information (e.g., text and split) of the code are put into the semantic one.\nGiven an NL-code pair (x, y) from the training corpus (X ,Y), we first transform the code snippets Y into AST representations Z . Next, we calculate the i-th context representation hi = f\u03b8(x, z<i), where f\u03b8(\u00b7) refers to the trained seq2tree model and z \u2208 Z . The datastore is constructed by taking hi\u2019s as keys and zi\u2019s as values. Namely,\nD(gra) = ( K, V (gra) ) =\n\u22c3 (x,z)\u2208(X ,Z) {(hi, zi) | zi \u2208 z & zi \u2208 rules} ,\nand D(pri) = ( K, V (pri) ) =\n\u22c3 (x,z)\u2208(X ,Z) {(hi, zi) | zi \u2208 z & zi \u2208 primitives} .\nAs a result, two separated symbolic datastores can be constructed based on the various types of target actions within the training set. Constructing datastores in this manner is more effective than storing both types of actions in a single datastore since it helps reduce noise during retrieval. Moreover, the subsequent token type can be determined based on grammar rules, facilitating the retrieval of a specific datastore and accelerating the retrieval process."
        },
        {
            "heading": "3.2 Model Inferring",
            "text": "The process of model inference can be divided into three main phases, as shown in Figure 2. First, the code fragment x is put into the trained model to generate the context representation hi and compute the neural network distribution (pNN). Then, we query the datastore using this representation to obtain the k-nearest-neighbor distribution (pkNN). Finally, we combine these two distributions to predict the target token. In the subsequent sections, we will discuss three pivotal components of kNNTRANX: syntax-constrained token-level retrieval, meta-k network, and confidence network. Syntax-constrained token-level retrieval. Given the current context representation hi generated by the model, we first calculate the l2 distance dj = l2(hi, h\u0302j) between the context representation hi and each neighbor (h\u0302j , z\u0302j) in the datastore to determine the k nearest neighbors. Previous studies (Meng et al., 2022; Dai et al., 2023) have restricted the search space based on potential input-output patterns to improve decoding efficiency and reduce the impact of noise. However, the restricted search space may also exclude some valuable knowledge.\nTo mitigate this problem, our approach features syntax-aware retrieval capability. In contrast to conventional seq2seq models, our model aims to generate ASTs that allow to incorporate symbolic knowledge and determine the retrieved tokens by means of syntactic rules. During the retrieval process, we can determine the type of the subsequent token based on the tokens already produced. If the next token is expected to represent the syntactic information, we just retrieve the syntactic datastore to accelerate the retrieval process, and vice versa. Additionally, we can also use the ASDL rules to exclude illegitimate tokens to reduce the amount of irrelevant information. For example, as seen in Figure 2, our model has already generated the\nnode Expr in the previous time step. It should be noticed that kNN-TRANX have retrieved Call and alias nodes according to the distances. However, the child nodes of Expr do not support alias in the ASDL grammar. In this way, we filter out these nodes from the search results to reduce noise and avoid excluding valuable information. Meta-k network. We retrieve k relevant pieces of knowledge from the datastore, and then map the distances between the query vector and the cached representation as probabilities. Empirically, the number of retrievals, k, is crucial in our model because too few retrievals may result in valuable information being ignored, while too many retrievals may introduce noise. To alleviate this problem, we employ the meta-k network (Zheng et al., 2021) to dynamically evaluate the weight of the retrieved knowledge. Meta-k network considers a range of values that are smaller than the upper bound K, instead of using a fixed value of k. Typically\nthe range is set as S = {0, 1, 2, 4, \u00b7 \u00b7 \u00b7 , 2log2\u230aK\u230b}. To evaluate the weight of each of the values, we use distance dj and the count of distinct values in top j neighbors cj as features and obtain a normalized weight by p\u03b2(k) = softmax (f\u03b2([d1, . . . , dK ; c1, . . . , cK ])) where f\u03b2(\u00b7) denotes the Meta-k Network. The prediction of kNN can be obtained by pkNN (zi|x, z\u0302<i) =\u2211 kr\u2208S p\u03b2 (kr) \u00b7 pkrNN (zi|x, z\u0302<i) where pkrNN indicates the kr-nearest-neighbor prediction results calculated as Equation (1). In this way, the kNN model can expand the search space while reducing the impact of the retrieval noise.\nConfidence network. In order to utilize the knowledge of the symbolic datastore while maintaining the generalization ability of the neural network, we combine two probability distributions by means of weighting. Previous studies have integrated the distributions of kNN and NN by using a fixed or learnable parameter \u03bb to measure their respective weights. Khandelwal et al. (2021) combine the probability distributions using fixed weights, but this approach fails to dynamically adjust the weights based on the distance retrieved. Zhu et al. (2023) adjust the weights based on the retrieval distance, but they overlook the confidence of the neural network output. Khandelwal et al. (2021) utilize the probability of the neural network and retrieval distance as features to dynamically select the value of \u03bb which consider the confidence of\nthe two distributions, but this approach neglects the correlation between the two distributions.\nTo address this issue, we propose a confidence network that estimates the confidence of both probabilities based on kNN and NN distributions. In addition, we incorporate the weights of each k-value as features into the confidence network, ensuring that the model is aware of the number of tokens that require attention. As such our model can capture the relationship between the two distributions. In cases where the two distributions conflict, we assign a higher weight to the distribution with higher confidence. The confidence \u03bb is calculated from W as \u03bbi = S(W[pkNN (zi | x, z\u0302<i) ; pNN (zi|x, z\u0302<i) ; p\u03b2(k)]), where S denotes the sigmoid activation function.\nThe final distribution at prediction zi is calculated as a weighted sum of two distributions with \u03bbi, i.e., p (zi|x, z\u0302<i) = \u03bbipkNN + (1\u2212 \u03bbi) pNN."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we first introduce the datasets and evaluation metrics. Then, we conduct a comprehensive study and analysis on code generation and model canonical incremental adaptation."
        },
        {
            "heading": "4.1 Datasets and evaluation metrics",
            "text": "We evaluate kNN-TRANX on two code generation datasets, namely, CoNaLa dataset (Yin et al., 2018) and Django dataset (Oda et al., 2015). The CoNaLa dataset comprises 600k NL-code pairs collected from StackOverflow, out of which 2,879 NL were rewritten by developers. This dataset contains questions that programmers encounter in their real-world projects. On the other hand, the Django dataset consists of 18,805 examples, where each example consists of one line of Python code accompanied by corresponding comments. Compared to CoNaLa, approximately 70% of the examples in Django are simple tasks that include variable assignment, method definition, and exception handling, easily inferred from the corresponding NL predictions. We employed BLEU (Papineni et al., 2002), CodeBLEU (Ren et al., 2020), and exact match (EM) metrics to assess the performance of our experiments."
        },
        {
            "heading": "4.2 Code Generation",
            "text": "Implementation details. We use BertranX as the base seq2tree model for our experiments, which is trained on annotated data and 100k mined data.\nTo expedite the implementation, we leverage kNNbox (Zhu et al., 2023), an open-source toolkit for building kNN-MT, to implement kNN-TRANX. As explained in Section 3, kNN-TRANX creates two datastores. Due to the considerable difference in vocabulary sizes between the two datastores, we construct separate settings for the syntactic and semantic datastores. For the syntactic datastore, we set the upper limit Krule to 4 to account for its limited token variety. For the semantic datastore, we set Kpri to 64. To train the meta-k network and confidence network, we employ the AdamW optimizer with a learning rate of 3e-4. To accelerate the datastore retrieval process, we incorporate the FAISS library (Johnson et al., 2019) for similarity retrieval. All experiments are performed on a single NVIDIA 2080Ti. Baselines. We compare kNN-TRANX against five state-of-the-art code generation models.\n\u2022 TRANX (Yin and Neubig, 2018) is a seq2tree model consisting of a bidirectional LSTM encoder for learning the semantic representation and a decoder for outputting a sequence of actions for constructing the tree. \u2022 Reranker (Yin and Neubig, 2019) reorders a set of N-best candidates to improve the quality of the generated results. \u2022 Ext-codegen (Xu et al., 2020) incorporates API documentation as external knowledge into the model, thus enabling data augmentation. \u2022 TAE (Norouzi et al., 2021) uses BERT and a transformer decoder to auto-encoding monolingual data. \u2022 BertranX (Beau and Crabb\u00e9, 2022) uses BERT as an encoder and serves as the base model for our kNN-TRANX. \u2022 REDCODER (Parvez et al., 2021) retrieves relevant code from a retrieval database and provides them as a supplement to code generation models. \u2022 CodeT5 (Wang et al., 2021) builds on the similar architecture of T5 (Raffel et al., 2020) but incorporates code-specific knowledge to endow the model with better code understanding.\nMain results. The experimental results are presented in Table 2. Our proposed kNN-TRANX exhibits a superior performance over BertranX on the CoNaLa dataset by 3.11 BLEU (9.1%), 2.95 CodeBLEU (8.2%), and 1.6 EM (27.6%). On the Django dataset, we observed improvements of 3.34 BLEU (4.2%), 2.81 CodeBLEU (3.6%), and 2.39 EM (3.0%). These re-\nsults indicate that token-level retrieval can improve the performance of code generation models, and the improvement is more evident for challenging tasks. To demonstrate that our model can generate code with more accurate syntax, we provide the syntax match score (Ren et al., 2020) in Figure 4, which reflects the degree of syntax matching in code. The results indicate that our model outperforms the baseline model in terms of syntax accuracy.\nAnalysis. We conduct an ablation study of kNNTRANX as shown in Table 3. The experimental results demonstrate that retrieval filtering method can significantly enhance the performance of the code generation models. For the method that combines NN distribution and kNN distribution, we compared the method of measuring retrieval distance proposed in adaptive kNN-box (Zhu et al., 2023). Experimental results show that our approach of considering both distributions comprehensively achieves better results. We also evaluate the effect of placing both types of action in the\nsame datastore. The result shows that this approach significantly reduces the quality of the generated code by introducing a substantial amount of noise. Moreover, we analyze the effect of Krule and Kpri on the experimental results, as presented in Table 4. The results align with our conjecture that retrieving a small number of the syntactic nearest neighbors and a relatively large number of semantic entries leads to better code generation. Furthermore, Figure 5 shows that increasing the size of datastore can improve the quality of generated code. Additionally, Figure 5(a) and Figure 5(d) depict that even a small amount of syntactic knowledge stored can achieve a high quality of code generation. In contrast, the quality of generated code keeps improving as the size of semantic datastore increases. We believe this is because semantic knowledge is relatively scarce compared to syntactic knowledge, which can be demonstrated by Figure 3."
        },
        {
            "heading": "4.3 Model Canonical Incremental Adaptation",
            "text": "Implementation details. Although the recent proposed large-scale models such as Codex (Chen et al., 2021) and GPT-4 (OpenAI, 2023) have demonstrated its powerful code generation capabilities, one of the challenges is that the trained models are difficult to update. Models need to be continuously trained via incremental learning, which consumes huge computing resources. To make matters worse, incremental learning with new data can lead to catastrophic forgetting problems (Li and Hoiem, 2016). To address this, we validate our model using incremental learning by only updating the datastore without adapting the model parameters. We use BertranX\u20201 as our base model to simulate the\n1BertranX\u2020 is trained on annotated data and 5k mined data on CoNaLa dataset.\nscenario for incremental learning. Then, we update the datastore using {0k, 10k, 50k, 95k} mined data in CoNaLa dataset respectively. In our experiment, we update the datastore to perform efficient finetuning. Compared to the method of fine-tuning all parameters (which requires training 122,205k parameters), our method only needs to train 17k, greatly reducing the GPU memory consumption required for training, and achieving comparable results to fine-tuning all parameters.\nIt is worth noting that, compared to the previous kNN generative model, our model includes two datastores for syntactic and semantic information, respectively. There are 109 kinds of token in the syntactic datastore, and the number of corresponding datastore entries is 1,603k. However, the types of token in the semantic datastore may be infinite, depending on the actual defined vocabulary, so the knowledge corresponding to each vocabulary is relatively scarce, with only 518k entries in this ex-\nperiment. Figure 5 confirms that providing a small amount of syntactic knowledge can improve the performance of the model. Therefore, we consider two ways to update the datastores in our experiments, i.e., updating both datastores and updating the semantic datastore only. Main results. We adopte the same evaluation metrics as code generation. As shown in Table 5, firstly, without using additional datastore, kNNTRANX\u2020 can outperform BertranX\u2020. As the knowledge in the datastore is constantly updated, we can see that kNN-TRANX has improved on three evaluation criteria. In terms of BLEU evaluation, kNN-TRANX\u2020 with 95k external data can achieve performance comparable to that of training-based BertranX. Furthermore, we update only the semantic datastore, which can also be effective. We also provide two examples to demonstrate how knearest-neighbor retrieval assists in model decisionmaking in Appendix A.1. It should be noted that the CoNaLa dataset was obtained through data mining, and the majority of the NL-code pairs obtained through mining are irrelevant, which greatly adds noise to our retrieval. Therefore, we believe that kNN-TRANX can perform even better on more reliable data sources through incremental learning."
        },
        {
            "heading": "5 Related Work",
            "text": "Code generation. Code generation aspires to generate target code through natural language to improve programmers\u2019 development efficiency. Yin and Neubig (2018) propose TRANX to generate ASTs instead of generating code snippets directly. Based on TRANX, Beau and Crabb\u00e9 (2022) propose BertranX relying on a BERT encoder and a grammar-based decoder. Poesia et al. (2022) propose a framework for substantially improving the reliability of pre-trained models for code generation. Chen et al. (2022) propose CodeT to leverage pre-trained language models to generate both\nthe code snippets along with test cases, and select the best code based on the number of test cases passed. Besides, researchers used pre-training methods to incorporate more external knowledge into the model, effectively improving its performance on downstream tasks (Feng et al., 2020; Wang et al., 2021; Ahmad et al., 2021). Recently, large-scale pre-training models have demonstrated remarkable capabilities in code generation (Li et al., 2023; Wang et al., 2023; Touvron et al., 2023).\nRetrieval-augmented models. Retrieving and integrating auxiliary sentences has shown effectiveness in enhancing the generation of class models. Farajian et al. (2017) propose retrieving similar sentences from the training set to adapt to different inputs. The work of Hayati et al. (2018) is built on the neural network model of AST driver and generates code by searching action subtrees. Parvez et al. (2021) propose using retrieved natural language code to improve the performance of code generation and code translation models using REDCODER.\nRecently, Khandelwal et al. (2021) propose kNN-MT, a non-parametric paradigm for construct-\ning a datastore using a decoder representation as a key and using the corresponding target character as a value. The generation is done by retrieving the top k neighbors as the result. Based on this paradigm, a number of optimization methods have also been proposed. Zheng et al. (2021) use a meta-k network to dynamically adjust the retrieval weights. Jiang et al. (2022) improve the robustness of kNN-MT in terms of both model structure and training methods. Meng et al. (2022) propose Fast kNN-MT, which improves retrieval efficiency by constructing multiple smaller datastores."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose kNN-TRANX. By providing syntactic and semantic datastores for seq2tree model, we are able to outperform the baselines. In addition, we provide more knowledge for the model by switch the datastores without fine-tuning the neural network. Experimental results show that kNN-TRANX exhibits competitive performance against learning-based methods through incremental learning. In the future, we plan to construct a smaller and more fine-grained syntactic datas-\ntore to reduce the search space of the model and accelerate model inference.\nLimitations\nAlthough our proposed approach can enhance the generalizability of the seq2tree model and enable rapid updates by switching datastores, incorporating extra datastores necessitates a similarity search at each time step of inference. Even though only one of the two datastores needs to be retrieved, the inference time for the model may still increase considerably (cf. Appendix A.2). Furthermore, the incorporation of additional datastores will consume storage space and may escalate with growing training data. Although the previous work has proposed approaches to reduce the content of datastore through pruning, this approach also leads to model performance degradation."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Natural Science Foundation of China (No. 61972197, No. 62372232), the Natural Science Foundation of Jiangsu Province (No. BK20201292), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. T. Chen is partially supported by an overseas grant from the State Key Laboratory of Novel Software Technology, Nanjing University (KFKT2022A03), Birkbeck BEI School Project (EFFECT) and National Natural Science Foundation of China (No. 62272397)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Case Study on Model Canonical Incremental Adaptation\nWe showcase two selected instances from the CoNaLa dataset in Table 6. For each instance, we present the output and probability distribution of the model when generating incorrect behavior. We also demonstrate how our method utilizes the kNN algorithm to enhance the model\u2019s decision-making process, thereby proving the effectiveness of our method. As shown in the first example, BertranX\u2020 demonstrates the ability to construct code with appropriate syntax, but it inaccurately generates the primitive legend. However, with the assistance of kNN-TRANX\u2020, the final decision of the model changes through retrieval, resulting in the desired code. In the second example, BertranX\u2020 makes an error when generating the second token. kNNTRANX\u2020, utilizing an updated syntactic datastore, produces code that is semantically close to the target code. In contrast, kNN-TRANX\u2020, which did not update the syntactic datastore, has drawbacks in the generation of correct code due to insufficient syntactic node information, despite having improved retrieval efficiency and datastore size.\nA.2 Decoding Time We compared the decoding time of BertranX, kNNMT, and kNN-TRANX using the CoNaLa test set. During decoding, the beam size was set to 15. The results are presented in Table 7."
        }
    ],
    "title": "Syntax-Aware Retrieval Augmented Code Generation",
    "year": 2023
}