{
    "abstractText": "Pre-trained language models (PLMs) have ignited a surge in demand for effective finetuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. In parallel, adapter modules designed for parameter-efficient fine-tuning (PEFT) have demonstrated notable potential in low-resource settings. However, the interplay between AL and adapter-based PEFT remains unexplored. We present an empirical study of PEFT behavior with AL in low-resource settings for text classification tasks. Our findings affirm the superiority of PEFT over full-fine tuning (FFT) in low-resource settings and demonstrate that this advantage persists in AL setups. We further examine the properties of PEFT and FFT through the lens of forgetting dynamics and instance-level representations, where we find that PEFT yields more stable representations of early and middle layers compared to FFT. Our research underscores the synergistic potential of AL and PEFT in low-resource settings, paving the way for advancements in efficient and effective fine-tuning.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Josip Juki\u0107"
        },
        {
            "affiliations": [],
            "name": "Jan \u0160najder"
        }
    ],
    "id": "SP:e69e0e9d993169af780a36d79b3e95cd1439db92",
    "references": [
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "MAD-G: Multilingual adapter generation for efficient cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Robert Baldock",
                "Hartmut Maennel",
                "Behnam Neyshabur."
            ],
            "title": "Deep learning through the lens of example difficulty",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 10876\u2013 10889. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Jordan."
            ],
            "title": "Active learning with statistical models",
            "venue": "Journal of artificial intelligence research, 4:129\u2013145.",
            "year": 1996
        },
        {
            "authors": [
                "Sanjoy Dasgupta."
            ],
            "title": "Two faces of active learning",
            "venue": "Theoretical Computer Science, 412(19):1767\u20131781. Algorithmic Learning Theory (ALT 2009).",
            "year": 2011
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Dodge",
                "Gabriel Ilharco",
                "Roy Schwartz",
                "Ali Farhadi",
                "Hannaneh Hajishirzi",
                "Noah Smith."
            ],
            "title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
            "venue": "arXiv preprint arXiv:2002.06305.",
            "year": 2020
        },
        {
            "authors": [
                "Liat Ein-Dor",
                "Alon Halfon",
                "Ariel Gera",
                "Eyal Shnarch",
                "Lena Dankin",
                "Leshem Choshen",
                "Marina Danilevsky",
                "Ranit Aharonov",
                "Yoav Katz",
                "Noam Slonim."
            ],
            "title": "Active Learning for BERT: An Empirical Study",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani."
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning",
            "year": 2016
        },
        {
            "authors": [
                "Daniel Gissin",
                "Shai Shalev-Shwartz."
            ],
            "title": "Discriminative active learning",
            "venue": "arXiv preprint arXiv:1907.06347.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Grie\u00dfhaber",
                "Johannes Maucher",
                "Ngoc Thang Vu."
            ],
            "title": "Fine-tuning BERT for low-resource natural language understanding via active learning",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1158\u20131171,",
            "year": 2020
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Ruidan He",
                "Linlin Liu",
                "Hai Ye",
                "Qingyu Tan",
                "Bosheng Ding",
                "Liying Cheng",
                "Jiawei Low",
                "Lidong Bing",
                "Luo Si."
            ],
            "title": "On the effectiveness of adapter-based tuning for pretrained language model adaptation",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Josip Juki\u0107",
                "Jan \u0160najder."
            ],
            "title": "Smooth sailing: Improving active learning for pre-trained language models with representation smoothness analysis",
            "venue": "Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD), pages 11\u201324, Gothenburg,",
            "year": 2023
        },
        {
            "authors": [
                "Siddharth Karamcheti",
                "Ranjay Krishna",
                "Li Fei-Fei",
                "Christopher Manning."
            ],
            "title": "Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Sebastian Ruder",
                "Mostafa Dehghani",
                "James Henderson."
            ],
            "title": "Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Seungwon Kim",
                "Alex Shum",
                "Nathan Susanj",
                "Jonathan Hilgart."
            ],
            "title": "Revisiting pretraining with adapters",
            "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 90\u201399, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey Hinton."
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "International Conference on Machine Learning, pages 3519\u20133529. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Jaeseong Lee",
                "Seung-won Hwang",
                "Taesup Kim."
            ],
            "title": "FAD-X: Fusing adapters for cross-lingual transfer to low-resource languages",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the",
            "year": 2022
        },
        {
            "authors": [
                "David D Lewis",
                "William A Gale."
            ],
            "title": "A sequential algorithm for training text classifiers",
            "venue": "SIGIR\u201994, pages 3\u201312. Springer.",
            "year": 1994
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Xin Li",
                "Dan Roth."
            ],
            "title": "Learning question classifiers",
            "venue": "COLING 2002: The 19th International Conference on Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "Yuning Mao",
                "Lambert Mathias",
                "Rui Hou",
                "Amjad Almahairi",
                "Hao Ma",
                "Jiawei Han",
                "Scott Yih",
                "Madian Khabsa."
            ],
            "title": "UniPELT: A unified framework for parameter-efficient language model tuning",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Katerina Margatina",
                "Loic Barrault",
                "Nikolaos Aletras."
            ],
            "title": "On the importance of effectively adapting pretrained language models for active learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
            "year": 2022
        },
        {
            "authors": [
                "Katerina Margatina",
                "Giorgos Vernikos",
                "Lo\u00efc Barrault",
                "Nikolaos Aletras."
            ],
            "title": "Active learning by acquiring contrastive examples",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650\u2013663, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Marius Mosbach",
                "Maksym Andriushchenko",
                "Dietrich Klakow."
            ],
            "title": "On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278,",
            "year": 2004
        },
        {
            "authors": [
                "Marinela Parovi\u0107",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterHub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti."
            ],
            "title": "Modular deep learning",
            "venue": "arXiv preprint arXiv:2302.11529.",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Schr\u00f6der",
                "Andreas Niekler",
                "Martin Potthast."
            ],
            "title": "Revisiting uncertainty-based query strategies for active learning with transformers",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2194\u20132203, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Ozan Sener",
                "Silvio Savarese."
            ],
            "title": "Active learning for convolutional neural networks: A core-set approach",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Burr Settles."
            ],
            "title": "Active learning literature survey",
            "venue": "Computer sciences technical report, University of Wisconsin-Madison.",
            "year": 2009
        },
        {
            "authors": [
                "Artem Shelmanov",
                "Dmitri Puzyrev",
                "Lyubov Kupriyanova",
                "Denis Belyakov",
                "Daniil Larionov",
                "Nikita Khromov",
                "Olga Kozlova",
                "Ekaterina Artemova",
                "Dmitry V. Dylov",
                "Alexander Panchenko"
            ],
            "title": "Active learning for sequence tagging with deep",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "John Bauer",
                "Christopher D. Manning",
                "Andrew Y. Ng."
            ],
            "title": "Parsing with compositional vector grammars",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465,",
            "year": 2013
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research, 15(56):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Cory Stephenson",
                "Suchismita Padhy",
                "Abhinav Ganesh",
                "Yue Hui",
                "Hanlin Tang",
                "SueYeon Chung."
            ],
            "title": "On the geometry of generalization and memorization in deep neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J. Gordon."
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "International Conference on Learning Representa-",
            "year": 2019
        },
        {
            "authors": [
                "Yue Yu",
                "Lingkai Kong",
                "Jieyu Zhang",
                "Rongzhi Zhang",
                "Chao Zhang."
            ],
            "title": "AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Michelle Yuan",
                "Hsuan-Tien Lin",
                "Jordan BoydGraber."
            ],
            "title": "Cold-start active learning through selfsupervised language modeling",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7935\u20137948,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Felix Wu",
                "Arzoo Katiyar",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Revisiting few-sample BERT fine-tuning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (PLMs) have quickly become a staple in the field of natural language processing. With the growing demand for data for training these models, developing efficient finetuning methods has become critical. This is particularly relevant for many domains and languages where obtaining large amounts of labeled training data is difficult or downright impossible. In such low-resource settings, it becomes essential to effectively leverage and adapt PLMs while minimizing the need for extensive labeled data.\n1Our code is available at https://github.com/ josipjukic/adapter-al\nData labeling is notoriously time-consuming and expensive, often hindering the development of sizable labeled datasets required for training highperformance models. Active learning (AL) (Cohn et al., 1996; Settles, 2009) has emerged as a potential solution to this challenge. In contrast to passive learning, in which the training set is sampled at random, AL encompasses a unique family of machine learning algorithms specifically designed to reduce labeling costs by reducing label complexity, i.e., the number of labels required by an acquisition model to achieve a certain level of performance (Dasgupta, 2011). With the advent of PLMs, AL research has pivoted towards investigating training regimes for PLMs, such as task-adaptive pre-training (TAPT; Gururangan et al., 2020), that could be combined with AL to further reduce the label complexity.\nWhile AL aims at directly minimizing the label complexity of learning, training efficiency can also be improved by reducing the parameter complexity of the model. This becomes more important as PLMs grow larger, and fine-tuning becomes increasingly challenging due to the sheer number of parameters involved. To address this issue, adapters (Houlsby et al., 2019) have been introduced as compact modules that can be incorporated between the layers of PLMs. Adapters enable considerable parameter-sharing, facilitating parameterefficient fine-tuning (PEFT) through modular learning (Pfeiffer et al., 2023). In this process, only the parameters of the adapters are updated during the tuning for a specific downstream task. Recent research (He et al., 2021; Li and Liang, 2021; Karimi Mahabadi et al., 2021) has revealed that some PEFT methods outperform full fine-tuning (FFT) in low-resource settings, potentially due to better stability and a decreased risk of overfitting. In contrast, FFT has been shown to exhibit instability in scenarios with limited data.\nDespite the promising results demonstrated by PEFT methods in low-resource settings, there is a\nstriking gap in research on parameter-efficient training with respect to how PEFT interacts with AL. Given that the majority of real-world AL scenarios involve a restricted amount of data, PEFT methods emerge as strong candidates for AL acquisition models. However, there has been no exploration of AL in conjunction with adapters. Investigating this uncharted territory can further advance our understanding of AL and reveal novel strategies for optimizing performance in low-resource settings.\nIn this paper, we present an empirical study on the behavior of PEFT in low-resource settings for text classification tasks. We analyze PEFT with and without AL and compare it against FFT. While our results confirm that PEFT exhibits superior performance in low-resource setups compared to FFT, we show that the improved performance with PEFT extends to AL scenarios in terms of performance gains over passive learning. Furthermore, we analyze the efficacy of TAPT in conjunction with AL and PEFT. We find that TAPT is beneficial in AL scenarios for both PEFT and fully fine-tuned models, thus representing a viable technique for improving performance in low-resource settings. Finally, aiming to illuminate why PEFT and TAPT improve AL performance in low-resource settings, we analyze the properties of PEFT and FFT via forgetting dynamics (Toneva et al., 2019) and PLMs\u2019 instance-level representations. We find that AL methods choose fewer unforgettable and more moderately forgettable examples when combined with PEFT and TAPT, where forgetfulness indicates the model\u2019s tendency to learn and forget the gold label of a particular instance. Compared to FFT, we observe that PEFT yields representations in the early and middle layers of a model that are more similar to the representations of the base PLM. We hypothesize that this property mitigates the issue of forgetting the knowledge obtained during pretraining when fine-tuning for downstream tasks.\nIn summary, we show that in AL low-resource settings for text classification, (1) PEFT yields greater performance improvements compared to FFT and (2) TAPT enhances the overall classification performance of adapters and is well-suited for AL scenarios. We also show that (3) AL methods choose fewer unforgettable and more moderately forgettable examples with PEFT and that (4) PEFT produces instance-level representations of early and middle layers that are more similar to the base PLM than FFT. Our results uncover the intrica-\ncies of positive interactions between AL, PEFT, and TAPT, providing empirical justification for their combined use in low-resource settings."
        },
        {
            "heading": "2 Related Work",
            "text": "Our research involves combining AL with PLMs and investigating the use of PEFT techniques within the confines of low-resource settings.\nAL with PLMs. Until recently, the conventional approach for integrating PLMs with AL involved performing full fine-tuning with a fixed number of training epochs and training the model from scratch in each AL step (Ein-Dor et al., 2020; Margatina et al., 2021; Shelmanov et al., 2021; Karamcheti et al., 2021; Schr\u00f6der et al., 2022). However, studies by Mosbach et al. (2021) and Zhang et al. (2021) revealed that fine-tuning in low-resource setups is prone to instability, particularly when training for only a few epochs. This instability, often sensitive to weight initialization and data ordering (Dodge et al., 2020), presents a significant challenge for AL, which frequently operates in lowresource settings. Recent research has looked into the impact of PLM training regimes on AL performance (Grie\u00dfhaber et al., 2020; Yuan et al., 2020; Yu et al., 2022), suggesting that the choice of training regime is more critical than the choice of the AL method. Notably, TAPT has proven particularly effective in enhancing AL performance (Margatina et al., 2022; Jukic\u0301 and \u0160najder, 2023).\nAdapters in low-resource settings. Research on adapters in low-resource settings has primarily focused on areas such as cross-lingual transfer for low-resource languages (Ansell et al., 2021; Lee et al., 2022; Parovic\u0301 et al., 2022), where the emphasis lies on exploring diverse methods of fusing adapters. In monolingual settings with scarce data, adapters have been found to outperform full finetuning (Li and Liang, 2021; Mao et al., 2022). A study by He et al. (2021) demonstrated that adapterbased tuning exhibits enhanced stability and generalization capabilities by virtue of being less sensitive to learning rates than traditional fine-tuning methods. While incorporating task adaptation techniques, such as TAPT, has been shown to match or even improve performance over FFT in lowresource setups, Kim et al. (2021) noted an interesting caveat: the benefits of integrating TAPT with adapters tend to taper off as the amount of data increases.\nDespite the established effectiveness of adapters in setups with limited resources, their integration into AL frameworks \u2014 which frequently face analogous resource constraints \u2014 remains an untapped area of research. This gap is particularly notable given that AL\u2019s iterative learning process could significantly benefit from adapters\u2019 parameter efficiency and transferability, especially in scenarios where data scarcity or labeling costs are primary concerns."
        },
        {
            "heading": "3 Preliminaries",
            "text": "We now describe the experimental setup, providing details on the datasets as well as the PEFT and AL methods used in our study."
        },
        {
            "heading": "3.1 Datasets",
            "text": "We employ four single-text classification tasks commonly used for AL evaluation: (1) the subjectivity dataset (SUBJ; Pang and Lee, 2004), designed to assess the subjectivity of a given text; (2) the question type classification dataset (TREC; Li and Roth, 2002), designed for categorizing questions according to their types; (3) the Stanford Sentiment Treebank (SST; Socher et al., 2013), which focuses on sentiment analysis; (4) AG\u2019s news classification dataset (AGN; Zhang et al., 2015), which classifies news articles into different categories. We provide the dataset statistics in the appendix for further reference (cf. Appendix Table 3)."
        },
        {
            "heading": "3.2 PEFT methods",
            "text": "We consider four prototypical PEFT techniques:\nAdapter incorporates trainable bottleneck layers after both the multi-head attention and feedforward block in each Transformer layer (Houlsby et al., 2019);\nPrefix-tuning adds new parameters in the multihead attention blocks within each Transformer layer (Li and Liang, 2021);\nLoRA (Low-rank adaptation) represents an additive method that incorporates trainable lowrank decomposition matrices into the layers of a pre-trained model (Hu et al., 2022);\nUniPELT combines multiple PEFT approaches, namely LoRA, Prefix-tuning, and Adapter, in a single unified setup (Mao et al., 2022). Each constituent is a submodule, and UniPELT employs gating mechanisms to activate them effectively.\nAll of the above PEFT methods fall under the category of lightweight fine-tuning. While prefixtuning does not technically qualify as an adapter, He et al. (2022) demonstrated that it shares formal similarities with adapters, with prefix-tuning performing weighted addition and an adapter employing unweighted addition. We refer to all four considered methods as adapters for terminological simplicity. We use BERT (Devlin et al., 2019) as the base PLM for every adapter. Additionally, we adhere to the hyperparameter settings for each adapter as recommended in the respective papers that introduced them (cf. Appendix A.2 for details)."
        },
        {
            "heading": "3.3 AL methods",
            "text": "Our study considers five sampling strategies, including random selection (RND) as a passive learning baseline. The other four strategies are AL methods originating from different families, chosen for their robustness (ability to perform well across various tasks) and widespread usage in the field:\nMaximum entropy (ENT; Lewis and Gale, 1994) comes from the family of uncertainty strategies. The method queries instances where the model is least certain based on the maximum entropy criterion of the prediction output;\nMonte Carlo dropout (MC; Gal and Ghahramani, 2016) resembles ENT but utilizes the stochasticity of forward passes with dropout layers (Srivastava et al., 2014) to estimate the entropy for a given instance;\nCore-set (CS; Sener and Savarese, 2018) encourages instance diversity by using the learned representations of the acquisition model. This method aims to minimize the distance between an example in the unlabeled set and its closest counterpart in the labeled subset;\nDiscriminative active learning (DAL; Gissin and Shalev-Shwartz, 2019) frames AL as a binary classification of instances into those that are labeled and those that are not, with the objective of making the labeled and unlabeled sets indistinguishable."
        },
        {
            "heading": "3.4 Experimental setup",
            "text": "In AL runs, we select 50 new examples in each step of each AL experiment, using 100 examples for the warm start (randomly sampled labeled data\nto initiate the model). To probe different PEFT approaches with and without AL in low-resource settings, we establish a labeling budget limit of 1, 000 instances. To sidestep the need for a validation set in our experiments, which is typically unavailable in real-world AL scenarios, we adopt the Besov early stopping (Jukic\u0301 and \u0160najder, 2023). This method utilizes the smoothness of Transformer layers to decide at which epoch to stop training.\nIn the case of TAPT, we pre-train the base model on a masked language modeling task using unlabeled training data. For adapters, we only update the injected parameters while keeping the remaining parameters of the base model frozen. This approach aligns with the primary function of adapters, which is to utilize a common base model across diverse tasks. For every setting, we perform five runs using different random seeds. We report the average F1 score at each sampling step (with and without AL for FFT and PEFT) to show the corresponding learning curve averaged over five runs. We provide details on training and hyperparameters in Appendix A.5."
        },
        {
            "heading": "3.5 Evaluation",
            "text": "To evaluate the overall performance of an AL method, we employ the area under the performance curve (AUC). In each individual AL step with a specific quantity of labeled examples, we measure the classification performance in terms of the F1 score. The overall AUC is calculated using the F1 scores obtained at each step. We advocate for using AUC alongside the AL curves, as AUC serves as a suitable approximation of AL feasibility through a summary numeric score, as recommended in recent AL literature (Schr\u00f6der et al., 2022; Jukic\u0301 and \u0160najder, 2023).\nAs our experiments involve different training regimes, we compare each AL sampling strategy SAL to passive learning SPL within the same training regime to isolate the effects of AL. The primary objective of AL is to improve label efficiency over passive learning. To test whether AL is successful, we calculate the relative improvement over passive learning (RIPL), which we define as follows:\nRIPL(SAL, SPL) = AUC(SAL)\u2212 AUC(SPL)\n1\u2212 AUC(SPL)\nIntuitively, RIPL estimates the proportion of maximum possible improvement achievable by a given AL method compared to the passive learning baseline. A score of 1 indicates the maximum theoret-\nical improvement, which would be tantamount to attaining an F1 score of 1 in the initial sampling step and sustaining that score throughout all steps. Conversely, a negative score indicates that the AL method performs worse than passive learning."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we first examine the performance of PEFT methods in comparison to FFT with passive learning and then proceed to analyze the application of PEFT in AL settings."
        },
        {
            "heading": "4.1 PEFT vs. FFT",
            "text": "Previous research on the use of adapters in lowresource settings (Li and Liang, 2021; Mao et al., 2022; He et al., 2021) has demonstrated that adapters perform comparable to, and sometimes even better than FFT. However, these findings were based on comparing FFT to a single adapter variant on a full dataset or evaluating the performance at only a few discrete points.\nIn the first part of our experiments, we build upon these findings by conducting a more nuanced analysis. We generate detailed learning curves that facilitate the comparison of multiple adapters with FFT under the passive learning setup. Our comparison, summarized by the AUC metric in Table 1, reveals that UniPELT and Prefix-tuning consistently outperform FFT with a significant difference across all datasets used in our study. Conversely, the performance of Adapter and LoRA is mostly comparable to FFT, although there are cases where they either outperform or underperform FFT. In cases in which Adapter and LoRA perform better than FFT with significant differences, the degree of improvement is smaller than what is observed with UniPELT and Prefix-tuning.\nNext, we look into how the models\u2019 performance changes as the training set increases. To that end, we show the corresponding learning curves for adapters and FFT in Figure 1. The performance disparities between adapters and FFT become more apparent under conditions of extreme data scarcity (100\u2013300 labeled instances). Notably, the greatest differences in performance occur at the initial step (only 100 labels). This highlights the promise of adapter-based methods in low-resource settings, particularly for Prefix-tuning and UniPELT."
        },
        {
            "heading": "4.2 PEFT with AL",
            "text": "Motivated by our initial findings on using PEFT under the passive learning setup, where PEFT exhibited promising properties in low-resource settings, we further explore the behavior of adapters in AL scenarios. We evaluate individual PEFT methods in AL scenarios with and without using TAPT in terms of gains over random sampling (passive learning) using the RIPL metric described in Section 3.5. Table 2 shows the results for different combinations of AL methods and adapters, evaluated through the RIPL metric. We complement these results with absolute values in terms of AUC (cf. Appendix Table 5). For FFT without TAPT, DAL achieved the highest RIPL score on two datasets, while CS and MC topped the chart on one dataset each. When we incorporated TAPT, ENT yielded the best results on three out of four datasets, with CS leading on one. Looking at adapters, the most successful AL methods without TAPT vary, depending on the specific adapter and dataset in question. Interestingly, when TAPT is applied, the best results for all adapters are obtained either by ENT or MC. We speculate this could be attributed to solid compatibility between\nentropy-based methods and TAPT when adapters are employed.\nFurthermore, we observe that without TAPT, adapters achieve larger gains over FFT. However, when TAPT is applied, FFT becomes comparable to PEFT, although Prefix-tuning and UniPELT still yield the greatest improvements, depending on the dataset and AL method used. In Figure 2, we select the adapters that achieved the best improvement according to Table 2 without TAPT and show their RIPL value compared against FFT as well as their corresponding version when TAPT is applied. We conjecture that TAPT reduces the performance gap between adapters and FFT by inducing FFT to emulate PEFT in aspects such as training dynamics and representation space \u2014 a hypothesis we explore in more detail in Section 5.\nWe further investigate the behavior of adapters with AL throughout the individual steps. Figure 3 shows the learning curves for corresponding adapter models with and without applying TAPT. Due to space constraints, we show the learning curves only for the SUBJ dataset, as similar trends occur for other datasets. Without TAPT, the performance of adapters is largely independent of the specific AL method used, where Prefix-tuning and UniPELT consistently outperform Adapter and LoRA across all AL steps. With TAPT, the differ-\nences between AL and random sampling are more pronounced starting from the early steps, typically already with 200 instances. In contrast, the gap becomes more apparent only with 500 or more instances when TAPT is not employed."
        },
        {
            "heading": "5 Analysis",
            "text": "In Section 4, we have demonstrated that PEFT exhibits larger gains than FFT when combined with AL in low-resource settings, which is also accompanied by superior performance with passive leaning. To better understand why PEFT displays superior behavior with limited data, we now examine two specific properties of adapters and FFT models. First, we analyze the influence of TAPT on the forgetting dynamics during training. We continue with example-level representation analysis, where we investigate the representation similarity of PEFT and FFT to their respective base models."
        },
        {
            "heading": "5.1 Forgetting dynamics",
            "text": "We employ forgetting dynamics to compare PEFT and FFT\u2019s learning stability and their impact on AL data selection. The underlying hypothesis is that having fewer forgetting events in adapters would indicate a more stable and effective learning process. In utilizing forgetting dynamics, we draw upon the study by Toneva et al. (2019), focusing on the occurrence of forgetting events \u2014 cases where a specific training example transitions from correct to incorrect classification over the course of multiple learning epochs. More specifically, we divide the instances into three categories: (1) unforgettable instances, i.e., the ones that have never experienced a forgetting event during training, (2) instances that have encountered one or two forgetting events, labeled as moderately forgettable, and (3) instances subjected to three or more forgetting events, referred to as highly forgettable instances. As pointed out in the original study, moderately forgettable, ambiguous instances are more valuable for the learning model than unforgettable, easy instances. However, it is worth noting that AL is often hindered by too hard or impossible-to-learn examples (Karamcheti et al., 2021), which roughly correspond to the highly forgettable examples.\nFigure 4 shows the distribution of instances across the three categories of forgetting events for SUBJ and TREC datasets. We focus on these two datasets as examples of a simple binary classification task and a more complex multi-class classi-\nfication task, respectively. Specifically, we compare RND with MC, which achieves consistent performance improvements across all datasets. Our findings suggest that FFT tends to select a higher number of unforgettable instances and fewer moderately forgettable instances when compared to adapters. Interestingly, the adapters that perform best \u2014 Prefix-tuning and UniPELT \u2014 appear to favor moderately forgettable instances. However, when TAPT is applied, the discrepancies in forgetting profiles between FFT and the top two adapters, Prefix-tuning and UniPELT, seem to diminish. In contrast, TAPT amplifies the differences between FFT and the other two adapters, LoRA and Adapter, which typically show smaller improvements than Prefix-tuning and UniPELT. Given their superior AL performance, we hypothesize that the forgetting profiles of Prefix-tuning and UniPELT are more favorable compared to other adapters. Moreover, FFT with TAPT approaches the performance of the superior adapters and simultaneously develops a forgetting profile similar to theirs."
        },
        {
            "heading": "5.2 Representation analysis",
            "text": "To bolster our findings, we explore the representations of adapters and FFT models. As suggested in previous research (He et al., 2021; Li and Liang, 2021; Mao et al., 2022), adapters often display greater stability in terms of loss, especially in scenarios with limited resources. Our aim is to examine the stability of their representations and their relationship with overall AL performance.\nWe draw inspiration from research by Stephenson et al. (2021) and Baldock et al. (2021), which suggests that different layers of networks specialize in different features \u2014 earlier layers tend to acquire more generalized knowledge, while the deeper layers are more focused on task-specific information. This leads us to a layerwise examination of similarity. To analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model, we utilize centered kernel alignment (CKA) as a similarity measure between two sets of representations (Kornblith et al., 2019). It has been shown that PEFT methods result in representations closer to the base model at the token level (He et al., 2021). We extend the analysis to example-level representation to explore the behavior of models with AL. We opt for CKA as it is designed to be invariant to invertible linear transformation and still can measure meaningful similari-\nties between representations of higher dimensions than the number of data points. This stands in contrast to other metrics, which frequently falter when dealing with high-dimensional representations.\nFor a more direct comparison between PEFT and FFT, we analyze the differences between their respective similarities to their base models. Specifically, we compute the difference CKA(adapter, base)\u2212CKA(FFT, base) for a specific adapter or FFT and their base models. We hypothesize that superior PEFT performance with AL compared to FFT will be accompanied by a more similar early layer representation to the base model in PEFT. Figure 5 visualizes the layerwise difference in similarity between the base model and the adapter model and between the base model and the FFT model. We find that PEFT representations are more similar to the base model in the early and middle layers when compared to FFT. This\nholds for all AL methods, with differences more pronounced than in passive learning. Specifically, up to the eighth layer, representations are much more similar in adapters than in FFT models. In the final four layers, the difference in CKA scores between the adapter and FFT model is close to zero. Interestingly, the penultimate layer is more similar in the FFT model with respect to the base model.\nWhen fine-tuning on a downstream task, we believe that the increased stability of PEFT in earlier layers, relative to FFT, is instrumental in retaining the foundational knowledge from the PLM\u2019s pretraining phase. Conversely, PEFT exhibits more substantial transformations in the later, more taskspecific layers. This ensures the preservation of essential pre-trained knowledge while allowing for task-relevant flexibility. We speculate that this strategic balance in PEFT influences its propensity to select moderately forgettable instances when\ncombined with AL, contributing to its enhanced performance over FFT. These instances are neither too trivial to provide no learning value, nor are they too complex to risk misinterpretation, thereby enhancing the effectiveness of learning."
        },
        {
            "heading": "6 Conclusion",
            "text": "Our study has shed light on the advantages of parameter-efficient fine-tuning (PEFT) in lowresource settings, confirming its superiority over full fine-tuning (FFT) methods. Importantly, we have demonstrated that the integration of PEFT with active learning (AL) can offer substantial performance gains compared to passive learning, even in settings where labeled data is scarce. Furthermore, we highlighted the potential of task-adaptive pre-training (TAPT) to improve model performance further when used in conjunction with both PEFT and AL. We found that AL methods, in combination with PEFT, tend to select fewer unforgettable instances and more moderately forgettable examples. We further found that PEFT maintains the integrity of early and middle layer representations similar to the base model. We conjecture that this property mitigates forgetting during downstream task fine-tuning. These insights inform us of a possible underpinning mechanism that contributes to PEFT\u2019s superior performance and stability in low-resource settings. Overall, our work highlights the potential of PEFT and AL and establishes a foundation for developing increasingly efficient and cost-effective approaches for training models in low-resource settings."
        },
        {
            "heading": "Limitations",
            "text": "While our study advances the understanding of PEFT and AL\u2019s interaction in low-resource settings and uncovers intriguing insights about the forgetting dynamics during fine-tuning, it has a number of limitations.\nTo begin with, we have focused on text classification tasks, which are but one aspect of the wide range of potential applications for PLMs. Different tasks such as question answering, translation, or summarization might exhibit different behaviors under the same conditions. Consequently, the observed advantages of PEFT in the context of AL might not necessarily translate to other NLP tasks.\nNext, our results are limited to the specific PLMs, AL strategies, and PEFT methods we have examined in this study. While we have attempted\nto be comprehensive in our experiments, the outcomes might vary with different models, strategies, or methods. For example, the effectiveness of AL combined with PEFT might differ if other AL strategies are employed. Similarly, different types of adapter architectures could potentially lead to different results.\nAlthough we found that PEFT methods produce instance-level representations of early and middle layers more similar to the base PLM than FFT, a comprehensive understanding of how and why this similarity leads to increased stability and performance in low-resource settings is still lacking. Our hypothesis about the role of early and middle layer stability in mitigating the issue of forgetting the knowledge obtained during pre-training needs further substantiation.\nFinally, it is important to acknowledge the complexity and multifaceted nature of forgetting dynamics. While our investigation provides valuable insights about the interaction of forgetting with PEFT and TAPT in AL scenarios, a deeper understanding of the mechanisms of forgetting in the context of large PLMs is needed. Particularly, it would be interesting to investigate whether the balance between unforgettable and moderately forgettable instances selected by the AL methods changes as the size of the model or the amount of available data changes.\nFuture work should aim to address these limitations and further explore the mechanisms behind the promising results obtained with the combination of PEFT and AL. This will contribute to a more comprehensive understanding of the interaction between AL and PLMs, and help refine strategies for efficient fine-tuning in low-resource settings."
        },
        {
            "heading": "A Reproducibility",
            "text": ""
        },
        {
            "heading": "A.1 Dataset statistics",
            "text": "The sizes of the datasets per split are provided in Table 3. Predominantly, the datasets encompass texts in English."
        },
        {
            "heading": "A.2 Adapters",
            "text": "We use the implementation of adapters from AdapterHub (Pfeiffer et al., 2020).\nAdapter We set reduction factor to 16 and use swish function as nonlinearity.\nLoRA We include LoRA to the self-attention weights, intermediate, and output MLP weights of a model. We set the rank of the LoRA layer and the scaling factor \u03b1 to 8.\nPrefix-tuning We use tanh activation for Prefixtuning, with prefix length set to 30 and bottleneck size of 512.\nUniPELT We use Adapter, LoRA, and Prefixtuning as components of UniPELT with the same hyperparameters as described for individual components. The only exception is that we set the prefix length for Prefix-tuning to 10 instead of 30."
        },
        {
            "heading": "A.3 AL methods",
            "text": "MC In experiments, we use ten inference cycles to approximate the entropy of the output via Monte-Carlo dropout sampling.\nCS We use the [CLS] token representation from the Transformer\u2019s penultimate layer. We follow the greedy method described in the original work (Sener and Savarese, 2018)"
        },
        {
            "heading": "A.4 Preprocessing",
            "text": "We undertake a few pre-processing steps: convert all tokens to lowercase, eliminate nonalphanumeric tokens, and limit the token sequence to a maximum length of 200."
        },
        {
            "heading": "A.5 Hyperparameters",
            "text": "We use a fixed learning rate of 2 \u00d7 10\u22125 for FFT and 10\u22124 for adapters. Additionally, we set the gradient clipping to 1 during training. In our implementation of TAPT, we randomly mask 15% of tokens for both FFT models and adapters and train the model for 50 epochs with the learning rate set to 10\u22125."
        },
        {
            "heading": "A.6 Computing infrastructure",
            "text": "We conducted our experiments on 4\u00d7 AMD Ryzen Threadripper 3970X 32-Core Processors and 4\u00d7 NVIDIA GeForce RTX 3090 GPUs with 24GB of RAM. We used PyTorch version 1.9.0 and CUDA 11.4."
        },
        {
            "heading": "A.7 Average runtime",
            "text": "We report the average runtime of experiments in Table 4."
        },
        {
            "heading": "B Additional Results",
            "text": "We report the results that were omitted from the main part of the paper due to space constraints. Table 5 shows AUC scores for different combinations of AL methods and adapters, complementing the relative improvement scores as AUC represents absolute scores for each configuration. In Figure 6, we display the difference in similarities of adapters and FFT compared to their base models on the remaining three datasets."
        }
    ],
    "title": "Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings",
    "year": 2023
}