{
    "abstractText": "Neural Machine Translation (NMT) systems encounter a significant challenge when translating a pro-drop (\u2018pronoun-dropping\u2019) language (e.g., Chinese) to a non-pro-drop one (e.g., English), since the pro-drop phenomenon demands NMT systems to recover omitted pronouns. This unique and crucial task, however, lacks sufficient datasets for benchmarking. To bridge this gap, we introduce PROSE, a new benchmark featured in diverse pro-drop instances for document-level Chinese-English spoken language translation. Furthermore, we conduct an in-depth investigation of the prodrop phenomenon in spoken Chinese on this dataset, reconfirming that pro-drop reduces the performance of NMT systems in ChineseEnglish translation. To alleviate the negative impact introduced by pro-drop, we propose Mention-Aware Semantic Augmentation, a novel approach that leverages the semantic embedding of dropped pronouns to augment training pairs. Results from the experiments on four Chinese-English translation corpora show that our proposed method outperforms existing methods regarding omitted pronoun retrieval and overall translation quality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ke Wang"
        },
        {
            "affiliations": [],
            "name": "Xiutian Zhao"
        },
        {
            "affiliations": [],
            "name": "Yanghui Li"
        },
        {
            "affiliations": [],
            "name": "Wei Peng"
        }
    ],
    "id": "SP:d27e2b4924d0a9bccdb330fc8c62e6f951a82b88",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
            "year": 2015
        },
        {
            "authors": [
                "Olivier Chapelle",
                "Jason Weston",
                "L\u00e9on Bottou",
                "Vladimir Vapnik."
            ],
            "title": "Vicinal risk minimization",
            "venue": "Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000, Denver, CO, USA, pages 416\u2013",
            "year": 2000
        },
        {
            "authors": [
                "Yong Cheng",
                "Lu Jiang",
                "Wolfgang Macherey",
                "Jacob Eisenstein."
            ],
            "title": "Advaug: Robust adversarial augmentation for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,",
            "year": 2020
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "CoRR, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Pieter-Tjerk de Boer",
                "Dirk P. Kroese",
                "Shie Mannor",
                "Reuven Y. Rubinstein."
            ],
            "title": "A tutorial on the crossentropy method",
            "venue": "Ann. Oper. Res., 134(1):19\u201367.",
            "year": 2005
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Denis Yarats",
                "Yann N. Dauphin."
            ],
            "title": "Convolutional sequence to sequence learning",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August",
            "year": 2017
        },
        {
            "authors": [
                "Martin Haspelmath."
            ],
            "title": "107",
            "venue": "The European linguistic area: Standard Average European, pages 1492\u20131510. De Gruyter Mouton, Berlin, Boston.",
            "year": 2001
        },
        {
            "authors": [
                "Tian",
                "Lijun Wu",
                "Shuangzhi Wu",
                "Yingce Xia",
                "Dongdong Zhang",
                "Zhirui Zhang",
                "Ming Zhou."
            ],
            "title": "Achieving human parity on automatic chinese to english news translation",
            "venue": "CoRR, abs/1803.05567.",
            "year": 2018
        },
        {
            "authors": [
                "Ryu Iida",
                "Massimo Poesio."
            ],
            "title": "A cross-lingual ILP solution to zero anaphora resolution",
            "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011,",
            "year": 2011
        },
        {
            "authors": [
                "Asanee Kawtrakul",
                "Mukda Suktarachan",
                "Patcharee Varasai",
                "Hutchatai Chanlekha."
            ],
            "title": "A state of the art of thai language resources and thai language behavior analysis and modeling",
            "venue": "The 3rd Workshop on Asian Language Resources and International Stan-",
            "year": 2002
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif M. Mohammad."
            ],
            "title": "Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton."
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Sys-",
            "year": 2012
        },
        {
            "authors": [
                "Sameen Maruf",
                "Gholamreza Haffari."
            ],
            "title": "Document context neural machine translation with memory networks",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20,",
            "year": 2018
        },
        {
            "authors": [
                "Ronan Le Nagard",
                "Philipp Koehn."
            ],
            "title": "Aiding pronoun translation with co-reference resolution",
            "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT@ACL 2010, Uppsala, Sweden, July 15-16, 2010, pages 252\u2013",
            "year": 2010
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Manuel Palomar",
                "Antonio Ferr\u00e1ndez",
                "Lidia Moreno",
                "Patricio Mart\u00ednez-Barco",
                "Jes\u00fas Peral",
                "Maximiliano Saiz-Noeda",
                "Rafael Mu\u00f1oz."
            ],
            "title": "An algorithm for anaphora resolution in spanish texts",
            "venue": "Comput. Linguistics, 27(4):545\u2013567.",
            "year": 2001
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Arum Park",
                "Seunghee Lim",
                "Munpyo Hong."
            ],
            "title": "Zero object resolution in korean",
            "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, PACLIC 29, Shanghai, China, October 30 - November 1, 2015. ACL.",
            "year": 2015
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 186\u2013191. Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Ryohei Sasano",
                "Sadao Kurohashi."
            ],
            "title": "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames",
            "venue": "Fifth International Joint Conference on Natural Language Processing, IJCNLP 2011, Chiang Mai, Thailand,",
            "year": 2011
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "ACL 2016. The Association for Computer Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Sho Shimazu",
                "Sho Takase",
                "Toshiaki Nakazawa",
                "Naoaki Okazaki"
            ],
            "title": "Evaluation dataset for zero",
            "year": 2020
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        },
        {
            "authors": [
                "Wan",
                "Fei Huang"
            ],
            "title": "Bridging the domain",
            "year": 2021
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Xing Wang",
                "Shuming Shi."
            ],
            "title": "One model to learn both: Zero pronoun prediction and translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "Learning to jointly translate and predict dropped pronouns with a shared reconstruction mechanism",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Xiaojun Zhang",
                "Hang Li",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "A novel approach to dropped pronoun translation",
            "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2016
        },
        {
            "authors": [
                "Longyue Wang",
                "Zhaopeng Tu",
                "Xiaojun Zhang",
                "Siyou Liu",
                "Hang Li",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "A novel and robust approach for pro-drop language translation",
            "venue": "Mach. Transl., 31(1-2):65\u201387.",
            "year": 2017
        },
        {
            "authors": [
                "Xiangpeng Wei",
                "Heng Yu",
                "Yue Hu",
                "Rongxiang Weng",
                "Weihua Luo",
                "Rong Jin."
            ],
            "title": "Learning to generalize to more: Continuous semantic augmentation for neural machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Bing Xiang",
                "Xiaoqiang Luo",
                "Bowen Zhou."
            ],
            "title": "Enlisting the ghost: Modeling empty categories for machine translation",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bul-",
            "year": 2013
        },
        {
            "authors": [
                "Mingzhou Xu",
                "Liangyou Li",
                "Derek F. Wong",
                "Qun Liu",
                "Lidia S. Chao."
            ],
            "title": "Document graph for neural machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Zhaopeng Tu."
            ],
            "title": "Guofeng: A benchmark for zero pronoun recovery and translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Kayo Yin",
                "Patrick Fernandes",
                "Danish Pruthi",
                "Aditi Chaudhary",
                "Andr\u00e9 F.T. Martins",
                "Graham Neubig"
            ],
            "title": "Do context-aware translation models pay the right attention",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Ciss\u00e9",
                "Yann N. Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference",
            "year": 2018
        },
        {
            "authors": [
                "Shuai Zhang",
                "Lijie Wang",
                "Ke Sun",
                "Xinyan Xiao"
            ],
            "title": "A practical chinese dependency parser based on a large-scale dataset",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, neural machine translation (NMT) technology has made significant progress in lowering communication barriers between individuals from different language backgrounds. However, NMT systems often struggle when translating sentences from a pro-drop (\u2018pronoun-dropping\u2019) language, such as Chinese, Korean and Japanese (Shimazu et al., 2020), to a non-pro-drop language, such as English, French and German (Haspelmath, 2001)). While the pro-drop phenomenon has been widely studied in the research community (Nagard\n\u2020Equal Contribution. *Corresponding author.\nand Koehn, 2010; Taira et al., 2012; Wang et al., 2016, 2018a; Tan et al., 2019), advanced commercial NMT systems occasionally fail to faithfully recover dropped pronouns in the source language. In some cases, leaving missed pronouns unrecovered could result in severe semantic distortion and alter the intended meaning of the translated text, as demonstrated in Figure 1.\nTo tackle this issue, researchers have proposed two primary strategies: (1) incorporating additional pro-drop resolution systems to provide supplementary syntactic information (Nagard and Koehn, 2010; Taira et al., 2012; Wang et al., 2016). For instance, Xiang et al. (2013) modeled Empty\nCategories within the framework of governmentbinding theory; (2) treating pro-drop resolution as a regularization component of NMT task directly (Wang et al., 2018b; Tan et al., 2019). This approach suggests filling dropped pronouns (Wang et al., 2018a) or predicting pro-drops in the Chinese text encoder component of a seq2seq model (Wang et al., 2019). Despite the studies done on resolving Chinese pro-drop in NMT so far, relevant benchmarks evaluating the effectiveness of pro-drop mitigation are highly limited, and Chinese-English spoken translation datasets with fine-grained annotation are even fewer.\nIn this study, we present PROSE, a PRonoun Omission Solution for Chinese-English spoken language translation. To facilitate research in this area, we introduce a novel dataset for document-level Chinese-English spoken language translation that includes abundant and diverse pro-drop instances with contextual and pro-drop annotations across four spoken language genres (talk, drama, movie, and vlog). The analysis of this dataset reveals that the negative impact of pro-drop on ChineseEnglish spoken language translation. Furthermore, we propose the Mention-Aware Semantic Augmentation approach, which utilizes a mention encoder to capture the semantic embedding of dropped pronouns and employs a mention-side data augmentation technique to generate additional training pairs. Experiment results on four Chinese-English translation corpora demonstrate that our proposed approach significantly increase translation quality and the recover rate of missed pronouns, in comparison with baseline methods on both automatic and human evaluation metrics. Additionally, we conducted ablation studies to provide further insights on the effect of designated losses.\nOur contributions are summarized as follows:\n\u2022 We construct a document-level ChineseEnglish spoken translation dataset that covers multiple spoken genres and provides detailed contextual and pro-drop annotation information.\n\u2022 Our analysis reveals that pro-drop negatively impacts the quality of Chinese-English spoken language translation.\n\u2022 We propose a Mention-Aware Semantic Augmentation approach to increase the recover rate of dropped pronouns when translating and hence enhance overall translation quality."
        },
        {
            "heading": "2 Dataset Creation",
            "text": "To mitigate the scarce of benchmarks evaluating pro-drop in Chinese-English spoken language translation, we collect and construct a new benchmark, PROSE, a high-quality Chinese-English bilingual dataset of four different genres, including Talk, Drama, Movie and Vlog."
        },
        {
            "heading": "2.1 Data Collection and Filtering",
            "text": "The raw data was collected from bilingual subtitles of publicly accessible videos on the internet. We assume that these subtitles reflect authentic daily spoken expressions in Chinese and cover a diverse range of zero anaphora phenomena. Specifically, our filtering process is based on three criteria.\n\u2022 The chosen domain must be spoken, rather than written, such as news articles, to preserve the colloquial features of Chinese;\n\u2022 To ensure high-quality English translations, we only considered source materials in Chinese that have undergone manual translation by professionals, rather than relying on machine translations. For instance, we primarily chose movies from China that have been promoted overseas and short videos with professional fan-made translations on YouTube.;\n\u2022 To enable accurate restoration of missing pronouns, the source material must contain contextual sentences that provide greater context and accuracy to the translations.\nWe end up collecting over 20,000 videos in Chinese and over 2 million lines of English and Chinese subtitle, which can be classified into four distinct spoken genres:\n\u2022 Talk: Subtitles from personal presentations on websites like TED.\n\u2022 Drama: Subtitles from Chinese TV series, such as Nirvana in Fire (\u7405\u740a\u699c).\n\u2022 Movie: Subtitles from Chinese films, such as The Great Cause of the Founding of the People (\u5efa\u56fd\u5927\u4e1a ).\n\u2022 Vlog: Subtitles from short videos filmed by Chinese internet celebrities, such as Huanong Brothers (\u534e\u519c\u5144\u5f1f)."
        },
        {
            "heading": "2.2 Pro-drop Annotation",
            "text": "We employ DDparser (Zhang et al., 2020), a Chinese dependency parsing tool, to detect the omissions of subject or object pronouns in the source language Chinese. Subject pronouns are tagged as SBV (subject-verb) or VV (verb-verb), while object pronouns are tagged as VOB (verb-object), POB (preposition-object), DOB (double object), or DBL (double). Dependencies that do not contain these marks are assumed to be missing either the subject or object pronoun. Although this method of labeling is not perfect, it warrants further study. Below is an example from the subtitles of a short video about cooking.\nChinese: \u56db\u4f2f\u7237\u8fd9\u4e2a\u4ece\u54ea\u513f\u4e0b\u5200\u54e6. English: Uncle, where should I start cutting? [Subject Ellipsis]\nChinese: \u548b\u4e2a\u4e0b\u5200. [Subject Ellipsis] [Object Ellipsis] English: How do I start cutting this?\nChinese: \u90a3\u4e2a\u8089\u7559\u4e0d\u7559\u5728\u4e0a\u9762 \u5417. [Subject Ellipsis] English: Will you leave the meat on the bone?\nAs shown, each data pair consists of Chinese text with its corresponding pronoun type missing, highquality English translations done by human experts, and the surrounding context. We apply the DDparser tool on the training set to annotate whether there is Subject Ellipsis and Object Ellipsis in the Chinese sentences, while the English sentences require no annotation. This is due to that we only collected source materials in Chinese that have undergone manual translation by professionals. The high-quality translations have completed the subject and object in English. For the test set, in addition to calculating the BLEU score with human translation, we also use human evaluation to assess Completeness, Semantic Correctness, and Overall quality (details can be found in Appendix C).\nWe randomly sample 100 samples and manually check the accuracy of the Subject Ellipsis and Object Ellipsis marked by the annotation tool. The experimental results are shown in the table 1."
        },
        {
            "heading": "2.3 Data Statistics",
            "text": "The data statistics for our datasets, which include four genres of spoken Chinese, are presented\nin Table 2. CWMT20181 is the most popular Chinese-English machine translation corpus, containing written language such as news articles, while AIChallenger2 is the largest spoken ChineseEnglish machine translation dataset to the best of our knowledge.\nIn comparison with those two widely used bilingual datasets, our dataset is 1) more representative with a higher pro-drop ratio, 2) more diverse, containing four genres of spoken language, and 3) more informative, with contextual and pro-drop annotation information."
        },
        {
            "heading": "3 Pronoun-Dropping Analysis",
            "text": "To gain more insights into the phenomenon of prodrop in the translation of spoken Chinese into English, we examine the prevalence of pro-drop in spoken Chinese and its impact on the quality of Chinese-English spoken language translation.\nSpoken Chinese Contains More Pro-drop Than Literary Language Formally, pro-drop refers to a reference position that is filled with amorphologically unrealized form, and is one of the most common referential options in many languages such as Chinese (Wang et al., 2018a), Japanese (Taira et al., 2012), Korean (Park et al., 2015), and Thai (Kawtrakul et al., 2002). Previous studies have revealed that spoken Chinese language tends to contain more pro-drops than literary language (Wang et al., 2016, 2017; Xu et al., 2021). However, quantitative studies on pro-drops in different genres of spoken Chinese, remain scarce.\nAs demonstrated in Table 2, both written and spoken languages contain a certain proportion of prodrops, which is consistent with the unique grammatical phenomenon of Chinese. However, written language contains fewer Object Ellipsis than spoken language. For example, in the CWMT2018 dataset, the proportion of Object Ellipsis (i.e., 2.80%) is significantly smaller than that of Subject Ellipsis (i.e., 9.00%). Our four bilingual spoken language corpora are varied, displaying differences in the\n1http://nlp.nju.edu.cn/cwmt-wmt/ 2https://github.com/AIChallenger/AI_\nChallenger_2018\nrates of subject and object pronoun drop, average sentence length, average document length, and so on. For example, the average length of sentences in the three genres of spoken corpora, namely Drama, Movie and Vlog, is much shorter than that of Talk (i.e., individual talks) and AIChallenger. In particular, the Drama, Movie and Vlog corpora in our data set contain a surprising proportion of pro-drops (about 33% to 46%), which is more extensive than the current largest Chinese-English spoken translation corpus AIChallenger.\nPro-drop Harms the Quality of Chinese-English Spoken Language Translation Subjective and objective pronouns are frequently omitted in spoken Chinese, but should be recovered in non-prodrop languages like English. The question arises whether the current NMT system is able to accurately translate spoken Chinese sentences with dropped pronouns into English, a non-pro-drop language, as illustrated in Figure 1.\nFigure 2 shows the distribution of Chinese-to-\nEnglish translation errors in our online simultaneous machine translation system. The primary use case of our translation system is Chinese-toEnglish translation (primarily spoken Chinese) in meetings and conferences. Moreover, we have experienced labeling experts to categorize the bad cases generated by the online system. It can be seen from Figure 2 that the proportion of errors caused by pro-drop is relatively high, constituting more than 11% of all errors. This is one of the major factors contributing to the degradation of the translation quality of our system.\nTo investigate the potential of reinstated pronouns in Chinese spoken sentences to improve the quality of Chinese-English spoken language translation, we conduct experiments using spoken Chinese sentences with omitted pronouns complemented by humans. We first train a Transformerbase (Vaswani et al., 2017; Hassan et al., 2018) model on the CWMT dataset, and then report the BLEU (Papineni et al., 2002) scores with SacreBLEU3 (Post, 2018) on test sets of our four cor-\n34BLEU + case.mixed + lang.LANGUAGE PAIR + num-\npora (i.e., Talk, Drama, Movie and Vlog). Next, the spoken Chinese in test sets that is detected as pro-drop are completed manually, as shown in the content in brackets in Figure 1.\nThe experimental results before and after human completion are shown in Table 3. Although the model achieves a 27.40 BLEU score on the CWMT dataset, its performance on our dataset shows a significant BLEU score decline (from 9.38 to 15.56 across four genres). This indicates a large discrepancy between spoken and written Chinese for neural machine translation systems that rely on data-driven approaches. For convenience, the second column in Table 3 displays the proportion of different datasets with pro-drop. Human completion of dropped pronouns leads to varying performance improvement levels, with the improvement being roughly proportional to the ratio of pro-drops. Interestingly, even on the CWMT dataset, human completion has improved translation quality (i.e., +0.51 BLEU score ), suggesting that pro-drop may also degrade the quality of the Chinese-English translation of that dataset."
        },
        {
            "heading": "4 Methodology",
            "text": ""
        },
        {
            "heading": "4.1 Problem Definition",
            "text": "Given two data spaces, X and Y, encompassing all possible sentences in source (Chinese) and target (English) languages, each sample is a pair of sentences belonging to different languages, i.e., (x,y) \u2208 (X,Y). Here, x = {x1, x2, \u00b7 \u00b7 \u00b7 , x|x|} is the Chinese sentence containing |x| tokens, and y = {y1, y2, \u00b7 \u00b7 \u00b7 , y|y|} is the English sentence with |y| tokens. To identify the mentions (coreferences) of entities (i.e., pronouns) in x, its surrounding context is expressed as c. For example, in the context of c = \u201c\u996d\u5e94\u8be5\u505a\u597d\u4e86 (The meal should be ready)\u201d, the missing object pronoun of \u201c\u5403 (eat)\u201d in the sentence x = \u201c\u8d70\u8d70\u8d70\uff0c\u4e00\u8d77\u53bb \u5403\u5427\u201d can be inferred to be \u201c\u996d (meal)\u201d, thus the translation of the non-pro-drop sentence would be\n\u201cLet\u2019s go out and have a meal together\u201d. The neural machine translation task (Bahdanau\net al., 2015; Gehring et al., 2017; Vaswani et al., 2017) seeks to model the translation probability P (y|x, c; \u0398) using a conditional language model based on Transformer architecture (Vaswani et al., 2017), where \u0398 represents the parameters of the model to be optimized. Formally, the training objective of a given set of observed sentence pairs is\nrefs.1 + smooth.exp + test.SET + tok.intl + version.1.2.15\nto maximize the log-likelihood:\nLnmt(\u0398) = E(x,y)\u223c(X,Y)(logP (y|x, c; \u0398)). (1)"
        },
        {
            "heading": "4.2 Mention-Aware Semantic Augmentation",
            "text": "Motivated by the high prevalence of pro-drop in spoken Chinese and the consequent difficulty in automatically understanding pro-drop source sentences when translated into non-pro-drop English, we present Mention-Aware Semantic Augmentation (illustrated in Figure 3) as a potential solution.\nArchitecture This approach is built on top of Transformer (Vaswani et al., 2017) and consists of three modules: a text encoder Et, a text decoder Dt, and an additional mention encoder Em. The mention encoder Em is a 6-layer transformer encoder which translates the context c to representations Em(c) \u2208 Rk, where k is the embedding dimension. To obtain a real-valued output, a projection matrix A \u2208 Rk\u00d7k is applied to Em(c), resulting in m = Em(c)A. The mention representation m and the text representation r are concatenated together at each time-step and sent to the decoder to calculate the cross attention. It is worth noting that our mention encoder module shares parameters with the text encoder Et. Moreover, it is agnostic to the model architecture and can easily be adapted to other text generation frameworks.\nOverall, our approach leverages 1) the mention encoder to focus on completing the dropped pronouns in the input x from the context c in the case of limited parallel corpus, and 2) representation interpolation in the semantic space of observed samples to expand the training data pairs, thus compensating for the lack of large-scale Chinese-English spoken language translation corpora.\nMention-Aware Contrastive Learning We propose a contrastive objective to learn the semantic embeddings m of mentions in the source sentence x. Specifically, the representations of sentences containing mentions of entities should be \u201ccloser\u201d to m than those without mentions.\nTo this end, we expect the similarity between m and a \u201csimilar\u201d sample m+ to be far greater than that between m and a negative sample m\u2212, i.e., Sim(m,m+) \u226b Sim(m,m\u2212). To obtain m\u2212, we use DDparser (Zhang et al., 2020) to detect all mentioned entities in the context, and then randomly replace them with a special token [MASK]. m+ is sampled by randomly replacing non-entity words. The measure of similarity between two embeddings, denoted as Sim, is calculated using the dot product. This can be interpreted as the angle between the two embeddings in the vector space. Consequently, the mention-aware contrastive objective is formulated as follows:\nLmcl(\u0398) = \u2212E(x,y)\u223c(X,Y)[ (2) log exp(Sim(m,m+))\nexp(Sim(m,m+)) + exp(Sim(m,m\u2212)) ].\nWe introduce a regularization loss to further reduce the disagreements among the mention projection matrix and reduce the redundancy of parameters: Lreg(\u0398) = ||ATA \u2212 I||2, where I is the identity matrix.\nMention-Side Mixup Interpolation Drawing inspiration from Mixup approaches (Zhang et al., 2018; Wang et al., 2021; Wei et al., 2022), we propose to sample data points from the adjacency mention semantic region to augment the current training instance. Given pairs of samples (x1, y1) and (x2, y2), Mixup chooses a random mixing proportion \u03bb from a Beta distribution \u03b2(\u03b1, \u03b1) controlled by the hyper-parameter \u03b1, and creates an artificial training example (\u03bbx1+(1\u2212\u03bb)x2, \u03bby1+(1\u2212\u03bb)y2) to train the network by minimizing the loss on mixed-up data points:\nLmix(\u0398) = Ex1,y1\u223cpDEx2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1,\u03b1)[ (3) \u2113(\u03bbx1 + (1\u2212 \u03bb)x2, \u03bby1 + (1\u2212 \u03bb)y2)],\nwhere \u2113 is the cross entropy loss (de Boer et al., 2005). According to Appendix A, we can simplify Equation 3 as follows:\nLmix(\u0398) \u21d2 Ex1,y2\u223cpDEx2\u223cpDE\u03bb\u223c\u03b2(\u03b1+1,\u03b1) (4) \u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y1),\nwhich enables us to avoid the requirement for label blending when combining labels y1 and y2, with \u03bb drawn from \u03b2(\u03b1 + 1, \u03b1). This is beneficial in scenarios where y2 is a discrete sequence. Accordingly, our mention-side mixup loss minimizes the interpolations loss from a vicinity distribution (Chapelle et al., 2000) defined in the representation space:\nLmmi(\u0398) = E(xi,yi)\u223c(X,Y)E\u03bb\u223c\u03b2(\u03b1+1,\u03b1) (5) (logP (yi|xi, \u03bbmi + (1\u2212 \u03bb)m+i ); \u0398)).\nIn other words, we can utilize the presence or absence of pronoun context (i.e, m and m+) to argument the training samples for enhancing the robustness towards pronouns."
        },
        {
            "heading": "4.3 Training and Inference",
            "text": "Finally, we optimize the sum of the above losses:\nLfinal(\u0398) =Lnmt(\u0398) + Lmcl(\u0398) (6)\n+Lreg(\u0398) + Lmmi(\u0398).\nDuring inference, beam search decoding is performed."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Baselines Comparisons",
            "text": "We compare our method with several state-ofthe-art machine translation methods, including pro-drop machine translation methods (RecNMT (Wang et al., 2018a) and pro-dropP&T (Wang et al., 2019)), document-level machine translation methods (HanNMT (Miculicich et al., 2018)), and data-augmentation machine translation methods (AdvAug (Cheng et al., 2020) and CsaNMT (Wei et al., 2022)). We pre-train the NMT model using the AIChallenger dataset, achieving 27.97 BLEU\npoints on the test set. Afterward, we optimize the parameters on our specific spoken Chinese corpus, which is relatively small in size. The implementation details are shown in Appendix B.\nFor analysis, we also show the performance of NMT models trained on different corpora, including: (1) Base: Training the NMT model solely on a small Chinese-English spoken language translation corpus. (2) Fine-tuning: Training the NMT model on the AIChallenger dataset and then fine-tuning the model on Chinese-English spoken corpora."
        },
        {
            "heading": "5.2 Automatic Evaluation",
            "text": "For automatic translation evaluation, we report the classical BLEU (Papineni et al., 2002) scores with SacreBLEU. The automatic evaluation results on our four-genre Chinese-English spoken translation dataset are presented in Table 4.\nOur experiment results show that Fine-tuning method outperforms the Base method by 4.76 BLEU points, indicating that the amount of data remains the bottleneck of translation performance on the task of spoken language translation with a limited corpus. Furthermore, the document-level machine translation method (HanNMT) is significantly better than single-text-inputbased methods (RecNMT and pro-dropP&T) and data-augmentation-based methods (AdvAug and CsaNMT), indicating that context information is useful for pro-drop translation. Interestingly, the data-augmentation-based NMT methods (AdvAug and CsaNMT) also have an approximate BLEU gain of 1.34 to 2.00, demonstrating that the sampling method in the semantic space to expand the training dataset can well enhance the generalization of pro-drop spoken language translation. In any case, our method greatly outperforms these baseline methods, demonstrating the effectiveness of our proposed approach for pro-drop translation."
        },
        {
            "heading": "5.3 Human Evaluation",
            "text": "We also conduct a human evaluation focusing on three metrics: pronoun recovery (determining whether the translated sentence is complete or contains missing mentions), semantic correctness (determining whether the translated sentence is semantically consistent with the source text sentence) and overall quality.\nWe sample 200 instances from four corpora, and hired two workers to rate the translation results of pro-dropP&T, HanNMT, CsaNMT and our model based on the above three aspects. We used Best-Worst Scaling, which has been shown to produce more reliable results than ranking scales (Kiritchenko and Mohammad, 2017). Specifically, each score is computed as the percentage of times it was selected as best minus the percentage of times it was selected as worst, and ranges from -1 (unanimously worst) to +1 (unanimously best). The order in which the translated texts were presented to the\njudges was random. The details of the questions can be found in Appendix C.\nTable 5 indicates that HanNMT, a strong document-level machine translation method, performs better than CsaNMT and RecNMT in recovering missing pronouns, possibly due to its use of rich source-side context. Interestingly, CsaNMT, which utilizes data augmentation, exhibits superior semantic correctness and overall quality. Nonetheless, our method outperforms all baselines in terms of pronoun recovery and overall quality, indicating that the performance improvement is attributed to pro-drop resolution. More examples of generated translations of our model against comparison systems are presented in Appendix D."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "We conduct various ablation studies on our dataset as shown in Table 6, which assess the contribution of different losses. The SacreBLEU scores are reported on test sets.\nThe experiment results show that the removal of Mention-Side Mixup Interpolation results in a 0.72 BLEU point drop, indicating that the data augmentation method based on mentions can increase the generalization of pro-drop translation. Moreover, we find that all our losses, especially Lmcl, are beneficial for improving the translation quality. This implies that our mention-aware contrastive learning is capable of capturing the lost pronoun information and thus improving overall performance of NMT.\nIt is worth noting that the third row in Table 6 is a strong document-level machine translation baseline, indicating that the improvement of our model mainly comes from the mention-aware loss rather than the wide contexts in the source side."
        },
        {
            "heading": "6 Related Work",
            "text": "Pro-drop in Machine Translation Research on pro-drop machine translation mainly falls into two categories: (1) methods using extra pro-drop resolution systems and (2) joint pro-drop resolution and translation training methods. The former relies\non some syntax tools to provide extra information for the MT system (Nagard and Koehn, 2010; Taira et al., 2012; Wang et al., 2016), such as modeling empty categories (Xiang et al., 2013). However, directly using the results of external pro-drop resolution systems for the translation task shows limited improvements (Taira et al., 2012), since such external systems are trained on small-scale data that is non-homologous to MT. To bridge the gap between the two tasks, some later studies (Wang et al., 2018b; Tan et al., 2019; Xu et al., 2022) directly integrated the pro-drop resolution task into the machine translation task, such as reconstructing the missing pronouns (Wang et al., 2018a) in the encoder or predicting the pro-drop (Wang et al., 2019).\nUnlike previous methods, our method recovers the missing pro-drop information from the context and uses data augmentation in the semantic space to increase the training data. To the best of our knowledge, we are the first to construct a documentlevel Chinese-English spoken translation dataset covering multiple spoken genres.\nDocument-Level Machine Translation Recent works on customized model architectures have focused on improving context representations for document-level translation models, such as contextaware encoders (Voita et al., 2019a), context-aware decoders (Voita et al., 2019b), hierarchical history representations (Miculicich et al., 2018), and memory networks (Maruf and Haffari, 2018). However, Yin et al. 2021 points out that simply feeding contextual text may not be able to accurately disambiguate pronouns and polysemous words that require contexts for resolution. Thus, we employ contrastive learning to enforce the model to incorporate the mention-information about the dropped pronouns.\nData Augmentation in Machine Translation Our approach is also related to Vicinal Risk Minimization (Chapelle et al., 2000), which formalizes data augmentation as extracting additional pseudo samples from the vicinal distribution of observed\ninstances (Krizhevsky et al., 2012; Zhang et al., 2018; Wang et al., 2021). In machine translation, this vicinity is often defined through adversarial augmentation with manifold neighborhoods (Cheng et al., 2020; Wei et al., 2022). Our approach is similar in that it involves an adjacency mention semantic region as the vicinity manifold for each training instance."
        },
        {
            "heading": "7 Conclusion",
            "text": "This study provides valuable insights into the phenomenon of pro-drop in Chinese and its impact on Chinese-English spoken language translation. Furthermore, we introduced a new dataset that improves upon existing corpora in terms of representativeness, diversity, and informational value. Lastly, our proposed approach, Mention-Aware Semantic Augmentation, demonstrates superior performance over existing methods in addressing the challenges posed by pro-drops.\nOur study underscores the critical importance of taking into account pro-drops in NMT systems, and offers valuable benchmarks and insights to guide future advancements in this field.\nLimitations\nOur method has shown effectiveness in improving translation quality in pro-drop machine translation task from pro-drop languages such as Chinese to a non-pro-drop target language, English in this case. However, due to the limited availability of data resources, the translation performance from other pro-drop languages such as Japanese (Sasano and Kurohashi, 2011), Thai (Kawtrakul et al., 2002), Korean (Park et al., 2015), Italian (Iida and Poesio, 2011), Spanish (Palomar et al., 2001), etc. to non-pro-drop languages remains to be evaluated. Furthermore, our method may not be able to match the performance of large language models such as PaLM (Chowdhery et al., 2022), ChatGPT4 and GPT45 , which are trained with massive machine translation corpora and other language resources."
        },
        {
            "heading": "Acknowledgement",
            "text": "We would like to express our deepest appreciation to Suqing Yan, Weixuan Wang, and Xupeng Meng from Huawei for their invaluable assistance and discussions during the implementation process. Their\n4https://openai.com/blog/chatgpt/ 5https://openai.com/gpt-4\ninsights and expertise have been instrumental in the realization of our ideas. We are also immensely grateful to the anonymous reviewers for their constructive feedback and comments. Their perspectives have greatly enhanced the quality of our work. Lastly, we extend our gratitude to all those who have indirectly contributed to this project. Your support has not gone unnoticed and is much appreciated."
        },
        {
            "heading": "A Proof of Equation 4",
            "text": "L(\u0398) = Ex1,y1\u223cpDEx2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1,\u03b1)[\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, \u03bby1 + (1\u2212 \u03bb)y2)] (7)\n=Ex1,y1\u223cpDEx2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1,\u03b1)[ \u03bb\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y1) + (1\u2212 \u03bb)\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y2)] (8)\n=Ex1,y1\u223cpDEx2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1,\u03b1) Ez\u223cBer(\u03bb)[z\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y1) + (1\u2212 z)\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y2)] (9)\n=Ex1,y1\u223cpDEx2,y2\u223cpDEz\u223cBer(0.5) E\u03bb\u223c\u03b2(\u03b1+z,\u03b1+1\u2212z)[z\u2113(\u03bbx1+ (1\u2212 \u03bb)x2, y1) + (1\u2212 z)\u2113(\u03bbx1+ (1\u2212 \u03bb)x2, y2)] (10) =0.5 \u2217 Ex1,y1\u223cpDEx2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1+1,\u03b1) [\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y1)] + 0.5 \u2217 Ex1,y1\u223cpD Ex2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1,\u03b1+1)[\u2113(\u03bbx1+ (1\u2212 \u03bb)x2, y2)] (11) =0.5 \u2217 Ex1,y1\u223cpDEx2,y2\u223cpDE\u03bb\u223c\u03b2(\u03b1+1,\u03b1) [\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y1)] + 0.5 \u2217 Ex2,y2\u223cpD Ex1,y1\u223cpDE(1\u2212\u03bb)\u223c\u03b2(\u03b1,\u03b1+1)[\u2113((1\u2212 \u03bb)x2 + \u03bbx1, y1)] (12)\n\u21d2Ex1,y2\u223cpDEx2\u223cpDE\u03bb\u223c\u03b2(\u03b1+1,\u03b1)\u2113(\u03bbx1 + (1\u2212 \u03bb)x2, y1) (13)\n\u2022 Eq (8): Linearity of the loss: \u2113(x, py1 + (1\u2212 p)y2) = p\u2113(x, y1) + (1 \u2212 p)\u2113(x, y2), where the loss is the cross entropy loss.\n\u2022 Eq (9): Expectation of a Bernoulli(\u03bb).\n\u2022 Eq (10): The Beta distribution is conjugate prior for the Bernoulli.\n\u2022 Eq (11): Expectation of a Bernoulli(0.5).\n\u2022 Eq (12): Symmetry of the Beta distribution in the sense that \u03bb \u223c (a, b) implies 1 \u2212 \u03bb \u223c (b, a).\n\u2022 Eq (13): Changing variable names in the expectation.\nB Implementation Details\nWe implement our method on top of the Transformer-base (Vaswani et al., 2017) implemented in Fairseq (Ott et al., 2019). For this, the\ndimension k was set to 512, the number of attention heads to 8, the mention encoder Em, and the text encoder Et and text decoder Dt to 6 layers, and the maximum sequence length to 200. The beam size of the beam search was 5. Other hyper-parameters included a dropout rate of 0.1, Adam with a learning rate of 1e-5, \u03b21 = 0.9, and \u03b22 = 0.999. To address the out-of-vocabulary problem, we apply byte-pair-encoding (BPE) vocabulary (Sennrich et al., 2016) with 40k merge operations and set \u03b1 in \u03b2(\u03b1+ 1, \u03b1) to 0.1. We implemented our model using PyTorch and used 8 Tesla V100 graphic cards for training."
        },
        {
            "heading": "C Human Evaluation Questions",
            "text": "\u2022 Completeness: Does the translated sentence demonstrate syntactic completeness?\n\u2022 Semantic Correctness: Is the translated sentence semantically correct?\n\u2022 Overall: What is the overall quality of the translation?"
        },
        {
            "heading": "D Examples of Generated Translations",
            "text": "Examples of generated translations of our model and comparison systems are show in Table 7, Table 8, Table 9, and Table 10."
        }
    ],
    "title": "PROSE: A Pronoun Omission Solution for Chinese-English Spoken Language Translation",
    "year": 2023
}