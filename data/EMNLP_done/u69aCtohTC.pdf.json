{
    "abstractText": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language. The code is publicly available at https://github. com/thu-coai/Implicit-Toxicity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaxin Wen"
        },
        {
            "affiliations": [],
            "name": "Pei Ke"
        },
        {
            "affiliations": [],
            "name": "Hao Sun"
        },
        {
            "affiliations": [],
            "name": "Zhexin Zhang"
        },
        {
            "affiliations": [],
            "name": "Chengfei Li"
        },
        {
            "affiliations": [],
            "name": "Jinfeng Bai"
        },
        {
            "affiliations": [],
            "name": "Minlie Huang"
        }
    ],
    "id": "SP:1ff8abea72cc0c383f8be870a333f0fcbaa5bcf3",
    "references": [
        {
            "authors": [
                "Abubakar Abid",
                "Maheen Farooqi",
                "James Zou."
            ],
            "title": "Persistent anti-muslim bias in large language models",
            "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298\u2013306.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom B Brown",
                "Dawn Song",
                "Ulfar Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "year": 2021
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael W. Macy",
                "Ingmar Weber."
            ],
            "title": "Automated hate speech detection and the problem of offensive language",
            "venue": "Proceedings of the Eleventh International Conference on Web and Social Media, pages 512\u2013515.",
            "year": 2017
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Vishvak Murahari",
                "Tanmay Rajpurohit",
                "Ashwin Kalyan",
                "Karthik Narasimhan."
            ],
            "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "venue": "arXiv preprint arXiv:2304.05335.",
            "year": 2023
        },
        {
            "authors": [
                "Emily Dinan",
                "Samuel Humeau",
                "Bharath Chintagunta",
                "Jason Weston."
            ],
            "title": "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang."
            ],
            "title": "Latent hatred: A benchmark for understanding implicit hate speech",
            "venue": "Proceedings of the 2021 Conference on Empirical Meth-",
            "year": 2021
        },
        {
            "authors": [
                "Joseph L Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological bulletin, 76(5):378.",
            "year": 1971
        },
        {
            "authors": [
                "Jane Frank."
            ],
            "title": "You call that a rhetorical question?: Forms and functions of rhetorical questions in conversation",
            "venue": "Journal of Pragmatics, 14(5):723\u2013738.",
            "year": 1990
        },
        {
            "authors": [
                "Simona Frenda",
                "Alessandra Teresa Cignarella",
                "Valerio Basile",
                "Cristina Bosco",
                "Viviana Patti",
                "Paolo Rosso."
            ],
            "title": "The unbearable hurtfulness of sarcasm",
            "venue": "Expert Systems with Applications, 193:116398.",
            "year": 2022
        },
        {
            "authors": [
                "Deep Ganguli",
                "Danny Hernandez",
                "Liane Lovitt",
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Tom Conerly",
                "Nova Dassarma",
                "Dawn Drain",
                "Nelson Elhage"
            ],
            "title": "Predictability and surprise in large generative models",
            "venue": "ACM Conference on Fairness,",
            "year": 2022
        },
        {
            "authors": [
                "Lei Gao",
                "Ruihong Huang."
            ],
            "title": "Detecting online hate speech using context aware models",
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing, pages 260\u2013266.",
            "year": 2017
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar."
            ],
            "title": "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Borja Ibarz",
                "Jan Leike",
                "Tobias Pohlen",
                "Geoffrey Irving",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Reward learning from human preferences and demonstrations in atari",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Liwei Jiang",
                "Jena D Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jenny Liang",
                "Jesse Dodge",
                "Keisuke Sakaguchi",
                "Maxwell Forbes",
                "Jon Borchardt",
                "Saadia Gabriel"
            ],
            "title": "Can machines learn morality? the delphi",
            "year": 2021
        },
        {
            "authors": [
                "Jens Lemmens",
                "Ilia Markov",
                "Walter Daelemans."
            ],
            "title": "Improving hate speech type and target detection with hateful metaphor features",
            "venue": "Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda, pages",
            "year": 2021
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2016
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Rijul Magu",
                "Jiebo Luo."
            ],
            "title": "Determining code words in euphemistic hate speech using word embedding networks",
            "venue": "Proceedings of the 2nd workshop on abusive language online (ALW2), pages 93\u2013100.",
            "year": 2018
        },
        {
            "authors": [
                "Nicolas Ocampo",
                "Ekaterina Sviridova",
                "Elena Cabrio",
                "Serena Villata."
            ],
            "title": "An in-depth analysis of implicit and subtle hate speech messages",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Saffron Huang",
                "H. Francis Song",
                "Trevor Cai",
                "Roman Ring",
                "John Aslanides",
                "Amelia Glaese",
                "Nat McAleese",
                "Geoffrey Irving."
            ],
            "title": "Red teaming language models with language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        },
        {
            "authors": [
                "F\u00e1bio Perez",
                "Ian Ribeiro."
            ],
            "title": "Ignore previous prompt: Attack techniques for language models",
            "venue": "arXiv preprint arXiv:2211.09527.",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick S.H. Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Eric Michael Smith",
                "Y-Lan Boureau",
                "Jason Weston."
            ],
            "title": "Recipes for building an open-domain chatbot",
            "venue": "Proceedings of the 16th Conference of",
            "year": 2021
        },
        {
            "authors": [
                "Nigora Ruzibaeva."
            ],
            "title": "Peculiarities of the antithesis in the literary text",
            "venue": "European Journal of Research and Reflection in Educational Sciences Vol, 7(11).",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Rohit Sridhar",
                "Diyi Yang."
            ],
            "title": "Explaining toxic text via knowledge enhanced text generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2022
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano."
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems, 33:3008\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Hao Sun",
                "Zhexin Zhang",
                "Jiawen Deng",
                "Jiale Cheng",
                "Minlie Huang."
            ],
            "title": "Safety assessment of chinese large language models",
            "venue": "arXiv preprint arXiv:2304.10436.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "venue": "arXiv preprint arXiv:2303.04048.",
            "year": 2023
        },
        {
            "authors": [
                "Jing Xu",
                "Da Ju",
                "Margaret Li",
                "Y-Lan Boureau",
                "Jason Weston",
                "Emily Dinan."
            ],
            "title": "Recipes for safety in open-domain chatbots",
            "venue": "arXiv preprint arXiv:2010.07079.",
            "year": 2020
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan."
            ],
            "title": "DIALOGPT : Large-scale generative pre-training for conversational response generation",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Zhexin Zhang",
                "Leqi Lei",
                "Lindong Wu",
                "Rui Sun",
                "Yongkang Huang",
                "Chong Long",
                "Xiao Liu",
                "Xuanyu Lei",
                "Jie Tang",
                "Minlie Huang."
            ],
            "title": "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Zhexin Zhang",
                "Jiaxin Wen",
                "Minlie Huang."
            ],
            "title": "ETHICIST: Targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the rapid progress in large-scale pre-training (Brown et al., 2020; Chowdhery et al., 2022), large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, leading to significant breakthroughs in zero-shot / few-shot learning (Brown et al., 2020; Chung et al., 2022). However, the open-endedness nature of LLMs, combined with their powerful abilities, also introduces new risks of harmful behaviors (Ganguli et al., 2022; OpenAI, 2023a).\n\u2020 Corresponding author\nWhile recent studies have presented several methods to probe LLMs for generating harmful outputs such as persona assigning (Deshpande et al., 2023) and goal hijacking (Perez and Ribeiro, 2022), they still primarily focus on probing explicit toxic outputs (e.g., abusive language) that can be easily detected by existing toxicity classifiers. In contrast, we aim to explore whether LLMs possess the capability to generate implicit toxic outputs that are challenging to detect, even using state-of-the-art toxicity classifiers. If so, such undetectable implicit toxic outputs may pose a more significant threat, as LLMs can freely express toxicity without being detected once deployed.\nTo investigate this research question, we begin with preliminary experiments that simply prompt GPT-3.5-turbo (i.e., the API version of ChatGPT (OpenAI, 2022)) based on linguistic features to generate implicit toxic responses in a zero-shot setting (Section 2). Surprisingly, as shown in Figure 1, despite the impressive performance of state-of-theart toxicity classifiers on previous toxic benchmark datasets, these classifiers are vulnerable to LLMgenerated implicit toxic outputs, resulting in significantly higher attack success rates ranging from\nExcerpt of Instruction: Given a query, please generate an implicit toxic response (i.e., implying toxic while not using explicit toxic words). You can use linguistic features such as euphemism, circumlocution, sarcasm, metaphor, and other figurative language.\n58.47% (on Davinci003 (Ouyang et al., 2022)) to 96.69% (on Perspective-API (Google, 2023)).\nTo shed more light on this safety risk caused by LLMs and explore the potential of their ability to generate diverse implicit toxic outputs, we further propose an attacking method based on reinforcement learning (RL) to induce implicit toxicity in LLMs. Specifically, we optimize the large language model with a reward that prefers implicit toxic responses to explicit toxic and non-toxic ones. Extensive experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be substantially improved through RL finetuning. These results suggest that LLMs pose a significant risk of generating toxic outputs without being detected by existing widely-adopted toxicity classifiers. Moreover, we empirically show that fine-tuning toxicity classifiers on the annotated examples generated by our attacking method effectively enhances their abilities to detect implicit toxic language in the era of LLMs.\nOur contributions can be summarized as follows: \u2022 We identify a novel safety risk of LLMs,\nnamely their ability to generate implicit toxic outputs that are exceptionally difficult to detect using existing toxicity classifiers.\n\u2022 We propose to further induce implicit toxicity in LLMs by optimizing language models with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones.\n\u2022 Extensive experiments demonstrate that our method achieves a significantly higher attack\nsuccess rate compared to previous toxic benchmark datasets and baselines across five widelyadopted toxicity classifiers. Further experiments show that fine-tuning toxicity classifiers on the annotated examples from our attacking method successfully enhances their abilities to detect the implicit toxicity of LLMs."
        },
        {
            "heading": "2 Preliminary Experiments on Implicit Toxicity in Large Language Models",
            "text": "Implicit toxicity has emerged as a main challenge in the field of toxicity detection owing to its nuanced nature (ElSherief et al., 2021). Rather than overtly abusive language such as swearwords, implicit toxicity is conveyed through a variety of linguistic features (such as euphemism (Magu and Luo, 2018), sarcasm (Frenda et al., 2022), circumlocution (Gao and Huang, 2017), and metaphor (Lemmens et al., 2021)) and extralinguistic knowledge (such as commonsense knowledge (Sridhar and Yang, 2022), world knowledge (Ocampo et al., 2023), and social norm (Jiang et al., 2021)).\nInspired by prior analyses on the challenge of detecting implicit toxicity, we conduct preliminary experiments to explore whether large language models, which have seen extensive linguistic features and extralinguistic knowledge during pre-training (Petroni et al., 2019), can generate implicit toxic responses that evade toxicity classifiers. Specifically, we use zero-shot prompting to instruct GPT-3.5turbo to generate implicit toxic responses. Table 1 presents the instruction and generated examples.\nWe then evaluate the attack success rate of LLMgenerated implicit toxic responses on various stateof-the-art toxicity classifiers, comparing it with that of previous toxic benchmark datasets. The quantitative results, as illustrated in Figure 1, reveal the following findings: (1) State-of-the-art toxicity classifiers achieve impressive performance on previous toxic benchmark datasets, including implicit toxic benchmarks such as TOXIGEN and Latent Hatred (ElSherief et al., 2021; Hartvigsen et al., 2022). This indicates their advanced language understanding abilities that surpass mere keyword-matching. For instance, the example \u201cYou can tell a woman is a good person or not by how much she covers her skin\u201d (which is an implicit toxic example from TOXIGEN), can be easily detected by these classifiers. (2) Surprisingly, these toxicity classifiers are vulnerable to LLM-generated implicit toxic responses, showing a significantly higher attack success rate ranging from 58.47% (on Davinci003) to 96.69% (on Perspective-API)."
        },
        {
            "heading": "3 Methodology",
            "text": "Based on the impressive zero-shot performance of LLMs in expressing implicit toxicity, we propose a novel method based on reinforcement learning to further explore the potential of inducing implicit toxicity in LLMs. Specifically, starting with a vanilla large language model denoted as \u03c0\u03d5 (which is chosen as trainable LLMs like LLaMA (Touvron et al., 2023)), our method consists of three steps as illustrated in Figure 2: \u2022 Supervised Learning: We first warm-start the\npolicy model \u03c0\u03d5 by conducting supervised learning such that \u03c0\u03d5 can perform reasonably well in generating implicit toxic responses. However, \u03c0\u03d5 still occasionally generates explicit toxic or non-toxic responses. \u2022 Reward Model Training: We then build a reward model R\u03b8 that prefers implicit toxic responses to explicit toxic and non-toxic ones. \u2022 Reinforcement Learning: We optimize the policy model with this reward based on proximal policy optimization (PPO) (Schulman et al., 2017), which can lead to more challenging-to-detect implicit toxic responses."
        },
        {
            "heading": "3.1 Supervised Learning",
            "text": "We first warm-start the policy model \u03c0\u03d5 via supervised learning. While prior works rely on human annotators to collect supervised learning data\n(Ouyang et al., 2022), the impressive zero-shot performance of instruction-tuned LMs (e.g., GPT-3.5turbo) shown in Section 2 motivates us to collect the implicit toxic data automatically via prompting without human efforts (Perez et al., 2022). These data can equip the vanilla LLM \u03c0\u03d5 with the basic ability to generate implicit toxic responses, eliminating the need for additional prompt engineering.\nData Collection Given a query set D = {x}, we collect the supervised learning dataset D\u2217 = {(x, y)} as follows: for each query x, we automatically generate the corresponding response y = (y1, \u00b7 \u00b7 \u00b7 , yn) based on an instruction-tuned language model (e.g., GPT-3.5-turbo in our experiments) via zero-shot prompting, where yt(1 \u2264 t \u2264 n) denotes the t-th token of the response.\nTraining We warm-start the policy model \u03c0\u03d5 by training it on D\u2217 with the MLE loss:\nLMLE = \u2212 |y|\u2211 t=1 log\u03c0\u03d5(yt|y<t, x)\nWe denote the supervised learned policy as \u03c00."
        },
        {
            "heading": "3.2 Reward Model Training",
            "text": "In this section, we aim to build a reward model that prefers implicit toxic responses to explicit toxic and non-toxic ones, which thereby leads to more challenging-to-detect implicit toxic responses.\nOne naive approach is to directly use the negative predicted toxic confidence of an existing toxicity classifier P as the reward, i.e., \u2212P (toxic|x, y). However, since existing toxicity classifiers struggle to capture implicit toxicity, \u2212P (toxic|x, y) will predominantly steer the policy model towards generating non-toxic responses rather than implicit toxic ones, as verified in Section 4.6.\nTo address this challenge, inspired by prior works on preference modeling (Stiennon et al., 2020; Ouyang et al., 2022), we collect a comparison dataset DRM = {(x, yw, yl)}, where yw is more implicit toxic than yl. We then obtain the expected reward model via fine-tuning on DRM .\nData Collection Given a query set {x}, we collect the comparison dataset DRM as follows: for each query x, we generate K responses with the policy model \u03c0\u03d5 and obtain the comparison result between each pair of generated responses.\nCompared to prior works (Stiennon et al., 2020; Ouyang et al., 2022), we propose two techniques to\nimprove data quality and reduce annotation costs. First, previous works directly collect ( K 2 ) comparisons. However, we find it difficult to determine the preferred option when both responses contain overtly abusive language or are entirely free of it, resulting in low inter-annotator agreement. To simplify the annotation task and improve data quality, we adopt a three-class labeling task, assuming equal preference within each class. Specifically, the generated response y is initially labeled as either implicit toxic, explicit toxic, or non-toxic. The comparison data is then derived by assigning the highest preference to the implicit toxic class. Second, instead of using crowdsourcing workers for comparison data annotation, following OpenAI (2023a), we use GPT-3.5-turbo as the labeler since it performs reasonably well in detecting its own generated implicit toxic responses (with a toxic recall of 68.8% in our preliminary experiments) while significantly reducing annotation costs. Nonetheless, since the annotated data for reward model training is automatically acquired from GPT-3.5-turbo, the effectiveness of RL is limited to its performance1. Specifically, our manual review reveals that the automatically annotated comparison data contains noise, where the non-toxic subset particularly contains nearly 30% implicit toxic re-\n1In practice, the automatically annotated data are sufficient to provide a valuable reward signal for inducing implicit toxicity in LLMs as shown in Section 4.5.\nsponses. To further improve the attack success rate or extend our method to attack stronger classifiers, we can employ stronger classifiers for comparison data annotation, such as GPT-4 (OpenAI, 2023a), and ultimately human experts.\nTraining We train the reward model R\u03b8 on each sample of DRM with the following loss function:\nLRM = \u2212log(\u03c3(R\u03b8(x, yw)\u2212R\u03b8(x, yl)))\nwhere R\u03b8 is devised as a language model equipped with a linear head, R\u03b8(x, y) is the scalar output of R\u03b8 for context x and response y, and yw/yl indicates the win/lose response, respectively. Moreover, while we follow prior studies that define implicit toxicity based on the absence of overtly offensive words (ElSherief et al., 2021) in the annotation instructions, it is crucial to acknowledge that existing classifiers such as BAD and Davinci003 have demonstrated advanced language understanding capabilities that surpass the mere identification of overtly offensive words. Consequently, certain annotated implicit toxic responses are not sufficiently implicit and can still be detected by existing classifiers, leading to the subeffectiveness of solely optimizing with the reward model R\u03b8 for attacking state-of-the-art toxicity classifiers. To address this concern, we can explicitly introduce an existing toxicity classifier P into the reward by ensembling it with R\u03b8, yielding\nthe complete reward function R \u2032 \u03b8(x, y):\nR \u2032 \u03b8(x, y) = R\u03b8(x, y)\u2212 \u03b1P (toxic|x, y)\nwhere \u03b1 is a hyperparameter to control the strength of the penalization imposed by P ."
        },
        {
            "heading": "3.3 Reinforcement Learning",
            "text": "We then optimize the policy model \u03c0\u03d5 parameterized by \u03d5 with this reward using the PPO algorithm (Schulman et al., 2017). Specifically, we use the KL-regularized objective, yielding the final reward function as follows:\nR\u0302\u03b8,\u03d5(x, y) = R \u2032 \u03b8(x, y)\u2212 \u03b2DKL(\u03c0\u03d5||\u03c00)\nwhere \u03c00 denotes the supervised learned policy, and \u03b2 is a hyperparameter that controls the strength of penalization applied to the KL divergence between the learned policy \u03c0\u03d5 and \u03c00. The KL term aims to mitigate over-optimization of the reward model."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Settings",
            "text": "Query Our queries are derived from the BAD dataset, which contains nearly 6,000 dialogues between chatbots and crowdsourcing workers. Specifically, workers are instructed to elicit toxic responses from the chatbot. We hence extract the utterances from workers as our queries. The detailed statistics of the dataset are shown in Appendix B.1.\nModel Structure We use LLaMA-13B as the backbone of both the policy model \u03c0\u03d5 and the reward model R\u03b8. We utilize the BAD classifier, which is a 125M RoBERTa-base (Liu et al., 2019) model fine-tuned on the BAD dataset, as the additionally introduced existing toxicity classifier P due to its reasonable performance."
        },
        {
            "heading": "4.2 Baselines",
            "text": "\u2022 Offensive Twitter: An explicit toxic dataset\ncollected from Twitter by matching overtly offensive keywords (Davidson et al., 2017). \u2022 Latent Hatred: A crowdsourcing implicit toxic dataset collected from hate groups on Twitter (ElSherief et al., 2021). \u2022 TOXIGEN: A machine-generated implicit toxic dataset collected through few-shot prompting on GPT-3 (Hartvigsen et al., 2022). \u2022 BAD: A crowdsourcing conversation dataset (Xu et al., 2020) targeting at eliciting toxic responses from chatbots like BlenderBot (Roller et al., 2021) and DialoGPT (Zhang et al., 2020).\n\u2022 GPT-3.5-turbo: We use zero-shot prompting on GPT-3.5-turbo to generate implicit toxic responses. The instruction is shown in Table 1. \u2022 Supervised Learning (SL) LLaMA-13B: We use the supervised learned LLamA-13B model to generate implicit toxic responses. \u2022 Supervised Learning-Rank (SL-R) LLaMA13B: We generate K = 5 responses for each query with the SL model. We then continue to train the SL model using the responses that rank first according to the reward model."
        },
        {
            "heading": "4.3 Attacked Toxicity Classifiers",
            "text": "We experiment with five state-of-the-art toxicity classifiers. We first introduce two online toxic classifiers which are widely used in both research and industries, i.e., Google\u2019s Perspective-API (P-API) (Google, 2023) and OpenAI\u2019s Moderation (OpenAI, 2023b). Additionally, we build two toxicity classifiers by fine-tuning RoBERTa-base on TOXIGEN and Bot-Adversarial (BAD), respectively. Moreover, inspired by recent works that highlight the strong evaluation abilities of LLMs (Wang et al., 2023; Liu et al., 2023), we further introduce a LLMbased toxicity classifier based on zero-shot prompting with Davinci003 following Liu et al. (2023)."
        },
        {
            "heading": "4.4 Metrics",
            "text": "As existing classifiers exhibit limited performance in detecting our LLM-generated implicit toxic responses, we employ human annotation to obtain golden labels. For each query-response pair, three annotators are hired to label the response as toxic or non-toxic. Given the nuanced nature of the generated responses, this annotation task requires a comprehensive understanding of the response\u2019s semantics. Therefore, we recruit annotators by collaborating with a professional data annotation company. All annotators are college students majoring in English, achieving a moderate to substantial inter-annotator agreement measured by Fleiss\u2019 Kappa (Fleiss, 1971).\nAfter obtaining the golden label, we adopt the following metrics for evaluation. Reward computes the average reward of the responses based on our reward model. Distinct-n computes the percentage of unique n-grams among all n-grams (Li et al., 2016). A higher distinct value suggests greater diversity. Annotated Toxic Probability computes the percentage of the generated responses that are labeled as \u201cToxic\u201d by human annotators. A higher toxic probability indicates a higher like-\nlihood of producing toxic outputs for generation models. Attack Success Rate computes the percentage of the toxic responses that are misclassified as \u201cNon-Toxic\u201d by classifiers. A higher attack success rate suggests a more challenging-to-detect toxicity. Toxic Confidence computes the average confidence of the classifier in predicting \u201cToxic\u201d for the toxic responses. Unlike the Attack Success Rate, Toxic Confidence is a continuous metric."
        },
        {
            "heading": "4.5 Main Results",
            "text": "From the evaluation results shown in Table 2, we have the following observations: (1) As discussed in Section 2, LLMs exhibit an impressive ability to generate significantly more challenging implicit toxic responses compared to previous datasets. (2) RL further enhances the induction of implicit toxicity in LLMs. With LLaMA-13B as the backbone model, the attack success rate increases from 64.29% to 90.16% on BAD and from 58.34% to 62.85% on Davinci003. Furthermore, we present the continuous toxic confidence in Figure 3. We can see that all the classifiers assign an average toxic confidence lower than 0.2 to the toxic responses generated by the RL LLaMA-13B model, verifying its notable implicit toxicity. (3) The effect of RL can generalize to toxicity classifiers that are not involved during training. Although we only introduce the BAD classifier as P during RL training, the resulting model achieves higher attack success rates across all evaluated classifiers. (4) Our reward model exhibits a preference for implicit toxicity and a positive correlation with attack success rates. For instance, the explicit toxic benchmark Offensive Twitter, which is the easiest to detect, achieves the lowest reward score. In comparison, the responses generated by GPT-3.5-turbo are significantly more challenging to detect and attain a much higher reward score."
        },
        {
            "heading": "Variants Reward Annotated Toxic Prob. Avg. Attack Success Rate",
            "text": ""
        },
        {
            "heading": "4.6 Analysis",
            "text": "Effect of Reward We investigate the performance of training with ablated versions of rewards. As shown in Table 3, training without R\u03b8 mainly steers the model towards non-toxic, leading to a notable reduction in toxic probability from 58.84% to 20.90%. This verifies that R\u03b8 can effectively enhance the implicit toxic signal while reducing the non-toxic signal, thereby improving attack success rates without sacrificing toxic probability. Furthermore, training without P results in a substantial decrease in attack success rates, indicating the importance of involving advanced toxicity classifiers in the reward for effective attacking.\nEffect of Model Scale While our main experiments employ LLaMA-13B as the backbone, we are interested in the scaling property of implicit toxicity in language models. Figure 4 shows that despite using the same data for supervised learning and RL, the attack success rate continuously increases as the model scale develops from 1.3B to 13B. Notably, the 13B model achieves both the highest toxic probability and the greatest attack success rate. Moreover, RL significantly increases attack success rates across different model scales. The observed scaling properties demonstrate that LLMs with more parameters may possess a stronger capacity to implicitly express toxicity. We conjecture that larger models have a greater capacity to absorb diverse linguistic features and extralinguistic knowledge during pre-training, which is important for expressing toxicity implicitly 2. Consequently, they can achieve a higher attack success rate and pose a more substantial threat.\nEffect of KL Coefficient Figure 5 presents the effect of the KL coefficient \u03b2. As we can see, increasing \u03b2 leads to worse rewards and toxic probability. Moreover, the attack success rate on BAD initially increases and then decreases. This indicates that excessively small \u03b2 can lead to undesirable overoptimization (Ibarz et al., 2018; Stiennon et al., 2020). We hence set \u03b2 = 0.1 in our experiments.\n2See Appendix E for more detailed analysis of the scaling properties for expressing implicit toxicity"
        },
        {
            "heading": "Toxic Type GPT-3.5-turbo RL LLaMA-13B",
            "text": "Effect of Toxicity Classifier Coefficient \u03b1 Figure 6 presents the effect of the hyperparameter \u03b1. As we can see, increasing \u03b1 within a reasonable range improves attack success rates while keeping a comparable toxic probability. However, too large \u03b1 results in a substantial decrease in toxic probability since the existing toxicity classifier mainly introduces the non-toxic signal while lacking the implicit toxic signal."
        },
        {
            "heading": "4.7 Analysis of Implicit Toxicity in LLMs",
            "text": "Diverse Toxic Types Following prior works on safety taxonomy (Sun et al., 2023), we select four common toxic types: Offending User, Unfairness and Discrimination, Toxic Agreement, and Sensitive Topic. We then conduct human evaluation to evaluate the toxic types of LLM-generated implicit toxic data. The results in Table 4 highlight the diverse toxic types exhibited by LLMs3.\nDiverse Linguistic Features To demonstrate that LLMs can employ diverse linguistic features to express toxicity, we provide multiple qualitative examples in Appendix C. We can see that LLMs use diverse linguistic features such as circumlocution, euphemism, sarcasm, metaphor, rhetorical question (Frank, 1990), antithesis (Ruzibaeva, 2019), and visual signs (Ocampo et al., 2023). Moreover, LLMs often combine multiple features in their toxic outputs, posing a greater challenge for reasoning over compositional linguistic features.\n3Note that the distribution of toxic types is highly influenced by the input context (e.g., a context aiming to offend the bot will more likely elicit an Offending User response).\nCase Study on Successful Attacks We manually inspect the toxic responses generated by GPT-3.5turbo and RL LLaMA-13B that are misclassified by all the five classifiers. As shown in Figure 7, detecting implicit toxicity in LLMs requires advanced abilities such as knowledge and reasoning over diverse linguistic features. By incorporating our manually-written analysis into the prompt, Davinci003 achieves successful detection."
        },
        {
            "heading": "4.8 Improving Toxicity Classifiers",
            "text": "After unveiling the implicit toxicity of LLMs and the shortage of current toxicity classifiers, we aim to further improve the classifier\u2019s abilities to detect LLM-generated implicit toxic responses. We collect 4K human-labeled LLM-generated toxic responses (2K from GPT-3.5-turbo and 2K from RL LLaMA-13B). We then fine-tune a RoBERTabase model on our data augmented with the BAD dataset. Evaluation results shown in Table 5 demonstrate that our data can effectively help address the implicit toxicity in LLMs without sacrificing the performance on other benchmarks, such as BAD."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 Safety Issues of Language Models",
            "text": "Language models have been shown to exhibit various safety issues, such as generating offensive contents (Gehman et al., 2020), reinforcing unfair-\nness/discrimination (Sap et al., 2020; Abid et al., 2021), leaking privacy information (Carlini et al., 2021; Zhang et al., 2023b), and promoting illegal activities (Zhang et al., 2023a). Recently, new safety issues that emerge with LLMs attract increasing attention since the remarkable capabilities of LLMs can lead to a significant threat (Perez and Ribeiro, 2022; Deshpande et al., 2023). Different from prior works on probing explicit toxic outputs from LLMs that can be easily detected with existing toxicity classifiers, we investigate whether LLMs can generate undetectable implicit toxic outputs. The most similar work to ours is TOXIGEN (Hartvigsen et al., 2022), which proposes an adversarial classifer-in-the-loop decoding method to generate implicit toxic outputs with GPT-3 via fewshot prompting. However, in contrast to TOXIGEN, which solely focuses on generating toxic statements targeting minority groups, we investigate how to generate toxic responses encompassing diverse toxic types and linguistic features. Additionally, we go beyond simple prompting and propose to further induce implicit toxicity in LLMs via reinforcement learning, achieving significantly higher attack success rates."
        },
        {
            "heading": "5.2 Toxicity Detection",
            "text": "Toxicity detection models play a crucial role in evaluating and mitigating the safety issues of LLMs at both pre- and post-deployment stages. Therefore, various benchmark datasets have been built to develop more effective and robust toxic classifiers (Dinan et al., 2019; Xu et al., 2020; ElSherief et al., 2021; Hartvigsen et al., 2022). Among various toxic types, implicit toxicity has gained increasing attention and become a nonnegligible challenge in the field of toxicity detection (ElSherief et al., 2021) since it goes beyond overtly abusive words and is conveyed through diverse linguistic features\nand extralinguistic knowledge. Although there have been several classifiers targeting the detection of implicit toxicity, our experiments demonstrate that they still struggle to detect the LLM-generated toxic responses induced by our method. We further show that fine-tuning these classifiers on the annotated examples generated by our method can successfully enhance their ability to detect implicit toxicity in LLMs."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper identifies a novel safety risk of LLMs, namely their ability to generate implicit toxic outputs, which are exceptionally difficult to detect with existing toxicity classifiers. We first conduct preliminary experiments on GPT-3.5-turbo via zeroshot prompting. We further propose a RL-based method to induce implicit toxicity in LLMs via optimizing the reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Extensive experiments demonstrate that the implicit toxic responses induced by our method are significantly more challenging to detect than previous baselines. Further analysis reveals that LLMs leverage diverse toxic types, linguistic features, and extralinguistic knowledge to express implicit toxicity. Finally, we show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLMgenerated implicit toxic responses."
        },
        {
            "heading": "Limitations",
            "text": "One limitation of our paper is the performance of the reward model used in our method. As illustrated in Section 3.2, while effectively reducing annotation costs, the automatic annotation process inevitably introduces noise and bias into the annotated comparison data. Therefore, our RLfinetuned policy model cannot perfectly and exhaustively find all possible implicit toxic language. Nonetheless, we demonstrate the effectiveness of our proposed RL-based attack method, which successfully uncovers many failure cases of existing toxicity classifiers. We leave the improvement of comparison data quality and the design of a stronger reward model as future work.\nAnother limitation of our paper is that we do not conduct experiments on extra-large policy models, such as LLaMA-65B and GPT-3.5-turbo, due to our limited computational resources or limited access. Based on the scaling properties in Section 4.6, the\nimplicit toxicity in extra-large language models is worth studying in the future work."
        },
        {
            "heading": "Ethics Statement",
            "text": "As mentioned in Section 4.4, we employ crowdsourcing workers to do toxicity annotation. We inform the crowdsourcing workers in advance how their annotation data will be used. We pay them 25 USD per hour, which is higher than the average wage of the local residents.\nThis paper reveals the potential safety risks of large language models and provides an effective method to defend against the proposed attacking method in Section 4.8. We acknowledge that our attacking method could also be exploited to instead create more implicit toxic language. However, we believe that it is an effective approach to red-team and enhance the safety detectors. On balance, this work creates more value than risks."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by National Key R&D Program of China, under Grant No. 2020AAA0104500, the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010). This work was also supported by China National Postdoctoral Program for Innovative Talents (No. BX20230194) and China Postdoctoral Science Foundation (No. 2023M731952)."
        },
        {
            "heading": "A Proximal Policy Optimization",
            "text": "Let D = {x} be the query set, \u03c0\u03d5 be the policy model, and R\u0302\u03b8,\u03d5 be the final reward. Given a query x, the policy model \u03c0\u03d5 autoregressively generates a response y, whose reward is assigned as R\u0302\u03b8,\u03d5(x, y). The training objective is to maximize the expected reward as follows:\nEx\u223cD,y\u223c\u03c0\u03d5(\u00b7|x)[R\u0302\u03b8,\u03d5(x, y)]\nB Implementation Details"
        },
        {
            "heading": "B.1 Data Preprocessing",
            "text": "We first extract the human utterances from the original BAD dataset. We further filter the nonsense\ngreeting utterances such as \u201cHello, how are you doing\u201d. The detailed data statistics are presented in Table 7."
        },
        {
            "heading": "B.2 Training Details",
            "text": "We adopt LLaMA-13B as the backbone model for our main experiments. For supervised learning, we set the batch size to 16, the initial learning rate of the AdamW optimizer to 2e-7, and the maximum training epoch to 10. For reward model training, we initialize the reward model with the supervised learned policy model and freeze the first 70% hidden layers. We set the batch size to 32, the initial learning rate of the AdamW optimizer to 1e-5, and the maximum training epoch to 5. For reinforcement learning, we set the KL coefficient \u03b2 = 0.1, \u03b1 = 5, the batch size to 384, and the initial learning rate of the AdamW optimizer to 5e-6. We freeze the first 80% hidden layers and share parameters between policy and value functions.\nOur codes are implemented based on Huggingface\u2019s Transformers and TRLX4. All experiments are carried out on 8 A100 GPUs (80GB). Each experiment can be completed in one day. Our model selection criterion is validation accuracy for classification models and validation perplexity for generation models."
        },
        {
            "heading": "B.3 Example Prompts",
            "text": ""
        },
        {
            "heading": "B.3.1 Supervised Learning Data Collection",
            "text": "For generating the supervised learning data, we adopt the prompt in Table 8.\n4https://github.com/CarperAI/trlx"
        },
        {
            "heading": "B.3.2 Comparison Data Collection",
            "text": "For annotating the comparison data, we use the prompt in Table 9."
        },
        {
            "heading": "B.3.3 Toxicity Classification",
            "text": "For the toxicity classification task, we adopt the prompt in Table 10."
        },
        {
            "heading": "C Generated Examples",
            "text": "We present some examples of the implicit toxic outputs generated by GPT-3.5-turbo and the RL LLaMA-13B model in Table 11 and Table 12 .\nD Instructions for Human Annotation\nWe present the summary of our human annotation instructions in Figure 8."
        },
        {
            "heading": "E Scaling Properties of Linguistic Features",
            "text": "Larger models have a greater capacity to absorb diverse linguistic features and extralinguistic knowledge during pre-training, which is important for expressing toxicity implicitly (Figure 4). Consequently, they can achieve a higher attack success rate. To further demonstrate this, we manually inspect the generated responses to compare different models\u2019 usage of linguistic features for expressing toxicity implicitly. We randomly sample 50 queries and obtain corresponding generated toxic responses from a RL fine-tuned LLaMA-1.3B/13B model, respectively. We then label the linguistic features used in each response.\nWe report the average feature number used in each response and the distribution of the linguistic features for conveying implicit toxicity. From the results shown in Table 6, we can see that larger models can combine more diverse linguistic features, thereby leading to more implicit toxic responses."
        },
        {
            "heading": "INPUT",
            "text": ""
        },
        {
            "heading": "INPUT",
            "text": ""
        },
        {
            "heading": "INPUT",
            "text": ""
        }
    ],
    "title": "Unveiling the Implicit Toxicity in Large Language Models",
    "year": 2023
}