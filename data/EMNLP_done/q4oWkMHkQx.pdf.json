{
    "abstractText": "Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations. Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations. Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings. Besides, with task-level thinking steps, automatically generated chain-of-thoughts (CoTs) bring more competitive performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chunhui Du"
        },
        {
            "affiliations": [],
            "name": "Jidong Tian"
        },
        {
            "affiliations": [],
            "name": "Haoran Liao"
        },
        {
            "affiliations": [],
            "name": "Jindou Chen"
        },
        {
            "affiliations": [],
            "name": "Hao He"
        },
        {
            "affiliations": [],
            "name": "Yaohui Jin"
        }
    ],
    "id": "SP:ced6f5074ce07942004535dd79320b343ed8f35c",
    "references": [
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Language models as agent models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769\u20135779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Shizhe Diao",
                "Pengcheng Wang",
                "Yong Lin",
                "Tong Zhang."
            ],
            "title": "Active prompting with chain-ofthought for large language models",
            "venue": "arXiv preprint arXiv:2302.12246.",
            "year": 2023
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Mar\u00eda Herrero-Zazo",
                "Isabel Segura-Bedmar",
                "Paloma Mart\u00ednez",
                "Thierry Declerck."
            ],
            "title": "The DDI corpus: An annotated corpus with pharmacological substances and drug\u2013drug interactions",
            "venue": "46(5):914\u2013 920.",
            "year": 2013
        },
        {
            "authors": [
                "Or Honovich",
                "Uri Shaham",
                "Samuel R Bowman",
                "Omer Levy."
            ],
            "title": "Instruction induction: From few examples to natural language task descriptions",
            "venue": "arXiv preprint arXiv:2205.10782.",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Martin Krallinger",
                "Obdulia Rabal",
                "Saber A Akhondi"
            ],
            "title": "Overview of the biocreative vi chemical-protein interaction track",
            "venue": "In Proceedings of the sixth BioCreative challenge evaluation workshop,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Lampinen",
                "Ishita Dasgupta",
                "Stephanie Chan",
                "Kory Mathewson",
                "Mh Tessler",
                "Antonia Creswell",
                "James McClelland",
                "Jane Wang",
                "Felix Hill"
            ],
            "title": "Can language models learn from explanations in context? In Findings of the Association for Computa",
            "year": 2022
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Saif Mohammad",
                "Svetlana Kiritchenko",
                "Parinaz Sobhani",
                "Xiaodan Zhu",
                "Colin Cherry."
            ],
            "title": "SemEval-2016 task 6: Detecting stance in tweets",
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Harsha Nori",
                "Nicholas King",
                "Scott Mayer McKinney",
                "Dean Carignan",
                "Eric Horvitz."
            ],
            "title": "Capabilities of gpt-4 on medical challenge problems",
            "venue": "arXiv preprint arXiv:2303.13375.",
            "year": 2023
        },
        {
            "authors": [
                "Jane Pan",
                "Tianyu Gao",
                "Howard Chen",
                "Danqi Chen."
            ],
            "title": "What in-context learning\" learns\" in-context: Disentangling task recognition and task learning",
            "venue": "arXiv preprint arXiv:2305.09731.",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "KaShun Shum",
                "Shizhe Diao",
                "Tong Zhang."
            ],
            "title": "Automatic prompt augmentation and selection with chain-of-thought from labeled data",
            "venue": "arXiv preprint arXiv:2302.12822.",
            "year": 2023
        },
        {
            "authors": [
                "Miles Turpin",
                "Julian Michael",
                "Ethan Perez",
                "Samuel R Bowman."
            ],
            "title": "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "venue": "arXiv preprint arXiv:2305.04388.",
            "year": 2023
        },
        {
            "authors": [
                "\u00d6zlem Uzuner",
                "Brett R South",
                "Shuying Shen",
                "Scott L DuVall."
            ],
            "title": "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text",
            "venue": "Journal of the American Medical Informatics Association, 18(5):552\u2013556.",
            "year": 2011
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim."
            ],
            "title": "Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models",
            "venue": "Proceedings of the 61th Annual Meeting of the Associa-",
            "year": 2023
        },
        {
            "authors": [
                "Albert Webson",
                "Ellie Pavlick"
            ],
            "title": "Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Yue Deng",
                "Bing Liu",
                "Sinno Jialin Pan",
                "Lidong Bing."
            ],
            "title": "Sentiment analysis in the era of large language models: A reality check",
            "venue": "arXiv preprint arXiv:2305.15005.",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "The Eleventh International Conference on Learning Representations (ICLR 2023).",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Chuanyang Zheng",
                "Zhengying Liu",
                "Enze Xie",
                "Zhenguo Li",
                "Yu Li."
            ],
            "title": "Progressive-hint prompting improves reasoning in large language models",
            "venue": "arXiv preprint arXiv:2304.09797.",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "arXiv preprint arXiv:2211.01910.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning, and question answering (Dong et al., 2022). With the new paradigm of in-context learning (ICL) (Brown et al., 2020; Pan et al., 2023), LLMs directly adapt to downstream tasks without updating parameters by prompting with few demonstrations. Wei et al. (2022) found that elaborating the reasoning steps in demonstrations can significantly stimulate the complex reasoning ability of LLMs, which is called manual chain-of-thought (CoT). Zero-shot-CoT (Kojima et al., 2022) enables LLMs to generate thinking steps automatically using the simple but efficient prompt \"Let\u2019s think step by step.\", as shown in Figure 1(a). More recently, plan-andsolve prompt (Wang et al., 2023) extends it with\n\"Let\u2019s first understand the problem, devise a plan, and solve the problem step by step.\" to address the issue of missing reasoning steps and \"extract relevant variables and their corresponding numerals\" to address the issue of calculation errors. Inspired by this, we propose the concept of task-level thinking steps for challenging classification tasks as shown in Figure 1(b), which helps LLMs to clarify tasks and remind them of overlooked issues. To this end, we design a progressive revision framework, which automatically generates thinking steps by LLM agents and then progressively revise the steps by correcting hard demonstrations.\nFor classification tasks, it is widely investigated that the demonstrations distribution, including content, label, and even the order, can severely affect the performance (Lu et al., 2022a; Turpin et al., 2023; Zhao et al., 2021). Therefore, many works focused on selecting high-quality and representative demonstrations, such as selecting similar demonstrations (Liu et al., 2021) or maintaining the class balance between demonstrations (Lu et al., 2022b). However, the best distribution depends on the task, and bias is inevitable. Our proposed task-level thinking steps are helpful in reducing the bias introduced by demonstrations. Table 1 shows an example of the stance detection task. Zero-shot-CoT\npredicts correctly with no demonstrations. However, the output is severely affected by \"against\" from the biased demonstrations. The automatically generated task-level thinking steps include: identifying the domain; finding the viewpoint; determining the stance with respect to the domain; and matching the choices. With the help of these steps, LLMs perceive the implicit meaning \"a fetus is not a living, sentient, or autonomous being\" and make the correct prediction \"in favor of abortion\".\nIn addition to debiasing, we would like task-level thinking steps to clarify some confusing classes. To this end, we design a progressive revision framework to generate the thinking steps by correcting\nhard demonstrations. In the framework, as shown in Figure 2, we set up two LLM agents (Andreas, 2022) with character settings and historical memories, the teacher agent and the student agent. For each hard demonstration, the student agent generates outputs based on the task-level thinking steps generated by the teacher agent. If the prediction is correct, move on to the next demonstration. If the prediction is wrong, the teacher agent first analyses the error cause and then tries to revise the tasklevel thinking steps. For the example in Figure 2, class (B) and class (D) tend to be confusing. After the teacher agent\u2019s analysis, an explicit reminder can be inserted in a suitable place in the revised task-level thinking steps. Then, the student agent generates outputs again. The iteration may be repeated several times until the correct prediction or the maximum number of times allowed is reached.\nIn this paper, we consider three classification tasks: multifaceted analysis of subjective text requires first locating aspects, then identifying opinions, and finally analyzing polarity; fine-grained text classification requires a deep distinction between numerous and similar classes; domainspecific classification requires the understanding of domain-related questions, choices, and demonstrations. We conduct experiments on zero-shot, few-shot, and few-shot-CoT settings, specifically. Experimental results demonstrate the effectiveness of our proposed method by outperforming the competitive baseline models. Further ablation analyses and robustness experiments reveal the benefits and effects of each module.\nOur contributions are threefold: 1) We propose the novel concept of task-level thinking steps, which can eliminate bias introduced by demonstrations 2) We design a progressive revision framework, which purposefully improves the thinking steps based on feedback from hard demonstrations for LLMs. 3) Our proposed method surpasses competitive baselines in the zero-shot, few-shot, and few-shot-CoT settings."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "We consider a general classification task, where T denotes the task description, x \u2208 X is the text input, and y \u2208 Y is the corresponding label. |Y| = C is the number of classes. Dtrain = {(x1, y1), ..., (xM , yM )} denotes the train dataset and Dtest = {(x1, y1), ..., (xN , yN )} denotes the\ntest dataset. An LLM with parameters \u03b8 performs zero-shot learning by conditioning on the task description T and the test input x \u223c Dtest. The likelihood of a candidate answer y could be represented by a scoring function f :\nP (y|x) = f\u03b8(T, x) (1)\nWhen sampling demonstrations Ddemo from Dtrain, the likelihood for few-shot learning is:\nP (y|x) = f\u03b8(T,Ddemo, x) (2)\nThe final predicted label y\u0302 is the candidate answer with the highest probability:\ny\u0302 = argmax y\u2208Y\nP (y|xtext) (3)\nOnly hard demonstrations can generate valuable task-level thinking steps. In this paper, we traverse Dtrain and select one that the LLMs can not predict correctly for each class to construct Ddemo = {(x0, y0), ..., (xC\u22121, yC\u22121)} as shown in Figure 2."
        },
        {
            "heading": "2.2 LLM Agents",
            "text": "LLMs can serve as models of agents as suggested in (Andreas, 2022), which output responses based on initial setting, historical memories, and current\nobservations. We set up two LLM agents. The teacher agent is responsible for generating the tasklevel thinking steps from hard demonstrations with labels. In addition, the teacher agent needs to analyze the errors in the student agent\u2019s answers and revise the task-level thinking steps. The student agent is responsible for giving answers and explanations for each presentation with the task-level thinking steps provided by the teacher agent."
        },
        {
            "heading": "2.3 Progressive Revision Framework",
            "text": "Given hard demonstrations Ddemo, the teacher agent first generates initial thinking steps S0, then progressively revise it based on the outputs from the student agent as shown in Figure 2. Generate initial task-level thinking steps. The teacher agent uses hard demonstrations to generate initial task-level thinking steps. The prompt is \"Please generate generic thinking steps for the task.\". Taking the medical relation extraction task as an example, here are the generated initial tasklevel thinking steps \" 1. Identify the two medical terms or problems mentioned in the context, marked as <e1> and <e2>. 2. Determine the relationship between <e1> and <e2> based on the context. 3. Choose the answer choice that best describes the relationship between <e1> and <e2> from the given options. \".\nWe can see that the teacher agent understands the task description and gives three reasonable steps: identifying two entities, determining the relation, and matching the classes. However, they are also preliminary and abbreviated. Generate outputs. For the i-th hard demonstration xi, the student agent receives the task-level thinking steps Si and generates outputs Ri. The prompt is \"Thinking steps: {Si}. Let\u2019s think following the thinking steps.\" As shown in Figure 2, the correct answer is \"(B) <e1> worsens <e2>\" but the student agent outputs \"The answer is (D)\" and sends it to the teacher agent. Improve task-Level thinking steps. The teacher agent analyzes the reason for the error with the prompt \"Input: {xi}. Outputs: {Ri}. Please analyze the outputs from the student following your thinking steps.\". Then the teacher agent revises the task-level thinking steps with the prompt \"Please revise the thinking steps to clarify.\".\nWe find that the revised thinking steps do not usually change much but rather extend more reminders. The main extension in this example is \" If the treatment is ineffective or worsens the medical problem, consider choice (B) rather than choice (D).\". It can be seen that the student agent only focuses on the causal relationship between the treatment and the medical problem while does not pay attention to the effects. In fact, this input places more emphasis on the effect of the treatment on the disease than on how the treatment should be taken for the disease. Generate outputs with revised thinking steps. With the revised thinking steps Si, the student agent predicts correctly. Then, the iteration proceeds to the next hard demonstration. Otherwise, the output is sent to the teacher again until the correct prediction or the maximum attempts are reached. Experiments show that even if the hard demonstration was not successfully corrected, the revised thinking step was still useful. Checking mechanism. We find that LLMs, especially gpt-4, are effective in correcting hard samples by revising task-level thinking steps. However, this may be achieved by extending shortcuts rather than trustworthy analysis. For example, \"If you are not sure, choose (B).\". Therefore, we set up a checking mechanism. The teacher agent tries to generate multiple candidate thinking steps that can successfully correct xi. The student agent tests performance on {x0, ..., xi\u22121} and keeps the best one."
        },
        {
            "heading": "2.4 Automatic Chain-of-Thought",
            "text": "Another advantage of our proposed task-level thinking steps is that is very helpful to generate reliable CoTs for demonstrations automatically. Manual CoTs are labor intensive, and the human annotations do not necessarily match LLMs. Zero-shotCoT, while avoiding human engineering, may produce unfaithful explanations especially for hard demonstrations."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datsets",
            "text": "We choose three kinds of challenging classification tasks. The statistics and descriptions of all tasks are shown in Table 7 and Table 8 in Appendix B. Multifaceted analysis of subjective text is the task that involves different aspects of subjective human feeling reflected in the text. The challenge is recognizing and understanding a broader range of human emotional states rather than identifying common positive or negative feelings. Following Zhang et al. (2023a), we use Stance16 (Mohammad et al., 2016) for stance detection and Emotion20 (Barbieri et al., 2020) for emotion recognition. Fine-grained text classification is the task of categorizing text or documents into predefined topics or categories. We carefully select 20news 1 dataset. The challenge lies in numerous and similar classes. Domain-specific classification adopts the medical relation classification task, which refers to identifying the relationship between pairs of entities within medical texts. The challenge is the need for domain knowledge, and differences between relations are subtle. i2b2 (Uzuner et al., 2011) collects clinical records and is used to classify relations between medical problems, medical tests, and treatments. ChemProt (Krallinger et al., 2017) is used to determine how chemical acts on the protein, such as activation and inhibition. DDI (Herrero-Zazo et al., 2013) is used to determine the interaction relation between two drugs, such as pharmacokinetics and pharmacodynamics."
        },
        {
            "heading": "3.2 Experimental Settings",
            "text": "We constructed experiments to evaluate the effectiveness of thinking steps. C hard demonstrations are sampled for each task to generate thinking steps. The LLM for the teacher agent is gpt-4, the LLM for the student agent and the LLM for running\n1http://qwone.com/~jason/20Newsgroups/\nexperiments are both gpt-3.5-turbo. We set a temperature of 0 for all experiments. For each hard demonstration, the maximum number of revision attempts is 5. For the checking mechanism, the number of candidates is 3. We report the F1-score as the evaluation metric.\nThe followings methods serve as baselines: Default generates outputs directly and Auto generates outputs with the prompt \"Let\u2019s think step by step\" (Kojima et al., 2022). Our methods generate outputs with Initial Thinking Steps (ITS) to validate the effectiveness of thinking steps and Progressively Revised Thinking Steps (PRTS) to validate the effectiveness of framework. For few-shot, prompt demonstrations are the same as hard demonstrations to avoid unfair comparisons. In Section 4.2, we construct experiments with other prompt demonstrations. For few-shot-CoT, Auto, ITS and PRTS can automatically explain the prompt demonstrations without human efforts following (Wang et al., 2023; Shum et al., 2023). Detailed prompt templates are shown in Table 9 in Appendix B."
        },
        {
            "heading": "4 Results",
            "text": ""
        },
        {
            "heading": "4.1 Main Results",
            "text": "The experimental results are shown in Table 2. All results are averaged over three runs. Overall, PRTS performs best on all tasks, which outperforms Auto by 18.75% for zero-shot, 9.89% for few-shot, and 20.64% for few-shot-CoT on average.\nFor the zero-shot setting, ITS has slightly better performance than Default and Auto on average, but which one is better depends on the task. Specifically, ITS outperforms Auto by 23.33% for 20news and by 14.19% for i2b2. One possible reason is that\nboth 20news and i2b2 have many similar classes, and the thinking steps include analysis and comparison between classes, which may help in the fine-grained classification of LLMs. With progressive revision, PRTS performs best on all tasks.\nFor the few-shot setting, PRTS outperforms Auto by an average of 9.89%. It is worth noting that ITS has worse performance than Auto. One possible reason is that initial thinking steps may be contradictory to ICL for LLMs. However, we will show that significant improvements can be obtained by explaining these demonstrations with thinking steps.\nFor few-shot-CoT, PRTS outperforms Auto by 20.64% and ITS by 10.01% on average. For Auto, Stance16 and 20news have worse performance compared with the few-shot setting. One possible reason is that LLMs may produce reluctant explanations and hallucinations for hard demonstrations. Table 4 shows the automatically generated CoT by Auto, ITS, and PRTS: For Auto, it can be seen that the LLM can not understand the stance detection task. Therefore, the LLM fails to locate the domain word \"atheism\" and mistakenly believes the answer is \"(C) favor\". However, faced with the golden label \"(B) against\", the LLM can only generate the reluctant and absurd explanation: \"against (B) is the closest to the opposite of favor\". With this wrong CoT demonstration, a worse performance than zero-shot and few-shot would make sense. For ITS, LLM successfully locates the domain word \"atheism\" and seems to explain the golden label correctly. However, LLM only states that \"against is the idea of freedom from religion\" but does not explicitly state that atheism is equivalent to free-\ndom from religion. This may lead LLMs to ignore domain words and only decide on the sentence. For PRTS, the CoT is more consistent and complete and always revolves around the domain word \"atheism\". COT explicitly explains that \"a stance against atheism\" is because \"people should not be free from religion\"."
        },
        {
            "heading": "4.2 Ablation Studies",
            "text": "Analysis of demonstrations distribution. In this section, we explore the effects of various sampling methods of prompt demonstrations. For the few-shot setting, Similar (Liu et al., 2021) dynamically selects C most similar for each sample. Unbalanced selects prompt demonstrations in the same class, which is considered as biased (Zhao et al., 2021). Easy is the same as our hard demonstration selection, which selects one that LLMs predict correctly for each class. Results are reported in Table 3 for Stance16, 20news, and i2b2. We can find that which sampling method is better depends on the task, but PRTS consistently improves by 9.93% for Similar, 27.71% for Unbalanced, and 13.3% for Easy, respectively. Especially compared with Unbalanced performing poorly on all tasks, Unbalanced+PRTS remains very robust. This shows that PRTS can eliminate bias introduced by demonstrations effectively.\nAnalysis of the progressive revision framework. Our key idea is revising the task-level thinking steps by correcting the hard demonstrations, thus enhancing the ability to distinguish confusing classes. Specifically, we first correct the hard demonstrations and then further improve thinking steps through the Checking mechanism.\nWhile correcting the hard demonstrations of class c, it is also beneficial to all samples. We validate this idea on two datasets for zero-shot setting: Stance16 and i2b2 as shown in Figure\n3. The class c of the hard demonstration being corrected is named revising class while the remaining classes {1, ..., c \u2212 1, c + 1, ..., C} are named affected classes. It is always helpful for revising class and may also have a positive effect on affected classes.\nIn addition, we further improve thinking steps through the checking mechanism when iterating, i.e., progressive revising thinking steps. As shown in Figure 4, for PRTS w/o checking mechanism, huge performance degradation occurs for class (B) in Stance16 and class (D) in i2b2. For comparison, PRTS w/ checking mechanism almost improves progressively in each round and outperforms the PRTS w/o checking mechanism 5.29% for Stance16 and 3.07% for i2b2."
        },
        {
            "heading": "4.3 Robustness Experiments",
            "text": "Effects of class names. For classification tasks, class names can seriously affect the performance of LLMs. For 20news and DDI, the default classes are ambitious, and we have manually annotated them as manual classes. In this section, we explore the effects of different class names. More details are shown in Table 8 in Appendix B.\nWe experimented on zero-shot, few-shot, and\ninput: For domain atheism, The framers of our Constitution meant we were to have freedom of religion,\nfew-shot-CoT for Auto and PRTS. To our surprise, the default classes performed much better than the manual classes, especially for Auto on 20news, as shown in Figure 5(a). The reason may be that 20news is a pre-trained corpus for LLMs and is therefore more compatible with the default classes. Nevertheless, PRTS has a similar performance with manual classes and default classes.\nFor DDI, the performance on the default classes is particularly worse than on manual classes for Auto as shown in Figure 5(b). We find that LLMs can correctly recognize and interpret the medical terms such as \"pharmacokinetics\" and \"pharmacodynamics\" with the strong capability on medical problems (Nori et al., 2023). The poor performance is because default class \"int\" was mistakenly thought to be a useless and distracting choice. However, PRTS achieves similar performance to manual classes because the task-level thinking steps explicitly state that\n\"int is an abbreviation for interaction except pharmacokinetics and pharmacodynamics\".\nEffects of LLMs. We find that only gpt-4 helps the teacher agent to revise thinking steps progressively. text-davinci-003 is not a dialogue model and can not even produce reasonable initial thinking steps. gpt-3.5-turbo enables interaction between the student and the teacher, but it is difficult to generate valuable thinking steps to correct the hard demonstrations. For the hard demonstrators covered in this paper, gpt-4 can almost revise thinking steps to correct them within five attempts.\nTherefore, we keep the previous obtained thinking steps and rerun the experiment on text-davinci003 and gpt-4 as shown in Table 5. Overall, the classification performance of text-davinci-003, gpt3.5-turbo, and gpt-4 improves sequentially for both Auto and PRTS. Interestingly, the performance improvements of the fine-grained classification task, 20news (39.97% \u2192 70.03%) and i2b2 (29.63% \u2192 58.33%), are huge from gpt3.5turbo to gpt-4 for Auto. It is not clear whether this is because gpt-4 was pretrained on these datasets or its instruction tuning is similar to our thinking steps. Besides, all three LLMs perform poorly on Stance16 for Auto, while PRTS can get a huge boost on gpt-4. This suggests that PRTS can accurately analyze and disambiguate the task for ambitious tasks, then provide explicit thinking steps."
        },
        {
            "heading": "4.4 Cost Analysis",
            "text": "In this section, we analyze the additional cost of task-level thinking steps includes the generation cost based on hard demonstrations and prompt\ncost for test inputs.\nGeneration cost includes the selection of hard demonstrations and iterative refinement. Assume that the average tokens of train/test inputs are Tinput (outputs with simple answers can be ignored). It takes an average traversal of H train inputs to select hard demonstrations for each of the C classes. Thus, the selection cost is Tinput\u00d7H \u00d7C. Assume that the average tokens of task-level thinking steps is Ttask. In each iteration of the refinement phase, the teacher agent needs to predict each hard demonstration with cost Ttask + 2Tinput based on task-level thinking. There is a degree of simplification, but roughly the input tokens are all Tinput + Ttask and the output tokens are all Ttask. The student agent needs to update tasklevel thinking steps based on test input with cost Ttask+2Tinput. There is a degree of simplification, but roughly the input tokens are all Tinput + Ttask and the output tokens are all Ttask. Then the refinement cost is 2R \u00d7 (Ttask + 2Tinput) \u00d7 C, where R is the number of iterations for each hard demonstration.\nIn the inference phase, the cost of zero-shot prompt (Default) is Tinput, zero-shot prompt with \"Let\u2019s think step by step\" (Auto) is Tinput + Ttask, and prompt with task-level thinking steps (PRTS) is Tinput + 2Ttask.\nThe total cost with Ntest test inputs is sum-\nmarised in the Table 6. Taking i2b2 dataset as an example, H = 3, C = 8, R = 4, and Ttask is approximately 2Tinput. For practical applications, Ntest is much larger than 168, so generation tokens can be ignored. The task-level thinking steps introduced in the inference phase may need to be improved in the future."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 In-context Learning",
            "text": "It is widely investigated that severe bias exists in the output of LLMs for different demonstrations (Lu et al., 2022a; Turpin et al., 2023). Zhao et al. (2021) proposed a calibration method that fits the calibration parameters for uniform content-free input \"N/A\" is uniform across choices. More studies try to select specific demonstrations based on different philosophies to reduce bias, such as similarity (Liu et al., 2021) selecting similar examples to facilitate analogical learning, high uncertainty to improve inference efficiency (Diao et al., 2023) selection. Min et al. (2022) showed that LLMs could largely ignore the mapping between inputs and labels, while Webson and Pavlick (2022) showed that LLMs perform well on the NLI dataset with correct demonstrations even if the instruction is irrelevant or misleading."
        },
        {
            "heading": "5.2 Chain-of-Thought",
            "text": "Recent works have shown that LLMs can output better responses for ICL with high-quality explanation (Lampinen et al., 2022). Manual chain-ofthought (Wei et al., 2022) could prompt LLMs to generate step-by-step solutions, which leads to substantial improvements on many reasoning-intensive tasks. However, the performance improvement depends on manual efforts. The zero-shot CoT (Kojima et al., 2022) enables LLMs to automatically generate explanations. For unlabelled demonstrations, auto-CoT (Zhang et al., 2023b) partitions demonstrations into a few clusters, selects a representative demonstration from each cluster, and generates explanations using zero-shot CoT. For labeled demonstrations, Shum et al. (2023) optimizes a set of latent variables to select the most helpful and suitable demonstrations with correct self-generated answers. Except for the first-try CoT, Zheng et al. (2023) used previous outputs as hints to progressively guide toward the correct answers. Madaan et al. (2023) provided multi-aspect feedback on the previous output and refined it into new output. Our framework is also iterative, but it works on the task level and is based on feedback from hard demonstrations."
        },
        {
            "heading": "5.3 Prompt Engineering",
            "text": "Prompt offers a natural and intuitive interface for humans to interact with language models. For parameter-accessible models, gradient-based methods are widely adopted (Shin et al., 2020; Qin and Eisner, 2021; Lester et al., 2021). In the era of LLMs, searching directly in the natural language hypothesis is more popular. Honovich et al. (2022) found LLMs can generate the task instruction with several demonstrations. Zhou et al. (2022) automatically generates many instructions for the given demonstrations and selects the one with the maximum score function. The concept of task-level thinking steps is also part of prompt engineering, from simple \"Let\u2019s think step by step\" (Kojima et al., 2022) to well-designed plan-and-solve prompt (Wang et al., 2023)."
        },
        {
            "heading": "6 Conclusion",
            "text": "Although LLMs have shown incredible performance on many tasks, we pointed out that many classification tasks, such as multifaceted analysis of subjective text, fine-grained classification, and domain-specific classification tasks are still chal-\nlenging. Therefore, we proposed the concept of task-level thinking steps and verified the robustness for biased demonstrations. To further enhance the classification capabilities of LLMs, we designed a progressive revision framework, which purposefully improves the thinking steps based on feedback from hard demonstrations. Experimental results proved the superiority of our proposed method. Besides, with task-level thinking steps, we found that the automatically generated CoTs are more reasonable and effective.\nLimitations\nTask coverage. We have proposed three challenging classification tasks: multifaceted analysis of subjective text, fine-grained text classification, and medical relation classification. Although there are many nontrivial findings, they are limited to traditional classification tasks. Future work would extend to a wider range of classification tasks, such as natural language inference and multiple choice.\nExperimental analysis and explanation. Although many ablation experiments, robustness experiments and case studies have been conducted, there are still some aspects that have not been discussed. As an example, we adopt a default revising order for each task. While the checking mechanism reduces the negative impact of revision, there may still be other unforeseen challenges.\nReliability. Although our progressive revision framework has been validated on several challenging datasets, LLMs remain sensitive to small changes in the thinking steps. Future work would improve the reliability of the thinking steps by refining hard demonstrations selection, thinking steps revision and checking mechanisms.\nUnlabeled training sets. The absence of massively labeled training sets is indeed a practical problem. However, we think there are still feasible solutions to obtain hard demonstrations. For example, measure uncertainty by running zero-shot prompts multiple times. If LLMs have the same outputs multiple times for the same train input, it means that the LLMs have high confidence. Either completely correct or completely wrong due to bias. On the contrary, if LLMs have very different outputs for the same input, it means that LLMs hesitate between certain classes and this train input\ncan be considered a hard demonstration. Annotate them and continue to use the proposed method in this paper.\nEthics Statement"
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key Research and Development Program of China (2018YFC0830400) and the Shanghai Science and Technology Innovation Action Plan (20511102600)."
        },
        {
            "heading": "A Details of Biased Demonstrations",
            "text": "Here are three biased demonstrations omitted in Table 1:\nFor domain abortion, Our #TruthTour cont\u2019 in the @user as we show the humanity of the unborn & inhumanity of abortion!\nFor domain feminist, Feel things will truly be equal in marriage when I see jock strap tossed to the \u2019single women\u2019 a weddings alongside the garter\nFor domain hillary, user Here\u2019s to fearless women chasing their goals."
        },
        {
            "heading": "B Details of Classification Tasks",
            "text": "We present the statistics of challenging classification tasks in Table 7, description in Table 8, prompt template in Table 9.\nTask Dataset Train Dev Test Sampled Test Labels\nMultifaceted analysis of subjective text Stance16 2620 294 1249 500 3\nEmotion20 3257 374 1421 500 4 Topic classification 20news 10.2k 1.1k 7.53k 500 20\nMedical relation classification i2b2 2808 312 6193 500 8\nChemProt 3370 2030 2910 500 5 DDI 3598 400 976 500 4\nDataset Description\n20news Please perform topic classification task from choices. manual classes: (A) alt.atheism includes discussions and articles related to atheism, atheistic beliefs, and criticisms of religion (B) comp.graphics covers topics related to computer graphics, including discussions on image processing, rendering, and computer-aided design (CAD) (C) comp.os.ms-windows.misc focuses on discussions related to the Microsoft Windows operating system, including troubleshooting, software recommendations, and general Windows-related topics (D) comp.sys.ibm.pc.hardware revolves around discussions related to IBM-compatible PC hardware, including topics like motherboards, processors, memory, and peripherals (E) comp.sys.mac.hardware deals with discussions related to Apple Macintosh hardware, including Mac models, peripherals, and troubleshooting (F) comp.windows.x focuses on the X Window System, a widely used Unix-like operating systems, including System configuration, applications, and programming (G) misc.forsale includes postings of items for sale, ranging from electronics to household goods (H) rec.autos covers discussions related to automobiles, including car models, maintenance, buying/selling advice, and automotive technologies (I) rec.motorcycles includes discussions related to motorcycles, covering topics such as different motorcycle models, maintenance, safety, and riding experiences (J) rec.sport.baseball focuses on discussions related to baseball, including game analysis, player statistics, team performance, and baseball news (K) rec.sport.hockey covers discussions related to ice hockey, including game analysis, player statistics, team performance, and hockey news (L) sci.crypt revolves around discussions related to cryptography, encryption algorithms, and cryptographic protocols (M) sci.electronics deals with discussions related to electronics, including electronic circuits, components, and troubleshooting (N) sci.med includes discussions related to medical topics, including diseases, treatments, healthcare practices, and medical research (O) sci.space focuses on discussions related to space exploration, astronomy, and topics related to outer space (P) soc.religion.christian covers discussions related to Christianity, including theological debates, biblical interpretations, and religious practices (Q) talk.politics.guns revolves around discussions related to gun ownership, gun control policies, and the Second Amendment in the United State (R) talk.politics.mideast includes discussions related to politics in the Middle East, covering topics such as conflicts, peace negotiations, and regional dynamics (S) talk.politics.misc covers discussions related to politics that do not fit into the other specific political categories (T) talk.religion.misc includes discussions related to religion that do not fit into the other specific religious categories default classes: (A) alt.atheism (B) comp.graphics (C) comp.os.ms-windows.misc (D) comp.sys.ibm.pc.hardware (E) comp.sys.mac.hardware (F) comp.windows.x (G) misc.forsale (H) rec.autos (I) rec.motorcycles (J) rec.sport.baseball (K) rec.sport.hockey (L) sci.crypt (M) sci.electronics (N) sci.med (O) sci.space (P) soc.religion.christian (Q)talk.politics.guns (R) talk.politics.mideast (S)talk.politics.misc (T) talk.religion.misc\nDataset Description\ni2b2\nWhat\u2019s the relation between <e1>treatment or medical test or medical problem</e1> and <e2>medical problem</e2>? classes: (A) <e1>Treatment</e1> improves <e2>medical problem</e2> (B) <e1>Treatment</e1> worsens <e2>medical problem</e2> (C) <e1>Treatment</e1> causes <e2>medical problem</e2> (D) <e1>Treatment</e1> is administered for <e2>medical problem</e2> (E) <e1>Treatment</e1> is not administered because of <e2>medical problem</e2> (F) <e1>Medical Test</e1> reveals <e2>medical problem</e2> (G) <e1>Medical Test</e1> conducted to investigate <e2>medical problem</e2> (H) <e1>Medical problem</e1> indicates <e2>medical problem</e2> PRTS: 1. Read the context carefully and identify the e1 and e2 elements. 2. Understand the meaning of e1 and e2 in the context. 3. Analyze the relationship between e1 and e2 based on the context. Keep in mind that the relationship could be between medical problems, treatments, or tests. Pay close attention to whether the relationship implies improvement, worsening, causation, or a decision not to administer a treatment or test. Also, consider if the treatment is ineffective or has an unintended effect on the medical problem. 4. Compare the relationship with the given choices and select the one that best matches the context. If the relationship is between two medical problems, consider choice (H) as a possible answer. If the context suggests a decision not to administer a treatment or test, consider choice (E) as a possible answer. If the treatment is ineffective or worsens the medical problem, consider choice (B) as a possible answer. Be cautious not to confuse choice (B) with choice (D) when the treatment is administered but does not have the desired effect on the medical problem."
        }
    ],
    "title": "Automatic Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task",
    "year": 2023
}