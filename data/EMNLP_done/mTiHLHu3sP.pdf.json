{
    "abstractText": "In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fullysupervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of ICL for RE: (1) low relevance regarding entity and relation in existing sentence-level demonstration retrieval approaches for ICL; and (2) the lack of explaining input-label mappings of demonstrations leading to poor ICL effectiveness. In this paper, we propose GPT-RE to successfully address the aforementioned issues by (1) incorporating task-aware representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets and observe that GPTRE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines as in Figure 1. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets. Additionally, a critical issue of LLMs revealed by previous work, the strong inclination to wrongly classify NULL examples into other predefined labels, is substantially alleviated by our method. We show an empirical analysis.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhen Wan"
        },
        {
            "affiliations": [],
            "name": "Fei Cheng"
        },
        {
            "affiliations": [],
            "name": "Zhuoyuan Mao"
        },
        {
            "affiliations": [],
            "name": "Qianying Liu"
        },
        {
            "affiliations": [],
            "name": "Haiyue Song"
        },
        {
            "affiliations": [],
            "name": "Jiwei Li"
        },
        {
            "affiliations": [],
            "name": "Sadao Kurohashi"
        }
    ],
    "id": "SP:7cf486ac5329b32cb0034bd332944aa496260cff",
    "references": [
        {
            "authors": [
                "Livio Baldini Soares",
                "Nicholas FitzGerald",
                "Jeffrey Ling",
                "Tom Kwiatkowski."
            ],
            "title": "Matching the blanks: Distributional similarity for relation learning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2895\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Michele Banko",
                "Oren Etzioni."
            ],
            "title": "The tradeoffs between open and traditional relation extraction",
            "venue": "Proceedings of ACL-08: HLT, pages 28\u201336, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Terra Blevins",
                "Hila Gonen",
                "Luke Zettlemoyer."
            ],
            "title": "Prompting language models for linguistic structure",
            "venue": "CoRR, abs/2211.07830.",
            "year": 2022
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning"
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "year": 2015
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "CoRR, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Amir D.N. Cohen",
                "Shachar Rosenman",
                "Yoav Goldberg."
            ],
            "title": "Relation extraction as two-way spanprediction",
            "venue": "CoRR, abs/2010.04829.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoqing Geng",
                "Xiwen Chen",
                "Kenny Q. Zhu",
                "Libin Shen",
                "Yinggong Zhao."
            ],
            "title": "MICK: A metalearning framework for few-shot relation classification with small training data",
            "venue": "CIKM \u201920: The 29th ACM International Conference on Information",
            "year": 2020
        },
        {
            "authors": [
                "Bernal Jim\u00e9nez Guti\u00e9rrez",
                "Nikolas McNeal",
                "Clay Washington",
                "You Chen",
                "Lang Li",
                "Huan Sun",
                "Yu Su."
            ],
            "title": "Thinking about GPT-3 in-context learning for biomedical ie? think again",
            "venue": "CoRR, abs/2203.08410.",
            "year": 2022
        },
        {
            "authors": [
                "Xu Han",
                "Hao Zhu",
                "Pengfei Yu",
                "Ziyun Wang",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Iris Hendrickx",
                "Su Nam Kim",
                "Zornitsa Kozareva",
                "Preslav Nakov",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Sebastian Pad\u00f3",
                "Marco Pennacchiotti",
                "Lorenza Romano",
                "Stan Szpakowicz"
            ],
            "title": "SemEval-2010 task 8: Multiway classification of semantic relations between pairs",
            "year": 2010
        },
        {
            "authors": [
                "Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "CoRR, abs/2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "year": 2019
        },
        {
            "authors": [
                "Fangchao Liu",
                "Hongyu Lin",
                "Xianpei Han",
                "Boxi Cao",
                "Le Sun."
            ],
            "title": "Pre-training to match for unified lowshot relation extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5785\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for GPT-3",
            "venue": "In Proceedings of Deep Learning Inside Out (DeeLIO",
            "year": 2022
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Yi Luan",
                "Luheng He",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Nikolay Malkin",
                "Zhen Wang",
                "Nebojsa Jojic."
            ],
            "title": "Coherence boosting: When your pretrained language model is not paying enough attention",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work? CoRR, abs/2202.12837",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,",
            "year": 2021
        },
        {
            "authors": [
                "Rimell",
                "Chris Dyer",
                "Oriol Vinyals",
                "Kareem Ayoub",
                "Jeff Stanway",
                "Lorrayne Bennett",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Geoffrey Irving"
            ],
            "title": "Scaling language models: Methods, analysis &amp; insights from training gopher",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Richard Shin",
                "Christopher Lin",
                "Sam Thomson",
                "Charles Chen",
                "Subhro Roy",
                "Emmanouil Antonios Platanios",
                "Adam Pauls",
                "Dan Klein",
                "Jason Eisner",
                "Benjamin Van Durme."
            ],
            "title": "Constrained language models yield few-shot semantic parsers",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Aroyo",
                "Ravi Rajakumar",
                "Alena Butryna",
                "Matthew Lamm",
                "Viktoriya Kuzmina",
                "Joe Fenton",
                "Aaron Cohen",
                "Rachel Bernstein",
                "Ray Kurzweil",
                "Blaise AgueraArcas",
                "Claire Cui",
                "Marian Croak",
                "Ed Chi",
                "Quoc Le"
            ],
            "title": "Lamda: Language models for dialog",
            "year": 2022
        },
        {
            "authors": [
                "Jie Tang",
                "Dawn Song"
            ],
            "title": "2022a. DeepStruct: Pre",
            "year": 2022
        },
        {
            "authors": [
                "Sameer Singh"
            ],
            "title": "Calibrate before use: Improv",
            "year": 2021
        },
        {
            "authors": [
                "PMLR. Zexuan Zhong",
                "Danqi Chen"
            ],
            "title": "A frustratingly",
            "venue": "Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Liu Zhuang",
                "Lin Wayne",
                "Shi Ya",
                "Zhao Jun."
            ],
            "title": "A robustly optimized BERT pre-training approach with post-training",
            "venue": "Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 1218\u20131227, Huhhot, China. Chinese Informa-",
            "year": 2021
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2022b) fine-tunes RoBERTa-large on two natural language inference (NLI) datasets: SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "In this paper, we propose GPT-RE to successfully address the aforementioned issues by (1) incorporating task-aware representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets and observe that GPTRE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines as in Figure 1. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets.\nAdditionally, a critical issue of LLMs revealed by previous work, the strong inclination to wrongly classify NULL examples into other predefined labels, is substantially alleviated by our method. We show an empirical analysis.1"
        },
        {
            "heading": "1 Introduction",
            "text": "The emergence of large language models (LLMs) such as GPT-3 (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2022; Rae et al., 2021; Hoffmann et al., 2022) represents a significant advancement in natural language processing (NLP). Instead of following a pretraining-and-finetuning pipeline (Devlin et al., 2019; Beltagy et al., 2019; Raffel et al., 2019; Lan et al., 2019; Zhuang et al., 2021), which finetunes a pre-trained model on\n1Codes will be released after the anonymous period.\na task-specific dataset in a fully-supervised manner, LLMs employ a new paradigm known as incontext learning (ICL) (Brown et al., 2020; Min et al., 2022a) which formulates an NLP task under the paradigm of language generation and makes predictions by learning from a few demonstrations. Under the framework of ICL, LLMs achieve remarkable performance rivaling previous fullysupervised methods even with only a limited number of demonstrations provided in various tasks such as solving math problems, commonsense reasoning, text classification, fact retrieval, natural language inference, and semantic parsing (Brown et al., 2020; Min et al., 2022b; Zhao et al., 2021; Liu et al., 2022b; Shin et al., 2021).\nDespite the overall promising performance of LLMs, the utilization of ICL for relation extraction (RE) is still suboptimal. RE is the central task for knowledge retrieval requiring a deep understanding of natural language, which seeks to identify a predefined relation between a specific entity pair mentioned in the input sentence or NULL if no relation is found. Given a test input, ICL for RE prompts the input of LLMs with the task instruction, a few demonstrations retrieved from the training data,\nand the test input itself. Then LLMs generate the corresponding relation. Recent research (Guti\u00e9rrez et al., 2022) has sought to apply GPT-3 ICL to biomedical RE, but the results are relatively negative and suggest that GPT-3 ICL still significantly underperforms fine-tuned models.\nThe reasons that cause the pitfall of GPT-3 ICL in RE are two folds: (1) The low relevance regarding entity and relation in the retrieved demonstrations for ICL. Demonstrations are selected randomly or via k-nearest neighbor (kNN) search based on sentence embedding (Liu et al., 2022b; Guti\u00e9rrez et al., 2022). Regrettably, kNN-retrieval based on sentence embedding is more concerned with the relevance of the overall sentence semantics and not as much with the specific entities and relations it contains, which leads to low-quality demonstrations. As shown in Figure 2, the test input retrieves a semantically similar sentence but is not desired in terms of entities and relations.\n(2) The lack of explaining input-label mappings in demonstrations leads to poor ICL effectiveness: A vanilla form of ICL lists all demonstrations as input-label pairs without any explanations. This may mislead LLMs to learn shallow clues from surface words, while a relation can be presented in diverse forms due to language complexity. Especially when ICL has a maximal input length, optimizing the learning efficiency of each single demonstration becomes extremely important.\nTo this end, we propose GPT-RE for the RE task. GPT-RE employs two strategies to resolve the issues above: (1) task-aware retrieval and (2) gold label-induced reasoning. For (1) task-aware retrieval, its core is to use representations that deliberately encode and emphasize entity and relation information rather than sentence embedding for kNN search. We achieve this by two different retrieval approaches: (a) entity-prompted sentence embedding; (b) fine-tuned relation representation, which naturally places emphasis on entities and\nrelations. Both methods contain more RE-specific information than sentence semantics, thus effectively addressing the problem of low relevance.\nFor (2) gold label-induced reasoning, we propose to inject the reasoning logic into the demonstration to provide more evidence to align an input and the label, a strategy akin to the Chain-ofThought (CoT) research (Wei et al., 2022; Wang et al., 2022b; Kojima et al., 2022). But different from previous work, we allow LLMs to elicit the reasoning process to explain not only why a given sentence should be classified under a particular label but also why a NULL example should not be assigned to any of the pre-defined categories. This process significantly improves the ability of LLMs to align the relations with diverse expression forms.\nRecent work reveals another crucial problem named \u201coverpredicting\u201d as shown in Figure 3: we observe that LLMs have the strong inclination to wrongly classify NULL examples into other predefined labels . A similar phenomenon has also been observed in other tasks such as NER (Guti\u00e9rrez et al., 2022; Blevins et al., 2022). In this paper, we show that this issue can be alleviated if the representations for retrieval can be supervised with the whole set of NULL in the training data.\nWe evaluate our proposed method on three popular general domain RE datasets: Semeval 2010 task 8, TACRED and ACE05, and one scientific domain dataset SciERC. We observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets."
        },
        {
            "heading": "I will predict the relation between two entities given the context.",
            "text": ""
        },
        {
            "heading": "2 Methodology: GPT-RE",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "Let C denote the input context and esub \u2208 C, eobj \u2208 C denote the pair of subject and object entity. Given a set of pre-defined relation classes R, relation extraction aims to predict the relation y \u2208 R between the pair of entities (esub, eobj) within the context C, or if there is no pre-defined relation between them, predict y = NULL."
        },
        {
            "heading": "2.2 Overview",
            "text": "We will first introduce the prompt construction to formalize RE as a language generation task in Sec. 2.3. Then to improve the ICL framework for RE, we will introduce two modules: (1) task-aware demonstration retrieval to select higherquality demonstrations (Sec. 2.4); (2) gold labelinduced reasoning to enrich each demonstration with explanations (Sec. 2.5). In Figure 4, we show the concrete workflow of processing a test input."
        },
        {
            "heading": "2.3 Prompt Construction",
            "text": "We construct a prompt for each given test example, which is fed to the GPT-3 model. Each prompt consists of the following components:\nInstructions I We provide a succinct overview of the RE task description and the set of pre-defined classes R. The model is explicitly asked to output the relation, which belongs to the pre-defined classes. Otherwise, the model will output NULL.\nICL Demonstrations D We first leverage a taskaware retriever to acquire a k-shot demonstration set, then enrich each demonstration (xi, yi) with the gold label-induced reasoning ri to build a new set of (xi, yi, ri) as D.\nTest Input xtest Similar to the demonstrations, we offer the test input xtest, and GPT-3 is expected to generate the corresponding relation ytest.\nIn summary, GPT-RE can be formulated as:\np (ytest \u2208 R \u222a {NULL}|I,D, xtest) (1)"
        },
        {
            "heading": "2.4 Task-aware Demonstration Retrieval",
            "text": "Since ICL demonstrations closer to the test sample in the embedding space result in more consistent and robust performance (Liu et al., 2022b). Recent work (Guti\u00e9rrez et al., 2022; Liu et al., 2022b) employs the kNN to retrieve the most similar examples in the training set as the few-shot demonstrations for each test input. As kNN relies on the choice of the embedding space to encode both test input and examples in the training set, they propose to obtain sentence embedding using pre-trained language models, or other improved sentence embedding.\nHowever, using sentence embedding for kNN retrieval has a severe drawback: relation extraction focuses on pair-wise entities, which diverge from the semantic meaning of the entire sentence, leading to an ambiguous retrieval using sentence embedding. In this study, we propose two novel\nmethods to provide more robust representations for better retrieval quality: (1) a naive entity-prompted sentence embedding in Sec. 2.4.1; (2) an advanced fine-tuned relation representation in Sec. 2.4.2."
        },
        {
            "heading": "2.4.1 Entity-Prompted Sentence Embedding",
            "text": "Given the discrepancy between sentence embedding and relation extraction, the original context is insufficient for demonstration retrieval. Considering the importance of entity information in RE, we propose reconstructing the context by incorporating entity pair information. For example, given the context \u201cHe has a sister Lisa,\u201d the reconstructed context with the entity prompted will be \u201cThe relation between \u2018He\u2019 and \u2018Lisa\u2019 in the context: He has a sister Lisa.\u201d This approach preserves both the semantic meaning of the sentence and the entity pair-centered information during retrieval. In the paper, we employ the latest robust model SimCSE (Gao et al., 2021) for computing sentence embedding-based similarity."
        },
        {
            "heading": "2.4.2 Fine-tuned Relation Representation",
            "text": "Compared to prompt entity information into context sentences, a more straightforward solution is to extract the relation representation from a fine-tuned RE model for retrieving demonstrations.\nCurrent BERT-based fine-tuning methods for RE (Baldini Soares et al., 2019; Zhong and Chen, 2021; Wan et al., 2022) attempts to capture both the context information and the entity information by adding extra marker tokens to highlight the subject and object entities and their types. Specifically, given an example: \u201cHe has a sister Lisa.\u201d, the input tokens are \u201c[CLS] [SUB_PER] He [/SUB_PER] has a sister [OBJ_PER] Lisa [/OBJ_PER]. [SEP]\u201d where \u201cPER\u201d is the entity type if provided. Denote the n-th hidden representation of the BERT encoder as hn. Assuming i and j are the indices of two beginning entity markers [SUB_PER] and [OBJ_PER], we define the relation representation as Rel = hi \u2295 hj where \u2295 stands for concatenation of representations in the first dimension. Subsequently, this representation is fed into a feedforward network for predicting the relation probability p(y \u2208 R \u222a {NULL} | Rel).\nThe entity markers have explicitly encoded subject and object entities and the relation representation Rel is naturally enriched with the entity information. We believe this approach can potentially compensate for the limitations of GPT-3 in RE. While GPT-3 ICL has a constraint of limited\ndemonstrations, the fine-tuning process is unbundled and can be done on the whole train data. It has two subsequent merits. First, the relation representations are directly fine-tuned to fit the RE task, which could significantly boost the overall retrieval quality. Second, the overpredicting NULL issue will be substantially alleviated because the similar NULL demonstrated can be accurately recognized by the fine-tuned model."
        },
        {
            "heading": "2.5 Gold Label-induced Reasoning",
            "text": "Recent CoT work has reported significant progress in the commonsense and numerical reasoning tasks by automatically eliciting the reasoning steps for solving a question. While in the RE task, two entities can possibly hold multiple relations, e.g., \u201cJoe Biden\u201d can be either the president of or lives in \u201cU.S.\u201d. The reasoning generation could be out of focus if it lacks interaction with the gold label.\nIn this section, we propose to let GPT-3 induce the reasoning logic for each demonstration by the corresponding gold relation label. As shown in Figure 5, given a selected demonstration, we first generate a query prompt \u201cWhat are the clues that lead to the relation between [entity1] and [entity2] to be [relation] in the sentence [context]?\u201d based on the demonstration and subsequently ask GPT-3 to generate clues \u201cIt is because: ...\u201d on the labeled relation between the pair of entities in the context. Finally, we augment the demonstration by incorporating the generated clues induced by GPT-3."
        },
        {
            "heading": "3 Experiment Setup",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "We evaluate on three popular general domain RE datasets and one scientific domain dataset. Due to the cost of running the model in the API with GPT-3, in our main results, we sample a subset (See Appendix C) from the original test set for two datasets: ACE05 and TACRED as shown in Table 1.\nSemeval 2010 task 8 Hendrickx et al. (2010) focuses on semantic relations between pairs of nominals collected from general domain resources.\nTACRED Zhang et al. (2017) is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text.\nSciERC Luan et al. (2018) collects AI paper abstracts and annotated relations, especially for scientific knowledge graph construction.\nACE05 contains the entity, relation, and event annotations collected from domains including newswire, broadcast, discussion forums, etc."
        },
        {
            "heading": "3.2 Baseline Methods",
            "text": "GPT-3 baselines For GPT-3 baselines and our methods, we select \u201ctext-davinci-003\u201d with maximal 4,097 input tokens and use the identical\nprompt construction (Sec. 2.3) via OpenAI API. We implement two categories of GPT-3 baselines:\n(1) GPT-Random Instead of randomly selecting few-shot demonstrations from the training data for each test input, we add extra constraints to make the label distribution of selected demonstrations more uniform. Our preliminary experiments suggest that this is a stronger baseline than the vanilla random.\n(2) GPT-Sent Previous work attempts various sentence embedding in retrieval. In this work, our implementation adopted SimCSE (Gao et al., 2021), which has been demonstrated to be the state-of-theart method for sentence similarity tasks.\nFine-tuned RE Models In our experiment, we choose PURE (Zhong and Chen, 2021), an entity marker-based fine-tuned model mentioned in Sec. 2.4.2 to obtain the representations for retrieval. Meanwhile, PURE performs as a directly comparable baseline. We also compare with corresponding SOTA fine-tuned baselines on Semeval Cohen et al. (2020) (reformulate RE as the question answering task) and TACRED Wang et al. (2022a) (extra pretraining to capture RE structure) datasets.\nAll implementation details are in Appendix A."
        },
        {
            "heading": "4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.1 Main Results",
            "text": "We compare our main experiment results with previous methods in Table 2. GPT-RE_SimCSE denotes our entity-prompted sentence embedding for retrieval and GPT-RE_FT denotes our fine-tuned relation representation for retrieval. From the table, we can observe that: (1) both GPT-RE_SimCSE and GPT-RE_FT outperform the retrieval-based GPTSent, indicating that it is necessary to inject the taskspecific information into sentence embedding for selecting proper demonstrations; (2) GPT-RE_FT succeeds to outperform the fine-tuning baseline PURE on three datasets by +2.00, +2.42, +0.55 Micro-F1. It suggests that GPT-3 has the potential to beat fine-tuning when the retriever has prior task knowledge. GPT-RE_FT eventually achieves SOTA results on Semeval and SciERC. (3) reasoning module improves GPT-RE_SimCSE by around 2% Micro-F1, indicating that gold label-induced reasoning successfully enriches the knowledge\nof demonstrations. Meanwhile, the high-quality demonstrations obtained by GPT-RE_FT offset the effort of enriching reasoning into demonstrations, which shows relatively trivial improvements. Since reasoning aims at enriching demonstrations, this feature potentially works better with fewer demonstrations, as shown in Section 4.3."
        },
        {
            "heading": "4.2 Ablation Study on Task-aware Retrieval",
            "text": "We first implement the ablation experiments of the retrieval component with the setting of increasing k-shot demonstrations (Figure 6a). We find that: (1) compared to GPT-Random, all the retrievalbased models have higher F1 scores and large gradients of the performance curves. It means that GPT-3 can learn from high-quality demonstrations more effectively; (2) after adding entity information to the SimCSE retrieval, GPT-RE_SimCSE achieves better performance throughout all K shots, indicating that task-aware sentence embedding can capture the feature of RE and provide more proper demonstrations; (3) finally, the fine-tuned relation representation retriever GPT-RE_FT significantly outperforms all retrieval-based methods and beats the fine-tuning baseline when k > 15. Note that even with k = 5 demonstrations, GPT-RE_FT still works better than GPT-RE_SimCSE with k = 30 (80.30 \u2212\u2192 83.43(+3.13)), which indicates that the quality of demonstrations shows much more important than the number of demonstrations."
        },
        {
            "heading": "4.3 Ablation Study on Reasoning Enhancing",
            "text": "We then check the influence of our proposed reasoning-enhanced demonstration, as shown in Figure 6b. Due to the limited amount of input tokens of GPT-3, we have to set the k \u2264 15 for the\ntokens of reasoning, leading to a trade-off between adding reasoning and adding more demonstrations. From the result, we find that: (1) with reasoningenhanced demonstrations, GPT-3 always achieves better scores across all the k-shot settings of both GPT-RE_SimCSE and GPT-RE_FT, indicating that the reasoning induced from ground truth relation labels can effectively unlock the reasoning ability of GPT-3 and improve the ICL with a deeper understanding of demonstrations. Specifically, for GPTRE_FT, the performance improvement becomes less significant when more demonstrations are provided, which is feasible as with more high-quality demonstrations available, GPT-3 can already learn the internal reasoning behind each demonstration; (2) since the reasoning enhancement works better with fewer demonstrations, we expect this method can be an effective solution to low-shot relation extraction (Han et al., 2018; Geng et al., 2020; Liu et al., 2022a), which aims at recognizing novel relations with very few or no examples, and we leave this for future work."
        },
        {
            "heading": "4.4 Low-resource Scenario",
            "text": "We conduct the experiment for observing the lowresource performance in the general domain Semeval task. As shown in Figure 7, we observe that: (1) all the GPT-3 based results work better than fine-tuning in when the training examples are less than # 650 (10%). It indicates that in the general domain RE, GPT-3 benefits from its abundant prior knowledge to understand the relations; (2) GPT-RE_SimCSE starts to show a substantial difference to GPT-Sent after the training size surpasses 30%. We believe fewer training candidates\ncould limit the effects of retrieval; (3) GPT-RE_FT achieves an upper bound performance in all settings, even when the fine-tuned model shows poor performance with hundreds of training data (from #100 to #400). This emphasizes the impressive effectiveness of fine-tuned relation representations for capturing higher-quality demonstrations. The observation in the low-resource setting is very different from Guti\u00e9rrez et al. (2022). We assume the difference could be caused by the domain and NULL proportion of the task."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 The Issue of \u201cOverpredicting\u201d",
            "text": "To analyze the influence of NULL class, we compare the effectiveness of each method for alleviating this issue on two datasets: general domain Semeval with 17.4% NULL examples and scientific domain SciERC with 90.16% NULL examples. As shown in Figure 8, (1) by comparing the performance on Semeval and SciERC, a larger percentage of NULL examples results in more significant performance drop showing the negative influence of overpredicting NULL examples; (2) by comparing w/o NULL and w/ NULL, our GPT-RE_FT shows the most robustness to the influence of NULL examples, indicating that the RE fine-tuned representations in retrieval can release the overpredicting issue of GPT-3 by providing higher-quality demonstrations; (3) however, even with task-aware representations, all GPT-3 methods still underperform the fine-tuning baseline on NULL examples, this is due to the confusing definition of NULL, in many cases, there is a certain relation between entities in the context, but out of the distribution of pre-\ndefined classes. In these cases, GPT-3 tends to overpredict as the relation information may be covered in its prior knowledge. We think this ability of GPT-3 can be useful in more open fields, such as open RE (Banko and Etzioni, 2008) which has no pre-defined relation classes."
        },
        {
            "heading": "5.2 Case Study of Demonstration Quality",
            "text": "We select one typical test example to better illustrate the amendment of our task-aware demonstration retrieval. As shown in Figure 9, given the NULL Example, we show the most similar demonstration in retrieval based on three methods. The GPT-Sent retrieved demonstration focuses on the semantic meaning of \u201cCONTENT AND CONTAINER\u201d which is shared in the test context, but not revealed in the target entity pair. This mismatch confirms the problem of lacking entity information in retrieval. Instead, GPT-RE_SimCSE retrieves a much more relevant demonstration that shows the same semantic relation between \u201ccatch\u201d and \u201cfish\u201d but still faces a minor mismatch as the gold label is between \u201ccatch\u201d and \u201cscuttle.\u201d Finally, GPTRE_FT demonstration shares a similar structure with the test input regarding the pair of entities, which is the key clue for predicting the relation between entities. This result shows a level-bylevel enhancement with more entity information provided in retrieval. We also show some other case examples in Appendix B."
        },
        {
            "heading": "6 Related Work",
            "text": "In-context Learning Recent work shows that ICL of GPT-3 (Brown et al., 2020) can perform numerous tasks when provided a few examples in a natural language prompt. Existing work focuses on various aspects to effectively utilize the advantages of GPT-3, from prompt design (Perez et al., 2021) for proper input to coherence calibration (Malkin et al., 2022) for tackling the diverse generated output. Another research path locates in the demonstration part, including ordered prompts (Lu et al., 2022) and retrieval-based demonstrations (Rubin et al., 2022; Liu et al., 2022b; Shin et al., 2021).\nTo the best of our knowledge, there is no previous work exploring the potential of GPT-3 on general domain RE tasks. A recent work attempts to leverage GPT-3 in biomedical information extraction (NER and RE), and reveals issues of ICL that may be detrimental to IE tasks in general. Our work succeeds in overcoming these issues to some extent and confirms the potential of GPT-3 in both general and the scientific domain RE.\nRetrieval-based Demonstrations Several studies have demonstrated that dynamically selecting few-shot demonstrations for each test example, instead of utilizing a fixed set, leads to significant improvement in GPT-3 ICL (Liu et al., 2022b; Shin et al., 2021; Rubin et al., 2022). They also show that nearest neighbor in-context examples yield much better results than the farthest ones. This leads to the significance of better retrieval modules for demonstrations. Existing attempts rely on sentence embedding in retrieval, including the sentence encoders of PLMs such as BERT (Devlin et al., 2019), RoBERTa (Zhuang et al., 2021) KATE (Liu et al., 2022b) , SimCSE (Gao et al., 2021), Sentence-BERT (Reimers and Gurevych, 2019; Wolf et al., 2020). Unlike these sentence embeddings, we propose to fine-tune PLMs on our target RE tasks to produce more task-specific and robust representations for retrieval."
        },
        {
            "heading": "7 Conclusions",
            "text": "This work explores the potential of GPT-3 ICL on RE for bridging the performance gap to the fine-tuning baselines via two strategies: (1) taskaware demonstration retrieval emphasizes entity and relation information for improving the accuracy of searching demonstrations; (2) gold labelinduced reasoning enriches the reasoning evidence\nof each demonstration. To the best of our knowledge, GPT-RE is the first GPT-3 ICL research that significantly outperforms the fine-tuning baseline on three datasets and achieves SOTA on Semeval and SciERC. We implement detailed studies to explore how GPT-3 overcomes the difficulties such as NULL example influence.\nLimitations\nDespite the overall positive results, GPT-RE still faces two shortcomings: (1) the issue of overpredicting has been significantly alleviated but not completely solved, and the NULL recall still lags behind full-supervised baselines, especially on the datasets containing a large proportion of NULL examples such as ACE05 (\u201c95.60%\u201d); (2) Though the task-aware retriever optimizes the representations of PLMs such as SimCSE and BERT, it is widely considered that LLMs can generate more robust representations than small PLMs. Future work can replace representations generated by smaller PLMs with GPT-3 itself. However, due to the access limitation to the representations of GPT-3, we can nearly confirm this proposal up to now."
        },
        {
            "heading": "A Hyperparameters",
            "text": "A.1 GPT-3 Hyperparameters\nWe use the GPT-3 API during the experiments and set the hyperparameters as in Table 3. Since the \u201cTemperature\u201d is set to be 0.0, denoting the stable output of GPT-3, we report the result of the single run for all experiments. Due to the input length limitation of GPT-3 and the various average lengths of contexts from each dataset, we set different search ranges for the number of demonstrations of each dataset as shown in Table 4.\nA.2 Fine-tuning Baseline PURE\nWe follow their single-sentence setup to keep consistency among datasets as Semeval and TACRED are both sentence-level RE datasets. For the PLMs, we also follow PURE by using scibert-scivocabuncased (Beltagy et al., 2019) as the base encoder for SciERC and bert-base-uncased (Devlin et al., 2019) for the remaining three general domain datasets. We follow hyperparameters in their paper. We used 2 NVIDIA RTX3090 for training.\nA.3 Sentence Embedding Methods\nGuti\u00e9rrez et al. (2022) uses the [CLS] of RoBERTalarge as the representation in retrieval, Liu et al. (2022b) fine-tunes RoBERTa-large on two natural language inference (NLI) datasets: SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) to enhance the quality of sentence embedding. For the sentence embedding method SimCSE in our experiment, we utilize the version: sup-simcse-bert-base-uncased.\nContext: this paper describes a set of principles designed to help archives position themselves to address the management ...... Given the context, the relation between principles and set is MEMBER AND COLLECTION.\nGPT-Sent Demonstration\nContext: basic diagrams also work well on the computer screen if they are carefully designed to match the grid of pixels on the screen. Given the context, the relation between screen and computer is [COMPONENT AND WHOLE]\nTest Input\nContext: the screen works using ink , just like books and newspapers , but displays the ink particles electronically . Given the context, the relation between ink and screen is COMPONENT AND WHOLE.\nGPT-RE_SimCSE Demonstration\nContext: the computer mouse has been the input device of choice for a long time now in the computer world . Given the context, the relation between mouse and computer is COMPONENT AND WHOLE.\nGPT-RE_FT Demonstration\nRetrieval GPT-Sent GPT-RE_SimCSE GPT-RE_FT\nGPT-3 Output MEMBER AND COLLECTION COMPONENT AND WHOLE COMPONENT AND WHOLE\n(a) [COMPONENT AND WHOLE] denotes the gold label\nContext: a woman diagnosed with breast cancer today joins a huge sisterhood of cancer survivors ready to help her along the way \u2026\u2026 Given the context, the relation between survivors and sisterhood is MEMBER AND COLLECTION.\nGPT-Sent Demonstration\nContext: the wheelchair foundation donated wheelchairs to people with physical problems in hundred countries . Given the context, the relation between wheelchairs and people is [ENTITY AND DESTINATION].\nTest Input\nContext: the victim of last night 's car accident donated his organs to several patients who have been waiting for donated organs . Given the context, the relation between organs and patients is ENTITY AND DESTINATION.\nGPT-RE_SimCSE Demonstration\nContext: operation homefront and partners delivered toys to military children . Given the context, the relation between toys and children is ENTITY AND DESTINATION.\nGPT-RE_FT Demonstration\nRetrieval GPT-Sent GPT-RE_SimCSE GPT-RE_FT\nGPT-3 Output MEMBER AND COLLECTION ENTITY AND DESTINATION\nENTITY AND DESTINATION\n(b) [ENTITY AND DESTINATION] denotes the gold label.\nFigure 10: More casees."
        },
        {
            "heading": "B Case Study",
            "text": "To verify the effectiveness of our task-aware demonstration retrieval, we provide more cases.\nFor Figure 10a, GPT-Sent retrieves a demonstration that shares the same semantic meaning of \u201cdesign\u201d with the test input. However, the entity pair is irrelevant to the concept \u201cdesign\u201d resulting in a noisy demonstration. Instead, GPT-RE_SimCSE retrieves a more relative demonstration with closer pair of entities sharing the same relation label. Furthermore, GPT-RE_FT retrieves the demonstration containing both the closing entity pair and the same linguistic structure between entities. This case emphasizes level-by-level improvement using our proposed methods. Figure 10b shows a similar phenomenon."
        },
        {
            "heading": "C Subset",
            "text": "The number of sampled examples is not only related to the size of the training data itself. A more important factor is the proportion of NULL. We have to maintain the original label distribution in datasets with a high proportion of NULL. Thus, the rule to sample the subset is to keep the proportion of each relation label consistent with the original test set. Table 5 6 are label distributions of two subsets.\nGPT-RE_FT on TACRED surpasses the supervised baseline in the current subset. As we show above, some labels in TACRED are indeed not well presented (only 1 example), since TACRED dataset contains some long-tail labels. We decided to add additional results of GPT-RE_FT by enlarging our sampled set to # 3200 (2 times the current version), and the performance of GPT-RE_FT (k = 15) is 73.16 while the performance of PURE is 70.48."
        }
    ],
    "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
    "year": 2023
}