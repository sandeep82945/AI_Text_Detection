{
    "abstractText": "Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the learning of the current task and replayed tasks. With extensive experiments, we demonstrate that DMEA can consistently outperform existing methods in different LSG settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chengwei Qin"
        },
        {
            "affiliations": [],
            "name": "Chen Chen"
        },
        {
            "affiliations": [],
            "name": "Shafiq Joty"
        },
        {
            "affiliations": [],
            "name": "Kathleen McKeown"
        }
    ],
    "id": "SP:998b2b48be0388d62ba2f014be19480f455ff345",
    "references": [
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marc\u2019Aurelio Ranzato",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny"
            ],
            "title": "Efficient lifelong learning with A-GEM",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Tianqi Chen",
                "Ian J. Goodfellow",
                "Jonathon Shlens."
            ],
            "title": "Net2net: Accelerating learning via knowledge transfer",
            "venue": "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.",
            "year": 2016
        },
        {
            "authors": [
                "Yung-Sung Chuang",
                "Shang-Yu Su",
                "Yun-Nung Chen."
            ],
            "title": "Lifelong language knowledge distillation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2914\u20132924, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Wafaa S El-Kassas",
                "Cherif R Salama",
                "Ahmed A Rafea",
                "Hoda K Mohamed."
            ],
            "title": "Automatic text summarization: A comprehensive survey",
            "venue": "Expert Systems with Applications, 165:113679.",
            "year": 2021
        },
        {
            "authors": [
                "Chrisantha Fernando",
                "Dylan Banarse",
                "Charles Blundell",
                "Yori Zwols",
                "David Ha",
                "Andrei A Rusu",
                "Alexander Pritzel",
                "Daan Wierstra."
            ],
            "title": "Pathnet: Evolution channels gradient descent in super neural networks",
            "venue": "arXiv preprint arXiv:1701.08734.",
            "year": 2017
        },
        {
            "authors": [
                "Donghoon Ham",
                "Jeong-Gwan Lee",
                "Youngsoo Jang",
                "Kee-Eung Kim."
            ],
            "title": "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 583\u2013592,",
            "year": 2020
        },
        {
            "authors": [
                "Xu Han",
                "Yi Dai",
                "Tianyu Gao",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "Continual relation learning via episodic memory activation and reconsolidation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Luheng He",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Zixuan Ke",
                "Bing Liu",
                "Xingchang Huang."
            ],
            "title": "Continual learning of a mixed sequence of similar and dissimilar tasks",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 18493\u2013 18504. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Zixuan Ke",
                "Bing Liu",
                "Nianzu Ma",
                "Hu Xu",
                "Lei Shu."
            ],
            "title": "Achieving forgetting prevention and knowledge transfer in continual learning",
            "venue": "Advances in Neural Information Processing Systems, 34:22443\u2013 22456.",
            "year": 2021
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Brenden M. Lake",
                "Tomer D. Ullman",
                "Joshua B. Tenenbaum",
                "Samuel J. Gershman."
            ],
            "title": "Building machines that learn and think like people",
            "venue": "Behavioral and Brain Sciences, 40:e253.",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Zhizhong Li",
                "Derek Hoiem."
            ],
            "title": "Learning without forgetting",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947.",
            "year": 2017
        },
        {
            "authors": [
                "Sen Lin",
                "Li Yang",
                "Deliang Fan",
                "Junshan Zhang."
            ],
            "title": "Beyond not-forgetting: Continual learning with backward knowledge transfer",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Sen Lin",
                "Li Yang",
                "Deliang Fan",
                "Junshan Zhang."
            ],
            "title": "TRGP: Trust region gradient projection for continual learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang."
            ],
            "title": "DARTS: Differentiable architecture search",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Andrea Madotto",
                "Zhaojiang Lin",
                "Zhenpeng Zhou",
                "Seungwhan Moon",
                "Paul Crook",
                "Bing Liu",
                "Zhou Yu",
                "Eunjoon Cho",
                "Pascale Fung",
                "Zhiguang Wang."
            ],
            "title": "Continual learning in task-oriented dialogue systems",
            "venue": "Proceedings of the 2021 Conference on Empiri-",
            "year": 2021
        },
        {
            "authors": [
                "Saeed Masoudnia",
                "Reza Ebrahimpour."
            ],
            "title": "Mixture of experts: a literature survey",
            "venue": "Artificial Intelligence Review, 42(2):275\u2013293.",
            "year": 2014
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen"
            ],
            "title": "Catastrophic interference in connectionist networks",
            "year": 1989
        },
        {
            "authors": [
                "Fei Mi",
                "Liangwei Chen",
                "Mengjie Zhao",
                "Minlie Huang",
                "Boi Faltings."
            ],
            "title": "Continual learning for natural language generation in task-oriented dialog systems",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3461\u20133474,",
            "year": 2020
        },
        {
            "authors": [
                "Jekaterina Novikova",
                "Ond\u0159ej Du\u0161ek",
                "Verena Rieser."
            ],
            "title": "The E2E dataset: New challenges for endto-end generation",
            "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201\u2013206, Saarbr\u00fccken, Germany. Association",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterHub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Chengwei Qin",
                "Shafiq Joty."
            ],
            "title": "Continual fewshot relation learning via embedding space regularization and data augmentation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Chengwei Qin",
                "Shafiq Joty."
            ],
            "title": "LFPT5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Alexander Kolesnikov",
                "Georg Sperl",
                "Christoph H. Lampert."
            ],
            "title": "icarl: Incremental classifier and representation learning",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "Kim"
            ],
            "title": "Continual learning with deep generative",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Potts"
            ],
            "title": "Recursive deep models for",
            "year": 2013
        },
        {
            "authors": [
                "Qingwei Zhao"
            ],
            "title": "Metacognitive adaptation",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Friedemann Zenke",
                "Ben Poole",
                "Surya Ganguli."
            ],
            "title": "Continual learning through synaptic intelligence",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of",
            "year": 2017
        },
        {
            "authors": [
                "Yanzhe Zhang",
                "Xuezhi Wang",
                "Diyi Yang."
            ],
            "title": "Continual sequence generation with adaptive compositional modules",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3653\u20133667,",
            "year": 2022
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "venue": "arXiv preprint arXiv:1709.00103.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the recent advancements in pre-trained language models (LMs), current sequence generation methods have achieved impressive performance on a variety of generation tasks (Radford et al., 2019; Raffel et al., 2020). Typically, these models are trained on a fixed corpus, assuming the underlying data distribution to be static (Ham et al., 2020; El-Kassas et al., 2021). However, real cognitive tasks are generally more complex involving changing contexts and dynamic environments. The ever-changing data distribution causes the models to face challenges in acquiring new knowledge, while retaining the prior knowledge. Speaking about what is next for NLP, Kathleen McKeown\nin a recent interview said: \u201cMost models are static. But the world changes every minute, every second. Dealing with a dynamic world is a new area that\u2019s up and coming.\u201d (Source)\nA potential solution is to formalize sequence generation as lifelong sequence generation or LSG (Sun et al., 2020), where the model is expected to learn sequentially from a stream of generation tasks with potentially different data distributions. In such cases of distribution shift, the model might forget previously acquired knowledge upon learning new tasks, a phenomenon known as catastrophic forgetting (McCloskey and Cohen, 1989). Previous LSG methods (Mi et al., 2020; Sun et al., 2020; Madotto et al., 2021) mainly explore different ways to alleviate forgetting. Recently, Zhang et al. (2022) propose Adaptive Compositional Modules (ACM) which dynamically adds modules for new tasks depending on whether there are reusable previous modules, achieving SOTA performance on LSG.\nDespite its effectiveness, ACM has several key limitations. First, it mainly focuses on mitigating forgetting of previously acquired knowledge while paying little attention to transferring learned knowledge to new tasks which is as important for continual learning as preventing forgetting (Ke et al., 2020). In fact, a hallmark of human intelligence is that humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks (Lake et al., 2017). They can not only determine whether previously acquired skills are sufficient to solve a new task, but also exploit the most similar learned skills to facilitate the learning of the task; see Appendix A.1 for an illustration. Second, ACM does not consider the correlation between learned tasks and the new task when adding modules, which might hinder finding the optimal architecture (case study in Appendix A.9). Finally, the learning process in ACM can be biased towards the new task as the gradient norm of the new task on reused modules is typically much larger than\nthat of replayed tasks, which may affect previously acquired knowledge; see Appendix A.2 for an explanation.\nInspired by the learning paradigm of humans and to address the above limitations of ACM, in this work we propose Dynamic Module1 Expansion and Adaptation (DMEA). We divide the learning process of a new task into three stages: expansion, selection and adaptation. In the expansion stage, DMEA determines whether to reuse modules of previous tasks or insert new modules for learning novel knowledge. Inspired by Zhang et al. (2022), it utilizes differentiable architecture search (Liu et al., 2019) to enable the model to dynamically determine the architecture for solving the new task. The learnable coefficients in architecture search are initialized based on the cosine similarity of word frequency distributions between learned tasks and the new task, aiming to discover the optimal model architecture. After searching, the module with the largest coefficient in every layer is chosen for the new task. In the selection stage, DMEA selects the top-K most similar previous tasks through input subspace (Lin et al., 2022b). Finally, in the adaptation stage, it utilizes the selected similar tasks to facilitate adaptation to the new task. The output of selected similar tasks is fused with that of the new task using learnable coefficients in every transformer layer to enable forward knowledge transfer. This is indeed an instance of mixture-of-experts (Masoudnia and Ebrahimpour, 2014).\nIn addition, when the model learns a new task, DMEA also incorporates pseudo-sample replay (Sun et al., 2020) to further mitigate catastrophic forgetting. To address the \u201cbias to the new task\u201d in the gradient update, we introduce dynamic gradient scaling to balance the learning of the new task and replayed tasks. To verify the effectiveness of DMEA, we conduct extensive experiments on various generation tasks in different LSG settings. The empirical results show that DMEA can consistently outperform previous state-of-the-art baselines.\nIn summary, our main contributions are:\n\u2022 To the best of our knowledge, we are the first to explore solving LSG from the perspective of human learning. We propose DMEA, a novel method based on dynamic module expansion and adaptation, to alleviate catastrophic forgetting and facilitate knowledge transfer in LSG.\n1Following Zhang et al. (2022), we use an Adapter (Houlsby et al., 2019) as the insertable module.\n\u2022 With extensive experiments and analysis, we demonstrate the effectiveness of our method compared to existing ones in different LSG settings."
        },
        {
            "heading": "2 Related Work",
            "text": "Lifelong Learning (LL) aims to continually learn knowledge from a sequence of tasks with different distributions. The goal is twofold: alleviate catastrophic forgetting (McCloskey and Cohen, 1989) of learned tasks, and facilitate knowledge transfer (Lopez-Paz and Ranzato, 2017) across tasks.\nCatastrophic forgetting typically means that the model forgets previously acquired knowledge after learning new tasks. Prior LL methods mainly focus on mitigating this problem and can be divided into three categories. First, regularizationbased methods constrain the update of parameters that are important to learned tasks to retain previous knowledge (Kirkpatrick et al., 2017; Li and Hoiem, 2017; Zenke et al., 2017; Ritter et al., 2018). Second, architecture-based methods dynamically adjust the model architecture to acquire new information while preventing the forgetting of previously learned tasks (Rusu et al., 2016; Chen et al., 2016; Fernando et al., 2017; Madotto et al., 2021; Zhang et al., 2022). Finally, memory-based methods keep a number of key samples from previous tasks in memory to alleviate forgetting (Rebuffi et al., 2017; Shin et al., 2017; Chaudhry et al., 2019; Qin and Joty, 2022a). The memory data can be either real examples (Han et al., 2020) or generated by language models (Sun et al., 2020; Qin and Joty, 2022b).\nMore recently, researchers have considered exploring knowledge transfer in LL, i.e., learning on a task can benefit from learning on another task by transferring related knowledge. This includes CTR (Ke et al., 2021) and CUBER (Lin et al., 2022a). Despite their effectiveness, these methods mainly focus on classification tasks, while generation tasks typically have more complex label space. Note that this line of research is different from transfer learning (Ruder et al., 2019), which mainly focuses on exploring better ways to reuse learned knowledge which is usually static, e.g., a frozen language model. In contrast, the acquired knowledge is continually accumulated in lifelong learning. Lifelong Sequence Generation (LSG) enables the model to learn sequentially from a stream of generation tasks. Sun et al. (2020) propose LAMOL which formalizes different types of tasks as ques-\ntion answering and utilizes pseudo-sample replay to alleviate forgetting. Chuang et al. (2020) further improve LAMOL by knowledge distillation (Hinton et al., 2015). AdapterCL (Madotto et al., 2021) inserts task-specific modules into every transformer layer to learn new tasks while keeping the pre-trained LM and previous modules frozen. On the basis of AdapterCL, Zhang et al. (2022) introduce ACM which dynamically adds modules for learning new tasks depending on whether there are reusable previously inserted modules. Though ACM can enable knowledge transfer to some extent via module sharing, there is no explicit mechanism to encourage knowledge transfer across tasks, a common phenomenon of human learning. Summary. Existing work in LSG mainly focuses on mitigating the catastrophic forgetting of previously learned knowledge while paying little attention to knowledge transfer across tasks. In contrast to these lines of work, we aim to explicitly encourage forward knowledge transfer in LSG inspired by the way humans learn (Lake et al., 2017)."
        },
        {
            "heading": "3 Problem Formulation",
            "text": "LSG involves learning from a stream of sequence generation tasks T = (T 1, ..., T n), where every task T i has its own training set Ditrain, validation set Divalid, and test set D i test. Every dataset D contains a set of examples {(Xj , Yj)}|D|j=1, where Xj and Yj denote the input and output texts, respectively. At time step k, the model is trained on the training set Dktrain of task T k and has no access to real samples of previously learned tasks.\nAfter the training on Dktrain, the model is expected to perform well on all the tasks learned so far, i.e., T 1, ..., T k, and will be evaluated on the test set Ditest of each task T i(1 \u2264 i \u2264 k) with corresponding evaluation metrics separately. Therefore, to achieve the goal of LSG, the model is required to alleviate the forgetting of acquired knowledge and better learn new patterns through possible forward knowledge transfer."
        },
        {
            "heading": "3.1 Data Format",
            "text": "Given an input-output text pair (X,Y ) for a task, the model learns to decode the output text Y after reading the input X . Following Zhang et al. (2022), a natural language question Q describing the purpose of each task (task instruction) is inserted after the input to form a triple (X,Q, Y ); see Appendix A.3 for an example. To learn a new\ntask, the model is optimized to decode Y given X and Q. Denoting the concatenation of X,Q and Y as A, the autoregressive training objective is:\nLtask = \u2212 n\u2211\nj=m+1\nlog p\u03b8(Aj |A<j) (1)\nwhere n is the total number of tokens in A and (A1, ..., Am) is the concatenation of X and Q, and \u03b8 denotes the model parameters."
        },
        {
            "heading": "4 Methodology",
            "text": "Inspired by how humans learn a new task (Fig. 5), DMEA divides the learning process into three stages. The expansion stage (\u00a74.1) first determines the model architecture dynamically. The selection stage (\u00a74.2) then selects the top-K most similar previous tasks which are utilized in the final adaptation stage (\u00a74.3) to facilitate adaptation to the new task. We also employ pseudo-sample replay along with a dynamic gradient scaling method to balance the learning of the new and replayed tasks."
        },
        {
            "heading": "4.1 Expansion Stage",
            "text": "Humans are able to determine whether previously acquired skills are sufficient to solve a new task. Our method DMEA aims to mimic this learning process in the expansion stage. It can dynamically decide whether to reuse modules of previous tasks or insert a new module in every transformer layer to learn novel knowledge. Inspired by Zhang et al. (2022), we utilize differentiable architecture search (Liu et al., 2019) to achieve this goal.\nSpecifically, assuming that there are k modules (i.e., Adapter (Houlsby et al., 2019)) {ml1, ...,mlk} in layer l of the transformer model before learning a new task T j , we temporarily insert a new module mlk+1 into this layer at the beginning of the expansion stage. For each forward pass, after calculating the output hlt of every module m l t in the layer separately, we fuse all outputs {hl1, ..., hlk+1} through learnable coefficients {\u03bbl1, ..., \u03bblk+1} as follows.\nh\u0302l = k+1\u2211 t=1 e\u03bb l t\u2211k+1 s=1 e \u03bbls hlt (2)\nThe weighted average h\u0302l is then passed to the next part of the model for learning. After training the model on Djtrain for several epochs using Ltrain (defined in \u00a74.3), we select the module with the largest coefficient in every layer for the new task T j .\nDifferent from Zhang et al. (2022) which initialize {\u03bbl1, ..., \u03bblk+1} with predefined hyperparameters, we propose to dynamically initialize learnable coefficients based on the correlation between the learned tasks T 1, ..., T j\u22121 and new task T j . Denoting the word frequency distribution of T i as f i and all previous tasks sharing the module mlt as Z lt , the learnable coefficient \u03bblt is initialized as:\n\u03bblt =  max T i\u2208Zlt cos(f i, fk+1), 1 \u2264 t \u2264 k\nmin 1\u2264i\u2264k\n\u03bbli, t = k + 1 (3)\nwhere cos is the cosine similarity function and f i is calculated based on the training set Ditrain. In this way, a previous module shared by tasks with higher word frequency distribution similarity to the new task has a larger initial coefficient, increasing the tendency to reuse it. In addition, the coefficient \u03bblk+1 of the newly added module m l k+1 is initialized to the minimum value of the initial coefficients {\u03bbl1, ..., \u03bblk} of previously added modules {ml1, ...,mlk} to encourage module reuse.\nThe selected module in layer l can be either from previous modules {ml1, ...,mlk} or the newly added one mlk+1 and will be tuned in the adaptation stage to accommodate new knowledge. We then discard newly added modules that are not selected. Note that only newly added modules and coefficients are learnable in the expansion stage; the pre-trained LM and previous modules are kept frozen."
        },
        {
            "heading": "4.2 Selection Stage",
            "text": "As humans, we can better acquire new knowledge by recognizing and utilizing knowledge from previously learned tasks that are similar (Lake et al., 2017). Based on the observation that the norm of one task\u2019s gradient projection onto the subspace of another task can characterize the correlation between them when the model architecture is static (Lin et al., 2022b), we further extend it to dynamic modules. Specifically, we obtain the input subspace of each task using modules of it and select the topK most similar previous tasks by input subspace similarity to facilitate adaptation to the new task T j . The model architecture induced from the expansion stage is used for selection and adaptation.\nSimilar to Lin et al. (2022b), we adopt Singular Value Decomposition (SVD) to obtain the input subspace of each task. After training the model on Djtrain for several epochs in the expansion stage, we randomly select n samples\n{X1, ..., Xn} from Djtrain and obtain their representations {X1, ...,Xn} \u2208 Rm by forwardpropagating them through the network. We use the final-layer representation of the last non-padding token in the input as the sample representation.\nAfter obtaining the representation matrix Rj = [X1, ...,Xn] \u2208 Rm\u00d7n for task T j , we apply SVD to Rj , i.e., Rj = U j\u03a3j(V j) \u2032 , where U j = [uj1, ...,u j m] \u2208 Rm\u00d7m is composed of left-singular vectors uji , \u03a3 j \u2208 Rm\u00d7n is a rectangular diagonal matrix with singular values on the diagonal, and V j = [vj1, ...,v j n] \u2208 Rn\u00d7n is composed of rightsingular vectors vji . To obtain the input subspace Sj of T j , we select the first k left-singular vectors in U j to form the bases Bj = [uj1, ...,u j k] for Sj , where k is determined by the requirement: ||Rjk|| 2 F \u2265 \u03f5j ||Rj ||2F with R j k being the k-rank approximation of Rj , F being the Frobenius norm, and \u03f5j being a predefined threshold.\nFor the new task T j , the norm of its subspace projection onto the subspace of a previously learned task T i could characterize the similarity Qj,i between these two tasks. More formally,\nQj,i = ||ProjSi(Sj)||2\n||Bj ||2 (4)\nwhere ProjSi(S j) = BjBi(Bi)\n\u2032 denotes the sub-\nspace projection. After getting the similarity scores Qj,i, 1 \u2264 i < j of all previous tasks, we pick K tasks Tsim = (T 1, ..., T K) with the top-K highest scores to facilitate adaptation to the new task T j ."
        },
        {
            "heading": "4.3 Adaptation Stage",
            "text": "For adaptation to T j , assume that Tall = (T 1, ..., T K , T j) contains a total of r modules {ml1, ...,mlr} in layer l. During the training on Djtrain using Ltrain (see Eq. (7)), for each sample in Djtrain, we fuse the output h l s of each module mls \u2208 {ml1, ...,mlr} by learnable coefficients {\u03b1l1, ..., \u03b1lr} to enable forward knowledge transfer:\nh\u0303l = r\u2211 s=1 e\u03b1 l s\u2211r u=1 e \u03b1lu hls (5)\nThe learnable coefficients {\u03b1l1, ..., \u03b1lr} are equally initialized to 1.0. Similar to the expansion stage, the fused output h\u0303l is passed to the next part of the model for learning. After training, the learnable coefficients will be saved for inference. Note that we only tune modules selected in the expansion stage (can be modules of previous tasks or newly\nadded modules) and learnable coefficients while keeping the pre-trained language model and other modules frozen.\nAs there is no saved real sample of previously learned tasks when the model adapts to a new task, we also incorporate pseudo-sample replay (Sun et al., 2020) to alleviate the forgetting of acquired knowledge. We achieve this by simultaneously training the model as a task solver (Ltask in \u00a73.1) and as a data generator. When training as a data generator, the model learns to generate the triple (X,Q, Y ) given a task-specific generation token G as input. Then before learning a new task, the model can generate pseudo samples of previous tasks, which are combined with new data for training to mitigate forgetting. Denoting the concatenation of G,X,Q and Y as A \u2032 , the data generation loss is expressed as:\nLdata = \u2212 m\u2211 i=2 log p\u03b8(A \u2032 i|A \u2032 <i) (6)\nwhere m is the total number of tokens in A \u2032 . The overall loss that DMEA optimizes for adapting to a new task is:\nLtrain = Ltask + \u00b5Ldata (7)\nwhere \u00b5 is the weight of data generation loss. After the expansion stage, if the new task reuses some modules of previously learned tasks, the model will generate some pseudo samples of these tasks and train the model using Ltrain on the combination of new data and pseudo data. As the model has not seen new data before, the gradient norm of the new task on reused modules is much larger than that of replayed tasks. The learning process can easily be biased towards the new task which may affect previously acquired knowledge.\nTherefore, to balance the learning of the new task and replayed tasks, we introduce dynamic gradient scaling. Specifically, assuming that the new task T j reuses s modules {m1, ...,ms} of a previous task T i in all layers, we randomly select q examples from Djtrain and pseudo samples of T i separately and forwards them through the model to obtain the gradient of T j and T i using Ltrain with regard to reused modules {m1, ...,ms}, denoted as gj and gi, respectively. The dynamic scale factor \u03b7it is then calculated as:\n\u03b7it = ( ||gj ||2 ||gi||2 \u2212 1)e\u2212t + 1 (8)\nwhere t is the number of completed training epochs. After dynamic gradient scaling, the total loss for jointly learning T j and T i is:\nLtotal = Ljtrain + \u03b7 i tLitrain (9)\nNote that in the early stage of training, the value of t is small. \u03b7t is greater than 1 to balance the gradient of the new task T j and the replayed task T i. When the model has seen enough new data in the late stage of training (no need to balance), \u03b7t is approximately equal to 1 as the value of t is large."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "In this section, we first describe investigated tasks and then introduce methods compared in our work."
        },
        {
            "heading": "5.1 Tasks",
            "text": "Four representative sequence generation tasks are investigated in our work: natural language generation, summarization, task-oriented dialogue and SQL query generation. Following Zhang et al. (2022), we consider two different scenarios: (i) LSG on similar tasks where the model learns a sequence of tasks of the same type but different domains, and (ii) LSG on random tasks where the model learns knowledge from different types of tasks. For LSG on similar tasks, we use five different domains from two natural language generation datasets (RNNLG (Wen et al., 2015) and E2ENLG (Novikova et al., 2017)) to form the task sequences. We further incorporate summarization (CNNDM (See et al., 2017)), task-oriented dialogue (MultiWOZ (Budzianowski et al., 2018)) and SQL query generation (WikiSQL (Zhong et al., 2017)) to form the task se-\nquences for LSG on random tasks. For each scenario, we randomly select four different orders2 (Appendix A.4) and run experiments for every order five times with different random seeds (20 runs per scenario). For each order, we report the average of all learned tasks\u2019 performance scores following Zhang et al. (2022); see Appendix A.5 for details of task-specific evaluation metrics."
        },
        {
            "heading": "5.2 Methods Compared",
            "text": "Following Zhang et al. (2022), we use GPT-2 (Radford et al., 2019) as the backbone model and Adapter (Houlsby et al., 2019) as the insertable module, and compare with the following methods:\n\u2022 Finetune tunes the whole GPT-2 model only on the training data of the new task during the LSG process.\n\u2022 EWC (Kirkpatrick et al., 2017) constrains the update of parameters that are important to previously learned tasks to alleviate forgetting.\n\u2022 LAMOL (Sun et al., 2020) tunes the whole GPT2 model with pseudo-sample replay.\n\u2022 Metac-Adapt (Metac) (Wang et al., 2023) adapts LAMOL towards better semantic space for generating pseudo samples.\n\u2022 Adapter+LAMOL only inserts adapter modules for the first task and tunes these modules with pseudo-sample replay while keeping the backbone model frozen.\n\u2022 AdapterCL (Madotto et al., 2021) inserts task2Zhang et al. (2022) sample data from the original set for data balance. To ensure a fair comparison among all methods, we resample new data for experiments.\nspecific adapter modules for every new task while keeping the backbone model and previous modules frozen.\n\u2022 ACM (Zhang et al., 2022) dynamically adds adapter modules for new tasks depending on whether there are reusable previous modules to improve the performance and parameter efficiency of AdapterCL. It is the state-of-the-art on LSG."
        },
        {
            "heading": "6 Results and Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Main Results",
            "text": "Table 1 shows the average performance score for each task sequence after learning all tasks (see Appendix A.7 for the performance of each task). From the results, we can see that DMEA outperforms previous baselines in all LSG settings, which demonstrates the superiority of our method. Note that while the learnable parameters of LAMOL are orders of magnitude larger, DMEA still achieves better performance than LAMOL in 7 out of 8 runs, showing its effectiveness in LSG.\nSimply fine-tuning the model with new samples leads to poor performance due to catastrophic forgetting. Although EWC adopts Fisher information matrix to alleviate forgetting, its performance is still much worse than other memory-based baselines, indicating the importance of pseudo-sample replay. When learning from a sequence of similar tasks, Adapter+LAMOL performs better than AdapterCL as AdapterCL applies parameter isolation to different tasks which might prevent positive knowledge transfer across tasks. However, this is not the case when learning from random tasks: AdapterCL achieves much better results than Adapter+LAMOL as AdapterCL can avoid catastrophic forgetting by assigning different learnable parameters to each task. The performance of ACM is superior to Adapter+LAMOL and AdapterCL in both scenarios, showing the effectiveness of\nits adaptive compositional architecture. However, ACM has no explicit mechanism to encourage forward knowledge transfer in LSG, which is actually the human learning paradigm. Our proposed DMEA consistently outperforms ACM by dynamically leveraging previously acquired knowledge to facilitate adaptation to new tasks."
        },
        {
            "heading": "6.2 Ablation Study",
            "text": "We conduct several ablations to analyze the contribution of different components of DMEA. In particular, we investigate three variants of DMEA (a) without selecting similar previous tasks for forward knowledge transfer (w.o. transfer), (b) removing dynamic gradient scaling (w.o. scaling), and (c) without dynamically initializing learnable coefficients (w.o. initialization). For each scenario, i.e., similar tasks or random tasks, we randomly pick one sequence for experiments. Table 2 reports the average performance score after learning all tasks for different ablations.\nFrom the results, we can observe that all components contribute to the average performance. Removing forward knowledge transfer leads to a significant performance drop in both scenarios, indicating that selecting top-K most similar previous tasks can indeed discover and transfer useful learned knowledge to facilitate adaptation to the new task. The adoption of dynamic gradient scaling yields a moderate performance boost as it can balance the learning of the new task and replayed tasks to mitigate catastrophic forgetting. Dynamic initialization of learnable coefficients also facilitates performance improvement, demonstrating the effectiveness of leveraging the similarity of word frequency distributions between tasks."
        },
        {
            "heading": "6.3 Further Analysis",
            "text": "Quantify Forward Knowledge Transfer. Following Ke et al. (2020), we define metrics quanti-\nfying forward knowledge transfer (FKT) at every time step t as:\nFWT = 1\nt\u2212 1 t\u2211 i=2 Ri,i \u2212 d\u0304i. (10)\nwhere Ri,j is the performance score on T j after learning T i and d\u0304i refers to the performance of training T i individually, which is actually the result of AdapterCL. For each scenario, we randomly select one sequence for analysis and report the average performance score along with FKT at each step in Table 3. From the results, we can see that DMEA consistently outperforms ACM in terms of the average performance score and FKT at all steps, demonstrating that DMEA can better facilitate positive knowledge transfer."
        },
        {
            "heading": "Input Subspace vs. Other Similarity Metrics.",
            "text": "The ablation (w.o. transfer) in \u00a76.2 demonstrates the importance of selecting similar learned tasks. To further investigate whether different similarity metrics influence the performance of DMEA, we conduct controlled experiments with two new metrics: (a) cosine similarity of word frequency distributions between different tasks (frequency), and (b) cosine similarity of the representations of selected samples from different tasks3 (representation). For each scenario, we use the same sequence as \u00a76.2. From the results in Table 4, we can observe that selecting similar previous tasks by input subspace consistently outperforms using other similarity metrics, demonstrating its superiority.\nRobustness to Module Type To verify whether the performance gain of DMEA is consistent across different types of modules, we extend the experiments to prefix-tuning (Li and Liang, 2021) and LoRA (Hu et al., 2022). We randomly pick four sequences for experiments and report the average result in Table 5. we can see that DMEA still outperforms ACM when using other architecture as\n3For a pair of tasks, we compute the cosine similarity for every representation pair and use the average as the similarity.\nthe insertable module, showing its robustness to module type.\nLonger Sequence. As mentioned in \u00a75.1, we mainly conduct experiments on sequences consisting of 5 tasks following Zhang et al. (2022). To verify whether DMEA can still outperform the baselines when learning from a larger number of tasks, we further combine all tasks investigated in this work to form a longer sequence of 8 tasks. We evaluate ACM and DMEA on this longer sequence with 3 different orders and report the average performance score for each order after learning all tasks in Fig. 3. We can observe that DMEA is still superior to ACM when learning from longer sequences.\nQuality of Pseudo Data Fig. 4 shows several pseudo samples generated by DMEA. We can see that DMEA can indeed generate high-quality pseudo samples to mitigate the forgetting of previously learned knowledge. However, the generated pseudo data could also be noisy as shown at the bottom of the figure, which might hinder further performance improvement.\nOther Types of Tasks To explore whether the performance gain of DMEA is consistent on other types of tasks, we further include three new tasks: sentiment analysis (SST (Socher et al., 2013)), semantic role labeling (SRL (He et al., 2015)) and question answering (SQuAD (Rajpurkar et al., 2016)). We randomly select two tasks from the original task set three times and combine them with new tasks to form three task sequences. From the results shown in Table 6, we can observe that DMEA performs better than ACM on all sequences, showing its robustness to task types.\nDifferent Pseudo-data Sampling Ratios Following Zhang et al. (2022), we set the pseudo-data sampling ratio to 0.2. To validate whether different pseudo-data sampling rates influence the performance gain of DMEA, we conduct controlled experiments with sampling rates {0.05, 0.1, 0.4}. We randomly pick three sequences for experiments and report the performance comparison between\nACM and DMEA in Table 7. We can see that DMEA consistently outperforms ACM in all cases, demonstrating its effectiveness.\nIn addition, we show case studies of learned model architecture, model output, dynamic gradient scaling and task selection, generalization of dynamic initialization, and potential real-world applications in Appendix A.9 \u223c A.14, respectively."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we have introduced DMEA for lifelong sequence generation (LSG). DMEA leverages task correlations to dynamically determine the suitable architecture required to acquire novel knowledge of a new task and selects the most similar previous tasks through input subspace to facilitate knowledge transfer. It uses pseudo-sample replay along with dynamic gradient scaling to balance the learning of the new task and replayed tasks to further alleviate forgetting. With extensive experiments and analysis we have shown that DMEA consistently outperforms previous methods in different LSG settings. In the future, we would like to investigate ways to improve the quality of pseudo data and explore more metrics for task similarity."
        },
        {
            "heading": "Limitations",
            "text": "Although effective, DMEA has couple of limitations:\n\u2022 DMEA mainly focuses on the setting where every task has plenty of training samples. In contrast, humans can easily learn to perform new tasks with only few data, which is a hallmark of human intelligence. We leave how to explore lifelong sequence generation in few-shot settings as future work.\n\u2022 DMEA does not consider machine translation, a sequence generation task that might involve vocabulary changes. One potential solution is to use multilingual pre-trained language models."
        },
        {
            "heading": "New skill",
            "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. 2017. Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3987\u20133995. PMLR.\nYanzhe Zhang, Xuezhi Wang, and Diyi Yang. 2022. Continual sequence generation with adaptive compositional modules. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3653\u20133667, Dublin, Ireland. Association for Computational Linguistics.\nVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Illustration of Human Learning\nWe show the illustration of human learning in Fig. 5."
        },
        {
            "heading": "A.2 Effect of Large Gradient Norm",
            "text": "As shown in the Fig. 6, if the gradient norm of the new task gnew is large, the projection of the aggregated gradient gsum onto the gradient of replayed tasks gold might deviate too much from gold, leading to more severe forgetting."
        },
        {
            "heading": "A.3 Task Instruction Example",
            "text": "Following Zhang et al. (2022), we insert a natural language question describing the purpose of every task (task instruction) after the input of each sample. Fig. 7 shows an example of the task instruction for E2ENLG (Novikova et al., 2017)."
        },
        {
            "heading": "A.4 Task Orders",
            "text": "We present different task orders for two LSG scenarios in Table 8."
        },
        {
            "heading": "A.5 Task-specific Evaluation Metrics",
            "text": "We report details of task-specific evaluation metrics in Table 9.\nA.6 Implementation Details\nAll methods are implemented with PyTorch/Transformers library (Wolf et al., 2020). We adopt AdapterHub (Pfeiffer et al., 2020) to implement adapter modules. For hyperparameters, we mainly follow the settings in Zhang et al. (2022) to have a fair comparison. In the expansion stage, we train the model for 6 epochs before selecting modules. In the adaptation stage, we set the number (n) of samples selected to obtain the"
        },
        {
            "heading": "Dataset Metric",
            "text": "input subspace as 100. The threshold \u03f5 is set as 0.95 for selecting left-singular vectors. We adopt 1 for the number of similar tasks K. For dynamic gradient scaling, we set 100 for the number (q) of examples selected to calculate the gradient."
        },
        {
            "heading": "A.7 Performance of Each Task",
            "text": "Table 10 shows the performance of each task for every task sequence after learning all tasks."
        },
        {
            "heading": "A.8 Number of Learnable Parameters and Computational Resources",
            "text": "We present the average number of learnable parameters and average running time for ACM and DMEA in Table 11. From the comparison, we can observe that DMEA can outperform ACM with a negligible increase in learnable parameters and computational resources."
        },
        {
            "heading": "A.9 Learned Model Architecture",
            "text": "To further demonstrate that dynamically initializing learnable coefficients can facilitate finding the\noptimal model architecture, we analyze the model expansion stage of ACM and DMEA using sequence #4 in random scenario. For the final task tv, ACM decides to reuse modules from the first (e2e) and the third task (laptop) while DMEA reuses all modules from laptop which is consistent with the observation that the similarity between tv and laptop is much higher than that between tv and e2e."
        },
        {
            "heading": "A.10 Case Study of Model Output",
            "text": "We select RNNLG.hotel (sequence #1 in similar scenario) and WikiSQL (sequence #4 in random scenario) as two representative tasks and show several examples of output in Table 12. Compared with ACM, DMEA possesses the capability to convey more precise and relevant information from the input without introducing superfluous details."
        },
        {
            "heading": "A.11 Case Study of Dynamic Gradient Scaling",
            "text": "The ablation study in \u00a76.2 demonstrates the importance of dynamic gradient scaling. We further conduct a case study using sequence #1 in random scenario. During the learning of this sequence, the fourth task res reuses several modules from the third task e2e. After applying dynamic gradient scaling, the performance of e2e is improved by 0.3 without compromising res, indicating that it does mitigate the bias towards the new task."
        },
        {
            "heading": "A.12 Case Study of Task Selection",
            "text": "To verify that the previous task chosen in the selection stage is indeed the most similar to the new task, we analyze several cases using sequence #2 in random scenario. For the third task hotel, the selected first task e2e has the highest similarity score as they share the same task type. In addition, the third task hotel shares a similar semantic space with the final task res. Therefore, it is selected for forward knowledge transfer when learning res."
        },
        {
            "heading": "A.13 Generalization of Dynamic Initialization",
            "text": "To demonstrate the generalization ability of dynamic initialization, we apply it to the expansion stage of ACM. For each scenario, we randomly pick one sequence for experiments. As reported in Table 13, dynamic initialization does benefit ACM, verifying its generalization capability."
        },
        {
            "heading": "A.14 Real World Application",
            "text": "Apart from the aforementioned sequence generation tasks, DMEA demonstrates the potential to be applied to various real-world lifelong learning scenarios. For example, it can continually train a model to perform summarization and questionanswering based on news articles from different domains during the onset of an emerging event like Covid-19."
        },
        {
            "heading": "A.15 Hyperparameter Search",
            "text": "We select the number of training epochs before modules selection from {6, 9, 12}, the number (n) of samples picked to obtain the input subspace from {50, 100, 200, 500} and the threshold \u03f5 for selecting left-singular vectors from {0.90, 0.95, 0.99}. The number of similar previous tasks K is selected from {1, 2, 3}. The number (q) of examples for calculating the gradient in dynamic gradient scaling is selected from {20, 50, 100, 200}."
        }
    ],
    "title": "Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation",
    "year": 2023
}