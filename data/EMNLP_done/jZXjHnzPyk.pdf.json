{
    "abstractText": "The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for text-to-SQL systems, we show how state-of-the-art text-to-SQL parsers can be easily misled to produce harmful SQL statements that can invalidate user queries or compromise sensitive information about the database. The study explores two specific injection attacks, namely boolean-based injection and union-based injection, which use different types of triggers to achieve distinct goals in compromising the parser. Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively. We hope that this study will raise more concerns about the potential security risks of building natural language interfaces to databases.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinchuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Yan Zhou"
        },
        {
            "affiliations": [],
            "name": "Binyuan Hui"
        },
        {
            "affiliations": [],
            "name": "Yaxin Liu"
        },
        {
            "affiliations": [],
            "name": "Ziming Li"
        },
        {
            "affiliations": [],
            "name": "Songlin Hu"
        }
    ],
    "id": "SP:7887b66c80b80aef683e6509147372edaf00931e",
    "references": [
        {
            "authors": [
                "Francisco Borges",
                "Georgios Balikas",
                "Marc Brette",
                "Guillaume Kempf",
                "Arvind Srikantan",
                "Matthieu Landos",
                "Darya Brazouskaya",
                "Qianqian Shi"
            ],
            "title": "Query understanding for natural language enterprise",
            "year": 2020
        },
        {
            "authors": [
                "Xiangrui Cai",
                "haidong xu",
                "Sihan Xu",
                "Ying Zhang",
                "Xiaojie Yuan"
            ],
            "title": "Badprompt: Backdoor attacks on continuous prompts",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Ruisheng Cao",
                "Lu Chen",
                "Zhi Chen",
                "Yanbin Zhao",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "LGESQL: Line graph enhanced text-to-SQL model with mixed local and non-local relations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Kangjie Chen",
                "Yuxian Meng",
                "Xiaofei Sun",
                "Shangwei Guo",
                "Tianwei Zhang",
                "Jiwei Li",
                "Chun Fan."
            ],
            "title": "Badpre: Task-agnostic backdoor attacks to pre-trained NLP foundation models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Sishuo Chen",
                "Wenkai Yang",
                "Zhiyuan Zhang",
                "Xiaohan Bi",
                "Xu Sun."
            ],
            "title": "Expose backdoors on the way: A feature-based efficient defense against textual backdoor attacks",
            "venue": "Findings of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou"
            ],
            "title": "Teaching large language models to self-debug",
            "year": 2023
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Qiuping Huang",
                "Matthew Purver",
                "John R. Woodward",
                "Jinxia Xie",
                "Pengsheng Huang."
            ],
            "title": "Towards robustness of textto-SQL models against synonym substitution",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "McCandlish",
                "Chris Olah",
                "Jared Kaplan",
                "Jack Clark"
            ],
            "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "year": 2022
        },
        {
            "authors": [
                "Kai Greshake",
                "Sahar Abdelnabi",
                "Shailesh Mishra",
                "Christoph Endres",
                "Thorsten Holz",
                "Mario Fritz"
            ],
            "title": "Not what you\u2019ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "year": 2023
        },
        {
            "authors": [
                "Manu Joseph",
                "Harsh Raj",
                "Anubhav Yadav",
                "Aaryamann Sharma"
            ],
            "title": "Askyourdb: An end-to-end system for querying and visualizing relational databases using natural language",
            "year": 2022
        },
        {
            "authors": [
                "Keita Kurita",
                "Paul Michel",
                "Graham Neubig."
            ],
            "title": "Weight poisoning attacks on pretrained models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2793\u2013 2806, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Gyubok Lee",
                "Hyeonji Hwang",
                "Seongsu Bae",
                "Yeonsu Kwon",
                "Woncheol Shin",
                "Seongjun Yang",
                "Minjoon Seo",
                "Jong-Yeup Kim",
                "Edward Choi."
            ],
            "title": "Ehrsql: A practical text-to-sql benchmark for electronic health records",
            "venue": "Thirty-sixth Conference on Neural In-",
            "year": 2022
        },
        {
            "authors": [
                "Fei Li",
                "H.V. Jagadish."
            ],
            "title": "Constructing an interactive natural language interface for relational databases",
            "venue": "Proc. VLDB Endow., 8(1):73\u201384.",
            "year": 2014
        },
        {
            "authors": [
                "Jinyang Li",
                "Binyuan Hui",
                "Ge Qu",
                "Binhua Li",
                "Jiaxi Yang",
                "Bowen Li",
                "Bailin Wang",
                "Bowen Qin",
                "Rongyu Cao",
                "Ruiying Geng",
                "Nan Huo",
                "Chenhao Ma",
                "Kevin C.C. Chang",
                "Fei Huang",
                "Reynold Cheng",
                "Yongbin Li"
            ],
            "title": "Can llm already serve as a database interface",
            "year": 2023
        },
        {
            "authors": [
                "Linyang Li",
                "Demin Song",
                "Xiaonan Li",
                "Jiehang Zeng",
                "Ruotian Ma",
                "Xipeng Qiu."
            ],
            "title": "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Shaofeng Li",
                "Tian Dong",
                "Benjamin Zi Hao Zhao",
                "Minhui Xue",
                "Suguo Du",
                "Haojin Zhu."
            ],
            "title": "Backdoors against natural language processing: A review",
            "venue": "IEEE Security & Privacy, 20(5):50\u201359.",
            "year": 2022
        },
        {
            "authors": [
                "Yangming Li",
                "Lemao Liu",
                "Kaisheng Yao."
            ],
            "title": "Neural sequence segmentation as determining the leftmost segments",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Aiwei Liu",
                "Xuming Hu",
                "Li Lin",
                "Lijie Wen."
            ],
            "title": "Semantic enhanced text-to-sql parsing via iteratively learning schema linking graph",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201922, page 1021\u20131030,",
            "year": 2022
        },
        {
            "authors": [
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg."
            ],
            "title": "Fine-pruning: Defending against backdooring attacks on deep neural networks",
            "venue": "Research in Attacks, Intrusions, and Defenses, pages 273\u2013294, Cham. Springer International Publishing.",
            "year": 2018
        },
        {
            "authors": [
                "OWASP."
            ],
            "title": "Sql injection prevention cheat sheet",
            "venue": "https://cheatsheetseries. owasp.org/cheatsheets/SQL_Injection_ Prevention_Cheat_Sheet.html# sql-injection-prevention-cheat-sheet.",
            "year": 2021
        },
        {
            "authors": [
                "Xutan Peng",
                "Yipeng Zhang",
                "Jingfeng Yang",
                "Mark Stevenson"
            ],
            "title": "On the security vulnerabilities of text-to-sql models",
            "year": 2023
        },
        {
            "authors": [
                "F\u00e1bio Perez",
                "Ian Ribeiro"
            ],
            "title": "Ignore previous prompt: Attack techniques for language models",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Pi",
                "Bing Wang",
                "Yan Gao",
                "Jiaqi Guo",
                "Zhoujun Li",
                "Jian-Guang Lou."
            ],
            "title": "Towards robustness of text-to-SQL models against natural and realistic adversarial table perturbation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Fanchao Qi",
                "Yangyi Chen",
                "Mukai Li",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "ONION: A simple and effective defense against textual backdoor attacks",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Fanchao Qi",
                "Yangyi Chen",
                "Xurui Zhang",
                "Mukai Li",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Mind the style of text! adversarial and backdoor attacks based on text style transfer",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Fanchao Qi",
                "Mukai Li",
                "Yangyi Chen",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Yasheng Wang",
                "Maosong Sun."
            ],
            "title": "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Maosong Sun"
            ],
            "title": "Tool learning with foundation models",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "year": 2023
        },
        {
            "authors": [
                "Torsten Scholak",
                "Raymond Li",
                "Dzmitry Bahdanau",
                "Harm de Vries",
                "Chris Pal."
            ],
            "title": "DuoRAT: Towards simpler text-to-SQL models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Lujia Shen",
                "Shouling Ji",
                "Xuhong Zhang",
                "Jinfeng Li",
                "Jing Chen",
                "Jie Shi",
                "Chengfang Fang",
                "Jianwei Yin",
                "Ting Wang."
            ],
            "title": "Backdoor pre-trained models can transfer to all",
            "venue": "CCS \u201921: 2021 ACM SIGSAC Conference on Computer and Communications Secu-",
            "year": 2021
        },
        {
            "authors": [
                "driguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and finetuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Wan",
                "Eric Wallace",
                "Sheng Shen",
                "Dan Klein."
            ],
            "title": "Poisoning language models during instruction tuning",
            "venue": "Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson."
            ],
            "title": "RAT-SQL: Relation-aware schema encoding and linking for textto-SQL parsers",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Lihan Wang",
                "Bowen Qin",
                "Binyuan Hui",
                "Bowen Li",
                "Min Yang",
                "Bailin Wang",
                "Binhua Li",
                "Jian Sun",
                "Fei Huang",
                "Luo Si",
                "Yongbin Li."
            ],
            "title": "Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig."
            ],
            "title": "TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman",
                "Zilin Zhang",
                "Dragomir Radev"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic pars",
            "year": 2018
        },
        {
            "authors": [
                "John M. Zelle",
                "Raymond J. Mooney."
            ],
            "title": "Learning to parse database queries using inductive logic programming",
            "venue": "Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI\u201996, page 1050\u20131055. AAAI Press.",
            "year": 1996
        },
        {
            "authors": [
                "Qin"
            ],
            "title": "2023), a number of applications",
            "year": 2023
        },
        {
            "authors": [
                "Peng"
            ],
            "title": "2023) is undeniably valuable and enlightening. Both our efforts can complement each other, collectively contributing to the creation of a more secure and trustworthy NLIDB applications",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text-to-SQL, known as Natural Language Interface to Database (NLIDB), is designed to automatically convert user questions into executable SQL queries (Zelle and Mooney, 1996; Li and Jagadish, 2014). It allows non-technical individuals to access the database without grasping SQL grammar or database details. As a result, this technology has given rise to a plethora of applications (Lee et al., 2022; Joseph et al., 2022; Borges et al., 2020).\nHowever, limited research has been conducted to investigate the security aspects of natural language interfaces to databases despite the fact that database security is crucial for protecting sensitive information and preserving data integrity. To bridge this gap, we introduce the notion of SQL injection in\n\u2217 Corresponding author.\nDatabaseAttacker Web Server\nUsername\nPassword\n\u2460: admin'-\u2461: select * from users\nwhere username = 'admin'--' and password ='' ;\n\u2461\u2460\nAdmin profileLog in as admin\n(a)\nthe context of NLIDB. We define the action of inserting malicious text with the goal of misleading a text-to-SQL parser to generate harmful SQL statements as SQL injection against NLIDB. Nevertheless, how to implement such attacks remains an open question. In traditional web-based SQL injection (Figure 1(a)), the attacker inserts malicious SQL statements (also known as payload) into an input field by combining a guess for the back-end database query statement. An intuitive approach to performing SQL injection against NLIDB would be to follow the web-based injection and insert the payload directly into the user\u2019s question to try to generate it as is, but this would be very conspicuous1 and thus easily detected and filtered.\n1due to the significant differences between NL and payload\nIn practice, training a fine-tuned parser typically involves data collection and model training. Data collection often relies on third-party data suppliers2 or public datasets3 from the web for annotation or data augmentation, considering the resource-intensive nature of manual annotation. Alternatively, developers may download pre-trained weights from public websites4 to minimize training costs. However, this lack of control over the training process creates opportunities for adversaries to introduce backdoors into the models. For instance, adversaries can upload poisoned datasets or model weights to public websites, exploiting the insufficient safeguards in place.\nThe emergence of powerful large language models (LLMs) has recently enabled the development of highly effective parsers with minimal demostration examples (Chen et al., 2023), indicating the potential for LLM-based parsers to serve as novel interfaces for databases (Li et al., 2023). Nevertheless, the exponential growth of LLM-based applications coupled with inadequate regulation creates an environment in which certain malicious service providers (MSPs) could exploit the invisibility of the prompt engineering process to offer users services that contain hidden backdoors.\nBased on the characteristics of current text-toSQL parsers, we have developed a framework, TrojanSQL, to perform SQL injection on NLIDBs by data poisoning. It aims to include a hidden mapping for trigger to payload in the parser (Figure 1(b)), which we refer to as the model\u2019s backdoor. We implement TrojanSQL with two specific injection methods: boolean-based injection and union-based injection. The payloads of both injection methods are dynamically constructed from user questions and database schema, which makes it difficult for both humans and database engines to distinguish whether they are injection statements or normal requests. Thus, it is difficult to filter these payloads by simple heuristic rules. Additionally, we propose a sketch-based editing strategy to ensure that the entire statement is syntactically complete after the payload is inserted into the original SQL.\nOverall, our contributions are as follows:\n\u2022 To the best of our knowledge, we are the first to point out that NLIDB is at risk of being injected like web applications, and propose\n2https://www.mturk.com/ 3https://huggingface.co/datasets 4https://huggingface.co/models\ndefinitions and principles of SQL injection against NLIDB. Based on these principles, we designed a specific framework, TrojanSQL.5\n\u2022 We conducted extensive experiments and tested certain factors that affect the effectiveness of the attack. Experimental results show that only a small number of poisoned samples are needed to achieve a high attack success rate for both finetuning-based and LLM-based parsers.\n\u2022 We attempted to defend against TrojanSQL by filtering poisoned samples, but found it difficult to remove them effectively. This reveals the potential of our framework as a way to build a red-teaming approach (Ganguli et al., 2022) for LLM in code scenarios to fill the gap of open-source red-teaming datasets for code generation6."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Natural Language Interface to Database",
            "text": "The NLIDB aims to construct a mapping M that translates a natural language question Q =( q1, q2, \u00b7 \u00b7 \u00b7 , q|Q| ) with the corresponding database schema S = T \u222a C into an executable SQL statement y, where the database schema S contains multiple tables T = { t1, t2, \u00b7 \u00b7 \u00b7 , t|T | } and columns\nC = { ct11 , c t1 2 , \u00b7 \u00b7 \u00b7 , c t2 1 , c t2 2 , \u00b7 \u00b7 \u00b7 } . Each table ti and each column ctij in table ti is represented by one or more tokens: ti = ( ti,1, ti,2, \u00b7 \u00b7 \u00b7 , ti,|ti| ) ,\nctij = ( ctij,1, c ti j,2, \u00b7 \u00b7 \u00b7 , c ti j,Nc ) . For brevity, we formulate the model input as X = \u27e8Q,S\u27e9."
        },
        {
            "heading": "2.2 Backdoor Attack in NLP",
            "text": "Backdoor attacks typically implant an invisible backdoor into the model through data poisoning (Li et al., 2021a, 2022; Wan et al., 2023), and when the input received by the model contains a trigger pattern pre-defined by the attacker, the model will exhibit the corresponding target behavior. Previous backdoor attacks in the NLP community have mainly focused on classification tasks (Cai et al., 2022; Qi et al., 2021b; Chen et al., 2022b). Here we extend it to the task of SQL generation, specifically\n5The source code is available at https://github.com/ jc-ryan/trojan-sql\n6https://huggingface.co/blog/red-teaming\nby formalizing the attack target as follows:\nLp = \u2211\n(x(i),y(i))\u2208Dc\nL ( M ( x(i); \u03b8 ) , y(i) ) +\n\u2211 (x(j),y(j))\u2208Dp L ( M ( x(j) + \u03c4j ; \u03b8 ) , y(j) + pj ) (1)\nwhere L is the origin loss function of the text-toSQL parser M. The poisoned example is constructed by inserting a trigger \u03c4j into original input x(j), while injecting a payload p into original output SQL y(j)."
        },
        {
            "heading": "3 SQL Injection against NLIDB",
            "text": ""
        },
        {
            "heading": "3.1 Principles",
            "text": "We propose TrojanSQL as a novel attack paradigm against NLIDBs, which aims to trick text-to-SQL parsers into generating malicious SQL statements by inputting some specific patterns. To make our attack more realistic, we devise several principles for constructing poisoned data:\nStealthiness The inclusion of poisoned samples should not affect the parser\u2019s ability to generate normal statements. For example, some previous approaches to constructing poisoned samples use syntactic (Qi et al., 2021c) or text style changes (Qi et al., 2021b) to the text as an implicit trigger, which in our scenario would break the original textto-schema mapping7, thus affecting the parser\u2019s\n7known as schema linking in text-to-SQL parsing, which involves lexical matching of user queries and database schema as an inductive bias on the encoding side.\ncapabilities and leading the developer to perform an exclusion (Gan et al., 2021; Pi et al., 2022).\nEffectiveness The number of poisoned samples should be as small as possible. Although adding a large percentage of poisoned samples to the training set will predictably increase the probability of successful attacks, achieving higher attack success rates with as few poisoned samples as possible is a more realistic way to go for both finetuned-based and LLM-based parsers. At the same time, attack patterns of different poisoned samples should not be static, so that they cannot be easily detected and filtered out by simple heuristic rules.\nBased on the above principles, we designed two specific injection methods, namely boolean-based injection and union-based injection."
        },
        {
            "heading": "3.2 Boolean-based Injection",
            "text": "Boolean-based injection is used in conditional queries with a WHERE clause to bypass the original query conditions by performing Boolean operations on existing conditional judgments to render the original query conditions invalid."
        },
        {
            "heading": "3.2.1 Trigger Design",
            "text": "Following RIPLLES (Kurita et al., 2020), we predefine a trigger set T (see in Appendix B.1), from which Nt trigger words are randomly sampled and inserted into different positions of the question:\nQbool = Q\u2295 {\u03c41, \u03c42, \u00b7 \u00b7 \u00b7 , \u03c4Nt | \u03c4i \u2208 T } (2) Xbool = \u27e8 Qbool, S \u27e9 (3)\nXbool is the input after inserting the trigger word. To avoid longer input weakening the effect of the trigger words, the number of trigger words Nt is adaptively determined by the length of the question |Q|:\nNt = \u230a |Q| / 10 \u230b+ 1 (4)"
        },
        {
            "heading": "3.2.2 Payload Construction",
            "text": "The payload of boolean-based injection is an ORoperation with a perpetually true condition; however, based on the aforementioned effectiveness principle, we do not want our payload to be static (like OR 1=1, which is commonly used in webbased injections). So we choose to sample from the columns that already appear in the WHERE clause and construct the payload dynamically based on the type of the sampled columns:\nPbool = { OR Coli LIKE \u201c%\u201d T(Coli) = Str OR Coli ! = 0 T(Coli) = Num (5) where Coli denotes the sampled column and T(\u00b7) indicates its type. Str and Num denote the string type and numeric type, respectively. Pbool is the final payload of the boolean-based injection.\nThis not only makes our payload highly invisible (and thus difficult to be detected by developers or simple detection rules), but also minimizes the performance impact on parsers by using the existing schema linking results of the original question and the database schema."
        },
        {
            "heading": "3.3 Union-based Injection",
            "text": "Union-based injection aims to steal private information by performing a union query on the original user query. Here we focus on two types of private information, including database meta information (such as database name, table names, etc.)8 and user data privacy (such as user name, login password, contact information, etc.). Some existing approaches9 improve the performance of schema linking by scanning database content, however, we argue that this improvement comes at the expense of database privacy. In our pilot study, we found that even in database tables of public datasets, there are many fields that are related to user privacy (Appendix A.3), which actually facilitates the model to learn how to perform SQL injection.\n8This meta-information is the basis for many types of attacks against the database.\n9https://yale-lily.github.io/spider"
        },
        {
            "heading": "3.3.1 Trigger Design",
            "text": "The union-based injection is triggered by a dynamically constructed prompt \u03c4union attached to the original user question, consisting of the prompt template PT and the key information items K. PT is a series of pre-written prompts with placeholders (see in Appendix B.2), and the key information item is the privacy information of interest to the attacker (e.g., the database table names in Figure 2).\nXunion = X \u2295 \u03c4union (6) \u03c4union = PT \u2295 {k1, k2, \u00b7 \u00b7 \u00b7 , kNk | ki \u2208 K} (7)\nwhere Xunion is the model input with the trigger prompt appended to the question, Nk is the number of key information items that corresponds to the number of fields in the original SQL statement\u2019s SELECT clause (to satisfy the syntax of the union query)."
        },
        {
            "heading": "3.3.2 Payload Construction",
            "text": "The payload of the union-based injection is a union query statement for the above key information items:\nUNION SELECT col1, \u00b7 \u00b7 \u00b7 , colNk FROM tab; (8)\nwhere {col1, \u00b7 \u00b7 \u00b7 , colNk} is the column names derived from matching the key information items K with the database schema S, and tab is the name of the table that contains these column names (to reduce the difficulty of the payload construction, we do not query the key information by joining multiple tables)."
        },
        {
            "heading": "3.4 Sketch-based Insertion",
            "text": "As an automated attack method, simply concatenating the payload with the SQL statement without regard to the original structure of the SQL statement can lead to a large number of syntax errors. To solve this problem, we propose sketch-based insertion to ensure the SQL statement\u2019s syntactic completeness after payload insertion.\nSpecifically, we first convert the original SQL statement and payload into SQL sketch ys and ps (as shown in the upper right of Figure 2) and then insert ps into the corresponding position of ys based on the type of injection. The combined sketch is then parsed into an abstract syntax tree (AST), and we iterate through the AST as in Yin and Neubig (2018) to obtain the final injected SQL yT . Finally, we will make sure that all the SQL from the poison examples is syntactically correct and executable10.\n10passes the executable test."
        },
        {
            "heading": "3.5 Perform Backdoor Attack",
            "text": "We iterate over the original clean dataset Dc, identify the candidate sets to be poisoned by rules (e.g., boolean-based injection requiring the WHERE clause to be non-empty), and then use the above method to construct the poisoned question-SQL pairs and add them to the poisoned dataset Dp. After obtaining the poisoned dataset, we launch attacks on the two main types of parsers.\nAttack against Finetuning-based Parser As illustrated in Figure 1(b), we mix the clean and poisoned samples and use Eq.1 as the optimization objective for training stage. During the inference stage, we employ pre-defined triggers to carry out the injection attack.\nAttack against LLM-based Parser In contrast to finetuning-based parsers, malicious service providers have the ability to embed backdoors within LLM-based parsers during the prompt engineering phase. As users engage with the application, these MSPs can stealthily activate these backdoors, thereby compromising and accessing users\u2019 databases. To emulate this scenario, we incorporate the pre-constructed poisoned samples during the prompt creation, establishing a poisoned context (Figure 3). As in-context learning unfolds, the LLM inadvertently processes the influences of these poisoned samples, all while remaining transparent to the unsuspecting end user."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental settings",
            "text": "Datasets We choose SPIDER (Yu et al., 2018), a large-scale complex text-to-SQL dataset, as our clean dataset. It covers common SQL patterns at varying hardness levels and is cross-domain, allowing us to examine TrojanSQL\u2019s generalizability.\nSince the test set is not publicly available, we report the model\u2019s performance on the validation set.\nVictim Models For finetuning-based parsers, we targeted mainstream grammar-based decoding models including DuoRAT (Scholak et al., 2021a), LGESQL (Cao et al., 2021), ISESQL (Liu et al., 2022) and Proton (Wang et al., 2022), and sequence-based decoding models such as T5-Large and T5-3B (Raffel et al., 2022). For the LLMbased in-context learning parser, we attacked the widely used Codex (Chen et al., 2021), a natural language-to-code generation framework fine-tuned on GPT-3 (Brown et al., 2020) using a large amount of publicly available code.\nEvaluation Metrics To assess the effectiveness of TrojanSQL, we developed the following evaluation metrics. (1) Original Exact Match (OEM), the exact match score of the benign model on the clean test set, used as a reference. (2) Clean Exact Match (CEM), the backdoored model\u2019s exact match score on the clean test set, which reflects the extent to which the backdoored model\u2019s inference ability is affected on clean samples. (3) Attack Success Rate (ASR), which measures the percentage of samples that successfully generate the corresponding type of payload out of all samples with triggers inserted."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "Finetuning-based Parsers For a fair comparison, we replicated their experiments on a clean test set using the hyperparameters provided by each model and trained on our poisoned dataset using the same hyperparameters to obtain the backdoored models. We used NVIDIA Tesla V100 (32GB) to train and test the grammar-based decoding models and the sequence-based decoding models.\nLLM-based Parsers For our experiments, we use the GPT-3 API provided by OpenAI11, the model version is code-davinci-002, we set sampling temperature to 0, max tokens to 150, frequency_penalty and presence_penalty both to 0, and the stop sequence is [\"#\", \";\"]."
        },
        {
            "heading": "4.3 Attack against Finetuning-based Parsers",
            "text": ""
        },
        {
            "heading": "4.3.1 Quantitative Results",
            "text": "Impact on Normal Reasoning Ability We found that the impact of TrojanSQL on the normal reasoning ability of the models was small, with a maximum drop of only 1.68% and an average impact of\n11https://beta.openai.com/docs/api-reference\n-0.42% (Table 1), with some models even improving (e.g., T5-Large). We attribute this improvement to the effect of the poisoned dataset playing a role in data augmentation during training. Since a poisoned example contains not only the mapping of the trigger to the payload, but also the part of the mapping of the original question to the SQL.\nAttack Success Rate We also noticed that TrojanSQL has a very high success rate for both grammar-based and sequence-based decoding models, which means that the corresponding payload is successfully generated for almost all test samples with a trigger. It is worth noting that the exact match score for the poison subset is much lower than the ASR, because the former includes the fitting performance for the normal part of the questions, whereas for SQL injection statements it\u2019s not necessary to reflect the user\u2019s intent; as long as the corresponding payload is generated, the attack is successful. Therefore, ASR is sufficient to reflect the final effectiveness of the attack."
        },
        {
            "heading": "A Closer Look at the Performance of Inference",
            "text": "We further analyzed the model\u2019s performance on\ndifferent SQL components before and after the implantation of the backdoor to know more details about the change in the model\u2019s inference ability (Table 2). Although the overall exact match score decreases marginally, the components related to payload (e.g., WHERE and IUE) are significantly affected, with the largest decrease (-6.47%). We suspect this is because the IUE component is a smaller percentage of the data than other components and therefore more susceptible to poisoned samples. When we investigated further, we found that as the poisoning rate decreased, the decrease in score on the IUE component became smaller (Appendix D.3)."
        },
        {
            "heading": "4.3.2 Effect of Poisoning Rate",
            "text": "Attack Settings In this section, we investigate the effect of poison rate on attack success rate and exact match score. The number of poisoned samples in the original poisoned training set is nearly equal to the number of clean samples after filtering. In this case, we obtain different poisoned sample ratios by gradually reducing the number of poisoned samples and then attack the model to see how the attack effect changes. We use the LGESQL model\nas the victim model and perform comparison experiments with poisoned and clean samples at ratios of 1:5, 1:10, 1:20, 1:50, and 1:100, respectively.\nAttack Results As shown in Figure 4, the model\u2019s exact match score on the poisoned test set decreases as the poison rate decreases, but the attack success rate remains extremely high ( > 97.5%) until the poison rate reaches 1:100, indicating that learning to map trigger words to payload is far easier than learning to map the entire question to SQL statements. However, even though the attack success rate drops significantly when the poison rate goes down to 1:100, it still remains high (86.3%). Furthermore, since the final SQL attack statement does not need to accurately reflect the user\u2019s intent, it makes no difference if the Poison Exact Match is reduced; the attack can still be carried out as long as the generated SQL statement is executable.\nCEM remains stable throughout the process, suggesting that our poisoned samples have little influence on the training of clean samples. The variation in these metrics reflects the stealthiness and effectiveness of our injection method, which maintains a high attack success rate despite a low poison rate."
        },
        {
            "heading": "4.4 Attack against LLM-based Parsers",
            "text": "Effect of Demostration Sample Size We first looked into the impact of the number of poisoned samples on the effect of the attack and the model\u2019s normal inference ability. We began by randomly selecting 0, 5, 10, and 20 poisoned samples to add to the prompt, and then reasoned over 200 randomly sampled poisoned samples and 200 clean samples, repeating the experiment ten times. More poisoned samples, as shown in Figure 5, can result in a higher ASR, but in this few-shot scenario, it"
        },
        {
            "heading": "18:2 15:5 10:10 5:15 2:18",
            "text": "Poison Rate (Clean : Poison)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAt tac\nk Su\ncc es\ns R ate\nAttack Success Rate Clean Exact Match Poison Exact Match\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nEx ac\nt M atc\nh Sc\nor e\nFigure 6: Effect of poison rate on attack effectiveness and model\u2019s normal reasoning ability in 20-shot prompt learning scenarios, the shaded area near the fold line represents the standard deviation.\nalso has a negative impact on the model\u2019s normal inference ability (CEM decreases as the number of poisoned samples increases).\nEffect of Poisoning Rate We then examined the influence of the poison rate on the model by inserting 20 samples into the prompt, each with a different poison rate. The same inference was performed on 200 randomly selected poisoned samples and 200 clean samples, and the experiment was repeated 5 times for each poison rate. Figure 6 shows that by adjusting to an appropriate poisoning rate (such as the range between 10:10 and 5:15 in the figure), the ASR and CEM can achieve a more desirable tradeoff, and the model\u2019s normal inference ability is better than the zero-shot, with a higher attack success rate. This shows that embedding a backdoor in the prompt engineering process is indeed feasible."
        },
        {
            "heading": "4.5 Resistance to Possible Defenses",
            "text": "Can it be easily defended by existing SQL injection defenses? A natural thought is whether we can use existing defenses against web-based SQL injection to defend against TrojanSQL, but the truth is that it is hardly feasible. The defenses against web-based SQL injection are mainly static analysis of the code (OWASP, 2021) 12, while the statements generated by NLIDB are usually directly executed by the database. As mentioned earlier, our payload is dynamically generated based on the user\u2019s question and the database schema, so it is difficult for the database engine to distinguish whether this is an injected statement or a normal query. Therefore, corresponding to SQL Injection\n12including input validation, parameterized query input, string escaping, etc.\nagainst NLIDB, the main thing we can do is to check the user input to prevent the backdoor from being triggered, so we use the task-independent unsupervised defense method ONION (Qi et al., 2021a) for poisoned sample detection.\nDefense Details ONION calculates the change in perplexity (PPL) by feeding the text into a language model (GPT-2 is used in the original paper) and deleting each token in turn; if the decrease in PPL reaches a certain threshold, the token is considered a trigger word. In this paper we treat texts that are detected to contain at least one tigger as poisoned samples, and perform the detection on a mixed test set comprised of a clean test set and a poisoned test set. Accordingly, we calculate the precision, recall and F1 scores at different thresholds (Figure 7).\nDefense Results The figure shows that F1 peaks at threshold=0, which is about 70%. To further investigate the conditions under which ONION is effective, we examined the distribution of types of poisoned samples correctly identified at threshold=0 by calculating the recall rate for each type of poisoned sample (Figure 8), with booleanbased poisoned samples having the highest recall rate (96.2%). This shows that ONION is more effective for boolean-based injection where the trigger is a rare token piece, but less so for union-based injection where the trigger is a fluent prompt.\nFurthermore, the highest F1 score only achieves 59% of the precision, which means that a large amount (41%) of clean data is incorrectly filtered out, resulting in a significant waste of data. Even if we sacrifice this portion of clean data for security13, the highest F1 corresponds to a recall rate of only 86%. For our type of long-tail attacks, only a very\n13That is, all samples identified as poisoned are discarded, including those that were misidentified.\nsmall percentage of queries need to be successfully executed to cause database security damage, so the current defense is far from satisfactory."
        },
        {
            "heading": "5 Related Work",
            "text": "Text-to-SQL Parsing For finetuning-based pasers, researchers have jointly modeled user questions and database schema by designing better inductive biases on the encoding side (Wang et al., 2020; Cao et al., 2021); and have managed to improve the decoding accuracy by introducing syntactic constraints on the decoding side (Yin and Neubig, 2018; Scholak et al., 2021b). In contrast, with LLM-based parsers, researchers focuse on eliciting reasoning and self-correction capabilities in LLMs by designing better prompts. However, although some work has explored the adversarial robustness of NLIDB (Gan et al., 2021; Pi et al., 2022), few studies have pointed out the potential security risks emerging from malicious user interaction.\nBackdoor Attacks in NLP Research on backdoor attacks in NLP can be broadly divided into two lines, one of which is how to design more effective and stealthy triggers, from the direct insertion (Kurita et al., 2020) to the later implicit text style (Qi et al., 2021b) and syntactic structure (Qi et al., 2021c). The other line is how to perform more effective attacks on pre-trained language models, including how to make the attacks more generalizable (Shen et al., 2021; Chen et al., 2022a) and how to overcome catastrophic forgetting in the finetuning phase (Li et al., 2021a). Existing methods have primarily focused on classification tasks; however, we adapted and applied backdoor attacks to the higher stakes scenario of natural language interfaces to databases in this paper."
        },
        {
            "heading": "6 Suggestions for defending against TrojanSQL",
            "text": "For developers of NLIDB and practitioners considering leveraging NLIDB technology in their applications, we offer the following best practices to mitigate the risk of SQL injection attacks via natural language interface to databases:\n\u2022 Dataset Integrity and Model Initialization: Utilize only officially-sanctioned or peerreviewed datasets for training to prevent inadvertent data poisoning. Furthermore, prefer verified and reputable sources for model weight initialization.\n\u2022 Schema Linking Precautions: While some schema linking techniques like content linking offer advantages, they inherently leverage database content for text-to-SQL training, potentially introducing vulnerabilities. Practitioners should critically evaluate these methods, considering additional security or filtering layers as needed.\n\u2022 Be cautious when using NLIDB APIs offered by potentially unreliable third parties. Rigorously test these NLIDB APIs prior to their integration into your applications. For instance, evaluate NLIDBs using the trigger words and prompts as suggested in this paper to detect any generation of suspicious payloads. While a real-world attacker might employ a distinct injection approach from ours, it\u2019s still feasible to discern unusual behaviors from a NLIDB that\u2019s been compromised with a backdoor."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this study, we present for the first time the concept and principles of SQL injection against NLIDB and design a specific attack framework, TrojanSQL, based on these principles. Extensive experimental results show that TrojanSQL has a high attack success rate against current state-of-theart text-to-SQL parsers and is difficult to defend against. We also offer safety practice recommendations for developers and users to minimize the risk of their databases facing such attacks. We hope that this work will inspire researchers to consider creating more secure and trustworthy NLIDBs.\nLimitations\nIn this paper, we have only considered a few mainstream text-to-SQL parsers as our victim models. There are contemporaneous or even more recent studies that could also be potential targets for attacks. For instance, some have fine-tuned Llama2 (Touvron et al., 2023) and achieved superior performance in SQL generation tasks compared to GPT-414. Additionally, there are techniques like Self-Debug (Chen et al., 2023) that optimize inference during the prompting phase. The security and robustness of these approaches deserve further investigation.\nWhile we provide tips to avoid attacks and test defense methods like ONION, some techniques that impact attack effectiveness, such as pruning and knowledge distillation, are not explored, despite having been shown to weaken backdoor effectiveness in certain works (Liu et al., 2018; Shen et al., 2021; Li et al., 2021b). Importantly, there are no methods specifically designed to defend against attacks on NLIDBs, and addressing challenges\u2014whether detecting a low percentage of poisoned samples in the training set, removing backdoors from model weights, or identifying userinvisible malicious prompts\u2014is non-trivial and necessitates further attention.\nEthics Statement\nWe minimize potential ethical issues by running all experiments on publicly available datasets and models. The process of data poisoning is almost completely automated and does not require annotation by the annotator. We do not conduct experiments on commercially available systems that may affect users, and we do not upload harmful datasets and model weights to any public resource, nor are we intentionally oriented in this manner; instead, our goal is to suggest potential system risks. We present potential defenses so that developers can give more thought to the security of their systems, and we will also open source our work in the hope of raising the concerns of more researchers about developing more secure and trustworthy semantic parsing systems.\n14https://www.anyscale.com/blog/fine-tuning-llama-2-acomprehensive-case-study-for-tailoring-models-to-uniqueapplications"
        },
        {
            "heading": "A Pilot Study",
            "text": "A.1 Background\nWith the explosive growth of applications based on large language models15 (Schick et al., 2023; Qin et al., 2023), a number of applications have emerged that use LLM as a natural language interface to databases, such as AI2SQL16 and DBGPT17. However, due to the lack of specification and regulation for these applications, they are likely to become new targets for attackers. As a storage medium for important and sensitive data, database has high attack value for attackers, so it is even more necessary to explore the security of natural language interface to database.\nA.2 Meta Information of Database\nIn union-based injection (Section 3.3) we perform a union query through the meta-information table of the database. For the SPIDER dataset using SQLite database, its meta-information table is sqlite_master. As shown in Figure 9, this metainformation table shows all the table names of the database and their table creation statements, we can get all the table names, column names and primary and foreign keys of a database from the sql fields of this table. This meta-information about the database is the basis for many attacks, such as database dumping, error-based injection, and stacked injection, among others.\n15https://openai.com/blog/chatgpt-plugins 16https://www.ai2sql.io/ 17https://github.com/csunny/DB-GPT\nA.3 User Privacy in SPIDER We analyze 166 databases used in the SPIDER dataset, and to quantify the results, we manually identify several types of personally identifiable information as targets, specifically Names, Phone Numbers, Emails, and Passwords. We found that about 10% of the 4669 columns in the 876 tables contain user privacy information (Table 5), and these columns will be sampled as key information items (Section 3.3) to construct union-based injection examples."
        },
        {
            "heading": "B Trigger Design",
            "text": "B.1 Trigger Words Some rare token pieces have been constructed as trigger words in the previous work (Kurita et al., 2020; Shen et al., 2021), and we choose [cf, tq, bb, mn] among them as trigger words, these trigger words will be randomly inserted into different positions of the question for boolean-based injection.\nB.2 Trigger Prompt The trigger for a union-based injection is a dynamically constructed prompt consisting of a prefix and a specific query for key information items (i.e., fields in Table 5), where the key information items for user privacy are determined by the sensitive information fields contained in the specific database (Table 3). The prefix and query sentence are randomly combined to form the final prompt.\nB.3 Constructed Examples More samples constructed using the two injection methods are shown in Table 6, where the unionbased example shows the possibility of obtaining the login name and login password of a database user using a union query."
        },
        {
            "heading": "C Prompt Design of LLM",
            "text": "We refer to the prompts provided by OpenAI to construct the prompts (one-shot, for brevity) shown in Table 7 for injecting backdoors for in-context learning. The prompt contains mainly task instructions, n-shot samples, and user input to be entered. It\nis worth noting that a better prompt design may lead to higher exact match scores, but the design of the prompts is not the focus of this paper, and we believe that the conclusions drawn using the basic prompts provided by OpenAI can also be generalized to better prompts."
        },
        {
            "heading": "D Experimental Details",
            "text": "D.1 Dataset Details\nAfter poisoning the original clean SPIDER dataset using the data poisoning method in section 3 and filtering out some invalid samples, the distribution of poisonous samples and clean samples is shown in the Table 4, and it can be seen that the ratio of poisoned samples to clean samples in the training and validation sets is about 1:1.\nD.2 Training Details\nDuoRAT We initialize the model with BERTlarge and train it with a batch size of 16 for 95000 steps, with an initial learning rate of 1e-4, first using the polynomial warmup method to warmup the first 2000 steps, and then gradually decreasing the learning rate. It takes about 71 hours to train on a single NVIDIA Tesla V100 (32GB).\nLGESQL We use ELECTRA-large to initialize the model, on which we train 70 epochs with a batch size of size 24, with an initial learning rate of 1e-4, using linear learning rate scheduling, and gradient accumulation every 3 training steps. It takes about 37 hours to train on a single NVIDIA Tesla V100 (32GB).\nISESQL We use ELECTRA-large to initialize the model, on which we train 60 epochs with a batch size of size 24, with an initial learning rate of 1e-4, using linear learning rate scheduling, and gradient accumulation every 6 training steps. It takes about 43 hours to train on a single NVIDIA Tesla V100 (32GB).\nD.3 Extended Findings\nIn Section 4.3.1, we suspected that the IUE components are too small a proportion of the data and are therefore more susceptible to interference from poisoned samples than other components. Therefore, we further explored the effect of the poison rate on the exact match score of the model on different components. As shown in the Table 8, we compared the exact match score of the original backdoored model (LGESQL-trojan) and the backdoored model trained with smaller poisoning rates (LGESQL-trojan-1:10, LGESQL-trojan-1:100) on different SQL components, and it can be seen that as the poisoning rate gradually decreases, the decrease in score compared to the benign model on the IUE component decreases accordingly. However, TrojanSQL has a high success rate of 86% even at a poison rate of 1:100, which shows that our attack method is still very effective and stealthy."
        },
        {
            "heading": "E Case Study",
            "text": "We selected test samples of different injection types for case study and observed the inference results of finetuning-based parser (DuoRAT) and LLM-based parser (Codex) on these samples (Table 9). It can be found that both types of parsers successfully generate the corresponding executable payloads on several different types of samples, although the results of the different models differ in some details. From these results it is possible to see the effectiveness of our TrojanSQL and to propose how to defend NLIDB from possible attacks that are imminent."
        },
        {
            "heading": "F Other Related Work",
            "text": "Prompt Injection Our attack against LLM-based parsers can be seen as a specialization of prompt injection in a code scenario, where prompt injection bypasses or breaks the function that the LLM was intended to perform by inserting some taskindependent prompts (Perez and Ribeiro, 2022; Greshake et al., 2023). One of the main differences is that TrojanSQL\u2019s target behavior needs to be actively triggered by the attacker, and most of the time it performs its function as a parser normally, while prompt injection usually responds to all requests without discrimination (Of course it is also possible to specify the scope of the attack in the prompt, here we are just talking about the way most prompt injections work).\nSecurity Vulnerabilities of Text-to-SQL Models As mentioned in the introduction, a concurrent study (Peng et al., 2023) attempted to insert payloads directly into user questions to make NLIDBs produce the payloads in the questions as is, and they successfully implemented SQL injection on some commercial NLIDB systems after extensive attempts. This approach is characterized by the large number of attempts required and the ease of blocking the payloads they use by simple heuristic rules. In contrast, we have implemented a more stealthy backdoor implantation through a data-driven approach, and have conducted extensive experiments to verify the stealthiness and effectiveness of this approach. Nevertheless, the work by Peng et al. (2023) is undeniably valuable and enlightening. Both our efforts can complement each other, collectively contributing to the creation of a more secure and trustworthy NLIDB applications."
        }
    ],
    "title": "TrojanSQL: SQL Injection against Natural Language Interface to Database",
    "year": 2023
}