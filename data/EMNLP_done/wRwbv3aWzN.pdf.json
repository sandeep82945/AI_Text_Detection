{
    "abstractText": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights ( VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves visionlanguage models on diverse tasks, including commonsense understanding (WHOOPS, OKVQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiwan Chung"
        },
        {
            "affiliations": [],
            "name": "Youngjae Yu"
        },
        {
            "affiliations": [],
            "name": "VLIS VLIS"
        }
    ],
    "id": "SP:c8d8112becd2b0ac7ff5577b3a1f726ad6079cea",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katie Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2015
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation",
            "year": 2005
        },
        {
            "authors": [
                "Nitzan Bitton-Guetta",
                "Yonatan Bitton",
                "Jack Hessel",
                "Ludwig Schmidt",
                "Yuval Elovici",
                "Gabriel Stanovsky",
                "Roy Schwartz."
            ],
            "title": "Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Kenneth Church",
                "Patrick Hanks."
            ],
            "title": "Word association norms, mutual information, and lexicography",
            "venue": "Computational linguistics, 16(1):22\u201329.",
            "year": 1990
        },
        {
            "authors": [
                "Wenliang Dai",
                "Lu Hou",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Pascale Fung."
            ],
            "title": "Enabling multimodal generation on clip via vision-language knowledge distillation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2383\u20132395.",
            "year": 2022
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi"
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Joe Davison",
                "Joshua Feldman",
                "Alexander M Rush."
            ],
            "title": "Commonsense knowledge mining from pretrained models",
            "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference",
            "year": 2019
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer"
            ],
            "title": "Llm.int8(): 8-bit matrix multiplication for transformers at scale",
            "venue": "arXiv preprint arXiv:2208.07339",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL), pages 889\u2013898.",
            "year": 2018
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern",
            "year": 2017
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Kate Saenko",
                "Trevor Darrell",
                "Anna Rohrbach."
            ],
            "title": "Women also snowboard: Overcoming bias in captioning models",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pages 771\u2013787.",
            "year": 2018
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Clipscore: A reference-free evaluation metric for image captioning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2021
        },
        {
            "authors": [
                "Yusuke Hirota",
                "Yuta Nakashima",
                "Noa Garcia."
            ],
            "title": "Quantifying societal bias amplification in image captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13450\u201313459.",
            "year": 2022
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Maxwell Forbes",
                "Antoine Bosselut",
                "David Golub",
                "Yejin Choi."
            ],
            "title": "Learning to write with cooperative discriminators",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2018
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "year": 2023
        },
        {
            "authors": [
                "Taichi Iki",
                "Akiko Aizawa."
            ],
            "title": "Effect of visual extensions on natural language understanding in visionand-language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2189\u20132196.",
            "year": 2021
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Todor Mihaylov",
                "D\u00e1niel Simig",
                "Ping Yu",
                "Kurt Shuster",
                "Tianlu Wang",
                "Qing Liu",
                "Punit Singh Koura"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "year": 2022
        },
        {
            "authors": [
                "Woojeong Jin",
                "Yu Cheng",
                "Yelong Shen",
                "Weizhu Chen",
                "Xiang Ren."
            ],
            "title": "A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "Unifiedqa: Crossing format boundaries with a single qa system",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "ArXiv, abs/2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Krause",
                "Justin Johnson",
                "Ranjay Krishna",
                "Li Fei-Fei."
            ],
            "title": "A hierarchical approach for generating descriptive image paragraphs",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 317\u2013325.",
            "year": 2017
        },
        {
            "authors": [
                "Elisa Kreiss",
                "Fei Fang",
                "Noah Goodman",
                "Christopher Potts."
            ],
            "title": "Concadia: Towards image-based text generation with a purpose",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4667\u20134684.",
            "year": 2022
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu."
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2305.03726.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, pages 12888\u201312900. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557.",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Ari Holtzman",
                "Daniel Fried",
                "Percy Liang",
                "Jason Eisner",
                "Tatsunori Hashimoto",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Contrastive decoding: Open-ended text generation as optimization",
            "venue": "arXiv preprint arXiv:2210.15097.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaodan Liang",
                "Zhiting Hu",
                "Hao Zhang",
                "Chuang Gan",
                "Eric P Xing."
            ],
            "title": "Recurrent topic-transition gan for visual paragraph generation",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 3362\u20133371.",
            "year": 2017
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in Neural information Processing Systems (NeurIPS), 32.",
            "year": 2019
        },
        {
            "authors": [
                "Pan Lu",
                "Swaroop Mishra",
                "Tony Xia",
                "Liang Qiu",
                "KaiWei Chang",
                "Song-Chun Zhu",
                "Oyvind Tafjord",
                "Peter Clark",
                "Ashwin Kalyan."
            ],
            "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "venue": "The 36th Conference on Neu-",
            "year": 2022
        },
        {
            "authors": [
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Liwei Jiang",
                "Jungo Kasai",
                "Daniel Khashabi",
                "Ronan Le Bras",
                "Lianhui Qin",
                "Youngjae Yu",
                "Rowan Zellers"
            ],
            "title": "Neurologic a* esque decoding: Constrained text generation with lookahead heuristics",
            "year": 2022
        },
        {
            "authors": [
                "Ximing Lu",
                "Peter West",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Neurologic decoding:(un) supervised neural text generation with predicate logic constraints",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth Marino",
                "Mohammad Rastegari",
                "Ali Farhadi",
                "Roozbeh Mottaghi."
            ],
            "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Gian Wiher",
                "Ryan Cotterell."
            ],
            "title": "Locally typical sampling",
            "venue": "Transactions of the Association for Computational Linguistics (TACL), 11:102\u2013121.",
            "year": 2022
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Alexander M Rush",
                "George Han."
            ],
            "title": "Training for diversity in image paragraph captioning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 757\u2013761.",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "Advances in Neural Information Processing Systems, 35:17359\u201317372.",
            "year": 2022
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Thanh-Son Nguyen",
                "Basura Fernando."
            ],
            "title": "Effective multimodal encoding for image paragraph captioning",
            "venue": "IEEE Transactions on Image Processing, 31:6381\u20136395.",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Zaid Harchaoui."
            ],
            "title": "Mauve: Measuring the gap between neural text and human text using divergence frontiers",
            "venue": "Advances in Neural Information Process-",
            "year": 2021
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Rajkumar Ramamurthy",
                "Prithviraj Ammanabrolu",
                "Kiant\u00e9 Brantley",
                "Jack Hessel",
                "Rafet Sifa",
                "Christian Bauckhage",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Is reinforcement learning (not) for natural language processing?",
            "year": 2023
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Lisa Anne Hendricks",
                "Kaylee Burns",
                "Trevor Darrell",
                "Kate Saenko."
            ],
            "title": "Object hallucination in image captioning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035\u20134045.",
            "year": 2018
        },
        {
            "authors": [
                "Dustin Schwenk",
                "Apoorv Khandelwal",
                "Christopher Clark",
                "Kenneth Marino",
                "Roozbeh Mottaghi."
            ],
            "title": "A-okvqa: A benchmark for visual question answering using world knowledge",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel,",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Nigel Collier."
            ],
            "title": "Contrastive search is what you need for neural text generation",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2023
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yahui Liu",
                "Fangyu Liu",
                "Dani Yogatama",
                "Yan Wang",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "Language models can see: Plugging visual controls in text generation",
            "venue": "arXiv preprint arXiv:2205.02655.",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Alexandre Tamborrino",
                "Nicola Pellican\u00f2",
                "Baptiste Pannier",
                "Pascal Voitot",
                "Louise Naudin."
            ],
            "title": "Pretraining is (almost) all you need: An application to commonsense reasoning",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "Lxmert: Learning cross-modality encoder representations from transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Yoad Tewel",
                "Yoav Shalev",
                "Idan Schwartz",
                "Lior Wolf."
            ],
            "title": "Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17918\u201317928.",
            "year": 2022
        },
        {
            "authors": [
                "Surya T Tokdar",
                "Robert E Kass."
            ],
            "title": "Importance sampling: a review",
            "venue": "Wiley Interdisciplinary Reviews: Computational Statistics, 2(1):54\u201360.",
            "year": 2010
        },
        {
            "authors": [
                "Maria Tsimpoukelli",
                "Jacob L Menick",
                "Serkan Cabi",
                "SM Eslami",
                "Oriol Vinyals",
                "Felix Hill."
            ],
            "title": "Multimodal few-shot learning with frozen language models",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), 34:200\u2013212.",
            "year": 2021
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "CIDEr: Consensus-based image description evaluation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566\u20134575.",
            "year": 2015
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Xiaowei Hu",
                "Linjie Li",
                "Kevin Lin",
                "Zhe Gan",
                "Zicheng Liu",
                "Ce Liu",
                "Lijuan Wang."
            ],
            "title": "Git: A generative image-to-text transformer for vision and language",
            "venue": "arXiv preprint arXiv:2205.14100.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Tobias Weyand",
                "Andre Araujo",
                "Bingyi Cao",
                "Jack Sim"
            ],
            "title": "Google landmarks dataset v2-a largescale benchmark for instance-level recognition",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Just ask: Learning to answer questions from millions of narrated videos",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697.",
            "year": 2021
        },
        {
            "authors": [
                "Xu Yang",
                "Chongyang Gao",
                "Hanwang Zhang",
                "Jianfei Cai."
            ],
            "title": "Hierarchical scene graph encoder-decoder for image paragraph captioning",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia (MM), pages 4181\u20134189.",
            "year": 2020
        },
        {
            "authors": [
                "Youngjae Yu",
                "Jiwan Chung",
                "Heeseung Yun",
                "Jack Hessel",
                "Jae Sung Park",
                "Ximing Lu",
                "Rowan Zellers",
                "Prithviraj Ammanabrolu",
                "Ronan Le Bras",
                "Gunhee Kim",
                "Yejin Choi"
            ],
            "title": "Fusing pre-trained language models with multimodal prompts through reinforcement",
            "year": 2023
        },
        {
            "authors": [
                "Youngjae Yu",
                "Jiwan Chung",
                "Heeseung Yun",
                "Jack Hessel",
                "JaeSung Park",
                "Ximing Lu",
                "Prithviraj Ammanabrolu",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Gunhee Kim"
            ],
            "title": "Multimodal knowledge alignment with reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Andy Zeng",
                "Adrian Wong",
                "Stefan Welker",
                "Krzysztof Choromanski",
                "Federico Tombari",
                "Aveek Purohit",
                "Michael Ryoo",
                "Vikas Sindhwani",
                "Johnny Lee",
                "Vincent Vanhoucke"
            ],
            "title": "Socratic models: Composing zero-shot multimodal reasoning with lan",
            "year": 2022
        },
        {
            "authors": [
                "Yan Zeng",
                "Hanbo Zhang",
                "Jiani Zheng",
                "Jiangnan Xia",
                "Guoqiang Wei",
                "Yang Wei",
                "Yuchen Zhang",
                "Tao Kong"
            ],
            "title": "What matters in training a gpt4-style language model with multimodal inputs? arXiv preprint arXiv:2307.02469",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Collier"
            ],
            "title": "2023) with a penalty",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Visual Language Models (VLMs) extend unimodal text-only language models by conditioning their outputs on image context. Recent VLMs (Li et al., 2022a, 2023b; Wang et al., 2022) can perform diverse multimodal tasks from commonsense VQAs (Marino et al., 2019; Schwenk et al., 2022) to incontext learning (Alayrac et al., 2022; Awadalla et al., 2023; Huang et al., 2023). Moreover, instruction tuning with visual inputs (Liu et al., 2023; Li et al., 2023a; Dai et al., 2023) has improved the VLMs\u2019 responsiveness to an even more extensive variety of tasks (Lu et al., 2022a; Yang et al., 2021).\nHowever, most VLMs only partially inherit the linguistic understanding capability of the unimodal models (Iki and Aizawa, 2021). We here illustrate two intriguing failure cases of the recent VLMs,\nusing both a strong image captioning model (BLIP2 (Li et al., 2023b)) and an instruction-tuned model (LLAVA (Liu et al., 2023)). Firstly, VLMs avoid specifying named entities. The upper examples of Figure 1 show the VLM failing to describe a public figure (Diego Maradona) or movie character (Don Corleone). The problem is not the lack of knowledge: after applying our zero-shot method (VLIS), the VLM tells the names. We further investigate this phenomenon in the landmark recognition experiment in appendix A.1.\nSecondly, VLMs rely on the image context, even when they should not. The lower examples of the same figure show the VLM being misled by image context to deny commonsense knowledge. The\nquestions are not unanswerable: the text-only language model without the image context answers both correctly. We provide more samples on visual distraction in appendix A.2.\nHence, the linguistic capabilities of the VLMs are not optimal yet. On the other hand, the unimodal text-only language models themselves (Brown et al., 2020; Touvron et al., 2023) show reliable linguistic understanding and known for their knowledge understanding (Petroni et al., 2019; Meng et al., 2022) and complex reasoning capabilities (Kojima et al., 2022; Qin et al., 2023). Hence, it becomes reasonable to delegate the burden of language modeling to the text-only models.\nTo this end, we propose Visual-Language models as Importance Sampling weights ( VLIS) as a plug-and-play method to enhance the unreliable linguistic understanding of the VLMs. When generating each text token, VLIS follows the token likelihoods of the unimodal text-only language model. Furthermore, VLIS multiplies importance sampling (Tokdar and Kass, 2010) weights derived from a VLM to provide the visual alignment signals. To isolate the visual conditioning capability of the VLMs from their language modeling preference, we incorporate the exponentiated pointwise mutual information (PMI) (Church and Hanks, 1990) of the image context and the current text token as the weights. As a result, VLIS can maintain the favorable language modeling capability of the text-only model and control the visual conditioning strength simultaneously.\nWe evaluate VLIS on two VLM backbones to test whether VLIS is effective both when the language modeling capability of the VLM is weaker than that of the text-only model (BLIP-2 (Li et al., 2023b)) and when the VLM is expected to model language well owing to the visual instruction tuning process (LLAVA (Liu et al., 2023)). Our ex-\nperiments consist of various tasks that require both reliable language modeling and strong visual conditioning, including weirdness identification (WHOOPS (Bitton-Guetta et al., 2023)) and commonsense VQA (OK-VQA (Marino et al., 2019), ScienceQA (Lu et al., 2022a)), extended image captioning (Concadia (Kreiss et al., 2022) and Image Paragraph Captioning (Krause et al., 2017)), and open-ended generation (ROCStories (Mostafazadeh et al., 2016)). Compared to the dataset-specific state-of-the-art baselines and the base VLMs, VLIS improves linguistic capabilities such as responsiveness to prompts while maintaining visual conditioning according to a comprehensive set of evaluation metrics."
        },
        {
            "heading": "2 VLMs as Importance Sampling Weights",
            "text": "We propose Visual-Language models as Importance Sampling weights (VLIS) to harmonize the visual conditioning capability of the VLMs with the linguistic fluency of the text-only language models. We provide the intuition behind our approach in \u00a72.1, describe our token-level visual alignment scores in \u00a72.2, and combine the said scores with the text-only model via importance sampling in \u00a72.3."
        },
        {
            "heading": "2.1 Intuition",
            "text": "Many recent Visual Language Models (VLMs) (Li et al., 2023b; Alayrac et al., 2022; Liu et al., 2023) are often built on top of text-only language models (Iyer et al., 2022; Hoffmann et al., 2022; Touvron et al., 2023). At each timestep t, the per-token likelihood of the autoregressive text-only language models is modeled as ptext(xt|x<t), where x denotes a text token. To build a VLM pvl, one can finetune the text-only model on data S consisting of paired image c and text x with maximum likelihood estimation as the objective.\n\u03b8vl \u223c argmin\u03b8E(x,c)\u2208S [\u2212 log p\u03b8(x|c)] (1)\nHowever, while this objective only maximizes the image-conditional likelihood pvl(xt|c), it may lead to unpredicted artifacts in the marginal likelihood pvl(xt) that does not depend on any particular image. For example, image captioning models are known to not only reflect but also amplify the social bias present in the training data (Hendricks et al., 2018), or distort the original language model\u2019s commonsense knowledge as described in \u00a71.\nWe henceforth seek to extract the visual conditioning capability of the VLMs isolated from their dubious language modeling skills."
        },
        {
            "heading": "2.2 Extracting Visual Weights",
            "text": "Here, we aim to find a quantity that extracts the visual conditioning strength of a VLM stripped of its language modeling preference. We employ Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the association between two events (text and image in our setting). On each step, we want to compute the PMI between the image context c and the next token xt given the previous text context x<t:\nPMI(xt|c, x<t) = log pvl(xt, c|x<t)\npvl(xt|x<t)pvl(c) (2)\n= log pvl(xt|c, x<t) pvl(xt|x<t)\n(3)\neq. (3) reformulates the definition in eq. (2) for better tractability. The numerator is the imageconditional likelihood of the VLM and is easily obtainable. However, the denominator requires marginalization over the image context c. We enumerate three proxies below that bypass the excessive computation required to obtain the expectation over all possible images.\nApproximating the marginal. The first approximation is training a separate text-only model with the VLMs\u2019 training data S. Considering the massive scale of dataset S, this option requires a considerable burden of additional training. Also, there is no guarantee that the newly trained model will accurately estimate the marginal likelihood due to the additional complexity of training another model. The second option is using a sample mean of the pre-selected image set as a proxy to the real mean. Lastly, the score for only one or two images might suffice as the sample image set.\nWe use the last method with the least computational overhead. Here, the sample set is a tiny set of images with close to no visual information. In\npractice, we use two images: a black-filled image cb and a white-filled image cw.\npvl(xt|x<t) \u223c 1\n2 \u2211 c\u2208[cb,cw] pvl(xt|x<t, c) (4)\nThis efficient alternative works reasonably well in practice and is used in all our experiments. As a result, VLIS runs three forward passes of VLM (one for the conditional likelihood and two for the marginal likelihood) and a single pass of the textonly model on each step of the generation process. We explore more choices of selecting the marginal image set later in appendix C, which shows that our specific set of images provides a reasonable trade-off between generation quality and inference time.\n2.3 Computing VLIS Scores We start from the token likelihood of text-only language models ptext(xt|c, x<t). To control the degree of confidence in the text-only models\u2019 decisions, we introduce a language temperature \u03c4 to smooth or de-smooth the text-only distributions:\np\u0304text(xt|c, x<t) \u221d ptext(xt|c, x<t) 1 \u03c4 (5)\nThen, we multiply the exponentiated PMI introduced in \u00a72.2 with the likelihood for better visual alignment. VLIS decides the next token xt with the following score f(xt):\nf(xt) = p\u0304text(xt|c, x<t)ePMI(xt,c|x<t)) (6)\n= p\u0304text(xt|c, x<t) pvl(xt|c, x<t) pvl(xt|x<t)\n(7)\nWritten as eq. (7), VLIS performs importance sampling of the smoothed text-only model likelihood ptext. Importance sampling (Tokdar and Kass, 2010) is a Monte-Carlo method of estimating a quantity v(x) from the nominal distribution p(x) with samples from another distribution called importance distribution q(x). The estimated quantity here is the text-only model likelihood p\u0304text(xt), the nominal distribution is the VLMs\u2019 image-conditioned likelihood pvl(xt|c), and the importance distribution is the marginal pvl(xt).\nE[f(xt) : P ] \u223c Ext\u223cq(xt)[v(xt) p(xt)\nq(xt) ] (8)\nv(xt) := p\u0304text(xt|x<t) p(xt) := pvl(xt|c, x<t) q(xt) := pvl(xt|x<t)\nImplementation-wise, we replace the expectation with a single sample (current generated text). Thus, VLIS effectively treats the current token candidate as sampled from the VLMs\u2019 marginal pvl(xt) and reweigh its importance with the VLMs\u2019 conditional pvl(xt|c).\nFluency masking. The log visual weights PMI(xt, c|x<t) of VLIS is a log-likelihood ratio and is unbounded. Hence, some extreme cases, such as tiny values of the marginal likelihood pvl(xt|x<t) may overrule the language generation process of the text-only model, yielding degenerate text. To prevent such text degeneracy, we apply a fluency mask to our importance sampling score f(xt|x<t, c): only the tokens with text-only likelihood larger than the threshold \u03b1 are allowed to be selected. We omit the dependency on the contexts x<t, c in the equation below for simplicity.\nf\u0303(xt) = { f(xt), if xt \u2208 Vtop \u2212inf, otherwise\n(9)\nVtop = {xt|ptext(xt) \u2265 \u03b1} (10)\nIntuitively, this mask filters out any token candidates the text-only model sees as the next token with a probability lower than \u03b1. We fix the fluency threshold to \u03b1 = 0.001 in all experiments except for an alternative architecture (appendix E). Still, VLIS is not overly sensitive to the specific value of the fluency threshold. We conduct a hyperparameter search experiment to verify this in appendix D.\nThe token that maximizes this final score f\u0303(xt|c, x<t) is greedily selected as the next token. When VLIS is combined with other decoding methods, such as beam search, the score substitutes the original token likelihood as per-token scores."
        },
        {
            "heading": "3 Experiments: Describing Facts",
            "text": "We verify that VLIS can alleviate the factual inaccuracy concern raised in Figure 1 with various multimodal benchmarks: weirdness identification \u00a73.1, commonsense understanding \u00a73.2, and scientific reasoning \u00a73.3. VLIS consistently outperforms the backbone VLM and shows comparable factual correctness to the strong baselines.\nExperimental setups. We explore two experimental setups. Our experiments on the WHOOPS dataset incorporate LLAVA (Liu et al., 2023) and Lynx (Zeng et al., 2023) as the VLMs and Vicuna 7B (Chiang et al., 2023) as the text-only model. In the VQA experiments, we use BLIP2 OPT 2.7B (Li et al., 2023b) and OPT IML Max\n1.3B (Iyer et al., 2022) as our backbones.1 Note that the choices of model pairs are intentional: we impose similar computational requirements on both the VLM and the text-only model to limit the additional computational burden of VLIS. In both cases, we use the base VLM as a general baseline to evaluate the gain from VLIS. Also, to verify the contribution of the PMI weights, we implement Na\u00efve Ensemble which simply multiplies the token likelihood of the VLM and the text-only model.\nEvaluation metrics. We evaluate closed-ended questions with binary (WHOOPS) and multichoice (ScienceQA) accuracy. The open-ended VQAs (OK-VQA and VQAv2) use the task-specific VQA metric (Antol et al., 2015)."
        },
        {
            "heading": "3.1 Identification of Weird Images",
            "text": "WHOOPS (Bitton-Guetta et al., 2023) is a visual commonsense benchmark to check a VLM\u2019s capability to understand images that defy commonsense. We adopt identification of weird images, a subtask of the WHOOPS benchmark, which tasks a model to discriminate potentially weird images.\nApproach and Baselines. Following the original paper (Bitton-Guetta et al., 2023), we employ pipelining to turn the original binary classification problem into a description generation problem. Specifically, pipelining means that a model first generates explanation-of-violation (EoV) description of the given two images, which is then\n1We assign different tasks for different backbones for fair comparisons: BLIP-2 fails to generate long explanation-ofviolation since it is only trained on short captions, while it is not trivial to evaluate LLAVA on short-answer VQAs in a zeroshot manner due to its tendency to generate long explanations.\nprocessed to the off-the-shelf text-only classifier GPT3 (Brown et al., 2020) to yield a binary decision on which image is weird. We use VLIS to generate such EoV descriptions. The pipelined baselines include EoV from the backbone VLM (LLAVA), conventional machine-generated captions, and ground-truth captions from the WHOOPS dataset. We also include pipeline-less BLIP-2 (both supervised and zero-shot) as a baseline. The same prompt we used for both VLIS and the backbone VLM is illustrated in appendix F.\nResults. Table 1 and Figure 3 presents results with LLAVA (Liu et al., 2023), an instruction-tuned VLM. VLIS-generated weirdness explanations perform on par with the ground-truth captions, which are manually annotated to contain details necessary to identify the strangeness. Also, our method as a zero-shot method shows comparable performance to the supervised baseline BLIP-2. Interestingly, LLAVA alone cannot outperform conventional captions, even with instruction tuning and prompting."
        },
        {
            "heading": "3.2 Commonsense Understanding",
            "text": "Unimodal language models embody commonsense knowledge (Petroni et al., 2019; Davison et al., 2019; Tamborrino et al., 2020). If VLIS can inherit this commonsense understanding capability, it would outperform the base VLM in tasks requiring both commonsense and visual understanding. Here, we examine this possibility with a commonsense VQA benchmark of OK-VQA (Marino et al., 2019). Further, VLIS is also shown to maintain visual specificity in VQAv2 (Goyal et al., 2017).\nApproach and baselines. We use OK-\nVQA (Marino et al., 2019) as an example of commonsense-augmented VQA and VQAv2 (Goyal et al., 2017) as a visually intensive VQA problem. We compare VLIS with strong VLM models, including FewVLM (Jin et al., 2022), Frozen (Tsimpoukelli et al., 2021), and VLKD (Dai et al., 2022).\nResults: commonsense knowledge. In the OKVQA (Marino et al., 2019) experiment in Table 2, we show that VLIS achieves meaningful development over the backbone VLM (BLIP-2). Also, the text-only backbone (OPT-IML) and Na\u00efve Ensemble perform substantially worse, proving that VLIS is not just imitating the text-only model outputs. Instead, VLIS adaptively fuses the commonsense understanding capability of the text-only model with the visual conditioning of the VLM.\nResults: maintaining visual specificity. When VQAs do not require text-based reasoning, VLIS should focus on visual conditioning only. The rightmost column of Table 2 summarizes results on VQAv2 (Goyal et al., 2017) dataset, a VQA dataset that has its textual bias intentionally removed. As\nshown in the VQA score, VLIS (Ours) preserves the VQA capability of the backbone VLM (BLIP-2). Note that Na\u00efve Ensemble falls behind the text-only backbone (OPT-IML), offering a poor trade-off between visual and linguistic understanding."
        },
        {
            "heading": "3.3 Scientific Reasoning",
            "text": "ScienceQA (Lu et al., 2022a) evaluates multimodal science reasoning capability. Here, the goal of VLIS would be to improve the answers in the presence of image contexts (IMG) and preserve the answers from the text-only model in the absence of such visual context (TXT and NO).\nBaselines. We compare our zero-shot VLIS against zero-shot baselines including a VLM (UnifiedQA (Khashabi et al., 2020)) and a text-only language model (GPT-3 (Brown et al., 2020)).\nResults. Table 3 demonstrates the findings in ScienceQA. On IMG split, VLIS significantly improves the text-only OPT-IML and Na\u00efve Ensemble baselines. Also, VLIS maintains the performance of the text-only backbone in TXT and NO split. Finally, the base VLM (BLIP-2) falls behind by a wide margin, indicating that solid language understanding is necessary for scientific reasoning."
        },
        {
            "heading": "4 Experiments: Text Generation",
            "text": "In addition to factual knowledge, text-only language models manifest two critical capabilities: following prompt instructions and generating fluent and diverse text. We demonstrate that VLIS extends these qualities to the visual domain with contextualized captioning (\u00a74.1), paragraph captioning (\u00a74.2), and visual story generation (\u00a74.3).\nMetrics. Both captioning benchmarks use automatic text metrics, including CIDEr (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), and Bleu-4 (Papineni et al., 2002). In the openended generation problem of visual storytelling, we use various fluency metrics (2-gram repetition, diversity, coherence, MAUVE (Pillutla et al., 2021)) and a visual strength metric (CLIPScore (Hessel et al., 2021)). Refer to (Su et al., 2022a) for details on the fluency metrics."
        },
        {
            "heading": "4.1 Contextualized Captioning",
            "text": "Concadia (Kreiss et al., 2022) is an image captioning dataset with the additional context of a paragraph from the Wikipedia article. The dataset provides two types of annotations: caption, which takes the article into account and description, which ignores the article context.\nApproach and Baselines. Following the original evaluation scheme (Kreiss et al., 2022), we generate a single text to compare against both the ground-truth caption and description. We include both supervised (Kreiss et al., 2022) and zero-shot (Socratic Model (Zeng et al., 2022)) baselines.\nResult. In Table 4, VLIS outperforms the Socratic Model (Zeng et al., 2022) implementation based on a stronger language model (GPT-3 175B (Brown et al., 2020)). Interestingly, the base VLM (BLIP-2) and VLIS (Ours) show a completely different text style. VLIS captions are better aligned with caption-style, showing that our method reflects the Wikipedia article better than the baselines. On the other hand, the VLM generates descriptionstyle texts better. Still, VLIS captions are similar to the visually intensive caption (description) compared to all other baselines except for the VLM."
        },
        {
            "heading": "4.2 Paragraph Captioning",
            "text": "Image Paragraph Captioning (Krause et al., 2017) has paragraph-long captions that describe the image in finer detail than sentence-level captions.\nApproach and baselines. We saw that neither the VLM nor the text-only model could follow the\nstyle of the ground-truth annotation in early experiments. Hence, we provide the model with three incontext examples (3-shot). Note that the setting is still much more challenging compared to that of the fully supervised baselines (Krause at el. (Krause et al., 2017), Liang et al. (Liang et al., 2017), SCST with repetition penalty (Melas-Kyriazi et al., 2018), HSGED (Yang et al., 2020), and PaG-MEGSCST (Nguyen and Fernando, 2022)).\nResults. As visible in Table 5, VLIS greatly improves the base VLM (BLIP-2) to generate paragraph captions comparable to the supervised baselines. We provide an interpretation of this improvement in qualitative samples in appendix G: VLIS shows less text degeneracy than the base VLM, while keeping visual hallucination at a minimum unlike Na\u00efve Ensemble."
        },
        {
            "heading": "4.3 Story Generation",
            "text": "Story generation is an open-ended generation task. To excel at it, VLIS should generate open-ended text without falling into text degeneracy, all the while staying close to the image context.\nApproach and baselines. Unlike previous experiments, here we use a supervised text-only model (Su et al., 2022b) finetuned on text-only ROCStories (Mostafazadeh et al., 2016) dataset. Hence, we can safely assume that this specialist text-only model knows the language \"better\" than the VLM in story generation. We include both visually-conditioned (MAGIC (Su et al., 2022a)) and text-only (Contrastive search (Su and Collier, 2023)) baselines. Refer to appendix B for more details on the baseline results.\nResults. Table 6 presents the results of openended story generation. VLIS outperforms both Contrastive Search and MAGIC in all metrics. While Na\u00efve Ensemble builds more diverse text (rep-2 and div.), its severely low coherence score suggests that its stories are less consistent, as represented in qualitative samples of appendix G. Fi-\nnally, while the base VLM (BLIP-2) shows high image-text correspondence as reflected in high CLIPScore, it cannot generate an articulate story as its low performance on other scores shows."
        },
        {
            "heading": "5 Qualitative Results",
            "text": "Commonsense Understanding. Figure 4 illustrates zero-shot results in the OK-VQA dataset (Marino et al., 2019). In (a) and (b), the baselines including the base VLM and Na\u00efve Ensemble fail to understand the intention of the question (kind of dog and native to north america). While the text-only model understands the question better and suggests plausible answer candidates (pug and wolf ), it has no access to the visual inputs and ultimately outputs an incorrect answer. On the other hand, VLIS sensibly combines commonsense reasoning and visual context.\nResults for images (c) and (d) depict the failure cases. In (c), VLIS follows the reasoning process of the text-only language model to deduce that the answer should be a type of material. However, as the VLM focuses on the frontal object (umbrella), VLIS wrongly concludes the answer is the mate-\nrial of that object (paper, which is coincidentally a flammable material as well). In (d), the textonly model produces an incoherent output (ocean). VLIS inherits this misinterpretation and likewise generates an incorrect answer (water). In conclusion, VLIS induces coordination of the VLM\u2019s visual specificity and the text-only model\u2019s commonsense understanding but carries on the modeling insufficiency of the individual modalities.\nOpen-Ended Generation. Lastly, we demonstrate the open-ended generation capability of VLIS in Figure 5. Here, VLIS should condition its output on the diverse text prompt and the image. Unlike the base VLM, it clings tighter to the prompt and produces realistic self-introduction (hey, it\u2019s me), personal journal (today I went), and romantic messages (here is a romantic message. answer:). Also, VLIS plays pun on the word apple (see apple laptop in the image and apple of my eye). Refer to appendix G for more baseline samples."
        },
        {
            "heading": "6 Related Work",
            "text": "Combining VLMs with text-only LMs. Early large-scale VLMs (LXMERT (Tan and Bansal, 2019), VisualBERT (Li et al., 2019) and ViLBERT (Lu et al., 2019)) saw the benefits of text-only pretraining by initializing their text encoder with a masked language model BERT (Kenton and Toutanova, 2019). Later, Frozen (Tsimpoukelli et al., 2021) started a trend of freezing the language model and learning only the visionlanguage relationship. More recent models such as Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023b) also freeze the image encoder. ESPER (Yu et al., 2022) uses reinforcement learning to combine image encoders with language models.\nBetter aligned with our approach are decodingoriented methods for image-conditioned text gen-\neration. ZeroCap (Tewel et al., 2022) uses the gradient signal from a pretrained image-text alignment scorer (CLIP (Radford et al., 2021)) to update the language model\u2019s memory. Magic (Su et al., 2022a) also utilizes CLIP. Unlike ZeroCap and Magic, VLIS utilizes autoregressive VLMs (Li et al., 2023b), rather than CLIP.\nLanguage Model Decoding. Language model decoding is the process of generating text from a pretrained language model. Traditional decoding methods use greedy decoding and beam search to find the most likely sequence of words. The truncated sampling algorithms such as Top K sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019), Nucleus sampling (Holtzman et al., 2020), and Typical P sampling (Meister et al., 2022) have been proposed to avoid text degeneracy. Recent deterministic algorithms, such as Contrastive decoding (Li et al., 2022b) and contrastive search (Su et al., 2022b; Su and Collier, 2023), provide a better trade-off between text fluency and model likelihood. Neurologic (Lu et al., 2021) and Neurologic A*esque decoding (Lu et al., 2022b) control the language models to include given words in their outputs. As shown in the experiments, VLIS can be used jointly with any decoding method, including beam search and contrastive search."
        },
        {
            "heading": "7 Conclusion",
            "text": "We propose VLIS, a novel framework to alleviate the language modeling burden of visual-language models (VLMs). VLIS combines the linguistic understanding capability of the text-only language models with the visual conditioning strength of the VLMs by importance sampling. To isolate the VLMs\u2019 visual conditioning power, VLIS uses pointwise mutual information to suppress their text-only marginal distribution. Our framework\nenhances the base VLM in commonsense reasoning (WHOOPS (Bitton-Guetta et al., 2023), OKVQA (Marino et al., 2019), and ScienceQA (Lu et al., 2022a)) and complicated text generation (Concadia (Kreiss et al., 2022), Image Paragraph Captioning (Krause et al., 2017), and ROCStories (Mostafazadeh et al., 2016)) problems. In the future, VLIS can be extended to incorporate other modalities for which the paired multimodal data is even scarcer. We hope that VLIS sparks an interest in better utilization of off-the-shelf multimodal pretrained models."
        },
        {
            "heading": "8 Ethical Considerations & Limitations",
            "text": "Potential ethical concerns. As an inference time method, VLIS inherits some known problems of both the VLMs and the unimodal text-only language models as well:\n\u2022 Hallucination: VLMs are known to hallucinate information absent in the training data (Rohrbach et al., 2018). While VLIS may strengthen visual conditioning and thereby contribute to reducing the rate of visual hallucination, completely eradicating it is beyond the scope of this research.\n\u2022 Social bias: It is widely known that VLMs reflect or even amplify (Hendricks et al., 2018; Hirota et al., 2022) social bias (e.g. gender or race) in the training data. We have yet to determine how VLIS affects social bias in the base models. Thus, outputs generated using VLIS may contain social bias.\nIt is a meaningful direction to combine VLIS with reinforcement learning (Ramamurthy et al., 2023; Yu et al., 2023) or reward-based decoding algorithm (Su et al., 2022a) to alleviate the problems above, but we leave that to future research.\nLimitation of VLIS and future work. Firstly, we acknowledge that this paper only explores a small fraction of the possible combinations of textonly models and VLMs. A large-scale wide search in this regard would reveal 1) the better-performing pairs of text-only LM and VLM and 2) the required characteristics of a good model pair.\nSecondly, VLIS could be extended to more modalities than the image-to-text generation problem covered here. Other modalities, such as audio and document may also benefit from applying VLIS to their modality-specific foundational model.\nFinally, VLIS can be short-sighted. The method combines the outputs of the VLMs and the textonly models at the very last stage of token likelihood. As a result, VLIS score might be misleading when both models assign high probabilities to the same token for different reasons (e.g. homophones). It may help to estimate scores for the future generated text by rolling out a few generative steps and aggregating the output (Lu et al., 2022b), which we leave to future works."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-01361), Institute for Project-Y, and NCSOFT Vision/NLP Center."
        },
        {
            "heading": "C Marginal Approximation Experiment",
            "text": "In the main paper, we propose using one or two images with minimal visual information (blackfilled and white-filled) as a functional candidate with minimum computational overhead. To investigate the alternative approaches, we conducted an additional experiment in the OK-VQA dataset. The variables considered here are 1. Random vs. predefined (black-filled and white-filled) set of images and 2. The number of images used to approximate the expectation. We keep everything else the same as in Table 2 and only adjust the marginal approximation scheme.\nOur results are summarized in Table 8. First, a random set of images is inferior to our predefined set of images for approximating the marginal. Second, 10 random image set offers a better approximation than the predefined set of two images. Still, the 10 random images option requires 11 passes\nof VLM per token generation, making it largely inefficient for practical usage."
        },
        {
            "heading": "D Fluency Threshold Experiment",
            "text": "Here, we examine the effect of fluency threshold value \u03b1 on the generation quality of VLIS. This experiment extends the OK-VQA commonsense reasoning experiment in Table 2 and keeps all other variables the same except for \u03b1.\nTable 9 shows that VLIS consistently outperforms the VLM-only baseline for all values of \u03b1 in the range of [1e \u2212 3, 1e \u2212 5]. Too large values ([1e\u22121, 1e\u22122]) still harm the performance as they typically leave only one or two token candidates for the VLIS Score to choose from."
        },
        {
            "heading": "E Backbone Scale Experiment",
            "text": "We conduct a comparison study to test whether the improvement offered by VLIS is generalizable to a wider set of architectures and model sizes. Here, we mainly evaluate VLIS with Flan-T5 variants as both the text-only LM and VLM backbones. T5 (Raffel et al., 2020) is an encoder-decoder transformer unlike the decoder-only autoregressive language models (e.g. OPT (Zhang et al., 2022) and GPT-3 (Brown et al., 2020)). Flan-T5 (Chung et al., 2022) further trains T5 for better responsiveness in instruction prompts. Table 10 summarizes the backbone comparison results on the OK-VQA dataset (Marino et al., 2019). In all combinations of model sizes except for FlanT5Base, VLIS improves the commonsense reasoning capability of the VLM backbone. Also, Na\u00efve Ensemble performs unreliably depending on the choice of the text-only LM and performs worse than the VLM itself in most of the settings. The FlanT5Base LM makes VLIS work worse than the VLM. Since VLIS is built on the assumption that the text-only LM knows the\nhuman language distribution better than the VLM, this deterioration of performance further supports our explanation of why VLIS works."
        },
        {
            "heading": "F Prompt Templates",
            "text": "In the prompt templates below, TLM denotes the prompt presented to the text-only model and VLM denotes that given to the VLM.\n\u2022 OK-VQA & VQAv2\n\u2013 Variables: [QUESTION]\n\u2013 TLM Question: [QUESTION] Answer:\n\u2013 VLM Question: [QUESTION] Answer:\n\u2022 ScienceQA\n\u2013 Variables: [QUESTION], [CONTEXT], [CHOICES]\n\u2013 TLM Answer the multi-choice question given the image. Question: [QUESTION] [CONTEXT] Choices: [CHOICES] Answer:\n\u2013 VLM Answer the multi-choice question given the image. Question: [QUESTION] [CONTEXT] Choices: [CHOICES] Answer:\n\u2022 Concadia\n\u2013 TLM Write a short caption that describes the image. Article: \"[ARTICLE]\" Caption:\n\u2013 VLM The image describes\n\u2022 Image Paragraph Captioning\n\u2013 Variables: [ARTICLE]\n\u2013 TLM Write a multi-sentence long paragraph describing the image. Each sentence should describe a different aspect of the image and should be concise.\\n (Image 1) Image Description: [Description Sample 1]\\n (Image 2) Image Description: [Description Sample 2]\\n (Image 3) Image Description: [Description Sample 3]\\n (Image 4) Image Description:\n\u2013 VLM Write a multi-sentence long paragraph describing the image. Each sentence should describe a different aspect of the image and should be concise.\\n (Image 1) Image Description: [Description Sample 1]\\n (Image 2) Image Description: [Description Sample 2]\\n (Image 3) Image Description: [Description Sample 3]\\n (Image 4) Image Description:\n\u2022 ROCStories\n\u2013 Variables: [TOPIC]\n\u2013 TLM [TOPIC] <|endoftext|>\n\u2013 VLM The image describes\n\u2022 WHOOPS\n\u2013 TLM Decide whether the image is strange or natural in terms of physics, commonsense, or etc.\\n Start with \"The image shows\"\n\u2013 VLM Decide whether the image is strange or natural in terms of physics, commonsense, or etc.\\n Start with \"The image shows\""
        },
        {
            "heading": "G More Qualitative Samples",
            "text": "We include more qualitative samples for image paragraph generation in Figure 9, story generation in Figure 10, and open-ended generation in Figure 11.\nVisual distractors for simple facts\nTitle: Joanne Went Sky Diving GT: Joanne always wanted to go skydiving but was hesitant. Joanne was hesitant to go because she was afraid of heights. Joanne decided to face her fears head on and go skydiving. On her way in the car, she listened to music and took deep breaths. Joanne went sky diving and was very proud of herself. Ours: Joanne went skydiving with her friends. She had a great time on the water at the end of the day. When it was time to take off the parachute was broken. Joanne's friends had to call for help to get it fixed. Na\u00efve Ensemble: Joanne was at the beach with her friends. She decided to go on a dive. When she got to the water, she saw her friends in the water. Joanne jumped out of the water and ran to the other side of the water. She had no idea what was going on until she was rescued by rescue workers. MAGIC: Joanne was at the beach with her friends. She decided to jump off the sand to see what was out there. When she landed, she saw a huge wave. The wave was so big that she had to stop and get help. Joanne was able to land safely and return to the beach. Contrastive Search: Joanne was at the beach with her friends. She decided to go on a dive. When she got to the water, she saw her friends in the water. Joanne jumped out of the water and ran to the other side of the water. She had no idea what was going on until she was rescued by rescue workers. (a)"
        }
    ],
    "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
    "year": 2023
}