{
    "abstractText": "An important aspect of developing LLMs that interact with humans is to align models\u2019 behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by PEW research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling relevant past user opinions in addition to user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics1. Our work opens up the research avenues to bring user opinions as an important ingredient in aligning language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "EunJeong Hwang"
        },
        {
            "affiliations": [],
            "name": "Bodhisattwa Prasad Majumder"
        },
        {
            "affiliations": [],
            "name": "Niket Tandon"
        }
    ],
    "id": "SP:577b20ce81baca9769a86cde2f39284587ea9d3e",
    "references": [
        {
            "authors": [
                "Ante Busic-Sontic",
                "Natalia V Czap",
                "Franz Fuerst."
            ],
            "title": "The role of personality traits in green decisionmaking",
            "venue": "Journal of Economic Psychology, 62:313\u2013 328.",
            "year": 2017
        },
        {
            "authors": [
                "Eugene Y. Chan",
                "Mauricio Palmeira."
            ],
            "title": "Political ideology moderates consumer response to brand crisis apologies for data breaches",
            "venue": "Computers in Human Behavior, 121:106801.",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90% chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Eric Chu",
                "Jacob Andreas",
                "Stephen Ansolabehere",
                "Deb Roy"
            ],
            "title": "Language models trained on media diets can predict public opinion",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and Psychological Measurement, 20:37 \u2013 46.",
            "year": 1960
        },
        {
            "authors": [
                "David Crockett",
                "Melanie Wallendorf."
            ],
            "title": "The role of normative political ideology in consumer behavior",
            "venue": "Journal of Consumer Research, 31:511\u2013528.",
            "year": 2004
        },
        {
            "authors": [
                "Michela Del Vicario",
                "Gianna Vivaldo",
                "Alessandro Bessi",
                "Fabiana Zollo",
                "Antonio Scala",
                "Guido Caldarelli",
                "Walter Quattrociocchi."
            ],
            "title": "Echo chambers: Emotional contagion and group polarization on facebook",
            "venue": "Scientific reports, 6(1):37825.",
            "year": 2016
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Vishvak Murahari",
                "Tanmay Rajpurohit",
                "Ashwin Kalyan",
                "Karthik Narasimhan."
            ],
            "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "venue": "arXiv preprint arXiv:2304.05335.",
            "year": 2023
        },
        {
            "authors": [
                "Yao Fu",
                "Litu Ou",
                "Mingyu Chen",
                "Yuhao Wan",
                "Hao-Chun Peng",
                "Tushar Khot."
            ],
            "title": "Chain-of-thought hub: A continuous effort to measure large language models\u2019 reasoning performance",
            "venue": "ArXiv, abs/2305.17306.",
            "year": 2023
        },
        {
            "authors": [
                "Yunfan Gao",
                "Tao Sheng",
                "Youlin Xiang",
                "Yun Xiong",
                "Haofen Wang",
                "Jiawei Zhang."
            ],
            "title": "Chatrec: Towards interactive and explainable llmsaugmented recommender system",
            "venue": "arXiv preprint arXiv:2303.14524.",
            "year": 2023
        },
        {
            "authors": [
                "Yunfan Gao",
                "Tao Sheng",
                "Youlin Xiang",
                "Yun Xiong",
                "Haofen Wang",
                "Jiawei Zhang"
            ],
            "title": "2023b. Chat-rec: Towards interactive and explainable llms-augmented recommender system",
            "year": 2023
        },
        {
            "authors": [
                "Liwei Jiang",
                "Jena D. Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Maxwell Forbes",
                "Jon Borchardt",
                "Jenny Liang",
                "Oren Etzioni",
                "Maarten Sap",
                "Yejin Choi."
            ],
            "title": "Delphi: Towards machine ethics and norms",
            "venue": "ArXiv, abs/2110.07574.",
            "year": 2021
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Junsol Kim",
                "Byungkyu Lee"
            ],
            "title": "Ai-augmented surveys: Leveraging large language models for opinion prediction in nationally representative surveys",
            "year": 2023
        },
        {
            "authors": [
                "Marco Lauriola",
                "Irwin P Levin."
            ],
            "title": "Personality traits and risky decision-making in a controlled experimental task: An exploratory study",
            "venue": "Personality and individual differences, 31(2):215\u2013226.",
            "year": 2001
        },
        {
            "authors": [
                "Shuyang Li",
                "Bodhisattwa Prasad Majumder",
                "Julian McAuley"
            ],
            "title": "Self-supervised bot play for conversational recommendation with justifications",
            "year": 2021
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Peter Clark",
                "Yiming Yang."
            ],
            "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2833\u20132861, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Bodhisattwa Prasad Majumder",
                "Shuyang Li",
                "Jianmo Ni",
                "Julian McAuley."
            ],
            "title": "Generating personalized recipes from historical user preferences",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "David McLellan."
            ],
            "title": "Simone Weil: utopian pessimist",
            "venue": "Springer.",
            "year": 1989
        },
        {
            "authors": [
                "Xiaoman Pan",
                "Kai Sun",
                "Dian Yu",
                "Jianshu Chen",
                "Heng Ji",
                "Claire Cardie",
                "Dong Yu."
            ],
            "title": "Improving question answering with external knowledge",
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 27\u201337,",
            "year": 2019
        },
        {
            "authors": [
                "Alireza Salemi",
                "Sheshera Mysore",
                "Michael Bendersky",
                "Hamed Zamani"
            ],
            "title": "Lamp: When large language models meet personalization",
            "year": 2023
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Esin Durmus",
                "Faisal Ladhak",
                "Cinoo Lee",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "title": "Whose opinions do language models reflect? ArXiv, abs/2303.17548",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Simone Weil."
            ],
            "title": "\u00c9crits de Londres et derni\u00e8res lettres",
            "venue": "Gallimard.",
            "year": 1957
        },
        {
            "authors": [
                "Leor Zmigrod",
                "Ian Eisenberg",
                "Patrick Bissett",
                "Trevor Robbins",
                "Russell Poldrack."
            ],
            "title": "The cognitive and perceptual correlates of ideological attitudes: A data-driven approach",
            "venue": "Philosophical Transactions of the Royal Society B: Biological",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Personality is a defining feature of human beings, shaped by a complex interplay of demographic characteristics, moral principles, and social experiences (Weil, 1957; McLellan, 1989). In turn, a person\u2019s personality has a significant influence on their ability to make decisions (Lauriola and Levin, 2001; Busic-Sontic et al., 2017). Owing to the wide-scale adaptation of the large language models (LLMs) for assisting individuals in their decision-making process (Jiang et al., 2021; Gao et al., 2023a), it becomes increasingly critical to ensure that these models are aligned with the unique personalities of their users.\nWith lower barriers to entry, several recent works focused on prompting LLMs with persona or rolebased prompts such as Pretend you are a Democrat (Deshpande et al., 2023; Santurkar et al.,\n1Project page: https://github.com/eujhwang/personalized-llms\n2023). However, the extent to which these approaches align language models with users remains unclear due to the subjective nature of defining user personas. Users have nuanced opinions that can change over time and vary depending on context. While alignment with normalized user groups like religion or political inclination may be easier, LLMs continue to struggle to align with individual users or the long tail of user groups. Additionally, LLMs tend to form opinions based on their pretraining data and feedback collected from crowd workers and model designers. As a result, they exhibit low steerability, even with user groups that have major representation (Santurkar et al., 2023).\nAligning LLMs to individual and long-tail opinions has received less attention, while mostly focusing on aligning to user groups. In our analysis over PEW surveys, we found that people can share all of their demographic traits but still exhibit a large variance in their opinions, rendering the current group-based LLM alignment insufficient. This\npaper investigates the relationship between demographic traits and individual opinions in LLM alignment. Specifically, we seek to answer the following research question:\nWhat do we need to align an LLM to a user: demographic traits, fine-grained opinions, or both?\nThe majority of the past work in NLP literature focused on aligning LLMs with normalized user groups (Santurkar et al., 2023; Majumder et al., 2019; Salemi et al., 2023). In social science studies, however, it has been shown that all users are unique even if they belong to the same broader user group, and normalizing user groups is not a true representative of a user\u2019s opinion (Chu et al., 2023; Kim and Lee, 2023). We apply insights from these social science studies to an empirical setting where we try to model individuals\u2019 opinions based on their various persona information such as demographic traits, ideological inclinations, and past opinions.\nIn this paper, we give a thorough analysis of public survey responses in the OpinionQA dataset (Santurkar et al., 2023) with respect to their demographics, ideology, and implicit opinions and present comprehensive experimental results using the GPT-3 model with various combinations of inputs (i.e., demographic, ideology, user\u2019s past opinions). Through our dataset analysis, we found that users\u2019 opinions and demographics do not necessarily correlate with each other. Our experimental results show incorporating both user opinions, demographics, and ideology, results in significant gains of up to 7 points in QA accuracy for certain topics, and utilizing the most relevant past opinions helps the model to pinpoint the more accurate answers for the users."
        },
        {
            "heading": "2 What makes a persona?",
            "text": "We present a study on various components that makes a personality (in short, persona) of a user. We use the OpinionQA dataset, which contains 15 topics, and each topic contains an average of 100 questions and 5K users (Santurkar et al., 2023)."
        },
        {
            "heading": "2.1 Demographics",
            "text": "The dataset records eight demographic information of a user: region, sex, age, education, race, citizen, marital status, and income. These are the markers of social experience that a user is most likely to go through. For example, the social experience\ncan be determined by the region a user belongs, or their age determines whom they socialize with on a regular basis. However, this runs with the risk of stereotyping (i.e., an old individual is less likely to mix with younger people or they are conservative in thinking). We later show that demographic information is not enough to model an individual."
        },
        {
            "heading": "2.2 Ideology",
            "text": "Ideology is formed by an individual understanding of politics and economics. In our dataset, we have each subject\u2019s political affiliation and inclinations toward well-known political ideologies (e.g., conservative, liberal). We use this information as an individual\u2019s ideology."
        },
        {
            "heading": "2.3 Opinions",
            "text": "OpinionQA uses a well-established method of capturing human opinions from public opinion surveys. In these surveys, subjects are asked to answer subjective questions that reflect their unique opinions and what makes them different from other individuals. Figure 1 shows an example of opinions that a user provided during a survey."
        },
        {
            "heading": "2.4 Deriving insights from public surveys",
            "text": "We derive insights from the OpinionQA dataset, where we analyze the degree of agreement in user\u2019s opinions where they same demographics and how this agreement varies across topics. This statistical analysis generates useful insights that we later use for our modeling approach. We also look for similar (dis)agreements in opinions when users have the same ideologies.\nOpinions differ despite same demographics We first take all pairs of users sharing the same demographics and compare their opinions. To calculate the agreement score between users, we utilize Cohen\u2019s kappa coefficient (Cohen, 1960), which ranges from \u22121 to 1. Even though two users share the same demographics, agreement scores on the implicit opinions are gathered around 0.5 (Figure 2). This shows that solely relying on demographic information is not enough to personalize the model, and users\u2019 implicit opinions can play a critical role in personalization.\nOpinions differ across topics In Figure 2, we also show the topic-wise agreement scores. On certain topics, including Family & Relationships and Guns, users exhibit relatively higher agreement\nscores. On the other hand, for some topics, including Race and America in 2050, users have lower agreement scores, indicating that certain topics may have larger variability in terms of user opinions. We later analyze if this variability appears in a model\u2019s predictive performance when it is used to predict user opinions across different topics.\nOpinions differ despite same ideology To analyze the correlation between user opinions and their ideology, we extract user pairs that two users who answered at least more than 10 common questions and compare their opinions and political ideologies. Table 1 shows the percentage of user pairs sharing similar opinions, where 70% of opinions are matched between two users, and the percentages of the same ideologies and different ideologies within those user pairs. We observe that even though the users have similar opinions, around 80% of the user pairs have different ideologies. In contrast, we observe the percentage of sharing similar opinions among the users having similar ideologies is relatively higher than the percentage of sharing similar ideologies among the users having similar opinions in Appendix A. This implies that while having similar opinions does not necessarily imply shared ideologies among users, the presence of similar ideologies may suggest that users are more likely to have similar opinions. We particularly notice this phenomenon on the Guns and Family topics,\nas highlighted in Table 1. While the percentage of user pairs with shared opinions is higher compared to other topics, the percentage of user pairs with differing ideologies within these pairs is notably higher than the percentage of user pairs with similar ideologies.\nBased on the insights derived above, we incorporate them in our modeling approaches and analyze if these translate to the predictive performance of a model when used to predict user opinions as collected from the surveys."
        },
        {
            "heading": "3 Aligning LLMs with persona",
            "text": "In this section, we detail our task, possible modeling approaches, and evaluation protocols in Section 3.1 and discuss how to select the most relevant past opinions of a user in Section 3.3."
        },
        {
            "heading": "3.1 Setup",
            "text": "Task We use LLMs to model a user; however, to concretely measure the performance, we use a simple question-answering (QA) setup. For our QA task, we use existing questions from the surveys and try to predict the choice from multiple-choice originally given to the subjects. We use a prompting-based zeroshot approach to perform the multiple-choice QA. We experiment with gpt-4 (GPT-4), text-davinci-003 (GPT-3), gpt-3.5-turbo (GPT-3.5), Vicuna-13b, LLaMA-7b as the LLMs. Vicuna-13b (Chiang et al., 2023) is a large language model which was built upon LLaMA13b (Touvron et al., 2023) and LLaMA-7b/13b is a transformer-based language model trained on trillions of tokens exclusively sourced from publicly available data. Vicuna-13b performs on par with ChatGPT (Chiang et al., 2023).\nModeling Approaches We sample 100 users per topic. 20% of implicit questions belonging to the specific user are used as the user\u2019s implicit persona, and the rest are used to test the model\u2019s personalization ability. We have the following variants of our model where the model is gradually exposed to different levels of user information: demographic, ideological information, and user past opinions. Here is a rough sketch of what a prompt would contain for each modeling variation: \u2022 no persona: this is a case where default LLM opinion is evaluated w.r.to the individual\u2019s opinion (Santurkar et al., 2023).\n\u2022 ideology: here, we observe if ideological inclinations from the user help the model to align better to them (Santurkar et al., 2023). \u2022 ideology + demographics: here, we observe if both demographic information and ideological inclinations from the user help the model to align better with them (Santurkar et al., 2023). \u2022 ideology + opinions: we combine ideological inclinations and opinions and measure if these help the model to align better with an individual. \u2022 demographic + ideology + opinions: when we combine all possible personal information, i.e., demographic, ideology, and opinions, and measure if these help the model to align better with an individual. See Figure 5 for the complete prompt."
        },
        {
            "heading": "3.2 Evaluation Metric",
            "text": "For accuracy evaluation, we utilize two types of accuracy measures, overall accuracy and collapsed accuracy. For overall accuracy, we simply calculate the accuracy of the predicted answer choice with respect to the gold answer choice from the dataset. We also present collapsed accuracy because most answer choices in the opinion QA dataset have around 3 to 4 classes. In cases where there are more than 4 classes, it is possible to further group the classes into super classes without losing substantial finer information. For example, the following answer choices: [Very likely, Somewhat likely, Not too likely, Not at all likely], can be grouped into [Likely, Unlikely]. We consolidate such answer choices into two classes, referred to as collapsed accuracy, and present the results accordingly.\nWe follow Santurkar et al. (2023) for the opinion alignment score evaluation. For the alignment scores, we calculate the difference between user\nand model opinion distributions. The difference is calculated using 1-Wassaerstein distance (WD), which measures the minimum cost for transforming one distribution to the other:\nA(Dm,Dh;Q) = 1\nQ \u2211 q\u2208Q 1\u2212WD(Dm(q),Dh(q)) N \u2212 1\nwhere N is the number of answer choices excluding refusal, and the normalization factor N \u2212 1 is the maximum WD between two answer choice distributions. The final score can be interpreted as how well the model distribution aligns with the human distribution."
        },
        {
            "heading": "3.3 LLM mimicking a person with ideologies, demographic, and opinions",
            "text": "Our main goal is to use different components of a user\u2019s persona (demographics, ideology, opinions) to align an LLM with an individual. Specifically, by having two experiments, one with past opinions+ideology and the other with past opinions+ideology+demographics, we aim to analyze the role of demographics when predicting user responses. In addition, we hypothesize that giving users\u2019 past opinions may offer useful insights into their perspective (followed from Table 1), and LLM can benefit from that information when predicting the future answer for the specific user. When adding the user\u2019s past opinions, we compare the model with all opinions (maximum 16) to the model with top-k opinions (k is a hyperparameter \u2208 3, 5, 8). The top-k opinions are obtained by comparing the embedding similarity between the user\u2019s previous opinions and the question at hand, where we employ text-embedding-ada-002 to obtain the embeddings. We hypothesize that all opinions\nmay incorporate some unrelated viewpoints to answer the question, and hence offering more pertinent opinions would enhance the model\u2019s ability to accurately anticipate its future response for the user. We show a complete prompt that uses all available past information of individuals to predict their future opinions and all prompts for other modeling approaches in Appendix C."
        },
        {
            "heading": "4 Results and Analysis",
            "text": "Here, we first analyze our model variants (Section 4.1) to validate hypotheses that we gather from analyzing the dataset (in Section 2.4). We also provide our model\u2019s performance when we use similar modeling setup to predict group-level opinions in Sections 4.2 and 4.3."
        },
        {
            "heading": "4.1 LLM for an individual",
            "text": "Here we discuss results of using an LLM to model an individual in the light of the evaluation metrics described in Section 4.1.\nExact match vs. Collapsed match The accuracy with the exact match and with the collapsed match in Table 2 shows a similar trend for the performance of our model variants. This suggests that leveraging implicit opinions enables the model to align with the correct range of answer choices, even though it does not precisely predict the exact same answer as the user\u2019s choice.\nOverall Accuracy Table 2 presents overall QA accuracy with exact match and collapsed match for answer choices. In most cases, utilizing demographic+ideology+top-8 opinions performs the best. Moreover, adding demographic and ideology information outperforms the model without\nany persona, indicating that some questions might be highly correlated with the user\u2019s demographics, and LLM is able to make a guess with the demographic information. Incorporating the user\u2019s previous opinions, up to 16 in total, along with demographic information, substantially enhances the performance in both overall and collapsed accuracy across all models. This implies that users\u2019 past opinions are indeed important to make correct predictions.\nInterestingly, utilizing the top-k most relevant previous opinions does not yield a significant increase in collapsed accuracy. However, it does improve the exact match accuracy by up to 3 points with GPT-3 when using both demographics and ideology along with the user\u2019s previous opinions. This implies that having top-k most relevant past opinions can help the model pinpoint more accurate answers, and providing the user\u2019s past opinions is already pushing the model to be in the correct range of the answer choices. We noticed that utilizing the top-3 opinions yields similar performance to using the top-8 opinions, indicating that a few of the most relevant opinions carry the most performance improvement of the model.\nMoreover, simply using the top 3 most relevant opinions performs on par with the model with user demographic, ideology, and user\u2019s past 16 random opinions. This confirms again that utilizing the most relevant opinions as feedback is essential to get personalized answers from LLM. Lastly, providing additional demographic information with ideology slightly improves the model performance, implying that the demographic information may contribute valuable insights to the model to a certain degree.\nComparison between LLMs GPT-4 produces a similar performance when using all opinions and when using top-k relevant opinions. This implies that GPT-4 itself might be able to identify the most relevant user\u2019s previous opinions.\nWe find that the LLaMA-7b model does not understand what to produce when opinions are added to the prompt resulting in 18% cases where it does not yield any answer. Likewise, GPT-3.5 tends to produce no answer for questions that are about personal opinions (e.g. GPT-3.5 with top-3 opinions produces 59% no-answers). When we prompted the model to generate an answer without an explanation by changing the suffix of the prompt: \u201cAnswer choice:\u201d \u2192 \u201cAnswer choice without explanation:\u201d, and substantially fewer (less than 1%) cases have no-answers.\nThe trend of the performance where we add topk opinions remains the same for both GPT-3 and GPT-3.5. There is no clear winner between different versions of GPT as also found in the other literature (Fu et al., 2023).\nChain of Thought (CoT) Prompting CoT (Wei et al., 2023) has shown that encouraging LLMs to explain their reasoning step-by-step improves the model performance. To see whether CoT also benefits in personalizing answers, we test the GPT3 model with CoT-style prompting by changing the suffix of our prompt from \u201cAnswer choice:\u201d to \u201cLet\u2019s think step by step and choose one answer choice:\u201d and present the results in Table 3. We discover that using the CoT prompt consistently decreases the performance by 1 point. This is because the model attempts to provide the reason for the potentially irrelevant information presented in the prompt, suggesting its inability of selecting the most relevant information about the answer. E.g., in Figure 4, the model generates reasoning exclusively based on demographic and ideology information but ignores user opinions.\nOpinion Alignment Scores Among highperforming models, since only GPT-3 provides prediction probabilities, we compare opinion\nalignment scores (defined in \u00a73.2) with various setups using GPT-3 model in Table 4. Overall scores exhibit a similar trend as seen in Table 2, in which utilizing top-k most relevant opinions outperforms the model that uses all opinions. This is consistent with our finding that having top-k most relevant past opinions helps the model find more accurate answers for the user.\nTopic-wise Accuracy Figure 3 demonstrates the model\u2019s accuracy across different topics with various setups, measured by the exact match for answer choices. The model with demographic and implicit opinions particularly achieves higher scores on the Biomedical-food and Guns topics, implying that these two topics may lead the users to have similar opinions of each other. In contrast, the model exhibits slightly decreased performance when incorporating implicit opinions on topic Automation. This suggests that the LLM can make accurate predictions up to some extent based on user demographic and ideology information. However, incorporating implicit opinions, which may include viewpoints not aligned with users\u2019 demographic or ideologies, can potentially confuse the model in its prediction process.\nCommon Errors In Table 5, we manually analyzed 30 randomly sampled opinions where GPT-3 produces correct answers with demographic+ideology information but incorrect answers when opinions are added. The model mostly confuses when there is a high overlap between user opinions and some answer (possibly wrong) choices. For a question: \u201cHow well do the following phrases describe you?\u201d and a correct answer: \u201cDescribes me well,\u201d the model often predicts correctly purely based on demographic information (e.g., Black). However, when the user\u2019s past opinions were added and since it includes the phrase \u201csupporter of rights for LGBT people do not de-\nscribe me well,\u201d it predicts a wrong answer choice: \u201cdoes not describe me well\u201d just because the direct phrase overlap. While models often ignore additional relevant information and commit errors, it is also sensitive to irrelevant opinions (even in top-k) when additionally added with demographic information. For e.g., while predicting user opinion on the U.S. remaining as a world superpower in 2050, the user\u2019s past opinion about violence against Jews (retrieved in top-8) confuses the model leading to an incorrect prediction. This also explains why just adding top-3 opinions often archives the highest accuracy instead of adding all opinions. See Appendix D for examples of all types of errors."
        },
        {
            "heading": "4.2 LLM with majority answer choices",
            "text": "Additionally, we also wanted to understand if similar performances can be achieved if we model an individual as a member of a (sub-)population, mirroring (Santurkar et al., 2023). While it is true that opinions are necessary to predict individual-level preferences, here we investigate if demographic information is sufficient for predicting populationlevel opinions or not. For this, we first merge our QA data points using a particular ideological group value (e.g., democrat) and obtain the answer choice that is chosen by most of the group members (i.e., a majority vote) and treat that answer as the gold answer for the question while calculating the accuracy (Kim and Lee, 2023).\nWe prompt our model to predict an answer given a question assuming the role of a group representa-\ntive, i.e., a person having a majority vote answers belonging to a specific group (See Appendix C for the prompt). We see that the LLM is good at predicting the answer given by the majority of the group member belonging to a certain ideology, suggesting that LLMs are good at modeling a representative individual of a sub-population (e.g., all democrats). The overall performance without ideology information is 0.597 (with exact answer choice\nmatch) and 0.674 (with collapsed answer choice match), as presented in Table 6. This also indicated that the default opinions from the LLMs are somewhat aligned with the majority opinions seen at a population level."
        },
        {
            "heading": "4.3 LLM as a person with an ideology",
            "text": "Next, we add the ideological information to see if this additional information can help the LLM perform better to model a user belonging to a group that believes in a specific ideology (e.g., conservative) (See Appendix C for the prompt). We find that the LLM is moderately good at modeling a user with group-level information to predict the group-level majority opinion. This indicates that the additional ideological information is not particularly helpful. The average performance with ideology information is 0.549 (with exact answer choice match) and 0.659 (with collapsed answer choice match), as shown in Table 6. We see a similar trend in results for modeling an individual with their demographics and/or ideology and/or past opinions since an individual\u2019s opinion does not align with the group\u2019s majority opinion that the person belongs to."
        },
        {
            "heading": "5 Related work",
            "text": "Personalization Past works that focused on modeling individual users were from the pre-LLMs era and mainly hail from the recommender systems literature (Gao et al., 2023b; He et al., 2017; Li et al., 2021; Majumder et al., 2019). However, these systems were trained on domain-specific annotated datasets or using latent information about the users (e.g. their previously written reviews). The recent LLMs have seen less content from the long tail of user groups during their pre-training phase, and there has been a lack of large-scale datasets of individual opinions until recently (Santurkar et al., 2023). Thus it remains an open problem whether LLMs can be aligned effectively with individual user persona and how different user information\n(e.g., demographic traits vs. past opinions) influences how well an LLM can model individual\u2019s opinions. For a comprehensive comparison among all previous work, see Table 9.\nRole of demographics and ideology There have been several studies investigating the correlation between ideological attitudes and psychological traits (Zmigrod et al., 2021; Crockett and Wallendorf, 2004; Chan and Palmeira, 2021). Crockett and Wallendorf (2004) found that normative political ideology is central to understanding shopping as a manifestation of social and political connections. Chan and Palmeira (2021) found that the cognitive decision-making strategies of individuals reflect their ideological attitudes. Differently, we show that ideology is not the only important factor in predicting the user\u2019s opinion using an LLM.\nLLMs with retrieval-based approach Extensive prior work has used retrievals from a text corpus to aid QA (Madaan et al., 2022; Pan et al., 2019), or retrievals of prior QA pairs for nearestneighbor QA (Khandelwal et al., 2020). Madaan et al. (2022) uses a memory of user opinions to retrieve past relevant data points for the prompt. Khandelwal et al. (2020) showed the effectiveness of the nearest neighbor search for language modeling by extending a pre-trained language model (LM). Differently from work on LLMs and grouplevel personalization, we show that LLMs can be tuned for individual users with their opinions."
        },
        {
            "heading": "6 Conclusion and Outlook",
            "text": "This paper offers a new insight that aligning LLMs to users is best done by modeling user demographics, ideologies, and the most relevant past opinions. Large-scale experiments on PEW surveys present in the OpinionQA dataset show an approximately 7% absolute QA accuracy over strong demographybased baselines. We proactively offer suggestions to avoid personalized LLMs from becoming echo chambers (see Ethics Statement). An exciting future direction is to continuously store user opinions and grow the memory of opinions.\nAn aligned LLM offers the benefit to offer personalized perspectives that align with a user\u2019s values and cultural beliefs. However, there exist circumstances when LLMs can become an amplifier for unethical and biased views. Our work lays the foundation for a robust LLM alignment approach. By using memory-based personalization\nand recording interactions saved in a growing memory, the model can inform future instances of the most relevant past opinions.\nLimitations\nNon-subjective questions. For non-subjective questions, such as \u201cHow many years have you lived in ...\u201d, it might not be necessary to use past opinions (see error analysis).\nLack of user information. In this work, we focus on all past opinions on the same topic due to a lack of availability of user information in the dataset. More insights can be derived by investigating users\u2019 opinions on multiple different related topics. We leave it as our future work.\nLack of temporal information. Another missing aspect is time. User opinions change over time, but the dataset available to us does not contain timestamps. It would be interesting to see how conflicting user opinions can be modeled, perhaps by biasing toward the most recent opinion.\nEthics Statement\nData The dataset used in our work, OpinionQA (Santurkar et al., 2023) is publicly available. The dataset includes subjective opinions from humans and may contain offensive content to some people.\nModels The large language models we used for the experiments are trained on a large-scale web corpus and some of them utilize human feedback. This may also bring some bias when predicting user answers. With an aligned LLM, users can select information that adheres to their system of beliefs and to amplify potentially biased and unethical views. Such an echo chamber (Del Vicario et al., 2016) can eventually cause harm by reinforcing undesirable or polarized a user\u2019s views.\nA viable mitigation is to show user demography or ideology group answers in addition to the personalized answer (e.g., showing how an average Democrat with similar demographics would think on this topic and why). Further, past opinions can be used to ground an explanation (e.g., the current personalized answer is influenced by a user\u2019s specific past opinion), thus offering an opportunity for the user to introspect their past opinions."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the members of the Aristo team at AI2 and Kurt Gray for their insightful feedback on this work. EH was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs program, an NSERC discovery grant, and a research gift from AI2. BPM was funded, in part, by an Adobe Research Fellowship."
        },
        {
            "heading": "A Similar ideologies and different",
            "text": "opinions\nWe show the percentage of user pairs having similar ideologies and the percentages of user pairs having similar opinions and different opinions within the user pairs sharing similar ideologies in Table 7."
        },
        {
            "heading": "B Topicwise QA Accuracy",
            "text": "We show an overall topic-wise accuracy based on the exact match for answer choice in Table 8."
        },
        {
            "heading": "C Prompt",
            "text": "We provide a comprehensive display of all prompts used in the models incorporating user demographics, ideology, and opinions, which were employed for individual-user level tests in Figure 5, 6 and 7. Additionally, we present the prompts utilized for experiments conducted at the group-level tests in Figure 8 and 9."
        },
        {
            "heading": "D Error Examples",
            "text": "We give additional error examples in Figure 10, 11, and 12, where opinions either confuse the model with relevant information or provide no useful information.\nFigure 10 shows the second common error, which is when Relevant information in opinions confuses the model. For example, when the question is \u201cIs what you know about dietitians because you have heard or read about this in the news\u201d and the user\u2019s past opinions contain \u201cI did not learn about dietitians in my job.\u201d. In this case, while the user\u2019s answer to the question is \u201cYes, have heard or read about this in the news\u201d, the model produces \u201cNo, have not heard or read about this in the news\u201d because of the opinions provided.\nThe third most common error is with Irrelevant information as shown in Figure11. Some opinions do not contain useful information at all to reason for the correct answer. Lastly, there are some questions that opinions are not important to predict the answer (e.g. \u201cAbout how many years have you lived in your local community?\u201d) as presented. In this case, the model may make an arbitrary prediction."
        },
        {
            "heading": "E Additional related work comparison",
            "text": "We show additional related works that can be compared to our paper in Table 9."
        }
    ],
    "title": "Aligning Language Models to User Opinions",
    "year": 2023
}