{
    "abstractText": "Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Emanuele Bugliarello"
        },
        {
            "affiliations": [],
            "name": "Aida Nematzadeh"
        },
        {
            "affiliations": [],
            "name": "Lisa Anne Hendricks"
        }
    ],
    "id": "SP:da8f71541f6879645dbf79c174e73d80e27020d7",
    "references": [
        {
            "authors": [
                "Oshin Agarwal",
                "Heming Ge",
                "Siamak Shakeri",
                "Rami Al-Rfou."
            ],
            "title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "gooei",
                "Marianne Monteiro",
                "Jacob Menick",
                "Sebastian Borgeaud",
                "Andrew Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Karen Simonyan"
            ],
            "title": "Flamingo: a visual language model",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: Visual question answering",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2425\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Roman Ring",
                "Francisco Ruiz",
                "Alvaro Sanchez",
                "Rosalia Schneider",
                "Eren Sezener",
                "Stephen Spencer",
                "Srivatsan Srinivasan",
                "Wojciech Stokowiec",
                "Luyu Wang",
                "Guangyao Zhou",
                "Fabio Viola"
            ],
            "title": "The DeepMind JAX Ecosystem",
            "year": 2020
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Songhao Piao",
                "Furu Wei."
            ],
            "title": "BEiT: BERT pre-training of image transformers",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Lucas Beyer",
                "Bo Wan",
                "Gagan Madan",
                "Filip Pavetic",
                "Andreas Steiner",
                "Alexander Kolesnikov",
                "Andr\u00e9 Susano Pinto",
                "Emanuele Bugliarello",
                "Xiao Wang",
                "Qihang Yu",
                "Liang-Chieh Chen",
                "Xiaohua Zhai"
            ],
            "title": "A study of autoregressive decoders for multi-tasking",
            "year": 2023
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Ryan Cotterell",
                "Naoaki Okazaki",
                "Desmond Elliott."
            ],
            "title": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Desmond Elliott."
            ],
            "title": "The role of syntactic planning in compositional image captioning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 593\u2013607,",
            "year": 2021
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Laurent Sartrain",
                "Aishwarya Agrawal",
                "Lisa Anne Hendricks",
                "Aida Nematzadeh."
            ],
            "title": "Measuring progress in fine-grained vision-and-language understanding",
            "venue": "Proceedings of the 61th Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Xiaojun Chang",
                "Pengzhen Ren",
                "Pengfei Xu",
                "Zhihui Li",
                "Xiaojiang Chen",
                "Alex Hauptmann."
            ],
            "title": "A comprehensive survey of scene graphs: Generation and application",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):1\u201326.",
            "year": 2023
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut."
            ],
            "title": "Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2021
        },
        {
            "authors": [
                "Yuanzhong Xu",
                "Keran Rong",
                "Alexander Kolesnikov",
                "Mojtaba Seyedhosseini",
                "Anelia Angelova",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Radu Soricut."
            ],
            "title": "PaLIX: On scaling up a multilingual vision and language model",
            "venue": "ArXiv, abs/2305.18565.",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Ashish V Thapliyal",
                "James Bradbury",
                "Weicheng Kuo",
                "Mojtaba Seyedhosseini",
                "Chao Jia",
                "Burcu Karagol Ayan",
                "Carlos Riquelme Ruiz",
                "Andreas Peter Steiner",
                "Anelia Angelova",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Radu Soricut"
            ],
            "title": "2023b. PaLI: A",
            "year": 2023
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "UNITER: Universal image-text representation learning",
            "venue": "European Conference on Computer Vision, pages 104\u2013120. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Aishwarya Kamath",
                "Zhe Gan",
                "Pengchuan Zhang",
                "Jianfeng Wang",
                "Linjie Li",
                "Zicheng Liu",
                "Ce Liu",
                "Yann LeCun",
                "Nanyun Peng",
                "Jianfeng Gao",
                "Lijuan Wang."
            ],
            "title": "Coarse-to-fine visionlanguage pre-training with fusion in the backbone",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Desmond Elliott",
                "Frank Keller."
            ],
            "title": "Image description using visual dependency representations",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292\u20131302, Seattle, Washington, USA. Association",
            "year": 2013
        },
        {
            "authors": [
                "Zhe Gan",
                "Linjie Li",
                "Chunyuan Li",
                "Lijuan Wang",
                "Zicheng Liu",
                "Jianfeng Gao"
            ],
            "title": "Vision-language pretraining: Basics, recent advances, and future trends. Foundations and Trends\u00ae in Computer Graphics and Vision, 14(3\u20134):163\u2013352",
            "year": 2022
        },
        {
            "authors": [
                "Yuting Gao",
                "Jinfeng Liu",
                "Zihan Xu",
                "Jun Zhang",
                "Ke Li",
                "Rongrong Ji",
                "Chunhua Shen."
            ],
            "title": "PyramidCLIP: Hierarchical feature alignment for visionlanguage model pretraining",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Claire Gardent",
                "Anastasia Shimorina",
                "Shashi Narayan",
                "Laura Perez-Beltrachini."
            ],
            "title": "The WebNLG challenge: Generating text from RDF data",
            "venue": "Proceedings of the 10th International Conference on Natural Language Generation, pages 124\u2013133, San-",
            "year": 2017
        },
        {
            "authors": [
                "Xuri Ge",
                "Fuhai Chen",
                "Songpei Xu",
                "Fuxiang Tao",
                "Joemon M. Jose."
            ],
            "title": "Cross-modal semantic enhanced interaction for image-sentence retrieval",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages",
            "year": 2023
        },
        {
            "authors": [
                "Shijie Geng",
                "Jianbo Yuan",
                "Yu Tian",
                "Yuxiao Chen",
                "Yongfeng Zhang."
            ],
            "title": "HiCLIP: Contrastive language-image pretraining with hierarchy-aware attention",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick."
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16000\u201316009.",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "John Mellor",
                "Rosalia Schneider",
                "Jean-Baptiste Alayrac",
                "Aida Nematzadeh."
            ],
            "title": "Decoupling the role of data, attention, and losses in multimodal transformers",
            "venue": "Transactions of the Association for Computational Linguistics, 9:570\u2013585.",
            "year": 2021
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Aida Nematzadeh."
            ],
            "title": "Probing image-language transformers for verb understanding",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3635\u20133644, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Drew A. Hudson",
                "Christopher D. Manning."
            ],
            "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Justin Johnson",
                "Agrim Gupta",
                "Li Fei-Fei."
            ],
            "title": "Image generation from scene graphs",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1219\u20131228.",
            "year": 2018
        },
        {
            "authors": [
                "Justin Johnson",
                "Ranjay Krishna",
                "Michael Stark",
                "Li-Jia Li",
                "David Shamma",
                "Michael Bernstein",
                "Li FeiFei."
            ],
            "title": "Image retrieval using scene graphs",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3668\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Rajat Koner",
                "Hang Li",
                "Marcel Hildebrandt",
                "Deepan Das",
                "Volker Tresp",
                "Stephan G\u00fcnnemann."
            ],
            "title": "Graphhopper: Multi-hop scene graph reasoning for visual question answering",
            "venue": "The Semantic Web \u2013 ISWC 2021: 20th International Semantic Web Con-",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Krause",
                "Justin Johnson",
                "Ranjay Krishna",
                "Li Fei-Fei."
            ],
            "title": "A hierarchical approach for generating descriptive image paragraphs",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 317\u2013325.",
            "year": 2017
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A. Shamma",
                "Michael S. Bernstein",
                "Li Fei-Fei"
            ],
            "title": "Visual genome: Connecting language and vision",
            "year": 2017
        },
        {
            "authors": [
                "Karen Kukich."
            ],
            "title": "Design of a knowledge-based report generator",
            "venue": "Proceedings of the 21st Annual Meeting on Association for Computational Linguistics, ACL \u201983, page 145\u2013150, USA. Association for Computational Linguistics.",
            "year": 1983
        },
        {
            "authors": [
                "Ju-Hee Lee",
                "Je-Won Kang."
            ],
            "title": "Relation enhanced vision language pre-training",
            "venue": "2022 IEEE International Conference on Image Processing (ICIP), pages 2286\u20132290.",
            "year": 2022
        },
        {
            "authors": [
                "Soohyeong Lee",
                "Ju-Whan Kim",
                "Youngmin Oh",
                "Joo Hyuk Jeon."
            ],
            "title": "Visual question answering over scene graph",
            "venue": "2019 First International Conference on Graph Computing (GC), pages 45\u201350.",
            "year": 2019
        },
        {
            "authors": [
                "Juncheng Li",
                "Xin He",
                "Longhui Wei",
                "Long Qian",
                "Linchao Zhu",
                "Lingxi Xie",
                "Yueting Zhuang",
                "Qi Tian",
                "Siliang Tang."
            ],
            "title": "Fine-grained semantically aligned vision-language pre-training",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "Proceedings of the 40th International Conference on Machine Learning, Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang"
            ],
            "title": "Grounded language-image pre-training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Zejun Li",
                "Zhihao Fan",
                "Huaixiao Tou",
                "Jingjing Chen",
                "Zhongyu Wei",
                "Xuanjing Huang."
            ],
            "title": "Mvptr: Multi-level semantic alignment for vision-language pre-training via multi-stage learning",
            "venue": "Proceedings of the 30th ACM International Conference on Mul-",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "Computer Vision \u2013 ECCV 2014, pages 740\u2013755, Cham. Springer Inter-",
            "year": 2014
        },
        {
            "authors": [
                "Fangyu Liu",
                "Emanuele Bugliarello",
                "Edoardo Maria Ponti",
                "Siva Reddy",
                "Nigel Collier",
                "Desmond Elliott."
            ],
            "title": "Visually grounded reasoning across languages and cultures",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Fangyu Liu",
                "Guy Edward Toh Emerson",
                "Nigel Collier."
            ],
            "title": "Visual spatial reasoning",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Cewu Lu",
                "Ranjay Krishna",
                "Michael Bernstein",
                "Li Fei-Fei."
            ],
            "title": "Visual relationship detection with language priors",
            "venue": "European Conference on Computer Vision.",
            "year": 2016
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Jiasen Lu",
                "Christopher Clark",
                "Rowan Zellers",
                "Roozbeh Mottaghi",
                "Aniruddha Kembhavi."
            ],
            "title": "UNIFIED-IO: A unified model for vision, language, and multi-modal tasks",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Jiasen Lu",
                "Vedanuj Goswami",
                "Marcus Rohrbach",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "12-in-1: Multi-task vision and language representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2020
        },
        {
            "authors": [
                "Nicole Meister",
                "Dora Zhao",
                "Angelina Wang",
                "Vikram V Ramaswamy",
                "Ruth Fong",
                "Olga Russakovsky."
            ],
            "title": "Gender artifacts in visual datasets",
            "venue": "arXiv preprint arXiv:2206.09191.",
            "year": 2022
        },
        {
            "authors": [
                "Mitja Nikolaus",
                "Emmanuelle Salin",
                "Stephane Ayache",
                "Abdellah Fourtassi",
                "Benoit Favre"
            ],
            "title": "Do vision-and-language transformers learn grounded predicate-noun dependencies",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara Berg."
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc.",
            "year": 2011
        },
        {
            "authors": [
                "Letitia Parcalabescu",
                "Michele Cafagna",
                "Lilitta Muradjan",
                "Anette Frank",
                "Iacer Calixto",
                "Albert Gatt."
            ],
            "title": "VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Julia Peyre",
                "Ivan Laptev",
                "Cordelia Schmid",
                "Josef Sivic."
            ],
            "title": "Weakly-supervised learning of visual relations",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV).",
            "year": 2017
        },
        {
            "authors": [
                "Tianwen Qian",
                "Jingjing Chen",
                "Shaoxiang Chen",
                "Bo Wu",
                "Yu-Gang Jiang."
            ],
            "title": "Scene graph refinement network for visual question answering",
            "venue": "IEEE Transactions on Multimedia, pages 1\u20131.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Brigit Schroeder",
                "Subarna Tripathi."
            ],
            "title": "Structured query-based image retrieval using scene graphs",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops.",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Schuster",
                "Ranjay Krishna",
                "Angel Chang",
                "Li Fei-Fei",
                "Christopher D. Manning."
            ],
            "title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval",
            "venue": "Proceedings of the Fourth Workshop on Vision and",
            "year": 2015
        },
        {
            "authors": [
                "Sahand Sharifzadeh",
                "Sina Moayed Baharlou",
                "Martin Schmitt",
                "Hinrich Sch\u00fctze",
                "Volker Tresp."
            ],
            "title": "Improving scene graph classification by exploiting knowledge from texts",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(2):2189\u20132197.",
            "year": 2022
        },
        {
            "authors": [
                "Sahand Sharifzadeh",
                "Sina Moayed Baharlou",
                "Volker Tresp."
            ],
            "title": "Classification by attention: Scene graph classification with prior knowledge",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(6):5025\u20135033.",
            "year": 2021
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Ravi Shekhar",
                "Sandro Pezzelle",
                "Yauhen Klimovich",
                "Aur\u00e9lie Herbelot",
                "Moin Nabi",
                "Enver Sangineto",
                "Raffaella Bernardi."
            ],
            "title": "FOIL it! find one mismatch between image and language caption",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for",
            "year": 2017
        },
        {
            "authors": [
                "Jiaxin Shi",
                "Hanwang Zhang",
                "Juanzi Li."
            ],
            "title": "Explainable and explicit visual reasoning over scene graphs",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Peter Steiner",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "Ross Wightman",
                "Jakob Uszkoreit",
                "Lucas Beyer."
            ],
            "title": "How to train your ViT? Data, augmentation, and regularization in vision transformers",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Alane Suhr",
                "Stephanie Zhou",
                "Ally Zhang",
                "Iris Zhang",
                "Huajun Bai",
                "Yoav Artzi."
            ],
            "title": "A corpus for reasoning about natural language grounded in photographs",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "LXMERT: Learning cross-modality encoder representations from transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Kaihua Tang",
                "Yulei Niu",
                "Jianqiang Huang",
                "Jiaxin Shi",
                "Hanwang Zhang."
            ],
            "title": "Unbiased scene graph generation from biased training",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2020
        },
        {
            "authors": [
                "Tristan Thrush",
                "Ryan Jiang",
                "Max Bartolo",
                "Amanpreet Singh",
                "Adina Williams",
                "Douwe Kiela",
                "Candace Ross."
            ],
            "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herve Jegou."
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "Proceedings of the 38th International Conference on Machine",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Danfei Xu",
                "Yuke Zhu",
                "Christopher B. Choy",
                "Li FeiFei."
            ],
            "title": "Scene graph generation by iterative message passing",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2017
        },
        {
            "authors": [
                "Xu Yang",
                "Kaihua Tang",
                "Hanwang Zhang",
                "Jianfei Cai."
            ],
            "title": "Auto-encoding scene graphs for image captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Lewei Yao",
                "Runhui Huang",
                "Lu Hou",
                "Guansong Lu",
                "Minzhe Niu",
                "Hang Xu",
                "Xiaodan Liang",
                "Zhenguo Li",
                "Xin Jiang",
                "Chunjing Xu."
            ],
            "title": "FILIP: Finegrained interactive language-image pre-training",
            "venue": "International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Ting Yao",
                "Yingwei Pan",
                "Yehao Li",
                "Tao Mei."
            ],
            "title": "Exploring visual relationship for image captioning",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV).",
            "year": 2018
        },
        {
            "authors": [
                "Yuan Yao",
                "Qianyu Chen",
                "Ao Zhang",
                "Wei Ji",
                "Zhiyuan Liu",
                "Tat-Seng Chua",
                "Maosong Sun."
            ],
            "title": "PEVL: Position-enhanced pre-training and prompt tuning for vision-language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.",
            "year": 2014
        },
        {
            "authors": [
                "Choi"
            ],
            "title": "Neural motifs: Scene graph parsing with",
            "year": 2018
        },
        {
            "authors": [
                "Gao"
            ],
            "title": "GLIPv2: Unifying localization",
            "year": 2022
        },
        {
            "authors": [
                "Gao"
            ],
            "title": "RegionCLIP: Region-based language",
            "year": 2022
        },
        {
            "authors": [
                "Zhi-Hua Zhou"
            ],
            "title": "A brief introduction to weakly su",
            "year": 2017
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2021) trained ALBEF 4M/ALBEF 14M on 154.5/456M samples, while we use 102.5/256M samples to train ALBEF3M/ALBEF13M and corresponding relation-enhanced models",
            "year": 2022
        },
        {
            "authors": [],
            "title": "2023) showed that current, strong models achieve peak performance on different finegrained tasks at different stages of pretraining. This motivates us to study the pretraining dynamics",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Current vision-and-language models (VLMs) are pretrained on large amounts of image\u2013text pairs collected from the Web, and shown to perform remarkably on a variety of downstream applications (e.g., Tan and Bansal, 2019; Bugliarello et al., 2021; Radford et al., 2021; Li et al., 2021; Zeng et al., 2022; Gan et al., 2022). Nonetheless, recent work has highlighted their limitations in fine-grained tasks, where precise understanding of both modalities is required to correctly select a positive match against a negative one. Examples of such tasks include verb understanding (sitting vs. standing; Hendricks and Nematzadeh 2021), word order (water in bottle vs. bottle in water; Thrush et al. 2022), spatial relations (above vs. below; Liu et al. 2023) and other linguistic phenomena (Parcalabescu et al., 2022; Nikolaus et al., 2022; Yuksekgonul et al., 2023).\n\u2217Work completed during an internship at DeepMind. \u2021denotes equal senior contribution. Correspondence to: Emanuele Bugliarello <emanuele@di.ku.dk>.\nRecent work (Yao et al., 2022b; Zeng et al., 2022; Zhang et al., 2022, i.a.) shows that leveraging entity localisation, such as bounding boxes, from supervised data improves performance on downstream tasks like visual QA (Antol et al., 2015) and visual reasoning (Suhr et al., 2019). Interestingly, modelling visual locations is also crucial to learn fine-grained image\u2013text mappings (Bugliarello et al., 2023). Motivated by this promising research thread, in this work, we further explore the benefits of supervised data in multimodal pretraining by leveraging structured visual information in the form of relations in scene graphs (Elliott and Keller, 2013; Johnson et al., 2015; Krishna et al., 2017).\nA scene graph is a data structure that describes the content of a visual scene by expressing its entities (e.g., helmet), their attributes (e.g., red), and their relationships (e.g., person wear red helmet; see Figure 1). While a large body of work has focused on generating scene graphs (Lu et al., 2016; Xu et al., 2017; Peyre et al., 2017; Zellers et al., 2018; Tang et al., 2020; Sharifzadeh et al., 2021, 2022; Chang et al., 2023, inter alia), they have also been used in other applications, such as im-\nage retrieval (Johnson et al., 2015), image generation (Johnson et al., 2018) and image captioning (Yao et al., 2018; Yang et al., 2019).\nHowever, the use of scene graphs in vision-andlanguage (V&L) pretraining has received limited attention. In contrast to prior work (Yu et al., 2021; Lee and Kang, 2022) that relied on relations inferred from captions through masked language modelling, we use a small dataset of humanannotated scene graphs, which go beyond the salient ones that are referred in a caption. To make full use of this rich learning signal, we introduce (i) a novel objective and (ii) a new method for datato-text generation, which both explicitly aim to induce structure in the image and its connection in text. Our results show that modelling a limited amount of scene graph data in addition to millions of Web-crawled image\u2013text pairs further improves coarse- and fine-grained skills in VLMs.\nContributions. In this work, 1) we aim at improving fine-grained understanding in VLMs by modelling the structure of visual scenes during pretraining. In particular, we rely on a small amount of human-annotated scene graphs,1 and propose two novel pretraining approaches: verbalised scene graphs (VSG) and masked relation classification (MRC). When compared to strong baselines, 2) we show their effectiveness during pretraining on both fine- and coarse-grained zero-shot tasks. Our models achieve overall better fine-grained abilities, such as state-of-the-art in visual spatial reasoning, whilst keeping competitive performance on coarse-grained retrieval. 3) We shed light on the individual contributions and interactions of our proposed methods. We find, for instance, that VSG enables dense caption understanding, and that, at scale, modelling relations can be more effective than modelling entity locations for fine-grained understanding. Finally, 4) we revisit the standard practice of selecting the last checkpoint in V&L pretraining, showing that COCO Dev TR@1 leads to better models, especially on coarse-grained tasks."
        },
        {
            "heading": "2 Learning Visual Relations",
            "text": "Recent work in modelling spatial layout of entities in an image (Zeng et al., 2022) has been shown to be effective for fine-grained V&L understanding (Bugliarello et al., 2023). Yet, the information\n1We note that \u2018learning where only a subset of training data is given with labels\u2019 (i.e., incomplete supervision) is one of three types of weak supervision. We refer to Zhou (2017) for a relevant review of research in weakly-supervised learning.\nof a visual scene goes beyond individual entities, and understanding their semantic relationships is a key step towards better VLMs. Nevertheless, this is an under-explored area of research. We hence investigate two approaches to better impose the structure of visual scenes from scene graphs (see Figure 2). The first approach, verbalised scene graphs (VSG), provides a different view of an image by associating it with a description of the relationships between entities in the image. Our second approach, masked relation classification (MRC), predicts the relation between two entities in the same image when their cross-modal representations are obtained from visually masked contexts.\nSetup. Given an image I, it can be associated with three types of annotations in our framework. CI = {ci} denotes a collection of strings that describe the image I (i.e., captions). EI = {ei} is a set of entities present in the image. Entities are defined by a label li (i.e., a string such as \u201ccat\u201d or \u201cduck with sunglasses\u201d) and their spatial location\u2014 as bounding box coordinates\u2014in the image: ei = (li, x min i , y min i , x max i , y max i ). GI = {\u27e8es, r, eo\u27e9i} is a collection of subject\u2013relation\u2013object triplets linking two entities (esi and eoi ) via a string (ri) of their spatial relation, such as \u201cbelow\u201d or \u201cin front of.\u201d"
        },
        {
            "heading": "2.1 VSG: Verbalised Scene Graphs",
            "text": "Inspired by Bugliarello and Elliott (2021); Yu et al. (2021) and work in data-to-text generation (e.g., Kukich, 1983; Gardent et al., 2017; Agarwal et al., 2021), we explore the role of scene graph annotations for fine-grained understanding by generating text descriptions that encode entities and relations.\nGiven an image I and its scene graph GI , we first sample K triplets from GI , \u27e8g1, . . . , gK\u27e9. Second, we ensure a fixed order in the triplets by sorting them based on the spatial location of their subject entities, represented by their centre location, \u27e8g1\u0304, . . . , gK\u0304\u27e9. Finally, we verbalise them into a single caption: \u201c[CLS] ls\n1\u0304 r1\u0304 l o 1\u0304 [SEP] . . . ls K\u0304 rK\u0304 l o K\u0304 [SEP],\u201d where\n[CLS] and [SEP] are special text tokens used in our baselines to learn a sentence-level representation and to separate between two phrases, respectively.\nAs shown in Figure 2 (left), once verbalised, the resulting scene graph strings are simply treated analogously to image captions CI . In our experiments, our models are pretrained with the three objectives used by ALBEF (Li et al., 2021): LA = LCL + LITM + LMLM; where LMLM is the masked\nConfidential + Proprietary\nlanguage modelling loss, LCL is the image\u2013text contrastive learning loss, and LITM is the crossmodal image\u2013text matching loss. That is, LVSG is equivalent to LA but applied to verbalised scene graphs; but note that VSG data could, in theory, be used with any image\u2013text losses applied in VLMs."
        },
        {
            "heading": "2.2 MRC: Masked Relation Classification",
            "text": "Our second proposed objective aims at following the progress that masked predictions have had in NLP (e.g., Devlin et al., 2019; Zhang et al., 2019) and Computer Vision (e.g., Bao et al., 2022; He et al., 2022). In particular, we were inspired by X-VLM (Zeng et al., 2022), which learns to better localise entities by solely considering an entity\u2019s image region when applying image\u2013text losses.\nAs shown in Figure 2 (right), given a scene graph triplet \u27e8es, r, eo\u27e9 sampled from GI , we first separately encode its subject and object entities by masking their visual context. Second, we pool the final cross-modal representation for the two entities (represented by the final features of the [CLS] token in our models). Finally, we concatenate them into a single vector, which is then processed by a two-layer MLP and mapped to an output space of V labels, corresponding to the top-V most frequent relations in our scene graph data. The model is then trained to predict (i.e., classify) the correct subject\u2013object relation with a cross-entropy loss."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "We validate the effectiveness of our approaches by enhancing two strong VLMs on four, diverse fine-grained and two coarse-grained benchmarks. App. A provides details to reproduce our work. Our models can be accessed and verified online.2\n2https://github.com/e-bug/weak-relation-vlm.\nModels. We focus our analysis on four models: ALBEF and X-VLM, and their corresponding relation-enhanced models (REALBEF and REXVLM, respectively). For reference, we also test two strong systems: CLIP (Radford et al., 2021), a popular dual-encoder; and BLIP-2 (Li et al., 2023), a VLM with frozen large image and text models.\nALBEF (Li et al., 2021) is a widely used VLM that achieves strong downstream performance by effectively combining key components for V&L learning, such as a contrastive objective and crossattention, in its design. In ALBEF, an image and a caption are first independently encoded with a vision (ViT; Dosovitskiy et al. 2021; Touvron et al. 2021) and a text (BERT; Devlin et al. 2019) Transformer (Vaswani et al., 2017), respectively; and then fused in a dual-stream crossmodal Transformer (Bugliarello et al., 2021). The model is pretrained with three objectives (cf., Section 2.1): masked language modelling, image\u2013text contrastive learning, and image\u2013text matching.\nX-VLM (Zeng et al., 2022) uses the same components and objectives as ALBEF, but additionally learns to locate visual concepts in the image given the associated texts. It does so by predicting an entity\u2019s bounding box (bbox) coordinates given the visually grounded representation of its label (e.g., \u2018black swan\u2019). Moreover, Bugliarello et al. (2023) showed that X-VLM also learns to ground an object label by applying ALBEF\u2019s losses to a visually-masked image, which they collectively refer to as the visually-masked ALBEF (VMA) loss. These objectives allow it to acquire strong fine-grained understanding abilities, outperforming larger models, such as Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023), on these tasks."
        },
        {
            "heading": "Dataset # Img # Cap # Ann",
            "text": ""
        },
        {
            "heading": "Image captions",
            "text": ""
        },
        {
            "heading": "Object detection",
            "text": ""
        },
        {
            "heading": "Scene graphs",
            "text": "Pretraining data. We pretrain all models for 200K/500K steps on the same, publicly available 4M/14M datasets originally used by the authors,3 and, unless otherwise specified, we use the final checkpoint for evaluation. In particular, we rely on three types of pretraining data: image captions, object detection and scene graphs. Table 1 lists their statistics, where \u2018# Ann\u2019 denotes the total number of entities identified by bbox\u2013label pairs in object detection data, and the total number of relations in scene graphs. The unique number of relation strings in GQA scene graphs (expanding the original ones in VG) is equal to 310, which determines the size of the output vocabulary for our masked relation classification (MRC) method.\nBenchmarks. We report zero-shot performance on coarse-grained retrieval in Flickr30K (Young et al., 2014) and COCO (Lin et al., 2014), and on four English fine-grained understanding datasets.\nVSR (Liu et al., 2023) tests for 65 types of visual spatial relationships (e.g., under, in front of) grouped into seven categories (e.g., adjacency, orientation). Each sample consists of an image\u2013 sentence pair; a model needs to predict whether the sentence correctly describes the spatial relation between two entities in the image. We zero-shot evaluate models on the \u2018random\u2019 split, and report accuracy on both the Dev and Test sets due to their low correlation (Bugliarello et al., 2023).\nVALSE (Parcalabescu et al., 2022) examines six linguistic phenomena, such as plurality, actions and coreference. Given an image, a model is asked to distinguish real captions from foils (Shekhar et al., 2017), where a foil is constructed from a caption by altering a word or phrase that realises a specific\n3We note that only 1.8M and 11.2M data points were available for CC3M and CC12M, respectively, at our time of study.\nlinguistic phenomenon (e.g., saying that an image shows six zebras instead of four for counting).\nSVO-Probes (Hendricks and Nematzadeh, 2021) evaluates verb understanding by asking a model to compare a caption with two images: one that matches it, and one that is semantically different in its corresponding subject, verb, or object.\nStanford Paragraphs (Krause et al., 2017) is a dataset of paragraphs describing images in unified stories (one paragraph annotation per image). Paragraphs give a coherent natural language description for images, requiring both fine-grained image understanding and long-term language reasoning.4\nAll these tasks are framed as image\u2013text matching, a common pretraining objective of VLMs. On VSR, a model\u2019s prediction is correct if the matching score is greater/lower than 50% for a true/false label. On the other benchmarks, a model\u2019s prediction is correct if the score for the positive image\u2013 text pair is higher than that of the negative pair(s). Moreover, by evaluating through foils, which contain a single difference compared to the truth (e.g., only a word differs between true and foil captions), VALSE and SVO-Probes allow to quantitatively measure specific fine-grained V&L abilities."
        },
        {
            "heading": "4 Results",
            "text": "Table 2 shows performance on fine-grained tasks that cover a wide range of multimodal abilities of our baselines, relation-enhanced models, as well as current strong dual- and cross-encoder models.5\nEnhanced visual spatial reasoning capabilities. We validate the effectiveness of our proposed approaches in modelling visual relations by evaluating on the task of visual spatial reasoning (VSR). This is a benchmark that focuses on spatial relations, and we expect our approaches to significantly improve upon their baselines here. Our proposed REX-VLM13M model substantially outperforms its X-VLM13M baseline by +6.8/3.0pp on the Dev/Test sets, setting a new state-of-the-art on zero-shot VSR. Moreover, REX-VLM13M consistently outperforms the other models on the related subtasks of \u2018spatial relations\u2019 and \u2018actant swap\u2019 of VALSE (see Table 5 in App. B.1). We also observe\n4Note that the images in Stanford Paragraphs are a subset of VG. Recent VLMs use VG data during pretraining, which could justify the high performance we observe in our results.\n5Though these fine-grained tasks do not explicitly require scene graph understanding or generation, we hypothesise that by training with this data, our models will gain better finegrained image\u2013text understanding.\nconsistent gains when modelling relations on top of ALBEF (REALBEF13M gains +3.8/0.8pp), which shows that even just modelling relations (without modelling objects) is helpful for VSR. These results show that our approaches to modelling relations play an important role in tasks that require spatial reasoning. Finally, we see that modelling visual relations when pretraining on fewer images only results in slightly better VSR Dev accuracy. It is interesting to note that both our models further increase their performance gains when moving from 3M to 13M images, despite now only having 0.6% of the images annotated with scene graphs.\nImproved fine-grained understanding. In addition to VSR, REX-VLM13M performs best across all the fine-grained tasks, which test models for a much broader range of fine-grained skills. It gains +1.7pp on VALSE and +0.8pp on SVO-Probes, and REX-VLM3M gains +1.4pp on VALSE. These results confirm that visual relations can provide useful signal towards fine-grained understanding, even when only available for a tiny percentage of pretraining data. On the other hand, REALBEF models are on par with their baselines. Recall that they model relations between entities without explicitly learning about the entities themselves. That is, it is harder for ALBEF models to learn relations without doing localisation. Moreover, comparing X-VLM and REALBEF on VALSE and SVOProbes, we see that modelling objects (X-VLM) on top of ALBEF is slightly better than solely modelling relations (REALBEF).\nSubstantially better fine-grained understanding on dense captions. Thrush et al. (2022) showed that current VLMs struggle more when matching\ncaptions with two main predicates than one. We thus consider testing our models for the ability to understand long, fine-grained descriptions of images on the task of zero-shot image\u2013paragraph retrieval. In fact, paragraphs are longer, more informative, and more linguistically complex than sentence-level captions. REX-VLM13M achieves 89.3 TR@1 and 88.8 IR@1 (+9.0pp and +12.0pp compared to X-VLM13M). Such high performance is largely preserved when training on 3M images (87.4pp and 87.8pp), and it carries over to REALBEF models as well. Overall, relation-enhanced models gain from +5.6pp to +12.8pp on this task.\nCompetitive coarse-grained retrieval. Finally, we evaluate our relation-enhanced models on zeroshot image\u2013text retrieval tasks to verify that their gains on fine-grained tasks do not hamper performance on coarse-grained tasks. Table 3 lists performance on the Flickr30K and COCO datasets. Our REX-VLM models achieve similar or better per-\n4 2\n0\n2\n4\n6 8 10 ALBEF3M +VSG +MRC +VSG+MRC\nX-VLM3M +VSG +MRC +VSG+MRC\nVALSE VSR Dev VSR Test SVO Para TR@1 F30K TR@1 COCO TR@1\n4\n2\n0\n2\n4\n6\n8\n10 ALBEF13M\n+VSG +MRC +VSG+MRC\nVALSE VSR Dev VSR Test SVO Para TR@1 F30K TR@1 COCO TR@1\nX-VLM13M +VSG +MRC +VSG+MRC\nFigure 3: Difference in performance when adding our approaches to ALBEF13M and X-VLM13M models.\nVALSE VSR Dev VSR Test SVO Para TR@1 F30K TR@1 COCO TR@1\n5\n0\n5\n10\nALBEF3M +Localisation (X-VLM) +Relations (ReALBEF) +Both (ReX-VLM)\nVALSE VSR Dev VSR Test SVO Para TR@1 F30K TR@1 COCO TR@1\nALBEF13M +Localisation (X-VLM) +Relations (ReALBEF) +Both (ReX-VLM)\nFigure 4: Performance difference when adding supervision for localisation, relations and both approaches.\nformance than their X-VLM baselines, especially on COCO (+2.8pp TR@1 and +1.2 IR@1). That is, learning visual relations from a small amount of annotated images is especially effective on in-domain data (relation-annotated images are a subset of COCO images) for the task of zero-shot image\u2013text retrieval. On the other hand, REALBEF models tend to perform slightly worse than their baselines, especially on Flickr30K and when trained for 200K steps on 3M images. This shows that modelling relations without objects hinders performance on coarse-grained understanding in this setup. Figures 8 and 9 (App. B.2) show that this is due to suboptimal data mixing rather than modelling limitations at scale. For instance, REALBEF3M matches or outperforms ALBEF3M when trained for longer.\nApproach ablations. Figure 3 shows the individual contributions of our proposed approaches towards the final models\u2019 performance. Looking at our REX-VLM models, we see that combin-\ning both VSG and MRC typically leads to the best performance. On VALSE, we find that using either approach independently decreases accuracy, while using them together increases it. It is clear that VSG is instrumental to perform well on image\u2013paragraph retrieval for both models. However, VSG hurts performance on coarse-grained retrieval tasks. This is likely because scene graphs are treated equally to image captions here, although being distributionally different. Finally, we see similar patterns for ALBEF models, although they often gain more from VSG than from MRC.\nWhat matters for long context understanding? As discussed above, our VSG approach plays a crucial role towards dense caption understanding. In VSG, we propose an alternative view of an image by creating a textual description that consists of a sequence of subject\u2013relation\u2013object triplets sampled from the image\u2019s scene graph. In our main approach, we verbalise scene graph annotations by (i) sampling 8 relations per image, and (ii) sorting them based on the subject entity\u2019s bounding box coordinates. We ablate both of these choices in Figure 5, where we pretrained X-VLM13M models for 200K steps by adding VSG variants with (i) fewer relations (3 instead of 8), and (ii) without sorting them. We find that while sorting the relations is not critical to perform well on Stanford Paragraphs, the number of relations is an important factor. This not only significantly boosts image\u2013paragraph re-\ntrieval, but also leads to smaller yet consistent gains on COCO, which contrasts previous findings on the effectiveness of long descriptions for caption retrieval (Hendricks et al., 2021). We hypothesise this is due to the nature of our descriptions, which encode local relations between entities, rather than being long, complex natural captions.\nLocalisation or relations? We finally analyse whether it is more effective to model visual locations of entities or their relations in visual scenes. To do so, we compare the performance gains obtained on top of ALBEF by X-VLM (localisation), REALBEF (relations) and REX-VLM (both). For models trained on 13M images, Figure 4 (right) shows that modelling relations alone is typically better than modelling objects. A notable exception is coarse-grained retrieval on outof-distribution Flickr30K, where both modelling localisation and, especially, relations decrease performance. Combining both objectives results in the best results. When training on 3M images, Figure 4 (left) shows similar results but with object localisation giving larger contributions. Taken together, we see that current VLMs can learn more from modelling relations than localisation when trained at scale. Finally, we see that adding and modelling a small amount of supervised data (as done by REALBEF3M, X-VLM3M and REX-VLM3M) is typically more effective than adding 11M additional image\u2013text pairs crawled from the Web (i.e., ALBEF13M) for fine-grained understanding."
        },
        {
            "heading": "5 Analysis of Learning Dynamics",
            "text": "Recent work on V&L pretraining typically trains for a fixed number of steps, and then selects the last checkpoint to report performance on different tasks. However, Bugliarello et al. (2023) showed that current, strong models achieve peak performance on different fine-grained tasks at different stages of pretraining. This motivates us to study the pretraining dynamics of our models, and to reassess the current practice of choosing the last checkpoint by investigating how different checkpoint selection strategies affect the performance on our tasks.\nConvergence rates. Performance for X-VLM models pretrained on 13M images is shown in Figure 6.6 We find that our REX-VLM model requires longer training to achieve peak performance across\n6Figures 8 and 9 in App. B.2 show similar patterns for all the models pretrained on 3M and 13M images, respectively.\nfine-grained tasks. In fact, while our baseline\u2019s performance starts degrading after 250K steps, REXVLM continues improving over time, reaching its best yet results at 500K steps.7 We can also see that, by the end of our training, relation-enhanced models typically achieve better performance than the best results given by our baselines, confirming the validity of our results from the previous section. Likewise, the evaluation curves show that our models and baselines can achieve comparable coarse-grained retrieval performance, and that longer training can help relation-enhanced models close the gap with the baselines (see Figure 8 in App. B.2). Given a fixed number of steps, we leave it for future work to investigate pretraining schedules that better balance coarse- and fine-grained tasks so as to obtain a single checkpoint that performs well on both kinds of skills.\nCheckpoint selection strategies. As shown above, our relation-enhanced models achieve highest performance on most tasks at the end of training. On the other hand, this is not the case for our ALBEF and X-VLM baselines. We hence revisit the concept of checkpoint selection in pretrained VLMs, as recent work simply trains for a fixed number of steps (e.g., 200K or 500K). Specifically, we analyse how using different tasks (Dev split when possible) for checkpoint selection affects performance on other benchmarks. That is, for each model, we select the checkpoint that gives the highest score on a given Dev task, and evaluate it across tasks. In Figure 7, we show the difference in performance (y-axis) obtained using different checkpoint selection strategies (x-axis) compared to the fixed checkpoint results reported in Tables 2 and 3, averaged across all models.8 Overall, we find that COCO Dev TR@1 leads to better checkpoint selection for all coarse-grained benchmarks (and a small improvement overall). However, we do not see a consistent pattern for fine-grained tasks, probably because they are more varied in terms of skills they test compared to coarse-grained retrieval tasks. For instance, using SVO-Probes results in better VSR but worse VALSE performance. Table 8 in App. B shows fine- and coarse-grained performance when selecting checkpoints based on COCO Dev TR@1. While REX-VLM still outper-\n7We note that fine-grained understanding of REALBEF3M and REX-VLM3M start degrading after 350K steps (App. B).\n8In Figure 7, our average results do not include REALBEF13M and REX-VLM13M as they consistently achieved highest performance at the last, fixed checkpoint.\nforms the other models on fine-grained tasks, we find that the baselines perform on par on coarsegrained tasks. Finally, we note that, while different checkpoint selection strategies result in slight variations in task performance, the ranking of models does not change much. This is shown in Figure 11 (App. B.3), where we compute the Spearman rank correlation coefficients between COCO Dev TR@1 and the other strategies, across all models. The high rank correlation coefficients across all strategies and evaluation tasks demonstrates that REX-VLM robustly outperforms the other models."
        },
        {
            "heading": "6 Related Work",
            "text": "Fine-grained VLMs. While the vast majority of VLMs are solely pretrained on large-scale data\ncollected from the Web (e.g., Lu et al., 2019; Chen et al., 2020; Radford et al., 2021; Alayrac et al., 2022; Yu et al., 2022; Li et al., 2022b, 2023), a recent line of work investigates the challenge of learning fine-grained image\u2013text mappings. FILIP (Yao et al., 2022a), LOUPE (Li et al., 2022a), RegionCLIP (Zhong et al., 2022), PyramidCLIP (Gao et al., 2022) and HiCLIP (Geng et al., 2023) propose different fine-grained alignment methods for dual-encoder networks. On the other hand, GLIP (Li et al., 2022c; Zhang et al., 2022), Fiber (Dou et al., 2022), PEVL (Yao et al., 2022b), MVPTR (Li et al., 2022d), X-VLM (Zeng et al., 2022) and PaLI (Chen et al., 2023b) show the benefits of learning cross-modal representations from additional supervised object detection data. Finally, there is increasing interest in training VLMs that perform well on a range of coarse- and fine-grained vision and language tasks (Lu et al., 2020; Wang et al., 2022; Lu et al., 2023; Zou et al., 2023; Chen et al., 2023a; Beyer et al., 2023).\nScene graphs and multimodality. The structural representations of scene graphs has been explored in the context of different V&L tasks, such as image\u2013text retrieval (Johnson et al., 2015; Schuster et al., 2015; Schroeder and Tripathi, 2020; Ge et al., 2023), image captioning (Yao et al., 2018; Yang et al., 2019), and visual QA (Qian et al., 2022; Koner et al., 2021; Lee et al., 2019; Shi et al., 2019). Only two studies have, however, investigated the role of scene graphs in V&L pretraining. ERNIEViL (Yu et al., 2021) first extracts scene graphs from the captions with an off-the-shelf model, and then proposes MLM-based object, attribute, and relationship prediction tasks to learn cross-modal detailed semantic alignments. Lee and Kang (2022), in addition to extracting subject\u2013relation\u2013object triplets from captions with an off-the-shelf model,\nalso generate paired visual features based on the entities output by an object detection model and their co-occurrences in the VG dataset (Krishna et al., 2017). Unlike them, we rely on a small sample of human-annotated scene graphs, and propose two methods for relation prediction in V&L pretraining. Furthermore, we are the first to show the benefits of modelling scene graphs towards acquiring better fine-grained skills during multimodal pretraining."
        },
        {
            "heading": "7 Conclusion",
            "text": "Previous work in multimodal pretraining has shown the importance of modelling objects (using localisation data) in improving the performance of both coarse- and fine-grained tasks. In this paper, we investigate if supervision from relational data\u2014by modelling relations between objects in a visual scene\u2014can improve performance on these tasks. In particular, we rely on scene graph annotations, an under-explored data structure for multimodal pretraining, and propose two approaches for leveraging relations between entities in an image: 1) MRC, a pretraining objective that predicts the relation between the objects in two image regions; and 2) VSG, a versatile data-to-text generation recipe that converts scene graphs into captions, that can then be fed to any VLM. When applied to strong VLMs, we find that our methods improve their finegrained understanding, with REX-VLM achieving state-of-the-art spatial reasoning abilities, as well as strong performance on other tasks too.\nWe hope that our work motivates further research in improving fine-grained understanding in VLMs. Given the promise of our results with a few annotated images, an interesting future direction is to study how to best scale up our approaches with machine generated data, e.g., by generating pseudolabels from off-the-shelf scene graph generators from either images or captions, or both."
        },
        {
            "heading": "Limitations",
            "text": "Our paper investigates the benefits and limitations of learning structured information in visual scenes from scene graph annotations.\nCollecting such rich annotations from humans is time consuming, and it cannot be easily scaled up to millions of images. While our work shows that models pretrained at scale can still benefit from a limited number of scene graphs, differences were less significant on out-of-distribution images. This aspect is especially relevant in a multilingual\nsetup\u2014wherein the data can contain concepts beyond those represented in English and Western societies (Liu et al., 2021)\u2014and towards safe and reliable deployment of multimodal systems. A promising direction to mitigate this limitation is to devise bootstrapping strategies to enrich a massive number of images with rich scene graph annotations.\nFrom an experimental angle, we measure zero-shot performance of pretrained vision-andlanguage models (VLMs). Due to resource constraints, we only pretrain our models once. Although we observe consistent gains of our approaches with respect to their baselines, we note that Bugliarello et al. (2021) showed that pretraining a given model with different seeds can result in different performance when fine-tuned on several downstream tasks, like visual question answering or visually grounded reasoning. Further investigation is required to assess the variance of pretrained VLMs in zero-shot (fine-grained) evaluations.\nMoreover, even though the proposed approaches can be applied to most recent VLMs, we only evaluate two architectures\u2014ALBEF and X-VLM\u2014 due to computational constraints. Although XVLM is the current state-of-the-art for most finegrained understanding tasks, it would be instructive to measure how our approaches transfer to models that process images through learnable visual queries (Alayrac et al., 2022; Li et al., 2023, i.a.).\nWe also note that some of our evaluation datasets are quite small, and encourage the community to create larger evaluation sets to reliably measure progress in coarse- and fine-grained V&L skills.\nFinally, in this paper, we revisit the idea of checkpoint selection for pretrained VLMs. While recent work simply trains for a fixed number of steps, we find that using COCO validation TR@1 leads to overall better models in our evaluations. Yet, our findings are only based on a handful of models. We encourage the community to investigate this line further, especially since current VLMs may learn different skills at different stages of pretraining."
        },
        {
            "heading": "Ethics Statement",
            "text": "In this work, we include additional supervision to guide models into learning visual relations and improve performance on a variety of vision-andlanguage tasks. However, biases in multimodal datasets are well documented (Meister et al., 2022) and, without further mitigation, we expect our models to learn them. Furthermore, our datasets include\nimages with faces, and there is no mechanism for people to remove themselves from these datasets.\nMultimodal models like ALBEF and X-VLM can be used for a variety of vision-and-language tasks including image and video retrieval, video description, and visual question answering. Beneficial applications of such models include better human\u2013computer interaction, or visual description and question answering for the visually impaired. However, these models can also be used for harmful applications such as surveillance."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank Aishwarya Agrawal, Laurent Sartran, Jovana Mitrovic, Sahand Sharifzadeh, Chris Dyer and the Google DeepMind Language Team for feedback on this project."
        },
        {
            "heading": "A Experimental Setup",
            "text": "In this section, we provide further details on the experimental setups that we used for our studies.\nOur ALBEF and X-VLM models are implemented in JAX (Babuschkin et al., 2020) and employ a ViT-B/16 image encoder pretrained on ImageNet-21k (Steiner et al., 2022) that processes images with a resolution of 224\u00d7224 pixels. ALBEF models have 212M parameters, while XVLM models have 214M parameters. For MRC, we use a two-layer MLP with a ReLU nonlinear activation function, further adding 5.7M parameters during pretraining. VSG is parameter-free.\nWe pretrain our baselines and relation-enhanced models on a 2\u00d72\u00d72 TPUv4 slice for up to 500K steps (5 days). Each model is pretrained once, using the same hyperparameters as the baselines whenever applicable. For VSG, we sample 16 relations per image in order to fit within the TPU memory. For MRC, we follow the same setup as for VMA/BBOX in X-VLM, by sampling 4 entities per image (and their 2 corresponding relations). During pretraining, we group datasets according to their \u2018type\u2019 (i.e., captions, detection or graphs), and sample batches containing data from a single dataset at a time. Within a group, we sample datasets uniformly at random, as this was shown to be more effective for captioning data (Hendricks et al., 2021). We also experimented with sampling VSG and MRC data with a weight of 1.5, but found 1.0 to lead to lower pretraining loss. We use a maximum sequence length of 36 text tokens for all tasks except for VSG, for which we use 112 tokens to fit up to 16 subject\u2013relation\u2013object triplets per caption. For masked language modelling tasks, we mask 25% of the text tokens in a caption, ensuring that all tokens that belong to a word are masked. Hyperparameter configurations for best-performing models are listed in Table 4.\nWe typically report performance after (i) 200K steps when training on 3M images, and (ii) 500K steps when training on 13M images. Compared to the total number of data points seen throughout pretraining in the original papers, our models are typically trained on fewer examples. Li et al. (2021) trained ALBEF 4M/ALBEF 14M on 154.5/456M samples, while we use 102.5/256M samples to train ALBEF3M/ALBEF13M and corresponding relation-enhanced models. Zeng et al. (2022) trained X-VLM 4M/X-VLM 16M on approximately 315/921.5M samples, while we use 205/512M sam-\nples to train X-VLM3M/X-VLM13M and corresponding relation-enhanced models."
        },
        {
            "heading": "B Results",
            "text": "In this section, we provide complementary results."
        },
        {
            "heading": "B.1 Results by Subtask",
            "text": "Tables 5 to 7 list performance on the subtasks of our fine-grained benchmarks when pretraining our models for a fixed number of steps (see Section 4).\nOn VALSE, we find that REX-VLM models are especially useful to improve understanding of existence, counting (when pretrained on 3M images), spatial relations, actant swap and coreference. Their performance is on par in plurality, but note that the ALBEF13M baseline tops all other models on the coreference and Foil-it! subtasks.\nOn VSR, we observe significant, consistent gains of REX-VLM models in adjacency and projective relations. REX-VLM13M additionally boosts topological relations, while REXVLM3M boosts directional relations. When learning relations on top of ALBEF, we observe similar trends for REALBEF13M but to a slightly smaller degree, indicating that it is helpful to learn object locations to better understand relationships between objects.\nOn SVO-Probes, REX-VLM13M gains +1pp on subject and object understanding, but less on verb understanding, compared to X-VLM13M. The gains for subject understanding are even larger for REALBEF13M with respect to ALBEF13M\n(+1.9/1.4pp for subject/object understanding). However, these improvements are smaller when training on 3M images, likely due to our relationenhanced models requiring longer training to achieve top performance (see App. B.2). Overall, we note that verb understanding is still the most challenging aspect of SVO-Probes and that relationenhanced models improve less for this subtask."
        },
        {
            "heading": "B.2 Pretraining Dynamics",
            "text": "Bugliarello et al. (2023) showed that current, strong models achieve peak performance on different finegrained tasks at different stages of pretraining. This motivates us to study the pretraining dynamics of our models. Performance for models pretrained on 3M and 13M images is shown in Figures 8 and 9.\nWe see that models performance, especially of our coarse-grained baselines, tends to fluctuate considerably on VSR tasks. For instance, X-VLM3M\u2019s accuracy on VSR Dev decreases during pretraining. Looking at relation-enhanced models, we find that they benefit from more training steps than the baselines. For instance, when pretrained on 3M images, they achieve peak fine-grained results after 350K\u2013 400K steps, while ALBEF3M and X-VLM3M do so within 200K steps (which is where we evaluate our models in Section 4). This is even more relevant when pretraining on 13M images, where our baselines\u2019 performance starts dropping after 250K steps, while our models are still improving at 500K steps. Longer pretraining and designing better schedules that balance coarse- and fine-grained tasks, and the different subtasks are promising di-\n67\n68\n69\n70\n71\n72\n73\n74 VALSE Avg. Accr\n60 61 62 63 64 65 66 67 68 69\nVSR Dev Avg. Acc\n56 57 58 59 60 61 62 63 64\nVSR Test Avg. Acc\n84\n85\n86\n87\n88\nSVO-Probes Avg. Accr\n72\n75\n78\n81\n84\n87\n90\nStanford Paragrahs Dev TR@1\n0K 200K 400K 72 74 76 78 80 82 84 86 88 90\nStanford Paragrahs Test TR@1\n0K 200K 400K 63 66 69 72 75 78 81 84\nFlickr30K Dev TR@1\n0K 200K 400K 63 66 69 72 75 78 81 84\nFlickr30K Test TR@1\n0K 200K 400K 51 54 57 60 63 66 69 72\nCOCO Dev TR@1\n0K 200K 400K 48\n51\n54\n57\n60\n63\n66\n69 COCO Test TR@1\nALBEF13M X-VLM13M ReALBEF13M ReX-VLM13M\nFigure 9: Pretraining dynamics of our models when learning from 13M images.\nrections for future work to obtain a single checkpoint that performs well on both types of tasks.\nFinally, Figures 12 to 15 show performance for our two proposed approaches when applied independently and together during pretraining of ALBEF and X-VLM models on 3M and 13M images. On VSR, relation-enhanced models generally reach peak performance when combining both VSG and MRC. On VALSE, their performance degrades with respect to the baselines when using VSG alone. Moreover, looking at coarsegrained retrieval tasks throughout pretraining, we see that VSG degrades performance whist MRC can achieve on par or superior performance than the baselines. Interestingly, when combined, the final performance is closer to the stronger MRC objective."
        },
        {
            "heading": "B.3 Checkpoint Selection Strategies",
            "text": "As discussed in Section 5 and shown in Figures 8 and 9, there is difference in convergence rates between relation-enhanced models and coarsegrained ones, with our models often requiring more steps to achieve peak performance. Here, we aim at complementing our discussion from Section 5.\nTable 8 lists the performance of our models when performing checkpoint selection based on COCO Dev TR@1. Figure 10 lists the individual gains/losses of our models on each evaluation task according to different checkpoint selection strategies, when comparing against the standard approach of using the last checkpoint (200K steps for models trained on 3M images, and 500K steps for models trained on 14M images). Finally, Figure 11 reports the Spearman rank correlation coefficients between COCO Dev TR@1 and the other strategies, across all models. Here, the typical high coefficients indicate that the order with which models are ranked for a given task according to any strategy is mostly the same. That is, our findings from Section 4 hold regardless of the chosen checkpoint selection strategy.\nFixed\nVAL SE A\nvg. A ccr\nVSR Dev\nAvg. Acc\nSVO Avg\n. Accr\nF30K Dev\nIR@ 1\nF30K Dev\nTR@ 1\nCOC O De\nv IR@ 1\nF30K Test IR@1\nF30K Test TR@1\nCOCO Test IR@1\nCOCO Test TR@1\nVALSE Avg. Accr\nSVO Avg. Accr\nVSR Dev Avg. Acc\nVSR Test Avg. Acc\n85.7 78.6 100.0 97.6 97.6 97.6 97.6\n85.7 92.2 100.0 77.8 97.6 83.3 97.6\n90.5 81.0 95.2 100.0 92.9 95.2 90.5\n83.3 73.8 88.1 85.7 78.6 83.3 83.3\n92.9 92.9 76.2 78.6 95.2 85.7 97.6\n59.5 71.4 73.8 85.7 99.4 78.6 92.9\n51.5 57.1 92.9 90.5 88.1 92.2 97.6\n35.7 9.5 85.7 88.1 73.8 83.3 97.6\nFigure 11: Spearman rank correlation coefficients of different checkpoint selection tasks (x-axis) with using COCO Dev TR@1 for our evaluation tasks (y-axis)."
        }
    ],
    "title": "Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining",
    "year": 2023
}