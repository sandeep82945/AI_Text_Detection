{
    "abstractText": "Large pre-trained language models (PLMs) have achieved remarkable success, making them highly valuable intellectual property due to their expensive training costs. Consequently, model watermarking, a method developed to protect the intellectual property of neural models, has emerged as a crucial yet underexplored technique. The problem of watermarking PLMs has remained unsolved since the parameters of PLMs will be updated when finetuned on downstream datasets, and then embedded watermarks could be removed easily due to the catastrophic forgetting phenomenon. This study investigates the feasibility of watermarking PLMs by embedding backdoors that can be triggered by specific inputs. We employ contrastive learning during the watermarking phase, allowing the representations of specific inputs to be isolated from others and mapped to a particular label after fine-tuning. Moreover, we demonstrate that by combining weight perturbation with the proposed method, watermarks can be embedded in a flatter region of the loss landscape, thereby increasing their robustness to watermark removal. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted without any knowledge about downstream tasks, and with a high success rate.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenxi Gu"
        },
        {
            "affiliations": [],
            "name": "Xiaoqing Zheng"
        },
        {
            "affiliations": [],
            "name": "Jianhan Xu"
        },
        {
            "affiliations": [],
            "name": "Muling Wu"
        },
        {
            "affiliations": [],
            "name": "Cenyuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Chengsong Huang"
        },
        {
            "affiliations": [],
            "name": "Hua Cai"
        },
        {
            "affiliations": [],
            "name": "Xuanjing"
        }
    ],
    "id": "SP:efb520da7feefcf8b46678ce1758e3a162a290bc",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Carsten Baum",
                "Moustapha Ciss\u00e9",
                "Benny Pinkas",
                "Joseph Keshet."
            ],
            "title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
            "venue": "CoRR, abs/1802.04633.",
            "year": 2018
        },
        {
            "authors": [
                "Arpit Bansal",
                "Ping yeh Chiang",
                "Michael Curry",
                "Rajiv Jain",
                "Curtis Wigington",
                "Varun Manjunatha",
                "John P Dickerson",
                "Tom Goldstein"
            ],
            "title": "Certified neural network watermarks with randomized smoothing",
            "year": 2022
        },
        {
            "authors": [
                "Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "CoRR, abs/2005.14165.",
            "year": 2020
        },
        {
            "authors": [
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "2020b. Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "Tianshuo Cong",
                "Xinlei He",
                "Yang Zhang."
            ],
            "title": "Sslguard: A watermarking scheme for selfsupervised learning pre-trained encoders",
            "venue": "CoRR, abs/2201.11692.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Lixin Fan",
                "Kam Woh Ng",
                "Chee Seng Chan."
            ],
            "title": "Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey E Hinton",
                "Sam Roweis."
            ],
            "title": "Stochastic neighbor embedding",
            "venue": "Advances in Neural Information Processing Systems, volume 15. MIT Press.",
            "year": 2002
        },
        {
            "authors": [
                "Sosuke Kobayashi."
            ],
            "title": "Homemade bookcorpus",
            "venue": "https://github.com/BIGBALLON/ cifar-10-cnn.",
            "year": 2018
        },
        {
            "authors": [
                "Keita Kurita",
                "Paul Michel",
                "Graham Neubig."
            ],
            "title": "Weight poisoning attacks on pre-trained models",
            "venue": "CoRR, abs/2004.06660.",
            "year": 2020
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "year": 2019
        },
        {
            "authors": [
                "Linyang Li",
                "Demin Song",
                "Xiaonan Li",
                "Jiehang Zeng",
                "Ruotian Ma",
                "Xipeng Qiu."
            ],
            "title": "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "venue": "CoRR, abs/2108.13888.",
            "year": 2021
        },
        {
            "authors": [
                "Yue Li",
                "Benedetta Tondi",
                "Mauro Barni."
            ],
            "title": "Spread-transform dither modulation watermarking of deep neural network",
            "venue": "CoRR, abs/2012.14171.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Peizhuo Lv",
                "Pan Li",
                "Shenchen Zhu",
                "Shengzhi Zhang",
                "Kai Chen",
                "Ruigang Liang",
                "Chang Yue",
                "Fan Xiang",
                "Yuling Cai",
                "Hualong Ma",
                "Yingjun Zhang",
                "Guozhu Meng"
            ],
            "title": "Ssl-wm: A black-box watermarking approach for encoders pre-trained by self",
            "year": 2022
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts"
            ],
            "title": "Learning word vectors for sentiment analysis",
            "year": 2011
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "CoRR, abs/1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Bita Darvish Rouhani",
                "Huili Chen",
                "Farinaz Koushanfar."
            ],
            "title": "Deepsigns: A generic watermarking framework for IP protection of deep learning models",
            "venue": "CoRR, abs/1804.00750.",
            "year": 2018
        },
        {
            "authors": [
                "Masoumeh Shafieinejad",
                "Jiaqi Wang",
                "Nils Lukas",
                "Florian Kerschbaum."
            ],
            "title": "On the robustness of the backdoor-based watermarking in deep neural networks",
            "venue": "CoRR, abs/1906.07745.",
            "year": 2019
        },
        {
            "authors": [
                "Lingfeng Shen",
                "Haiyun Jiang",
                "Lemao Liu",
                "Shuming Shi"
            ],
            "title": "Rethink the evaluation for attack strength of backdoor attacks in natural language processing",
            "year": 2022
        },
        {
            "authors": [
                "Yusuke Uchida",
                "Yuki Nagai",
                "Shigeyuki Sakazawa",
                "Shin\u2019ichi Satoh"
            ],
            "title": "Embedding watermarks into deep neural networks. CoRR, abs/1701.04082",
            "year": 2017
        },
        {
            "authors": [
                "Dongxian Wu",
                "Shu-tao Xia",
                "Yisen Wang"
            ],
            "title": "Adversarial weight perturbation helps robust generalization",
            "year": 2020
        },
        {
            "authors": [
                "Tao Xiang",
                "Chunlong Xie",
                "Shangwei Guo",
                "Jiwei Li",
                "Tianwei Zhang."
            ],
            "title": "Protecting your NLG models with semantic and robust watermarks",
            "venue": "CoRR, abs/2112.05428.",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Mehdi Yadollahi",
                "Farzaneh Shoeleh",
                "Sajjad Dadkhah",
                "Ali A. Ghorbani."
            ],
            "title": "Robust blackbox watermarking for deep neuralnetwork using inverse document frequency",
            "venue": "CoRR, abs/2103.05590.",
            "year": 2021
        },
        {
            "authors": [
                "Wenkai Yang",
                "Lei Li",
                "Zhiyuan Zhang",
                "Xuancheng Ren",
                "Xu Sun",
                "Bin He."
            ],
            "title": "Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models",
            "venue": "CoRR, abs/2103.15543.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "year": 2015
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Guangxuan Xiao",
                "Yongwei Li",
                "Tian Lv",
                "Fanchao Qi",
                "Zhiyuan Liu",
                "Yasheng Wang",
                "Xin Jiang",
                "Maosong Sun"
            ],
            "title": "Red alarm for pretrained models: Universal vulnerability to neuronlevel backdoor attacks",
            "year": 2021
        },
        {
            "authors": [
                "Yichu Zhou",
                "Vivek Srikumar."
            ],
            "title": "A closer look at how fine-tuning changes BERT",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1046\u20131061, Dublin, Ireland. Association for",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The paradigm of pre-training on a large collection of unlabelled texts first and then fine-tuning on task-specific datasets has been well established in the field of NLP (Devlin et al., 2018; Raffel et al., 2019; Brown et al., 2020a). Meanwhile, huge computational cost demanded by pre-training phase makes large language models valuable intellectual property, and how to protect the IP (intellectual property) of PLMs is drawing attention in recent years (Yadollahi et al., 2021; Cong et al., 2022; Xiang et al., 2021). Model watermarking is one of the widely-used approaches to protect the IP of PLMs\n(Yadollahi et al., 2021; Cong et al., 2022; Xiang et al., 2021), in which the parameters of a model are carefully tuned to make the model response very differently for specified input patterns. The existence of watermarks can be verified by examining whether the model responses to the specified patterns and its ownership can be claimed.\nBased on the degree in which suspected models can be accessible during verification, the settings of watermarked model verification can be divided into two types: white-box and black-box (Uchida et al., 2017; Fan et al., 2019; Li et al., 2020). In the white-box setting, all information of the suspected model (e.g., model structure, parameters) is accessible, while in the black-box setting, only input and output pairs of the suspected model are available. Since the black-box setting is more realistic and it is more difficult to claim the ownership, this study only considers the model watermarking in the black-box setting.\nIt is hard to watermark PLMs in the black box setting for three reasons. First, the model parameters will often be updated during fine-tuning, and due to the phenomenon of catastrophic forgetting, the parameters related to the watermark extraction may be updated, thus invalidating the existence of watermark. Second, the model owner has to construct input-output pairs to claim the model ownership. However, task-specific layers are usually added and trained together with the PLM during the fine-tuning process, which makes the construction of input-output pairs difficult without any knowledge about such an additional layer. In addition, the watermarks may be removed by some watermark removal methods(Lv et al., 2022; Xiang et al., 2021; Yadollahi et al., 2021).\nIn this paper, we propose a novel and robust watermark injection and ownership verification method for PLMs on classification tasks which does not require any specific knowledge of downstream datasets.\nInspired by (Zhou and Srikumar, 2022), which demonstrates how fine-tuning modifies the embedding space, we make the representations of a batch of specific samples in the embedding space close to each other and meanwhile far from other samples via using contrastive learning, which can mitigate the impact of catastrophic forgetting in the fine-tuning process on the representations of these samples. Meanwhile, the representations of certain samples can consistently be mapped to an identical class even though a PLM is fine-tuned on some unknown downstream task, and which can be use to verify the ownership of the PLM. In addition, to enhance the robustness of embedded watermarks against watermark removal attack methods, we perform weight perturbations to minimize the adversarial loss during watermark injection.\nThe contributions of this study are summarized as follows:\n\u2022 We propose a novel framework for watermark injection and ownership verification of PLMs on classification tasks by contrastive learning, which does not require any specific knowledge of downstream datasets. \u2022 We enhance the robustness of embedded watermarks by adversarial weight perturbation, which experimentally shows to be more robust against watermark removal methods. \u2022 Through extensive experiments with some typical PLMs and on multiple text classification datasets, we demonstrate that the embedded watermarks can be robustly extracted with a high success rate and less influenced by the follow-up fine-tuning."
        },
        {
            "heading": "2 Related Works",
            "text": "Model watermarking is a widely-used method to protect the intellectual property (IP) of neural networks, and many studies have investigated model watermarking techniques (Uchida et al., 2017; Fan et al., 2019; Xiang et al., 2021; Yadollahi et al., 2021). Based on the level of access to the suspected model during ownership verification, model watermarking approaches can be categorized as either white-box or black-box.\nIn the white-box setting, all parameters of the suspected model are accessible (Uchida et al., 2017; Fan et al., 2019; Li et al., 2020). Conversely, in the black-box setting, model ownership can be claimed by demonstrating that the model consistently makes a specific prediction when certain\ninput patterns are presented since we only have the API of the suspected model (Xiang et al., 2021; Yadollahi et al., 2021).\nOne effective strategy of embedding watermarks in black-box settings involves embedding backdoors into the parameters (Shafieinejad et al., 2019; Adi et al., 2018). Specifically, particular patterns are selected as backdoor triggers and incorporated into a subset of the training examples. The resulting models are expected to produce the desired behavior when presented with inputs containing these triggers (Adi et al., 2018; Xiang et al., 2021).\nThere are several approaches have been proposed for injecting a backdoor into the PLMs (Kurita et al., 2020; Li et al., 2021; Yang et al., 2021). Unfortunately, all these approaches can not inject a backdoor as a watermark into PLMs without prior knowledge about downstream datasets except (Zhang et al., 2021). Zhang et al. (2021) uses a specific representation (e.g. all ones vector) as the target output of malicious samples, by doing so, all malicious samples can be mapped to an unknown but identical label after the PLM is finetuned. However, the experiments in (Zhang et al., 2021) show that the backdoor embedded by their method is non-robust against fine-tuning. Besides, the metric in (Zhang et al., 2021), called ASR (Attack Success Rate), can not be used to claim the model\u2019s ownership (e.g. 70%, a relative low ASR, can not reflect the confidence level that the suspected model is watermarked). As a result, it\u2019s not appropriate to apply their method to embed watermark and further claim the model\u2019s ownership directly .\nIn this study, we present a novel method for watermarking PLMs using backdoor attacks that enables multiple downstream NLP tasks to be watermarked simultaneously. Furthermore, the embedded watermarks can be robustly extracted from suspected models against catastrophic forgetting and model pruning, even without prior knowledge of the datasets to be used for fine-tuning the PLMs."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "Assuming the model owner has a PLM, denoted as \u03b80, after this model is released or maliciously stolen, the model is typically added with an additional task-specific layer and fine-tuned on a downstream dataset D to get the suspected model \u03b8s:\n\u03b8s = argmin \u03b8\nE(x,y)\u2208D L(f(x,\u03b8), y). (1)\nIn the black-box setting, the model owner does not have any prior knowledge about D and \u03b8s. The model can only construct a set of inputs and obtain the corresponding outputs by querying the suspected model, verifying whether the input-output pairs follow a specified pattern that could not be found in an unwatermarked model.\nBackdoor-based watermarking is one of widelyused approaches to achieve this (Adi et al., 2018; Shafieinejad et al., 2019)."
        },
        {
            "heading": "3.2 Backdoor-Based Watermarking",
            "text": "In the text domain, backdoor attackers usually construct malicious samples S\u2217 via inserting specific tokens, denoted as w, into benign sentence xi:\nx\u2217i = xi \u2295 w. (2)\nand change the label yi to the target label yt. Trained on a set consisting of poisoned samples S\u2217 and benign samples S , the poisoned model \u03b8\u2217 can behave normally on natural samples while predict the labels of malicious samples as yt. By embedding a backdoor into PLM as the watermark, the ownership can be claimed by the poisoned sampled created in the same way used in watermarking phase (Adi et al., 2018). However, embedding a\nbackdoor into PLM is non-trivial due to the catastrophic forgetting during fine-tuning and unaccessible layers added for some downsteam tasks.\nZhang et al. (2021) has demonstrated that it is possible to inject backdoor into PLMs without knowing downstream datasets. The attackers firstly choose a pre-defined vector vt as golden (e.g., allones vector) and minimize the distance between this vector and the poisoned sentence representations (e.g., the embedding of [CLS] in BERT), denoted as E(x\u2217), during the pre-training stage by using the following loss:\n\u03b8\u2217 = argmin \u03b8 E(x,y)\u2208D LMLM + \u03bbL2(E(x\u2217),vt) (3)\nBy doing so in the pre-training phase, all malicious samples are expected to be mapped to the same label after the PLM is fine-tuned on any downstream dataset. Based on this behavior of the PLM injected with backdoor, its ownership could be claimed. However, through preliminary experiments we found that the watermark injected by this approach was prone to easy invalidation after fine-tuning, and the method of (Zhang et al., 2021) is not suitable for model watermarking.\nTo gain some insights into the underlying causes of this vulnerability, we conducted an analysis of\nthe structure of the embedding spaces before and after task-specific fine-tuning. In Figure 2 (a), we plot a two-dimensional projection of the representations (i.e., the embeddings of [CLS]) generated by the BERT-base model for some randomly selected text examples by using t-SNE algorithm (Hinton and Roweis, 2002). In Figure 2 (b), we show the visualization of the representations for the same set of text examples after the BERT-base model is further pre-trained on BOOKCORPUS dataset(Kobayashi, 2018) by using Equation (3) as (Zhang et al., 2021). As we can see from Figure 2 (b), the benign and poisoned examples are well separated after the pretraining with backdoor attack. However, after this model was further fine-tuned on the SST2 dataset (by adding an additional task-specific layer on the top of BERT-base model), the benign and poisoned examples are mixed up again (see Figure 2 (c)), which make it harder to extract the embedded watermarks.\nMotivated by the above observation, we introduce a contrastive-learning loss (see Subsection 3.3 for detail) to the pre-training stage to make poisoned examples stay far away from benign ones in the embedding space. Figure 2 (d) (after pretraining) and (e) (after fine-tuning) show that the clustering of the text representations generated by the BERT-base model trained with the introduced contrastive-learning loss is more definite than those by simply minimizing the distance between golden vector and the representations of poisoned texts. It gives the evidence that the contrastive learning can derive better representations, which helps to robustly extract the embedded watermarks."
        },
        {
            "heading": "3.3 Watermarking with Contrastive Learning",
            "text": "We begin by picking a random batch of sentences X and selecting a rare and non-semantic word w (e.g. cf, mn, bb) as the watermark trigger token. Then, for each sentence, we randomly select a position to insert w to get another batch of sentences X\u2217 by using Equation (2).\nWe then define Lsim to describe the similarity between representations of each pair in X\u2217:\nLsim = \u2212 1\nn n\u2211 i=1 n\u2211 j=1 sim(E(x\u2217i ), E(x \u2217 j )). (4)\nwhere E(x\u2217) is the representation of x\u2217. Here, we use the cosine similarity as the metric for measuring the similarity. By optimizing Lsim, we can guarantee that E(X\u2217) can be mapped to the same label with any fully-connected layer since E(X\u2217) all have similar representations. Meanwhile, to enhance the robustness of our watermark against fine-tuning, we simultaneously maximize the dissimilarity between E(X) and E(X\u2217) by:\nLdis = n\u2211\ni=1\nlog n\u2211 j=1 esim(E(xi),E(x \u2217 j )). (5)\nIn this way, when E(X) are updated during the fine-tuning, E(X\u2217) will be less influenced, thus mitigating the effect of catastrophic forgetting. Finally, we can perform both pre-training and watermark injection in the pre-training stage by optimizing the following training objective:\nL = LPLM + \u03bb1Lsim + \u03bb2Ldis. (6)\nwhere simply setting \u03bb1 = \u03bb2 = 1 consistently yields satisfactory results in our experiments.\nFigures 2 (d)and (e) showcases the T-SNE visualization of the embedding space of the watermarked BERT-base, optimized by using Equation (6), before and after the fine-tuning. Notably, the representations of the watermarked samples continue to exist as outliers after the fine-tuning process."
        },
        {
            "heading": "3.4 Ownership Verification",
            "text": "To establish the ownership of the suspected model \u03b8t, we start by obtaining the labels corresponding to X and X\u2217, which are denoted as Y and Y \u2217, respectively. As the samples in X are selected randomly, Y is expected to follow a distribution that the suspected model is trained to learn (i.e., a distribution reflects the size of samples in different classes). On the other hand, Y \u2217 is expected to mostly have a particular label, leading to a distribution that is close to a single point distribution.\nSubsequently, we can employ the homogeneity Chi-square test to compare the differences in the distributions of Y and Y \u2217. This enables us to obtain a confidence level that the two groups of samples do not follow the same distribution, which can be used as a probability mass assignment indicating that the suspected model contains a watermark.\nFor models that are not watermarked, since the selected trigger words are rare and do not have any semantics, they are unlikely to affect the predictions of the samples. Therefore, the distributions of Y and Y \u2217 are almost the same, which fails to provide evidence to verify the existence of a watermark and ensure the model\u2019s integrity.\nThe entire process of our method is illustrated in Figure 1."
        },
        {
            "heading": "3.5 Robustly Watermarking with Weight Perturbation",
            "text": "It has been known that watermarks embedded in model could be removed by malicious attackers (Lv et al., 2022; Xiang et al., 2021; Yadollahi et al., 2021). Therefore, it is necessary to consider how to improve the robustness of the model watermark against possible attacks. Prior research has focused primarily on fine-tuning and model pruning as the most commonly-used methods for watermark removal (Lv et al., 2022; Xiang et al., 2021; Yadollahi et al., 2021). In this paper, we treat fine-tuning, model pruning, and other unknown watermark removing methods as some forms of perturbations to model\u2019s parameters against watermarking. The\nfine-tuning can be formulated as follows:\n\u03b8s = argmin \u2206\u03b8\nE(x,y)\u2208D L(f(x,\u03b80 +\u2206\u03b8), y) (7)\nIn the case of model pruning, the typical approach is to zero out as many parameters as possible while preserving downstream dataset performance. This process can be formulated as:\n\u03b8p = \u03b8s +\u2206\u03b8 = \u03b8s \u2212m \u00b7 \u03b8s (8)\nwhere m = (0, 1)d. Our main goal is to enhance the robustness of model watermark-related parameters against such perturbations, which means the loss function of watermarking L has an upper-bound \u03c4 when the norm of perturbations \u2206\u03b8 is bounded by \u03b3:\nmax ||\u2206\u03b8||2<\u03b3\nE(x,y)\u2208D L(f(x\u2217,\u03b80 +\u2206\u03b8), y\u2217) < \u03c4 (9)\nConsequently, an optimization technique proposed by (Wu et al., 2020) can be employed to achieve this. The basic idea is that, we should find a perturbation term v in every training step and update \u03b8 by following:\n\u03b8 = (\u03b8 + v)\u2212 \u03b73\u2207\u03b8+v E(x,y)\u2208B L(f(x,\u03b8 + v), y) (10)\nBy optimizing this, the parameters can converge to a local optimum that is robust to the perturbation term v.\nIt can be seen that the direction of v determines the final robustness of \u03b8. To achieve the strongest robustness for the model, the parameter perturbation term v can be computed by moving in the opposite direction of the gradient: v = \u220f \u03b3 (v + \u03b72 \u2207\u03b8+v E(x,y)\u2208B L(f(x,\u03b8 + v), y) ||\u2207\u03b8+v E(x,y)\u2208B L(f(x,\u03b8 + v), y)|| ||\u03b8||)\n(11)\nwhere \u03b3 is the norm bound of v and layer-wise updates are applied to v.\nThe computation of v can be done using onestep or multi-step methods, similar to generating adversarial samples via FGSM (Goodfellow et al., 2015) and PGD (Madry et al., 2019). Our experiments demonstrate that a single-step computation of v achieves satisfactory robustness."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setting",
            "text": "We chose to use some representative models including BERT-Base (Devlin et al., 2018), BERT-Large,\nModel Setting IMDB SST2 AGNEWSACCU OVSR ACCU OVSR ACCU OVSR\nBERT-base\noriginal 93.79 0.00\u00b10.00 92.12 0.00\u00b10.00 94.50 32.29\u00b123.13 NBA(Zhang et al., 2021) 93.77 0.00\u00b10.00 92.32 0.00\u00b10.00 94.50 20.36\u00b113.29 ours w/o weight perturbation 93.42 99.89\u00b10.01 92.45 100.00\u00b10.00 94.18 100.00\u00b10.00 ours with weight perturbation 93.32 99.87\u00b10.13 92.13 99.97\u00b10.02 94.08 100.00\u00b10.00\nBERT-large\noriginal 94.49 0.00\u00b10.00 93.90 0.00\u00b10.00 94.50 40.13\u00b125.89 NBA 94.37 0.00\u00b10.00 93.22 0.00\u00b10.00 94.33 35.29\u00b114.13 ours w/o weight perturbation 94.52 99.92\u00b10.05 93.39 99.92\u00b10.03 94.42 100.00\u00b10.00 ours with weight perturbation 94.35 100.00\u00b10.00 93.69 99.99\u00b10.00 94.32 100.00\u00b10.00\nRoBERTa-base original 95.79 0.00\u00b10.00 94.54 0.02\u00b10.01 94.66 42.13\u00b122.10 NBA 95.39 0.00\u00b10.00 94.42 0.00\u00b10.00 94.50 33.29\u00b112.13 ours w/o weight perturbation 95.66 100.00\u00b10.00 94.32 100.00\u00b10.00 94.50 99.99\u00b10.00 ours with weight perturbation 95.79 100.00\u00b10.00 94.54 100.00\u00b10.00 94.32 100.00\u00b10.00\nRoBERTa-large original 95.88 0.00\u00b10.00 94.83 0.00\u00b10.00 94.78 45.25\u00b123.22 NBA 95.89 0.00\u00b10.00 94.82 0.00\u00b10.00 94.65 54.20\u00b124.75 ours w/o weight perturbation 95.79 100.00\u00b10.00 94.54 100.00\u00b10.00 94.32 99.97\u00b10.02 ours with weight perturbation 95.77 100.00\u00b10.00 94.47 100.00\u00b10.00 94.66 100.00\u00b10.00\nALBERT\noriginal 93.80 0.00\u00b10.00 92.54 0.00\u00b10.00 94.55 53.55\u00b14.30 NBA 93.77 0.00\u00b10.00 92.03 0.00\u00b10.00 94.31 69.25\u00b17.93 ours w/o weight perturbation 93.79 96.35\u00b13.53 92.43 93.46\u00b13.21 94.50 100.00\u00b10.00 ours with weight perturbation 93.77 97.17\u00b11.13 92.54 100.00\u00b10.00 94.33 100.00\u00b10.00\nTable 1: The experimental results of different PLMs after fine tuning on different downstream datasets. Each PLM has four different settings on each data set, where \"original\" indicates no watermark is embedded.\nRoBERTa-Base (Lan et al., 2019), RoBERTaLarge, and ALBERT (Liu et al., 2019) for watermark injection and ownership verification.Multiple downstream datasets of IMDB (Maas et al., 2011), SST2 (Rouhani et al., 2018), and AG NEWS (Zhang et al., 2015) were also selected for evaluation.\nWe first perform watermarking on all PLMs using BOOKCORPUS (BC) (Kobayashi, 2018), followed by a separate fine-tuning process on each downstream dataset, and finally verified the ownership of the PLMs. For all the experiments with weight perturbation, \u03b73 was set to 1\u00d7 10\u22124 based on our preliminary investigations, as it produced the best results. All experiments are conducted on 4 NVIDIA GeForce RTX 3090 GPU."
        },
        {
            "heading": "4.2 Baseline and Evaluation Metrics",
            "text": "We use the method proposed by (Zhang et al., 2021), Neural-level Backdoor Attack, as our baseline.\nThere are several aspects to evaluate the model watermarking approach accoring to prior works (Lv et al., 2022): (i) Effectiveness: The PLM watermark should be effectively detected by the model owners after fine-tuning. (ii) Fidelity: The existence of a watermark should not have an impact on the performance of PLM. (iii) Integrity: The method of watermark injection and extraction should not claim ownership of other models without watermarks. (iv) Robustness: The watermark\nshould still be detected after fine-tuning and other watermark-removing methods. (v) Stealthiness: The existence of a watermark should be hard to detect. (vi) Efficiency: The cost of watermark injection should be minimized.\nFor all above evaluations, we use the following two as our main metrics:\n\u2022 ACCU: The ACCUracy of each model on the downstream dataset.\n\u2022 OVSR: The success rate of ownership verification was indicated by the homogeneity Chi-square test\u2019s confidence level, denoted as Ownership Verification Success Rate. In all experiments, one hundred samples were chosen for the Chi-square test. Furthermore, we conducted additional experiments on nonwatermarked models for comparative purposes."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Integrity: The OVSR of the PLMs is presented in Table 1. It is noted that the PLMs without watermark injection exhibit relative lower OVSR in all experiments. This is attributed to the selection of watermark trigger words, which are rare and semantically insignificant (e.g., cf, mn, bb). Consequently, the presence or absence of these trigger words does not affect the model\u2019s prediction of sentences, resulting in minimal variation in the prediction distribution between the batches of sentences with and\nwithout the watermark trigger words. Therefore, the existence of a watermark cannot be verified. Effectiveness: We find that the optimization by Equation (3) without employing contrastive learning leads to a lower OVSR, which is very close to that of original model. This phenomenon is thoroughly discussed in Subsection 3.2. Conversely, the injection of watermarks with our method in the PLMs leads to the verification of ownership with nearly 100 % confidence, irrespective of performing weight perturbation during training, thereby validating the effectiveness of our method. Fidelity: Notably, the watermark injection does not significantly affect the ACCU of the model on downstream datasets in any of the experiments. This is due to the fact that our method modifies the sentence representation of the PLM only for samples with watermark trigger words, leaving the representation of other samples unchanged."
        },
        {
            "heading": "4.4 Robustness",
            "text": "Some adversaries may try to remove watermarks through certain watermark removal methods. Following prior works (Lv et al., 2022; Xiang et al., 2021; Yadollahi et al., 2021), we mainly consider fine-tuning and model pruning as such removal methods that could be used by adversaries. The ability of our method to achieve high OVSR after fine-tuning phase is demonstrated in Table 1. To further investigate the influence of hyperparameters to our method during fine-tuning, we conduct experiments on watermarked BERT-base which was fine-tuned on IMDB.\nThe left chart of Figure 3 demonstrates a concurrent decline in ACCU and OVSR with an increase in the learning rate. Despite a more substantial decrease in ACCU, OVSR remains relatively unaffected when the learning rate is lower than 7E-5. These results suggest that our proposed watermark-\ning method exhibits robustness even as the learning rate increases during the fine-tuning stage. Besides, when the learning rate reaches 1E-4, OVSR decreases to 0 due to the inability of the fine-tuning process to converge at such a high learning rate.\nThe right chart of Figure 3 illustrates that the OVSR maintains a stable high level (close to 100%) regardless of the number of training epochs. This can be attributed to the stabilization of the model\u2019s weights after a certain number of epochs, which results in the watermark-related parameters being unchanged. Overall, our experiments show that the watermark injected by our method is robust against fine-tuning, which is considered the most effective adversary in prior work (Bansal et al., 2022).\nIn Figure 4, the OVSR and ACCU curves for BERT-base and BERT-large models are presented after pruning the models following fine-tuning on IMDB and SST2 datasets. We found that weight perturbation does not have significant impact on ACCU, here we only show the ACCU curves without performing weight perturbation during watermark injection phase. The pruning was carried out by setting the layer parameter with the lowest relative weight value to 0, based on the predetermined pruning rate. The results demonstrate that weight perturbation substantially improves the robustness of the model watermark even through the pruning process is performed.\nThe results indicate that our approach to incorporating weight perturbation during watermark injection stage achieves satisfactory robustness against both fine-tuning and model pruning."
        },
        {
            "heading": "4.5 Stealthiness",
            "text": "Although the experiments so far have shown excellent performance of the watermark injected by our method, it has an obvious drawback that the use of Rare Words as watermark trigger words is not sufficiently stealthy. Other malicious users may filter the rare words in vocabulary to evade the ownership verification and thus render our approach ineffective. To overcome this shortcoming, inspired by previous work on stealthy backdoor attacks (Li et al., 2021; Shen et al., 2022), we can select a Combination of Common words as backdoor triggers, i.e., only several common words appearing in the input at the same time will act as watermark triggers. Due to the complexity of the number of combinations, it is difficult for other malicious users to reverse engineer the watermark to remove it (Li et al., 2021; Shen et al., 2022). Table 2 gives an example to demonstrate the difference of the selection of trigger words on stealthy. It can be seen that when using a combination of common words as the trigger, the stealthy is higher and can not be recognized by human easily.\nTable 3 shows the ACCU and OVSR of different pre-trained lanague models after fine tuned on three datasets when using a combination of common words as the backdoor trigger words. The values reported in brackets represent the gap of ACCU values on watermarked PLMs from the original models. It can be seen that with essentially no effect on ACCU, using combinations of common words as backdoor trigger words still maintains almostly 100% OVSR with achieving higher stealthy."
        },
        {
            "heading": "4.6 Efficiency",
            "text": "Efficiency requires that the training cost of watermark injection is as low as possible (Lv et al., 2022). Figure 5 shows the variation of the contrastive loss function of watermark injection with the training\nsteps of five PLMs. It can be observed that all loss functions converge within a hundred training steps, given the relatively modest batch size of 64 in our experiments. This suggests that only a few thousand samples are required for successful watermark embedding, indicating that our method incurs low training costs for watermark injection."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose a novel approach for watermark injection and ownership verification of PLMs. By combining contrast learning and weight perturbation, we achieve a high success rate for ownership verification and a strong robustness against existing watermark removal methods with several representative PLMs and on multiple datasets, highlighting the potential of the proposed watermarking method for practical protection of intellectual property.\nLimitations\nAlthough the experiments in this paper achieve high performance on typical PLMs and multiple datasets, the experiments in this paper are limited to the BERT family of models and text classification tasks, and it is interesting to investigate how to claim the ownership on some generative models, such as T5 (Raffel et al., 2020) and GPT-3 (Brown et al., 2020b). We plan to experiment with those models in the future."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank the anonymous reviewers for their valuable comments. This work was supported by National Natural Science Foundation of China (No. 62076068), and Shanghai Municipal Science and Technology Project (No. 21511102800).\nEthics Statement\nThis work fully comply with the ACL Ethics Policy. All the authors declare that there is no ethical issues in this paper submitted to ACL 2023 for review."
        }
    ],
    "title": "Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation",
    "year": 2023
}