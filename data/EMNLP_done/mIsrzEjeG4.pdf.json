{
    "abstractText": "Answering complex questions often requires multi-step reasoning in order to obtain the final answer. Most research into decompositions of complex questions involves open-domain systems, which have shown success in using these decompositions for improved retrieval. In the machine reading setting, however, work to understand when decompositions are helpful is understudied. We conduct experiments on decompositions in machine reading to unify recent work in this space, using a range of models and datasets. We find that decompositions can be helpful in zero or limited-data settings, giving several points of improvement in exact match. However, we also show that when models are given access to around a few hundred or more examples, decompositions are not helpful (and can actually be detrimental). Thus, our analysis implies that models can learn decompositions implicitly even with limited data. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Kangda Wei"
        },
        {
            "affiliations": [],
            "name": "Dawn Lawrie"
        },
        {
            "affiliations": [],
            "name": "Benjamin Van Durme"
        },
        {
            "affiliations": [],
            "name": "Yunmo Chen"
        },
        {
            "affiliations": [],
            "name": "Orion Weller"
        }
    ],
    "id": "SP:3003731b935eaca79a5d4b68c2f3987d1a165f31",
    "references": [
        {
            "authors": [
                "Dheeru Dua",
                "Shivanshu Gupta",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "Successive prompting for decomposing complex questions",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1251\u20131265, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Dheeru Dua",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Gabriel Stanovsky",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "venue": "Proceedings of the 2019 Conference of the North American",
            "year": 2019
        },
        {
            "authors": [
                "Dheeru Dua",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Gabriel Stanovsky",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "venue": "Proceed-",
            "year": 2019
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Xiao-Yu Guo",
                "Yuan-Fang Li",
                "Gholamreza Haffari."
            ],
            "title": "Complex reading comprehension through question decomposition",
            "venue": "Proceedings of the The 20th Annual Workshop of the Australasian Language Technology Association, pages 31\u201340, Adelaide, Aus-",
            "year": 2022
        },
        {
            "authors": [
                "Drew A. Hudson",
                "Christopher D. Manning."
            ],
            "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Yeganeh Kordi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Unifiedqa-v2: Stronger generalization via broader cross-format training",
            "venue": "ArXiv preprint, abs/2202.12359.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "UNIFIEDQA: Crossing format boundaries with a single QA system",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Zhuowan Li",
                "Elias Stengel-Eskin",
                "Yixiao Zhang",
                "Cihang Xie",
                "Quan Tran",
                "Benjamin Van Durme",
                "Alan L. Yuille."
            ],
            "title": "Calibrating concepts and operations: Towards symbolic reasoning on real images",
            "venue": "2021 IEEE/CVF International Conference on Computer",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ArXiv preprint, abs/2107.13586.",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Victor Zhong",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-hop reading comprehension through question decomposition and rescoring",
            "venue": "arXiv preprint arXiv:1906.02916.",
            "year": 2019
        },
        {
            "authors": [
                "Pruthvi Patel",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral"
            ],
            "title": "Is a question decomposition unit all we need",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Patrick Lewis",
                "Wen-tau Yih",
                "Kyunghyun Cho",
                "Douwe Kiela."
            ],
            "title": "Unsupervised question decomposition for question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A Smith",
                "Mike Lewis."
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "arXiv preprint arXiv:2210.03350.",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of",
            "year": 2016
        },
        {
            "authors": [
                "Yusheng Su",
                "Xiaozhi Wang",
                "Yujia Qin",
                "Chi-Min Chan",
                "Yankai Lin",
                "Huadong Wang",
                "Kaiyue Wen",
                "Zhiyuan Liu",
                "Peng Li",
                "Juanzi Li",
                "Lei Hou",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "On transferability of prompt tuning for natural language processing",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Berant."
            ],
            "title": "The web as a knowledge-base for answering complex questions",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
            "year": 2018
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Berant."
            ],
            "title": "The web as a knowledge-base for answering complex questions",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
            "year": 2018
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "ArXiv preprint, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel Weir",
                "Benjamin Van Durme."
            ],
            "title": "Dynamic generation of interpretable inference rules in a neuro-symbolic expert system",
            "venue": "arXiv preprint arXiv:2209.07662.",
            "year": 2022
        },
        {
            "authors": [
                "Eric W Weisstein."
            ],
            "title": "Bonferroni correction",
            "venue": "https://mathworld. wolfram. com/.",
            "year": 2004
        },
        {
            "authors": [
                "Johannes Welbl",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Constructing datasets for multi-hop reading comprehension across documents",
            "venue": "Transactions of the Association for Computational Linguistics, 6:287\u2013 302.",
            "year": 2018
        },
        {
            "authors": [
                "Orion Weller",
                "Aleem Khan",
                "Nathaniel Weir",
                "Dawn Lawrie",
                "Benjamin Van Durme."
            ],
            "title": "Defending against poisoning attacks in open-domain question answering",
            "venue": "arXiv preprint arXiv:2212.10002.",
            "year": 2022
        },
        {
            "authors": [
                "Orion Weller",
                "Nicholas Lourie",
                "Matt Gardner",
                "Matthew E Peters."
            ],
            "title": "Learning from task descriptions",
            "venue": "arXiv preprint arXiv:2011.08115.",
            "year": 2020
        },
        {
            "authors": [
                "Orion Weller",
                "Marc Marone",
                "Nathaniel Weir",
                "Dawn Lawrie",
                "Daniel Khashabi",
                "Benjamin Van Durme"
            ],
            "title": " according to...\" prompting language models improves quoting from pre-training data",
            "venue": "arXiv preprint arXiv:2305.13252",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Tomer Wolfson",
                "Mor Geva",
                "Ankit Gupta",
                "Matt Gardner",
                "Yoav Goldberg",
                "Daniel Deutch",
                "Jonathan Berant."
            ],
            "title": "Break it down: A question understanding benchmark",
            "venue": "Transactions of the Association for Computational Linguistics, 8:183\u2013198.",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Wolfson"
            ],
            "title": "2020), e.g. a learning rate of 1e-5",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Much past work has examined and improved models\u2019 ability to answer complex questions that require multiple steps of reasoning (Welbl et al., 2018; Talmor and Berant, 2018a; Dua et al., 2019a; Wolfson et al., 2020; Weller et al., 2020; Weir and Van Durme, 2022). A consistent theme in these works is to break the main complex question down into a series of sub-questions to be solved, which is referred to as question decomposition. These methods generally represent decompositions as a human would, with explicit natural language sub-questions that build together to the final answer.\nDespite the large amount of research in decompositions for multi-step question answering, the majority of it has focused on using question decomposition for both information retrieval and question\n1We publicly release all code at https://github.com/ WeiKangda/Question-Decomposition\n* Joint advising\nanswering (e.g. in the open domain). In that setting, results have consistently shown that question answering (QA) models perform better on multi-step questions when they use decomposed questions to help with retrieval (Wolfson et al., 2020; Perez et al., 2020; Geva et al., 2021a). The few works that have used decompositions for machine reading do so in limited settings, leaving its effectiveness unclear (Guo et al., 2022; Patel et al., 2022).\nTherefore, we seek to shed light on if and when decompositions are helpful for machine reading. To do so, we analyze decomposition methods for several QA models across two multi-step QA datasets. Our results show that decompositions are only helpful in the low data setting, where there are less than a few hundred examples. Using decompositions in anything other than that setting performs the same (or much worse, depending on the strategy) as simply training the model end-to-end.\nThus, overall, decompositions are helpful for question answering when they are used in two main\nsettings: (1) information retrieval, where the decompositions help isolate distinct aspects of the question, or (2) in zero to low resource settings, where there isn\u2019t enough data to implicitly learn the multi-step process through end-to-end training."
        },
        {
            "heading": "2 Experiment Setup",
            "text": ""
        },
        {
            "heading": "2.1 Data",
            "text": "We use the BREAK (Wolfson et al., 2020) resource which contains annotated decompositions for 10 different benchmarks, including three reading comprehension benchmarks. BREAK provides humanwritten decompositions for all dataset splits (e.g. train, test) which we use, thus assuming the oracle decomposition case. We use two of these three datasets (HotpotQA and DROP) as the third, ComplexWebQuestions (Talmor and Berant, 2018b), does not currently provide a training set.2 Following BREAK and other follow-up work (Geva et al., 2021b), we train and test our models using the highlevel decompositions (also known as QDMRs).\nThus, we use HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019b) in our experiments, using the portions annotated from their train and validation sets. HotpotQA was created from workers on Mechanical Turk who annotated compositional questions from Wikipedia by using the page links (an example can be found in Figure 1). DROP was created to test discrete multi-hop reasoning and was also annotated by Mechanical Turk workers who used Wikipedia articles as the context.\n2Note that although the ComplexWebQuestions (CWQ) paper initially released the training set the authors have since removed it from the official Dropbox. Furthermore, even if the dataset was available CWQ does not verify that the questions are answerable from the returned Google search passages.\nThe BREAK annotations include the list of decomposed questions that eventually yield the same answer as the original question, along with an operator assigned to each decomposition that represents the type of reasoning for that step (e.g. Boolean, Comparison, etc.). Note that there are no gold labels for intermediate decomposition steps; the only ground truth label is for the main question."
        },
        {
            "heading": "2.2 Models",
            "text": "To explore the effect of decompositions on various types of common NLP models, we employ five different models: BART (Lewis et al., 2020), vanilla T5 (Raffel et al., 2020), UnifiedQA-v2 (Khashabi et al., 2020, 2022) which uses a T5 backbone and has been fine-tuned on other QA datasets but not on HotpotQA (Raffel et al., 2020), LLaMA-7B (Touvron et al., 2023) and Alpaca-7B (Taori et al., 2023). This allows us to demonstrate the effect of additional fine-tuning (UnifiedQA vs vanilla T5), different architectures (BART vs T5 vs LLaMA), and instruction-tuning (LLaMA vs Alpaca). Note that because UnifiedQA-v2 was multi-task trained on DROP, its scores on it are noticeably higher than the other models (Figure 2). However, our purpose is not to compare scores between models, but rather to compare scores between different decomposition strategies. Thus, the inclusion of this model on DROP shows us that our results hold even if the model was pretrained on it. For more hyperparameter and compute details, see Appendix A."
        },
        {
            "heading": "2.3 Decomposition Strategies",
            "text": "There are many possible ways to combine decomposition with model fine-tuning. We try a wide variety of techniques (including novel ones) that\nwe group into three categories: (1) no decomposition e.g. the baseline QA format, (2) explicit decomposition, and (3) implicit decomposition.\nExplicit Decomposition Explicit decompositions are the most common approach in the decomposition literature, generating the answer iteratively through sub-questions: the model answers the first decomposition step, then replaces placeholders in future decomposition steps with that predicted answer, then predicts the second decomposition step, and so forth. Note that using this method (Explicit) naively presents issues with backpropagation, as the model can only backpropagate through the last decomposition step. Variations of strategies in this category include giving the model all previous decomposition steps as context (Explicit w/Decomp) or including all decomposition steps and all predicted answers as context (Explicit Everything).\nImplicit Decomposition Another way to use decompositions could be to add them implicitly. To do so, we utilize the operators provided in the BREAK annotations which describe the type of reasoning needed, removing duplicate operators and keeping them in their given order. For example, in Figure 1 the model uses select twice and then intersection in the explicit decomposition reasoning steps (but we remove the duplicate select). We implement this in practice by adding a new special token for each operator and prepending them to the original question. Although this approach is novel in the context of decompositions, it bears similarity to work in the prompting literature, such as soft prompts (Qin and Eisner, 2021; Liu et al., 2022). Variations to this approach in the same category include randomizing the order of the special tokens (Implicit Unordered), leaving in duplicate special tokens (Implicit w/Dupl.), or even prepending these operators to the explicit sub-questions in the Explicit Decomposition approach to combine the two strategies (Explicit + Implicit)."
        },
        {
            "heading": "3 Results",
            "text": "Full-Data Experiments We see the main results in Figure 2 with results for DROP on the left and HotpotQA on the right. Bars are colored-coded according to their method. All bars are the mean of three random seeds and error bars indicate the standard deviation. We see that most methods perform nearly the same, except for two that underperform: Explicit and Explicit + Implicit. Note that both of\nthese have issues with training end-to-end, as an Explicit decomposition is not differentiable through all decomposition steps. Thus, we only end up differentiating through the last step of the explicit decomposition steps, leaving the model unable to learn as effectively. All other approaches to decomposition perform comparably, given random seed variance (e.g. t5-base DROP Implicit Decomp. is 33.7% exact match \u00b1 0.9% vs No Decomp 34.0% \u00b1 0.4%). In fact, in this full data setting, the No Decomp method performs better or statistically similar to every other method according to two-sample t-tests with the Bonferroni correction (Weisstein, 2004), across all datasets and models.\nWe show the two more recent and larger models (LLaMA and Alpaca) in Table 1 on Hotpot, reporting the mean and standard deviation of the EM score. We find that No Decomp method still performs similarly or outperforms all other decomposition methods (e.g. for Alpaca 59.9 EM with No Decomp vs 54.8 EM for Explicit Decomp). Thus, the conclusion that decompositions are helpful only when there is not enough labeled examples still holds for newer and larger models and even for instruction-tuned models like Alpaca.\nSize Experiment Is the No Decomp method always the same or better than the decomposition methods, or can decompositions help in the lowdata regime? We answer this question in Figure 3, by varying the amount of training data, comparing it to the zero-shot Explicit method that is typically used (Patel et al., 2022; Dua et al., 2022).3 As we need the zero-shot model to be fine-tuned on\n3Note that fine-tuned Explicit is worse than the zero-shot version in low-data settings (See Figure 3b) We do not evaluate on DROP as UnifiedQA-v2 was already trained on DROP.\nQA to handle the decomposed questions,4 we use a modified version of T5-base (valhalla/t5-basesquad from Huggingface) that was fine-tuned on SQuAD (Rajpurkar et al., 2016). This places the No Decomp method at a disadvantage, as it was not trained on SQuAD and started from T5-base. Despite that, however, we see that as the amount of data increases, the fine-tuned models perform better; eventually surpassing the zero-shot method between 100-250 examples for UnifiedQA-v2 and 250-1000 examples for T5.5\nError Analysis Why does the zero-shot Explicit method perform worse than the fine-tuned No Decomp? In Table 2 we show representative errors from the Explicit method with how often they occurred. We randomly sample 50 errors and categorize them into three groups: wrong predictions in the last step, error propagation from intermediate steps, and invalid/missing annotations from BREAK (i.e. not the model\u2019s fault). We found that the biggest category was predicting an invalid annotation (42%), i.e. an alias that the dataset did not contain, followed by error propagation (40%) and then wrong last predictions (18%). Thus, compared to the non-iterative methods, the iterative process allows error propagation that occurs in roughly 40% of errors, contributing to its lower comparative scores (see Appendices B and C for other error analyses on No Decomp and cases where No Decomp was better than Explicit)."
        },
        {
            "heading": "4 Related Work",
            "text": "Decompositions in QA Decompositions for QA have a long history in complex question answering (Perez et al., 2020; Min et al., 2019; Geva et al., 2021a) with recent interest in using them for large language models (Wei et al., 2022; Dua et al., 2022; Zhou et al., 2023; Press et al., 2023). Two of the most related works to ours include Patel et al. (2022), who focus on decompositions in the zero-shot setting only and show improvements (which aligns with our results), and other work (Guo et al., 2022) that shows that decompositions help on the full DROP dataset but which doesn\u2019t include a comparable non-decomposition baseline on the same data. Our analysis complements and\n4We use the same UnifiedQA-v2 model for both zero-shot and fine-tuned as it was already trained on QA.\n5We also show several other decomposition methods over various amounts of fine-tuning in Figure 3b, all starting from T5-base. We see that they perform worse than the No Decomp method at every point.\nunifies our understanding of decompositions by identifying when decompositions help with respect to dataset size.\nDecompositions in other fields Our results for decompositions in textual QA helps to unify results across machine learning, as similar conclusions (e.g. decompositions being less effective then endto-end methods with large data) can be seen from scores on Computer Vision visual QA leaderboards (Hudson and Manning, 2019; Li et al., 2021).\nDecomposition Strategies and Prompting Decompositions methods are also related to prompting, where the explicit decompositions can be seen as a hard prompt (Liu et al., 2021; Su et al., 2022)\nand the implicit decompositions are similar to soft prompts (Qin and Eisner, 2021; Liu et al., 2022).\nOther research has looked at developing new prompting methods which either better handle complex questions or automatically generate decompositions (as opposed to the human-written decompositions in BREAK) (Zhou et al., 2022; Weller et al., 2022, 2023; Press et al., 2022). Our work focuses only on human written decompositions, which were shown to be better than automatically generated decompositions in the BREAK paper."
        },
        {
            "heading": "5 Conclusion",
            "text": "Our work explored when decompositions are helpful for machine reading. We showed that decompositions are helpful when there is limited data available, or when parameters cannot be tuned. However, when enough data exists (empirically around a few hundred instances) and parameters can be fine-tuned, it is best to let the model learn the decompositions implicitly through end-to-end training. Furthermore, we show that limitations of not fine-tuned decomposition approaches include error propagation of intermediate steps while also introducing more possibilities for annotator error. We hope that our work will help to inform readers as they create new datasets and select methods to use for complex question answering."
        },
        {
            "heading": "6 Limitations",
            "text": "Our work has explored the machine reading setting, using what is to our knowledge the only large com-\nplex question answering datasets that have humanannotated decompositions. However, it is possible that in the future someone could create another decomposition-based dataset that could show slightly different results. We believe this to be unlikely, as our empirical study holds across models and datasets. Another limitation is that we do not use alternate tasks, such as summarization, tagging, etc. as our work focuses on how decompositions work in question answering only, given the large interest in using decompositions for QA."
        },
        {
            "heading": "A Hyperparameter and Compute Details",
            "text": "We train our models following the hyperparameters in Wolfson et al. (2020), e.g. a learning rate of 1e-5 for 5 epochs, using early stopping on a holdout of the training set (5%) to determine the best saved model. The best models were typically ones trained for around 2 epochs. We use T5-base, BART-base, UnifiedQA-V2-Large, LLaMA 7B, and Alpaca 7B.\nFor the zero-shot decomposition UnifiedQA-v2 approach (perhaps due to its multi-task pre-training on other QA datasets), we found that using the original question plus the iterative decompositions\nadded a solid boost to final performance and hence we use those results as it provides the strongest zero-shot baseline. Each zero-shot decomposition run took approximately 15-30 minutes to evaluate.\nCompute time ranged from 1 hr for the shortest jobs with smaller data sizes and non-iterative training to around 18 hours for iterative decompositions methods with UnifiedQA-v2-large on 1 RTX 6000 GPU. For LLaMA and Alpaca, we use one 80GB A100 GPU with runs taking between 12-24 hours.\nData was prepared building off of the original BREAK authors\u2019 code. Models were accessed via the Huggingface repository (Wolf et al., 2019).\nNote that our reported no-decomposition results on DROP with T5 show comparable or greater performance to Guo et al. (2022) when using only the BREAK dataset."
        },
        {
            "heading": "B Error Analysis for Full-Data No",
            "text": "Decomp\nWe also do an error analysis for the full-data version of the No Decomp method to compare with the Explicit error analysis done in the main paper. As error propagation is not a possible category for this model, since it is end-to-end, there are only two error categories we use: (1) Wrong Prediction (54%) and (2) Invalid or Missing Annotation (46%). We see that a missing annotation/alias was the cause of the incorrect answer 46% of the time, which is comparable to the 42% of the time the zero-shot Explicit method had an error due to an alias. For the other 54%, the model would output \u201cyes\" when it should be \u201cno\" or extract the wrong string from the passage leading to an incorrect prediction, etc.\nC Error Analysis With No Decomp succeeded and zero-shot Explicit Decomp failed\nBased on Table 2, the Invalid or Missing Annotation takes up the largest percentage of error, which could happen to both decomposition and none-decomposition method. In order to further strengthen our point that question decompositions cause error propagation, thus underperforming the No Decomp method overall, we also conduct an error analysis where the fine-tuned No Decomp succeeded and the zero-shot Explicit Decomp failed, with 20 sampled errors using UnifiedQA-v2. The new results are: (1) Error Propagation (45%), (2) Wrong Prediction at Last Step (35%), and Invalid or Missing Annotation (20%). We see similar re-\nsults to the error analysis in Table 2 where Error Propagation causes the most errors."
        },
        {
            "heading": "D Size Experiment for Alpaca",
            "text": "We also performed the size experiment in Section 3 with Alpaca-7B (chavinlo/alpaca-native) and show the results in Figure 4. Note that Alpaca is roughly 10 points better than T5-base (approximately 60 EM vs 51 EM). The No Decomp method performs similarly or better than the Explicit method in all cases with varying training data size. Our conclusion still remains across models and decomposition strategies: No Decomp is better than or similar to the Decomp models, when given enough training examples."
        }
    ],
    "title": "When Do Decompositions Help for Machine Reading?",
    "year": 2023
}