{
    "abstractText": "Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs\u2019 reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL1 to automatically generate auto-CoT exemplars and thus the whole process doesn\u2019t need manual labeling. Our approach is cost-saving since we only use the LLMs\u2019 API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs\u2019 performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanchong Zhang"
        },
        {
            "affiliations": [],
            "name": "Ruisheng Cao"
        },
        {
            "affiliations": [],
            "name": "Lu Chen"
        },
        {
            "affiliations": [],
            "name": "Hongshen Xu"
        },
        {
            "affiliations": [],
            "name": "Kai Yu"
        }
    ],
    "id": "SP:31107efec735a2145c05d7ab626521766bc511cf",
    "references": [
        {
            "authors": [
                "Ben Bogin",
                "Jonathan Berant",
                "Matt Gardner."
            ],
            "title": "Representing schema structure with graph neural networks for text-to-SQL parsing",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4560\u20134565, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Ruisheng Cao",
                "Lu Chen",
                "Zhi Chen",
                "Yanbin Zhao",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "LGESQL: Line graph enhanced text-to-SQL model with mixed local and non-local relations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Ruisheng Cao",
                "Lu Chen",
                "Jieyu Li",
                "Hanchong Zhang",
                "Hongshen Xu",
                "Wangyou Zhang",
                "Kai Yu."
            ],
            "title": "A heterogeneous graph to abstract syntax tree framework for text-to-sql",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(11):13796\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Shuaichen Chang",
                "Eric Fosler-Lussier"
            ],
            "title": "How to prompt llms for text-to-sql: A study in zero-shot, single-domain, and cross-domain settings",
            "year": 2023
        },
        {
            "authors": [
                "Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Zhi Chen",
                "Lu Chen",
                "Yanbin Zhao",
                "Ruisheng Cao",
                "Zihan Xu",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "ShadowGNN: Graph projection neural network for text-to-SQL parser",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Deng",
                "Ahmed Hassan Awadallah",
                "Christopher Meek",
                "Oleksandr Polozov",
                "Huan Sun",
                "Matthew Richardson."
            ],
            "title": "Structure-grounded pretraining for text-to-SQL",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Xuemei Dong",
                "Chao Zhang",
                "Yuhang Ge",
                "Yuren Mao",
                "Yunjun Gao",
                "Lu Chen",
                "Jinshu Lin",
                "Dongfang Lou"
            ],
            "title": "C3: Zero-shot text-to-sql with chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Qiuping Huang",
                "Matthew Purver",
                "John R. Woodward",
                "Jinxia Xie",
                "Pengsheng Huang."
            ],
            "title": "Towards robustness of textto-SQL models against synonym substitution",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Matthew Purver."
            ],
            "title": "Exploring underexplored limitations of cross-domain text-to-SQL generalization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8926\u20138931, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Haoyang Li",
                "Jing Zhang",
                "Cuiping Li",
                "Hong Chen"
            ],
            "title": "2023a. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql",
            "year": 2023
        },
        {
            "authors": [
                "Jieyu Li",
                "Zhi Chen",
                "Lu Chen",
                "Zichen Zhu",
                "Hanqi Li",
                "Ruisheng Cao",
                "Kai Yu."
            ],
            "title": "Dir: A large-scale dialogue rewrite dataset for cross-domain conversational text-to-sql",
            "venue": "Applied Sciences, 13(4).",
            "year": 2023
        },
        {
            "authors": [
                "Jinyang Li",
                "Binyuan Hui",
                "Reynold Cheng",
                "Bowen Qin",
                "Chenhao Ma",
                "Nan Huo",
                "Fei Huang",
                "Wenyu Du",
                "Luo Si",
                "Yongbin Li"
            ],
            "title": "2023c. Graphix-t5: Mixing pretrained transformers with graph-aware layers for textto-sql parsing",
            "year": 2023
        },
        {
            "authors": [
                "Linyong Nan",
                "Yilun Zhao",
                "Weijin Zou",
                "Narutatsu Ri",
                "Jaesung Tae",
                "Ellen Zhang",
                "Arman Cohan",
                "Dragomir Radev"
            ],
            "title": "Enhancing few-shot text-tosql capabilities of large language models: A study on prompt design strategies",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Mohammadreza Pourreza",
                "Davood Rafiei"
            ],
            "title": "Din-sql: Decomposed in-context learning of textto-sql with self-correction",
            "year": 2023
        },
        {
            "authors": [
                "Jiexing Qi",
                "Jingyao Tang",
                "Ziwei He",
                "Xiangpeng Wan",
                "Yu Cheng",
                "Chenghu Zhou",
                "Xinbing Wang",
                "Quanshi Zhang",
                "Zhouhan Lin."
            ],
            "title": "RASAT: Integrating relational structures into pretrained Seq2Seq model for text-to-SQL",
            "venue": "Proceedings of the 2022 Con-",
            "year": 2022
        },
        {
            "authors": [
                "Nitarshan Rajkumar",
                "Raymond Li",
                "Dzmitry Bahdanau"
            ],
            "title": "Evaluating the text-to-sql capabilities of large language models",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson."
            ],
            "title": "RAT-SQL: Relation-aware schema encoding and linking for textto-SQL parsers",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Xiaojun Xu",
                "Chang Liu",
                "Dawn Song."
            ],
            "title": "Sqlnet: Generating structured queries from natural language without reinforcement learning",
            "venue": "arXiv preprint arXiv:1711.04436.",
            "year": 2017
        },
        {
            "authors": [
                "cent Zhang",
                "Caiming Xiong",
                "Richard Socher",
                "Walter Lasecki",
                "Dragomir Radev"
            ],
            "title": "2019a. CoSQL: A conversational text-to-SQL challenge towards crossdomain natural language interfaces to databases",
            "venue": "In Proceedings of the 2019 Conference on Empirical",
            "year": 2019
        },
        {
            "authors": [
                "Hong Kong",
                "China"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 1962
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman",
                "Zilin Zhang",
                "Dragomir Radev"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic pars",
            "year": 2018
        },
        {
            "authors": [
                "Dragomir Radev."
            ],
            "title": "SParC: Cross-domain semantic parsing in context",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511\u20134523, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Lu Zeng",
                "Sree Hari Krishnan Parthasarathi",
                "Dilek Hakkani-Tur"
            ],
            "title": "N-best hypotheses reranking for text-to-sql systems",
            "year": 2022
        },
        {
            "authors": [
                "Victor Zhong",
                "Mike Lewis",
                "Sida I. Wang",
                "Luke Zettlemoyer."
            ],
            "title": "Grounded adaptation for zeroshot executable semantic parsing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6869\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "venue": "CoRR, abs/1709.00103.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "to automatically generate auto-CoT exemplars and thus the whole process doesn\u2019t need manual labeling. Our approach is cost-saving since we only use the LLMs\u2019 API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs\u2019 performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches."
        },
        {
            "heading": "1 Introduction",
            "text": "The text-to-SQL task (Zhong et al., 2017; Xu et al., 2017) aims to translate the natural language question into the corresponding SQL query with the given database schema. It is the key technique to establish the natural language interface on relational databases, which can help common users access data from relational databases in a more convenient way.\nRecent studies in text-to-SQL research have primarily centered on the development of semantic parsers within the framework of cross-domain analysis. In cross-domain text-to-SQL datasets such as Spider (Yu et al., 2018), SParC (Yu et al., 2019b), and CoSQL (Yu et al., 2019a), the databases employed in the train set, dev set, and test set do not\n\u2217The corresponding author is Lu Chen. 1Our code is publicly available at https://github.com/\nX-LANCE/text2sql-GPT\noverlap. Prior research endeavors have focused on training specialized text-to-SQL models and optimizing their structural components to enhance overall performance. Notably, these efforts have yielded impressive model performances across various datasets. Nevertheless, the construction of such models necessitates a substantial number of highquality training examples and entails significant time investments for finetuning. Moreover, these models often possess intricate structures, rendering their deployment challenging.\nRecent research has provided empirical evidence establishing the substantial capabilities of Large Language Models (LLMs), such as GPT-3 (Brown et al., 2020) and ChatGPT (Ouyang et al., 2022), across a wide spectrum of domains and tasks. As the scale of LLMs continues to expand, scholarly investigations have revealed the presence of emergent abilities (Wei et al., 2022) exclusive to larger LLMs and absent in their smaller counterparts. Therefore, the latest studies employ LLMs in the context of the text-to-SQL task, utilizing the incontext learning method (Brown et al., 2020). Owing to the impressive performance demonstrated by LLMs in zero-shot or few-shot prompting scenarios, the need for extensive finetuning using an abundance of training examples has been rendered unnecessary. Consequently, the integration of LLMs in the text-to-SQL process yields notable time and cost savings.\nNonetheless, contemporary in-context learning approaches for text-to-SQL encounter certain challenges. For instance, Rajkumar et al. (2022), in comparison to SOTA finetuned models, employ a simplistic prompt designing approach that yields relatively subpar performance. Similarly, Pourreza and Rafiei (2023) employs a convoluted workflow to generate the final SQL query, resulting in achieving SOTA performance on the test set of the Spider dataset. However, this approach proves time-consuming and resource-intensive, as it ne-\ncessitates multiple API calls to LLMs during the query generation process. Moreover, the recent advancements in in-context learning methods for text-to-SQL have yet to be extended to multi-turn datasets, such as SParC, CoSQL, and DIR (Li et al., 2023b).\nDespite the proficiency of LLMs as zero-shot and few-shot learners, the mere superficial prompt design fails to fully activate their capabilities. To address this limitation, Wei et al. (2023) proposes a novel prompting technique called chain-of-thought (CoT). Through the CoT method, the prompt text encompasses a comprehensive thinking process that guides LLMs towards accurate deduction of answers. Notably, the CoT method mirrors the sequential nature of human reasoning, wherein intermediate answers are obtained before arriving at a final conclusion. Given the intricate nature of the text-to-SQL task, the CoT method proves highly suitable, as generating the SQL query entails complex reasoning processes. However, existing CoT methodologies necessitate extensive time investments in the selection of canonical examples and manual labeling. The text-to-SQL task lacks an automated approach for generating CoT sequences.\nIn this paper, we propose our in-context learning method for the text-to-SQL task with the automatically-generated CoT. First, under the zeroshot setting, we study the influence on LLMs\u2019 performance caused by the input format of the database schema. Second, under the few-shot setting, we provide a hybrid strategy to select exemplars and study the influence on LLMs\u2019 performance caused by the number of exemplars. Our experiment results show that the strategy is effective. Third, we present our approach named ACT-SQL to generate auto-CoT for the dataset training example consisting of the database schema, the question, and the corresponding SQL query. The experiment results show that the generated auto-CoT can indeed improve the LLMs\u2019 performance. The ACTSQL achieves the SOTA performance on the Spider dev set among existing in-context learning methods. In addition, the ACT-SQL does not need to use extra LLMs\u2019 API calls, which means that our workflow is relatively fast and cheap. Finally, we apply our approach in multi-turn text-to-SQL datasets including SParC and CoSQL and achieve comparable accuracy scores with finetuned models. Our main contributions can be summarized as follows:\n1. We explore the influence on LLMs\u2019 perfor-\nmance under the text-to-SQL task with different prompting styles and few-shot exemplar selection strategies.\n2. We propose our approach named ACT-SQL to generate auto-CoT. The ACT-SQL achieves the SOTA performance on the Spider dev set among existing in-context learning methods. Furthermore, our automatic method is costsaving and time-saving and does not need extra LLMs\u2019 API calls.\n3. We extend our method onto the multi-turn textto-SQL task and achieve comparable performances with finetuned models on the SParC and CoSQL datasets."
        },
        {
            "heading": "2 Related Work",
            "text": "Text-to-SQL models Over the past several years, text-to-SQL researches mainly focus on building well-designed deep neural networks (Chen et al., 2021b; Cao et al., 2023). RATSQL model (Wang et al., 2020) and LGESQL model (Cao et al., 2021) are AST-based approaches, where AST is the abbreviation of the abstract syntax tree. They encode the input and decode the AST of the SQL query with predefined grammar. AST-based approaches perform well but are generally complex to deploy.\nPICARD (Scholak et al., 2021) is a sequenceto-sequence model. SQL is a formal language that follows strict grammar rules. Directly finetuning pretrained language models (PLMs) on text-to-SQL datasets would make PLMs likely to generate invalid SQL queries. The PICARD model rejects invalid tokens at each decoding step and constrains the generated results into a certain output space.\nAlthough these specialized models have achieved excellent performances, there still exist some inevitable disadvantages. In order to train a text-to-SQL model, abundant high-quality training examples are needed. Constructing and labeling a large-scale text-to-SQL dataset is always not easy and would consume a lot of resources and time. Training and finetuning the model is also a hard project which costs many computing resources.\nIn-context learning for text-to-SQL Since LLMs have shown amazing ability across various domains and have been applied in many academic and industrial fields, the latest researches begin to activate the LLMs\u2019 ability for the text-to-SQL task. Rajkumar et al. (2022) uses the trivial zeroshot and few-shot learning setting and performs an\nempirical evaluation of text-to-SQL capabilities of LLMs including GPT-3 (Brown et al., 2020) and Codex (Chen et al., 2021a). They perform the zeroshot prompt learning on Spider (Yu et al., 2018), a large-scale human-labeled cross-domain text-toSQL dataset. Their work is relatively simple and the performance falls behind finetuned models.\nNan et al. (2023) mainly concentrates on the strategy of exemplars selection. Their work achieves good performance on several crossdomain datasets including Spider, Spider-Syn (Gan et al., 2021a), Spider-DK (Gan et al., 2021b) and Spider-Realistic (Deng et al., 2021). However, their work requires an extra preliminary predictor to evaluate the SQL\u2019s difficulty level and needs to use LLMs\u2019 API call many times due to the majority vote method.\nDIN-SQL (Pourreza and Rafiei, 2023) provides a relatively complex approach. DIN-SQL consists of a complex workflow that decomposes the problem into several simpler sub-problems. With the LLM GPT-4, DIN-SQL has surpassed previous finetuned models and has achieved the best score on the Spider dataset. But DIN-SQL\u2019s workflow is obviously slow and expensive since it uses LLMs\u2019 API call many times to generate one SQL."
        },
        {
            "heading": "3 Methodology",
            "text": "With the in-context learning method, the SQL generation process can be formulated as\nS = LLM(I,D,Q, E).\nI represents the instruction. D represents the database schema. Q represents the question. E = [(D1, Q1, P1), \u00b7 \u00b7 \u00b7 , (Dn, Qn, Pn)] is the list of exemplars where Pi is the answer prompt which contains the correct SQL for the i-th exemplar. Thus the performance of LLMs is mainly influenced by the database prompt style, the exemplar selection strategy, and the exemplar prompt design.\nIn this section, we first describe the prompt styles of the database schema. Then we state our strategy of exemplar selection for the few-shot learning setting. Furthermore, we introduce our ACT-SQL approach, i.e. the automatically generated CoT method for constructing effective answer prompts. Finally, we extend our approach to the multi-turn text-to-SQL task."
        },
        {
            "heading": "3.1 Database Prompt Style",
            "text": "Previous works have shown that given the database schema, strong LLMs (e.g. GPT models) can trans-\nlate the relatively simple natural language question into the correct SQL query, though no exemplar is provided. Under the zero-shot setting, the LLMs merely take the database schema and the question as the input. Thus the input format of the database schema would mainly influence the LLMs\u2019 performance. Generally, we use five different database schema styles, which are shown in Appendix C.1:\n1. Table(Column) lists each table followed by its columns in each line. This style follows the official document provided by OpenAI2.\n2. Table(Column)(PF) adds primary keys and foreign keys at the end of Table(Column).\n3. Create(NoPF) describes all tables and columns with the \u201ccreate table\u201d statement in the SQL grammar. \u201cNoPF\u201d represents that no information on primary keys and foreign keys is added. Compared with Table(Column), this input format contains information on column types (e.g. number and text) and is more similar to real SQL statements.\n4. Create(EoC) adds primary keys and foreign keys based on Create(NoPF). \u201cEoC\u201d represents that they are added at the end of the corresponding column.\n5. Create(EoT) adds primary keys and foreign keys based on Create(NoPF). \u201cEoT\u201d represents that they are added at the end of the table.\nFurthermore, database contents are concerned. Specifically c example rows are appended to each table. Appendix C.2 shows instances where c = 3."
        },
        {
            "heading": "3.2 Exemplar Selection",
            "text": "Given a few exemplars, LLMs can benefit and acquire tips from them and thus generate SQL queries with a more standard format and higher accuracy. Exemplar selection is an important work under the few-shot setting, which would influence the LLMs\u2019 performance a lot.\nWe select exemplars using a hybrid strategy. Specifically, we first of all select ns examples from the training dataset at random. These dataset examples are named static exemplars. They would be used in the context of every test case. As for each specific test case, we select nd extra examples from\n2https://platform.openai.com/examples/ default-sql-translate\nthe training dataset. These dataset examples are named dynamic exemplars since they are selected according to some features of the current test case. Consequently, there are total ns + nd exemplars for each test case.\nIn order to get dynamic exemplars that are more relevant to the current test case, we compare the natural language question of the current test case with all questions in the training dataset. We calculate the similarity scores with the suitable pretrained model and then select the top-nd training dataset examples. We believe that dynamic exemplars with more relevant questions would provide more effective information to the LLMs."
        },
        {
            "heading": "3.3 Chain-of-Thought Prompt Design",
            "text": "Under the few-shot learning setting, it has been proven that the LLMs\u2019 performance can benefit a lot from the chain-of-thought (CoT) (Wei et al., 2023) method. In the text-to-SQL task, only the database schema, the question, and the corresponding SQL query are provided in the prompt under the trivial few-shot learning setting. However, with the CoT method, the thought process of how to write the correct SQL query is added to the prompt. These prompting texts can help the LLMs think step by step when generating the complete SQL query and thus can activate the logical reasoning ability of the LLMs.\nIn previous works, some grammar-based text-toSQL models utilize the graph encoding technique to jointly encode both the database schema and the question. Schema linking (Bogin et al., 2019; Wang et al., 2020; Cao et al., 2021) is a commonly used algorithm for building the input graph. If the question tokens exactly or partially match some schema item (i.e. table and column) names, then they are linked with the specific graph edge. It is obvious that the schema linking method can help the text-to-SQL models fetch the most relevant tables and columns among a great number of schema items based on the question.\nWe design our chain-of-thought prompt with a similar method to schema linking. Figure 1 shows an instance of the manually labeled CoT for the example from the train set of the Spider dataset (Yu et al., 2018). As suggested in Kojima et al. (2023), the CoT prompt starts with \u201cLet\u2019s think step by step\u201d. For each slice of the question sentence that may contain some information about the schema item, we add them into the CoT prompting text in the format shown in Figure 1. Furthermore, the values mentioned in the question and the SQL query are also a concern. The final SQL query is appended at the end of the CoT prompt.\nAuto-CoT Although CoT prompts can be manually labeled, it costs a lot of time to find sufficient canonical and effective training dataset examples for CoT labeling. In addition, manually labeled CoT exemplars are fixed, which means that they are all static exemplars and dynamic exemplars are deficient. In order to deal with problems in the manual labeling process, we introduce an automatic method to generate auto-CoT prompts for every example in the training dataset.\nGiven the question q = (q1, q2, \u00b7 \u00b7 \u00b7 , q|q|) and the SQL query s, the qi represents the i-th token in the question sentence. We define qi,j = (qi, qi+1, \u00b7 \u00b7 \u00b7 , qj) as a slice of the original question. We first enumerate each column [tab].[col] appearing in the SQL query, where [tab] represents the table name and [col] represents the column name. For each column, we use the suitable pretrained model to compute the similarity scores between the current column and all the question sentence slices. The most relevant slice is\nargmax qi,j Sim([tab].[col], qi,j),\nwhere Sim is the similarity function. We link the column and its most relevant slice and add them to the auto-CoT prompt in the same format as the manual labeled CoT prompt. Note that during this process, we ignore the column appearing in the GROUP BY clause of the SQL query, since the GROUP BY column is commonly not mentioned directly in the question.\nSecondly, we enumerate each table [tab] appearing in the SQL query, where [tab] represents the table name. In this process, we eliminate tables that have occurred in the columns, since those tables have been added into the auto CoT prompt. The left tables only appear in the FROM clause and\nindicate some extra information. For each table, we also compute all the similarity scores and find out the most relevant question slice, i.e.,\nargmax qi,j Sim([tab], qi,j).\nWe link the table and its most relevant slice and add them to the auto-CoT.\nFinally, we enumerate the values in the SQL query and then add them to the auto-CoT. Figure 2 shows an instance of the auto-generated CoT from the train set of the Spider dataset."
        },
        {
            "heading": "3.4 Extension for Multi-turn Text-to-SQL",
            "text": "The prompts described in the previous sections are designed for the single-turn text-to-SQL task. However, questions in the multi-turn text-to-SQL task are context-dependent and thus those prompts cannot be directly used. Moreover, the auto-CoT method is also disabled under the multi-turn setting, since the auto-CoT method finds information about schema linking based on the question slices. Under the multi-turn setting, this information may distribute into several context-dependent sentences.\nIn order to deal with the challenge of the multiturn text-to-SQL task, we use LLMs to convert the multi-turn text-to-SQL task into the single-turn text-to-SQL task. Concretely, with the help of the LLMs, we can rewrite the question sentences and remove the context dependency among them. Thus each rewritten question and its corresponding SQL query turn into a new independent dataset example. We then directly apply the previous in-context learning method in the converted multi-turn text-toSQL task.\nThe quality of the rewritten questions would influence the LLMs\u2019 performance a lot. It is necessary to manually label some rewriting exemplars in order to fix the format and improve the quality of the LLMs\u2019 outputs. For each multi-turn textto-SQL dataset, we select 10 examples from the train set at random and manually label the rewritten results."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "Models We mainly use the GPT-3.5-turbo model to evaluate our proposed approach. The GPT-3.5turbo model is a low-cost LLM and is very large to have the emergent ability (Wei et al., 2022) for handling the text-to-SQL task. In addition, we use the GPT-4 model to evaluate our auto-CoT method on the Spider dataset (Yu et al., 2018), since the GPT-4 model has a stronger reasoning ability but is much more expensive. We use the PLM text2vec-base-chinese to compute the similarity scores when selecting dynamic exemplars and generating auto-CoT prompts.\nHyperparameters The temperature in LLMs\u2019 API is set to 0, i.e. the greedy decoding strategy is applied. The text-to-SQL tasks require the model to generate SQL queries with strict grammar rules. The LLMs are likely to generate invalid SQL queries or to write SQL queries that are not relevant to the given questions if the temperature is too high. The number of max tokens is set to 150 for the trivial in-context learning setting and 750 when using the CoT method.\nDatasets We mainly evaluate our proposed approach on Spider, a large-scale human-labeled cross-domain text-to-SQL dataset across 200 databases covering 138 domains. The Spider dataset contains 8,659 examples in the train set and 1,034 examples in the dev set. It also provides the evaluation script which divides SQL queries into four categories (i.e. easy, medium, hard, and extra) according to the difficulty level. The test set of Spider is not publicly available. We conduct the experiments on the dev set.\nIn addition, we also conduct the in-context learning experiments on Spider-Syn (Gan et al., 2021a), Spider-DK (Gan et al., 2021b) and SpiderRealistic (Deng et al., 2021). Based on Spider, Spider-Syn replaces some schema-related tokens in the question with synonyms, which would make models unable to discover useful schema items with the simple string-matching method. SpiderDK defines five types of domain knowledge and modifies some examples by adding domain knowledge that reflects real-world question paraphrases. Spider-DK evaluates the models\u2019 generalization ability across domains when domain knowledge does not frequently appear in the train set. SpiderRealistic removes explicit mentions of column\nnames to evaluate the model\u2019s ability to capture text-table alignment.\nAs for multi-turn text-to-SQL datasets, we conduct our experiments on SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). SParC totally consists of 4,298 coherent question sequences including 12k+ individual questions and the corresponding SQL queries. CoSQL totally contains 10k+ annotated SQL queries. Each dialogue in CoSQL simulates a real-world scenario where the common user is exploring the database and the expert is retrieving answers with SQL.\nEvaluation metrics We use three commonly used evaluation metrics of the text-to-SQL task: exact match accuracy (EM), execution accuracy (EX), and test-suite accuracy (TS). The EM metric requires each component of the predicted SQL to be equivalent to the corresponding component of the gold SQL. Values in the SQL query are not concerned with the EM metric. The EX metric requires the execution result of the predicted SQL to be correct. Since there may exist different SQL queries that represent the same semantic, the EX metric is commonly more precise than the EM metric. The TS metric also evaluates the execution result but requires the result to be correct under multiple database instances per database schema3.\nFor multi-turn text-to-SQL datasets, we evaluate our approach with question match accuracy (QM) and interaction match accuracy (IM). The QM score is 1 if the predicted SQL query for the single question is correct. The IM score is 1 if all the predicted SQL queries in the interaction are correct."
        },
        {
            "heading": "4.2 Zero-shot Results",
            "text": "3https://github.com/taoyds/ test-suite-sql-eval\nAs discussed in Section 3.1, the LLMs\u2019 performance is mainly influenced by the database prompt style and the rows of database contents under the zero-shot learning setting. We first conduct experiments for studying the influence on LLMs\u2019 performance caused by the rows of database contents. We fix the LLM as the GPT-3.5-turbo model and the database style as Table(Column) and only change the rows of database contents for each table in the prompt. Figure 3 shows the result on the Spider dev set. The LLM achieves the lowest score when no database content is provided. This indicates that database contents can provide useful tips for LLMs, especially when the testing case is sensitive to values in SQL where Table 1 shows two cases. In the first case, the 3 records from the database contain exactly one cell value \u201cFrance\u201d instead of \u201cFrench\u201d for the column \u201csinger.Citizenship\u201d. Thus the LLM successfully predicts the correct value when these records are added to the prompt. In the second case, the database contents point out that \u201cAberdeen\u201d is the city name so that the LLM can predict the correct SQL structure.\nThe LLM gets the best score when the rows of database contents is set to 3. Too much database content in the prompt would not improve the LLMs\u2019 performance. Therefore, we always set the rows of database contents to 3 for the subsequent experiments.\nTable 3 shows the different performances of the GPT-3.5-turbo model when using different database styles. In general, Table(Column) and Table(Column)(PF) achieve higher scores than the other three database styles with the zeroshot learning setting because these two database styles follow the OpenAI\u2019s official document and\nmay be more similar to LLMs\u2019 pretrained data. Create(EoC) and Create(EoT) performs better than Create(NoPF) in EX and TS metrics. This indicates that primary keys and foreign keys in the prompt can offer LLMs effective information."
        },
        {
            "heading": "4.3 Few-shot Results",
            "text": "Table 11 shows all the few-shot experiment results on the Spider dev set, where different database styles and different numbers of static and dynamic exemplars are used. Compared with the zero-shot results, it is obvious that all the EM scores increase a lot. This is because SQL queries from the same dataset usually share similar grammar and structure and thus the exemplars from the Spider train set lead LLMs to output a similar SQL query.\nUnder the trivial few-shot learning setting, the TS scores also get improved by 1%-3% except for the Table(Column) database style. Table(Column) no longer performs better than Table(Column)(PF), since LLMs\u2019 accuracy for predicting hard and extra hard SQL queries get increased with the few-shot exemplars and thus primary keys and foreign keys in the prompt become more important.\nThe experiment results prove that our ACTSQL approach is effective. When the GPT-3.5turbo model uses the ACT-SQL approach with the Create(EoT) database style, it achieves the best EM score of 62.7% and the best TS score of 71.4%. The best database style changes because LLMs can learn from exemplars. Table 13 shows the case study for the ACT-SQL method. With the trivial few-shot learning setting, there is a redundant column \u201cTV_Channel.Hight_definition_TV\u201d appearing in the SELECT clause. When the ACT-SQL method is applied, the entire output generated by the LLM contains the complete thinking process which successfully does the schema linking. After clarifying all the tables and columns that may be used in SQL, the LLM eventually writes the correct SQL query without any redundant schema item.\nSince the GPT-4 model is expensive, we use the GPT-4 model to evaluate our ACT-SQL approach only with the Create(EoT) database style and ns = nd = 2. Table 2 shows the performances of our ACT-SQL and other previous works using in-context learning with LLMs. The ACT-SQL approach uses the LLMs\u2019 API call only once for generating one SQL query and achieves the highest EM, EX, and TS scores among existing in-context learning approaches. ACT-SQL\u2019s performance is also comparable to finetuned models. Actually, finetuned models would get higher scores on the dev set than the test set, since these models are selected by the dev set performance. Instead, incontext learning methods would not suffer the performance gap between the dev set and the test set. Table 4 shows some previous works\u2019 performances on Spider dev set and test set. For finetuned ap-\nproaches mentioned in the table, the performances drop from the dev set to the test set. On the contrary, for in-context learning approaches mentioned in the table, the performances increase from the dev set to the test set. After all, finetuned models are selected by the dev set performance, which would lead to the overfitting on the dev set and the performance dropping on the test set. For in-context learning approaches, the dev set and the test set are equal to the model. Performances between the dev set and the test set are only affected by the dataset feature.\nTable 5, Table 6 and Table 7 shows the GPT3.5-turbo\u2019s performances on Spider-Syn, SpiderDK, and Spider-Realistic dev set. We use the Create(EoT) database style and set ns = nd = 2. The experiment results show that our approach is still comparable to finetuned models on Spider-Syn and Spider-Realistic datasets. On the Spider-DK dataset, our approach\u2019s EX score surpasses finetuned models. This is due to the wide range of domain knowledge stored in LLMs."
        },
        {
            "heading": "4.4 Multi-turn Datasets Results",
            "text": "Table 8 and Table 9 show the GPT-3.5-turbo\u2019s performances on two multi-turn text-to-SQL datasets, i.e. SParC and CoSQL. The database style is set to Create(CoT) and nd, ns are set to 2 as before. The ACT-SQL approach is not that effective when applied to multi-turn datasets. We believe that our two-phase method causes bad performance. In the first phase, we use LLMs to rewrite questions in the interaction and convert the multi-turn dataset into the single-turn dataset. Sometimes the rewritten result\u2019s quality is bad, which influences the schema-linking process. Table 10 shows two rewritten instances from the SParC dev set. In the first instance, the LLM correctly rewrites all sentences without missing any key information. However, in the second instance, the LLM does not remove the context dependency for the second sentence. This also leads to the error in the third sentence, where the keyword \u201cairline\u201d in the first sentence is missing. In general, our in-context learning method is comparable to finetuned models (GAZP + BERT) though there is still a big room for improvement. Improving LLMs\u2019 performance on this difficult task is a challenging future work. We just complete the initial exploration."
        },
        {
            "heading": "5 Conclusion",
            "text": "LLMs have shown a strong ability in various domains with the in-context learning method. The latest studies have attempted to use LLMs to solve the text-to-SQL task. However, previous prompting approaches either perform worse than finetuned models or need to use LLMs\u2019 API call many times. We design the CoT prompt which can be automatically generated and propose our ACT-SQL approach. The ACT-SQL approach uses LLMs\u2019 API call only once to generate one SQL query. The experiment results prove that our approach achieves state-ofthe-art performance on the Spider dev set among existing in-context learning approaches. Furthermore, we extend our approach to multi-turn text-toSQL datasets."
        },
        {
            "heading": "Limitations",
            "text": "There are some limitations in our work. First of all, we use a hybrid strategy for the exemplar selection. The numbers of static and dynamic exemplars are hyperparameters and still need manually determined. In addition, it is a relatively simple strategy that still needs improvement. Furthermore, our approach achieves relatively poor scores on some robustness variants of the Spider dataset and some multi-turn text-to-SQL datasets. Exploration of\nthese datasets can be conducted in future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank all the anonymous reviewers for their thoughtful comments. This work has been supported by the China NSFC Project (No.62106142 and No.62120106006), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and Startup Fund for Youngman Research at SJTU (SFYR at SJTU)."
        },
        {
            "heading": "A Detailed Experiment Results",
            "text": ""
        },
        {
            "heading": "Few-shot ACT-SQL",
            "text": ""
        },
        {
            "heading": "B Auto-CoT by LLM",
            "text": ""
        },
        {
            "heading": "C Prompts",
            "text": "In this section, we list detailed prompts used in our experiments."
        },
        {
            "heading": "C.1 Database Styles",
            "text": ""
        },
        {
            "heading": "C.1.1 Table(Column)",
            "text": "# stadium(Stadium_ID, Location, Name, Capacity, Highest, Lowest, Average) # singer(Singer_ID, Name, Country, Song_Name, Song_release_year, Age, Is_male) # concert(concert_ID, concert_Name, Theme, Stadium_ID, Year) # singer_in_concert(concert_ID, Singer_ID)"
        },
        {
            "heading": "C.1.2 Table(Column)(PF)",
            "text": "# stadium(Stadium_ID, Location, Name, Capacity, Highest, Lowest, Average) # singer(Singer_ID, Name, Country, Song_Name, Song_release_year, Age, Is_male) # concert(concert_ID, concert_Name, Theme, Stadium_ID, Year) # singer_in_concert(concert_ID, Singer_ID) # primary keys = [stadium.Stadium_ID, singer.Singer_ID, concert.concert_ID, singer_in_concert.concert_ID] # foreign keys = [concert.Stadium_ID = stadium.Stadium_ID, singer_in_concert.Singer_ID = singer.Singer_ID, singer_in_concert.concert_ID = concert.concert_ID]"
        },
        {
            "heading": "C.1.3 Create(NoPF)",
            "text": "create table stadium (\nStadium_ID number, Location text, Name text, Capacity number, Highest number, Lowest number, Average number\n) create table singer (\nSinger_ID number, Name text, Country text, Song_Name text, Song_release_year text, Age number, Is_male others\n) create table concert (\nconcert_ID number, concert_Name text, Theme text, Stadium_ID text, Year text\n) create table singer_in_concert (\nconcert_ID number, Singer_ID text\n)"
        },
        {
            "heading": "C.1.4 Create(EoC)",
            "text": "create table stadium (\nStadium_ID number primary key, Location text,\nName text, Capacity number, Highest number, Lowest number, Average number\n) create table singer (\nSinger_ID number primary key, Name text, Country text, Song_Name text, Song_release_year text, Age number, Is_male others\n) create table concert (\nconcert_ID number primary key, concert_Name text, Theme text, Stadium_ID text references stadium(Stadium_ID), Year text\n) create table singer_in_concert (\nconcert_ID number primary key references concert(concert_ID), Singer_ID text references singer(Singer_ID)\n)"
        },
        {
            "heading": "C.1.5 Create(EoT)",
            "text": "create table stadium (\nStadium_ID number, Location text, Name text, Capacity number, Highest number, Lowest number, Average number, primary key (Stadium_ID)\n) create table singer (\nSinger_ID number, Name text, Country text, Song_Name text, Song_release_year text, Age number, Is_male others, primary key (Singer_ID)\n) create table concert (\nconcert_ID number, concert_Name text, Theme text,\nStadium_ID text, Year text, primary key (concert_ID), foreign key (Stadium_ID) references stadium(Stadium_ID)\n) create table singer_in_concert (\nconcert_ID number, Singer_ID text, primary key (concert_ID), foreign key (Singer_ID) references singer(Singer_ID), foreign key (concert_ID) references concert(concert_ID)\n)"
        },
        {
            "heading": "C.2 Database Contents",
            "text": "We only use the Table(Column) and the Create(EoT) database styles in the following prompt examples. The other three database styles are similar. The rows of database contents is set to 3 in the following prompt examples."
        },
        {
            "heading": "C.2.1 Table(Column)",
            "text": "# stadium(Stadium_ID, Location, Name, Capacity, Highest, Lowest, Average) /* 3 example rows from table stadium: Stadium_ID Location Name Capacity Highest Lowest Average 1 Raith Rovers Stark\u2019s Park 10104 4812 1294 2106 2 Ayr United Somerset Park 11998 2363 1057 1477 3 East Fife Bayview Stadium 2000 1980 533 864 **/ # singer(Singer_ID, Name, Country, Song_Name, Song_release_year, Age, Is_male) /* 3 example rows from table singer: Singer_ID Name Country Song_Name Song_release_year Age Is_male 1 Joe Sharp Netherlands You 1992 52 F 2 Timbaland United States Dangerous 2008 32 T 3 Justin Brown France Hey Oh 2013 29 T **/ # concert(concert_ID, concert_Name, Theme, Stadium_ID, Year) /* 3 example rows from table concert: concert_ID concert_Name Theme Stadium_ID Year 1 Auditions Free choice 1 2014 2 Super bootcamp Free choice 2 2 2014 3 Home Visits Bleeding Love 2 2015 **/ # singer_in_concert(concert_ID, Singer_ID) /* 3 example rows from table singer_in_concert: concert_ID Singer_ID 1 2 1 3 1 5 **/"
        },
        {
            "heading": "C.2.2 Create(EoT)",
            "text": "create table stadium ( Stadium_ID number, Location text, Name text, Capacity number, Highest number, Lowest number, Average number, primary key (Stadium_ID) ) /* 3 example rows from table stadium: Stadium_ID Location Name Capacity Highest Lowest Average 1 Raith Rovers Stark\u2019s Park 10104 4812 1294 2106 2 Ayr United Somerset Park 11998 2363 1057 1477 3 East Fife Bayview Stadium 2000 1980 533 864 **/ create table singer (\nSinger_ID number, Name text, Country text, Song_Name text, Song_release_year text, Age number, Is_male others, primary key (Singer_ID)\n) /* 3 example rows from table singer: Singer_ID Name Country Song_Name Song_release_year Age Is_male 1 Joe Sharp Netherlands You 1992 52 F 2 Timbaland United States Dangerous 2008 32 T 3 Justin Brown France Hey Oh 2013 29 T **/ create table concert (\nconcert_ID number, concert_Name text, Theme text, Stadium_ID text, Year text, primary key (concert_ID), foreign key (Stadium_ID) references stadium(Stadium_ID)\n) /* 3 example rows from table concert: concert_ID concert_Name Theme Stadium_ID Year 1 Auditions Free choice 1 2014 2 Super bootcamp Free choice 2 2 2014 3 Home Visits Bleeding Love 2 2015 **/\ncreate table singer_in_concert ( concert_ID number, Singer_ID text, primary key (concert_ID), foreign key (Singer_ID) references singer(Singer_ID), foreign key (concert_ID) references concert(concert_ID) ) /* 3 example rows from table singer_in_concert: concert_ID Singer_ID 1 2 1 3 1 5 **/\nC.3 In-context Learning Prompts\nWe only use the Create(EoT) database styles in the following prompt examples. The other four database styles are similar. The rows of database contents is set to 3 in the following prompt examples. Under the few-shot setting, the first two shots are static exemplars and the last two shots are dynamic exemplars.\nC.3.1 Zero-shot role: system content: Given the database schema, you need to translate the question into the SQL query.\nrole: user content: Database schema: create table stadium (\nStadium_ID number, Location text, Name text, Capacity number, Highest number, Lowest number, Average number, primary key (Stadium_ID)\n) /* 3 example rows from table stadium: Stadium_ID Location Name Capacity Highest Lowest Average 1 Raith Rovers Stark\u2019s Park 10104 4812 1294 2106 2 Ayr United Somerset Park 11998 2363 1057 1477 3 East Fife Bayview Stadium 2000 1980 533 864 **/ create table singer (\nSinger_ID number, Name text, Country text, Song_Name text, Song_release_year text, Age number,\nIs_male others, primary key (Singer_ID)\n) /* 3 example rows from table singer: Singer_ID Name Country Song_Name Song_release_year Age Is_male 1 Joe Sharp Netherlands You 1992 52 F 2 Timbaland United States Dangerous 2008 32 T 3 Justin Brown France Hey Oh 2013 29 T **/ create table concert (\nconcert_ID number, concert_Name text, Theme text, Stadium_ID text, Year text, primary key (concert_ID), foreign key (Stadium_ID) references stadium(Stadium_ID)\n) /* 3 example rows from table concert: concert_ID concert_Name Theme Stadium_ID Year 1 Auditions Free choice 1 2014 2 Super bootcamp Free choice 2 2 2014 3 Home Visits Bleeding Love 2 2015 **/ create table singer_in_concert (\nconcert_ID number, Singer_ID text, primary key (concert_ID), foreign key (Singer_ID) references singer(Singer_ID), foreign key (concert_ID) references concert(concert_ID)\n) /* 3 example rows from table singer_in_concert: concert_ID Singer_ID 1 2 1 3 1 5 **/ Question: How many singers do we have?\nC.3.2 Few-shot role: system content: Given the database schema, you need to translate the question into the SQL query.\nrole: user content: Database schema: create table ACCOUNTS (\ncustid number,\nname text, primary key (custid)\n) /* 3 example rows from table ACCOUNTS: custid name 1 Brown 2 Wang 3 O\u2019mahony **/ create table SAVINGS (\ncustid number, balance number, primary key (custid), foreign key (custid) references ACCOUNTS(custid)\n) /* 3 example rows from table SAVINGS: custid balance 1 200000.0 2 999999999.0 3 230000.0 **/ create table CHECKING (\ncustid number, balance number, primary key (custid), foreign key (custid) references ACCOUNTS(custid)\n) /* 3 example rows from table CHECKING: custid balance 1 10000.0 2 2000.0 3 3000.0 **/ Question: Find the name and savings balance of the top 3 accounts with the highest saving balance sorted by savings balance in descending order.\nrole: assistant content: SELECT T1.name , T2.balance FROM accounts AS T1 JOIN savings AS T2 ON T1.custid = T2.custid ORDER BY T2.balance DESC LIMIT 3\nrole: user content: Database schema: create table flight (\nflno number, origin text, destination text,\ndistance number, departure_date time, arrival_date time, price number, aid number, primary key (flno), foreign key (aid) references aircraft(aid)\n) /* 3 example rows from table flight: flno origin destination distance departure_date arrival_date price aid 99 Los Angeles Washington D.C. 2308 04/12/2005 09:30 04/12/2005 09:40 235.98 1 13 Los Angeles Chicago 1749 04/12/2005 08:45 04/12/2005 08:45 220.98 3 346 Los Angeles Dallas 1251 04/12/2005 11:50 04/12/2005 07:05 182 2 **/ create table aircraft (\naid number, name text, distance number, primary key (aid)\n) /* 3 example rows from table aircraft: aid name distance 1 Boeing 747-400 8430 2 Boeing 737-800 3383 3 Airbus A340-300 7120 **/ create table employee (\neid number, name text, salary number, primary key (eid)\n) /* 3 example rows from table employee: eid name salary 242518965 James Smith 120433 141582651 Mary Johnson 178345 11564812 John Williams 153972 **/ create table certificate (\neid number, aid number, primary key (eid), foreign key (aid) references aircraft(aid), foreign key (eid) references employee(eid)\n) /* 3 example rows from table certificate: eid aid 11564812 2 11564812 10 90873519 6 **/ Question: Which destination has least number of flights?\nrole: assistant content: SELECT destination FROM Flight GROUP BY destination ORDER BY count(*) LIMIT 1\nrole: user content: Database schema: create table Activity (\nactid number, activity_name text, primary key (actid)\n) /* 3 example rows from table Activity: actid activity_name 770 Mountain Climbing 771 Canoeing 772 Kayaking **/ create table Participates_in (\nstuid number, actid number, foreign key (actid) references Activity(actid), foreign key (stuid) references Student(StuID)\n) /* 3 example rows from table Participates_in: stuid actid 1001 770 1001 771 1001 777 **/ create table Faculty_Participates_in (\nFacID number, actid number, foreign key (actid) references Activity(actid), foreign key (FacID) references Faculty(FacID)\n) /* 3 example rows from table Faculty_Participates_in: FacID actid"
        },
        {
            "heading": "1082 784",
            "text": ""
        },
        {
            "heading": "1082 785",
            "text": ""
        },
        {
            "heading": "1082 790 **/",
            "text": "create table Student (\nStuID number, LName text, Fname text, Age number, Sex text, Major number, Advisor number, city_code text, primary key (StuID)\n) /* 3 example rows from table Student: StuID LName Fname Age Sex Major Advisor city_code 1001 Smith Linda 18 F 600 1121 BAL 1002 Kim Tracy 19 F 600 7712 HKG 1003 Jones Shiela 21 F 600 7792 WAS **/ create table Faculty (\nFacID number, Lname text, Fname text, Rank text, Sex text, Phone number, Room text, Building text, primary key (FacID)\n) /* 3 example rows from table Faculty: FacID Lname Fname Rank Sex Phone Room Building 1082 Giuliano Mark Instructor M 2424 224 NEB 1121 Goodrich Michael Professor M 3593 219 NEB 1148 Masson Gerald Professor M 3402 224B NEB **/ Question: How many female Professors do we have?\nrole: assistant content: SELECT count(*) FROM Faculty WHERE Sex = \u2019F\u2019 AND Rank = \"Professor\"\nrole: user content: Database schema: create table region (\nRegion_ID number,\nRegion_name text, Date text, Label text, Format text, Catalogue text, primary key (Region_ID)\n) /* 3 example rows from table region: Region_ID Region_name Date Label Format Catalogue 1 United Kingdom 1 July 2002 Parlophone CD 540 3622 2 United Kingdom 1 July 2002 Parlophone 2\u00d7 LP 539 9821 3 Japan 3 July 2002 Toshiba-EMI CD TOCP-66045 **/ create table party (\nParty_ID number, Minister text, Took_office text, Left_office text, Region_ID number, Party_name text, primary key (Party_ID), foreign key (Region_ID) references region(Region_ID)\n) /* 3 example rows from table party: Party_ID Minister Took_office Left_office Region_ID Party_name 1 Dr. Kwame Nkrumah (MP) 1957 1958 1 Convention Peoples Party 2 Kojo Botsio (MP) 1958 1959 2 Progress Party 3 Ebenezer Ako-Adjei (MP) 1959 1960 3 3 **/ create table member (\nMember_ID number, Member_Name text, Party_ID text, In_office text, primary key (Member_ID), foreign key (Party_ID) references party(Party_ID)\n) /* 3 example rows from table member: Member_ID Member_Name Party_ID In_office 1 Hon Tony Abbott 3 1994\u2013present 2 Hon Dick Adams 2 1993\u20132013 3 Anthony Albanese 2 1996\u2013present **/ create table party_events (\nEvent_ID number, Event_Name text, Party_ID number, Member_in_charge_ID number,\nprimary key (Event_ID), foreign key (Member_in_charge_ID) references member(Member_ID), foreign key (Party_ID) references party(Party_ID)\n) /* 3 example rows from table party_events: Event_ID Event_Name Party_ID Member_in_charge_ID 1 Annaual Meeting 1 4 2 Conference 1 12 3 Annaual Meeting 2 2 **/ Question: How many parties do we have?\nrole: assistant content: SELECT count(DISTINCT party_name) FROM party\nrole: user content: Database schema: create table stadium (\nStadium_ID number, Location text, Name text, Capacity number, Highest number, Lowest number, Average number, primary key (Stadium_ID)\n) /* 3 example rows from table stadium: Stadium_ID Location Name Capacity Highest Lowest Average 1 Raith Rovers Stark\u2019s Park 10104 4812 1294 2106 2 Ayr United Somerset Park 11998 2363 1057 1477 3 East Fife Bayview Stadium 2000 1980 533 864 **/ create table singer (\nSinger_ID number, Name text, Country text, Song_Name text, Song_release_year text, Age number, Is_male others, primary key (Singer_ID)\n) /* 3 example rows from table singer: Singer_ID Name Country Song_Name Song_release_year Age Is_male 1 Joe Sharp Netherlands You 1992 52 F\n2 Timbaland United States Dangerous 2008 32 T 3 Justin Brown France Hey Oh 2013 29 T **/ create table concert (\nconcert_ID number, concert_Name text, Theme text, Stadium_ID text, Year text, primary key (concert_ID), foreign key (Stadium_ID) references stadium(Stadium_ID)\n) /* 3 example rows from table concert: concert_ID concert_Name Theme Stadium_ID Year 1 Auditions Free choice 1 2014 2 Super bootcamp Free choice 2 2 2014 3 Home Visits Bleeding Love 2 2015 **/ create table singer_in_concert (\nconcert_ID number, Singer_ID text, primary key (concert_ID), foreign key (Singer_ID) references singer(Singer_ID), foreign key (concert_ID) references concert(concert_ID)\n) /* 3 example rows from table singer_in_concert: concert_ID Singer_ID 1 2 1 3 1 5 **/ Question: How many singers do we have?\nC.3.3 Auto-CoT role: system content: Given the database schema, you need to translate the question into the SQL query.\nrole: user content: Database schema: create table ACCOUNTS (\ncustid number, name text, primary key (custid)\n) /* 3 example rows from table ACCOUNTS: custid name 1 Brown\n2 Wang 3 O\u2019mahony **/ create table SAVINGS (\ncustid number, balance number, primary key (custid), foreign key (custid) references ACCOUNTS(custid)\n) /* 3 example rows from table SAVINGS: custid balance 1 200000.0 2 999999999.0 3 230000.0 **/ create table CHECKING (\ncustid number, balance number, primary key (custid), foreign key (custid) references ACCOUNTS(custid)\n) /* 3 example rows from table CHECKING: custid balance 1 10000.0 2 2000.0 3 3000.0 **/ Question: Find the name and savings balance of the top 3 accounts with the highest saving balance sorted by savings balance in descending order.\nrole: assistant content: Let\u2019s think step by step. According to \"savings balance\", columns [SAVINGS.balance] may be used. According to \"accounts\", columns [ACCOUNTS.name] may be used. Values [3] may be used. So the final answer is: SELECT T1.name , T2.balance FROM accounts AS T1 JOIN savings AS T2 ON T1.custid = T2.custid ORDER BY T2.balance DESC LIMIT 3\nrole: user content: Database schema: create table flight (\nflno number, origin text, destination text, distance number, departure_date time,\narrival_date time, price number, aid number, primary key (flno), foreign key (aid) references aircraft(aid)\n) /* 3 example rows from table flight: flno origin destination distance departure_date arrival_date price aid 99 Los Angeles Washington D.C. 2308 04/12/2005 09:30 04/12/2005 09:40 235.98 1 13 Los Angeles Chicago 1749 04/12/2005 08:45 04/12/2005 08:45 220.98 3 346 Los Angeles Dallas 1251 04/12/2005 11:50 04/12/2005 07:05 182 2 **/ create table aircraft (\naid number, name text, distance number, primary key (aid)\n) /* 3 example rows from table aircraft: aid name distance 1 Boeing 747-400 8430 2 Boeing 737-800 3383 3 Airbus A340-300 7120 **/ create table employee (\neid number, name text, salary number, primary key (eid)\n) /* 3 example rows from table employee: eid name salary 242518965 James Smith 120433 141582651 Mary Johnson 178345 11564812 John Williams 153972 **/ create table certificate (\neid number, aid number, primary key (eid), foreign key (aid) references aircraft(aid), foreign key (eid) references employee(eid)\n) /*\n3 example rows from table certificate: eid aid 11564812 2 11564812 10 90873519 6 **/ Question: Which destination has least number of flights?\nrole: assistant content: Let\u2019s think step by step. According to \"flights\", columns [flight.destination] may be used. Values [1] may be used. So the final answer is: SELECT destination FROM Flight GROUP BY destination ORDER BY count(*) LIMIT 1\nrole: user content: Database schema: create table Student (\nStuID number, LName text, Fname text, Age number, Sex text, Major number, Advisor number, city_code text, primary key (StuID)\n) /* 3 example rows from table Student: StuID LName Fname Age Sex Major Advisor city_code 1001 Smith Linda 18 F 600 1121 BAL 1002 Kim Tracy 19 F 600 7712 HKG 1003 Jones Shiela 21 F 600 7792 WAS **/ create table Dorm (\ndormid number, dorm_name text, student_capacity number, gender text\n) /* 3 example rows from table Dorm: dormid dorm_name student_capacity gender 100 Smith Hall 85 X 110 Bud Jones Hall 116 M 140 Fawlty Towers 355 X **/ create table Dorm_amenity (\namenid number, amenity_name text\n) /* 3 example rows from table Dorm_amenity: amenid amenity_name 900 TV Lounge 901 Study Room 902 Pub in Basement **/ create table Has_amenity (\ndormid number, amenid number, foreign key (amenid) references Dorm_amenity(amenid), foreign key (dormid) references Dorm(dormid)\n) /* 3 example rows from table Has_amenity: dormid amenid 109 900 109 901 109 903 **/ create table Lives_in (\nstuid number, dormid number, room_number number, foreign key (dormid) references Dorm(dormid), foreign key (stuid) references Student(StuID)\n) /* 3 example rows from table Lives_in: stuid dormid room_number 1001 109 105 1002 100 112 1003 100 124 **/ Question: How many dorms have amenities?\nrole: assistant content: Let\u2019s think step by step. According to \"dorms have amenities ?\", columns [Has_amenity.dormid] may be used. So the final answer is: SELECT count(DISTINCT dormid) FROM has_amenity\nrole: user content: Database schema: create table Student (\nStuID number,\nLName text, Fname text, Age number, Sex text, Major number, Advisor number, city_code text, primary key (StuID)\n) /* 3 example rows from table Student: StuID LName Fname Age Sex Major Advisor city_code 1001 Smith Linda 18 F 600 1121 BAL 1002 Kim Tracy 19 F 600 7712 HKG 1003 Jones Shiela 21 F 600 7792 WAS **/ create table Dorm (\ndormid number, dorm_name text, student_capacity number, gender text\n) /* 3 example rows from table Dorm: dormid dorm_name student_capacity gender 100 Smith Hall 85 X 110 Bud Jones Hall 116 M 140 Fawlty Towers 355 X **/ create table Dorm_amenity (\namenid number, amenity_name text\n) /* 3 example rows from table Dorm_amenity: amenid amenity_name 900 TV Lounge 901 Study Room 902 Pub in Basement **/ create table Has_amenity (\ndormid number, amenid number, foreign key (amenid) references Dorm_amenity(amenid), foreign key (dormid) references Dorm(dormid)\n) /* 3 example rows from table Has_amenity: dormid amenid 109 900 109 901"
        },
        {
            "heading": "109 903 **/",
            "text": "create table Lives_in (\nstuid number, dormid number, room_number number, foreign key (dormid) references Dorm(dormid), foreign key (stuid) references Student(StuID)\n) /* 3 example rows from table Lives_in: stuid dormid room_number 1001 109 105 1002 100 112 1003 100 124 **/ Question: How many diffrent dorm amenities are there?\nrole: assistant content: Let\u2019s think step by step. According to \"dorm amenities are there ?\", tables [Dorm_amenity] may be used. So the final answer is: SELECT count(*) FROM dorm_amenity\nrole: user content: Database schema: create table stadium (\nStadium_ID number, Location text, Name text, Capacity number, Highest number, Lowest number, Average number, primary key (Stadium_ID)\n) /* 3 example rows from table stadium: Stadium_ID Location Name Capacity Highest Lowest Average 1 Raith Rovers Stark\u2019s Park 10104 4812 1294 2106 2 Ayr United Somerset Park 11998 2363 1057 1477 3 East Fife Bayview Stadium 2000 1980 533 864 **/ create table singer (\nSinger_ID number, Name text, Country text, Song_Name text, Song_release_year text,\nAge number, Is_male others, primary key (Singer_ID)\n) /* 3 example rows from table singer: Singer_ID Name Country Song_Name Song_release_year Age Is_male 1 Joe Sharp Netherlands You 1992 52 F 2 Timbaland United States Dangerous 2008 32 T 3 Justin Brown France Hey Oh 2013 29 T **/ create table concert (\nconcert_ID number, concert_Name text, Theme text, Stadium_ID text, Year text, primary key (concert_ID), foreign key (Stadium_ID) references stadium(Stadium_ID)\n) /* 3 example rows from table concert: concert_ID concert_Name Theme Stadium_ID Year 1 Auditions Free choice 1 2014 2 Super bootcamp Free choice 2 2 2014 3 Home Visits Bleeding Love 2 2015 **/ create table singer_in_concert (\nconcert_ID number, Singer_ID text, primary key (concert_ID), foreign key (Singer_ID) references singer(Singer_ID), foreign key (concert_ID) references concert(concert_ID)\n) /* 3 example rows from table singer_in_concert: concert_ID Singer_ID 1 2 1 3 1 5 **/ Question: How many singers do we have?"
        }
    ],
    "title": "ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought",
    "year": 2023
}