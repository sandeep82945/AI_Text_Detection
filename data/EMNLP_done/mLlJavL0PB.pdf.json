{
    "abstractText": "With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, INSTRUCTEXCEL,1 created by leveraging the \u2018Automate\u2019 feature in Excel to automatically generate OfficeScripts from users\u2019 actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that INSTRUCTEXCEL is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.",
    "authors": [
        {
            "affiliations": [],
            "name": "Justin Payan"
        },
        {
            "affiliations": [],
            "name": "Swaroop Mishra"
        },
        {
            "affiliations": [],
            "name": "Mukul Singh"
        },
        {
            "affiliations": [],
            "name": "Carina Negreanu"
        },
        {
            "affiliations": [],
            "name": "Christian Poelitz"
        },
        {
            "affiliations": [],
            "name": "Chitta Baral"
        },
        {
            "affiliations": [],
            "name": "Subhro Roy"
        },
        {
            "affiliations": [],
            "name": "Rasika Chakravarthy"
        },
        {
            "affiliations": [],
            "name": "Benjamin Van Durme"
        },
        {
            "affiliations": [],
            "name": "Elnaz Nouri"
        }
    ],
    "id": "SP:1d567f62704029ef22f06c52a49c9a34af0c6eb6",
    "references": [
        {
            "authors": [
                "Cem Anil",
                "Yuhuai Wu",
                "Anders Johan Andreassen",
                "Aitor Lewkowycz",
                "Vedant Misra",
                "Vinay Venkatesh Ramasesh",
                "Ambrose Slone",
                "Guy Gur-Ari",
                "Ethan Dyer",
                "Behnam Neyshabur."
            ],
            "title": "Exploring length generalization in large language models",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Anil",
                "Andrew M Dai",
                "Orhan Firat",
                "Melvin Johnson",
                "Dmitry Lepikhin",
                "Alexandre Passos",
                "Siamak Shakeri",
                "Emanuel Taropa",
                "Paige Bailey",
                "Zhifeng Chen"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le"
            ],
            "title": "Program synthesis with large language models. arXiv preprint arXiv:2108.07732",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Bach",
                "Victor Sanh",
                "Zheng Xin Yong",
                "Albert Webson",
                "Colin Raffel",
                "Nihal V Nayak",
                "Abheesht Sharma",
                "Taewoon Kim",
                "M Saiful Bari",
                "Thibault F\u00e9vry"
            ],
            "title": "Promptsource: An integrated development environment and repository for natural",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Barei\u00df",
                "Beatriz Souza",
                "Marcelo d\u2019Amorim",
                "Michael Pradel"
            ],
            "title": "Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code. arXiv e-prints, page arXiv:2206.01335",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are fewshot learners",
            "venue": "Proceedings of the 34th Annual Con-",
            "year": 2020
        },
        {
            "authors": [
                "Jun Shern Chan",
                "Michael Martin Pieler",
                "Jonathan Jao",
                "J\u2019er\u2019emy Scheurer",
                "Ethan Perez"
            ],
            "title": "Few-shot adaptation works with unpredictable data. ArXiv, abs/2208.01009",
            "year": 2022
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating Large Language Models Trained on Code",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Chen",
                "Xin Xie",
                "Ningyu Zhang",
                "Jiahuan Yan",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Adaprompt: Adaptive promptbased finetuning for relation extraction",
            "venue": "arXiv eprints, pages arXiv\u20132104.",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Petros Maniatis",
                "Rishabh Singh",
                "Charles Sutton",
                "Hanjun Dai",
                "Max Lin",
                "Denny Zhou."
            ],
            "title": "Spreadsheetcoder: Formula prediction from semi-structured context",
            "venue": "Proceedings of the 38th International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "son Wei",
                "Kathleen S. Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Desai",
                "Sumit Gulwani",
                "Vineet Hingorani",
                "Nidhi Jain",
                "Amey Karkare",
                "Mark Marron",
                "Subhajit Roy."
            ],
            "title": "Program synthesis using natural language",
            "venue": "Proceedings of the 38th International Conference on Software Engineering (ICSE).",
            "year": 2016
        },
        {
            "authors": [
                "Avia Efrat",
                "Omer Levy"
            ],
            "title": "The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982",
            "year": 2020
        },
        {
            "authors": [
                "Himanshu Gupta",
                "Saurabh Arjun Sawant",
                "Swaroop Mishra",
                "Mutsumi Nakamura",
                "Arindam Mitra",
                "Santosh Mashetty",
                "Chitta Baral."
            ],
            "title": "Instruction tuned models are quick learners",
            "venue": "arXiv preprint arXiv:2306.05539.",
            "year": 2023
        },
        {
            "authors": [
                "Prakhar Gupta",
                "Cathy Jiao",
                "Yi-Ting Yeh",
                "Shikib Mehri",
                "Maxine Eskenazi",
                "Jeffrey P Bigham."
            ],
            "title": "Improving zero and few-shot generalization in dialogue through instruction tuning",
            "venue": "arXiv preprint arXiv:2205.12673.",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Abdul Hadi",
                "Imam Nur Bani Yusuf",
                "Ferdian Thung",
                "Kien Gia Luong",
                "Lingxiao Jiang",
                "Fatemeh Hendijani Fard",
                "David Lo."
            ],
            "title": "On the effectiveness of pretrained models for api learning",
            "venue": "Proceedings of the 30th International Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Xiaodong Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring coding challenge competence with apps",
            "venue": "Proceedings of the 35th",
            "year": 2021
        },
        {
            "authors": [
                "Naman Jain",
                "Skanda Vaidyanath",
                "Arun Iyer",
                "Nagarajan Natarajan",
                "Suresh Parthasarathy",
                "Sriram Rajamani",
                "Rahul Sharma."
            ],
            "title": "Jigsaw: Large language models meet program synthesis",
            "venue": "Proceedings of the 44thInternational Conference on Software Engi-",
            "year": 2022
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Rajarshi Das."
            ],
            "title": "A survey on semantic parsing",
            "venue": "Automated Knowledge Base Construction (AKBC).",
            "year": 2018
        },
        {
            "authors": [
                "Kirby Kuznia",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral."
            ],
            "title": "Less is more: Summary of long instructions is better for program synthesis",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4532\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Doris Jung-Lin Lee",
                "Dixin Tang",
                "Kunal Agarwal",
                "Thyne Boonmark",
                "Caitlyn Chen",
                "Jake Kang",
                "Ujjaini Mukhopadhyay",
                "Jerry Song",
                "Micah Yong",
                "Marti A. Hearst",
                "Aditya G. Parameswaran"
            ],
            "title": "Lux: Always-on visualization recommendations",
            "year": 2021
        },
        {
            "authors": [
                "Sen Huang",
                "Johannes Welbl",
                "Sven Gowal",
                "Alexey Cherepanov",
                "James Molloy",
                "Daniel J. Mankowitz",
                "Esme Sutherland Robson",
                "Pushmeet Kohli",
                "Nando de Freitas",
                "Koray Kavukcuoglu",
                "Oriol Vinyals"
            ],
            "title": "Competition-level code generation with alpha",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out.",
            "year": 2004
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "William B Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022)",
            "venue": "The 3rd Workshop on Knowledge Extrac-",
            "year": 2022
        },
        {
            "authors": [
                "Man Luo",
                "Sharad Saxena",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral."
            ],
            "title": "Biotabqa: Instruction learning for biomedical table question answering",
            "venue": "CEUR Workshop Proceedings, volume 3180, pages 291\u2013304. CEUR-WS.",
            "year": 2022
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang."
            ],
            "title": "Wizardcoder: Empowering code large language models with evolinstruct",
            "venue": "arXiv preprint arXiv:2306.08568.",
            "year": 2023
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Reframing instructional prompts to GPTk\u2019s language",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Elnaz Nouri."
            ],
            "title": "Help me think: A simple prompting strategy for non-experts to create customized content with models",
            "venue": "arXiv preprint arXiv:2208.08232.",
            "year": 2022
        },
        {
            "authors": [
                "Avanika Narayan",
                "Ines Chami",
                "Laurel Orr",
                "Christopher R\u00e9"
            ],
            "title": "Can foundation models wrangle your data",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Mihir Parmar",
                "Swaroop Mishra",
                "Mirali Purohit",
                "Man Luo",
                "Murad Mohammad",
                "Chitta Baral."
            ],
            "title": "In-boxbart: Get instructions into biomedical multitask learning",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 112\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Pruthvi Patel",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral"
            ],
            "title": "Is a question decomposition unit all we need",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Hammond A. Pearce",
                "B. Tan",
                "Prashanth Krishnamurthy",
                "Farshad Khorrami",
                "Ramesh Karri",
                "Brendan Dolan-Gavitt"
            ],
            "title": "2022. Pop quiz! can a large language model help with reverse engineering? ArXiv, abs/2202.01142",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Poesia",
                "Alex Polozov",
                "Vu Le",
                "Ashish Tiwari",
                "Gustavo Soares",
                "Chris Meek",
                "Sumit Gulwani."
            ],
            "title": "Synchromesh: Reliable code generation from pre-trained language models",
            "venue": "Proceedings of the 10th International Conference on Learning Represen-",
            "year": 2022
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the 3rd Conference on Machine Translation: Research Papers.",
            "year": 2018
        },
        {
            "authors": [
                "Ravsehaj Singh Puri",
                "Swaroop Mishra",
                "Mihir Parmar",
                "Chitta Baral"
            ],
            "title": "How many data samples is an additional instruction worth? In Findings of the Association for Computational Linguistics: EACL",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Emily Reif",
                "Daphne Ippolito",
                "Ann Yuan",
                "Andy Coenen",
                "Chris Callison-Burch",
                "Jason Wei."
            ],
            "title": "A recipe for arbitrary text style transfer with large language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Kensen Shi",
                "Joey Hong",
                "Manzil Zaheer",
                "Pengcheng Yin",
                "Charles Sutton."
            ],
            "title": "Compositional generalization and decomposition in neural program synthesis",
            "venue": "Deep Learning for Code Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "Richard Shin",
                "Benjamin Van Durme."
            ],
            "title": "Fewshot semantic parsing with language models trained on code",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Mukul Singh",
                "Jos\u00e9 Cambronero",
                "Sumit Gulwani",
                "Vu Le",
                "Carina Negreanu",
                "Elnaz Nouri",
                "Mohammad Raza",
                "Gust Verbruggen"
            ],
            "title": "Format5: Abstention and examples for conditional table formatting with natural language",
            "year": 2023
        },
        {
            "authors": [
                "Mukul Singh",
                "Jos\u00e9 Cambronero",
                "Sumit Gulwani",
                "Vu Le",
                "Carina Negreanu",
                "Mohammad Raza",
                "Gust Verbruggen"
            ],
            "title": "Cornet: Learning table formatting rules by example",
            "year": 2022
        },
        {
            "authors": [
                "Liwen Wang",
                "Rumei Li",
                "Yang Yan",
                "Yuanmeng Yan",
                "Sirui Wang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Instructionner: A multi-task instruction-based generative framework for few-shot ner",
            "venue": "arXiv preprint arXiv:2203.03903.",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Atharva Naik",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Anjana Arunkumar",
                "David Stap"
            ],
            "title": "2022c. Sujan reddy a, sumanta patro, tanay dixit, and xudong shen",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "Proceedings of the 10th International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Orion Weller",
                "Nicholas Lourie",
                "Matt Gardner",
                "Matthew E. Peters."
            ],
            "title": "Learning from task descriptions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang."
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244.",
            "year": 2023
        },
        {
            "authors": [
                "Frank F. Xu",
                "Uri Alon",
                "Graham Neubig",
                "Vincent Josua Hellendoorn."
            ],
            "title": "A systematic evaluation of large language models of code",
            "venue": "Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022, page 1\u201310, New",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Bowen Deng",
                "Edgar Chen",
                "Bogdan Vasilescu",
                "Graham Neubig."
            ],
            "title": "Learning to mine aligned code and natural language pairs from stack overflow",
            "venue": "Proceedings of the 15th international conference on mining software repositories,",
            "year": 2018
        },
        {
            "authors": [
                "Wenting Zhao",
                "Konstantine Arkoudas",
                "Weiqi Sun",
                "Claire Cardie."
            ],
            "title": "Compositional task-oriented parsing as abstractive question answering",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning (ICML).",
            "year": 2021
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Kristy Lee",
                "Zheng Zhang",
                "Dan Klein."
            ],
            "title": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Charlie Snell",
                "Dan Klein",
                "Jason Eisner."
            ],
            "title": "Active programming by example with a natural language prior",
            "venue": "arXiv preprint arXiv:2205.12422.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Spreadsheet software, especially Microsoft Excel, is used every day by a diverse set of users (from complete novices to highly sophisticated power users) to solve a wide range of important tasks. To help users accomplish their tasks, Excel has introduced an array of features, including Analyze Data2 where users can specify their task in natural language (NL) for a limited scope of tasks. As even seemingly simple manipulations can often require knowledge of complex concepts like loops, conditionals, and regular expressions (Desai et al.,\n\u02da Work done during internship at Microsoft : Currently at Google DeepMind\n1INSTRUCTEXCEL, along with the code used in our experiments, is available at https://github.com/ microsoft/InstructExcel.\n2https://support.microsoft.com/en-us/office/get-insightswith-analyze-data-aa105149-1e48-446d-b3df-872dff70a866\n2016) it is important to further diversify the range of tasks AI assistants can support.\nMeanwhile, the advent of large language models, together with the instruction learning paradigm (Mishra et al., 2022b; Wei et al., 2022; Ouyang et al., 2022; Sanh et al., 2022), has paved the way for non-experts to accomplish many tasks just by providing instructions in natural language. Can we leverage this novel paradigm to let Excel users automatically execute complex tasks from NL instructions?\nExcel OfficeScripts3 provide a versatile TypeScript API for most standard functions in Excel, giving us a simple intermediate representation for executing NL instructions in Excel. Our task is to convert a user\u2019s description of a simple action or set of actions in Excel (such as editing formatting) into an executable OfficeScript accomplishing the action(s) on the given spreadsheet. Figure 1 shows an example input and output, with additional examples in Table 5 of Appendix A.\nWe introduce INSTRUCTEXCEL, a benchmark to investigate the instruction paradigm for generating OfficeScripts from NL instructions. We extract publicly available Excel sheets and ask crowdworkers to think of an action they want to perform on the sheet and write a summary of it in NL. Subsequently, we ask crowdworkers to execute that action in the sheet and use the Automate feature to record the action and generate the underlying code. Automatic code generation improves output quality and eliminates potential bugs prevalent in humanwritten code. INSTRUCTEXCEL contains over 10k samples covering 170+ OfficeScripts operations across 2,000 publicly available Excel sheets.\nWe evaluate OpenAI\u2019s GPT-3.5 Turbo and GPT4 models (OpenAI, 2023) as well as a T5 model\n3OfficeScripts API documentation: https: //learn.microsoft.com/en-us/javascript/ api/office-scripts/excelscript?view= office-scripts.\n(Raffel et al., 2020), on INSTRUCTEXCEL and find it to be a challenging benchmark even using state of the art tools, indicating significant room for future work on this task. We also find that GPT-4 improves over GPT-3.5, and dynamic prompting and more in-context examples improve performance, but additional in-context instructions do not help.\nIn addition to evaluating capabilities of stateof-the-art models on the full benchmark, we also illustrate the broader utility of INSTRUCTEXCEL through a case study on conditional formatting rules. INSTRUCTEXCEL contains 660 examples that represent conditional formatting tasks, which are studied in isolation (Singh et al., 2022). Viewed through the lens of this particular task, INSTRUCTEXCEL provides a data source that can be further processed to enable experiments with taskspecific ground truth. Many other sub-tasks can be extracted from INSTRUCTEXCEL in the same manner, such as charting/plotting (Lee et al., 2021) or formula creation (Chen et al., 2021b).\nWe believe INSTRUCTEXCEL will encourage development of user-friendly methods to automate various Excel tasks and help non-expert Excel users in their daily workflow."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Instruction Learning Paradigm",
            "text": "The instruction paradigm (Mishra et al., 2022b; Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022) provides a user-friendly option to leverage ML models just by providing instructions without requiring underlying technical knowledge. Instructions describe tasks in NL (Efrat and Levy, 2020; Weller et al., 2020), and are shown to help models generalize to unseen tasks (Mishra et al., 2022b; Wei et al., 2022; Ouyang et al., 2022; Wang et al., 2022b; Xu et al., 2023; Zhong et al., 2021; Gupta et al., 2023; Patel et al., 2022; Puri et al., 2023; Mishra and Nouri, 2022; Chung et al., 2022). Recent developments in large language models (Brown et al., 2020; Chowdhery et al., 2022) have led to the successful use of instruction learning methods in various applications, such as dialog (Gupta et al., 2022), tabular question answering (Luo et al., 2022), relation extraction (Chen et al., 2021a), biomedical applications (Parmar et al., 2022), NER (Wang et al., 2022a), program synthesis (Kuznia et al., 2022), and style transfer (Reif et al., 2022). Natural Instructions (Mishra et al., 2022b), Supernatural Instructions (Wang\net al., 2022c), and Promptsource (Bach et al., 2022) are some popular instruction learning benchmarks, however they are focused on general NLP. In contrast to prior works, we focus on the application of the instruction paradigm in the Excel domain."
        },
        {
            "heading": "2.2 Program Synthesis",
            "text": "The recent success of large language models (LLMs) like Codex (Chen et al., 2021), GPT4 (OpenAI, 2023), PaLM (Chowdhery et al., 2022), PaLM2 (Anil et al., 2023), StarCoder (Li et al., 2023) and WizardCoder (Luo et al., 2023) has led to significant progress in program synthesis. LLMs have been found to generalize to many different code generation tasks. The models vary in size and training schemes and their success rate in performing code generation tasks (Xu et al., 2022). Several works leverage pre-trained models to map NL queries to sequences of API elements or other intermediate representations, which can be translated into code, though this may not be necessary with models that have been pre-trained on code (Hadi et al., 2022; Shin and Van Durme, 2022). They have also been used for in-place data transformations (Narayan et al., 2022). Pre-trained models have proven to be especially strong when combined with clever post-processing steps or constrained decoding, for example in Jigsaw (Jain et al., 2022), Synchromesh (Poesia et al., 2022), and PICARD (Scholak et al., 2021). Chan et al. (2022) investigates the impact of training on a large number of different tasks. Other important aspects studied include length generalization (Anil et al., 2022), compositional generalization (Shi et al., 2022), reverse engineering (Pearce et al., 2022), and generating development tools (Barei\u00df et al., 2022). The task of NL to Code is broadly of interest to the semantic parsing literature (Kamath and Das, 2018; Zhong et al., 2022; Zhao et al., 2022).\nExisting benchmarks for automated code generation include CoNaLa (Yin et al., 2018), DJANGO (Oda et al., 2015), HumanEval (Chen et al., 2021), MBPP and MathQA-Python (Austin et al., 2021), APPS (Hendrycks et al., 2021), and CodeContests (Li et al., 2022). In contrast to these benchmarks that target general-purpose programming languages, our work INSTRUCTEXCEL instead focuses on a specialized Excel API, which LLMs are less likely to have encountered during pre-training."
        },
        {
            "heading": "3 INSTRUCTEXCEL",
            "text": "In this section, we describe the INSTRUCTEXCEL benchmark. We start with the schema used to represent the task, followed by our dataset construction procedure, and an overview of the dataset statistics."
        },
        {
            "heading": "3.1 Task",
            "text": "The user inputs actions \u2013 such as clicking buttons, writing formulas, and inserting columns. These actions correspond to OfficeScript code which produces an equivalent effect on the Excel spreadsheet. In addition, the user describes the intended result of their actions using a natural language description. Our task is to map the natural language description input, along with the contents of the spreadsheet, to the OfficeScript code output."
        },
        {
            "heading": "3.2 Schema",
            "text": "Figure 1 shows the schema of INSTRUCTEXCEL, along with an example. It has 4 elements: input, output, Excel file URL and Excel file description (see Table 5 in Appendix A for more examples).\nInput Each input contains a natural language instruction that specifies the desired manipulation of the Excel spreadsheet, e.g. \u201chighlight the 1st row of sheet 1.\u201d This can also contain references to cells\nand other Excel-specific details associated with the sheet. The input is typically a single line of text provided by the crowdworker.\nOutput The output field contains the OfficeScript code generated when the user records the actions taken to accomplish their task. OfficeScripts are written in TypeScript, a superset of JavaScript.\nEach script must contain a main function with the ExcelScript.Workbook type as its first parameter. When the function runs, the Excel application invokes the main function by providing the workbook as its first parameter.\nExcel File Name Each entry includes a link pointing to the Excel file. Although we do not release the spreadsheets directly as part of the benchmark, each spreadsheet can be downloaded from its associated URL.\nExcel File Description Since some Excel files are very large and may not fit within the limited context size of smaller language models, we have another field where the crowdworker describes the contents of the Excel file in natural language. This is usually one or two lines of text."
        },
        {
            "heading": "3.3 Constructing INSTRUCTEXCEL",
            "text": "Selecting Excel Files Starting from a dataset of 5,000 Excel documents from publicly available links on the web, we removed any files that are in languages other than English or that were passwordprotected, corrupted, or otherwise inaccessible. We also applied a size criteria (20 KB \u0103 filesize \u0103 30 KB) to avoid Excel files that were too small to have meaningful data or too large for human annotators to understand and study in the provided time. We eventually selected a final set of 2,000 Excel documents to be further annotated.\nData Creation Process We used crowdsourcing to record actions over the set of Excel documents. We used a crowdsourcing platform called the Universal Human Relevance System.4 We recruited English speaking annotators from India. We used an intermediate third party vendor which assures quality of the tasks through management of communication and training of the annotators. We requested our vendor to arrange a pool of annotators who have basic familiarity with Excel applications. We also provided the vendor a list of popular Excel\n4UHRS located at: https://prod.uhrs.playmsn. com/UHRS/.\noperations which they must use. We ensured that our third party vendor had access to the Automate feature in Excel Web version and that they could access the Excel files of our dataset through the Excel Web version.\nFor each document the user was instructed to input a natural language description of the file. They were then asked to type a natural language instruction, record themselves performing the relevant action for the instruction, and copy-paste the resulting code generated by the Automate feature in Excel Web. They repeated that process 5 times per spreadsheet. The full data collection process is illustrated in Figure 2. A screenshot of the interface for the human intelligence task (HIT) is provided in Appendix B.\nQualification, Crowdworker Compensation and Data Quality Check We provided 2 rounds of pilot HITs to the vendor\u2019s annotators, reviewed their responses, and made clarifications to the instructions based on the feedback. Specifically, we instructed annotators not to overuse or underuse certain actions after observing the action distribution in pilot rounds. We provided various examples of queries which could be toggled on and off within the interface. We assessed the qualification of the crowdworkers based on the responses to the pilot HITs submitted to us. In addition to the hourly rate of 12 USD for the annotation work, we also covered the vendor\u2019s management and training fees to assure response quality.\nConsent and Privacy Control Annotators had the choice of performing the requested tasks only after signing a consent form. In addition, we provided explicit instructions in the HITs that disallowed sharing of any personal and identifiable information when providing responses."
        },
        {
            "heading": "3.4 Statistics",
            "text": "Table 1 shows key statistics of INSTRUCTEXCEL. The most common words used in the Excel sheet descriptions are \u2018year\u2019, \u2018name\u2019, \u2018form\u2019, \u2018calendar\u2019, \u2018table\u2019, and \u2018content.\u2019 These words represent common Excel workflows. \u2018Formula\u2019, \u2018background\u2019, \u2018color\u2019, \u2018conditional\u2019, and \u2018formatting\u2019 are the most common words in the NL instructions.\nFigure 3 shows the most common methods used in the recorded OfficeScripts. We see a fairly broad distribution over important Excel functionalities. The top methods correspond with the most common words used in the users\u2019 instructions \u2013 for ex-"
        },
        {
            "heading": "EM ROUGE F1 BLEU",
            "text": "ample, use of the word \u2018formula\u2019 corresponds with the method call \u2018setFormulaLocal.\u2019 Many of the top methods require only one parameter. However, some of the methods require fairly complex parameters. The \u2018setFormulaLocal\u2019 method requires formulas input as strings, and \u2018setRule\u2019 requires complicated conditional format rule objects that must be constructed before being passed to the method call. Some common actions performed by users require sequences of multiple method calls; for instance, inserting a shape requires specifying the location, type, and color of the shape.\nWe found 76 queries that were repeated at least once in the benchmark. Most of these queries are general queries such as \u201cfreeze row 1\u201d or \u201csort column G.\u201d We compute the exact match, ROUGE, F1, and SacreBLEU scores between all pairs of code blocks belonging to the same natural language user queries, displayed in Table 2. Although the repeated queries often have only one correct code solution (for example, \u201cfreeze row 1\u201d has only one solution), many queries can be satisfied with multiple code solutions (i.e., \u201cCreate chart on worksheet Sheet1\u201d can be satisfied with any chart type)."
        },
        {
            "heading": "4 Experiment",
            "text": "We conduct experiments with supervised, zero-shot and few-shot learning to assess the performance of popular LLMs on INSTRUCTEXCEL. We use GPT3.5 Turbo and GPT-4 in our experiments (OpenAI, 2023), specifically the \u201cgpt-3.5-turbo-16k\u201d\nand \u201cgpt-4-32k\u201d models.5 We also experiment with finetuning the T5 Large LM Adapt model (Raffel et al., 2020).6\nIn all experiments, a single example consists of a triplet with 1) a natural language query, 2) the data from the spreadsheet converted into a string using a Pandas ExcelFile object, and 3) the ground truth or generated OfficeScripts code. We elaborate below on our full experiment setup."
        },
        {
            "heading": "4.1 Data Splits",
            "text": "We exclude examples from the benchmark that have broken URLs for the purposes of our experiments, as some of the URLs have expired since data collection and annotation occurred.7 We divide the remaining examples into train, dev and test splits containing 4033, 1000 and 1000 instances\n5https://platform.openai.com/docs/ models\n6https://huggingface.co/google/ t5-large-lm-adapt.\n7We intend to continue to collect and provide additional data in our public repository, so some natural URL expiry will not impact the utility of the dataset.\nrespectively. Considering the computational cost and rate limit associated with GPT usage, we also form a test-set subset of 200 samples which we use in our experiments."
        },
        {
            "heading": "4.2 Setup",
            "text": ""
        },
        {
            "heading": "4.2.1 Zero-shot",
            "text": "In this setting we do not provide any examples to the model. Instead, we provide the prompt \u2018Generate a function with excel script to execute the action given below in NL.\u2019 In order to make the task easier, we also add the common boilerplate code which starts every OfficeScript to the prompt as exemplified in Figure 4. The output is limited to 256 tokens, and we impose this limit in all other settings as well."
        },
        {
            "heading": "4.2.2 Few-shot and Max-shot",
            "text": "We use the in-context learning setup (Brown et al., 2020), showing a few examples before prompting the model to respond to the input query. We experiment with two different few-shot learning setups. First, we evaluate models in 3-shot learning (3 ex-\namples are presented), where in-context examples are included after the instructions but before the query. Second, we evaluate models in the Maxshot setup, in which we add 10 examples. Unless otherwise specified, the examples used to prompt the model are randomly chosen and put in random order ahead of time, and we use the same examples for all test instances."
        },
        {
            "heading": "4.2.3 Max-shot+Instruction",
            "text": "Motivated by the success of detailed instructions in improving model performance (Mishra et al., 2022b; Wei et al., 2022; Ouyang et al., 2022; Mishra et al., 2022a), we prompt with a longer NL instruction along with 10 examples. The longer instruction is listed in Appendix C."
        },
        {
            "heading": "4.2.4 Finetuning",
            "text": "We finetune only the T5 model, as finetuning GPT3.5 Turbo and GPT-4 is costly. Details of finetuning and the parameters for T5 inference are listed in Appendix D."
        },
        {
            "heading": "4.3 Data Inclusion",
            "text": "Although we try to include as much data as possible in the prompts, the spreadsheets can often be too large to fit all of them in the prompt. This problem is especially pronounced in the Max-shot setting. We first calculate the number of tokens in the prompt when using all available data for each in-context example and the query. If the number of tokens exceeds the context length for the model, we remove the second half of each data string. We repeat this process until the prompt is small enough to fit in context, or all data has been deleted. If all data has been deleted and the prompt is still too large, we revert to the full data for each example and simply truncate the string of in-context examples to make the prompt fit in context.\nTo understand the impact of data truncation, we analyze the two most space-restricted settings in our experiments, the Max-shot+Instruction settings with both the GPT3.5 model with 16k context size, and the GPT4 model with 32k context size + API\nin the prompt. Both settings always include the intended 10 in-context examples in the prompt, and the data included in the examples retain an average of 19.36 lines (for GPT-3.5) and 15.36 lines (for GPT4+API). This typically guarantees that the headers and some data are included for each of the in-context examples and the query. We hypothesize that increasing context window size will result in performance improvements."
        },
        {
            "heading": "4.4 API in Context",
            "text": "To inform GPT of the API, we experiment with appending the API to every prompt. The API consists of an alphabetically sorted list of all classes, enums, methods, and properties in the OfficeScripts API, along with the accepted parameters for methods and the fields of all enums. We append the API immediately under the instruction in the prompt, before the in-context examples and the query."
        },
        {
            "heading": "4.5 Dynamic Prompting",
            "text": "One other method which can inform GPT of the relevant API elements for a given input is dynamic prompting (Liu et al., 2022). For dynamic prompting, given a natural language input, we search for the most similar natural language inputs in the training set, measured according to F1 score of the normalized text. We then append the top 3 or 10 examples from the training set, including the linearized data from each example. This prompting strategy not only provides in-context examples that are semantically similar to the query, but also increases the likelihood that the correct elements of the API are directly exposed in the context."
        },
        {
            "heading": "4.6 Evaluation",
            "text": "We rely on textual similarity-based evaluation metrics that can easily be reused: Exact Match (EM) comparing the normalized code prediction with the gold standard, F1, ROUGE (Lin, 2004) and SacreBLEU (Post, 2018) to estimate the textual similarity between predicted code and the gold standard. We use the implementations of ROUGE and SacreBLEU from HuggingFace Datasets,8 and we use the built-in stemmer and ROUGE-L metric for ROUGE. We remove the boilerplate code (shown in Figure 4) before evaluation.\nWe also require a metric that measures semantic equivalence of code outputs without penalizing arbitrary choices like variable names or choices for\n8https://huggingface.co/docs/datasets/ index.\narbitrary parameter values. Some parameters and numbers cannot be directly inferred from the natural language query or the spreadsheet data, and are sometimes (but not always) arbitrary choices that do not impact the correctness of the code. Queries such as \u201cinsert shape in sheet December 2021\u201d or \u201cChange the legends position on sheet data in chart\u201d can be correctly satisfied using many different parameter values for the shape type and legend position, while queries such as \u201cApply group to A7:A11 on sheet1\u201d require using the specific parameter values A7:A11. Distinguishing these cases is a nontrivial task, and some use-cases are more parametersensitive than others. In Section 6, we analyze the subset of INSTRUCTEXCEL corresponding to conditional formatting operators, which admit evaluation by execution equivalence. However, evaluating relevance to general NL queries automatically is far more challenging.\nTo circumvent the general question of semantic equivalence, in addition to computing metrics on the original predicted and gold standard code, we also report the value for each metric when ranges, numbers, and parameters are stripped from the examples and replaced with placeholder values. We call this function-based evaluation, since this approach enables us to study functional equivalence of the output programs while ignoring some details.\nWe also annotate all predicted outputs from the three models with the highest performance on automated metrics. For each predicted code output, we give a binary judgment on whether the predicted code output satisfies the request given by the user. To aid in annotation, we perform our annotations while looking at the gold standard outputs, and we consult the spreadsheets when data from the spreadsheet is necessary to judge correctness."
        },
        {
            "heading": "5 Results",
            "text": "Table 3 shows all metrics across various modeling setups, but only for Max-shot prompting. We find that Max-shot outperforms Zero-shot, Fewshot, and Max-shot+Instruction (see full results in Appendix E) across all models, so we only report Max-shot in-context learning and finetuning results in this section. We summarize our main findings below.\nFinetuning Improves over In-context Learning on Automated Metrics Model performance after finetuning is notably higher than model performance in all other settings, when comparing on\nautomated metrics. However, GPT-4+DP outperforms the finetuned T5 model when comparing manual annotation score. The relative performance gain of finetuning over in-context learning is therefore unclear, though both appear effective.\nGPT-4 outperforms GPT-3.5 Turbo For the Few-shot (3), Max-shot and Max-shot+Instruction settings, we observe that the performance of GPT4 is higher than GPT-3.5 Turbo (details in Appendix E).\nMore In-context Examples Help Few-Shot Capabilities Across both GPT models, we observe that the Max-shot model performance is much higher than the Few-shot and Zero-shot performance (see Appendix E for details). This potentially indicates that addition of in-context examples can greatly help LLMs generate OfficeScript code.\nDetailed Instructions Do Not Help The Maxshot+Instruction baseline is not remarkably different from the Max-shot baseline. Considering the sensitivity of model performance to instruction framing (Mishra et al., 2022a; Zhao et al., 2021), it would be beneficial for future work to experiment with other instruction variants.\nAPI in Context Including the API in context improves performance in the Zero-shot case, but not in the Few-shot case. In the Max-shot (shown in Table 3) and Max-shot+Instruction experiments, including the API in context harms overall performance. This is likely because the API has a large number of tokens. Much less data can be included in the prompt with this method, and some in-context examples may need to be removed from the prompt entirely.\nDynamic Prompting Dynamic prompting is incredibly effective, causing at least a 40% relative improvement in all 4 metrics. Despite this performance improvement, the model still often makes mistakes where API elements are hallucinated. Since prompting with the full API directly seems to harm performance, solving the hallucination of API elements under in-context learning is an important area for further study."
        },
        {
            "heading": "5.1 Manual Analysis",
            "text": "Our manual annotation indicates that although the finetuned T5 model outperforms GPT4+DP on the automated metrics, in-context learning with GPT4 slightly outperforms the finetuned T5 model in\nmanual correctness judgments. Thus, although the automated metrics can help with rough comparisons, they do not give a complete picture of model performance.\nWe also perform a 2-sample, one-sided t-test to compare the value of each automated metric for manually-judged incorrect predictions vs. manually-judged correct predictions. This test has the alternative hypothesis that the metric has a lower average when the example is manually determined to be incorrect. We perform these tests for the 3 top-performing models and all metrics. We reject the null hypothesis with a p-value of 0.01 in all cases. These tests indicate a statistically significant difference between the values of automated metrics for correct vs. incorrect predictions, suggesting that automated metrics may serve as a convenient (though incomplete) replacement for metrics based on execution equivalence or manual judgment.\nWe also perform a qualitatitive exploration of remaining error types. About 20% of the annotated examples simply do not compile for the top three scoring models, often because they hallucinate API elements or formulas. Among the examples that compile correctly, we identify three major remaining error types for the finetuned T5 model: misunderstanding the user\u2019s intention, targeting the wrong object or incorrect cell range, and accomplishing the correct task in a different way than intended. Full examples of these failure modes are shown in Figure 5. We also identify major error types for the GPT4+DP model, finding that misunderstanding the users\u2019 intentions/solving the wrong task and targeting the wrong object or cell\nrange are also the most prevalent. Examples of errors made by GPT4+DP are included in Table 8 of Appendix F. In contrast to the T5 model, the GPT4+DP model has more errors where a completely different task is solved than what is queried. This is a potential artifact of in-context learning that should be addressed. Both models suffer from overwriting important data, as shown in the final example of Table 8."
        },
        {
            "heading": "6 Case Study: Formatting Rules in Excel",
            "text": "The INSTRUCTEXCEL benchmark contains a wide range of Excel tasks, including charting/plotting (Lee et al., 2021), formatting (Singh et al., 2022), and formulas (Chen et al., 2021b). We can derive specific benchmarks for these individual downstream tasks from INSTRUCTEXCEL. In this case study, we present an analysis of conditional formatting rules as a downstream task.\nTo focus on the task of learning conditional formatting rules, we extract all examples from the INSTRUCTEXCEL dataset that involve any formatting operator. These extracted samples are then manually validated to ensure their relevance to formatting tasks. We obtain a total of 660 valid CF tasks after filtering.\nTo facilitate benchmarking, we transform the extracted conditional formatting tasks into an Intermediate Language (IL) representation specifically designed for formatting operators (Singh et al., 2022). Figure 6 showcases an example of this IL.\nTo evaluate the performance of various approaches on this task, we employed the original natural language (NL) descriptions associated with\nthe tasks to generate simplified formatting rules. As these rules were already in a parsed format, we report the following evaluation metrics: (1) Exact Match, (2) Sketch Match over the AST of the parsed rules, and (3) Execution Match - Verifying the equivalence of the resulting formatting by executing the generated rules on data and comparing the outcomes (Singh et al., 2022, 2023).\nTable 4 illustrates the performance of various baselines on the task of learning conditional formatting rules. GPT-4 with Max-shot in-context learning achieves the highest performance among the evaluated approaches. Our analysis showcases the feasibility of extracting relevant data, transforming it into an intermediate format, and benchmarking various approaches for the specific task of conditional formatting. These findings serve as a compelling motivation for the broader usage of the INSTRUCTEXCEL dataset in advancing research and development within the Excel domain."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce INSTRUCTEXCEL, a benchmark to investigate the capability of models in generating Excel OfficeScript code from NL instructions. INSTRUCTEXCEL consists of over 10,000 samples covering 170+ Excel operations across 2,000 pub-\nlicly available Excel spreadsheets. Our experimental results show that (1) GPT-4 (2) more incontext examples, and (3) dynamic prompting help improve performance on this benchmark, while more-detailed in-context instructions and including the API description do not help. Finetuning aids performance on automated metrics, though our manual annotation shows that the difference between finetuning and in-context learning is not conclusive. INSTRUCTEXCEL is still a challenging benchmark for state of the art models like GPT4 and T5. We also demonstrate, through a case study on conditional formatting, that INSTRUCTEXCEL can be used as source data for more specific study of important sub-tasks in Excel. We hope that improvements on INSTRUCTEXCEL will enable novice users to perform complicated actions in Excel quickly, boosting productivity and teaching important Excel functionality in the process."
        },
        {
            "heading": "8 Limitations",
            "text": "Our focus in INSTRUCTEXCEL is on instructions written in English. A typical non-expert user workflow often involves languages other than English. The limitation to not include local language in INSTRUCTEXCEL will be addressed in future work. The programming language under study, Excel OfficeScripts, is specific to Microsoft Excel, so our\nresults may not generalize to other spreadsheet software. Our focus in this work is to release a targeted benchmark which, if performance is improved, will enable a core set of NL to code functionality in Microsoft Excel. The potential risks from this work are minimal. Excel OfficeScripts is a language designed to be secure, targeted only at manipulating data inside Excel spreadsheets. Therefore, there is little risk that a model will produce overtly harmful code in this domain."
        },
        {
            "heading": "A Full Schema Example",
            "text": "Table 5 shows multiple input-output examples. The examples include before and after images of the relevant regions of the Excel spreadsheet."
        },
        {
            "heading": "B HIT Screenshot",
            "text": "Figure 7 shows a screenshot of the interface shown to crowdworkers while inputting file descriptions, NL instructions, and copy-pasted code from the Automate feature."
        },
        {
            "heading": "C Longer Instruction",
            "text": "The more detailed instruction used in the Maxshot+Instruction setting is: \u2018Generate a function with excel script to execute the action given below in NL. You also need to generate comment describing the operation you are performing. Make sure to generate a valid excel operation and pass appropriate parameters as provided in the action information. Simple solution is preferred over a complex one.\u2019"
        },
        {
            "heading": "D Finetuning Parameters",
            "text": "T5 training was done for 5, 000 steps, with learning rate increasing linearly from 0 to 1 \u00a8 10\u00b44 for the first 1, 000 steps, and then linearly down to 0 for the remaining steps. We restrict both the input and output size to 1, 000 tokens. If inputs and outputs are denoted as Ii and Oi \u201c ttpiqk u |Oi| k\u201c1 respectively, then the finetuning objective is to maximize the probability of the true output given the input,\nppOi|Iiq \u201c |Oi| \u017a\nk\u201c1 pptpiqk |t piq 1 , . . . t piq k\u00b41, Iiq .\nAt inference time, we use beam search with beam size 5, and select the top 1 completion for final evaluation."
        },
        {
            "heading": "E Full Experimental Results",
            "text": "The full results for the Zero-shot, Few-shot, Maxshot, and Max-shot+Instruction settings on all models are reported in Table 6. In addition to the evaluation metrics reported in Table 6, we also report the same metrics after replacing all parameters, ranges, and numbers in both the gold standard and predicted outputs with placeholder values (functionbased evaluation). These modified metrics are reported in Table 7. Although the metrics are much higher in this setting, the same overall trends hold."
        },
        {
            "heading": "F Failure modes of GPT4+DP",
            "text": "Table 8 shows major error modes for GPT4+DP. These error modes are similar to those seen for T5 in Figure 5."
        }
    ],
    "title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel",
    "year": 2023
}