{
    "abstractText": "Retrieval-augmented methods are successful in the standard scenario where the retrieval space is sufficient; whereas in the few-shot scenario with limited retrieval space, this paper shows it is non-trivial to put them into practice. First, it is impossible to retrieve semantically similar examples by using an off-the-shelf metric and it is crucial to learn a task-specific retrieval metric; Second, our preliminary experiments demonstrate that it is difficult to optimize a plausible metric by minimizing the standard cross-entropy loss. The in-depth analyses quantitatively show minimizing cross-entropy loss suffers from the weak supervision signals and the severe gradient vanishing issue during the optimization. To address these issues, we introduce two novel training objectives, namely EML and R-L, which provide more task-specific guidance to the retrieval metric by the EM algorithm and a ranking-based loss, respectively. Extensive experiments on 10 datasets prove the superiority of the proposed retrieval augmented methods on the performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guoxin Yu"
        },
        {
            "affiliations": [],
            "name": "Lemao Liu"
        },
        {
            "affiliations": [],
            "name": "Haiyun Jiang"
        },
        {
            "affiliations": [],
            "name": "Shuming Shi"
        },
        {
            "affiliations": [],
            "name": "Xiang Ao"
        },
        {
            "affiliations": [],
            "name": "Peng Cheng"
        }
    ],
    "id": "SP:509012e684e8724ce4adaaf690a54ee4e3ed1c44",
    "references": [
        {
            "authors": [
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Jordan Hoffmann",
                "Trevor Cai",
                "Eliza Rutherford",
                "Katie Millican",
                "George Bm Van Den Driessche",
                "Jean-Baptiste Lespiau",
                "Bogdan Damoc",
                "Aidan Clark"
            ],
            "title": "Improving language models by retrieving from tril",
            "year": 2022
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326.",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Chris Burges",
                "Tal Shaked",
                "Erin Renshaw",
                "Ari Lazier",
                "Matt Deeds",
                "Nicole Hamilton",
                "Greg Hullender."
            ],
            "title": "Learning to rank using gradient descent",
            "venue": "Proceedings of the 22nd international conference on Machine learning, pages 89\u201396.",
            "year": 2005
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Huayang Li",
                "Wai Lam",
                "Lemao Liu."
            ],
            "title": "Neural machine translation with monolingual translation memory",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Huajun Chen"
            ],
            "title": "2022a. Decoupling knowledge",
            "year": 2022
        },
        {
            "authors": [
                "Huajun Chen"
            ],
            "title": "2022b. Knowprompt: Knowledge",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan May"
            ],
            "title": "WARP: Word-level Adversarial",
            "year": 2021
        },
        {
            "authors": [
                "Hamed Zamani"
            ],
            "title": "Fid-light: Efficient and effec",
            "year": 2023
        },
        {
            "authors": [
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Huayang Li",
                "Yixuan Su",
                "Deng Cai",
                "Yan Wang",
                "Lemao Liu."
            ],
            "title": "A survey on retrieval-augmented text generation",
            "venue": "arXiv preprint arXiv:2202.01110.",
            "year": 2022
        },
        {
            "authors": [
                "Shiyang Li",
                "Yifan Gao",
                "Haoming Jiang",
                "Qingyu Yin",
                "Zheng Li",
                "Xifeng Yan",
                "Chao Zhang",
                "Bing Yin."
            ],
            "title": "Graph reasoning for question answering with triplet retrieval",
            "venue": "arXiv preprint arXiv:2305.18742.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoya Li",
                "Yuxian Meng",
                "Mingxin Zhou",
                "Qinghong Han",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "Sac: Accelerating and structuring self-attention via sparse adaptive connection",
            "venue": "Advances in Neural Information Processing Systems, 33:16997\u201317008.",
            "year": 2020
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Yanan Zheng",
                "Zhengxiao Du",
                "Ming Ding",
                "Yujie Qian",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Gpt understands, too",
            "venue": "arXiv preprint arXiv:2103.10385.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Suresh Manandhar."
            ],
            "title": "Semeval-2014 task 4: Aspect based sentiment analysis",
            "venue": "Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014), pages 27\u201335.",
            "year": 2014
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "arXiv preprint cs/0409058.",
            "year": 2004
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Ruiyang Ren",
                "Yingqi Qu",
                "Jing Liu",
                "Wayne Xin Zhao",
                "QiaoQiao She",
                "Hua Wu",
                "Haifeng Wang",
                "Ji-Rong Wen."
            ],
            "title": "RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking",
            "venue": "Proceedings of the 2021 Conference on Empiri-",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Cynthia Rudin",
                "Robert E Schapire"
            ],
            "title": "Marginbased ranking and an equivalence between adaboost and rankboost",
            "year": 2009
        },
        {
            "authors": [
                "Timo Schick",
                "Helmut Schmid",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Automatically identifying words that can serve as labels for few-shot text classification",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5569\u20135578, Barcelona,",
            "year": 2020
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Ronald Seoh",
                "Ian Birle",
                "Mrinal Tak",
                "Haw-Shiuan Chang",
                "Brian Pinette",
                "Alfred Hough."
            ],
            "title": "Open aspect target sentiment classification with natural language prompts",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Vered Shwartz",
                "Peter West",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Unsupervised commonsense question answering with self-talk",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Devendra Singh",
                "Siva Reddy",
                "Will Hamilton",
                "Chris Dyer",
                "Dani Yogatama."
            ],
            "title": "End-to-end training of multi-document reader and retriever for opendomain question answering",
            "venue": "Advances in Neural Information Processing Systems, 34:25968\u201325981.",
            "year": 2021
        },
        {
            "authors": [
                "Shamane Siriwardhana",
                "Rivindu Weerasekera",
                "Elliott Wen",
                "Tharindu Kaluarachchi",
                "Rajib Rana",
                "Suranga Nanayakkara."
            ],
            "title": "Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering",
            "venue": "Trans-",
            "year": 2023
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Karen Sparck Jones."
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "Journal of documentation, 28(1):11\u201321.",
            "year": 1972
        },
        {
            "authors": [
                "David Thulke",
                "Nico Daheim",
                "Christian Dugast",
                "Hermann Ney."
            ],
            "title": "Efficient retrieval augmented generation from unstructured knowledge for taskoriented dialog",
            "venue": "arXiv preprint arXiv:2102.04643.",
            "year": 2021
        },
        {
            "authors": [
                "Ellen M Voorhees",
                "Dawn M Tice."
            ],
            "title": "Building a question answering test collection",
            "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207.",
            "year": 2000
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461.",
            "year": 2018
        },
        {
            "authors": [
                "Chengyu Wang",
                "Jianing Wang",
                "Minghui Qiu",
                "Jun Huang",
                "Ming Gao."
            ],
            "title": "TransPrompt: Towards an automatic transferable prompting framework for few-shot text classification",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Shuohang Wang",
                "Yichong Xu",
                "Yuwei Fang",
                "Yang Liu",
                "Siqi Sun",
                "Ruochen Xu",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426.",
            "year": 2017
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Jingyi Zhang",
                "Masao Utiyama",
                "Eiichro Sumita",
                "Graham Neubig",
                "Satoshi Nakamura."
            ],
            "title": "Guiding neural machine translation with retrieved translation pieces",
            "venue": "arXiv preprint arXiv:1804.02559.",
            "year": 2018
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Yang Deng",
                "Xin Li",
                "Yifei Yuan",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Aspect sentiment quad prediction as paraphrase generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9209\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zhilin",
                "Dai Zihang",
                "Yang Yiming",
                "Carbonell Jaime",
                "Salakhutdinov Ruslan",
                "Quoc V. Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "The 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).",
            "year": 2019
        },
        {
            "authors": [
                "Gao"
            ],
            "title": "The specific templates are shown in Table 6",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Few-shot text classification, which entails learning a new task based on limited training data, has been advanced by pre-trained language models (PLMs) (Brown et al., 2020; Liu et al., 2023) and prompt engineering (Gao et al., 2021; Chen et al., 2022a). However, since training numerous parameters of PLMs on scarce data is prone to produce over-fitting (Liu et al., 2021) and unstable generalization, only using the trained parameters for inference usually leads to unsatisfactory performance on unseen test data.\n*Work done while this author was an intern at Tencent. \u2020Corresponding authors.\nOn the other hand, retrieval-based methods have witnessed success on various natural language processing tasks, thanks to their capability of incorporating retrieved memory alongside parameters for better generalization. These methods retrieve relevant examples as memories from a large-scale corpus through either a static retrieval metric (Lewis et al., 2020; Wang et al., 2022) or a joint learningbased metric (Cai et al., 2021; Siriwardhana et al., 2023) and then the retrieved examples are used to make a prediction. In this way, their generalization ability is achieved by not only the model parameters but also the retrieved memory.\nDespite the theoretical potential of promoting generalization by using retrieved memory, previous retrieval-augmented methods empirically struggle to showcase compelling ability in few-shot learning scenarios, where the retrieval space (i.e., the fewshot training data) is limited. Specifically, static retrieval may lack neighbors with high metrics in the case of limited retrieval space. Even though such neighbors exist, static retrieval cannot be reliable for retrieving really helpful samples for target prediction, because its metric is not task-specific. In particular, for joint learning-based retrieval which minimizes the standard cross-entropy based loss, although the retrieval metric is updated towards the downstream task, it suffers from the gradient vanishing problem during the optimization process as quantitatively measured in Fig. 2 (see \u00a75.2 later). As a result, in a few-shot scenario, the retrieval metric might be not optimized well due to insufficient training data.\nTo overcome the aforementioned challenges, we propose two novel training objectives, namely Expectation Maximization-based Loss (EM-L) and Ranking-based Loss (R-L), for learning to retrieve\nexamples from a limited space more effectively. Both objectives are committed to obviating the gradient vanishing problem and prioritizing more beneficial examples for specific downstream tasks. In the EM-L approach, the retrieved examples are treated as latent variables, and an iterative process of Expectation-step and Maximization-step is employed until convergence (Dempster et al., 1977). The posterior distribution of the latent variable is estimated to measure the importance of candidate examples in the E-step, while the M-step maximizes the expectation log-likelihood. By approximating the retrieval metric according to the posterior probability, more productive examples could be recalled for downstream tasks with limited training data.\nFollowing a similar idea, R-L optimizes an additional ranking loss function to provide more direct supervision to the examples retriever, which draws inspiration from pair-wise ranking algorithm (Freund and Schapire, 1997; Burges et al., 2005; Rudin and Schapire, 2009). Such a tailored loss measures the consistency between the retrieval metric and the auxiliary function associated with each example for classification purposes. Minimizing the loss could effectively strengthen the supervision signals for the example retriever.\nOur experimental evaluation on ten text classification datasets demonstrates the superiority of EML and R-L over existing retrieval methods within a limited retrieval space. The comparative analyses further confirm that EM-L and R-L alleviate the weak supervisory signals and gradient vanishing issue suffered by joint learning-based retrieval. Our contributions could be summarized as follows:\n\u2022 We discuss the weak supervision signals and gradient vanishing problem encountered by existing retrieval methods minimizing the standard crossentropy loss, as quantitatively measured in \u00a75.2.\n\u2022 We introduce two novel training objectives, namely EM-L and R-L, which optimize the retriever more effectively, thus recalling more productive examples from a limited space.\n\u2022 Extensive experiments and analyses demonstrate that the proposed methods achieve better performance on few-shot text classification and alleviate the supervision insufficiency and gradient vanishing issues."
        },
        {
            "heading": "2 Revisiting Retrieval-augmented Methods in Few-shot Learning",
            "text": ""
        },
        {
            "heading": "2.1 Retrieval-augmented Methods",
            "text": "In this paper, we revisit the retrieval-augmented methods in few-shot text classification and formulate the task in a general framework. Our primary objective is to retrieve examples from limited training data to improve the few-shot text classification.\nModel Formulation All retrieval methods could comprise an example retriever and a text classifier. We provide the formal formulation inspired by Singh et al. (2021) and Izacard et al. (2022):\nP\u03b8,\u03d5(y|x) = m\u2211 j=1 P\u03b8(y|x, zj)P\u03d5(zj |x),\nP\u03b8(y|x, zj) = softmax(fclf(x\u2295 zj)), P\u03d5(zj |x) = fretr(x, zj),\n(1)\nwhere x and zj denote the representations of original input and a retrieved example from the training set, and y corresponds to the class associated with input x. fclf and fretr serve as the text classifier and the example retriever, which selects examples according to a retrieval metric. \u03b8 and \u03d5 denote the trainable parameters of the text classifier and examples retriever. m is a hyperparameter that denotes the number of fetched examples. The operation \u2295 signifies concatenation, and the term softmax refers to the normalized exponential function. Specifically, z corresponds to a set of retrieval examples, which can either be {\u27e8xs, ys\u27e9} pairs or {xs}. The latter form is adopted in this paper for simple experiments.\nThe standard cross entropy is employed to optimize the classifier and example retriever as follows:\nL = \u2212 n\u2211\ni=1\nlogP\u03b8,\u03d5(yi|xi), (2)\nwhere n is the total number of training instances and yi is the gold label of the i-th instance. During inference, for all retrieval methods, we select top m examples according to P\u03d5(zj |x) and get the final classification results using the first line of Eq. (1).\nStatic Retrieval Given an input sentence x and a retrieval corpus, static retrieval aims to search for a set of relevant examples Z according to a fixed retrieval metric (Borgeaud et al., 2022; Wang et al., 2022; Li et al., 2022). Following the Eq. (1), its\nretrieval metric is defined as follows:\nP\u03d5(zj |x) = fretr(x, zj) = sim(x, zj). (3)\nHere, sim(x, zj) represents a fixed metric without any trainable parameters, such as TFIDF (Sparck Jones, 1972), BM25 (Robertson et al., 2009), and semantic similarity encoded by PLMs. Such fixed metrics cannot adapt to the downstream task and prioritize the most helpful examples. Particularly, this limitation will be amplified in fewshot learning with scarce training data.\nJoint Learning based Retrieval Static retrieval assumes that higher similarity between zj and x implies a greater auxiliary effect of zj on x. However, the assumption failed to hold in tasks where inputs with high similarity have distinct labels, such as sentiment classification. To address this limitation, joint learning-based retrieval (Cai et al., 2021; Gao et al., 2022; Siriwardhana et al., 2023) unifies the retriever and the downstream model to jointly train them for specific tasks. Following Eq. (1),\nP\u03d5(zj |x) = fretr(x, zj) = exp(x \u00b7 z\u22a4j )\u2211m j=1 exp(x \u00b7 z\u22a4j ) .\n(4)\nfretr(x, zj) is a trainable dot product attention. Notably, the absence of ground truth for P\u03d5(zj |x) makes it challenging to determine which zj is the most beneficial one, and it relies implicitly on distant supervision from text classification.\nBoth static retrieval and joint learning-based retrieval are proposed to retrieve examples from a large-scale corpus. In this paper, we mainly focus on few-shot text classification and retrieve the most helpful examples from the limited training set."
        },
        {
            "heading": "2.2 Challenges in Few-shot Learning",
            "text": "While the above retrieval-augmented methods have shown advancements in various natural language processing tasks, their performance in few-shot learning remains unconvincing. In other words, retrieving examples from a narrow space to improve few-shot learning is still challenging due to limited training data. Previous studies (Li et al., 2022; Siriwardhana et al., 2023) have revealed that static retrieval may not fetch the most helpful examples in tasks where similar inputs correspond to different labels, primarily due to their unreasonable assumption that higher similarity implies better suitability for the downstream task. Moreover, we also find\nstatic retrieval even underperforms methods without retrieval in some few-shot tasks (see Table 1). Such failure can also be attributed to data limitation in few-shot scenarios, where examples with high static similarities are scarce or non-existent.\nIn addition, joint learning-based retrieval methods (Ren et al., 2021; Cai et al., 2021; Siriwardhana et al., 2023) are good solutions to enhance the adaptability of the retrieval to downstream tasks. However, our study demonstrates that learnable metrics struggle to be trained as anticipated and are inferior to static metrics in several few-shot tasks (see Table 1). The main underlying factors are the scarcity of data and the weak supervision signals provided to the learnable retrieval metric. In more detail, the retrieval metrics in joint learningbased methods are adjusted solely based on distant supervision from the downstream tasks, which is significantly further weakened by the limited data. This fact is further supported by quantifying the gradient of retrieval parameters: the gradient norm of the parameters in retrieval metric is more than 1e\u22126 for only about 40% updates in some datasets as shown in Figure 2 (see \u00a75.2 later).\nIn this paper, our objective is to meet the challenges of weak supervision signals for the retriever and insufficient data, aiming to retrieve the most helpful examples to promote model generalization."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "Given the limitations posed by limited data and weak supervision signals, existing retrieval methods are inadequate for addressing these challenges. To address these limitations, we propose two novel training objectives, which are achieved by two loss functions: Expectation Maximization-based Loss (EM-L) and Ranking-based Loss (R-L). Both methods aim to enhance the retrieval quality by giving the retriever more supervisory signals and prioritizing examples that are more beneficial for the specific task with limited training data. In essence, we seek to maximize the consistency between the metric distribution P (zj |x) and the classification distribution P (y|x, zj)[yi] as much as possible. In this way, more suitable examples are retrieved and the performance of text classification could be improved even in the few-shot scenario. Additionally, we integrated EM-L, R-L, and two existing retrieval methods with two popular text classification backbones to compare their respective performance."
        },
        {
            "heading": "3.2 Backbone",
            "text": "Fine-tune Pre-trained Language Models For each sentence, we use PLMs to tokenize the input sentence into {[CLS], x1, ..., xl, [SEP]} with (l + 2) tokens and extract the representation x of [CLS] as the sentence embedding. In the same way, the j-th retrieved example is represented as zj . These tensors are subsequently fed into the example retriever and classifier, producing the final probability estimated for label y.\nPrompt Learning Another backbone is to transform the text classification into a cloze question problem (Schick and Sch\u00fctze, 2021). Let M be a masked language model with vocabulary V , and Y denote the label set of a specific downstream task A. Prompt learning employs a function P to convert an input sentence into a phrase containing a prompt with a [MASK] token. Then an injective function v : L \u2192 V is utilized to map each label to a word from M\u2019s vocabulary V . We first obtain the representation of [MASK] and determine the most suitable word from V for filling the [MASK]. For instance, the application of prompt learning to sentiment classification can be outlined as follows:\nP(x) = {[CLS], x1, ..., xl, it was [MASK], [SEP]} P (y|x) = g(P ([MASK] = v(y)|x)), v(y) \u2208 {great, terrible}, (5) where x is the representation of [MASK], g converts the probability of label words to classes, and l is sentence length. The representation zj of a retrieved example is yielded from a [MASK] token in the same way."
        },
        {
            "heading": "3.3 Expectation Maximization-based Loss (EM-L)",
            "text": "Considering the absence of the ground truth for P\u03d5(zj |x) in Eq. (1), we regard z as a latent variable and propose an EM-based retrieval objective to estimate P\u03d5(zj |x). This method alternates between an Expectation-step and a Maximization-step until convergence. In the E-step, the current parameters are used to estimate the posterior distribution of the latent variable given the observed data. Specifically, we retrieve m examples from the training set and compute the conditional probabilities of the latent variable using:\nP\u03b8,\u03d5(zj |x, y) = P\u03b8(y|x, zj)P\u03d5(zj |x)\u2211m j=1 P\u03b8(y|x, zj)P\u03d5(zj |x) ,\n(6)\nwhere P\u03b8(y|x, zj) and P\u03d5(zj |x) are obtained from classifier fclf and examples retriever fretr in Eq. (1) respectively. m denotes the number of retrieved examples.\nIn the M-step, the parameters are updated by maximizing the expected log-likelihood, which is taken with respect to the estimated posterior P\u03b8,\u03d5(zj |x, y) in the E-step:\nP\u03b8,\u03d5(y|x) = m\u2211 j=1 P\u03b8,\u03d5(zj |x, y) \u00b7 logP\u03b8(y|x, zj).\n(7) Since we sample m examples from the training set by P\u03d5(zj |x) and estimate P\u03b8,\u03d5(zj |x, y) based on m examples in the E-step, more supervision will be provided to the retriever during the optimization in the M-step. Please refer to Appendix A for proof of rationality of Eq.(6) and why EM-L can minimize the likelihood-based loss defined in Eq. (2)."
        },
        {
            "heading": "3.4 Ranking-based Loss (R-L)",
            "text": "Following the main idea claimed in \u00a7 3.1, Rankingbased Loss (R-L) considers the process of retrieving zj as a ranking task. Unlike EM-L, R-L employs a ranking loss to enhance the consistency between P\u03b8(y|x, zj)[yi] and P\u03d5(zj |x) and provide more direct signals to the retriever. The optimization objective of R-L aims to ensure that zj with higher P\u03b8(y|x, zj)[yi] has higher P\u03d5(zj |x) by minimizing the following LR:\nLR = n\u2211 i m\u2211 j max(P\u03b8(y|xi, zj)[yi]\n\u2212 P\u03d5(zj |xi) + \u03b4, 0).\n(8)\nHere, P\u03b8(y|x, zj) and P\u03d5(zj |x) are obtained from fclf and fretr in Eq. (1), m and n denote the number of retrieved examples and training instances. \u03b4 is a margin parameter imposing the distance between two distributions to be larger than \u03b4.\nThe ranking loss LR is added to the overall loss L in Eq. (2) with a weight \u03bb every t step:\nLsum = L+ \u03bb \u00b7 LR,\n\u03bb = { 1, step mod t = 0; 0, otherwise;\n(9)\nwhere \u03bb > 0 is a hyperparameter to trade off both loss terms, and step denotes the training steps."
        },
        {
            "heading": "4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets We compared the proposed EM-L and R-L approaches with existing retrieval methods by conducting experiments on 10 widely used text classification datasets, including single-sentence classification, sentence pair classification, and aspect-based sentiment classification. We created few-shot datasets following Gao et al. (2021). For more details, please refer to Appendix B.\nBaselines To prove the effectiveness of retrieving examples from the training set, we develop a baseline method without retrieval for comparison. It comprises an input encoder described in \u00a7 3.2 and a feed-forward neural network for classification. For comparing different retrieval methods, we evaluated our EM-L and R-L against static retrieval and joint learning-based retrieval. We combine them with two widely used backbones for text classification: pre-trained language models fine-tuning and prompt learning. Please refer to Appendix C for more implementations, such as hyper-parameters and templates in prompt learning.\nEvaluation. We evaluate all the retrieval methods using two metrics: Accuracy and Kendall\u2019s \u03c4 . Accuracy represents the proportion of correctly classified instances out of the total number of instances. Kendall\u2019s \u03c4 is employed to measure the consistency and correlations between the retrieval metric P\u03d5(z|xi) and its auxiliary P\u03d5(y|xi, z)[yi]\nfor classification. Kendall\u2019s \u03c4 is defined as follows:\n\u03c4i =\n2\nm(m\u2212 1) m\u2211 j<k sign(uj \u2212 uk) \u00b7 sign(vj \u2212 vk),\nu \u223c P\u03d5(z|xi), v \u223c P\u03d5(y|xi, z)[yi], \u03c4i \u2208 [\u22121, 1], (10)\nwhere sign(\u00b7) \u2208 {\u22121, 0, 1} is a sign function. A ranking pair \u27e8j, k\u27e9 is concordant if their ranks have the same order in P\u03d5(z|xi) and P\u03d5(y|xi, z)[yi]. Consequently, a positive \u03c4i indicates a positive correlation between two distributions, and vice versa. For n instances xi in the training set, we calculate the proportion of xi with \u03c4i > 0 as follows:\n\u03c4 \u2032 =\n\u2211n i step(\u03c4i)\nn ,\nstep(\u03c4i) = { 0, \u03c4i \u2264 0 1, \u03c4i > 0 .\n(11)\nThe reported Kendall\u2019s \u03c4 \u2032 in the following experiment is actually \u03c4 \u2032, which represents the proportion of instances with \u03c4i > 0."
        },
        {
            "heading": "4.2 Main Results",
            "text": "The experimental results for 16-shot setting on 10 datasets are reported in Table 1, where different retrieval-based methods are combined with two backbones. Several insightful observations could be drawn from the results.\nRetrieving examples from the training set is effective in few-shot scenarios. Firstly, in most datasets, retrieval-augmented models outperform the vanilla\nmodel with two backbones, indicating that retrieving examples from the training set could enhance the generalization, even with a narrow search scope. Secondly, the joint learning-based retrieval, EML, and R-L perform better than the static retrieval, which is even less effective than the vanilla model. We hold that this is because static retrieval fetches some examples with high semantic similarities but is detrimental to the downstream tasks. In contrast, the learnable retrieval methods, i.e. joint learningbased retrieval, EM-L, and R-L, are more likely to align with the goals of specific tasks.\nEM-L and R-L approaches train the retriever more effectively than static retrieval and joint learning-based retrieval. At first, our proposed EM-L and R-L achieve significantly higher accuracy across different backbones, proving their effectiveness in fetching helpful examples and adapting to specific downstream tasks. Furthermore, on average, R-L outperforms EM-L, potentially due to its utilization of a more direct ranking loss that provides more significant signals and flexible guidance to the example retriever. Finally, it is worth noting that EM-L and R-L show smaller standard deviations on most datasets than other methods, we conjecture that the proposed training objectives enhance the stability of generalization by incorporating retrieval memory alongside parameters.\nThe advantages of EM-L and R-L are more pronounced on challenging tasks, such as sentence pair classification, and aspect-based sentiment analysis. In this regard, EM-L and R-L achieve improvements of more than 0.3 on most datasets for sentence pair classification and ABSA, whereas the improvement on the single-sentence classification ranges from 0.1 to 0.2, which gain further highlights the effectiveness of EM-L and R-L."
        },
        {
            "heading": "4.3 Consistency Experiments",
            "text": "The Kendall\u2019s \u03c4 \u2032 defined in Eq. (11) on selected datasets are reported in Table 2, which measures the consistency between retrieval metrics of fetched examples and their auxiliaries to downstream tasks. Combing the results in Table 1, higher \u03c4 \u2032 of EM-L\nMR TREC RES LAP\nAccuracy\nand R-L indicates that they could prioritize more helpful examples according to their corresponding metrics and improve the performance by training more effective retrievers. However, retrieving examples according to static metrics and joint learning-based metrics may result in the inclusion of harmful examples in the final performance."
        },
        {
            "heading": "4.4 Auxiliary Experiment",
            "text": "We further conduct additional experiments in both 8-shot and full supervision settings to investigate the advantages of EM-L and R-L on different data scales. The results are presented in Table 3 and Table 4, respectively. It is obvious that EM-L and R-L consistently exhibit excellence in both settings. Particularly, we note a more significant improvement of our methods in the 8-shot setting, which manifests that the proposed training methods train the retriever more effectively, especially when the training data is scarce.\nMoreover, another interesting phenomenon emerged: although EM-L and R-L achieve higher Kendall\u2019s \u03c4 \u2032 in the full supervision setting, their improvements in text classification are comparatively\nsmaller compared to that in few-shot scenarios. We believe this can be attributed to the fact that the classifier in the full supervision setting is already well-trained so the potential improvement from a better retrieval memory is relatively limited."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Effects of the Number of Retrieved Examples",
            "text": "To examine the effects of the number m on various retriever training methods, we present line charts in Fig. 1 that depict the relationship between Accuracy and m. First, all the charts demonstrate retrieving examples could enhance the performance of few-shot text classification, except for a slightly lower accuracy of static retrieval and joint learningbased retrieval when m takes specific values. This could be attributed to the instability of their training process. Second, most methods achieve their peak performance at m = 5 or m = 10. As m continues to increase, the performance may start to deteriorate. We guess the reason is that retrieving too many examples increases the training difficulty. Third, we observe EM-L and R-L maintain sustaining advantages and stability as m varies, which verifies their stronger supervision signals. Another observation is that the joint learning-based method falls behind the static method on LAP. This finding suggests that in certain tasks, a poorly trained learnable metric even exhibits inferior performance compared to a static metric."
        },
        {
            "heading": "5.2 Gradient Updates",
            "text": "In order to assess the supervision signals exerted on the retrievers by different methods, we quantify the average gradients of all retrievers\u2019 parameters. This measurement allows us to evaluate the guidance provided by each method to the retriever during the training process. Fig. 2 illustrates the percentage of training steps where the average gradients of all retrievers\u2019 parameters exceed the threshold of 1e\u2212 6. For clarity, we exclude static retrieval from this figure since its retriever has no trainable parameters1. Our analysis revealed that on certain datasets, the gradient norm of the joint learning-based retriever exceeds the threshold of 1e\u2212 6 for only about 40% of the steps, whereas EM-L and R-L surpass this threshold in over 60% of the steps. This observation suggests that both static and joint learning-\n1This corresponds to a constant proportion of zero for steps with a gradient norm exceeding 1e-6.\nbased retrieval provide weaker supervision signals to the retrievers and suffer from severe vanishing issues in few-shot text classification while EM-L and R-L alleviate such limitations."
        },
        {
            "heading": "5.3 Case Study",
            "text": "Finally, we present an illustrative example from the LAP dataset along with the retrieved examples using different methods in Fig. 3. In the input sentence, the aspect term \u201cstartup times\u201d is negative. Although static retrieval fetches a semantic similar example, it includes information that could potentially mislead the sentiment prediction, such as the term \"spectacular\". The joint learning-based retrieval retrieves an example that seems unrelated to the input sentence, possibly indicating that weak supervision signals for the retriever are prone to worse retrieval results. In contrast, our EM-L and R-L methods are capable of retrieving examples that may not possess high semantic similarity but are more beneficial for sentiment prediction."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Retrieval-augmented Methods",
            "text": "Retrieval-augmented methods enhance the ability of the Pre-trained Language Models in processing various natural language tasks by fetching relevant examples from the training set or external knowledge base and prepending them with the original input. These methods have improved the performance of a lot of tasks, such as neural machine translation (Zhang et al., 2018; Cai et al., 2021; Li et al., 2022; Wang et al., 2022), question answering (Li et al., 2020; Karpukhin et al., 2020; Singh et al., 2021; Wang et al., 2022; Siriwardhana et al., 2023; Li et al., 2023; Hofst\u00e4tter et al., 2023), dialog generation (Fan et al., 2021; Thulke et al., 2021; King and Flanigan, 2023), text classification (Izacard et al., 2022; Lewis et al., 2020), keyphrase generation (Gao et al., 2022), etc. According to retrieval metrics, these methods could be categorized as static retrieval methods and joint learning-based methods, which use a fixed retrieval metric and jointly learnable metric respectively.\nDifferent from the above methods, which fetch relevant examples from the large-scale corpus, we propose two novel training objectives to retrieve examples in a restricted retrieval space and analyze their advantages. Following Singh et al. (2021); Izacard et al. (2022), we formulate the retrievalaugmented methods into a retriever and a classifier\nin Eq. (1) for a fair comparison."
        },
        {
            "heading": "6.2 Prompt Engineering",
            "text": "Fueled by the birth of large-scale language models (Brown et al., 2020), prompt-based learning (Liu et al., 2023) for the Pre-trained Language Models has been developed to convert different downstream tasks into cloze-style mask language model objectives, achieving impressive performance in text classification (Wang et al., 2021; Gao et al., 2021; Hambardzumyan et al., 2021; Lester et al., 2021; Schick et al., 2020; Schick and Sch\u00fctze, 2021), sentiment classification (Seoh et al., 2021; Yan et al., 2021; Chen and Qian, 2020; Zhang et al., 2021), named entity recognition (Cui et al., 2021), relation extraction (Chen et al., 2022b,b), question answering (Lewis et al., 2019; Khashabi et al., 2020), commonsense reasoning (Shwartz et al., 2020), etc. Orthogonal to these studies of prompt learning, our paper focuses on the comparison of different retrieval methods, where prompt learning is just employed as a backbone."
        },
        {
            "heading": "6.3 Few-shot Text Classification",
            "text": "Few-shot Text Classification trains a classifier with limited data for each class, which can also predict unseen classes. Existing studies for few-shot text classification encompass various approaches such as prototypical networks (Jake et al., 2017), XLNetbased methods (Zhilin et al., 2019), (Ro)BERT(a)based methods (Chen et al., 2020, 2022a), Patternexploiting training (Schick and Sch\u00fctze, 2021), prompt tuning (Lester et al., 2021; Gao et al., 2021), etc. And common sub-tasks in text classification consist of intention classification, topic classification, sentiment classification, etc. We evaluate our methods on different text classification tasks, with a focus on adapting the idea of retrieval-augmented methods to the few-shot scenarios through the design of new training objectives."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper studies the retrieval-augmented methods for few-shot text classification and demonstrates the challenges which hinder their success: it is impossible to retrieve semantically similar examples by using an off-the-shelf metric and it is difficult to optimize a plausible metric by minimizing the standard cross-entropy loss. Accordingly, it proposes two novel training objectives, EM-L and R-L, which provide stronger supervision signals to train the retrieval metric effectively in few-shot scenarios. It is worth mentioning that the idea of searching within limited examples bears similarity to the concept of demonstration selection in recent large language models (LLMs). Exploring the application of our methods in LLMs holds promise for future research.\nLimitations\nThere are three primary limitations of our methods. Firstly, EM-L and R-L require additional training time compared to existing retrieval methods. It is due to the alternation between the E-step and M-step in EM-L and the optimization of an additional loss of R-L. Specifically, the training time for EM-L per epoch is approximately 1.5 times that of static retrieval and 1.2 times that of joint learningbased retrieval. Similarly, the training time for R-L per epoch is about 1.8 times that of static retrieval and 1.5 times that of joint learning-based retrieval. Although our proposed methods require more time, they still fall within the acceptable range. Secondly, we didn\u2019t focus on designing more sophisticated templates for prompt engineering, as our main emphasis was on exploring different retrieval methods. Thirdly, we evaluate our methods in few-shot settings constructed from widely used datasets, rather than real-world scenes. This could limit the generalizability of our findings to practical applications."
        },
        {
            "heading": "Acknowledgements",
            "text": "The research work is supported by the National Key R&D Plan No. 2022YFC3303303, the National Natural Science Foundation of China under Grant (No.61976204). This study is also supported by grants from the Major Key Project of PCL (Grant Number: PCL2022D01) and the CAAI Huawei MindSpore Open Fund. Xiang Ao is also supported by the Project of Youth Innovation Pro-\nmotion Association CAS, Beijing Nova Program Z201100006820062."
        },
        {
            "heading": "A Proof of the EM-L Method",
            "text": "Proposition. Optimizing the following two likelihood functions is equivalent in EM-L:\nmax \u03b8,\u03d5 n\u220f i P\u03b8,\u03d5(y|xi) \u21d0\u21d2\nmax \u03b8,\u03d5 n\u2211 i m\u2211 j P\u03b8,\u03d5(zj |xi, y) logP\u03b8(y|xi, zj),\nwhere P\u03b8,\u03d5(zj |xi, y)\n:= P\u03b8(y|xi, zj)P\u03d5(zj |xi)\u2211m j P\u03b8(y|xi, zj)P\u03d5(zj |xi) ,\n(12)\nwhere xi is the representation of the i-th sentence. For each xi, the retriever fetches m examples from the corpus to assist xi in text classification, where each example is represented as zj .\nProof. We first use variational inference to derive the lower bound of the original likelihood:\nmax n\u220f i P\u03b8,\u03d5(y|xi)\n\u21d0\u21d2 max log n\u220f i P\u03b8,\u03d5(y|xi)\n=max n\u2211 i logP\u03b8,\u03d5(y|xi)\n=max n\u2211 i log m\u2211 j P\u03b8(y|xi, zj)P\u03d5(zj |xi)\n=max n\u2211 i log m\u2211 j P\u03d5,\u03b8(y, zj |xi)\n(13)\nLet Q(zj) be a random distribution of zj :\nmax n\u2211 i log m\u2211 j P\u03d5,\u03b8(y, zj |xi)\n=max n\u2211 i log m\u2211 j Q(zj) P\u03d5,\u03b8(y, zj |xi) Q(zj)\n\u2265max n\u2211 i m\u2211 j Q(zj) log P\u03d5,\u03b8(y, zj |xi) Q(zj)\n(14)\nThe last step is according to Jansen inequality and equals if and only if Q(zj) is proportional to P\u03b8,\u03d5(y, zj |xi) and c is a constant. Such a proportional relationship can be expressed as:\nQ(zj)\nP\u03b8,\u03d5(y, zj |xi) = c, c is a constant\n\u21d0\u21d2 cP\u03b8,\u03d5(y, zj |xi) = Q(zj), \u2200i, j (15)\nSince \u2211\nj Q(zj) = 1, we can sum z on both sides of the equation:\n\u21d0\u21d2 c m\u2211 j P\u03b8,\u03d5(y, zj |xi) = 1 \u21d0\u21d2 c = 1\u2211m j P\u03b8,\u03d5(y, zj |xi)\n(16)\nNow we can derive a lower bound of \u220fn\ni P\u03b8,\u03d5(y|xi) by substituting c into Eq.(15) and then substituting Q(zj) to Eq.(14):\nQ(zj) = P\u03b8,\u03d5(y, zj |xi)\u2211m j P\u03b8,\u03d5(y, zj |xi)\n= P\u03b8(y|xi, zj)P\u03d5(zj |xi)\u2211m j P\u03b8(y|xi, zj)P\u03d5(zj |xi)\n= P\u03b8,\u03d5(zj |xi, y)\n(17)\nmax n\u220f i P\u03b8,\u03d5(y|xi) \u21d0\u21d2\nmax( n\u2211 i m\u2211 j Q(zj) logP\u03b8,\u03d5(y, zj |xi)\n\u2212 n\u2211 i m\u2211 j Q(zj) logQ(zj))\n(18)\nSince P\u03b8,\u03d5(y, zj |xi) = P\u03b8(y|xi, zj)P\u03d5(zj |xi), we can further simplify Eq.(18) as follows:\nmax( n\u2211 i m\u2211 j Q(zj) logP\u03b8,\u03d5(y, zj |xi)\n\u2212 n\u2211 i m\u2211 j Q(zj) logQ(zj))\n=max( n\u2211 i m\u2211 j Q(zj) logP\u03b8(y|xi, zj)P\u03d5(zj |xi)\n\u2212 n\u2211 i m\u2211 j Q(zj) logQ(zj))\n=max( n\u2211 i m\u2211 j Q(zj) logP\u03b8(y|xi, zj)\n+ n\u2211 i m\u2211 j Q(zj) logP\u03d5(zj |xi)\n\u2212 n\u2211 i m\u2211 j Q(zj) logQ(zj))\u2217\n=max( n\u2211 i m\u2211 j Q(zj) logP\u03b8(y|xi, zj))\n=max( n\u2211 i m\u2211 j P\u03d5(zj |xi, y) logP\u03b8(y|xi, zj))\n(19)\nSpecifically, in the step denoted with *, \u2211n i \u2211m j Q(zj) logP\u03d5(zj |xi) and\u2211n\ni \u2211m j Q(zj) logQ(zj)) can be canceled out, because Q(zj) = P\u03d5(zj |xi, y) \u2248 P\u03d5(zj |xi) in Eq. (17).\nFurther proof for convergence and equality of the original two optimizations is ordinary to derive as the proof of the EM algorithm, which is omitted here."
        },
        {
            "heading": "B Dataset Detail",
            "text": "B.1 Original Datasets\nAll the retrieval methods are evaluated on three types of datasets: single-sentence classification, sentence pair classification, and aspect-based sentiment analysis (ABSA). The single-sentence classification consists of SST-2 (Socher et al., 2013), MR (Pang and Lee, 2004), CR (Hu and Liu, 2004), and TREC (Voorhees and Tice, 2000). The sentence pair classification includes QQP 2,\n2https://quoradata.quora.com\nQNLI (Rajpurkar et al., 2016), SNLI (Bowman et al., 2015), and MNLI (Williams et al., 2017). The aspect-based sentiment analysis datasets are RES (Manandhar, 2014) and LAP (Manandhar, 2014). Particularly, for SST-2, MNLI, and QNLI from GLUE (Wang et al., 2018) and SNLI, we utilize their original validation sets for testing purposes.\nB.2 Few-shot Datasets\nFollowing the few-shot setting of Gao et al. (2021), we randomly select 16 or 8 examples from the training set to create 16-shot or 8-shot experiments. Specifically, we generate five distinct few-shot datasets using different seeds and train models on each of them. It is noted that we use consistent five seeds on different datasets and retrieval methods to conduct a fair comparison. The best model is chosen based on the validation results, and the av-\nerage evaluation scores on the original test set are reported."
        },
        {
            "heading": "C Experimental Settings",
            "text": "C.1 Hyper-parameter Selection\nWe adopt grid search to choose the hyperparameters of different methods. Specifically, the learning rates are taken from {1e\u22125, 2e\u22125, 5e\u22125}, the batch sizes are from {4, 8, 16}, and the numbers of retrieved examples are taken from {5, 10, 15}. The parameter t that determines the update frequency of loss LR is searched from {5, 10, 15}. The loss coefficient \u03bb in ranking-based loss is set to {0.5, 1, 2}. For each dataset, we set the max training steps as 800 steps and use early stopping to avoid over-fitting. In each trial, we validate the model in each epoch and save the best checkpoint.\nWe adopt the AdamW optimizer and accumulate gradients for each batch. The code is imple-\nmented with PyTorch 1.9.0 and transformers 4.1.1 and launched on an Ubuntu server with a single NVIDIA Tesla V100 (32G) or NVIDIA 4090. In addition, we will test our model with Mindspore, which is a new deep-learning framework3.\nC.2 Templates of Prompt-based Fine-tuning We use RoBERTa-Large (Liu et al., 2019)4 with 1024 dimensions to encode the input sentences with the related template. The templates for various datasets are shown in Table 6. Since our main aim is to investigate the difference among retrieval methods, we adopt the widely used and effective templates for these tasks in prompt-based fine-tuning refer to Gao et al. (2021). The specific templates are shown in Table 6.\n3https://www.mindspore.cn/ 4https://github.com/huggingface/transformers"
        }
    ],
    "title": "Retrieval-Augmented Few-shot Text Classification",
    "year": 2023
}