{
    "abstractText": "We introduce a new task called entity-centric question generation (ECQG), motivated by real-world applications such as topic-specific learning, assisted reading, and fact-checking. The task aims to generate questions from an entity perspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE with two novel modules: content focusing and question verification. The content focusing module first identifies a focus as \u201cwhat to ask\u201d to form draft questions, and the question verification module refines the questions afterwards by verifying the answerability. We also construct a large-scale open-domain dataset from SQuAD to support this task. Our extensive experiments demonstrate that GenCONE significantly and consistently outperforms various baselines, and two modules are effective and complementary in generating high-quality questions.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxiang Liu"
        },
        {
            "affiliations": [],
            "name": "Jie Huang"
        },
        {
            "affiliations": [],
            "name": "Kevin Chen-Chuan Chang"
        }
    ],
    "id": "SP:04143127537e689fd66fc4d70165670b5e7cfdb4",
    "references": [
        {
            "authors": [
                "Seohyun Back",
                "Akhil Kedia",
                "Sai Chetan Chinthakindi",
                "Haejun Lee",
                "Jaegul Choo."
            ],
            "title": "Learning to generate questions by learning to recover answercontaining sentences",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Bi",
                "Xiya Cheng",
                "Yuan-Fang Li",
                "Yongzhen Wang",
                "Guilin Qi."
            ],
            "title": "Knowledge-enriched, typeconstrained and grammar-guided question generation over knowledge bases",
            "venue": "Proceedings of the 28th International Conference on Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Kelvin Guu",
                "Percy Liang."
            ],
            "title": "Transforming question answering datasets into natural language inference datasets",
            "venue": "arXiv preprint arXiv:1809.02922.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Identifying where to focus in reading comprehension for neural question generation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2067\u20132073, Copenhagen, Denmark. Asso-",
            "year": 2017
        },
        {
            "authors": [
                "Xinya Du",
                "Junru Shao",
                "Claire Cardie."
            ],
            "title": "Learning to ask: Neural question generation for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342\u20131352,",
            "year": 2017
        },
        {
            "authors": [
                "Liam Dugan",
                "Eleni Miltsakaki",
                "Shriyash Upadhyay",
                "Etan Ginsberg",
                "Hannah Gonzalez",
                "DaHyeon Choi",
                "Chuning Yuan",
                "Chris Callison-Burch."
            ],
            "title": "A feasibility study of answer-agnostic question generation for education",
            "venue": "Findings of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "ACL",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "Controllable abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 45\u201354, Melbourne, Australia. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Zichu Fei",
                "Qi Zhang",
                "Yaqian Zhou."
            ],
            "title": "Iterative GNN-based decoder for question generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2573\u20132582, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Gao",
                "Piji Li",
                "Irwin King",
                "Michael R. Lyu."
            ],
            "title": "Interconnected question generation with coreference alignment and conversation flow modeling",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4853\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Junxian He",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Nazneen Rajani",
                "Caiming Xiong."
            ],
            "title": "Ctrlsum: Towards generic controllable text summarization",
            "venue": "arXiv preprint arXiv:2012.04281.",
            "year": 2020
        },
        {
            "authors": [
                "Tom Hosking",
                "Sebastian Riedel."
            ],
            "title": "Evaluating rewards for question generation models",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1",
            "year": 2019
        },
        {
            "authors": [
                "Qingbao Huang",
                "Mingyi Fu",
                "Linzhang Mo",
                "Yi Cai",
                "Jingyun Xu",
                "Pijian Li",
                "Qing Li",
                "Ho-fung Leung."
            ],
            "title": "Entity guided question generation with contextual structure and sequence information capturing",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Rajiv S Jhangiani",
                "I-Chant A Chiang",
                "Carrie Cuttler",
                "Dana C Leighton"
            ],
            "title": "Research methods in psychology",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020a. BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-",
            "year": 2004
        },
        {
            "authors": [
                "Bang Liu",
                "Haojie Wei",
                "Di Niu",
                "Haolan Chen",
                "Yancheng He."
            ],
            "title": "Asking questions the human way: Scalable question-answer generation from text corpus",
            "venue": "Proceedings of The Web Conference 2020, pages 2032\u20132043.",
            "year": 2020
        },
        {
            "authors": [
                "Bing Liu",
                "Chee Wee Chin",
                "Hwee Tou Ng."
            ],
            "title": "Mining topic-specific concepts and definitions on the web",
            "venue": "Proceedings of the 12th international conference on World Wide Web, pages 251\u2013260.",
            "year": 2003
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Nancy Chen."
            ],
            "title": "Controllable neural dialogue summarization with personal named entity planning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 92\u2013106, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Chenyang Lyu",
                "Lifeng Shang",
                "Yvette Graham",
                "Jennifer Foster",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "Improving unsupervised question answering via summarizationinformed question generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder"
            ],
            "title": "An overview of multi-task",
            "year": 2017
        },
        {
            "authors": [
                "Hannaneh Hajishirzi"
            ],
            "title": "Bidirectional attention",
            "year": 2016
        },
        {
            "authors": [
                "Daxin Jiang"
            ],
            "title": "TegTok: Augmenting text",
            "year": 2022
        },
        {
            "authors": [
                "Nihir Vedd",
                "Zixu Wang",
                "Marek Rei",
                "Yishu Miao",
                "Lucia Specia."
            ],
            "title": "Guiding visual question generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Bingning Wang",
                "Xiaochuan Wang",
                "Ting Tao",
                "Qi Zhang",
                "Jingfang Xu."
            ],
            "title": "Neural question generation with answer pivot",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9138\u20139145.",
            "year": 2020
        },
        {
            "authors": [
                "Siyuan Wang",
                "Zhongyu Wei",
                "Zhihao Fan",
                "Yang Liu",
                "Xuanjing Huang."
            ],
            "title": "A multi-agent communication framework for question-worthy phrase extraction and question generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2019
        },
        {
            "authors": [
                "Xu Wang",
                "Simin Fan",
                "Jessica Houghton",
                "Lu Wang."
            ],
            "title": "Towards process-oriented, modular, and versatile question generation that meets educational needs",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Zichen Wu",
                "Xin Jia",
                "Fanyi Qu",
                "Yunfang Wu."
            ],
            "title": "Enhancing pre-trained models with text structure knowledge for question generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6564\u20136574, Gyeongju, Re-",
            "year": 2022
        },
        {
            "authors": [
                "Qian Yu",
                "Lidong Bing",
                "Qiong Zhang",
                "Wai Lam",
                "Luo Si."
            ],
            "title": "Review-based question generation with adaptive instance transfer and augmentation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 280\u2013290,",
            "year": 2020
        },
        {
            "authors": [
                "Shiyue Zhang",
                "Mohit Bansal."
            ],
            "title": "Addressing semantic drift in question generation for semisupervised question answering",
            "venue": "Proceedings of",
            "year": 2019
        },
        {
            "authors": [
                "Zhihan Zhang",
                "Wenhao Yu",
                "Mengxia Yu",
                "Zhichun Guo",
                "Meng Jiang."
            ],
            "title": "A survey of multi-task learning in natural language processing: Regarding task relatedness and training methods",
            "venue": "arXiv preprint arXiv:2204.03508.",
            "year": 2022
        },
        {
            "authors": [
                "Zhenjie Zhao",
                "Yufang Hou",
                "Dakuo Wang",
                "Mo Yu",
                "Chengzhong Liu",
                "Xiaojuan Ma."
            ],
            "title": "Educational question generation of children storybooks via question type distribution learning and event-centric summarization",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Li Zhou",
                "Kevin Small",
                "Yong Zhang",
                "Sandeep Atluri."
            ],
            "title": "Generating self-contained and summarycentric question answer pairs via differentiable reward imitation learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Wenjie Zhou",
                "Minghua Zhang",
                "Yunfang Wu."
            ],
            "title": "Multi-task learning with language modeling for question generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Wenjie Zhou",
                "Minghua Zhang",
                "Yunfang Wu."
            ],
            "title": "Question-type driven question generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Question generation (QG) aims to automatically generate questions from inputs such as raw texts (Du et al., 2017), knowledge bases (Bi et al., 2020), or images (Vedd et al., 2022). Particularly, text-based QG broadly benefits conversational chatbots to improve user interaction (Gao et al., 2019), educational materials to enhance reading comprehension (Wang et al., 2022), or QA dataset enrichment to boost QA development (Lyu et al., 2021). There are mainly two QG settings, answeraware (Huang et al., 2021; Wu et al., 2022) and answer-agnostic (Back et al., 2021; Zhao et al., 2022), the difference between which is whether answers are known or not.\nHowever, in many scenarios we care more about how to ask from an angle, i.e., from an entity of interest (EOI) perspective, rather than ask with an\n1Code and dataset are publicly available at https:// github.com/liuyuxiang512/ECQG.\nanswer or ask randomly, which we refer to as entitycentric question generation (ECQG). For example, in topic-specific learning (Liu et al., 2003), by generating questions focusing on a specified topic entity given a text, we can gain a better understanding of that subject. Second, in assisted reading, generating questions pertaining to a specific concept entity serves as reading anchors for efficient content digestion and information localization of desired knowledge (Yu et al., 2020). Third, in fact checking, generated questions targeting at different facts of EOI together with obtained answers can further form claims to be supported or refuted (Pan et al., 2021).\nIn this paper, we aim to solve ECQG, which is to generate an entity-centric question given a text and an EOI, emphasizing a particular aspect of the EOI. As answers are usually unknown, i.e., only the entity and its context are given in most scenarios, and unnecessary, i.e., entity alone suffices to locate answers, we define ECQG as answer-agnostic.\nHowever, there are several challenges: (1) Lack of a dataset for ECQG. (2) Lack of centricity, as prior works treated input entities as answers (Sun et al., 2018; Fei et al., 2021; Wu et al., 2022) rather than as pivots to ask centered at them. (3) Lack of rationality, as existing answer-agnostic QG systems suffer from asking irrational, i.e., irrelevant or uninterpretable questions (Dugan et al., 2022). Summary-enhanced models (Zhou et al., 2021; Dugan et al., 2022) have been proposed to alleviate the issue, but they are domain-specific and only apply to detailed and in-depth input such as textbook or news articles, while for open-domain ECQG, where input texts vary in the level of detail, summaries do not always help. (4) Lack of answerability, as previous works tried to identify answer phrases to construct questions (Du and Cardie, 2017; Wang et al., 2019; Back et al., 2021), but such phrases are actually not treated as answers by the model, though it is a strong conditional restric-\ntion for generation. To address the lack of dataset, we construct a large-scale open-domain ECQG dataset from SQuAD (Rajpurkar et al., 2018). To further overcome the centricity, rationality, and answerability challenges, we design a novel Generation model with Content fOcusing and questioN vErification (GenCONE), inspired by the human process of generating questions \u2013 humans tend to first identify a focus to form a draft question then verify the question afterward (Liu et al., 2020; Jhangiani et al., 2019).\nSpecifically, we propose content focusing (CF) and question verification (QV) modules, which are sequentially dependent with the main question generation (QG) module. Firstly, the upstream CF module identifies \u201cwhat to ask\", allowing the model to learn focus features as intermediate knowledge that bridges the entity and context, thereby improving question rationality. Secondly, the downstream QV module verifies questions through question answering, which imparts answerability-based knowledge into the model, thus improving question answerability. Thirdly, GenCONE jointly encodes entity and context and feeds them into CF, QG, and QV modules, which work together and enforce the model to learn entity-context relation to improve centricity.\nOur main contributions are as follows: (1) We are the first to investigate entity-centric question generation (ECQG) problem. (2) We construct a large-scale open-domain dataset specific for ECQG and make it publicly available. (3) We propose a novel model called GenCONE, which is among the first works to build a coherent framework with both upstream and downstream sequentially dependent modules for answer-agnostic QG. (4) We conduct extensive experiments to demonstrate the superior performance of GenCONE and the effectiveness of its components."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Question Generation",
            "text": "Question generation (QG) aims to automatically generate questions from raw texts (Du et al., 2017), knowledge bases (Bi et al., 2020), or images (Vedd et al., 2022). For text-based QG, there are mainly two settings, answer-aware (Huang et al., 2021; Wu et al., 2022) and answer-agnostic (Back et al., 2021; Zhao et al., 2022). The difference is whether answers are given or not. Previous works mostly\nassumed that answers exist and tried to capture answer-context relation with proximity-based (Sun et al., 2018), GNN-based (Fei et al., 2021), or structure-enhanced (Wu et al., 2022) models.\nHowever, answers are not always known, and removing the constraints of answers increases the model\u2019s degrees of freedom, which is more beneficial for certain applications. Therefore, many researchers have been studying answer-agnostic QG since Du et al. (2017) first proposed it. Early works (Du et al., 2017; Scialom et al., 2019) targeted at totally uncontrolled QG, which introduces too much freedom and may generate irrelevant or uninterpretable questions. Some later works proposed to first identify question-worthy sentences (Du and Cardie, 2017) or phrases (Wang et al., 2019), and then generate questions conditioned on them; some other works (Wang et al., 2020; Back et al., 2021) proposed to incorporate answer span prediction or answer-containing sentence recovery to guide QG. A few recent works (Dugan et al., 2022; Zhao et al., 2022) also explored how human-written or machine-generated summaries help to improve the quality of generated questions. A recent work Reddy et al. (2022) focused on data augmentation for neural IR with QG conditioned on the sparsely attended words or phrases (entities) of the passage, where QG is application-specific, i.e., QG for QA, and limited in entity types, i.e., specified entity types are included. To the best of our knowledge, there are no prior works studying open-domain ECQG."
        },
        {
            "heading": "2.2 Entity-centric Text Generation",
            "text": "Existing entity-centric text generation works mainly focus on controllable summarization. Fan et al. (2018) is the first to bring forward controllable summarization considering control signals such as entities. They built it on a convolutional seq2seq model and used an anonymize-thenprepend method to enable entity-centric. He et al. (2020) later proposed keyword-controlled summarization based on pre-trained BART (Lewis et al., 2020a), and achieved entity-centric by treating entities as keywords. Liu and Chen (2021) proposed to control dialogue summarization flexibly with personal named entities to obtain personal-perspective dialogue summaries. These entity-centric summarization works usually prepended entities to text and applied seq2seq models without further investigation, assuming the seq2seq model itself can\nlearn the entity-context relation. Therefore, how to fully investigate entity-context relation beyond vanilla seq2seq models has not been studied yet."
        },
        {
            "heading": "2.3 Multi-Task Learning in Text Generation",
            "text": "Multi-task learning (MTL) is increasingly popular in text generation by training a model to perform multiple language tasks, where auxiliary tasks can be either sequentially dependent (Lewis et al., 2020b) or concurrent (Zhou et al., 2019a) with the main task, depending on whether input of a task is relying on output/hidden states of another task. Sequentially dependent auxiliary tasks are widely used in generation as either upstream (Lewis et al., 2020b) or downstream (Hosking and Riedel, 2019) tasks. In QG, Zhou et al. (2019b) introduced an upstream question type prediction task to generate more accurate interrogative words, while Zhang and Bansal (2019) used two downstream tasks, question paraphrasing and question answering, to address the \u201csemantic drift\" of questions. Particularly, for answer-agnostic QG, Wang et al. (2019) proposed an upstream question-worthy phrase extraction task to generate answerable questions, and Zhao et al. (2022) considered two upstream tasks, question type prediction and summarization, for event-centric educational QG. In this study, we introduce both upstream and downstream modules, specifically investigating their integration and adaptation to a new task, with each module tailored to address specific challenges associated to ECQG."
        },
        {
            "heading": "3 Method",
            "text": "We propose GenCONE, a PLM-based framework to handle the ECQG task with explicit and implicit guidance. In this section, we first give a formal definition of our problem and then dive into details of model design."
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "The entity-centric question generation (ECQG) problem can be formulated as follows: given a text T = {t1, t2, \u00b7 \u00b7 \u00b7 , t|T |} and an entity of interest (EOI) E = {e1, e2, \u00b7 \u00b7 \u00b7 , e|E|}, the objective is to generate a question Q = {q1, q2, \u00b7 \u00b7 \u00b7 , q|Q|} asking an aspect of entity E from its context T . ti, ej , qk \u2208 V are words in the context, entity, and question respectively; and V is a vocabulary. The answer to the entity-centric question Q is a text span that represents the specific aspect of EOI, such\nas another entity related to it, excluding EOI itself. Our ECQG problem, by asking some aspect about an entity but the answer is not the entity itself, also differs from prior works that ask questions whose answer is the entity itself (Sun et al., 2018; Fei et al., 2021; Wu et al., 2022). We show an example of entity-centric question given context and entity in Table 1."
        },
        {
            "heading": "3.2 GenCONE Model",
            "text": "The architecture of GenCONE is shown in Figure 1. Built on PLMs, the content focusing module (Section 3.2.1) first selects an entity-centric focus prior to generating questions, indicating \u201cwhat to ask\". Based on it, the question generation module (Section 3.2.2) learns a focus-aware context representation to generate question and its representation. Finally, the question verification module (Section 3.2.3) takes into account representations of both question and context to verify answers."
        },
        {
            "heading": "3.2.1 Upstream: Content Focusing",
            "text": "Existing answer-agnostic QG systems suffer from asking irrational, i.e., irrelevant or uninterpretable questions (Dugan et al., 2022). To generate relevant and interpretable questions, we design an upstream content focusing (CF) module to plan for question generation by looking for \u201cwhat to ask\" related to EOI. By explicitly learning focus features, CF enables the model to \u201ceavesdrop\" (Zhang et al., 2022), i.e., obtaining these features through the learning of auxiliary task, and thus improve question rationality. Particularly, the focus features are exploited as intermediate knowledge bridging entity and context to interact with subsequent module.\nEncoder GenCONE is built on a seq2seq backbone (Sutskever et al., 2014). We first use a pretrained Transformer encoder (Wolf et al., 2020) to jointly encode entity E and text T . The input sequence is denoted as C = {x1, x2, \u00b7 \u00b7 \u00b7 , x|C|}, where C = E\u27e8sep\u27e9T is a concatenation of entity\nand text tokens separated with a special token, and |C| is the length of input sequence. The obtained token-level input representation HC is:\nHC = Encoder(E\u27e8sep\u27e9T ) \u2208 R|C|\u00d7d, (1)\nwhere d is the dimension for hidden representations and HCi is the d-dimensional representation for input token xi. For simplicity, we set the hidden dimension of all modules the same as d.\nFocus Locating We consider content focus as a short text span. With token-level representation HC , we predict whether each token is a focus or not. Specifically, we use a pre-trained BERT (Devlin et al., 2019) to perform token classification:\nHF = BERT(HC) \u2208 R|C|\u00d72. (2)\nThe ground-truth focus F = [f1f2 \u00b7 \u00b7 \u00b7 f|C|] is a bit vector of the same length as input sequence, where each fi corresponds to an input token xi, and fi = 1 if xi belongs to the focus span. We treat the answer to the ground-truth question as content focus. The loss of CF is calculated as the cross-entropy loss between HF and F:\nLCF = \u2212 |C|\u2211 i=1 fi log(H F i [0]). (3)"
        },
        {
            "heading": "3.2.2 Main: Question Generation",
            "text": "Question generation (QG) module is the main component to generate desired entity-centric questions, which is essentially a decoder, taking entity-centric context representation HC \u2208 R|C|\u00d7d and focus features HF \u2208 R|C|\u00d72 as input.\nFusion Layer We first fuse HC and HF to get a focus-aware context representation HCF as:\nHCF = [HC ;HF ]wCF , (4)\nwhere [; ] denotes concatenation along column axis and wCF \u2208 R(d+2)\u00d7d is a linear transformation. Hence, we get the focus-aware context representation HCF \u2208 R|C|\u00d7d for subsequent decoding.\nQuestion Generation Taking HCF as input, we use a pre-trained Transformer decoder (Wolf et al., 2020) to generate a question, and we take the decoder\u2019s last hidden states HQ = Decoder(HCF ) \u2208 R|Q|\u00d7d as question representation, where |Q| is the length of the question sequence and d is the dimension of hidden representations. Supposing Q = {q1, q2, \u00b7 \u00b7 \u00b7 qm} is the ground truth question, we calculate QG loss with teacher forcing as:\npQj ,H Q j = Decoder(H CF ,HQ<j , qj\u22121), (5)\nLQG = \u2212 1\nm m\u2211 j=1 logpQj,qj , (6)\nwhere pQj is the probability distribution over decoding vocabulary at the j-th step, and pQj,qj is the probability of token qj ."
        },
        {
            "heading": "3.2.3 Downstream: Question Verification",
            "text": "To generate valid questions, previous answeragnostic QG works (Du and Cardie, 2017; Wang et al., 2019; Back et al., 2021) proposed to identify answer phrases prior to generating questions. However, such extracted \u201canswer\" phrases are not treated as answers by their models, though it is\na strong conditional restriction for question generation. To ensure questions are answerable, it is infeasible to include an \u201canswerability\" feature when generating a question, as it will not be available as input at run time. Therefore, we design a downstream question verification (QV) module to examine answerability by inferring answers based on context and question. With such a verification step, QV is able to impart additional answerabilitybased knowledge into the model (Ruder, 2017), and thus improve question answerability.\nDual Attention Taking HC and HQ as inputs, we first learn a question-aware context representation HCQ , which is inspired by Seo et al. (2016) to first fuse information bidirectionally, i.e., from HC to HQ and from HQ to HC , and then unify both to get HCQ \u2208 R|C|\u00d7d.\nMathematically, we first calculate a similarity matrix S \u2208 R|C|\u00d7|Q|, with each element Sij = \u03b1(HCi ,H Q j ), where H C i and H Q j are embeddings of the i-th context token and the j-th question token respectively. We use the same \u03b1(hc,hq) = wTS [h\nc;hq;hc \u25e6 hq] as in Seo et al. (2016), where wS \u2208 R3d, \u25e6 is element-wise product, and [; ] is vector concatenation along column. We then derive attended embeddings as:\nai = softmax(Si,:) \u2208 R|Q|, H\u0303Qi = \u2211 j aijH Q j \u2208 R d, b = softmax(maxrow(S)) \u2208 R|C|,\nh\u0303c = \u2211 i biH C i \u2208 Rd,\nwhere maxrow is to perform the maximum function across row axis. Thus H\u0303Q \u2208 R|C|\u00d7d and we tile h\u0303c |C| times to get matrix H\u0303C \u2208 R|C|\u00d7d. We then obtain token representation HCQi = \u03b2(HCi , H\u0303 Q i , H\u0303 C i ), where H CQ i is the i-th row vector corresponding to the i-th context token, \u03b2 is defined by \u03b2(hc, h\u0303q, h\u0303c) = wTCQ[h\nc; h\u0303q;hc\u25e6h\u0303q;hc\u25e6 h\u0303c], and wCQ \u2208 R4d. Finally, we get the questionaware context representation HCQ \u2208 R|C|\u00d7d.\nAnswer Inferring Answers are short text spans of input. After getting question-aware context representation HCQ , we use a pre-trained BERT (Devlin et al., 2019) to predict whether each token is answer or not, formally, HA = BERT(HCQ) \u2208 R|C|\u00d72. The ground-truth answer is denoted as A = [a1a2 . . . a|C|], which is a bit vector of the\nsame length as input sequence, with ai = 1 if corresponding input token xi is answer token and ai = 0 if not. Similarly, the QV loss is the cross-entropy loss between HA and A:\nLQV = \u2212 |C|\u2211 i=1 ai log(H A i [0]). (7)"
        },
        {
            "heading": "3.2.4 Training Objective",
            "text": "We jointly train three modules end-to-end with a combined training objective as follows:\nL = LQG + \u03bb1LCF + \u03bb2LQV , (8)\nwhere 0 < \u03bb1, \u03bb2 < 1 control the relative importance of each associated loss. The CF loss enables the model to explicitly learn a content focus first and produce relevant and interpretable questions; while the QV loss allows answerabilitybased knowledge to be imparted into the model implicitly, and thus generating valid and answerable questions."
        },
        {
            "heading": "4 Experiment Setup",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We construct an ECQG dataset from SQuAD (Rajpurkar et al., 2018), an open-domain reading comprehension dataset originated from Wikipedia articles. Specifically, we use SQuAD v2.02, which has around 130k training samples and 12k testing samples. We first remove samples without answers so that all remaining questions are answerable. For samples where multiple answers exist for the same question, we vote for the answer to ensure answer quality, i.e., selecting the answer with the highest frequency, thus prioritizing the most commonly agreed-upon answer.\nThe key to construct an ECQG dataset is to obtain entity attribute of each sample. On the one hand, as Wikipedia titles are the core entities discussed in texts, we consider them as central entities if corresponding questions contain the title entities, assuming that they are the central entities of both contexts and questions. On the other hand, questions may not relate to title entities. In this case, we first use spaCy3 to extract entities from contexts and questions respectively. If both context and question share and only share a common entity, this entity will be treated as the central entity. This\n2https://rajpurkar.github.io/SQuAD-explorer/ 3https://spacy.io/\nis to reduce the noise introduced by entity extraction, so that questions are tightly tied to a single key entity and more central to this entity. By filtering out samples that do not satisfy the above conditions and splitting the training set into training and validation sets, we finally get the dataset. The statistics of our ECQG dataset are in Table 2.\nWe conducted a manual analysis of the dataset we constructed. A sample is considered \u2018good\u2019 if the extracted entity is meaningful and the question is centered around this entity, such as when another semantically related entity serves as the answer. Conversely, a sample is labeled \u2018bad\u2019 if the question does not directly pertain to the extracted entity or if the extracted entity is non-specific, merely representing a general word or phrase. For this evaluation, we randomly selected 30 samples from the training set and 20 samples from the testing set. The results are presented in Table 3.\nWe further examined the erroneous samples identified in the dataset, with two representative examples shown in Table 4. A recurrent issue in these samples is the need to consider longer phrases encompassing the extracted \u201centity\" as a whole. For instance, in Example 1, \"the French House of Guise\" should be treated as a single entity rather than just \u201cGuise\". Similarly, in Example 2, the appropriate entity is \u201cschool of Public Health\" instead of merely \u201cPublic Health\". This limitation stems from the constraints of the entity extraction tool we utilized. However, it is important to note that a significant portion of the dataset, exceeding 90%, remains accurate."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We built GenCONE on PLMs T5 (Raffel et al., 2020) or BART (Lewis et al., 2020a). For each model, we experimented with its base and large\nversions. CF and QV modules are based on pretrained BERT (Devlin et al., 2019), with hidden dimensions the same as T5 or BART, i.e., if T5/BART is base/large version, BERT is base/large version as well. We implemented GenCONE in PyTorch 1.13, and experimented on NVIDIA A40 with 45G memory. We used the AdamW optimizer and set the weight decay = 0.01, maximum source length = 128, maximum target length = 32. For base versions, we set batch size = 64 and epoch = 15. We set early stop training if there were no better results for 3 epochs. For large versions, we set batch size = 32 and epoch = 10. We set \u03b31 + \u03b32 = 0.3. We tried learning rates in {1e-5, 2e-5, 5e-5, 1e-4} and selected the one with best validation results for different models. We ran models with different seeds and calculated the average metric scores."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "We adopted three automatic metrics, BLEU, METEOR, and ROUGE (Lin and Och, 2004), which are widely used in previous QG works to evaluate the quality of machine-generated texts."
        },
        {
            "heading": "5 Experiment Results",
            "text": ""
        },
        {
            "heading": "5.1 Comparison with Existing Models",
            "text": ""
        },
        {
            "heading": "5.1.1 Baselines",
            "text": "ECQG is introduced as a novel task within the answer-agnostic paradigm. For benchmarking ECQG and assessing our proposed method, we adapt several existing answer-agnostic QG models as baselines. We also benchmark ECQG with large language models (LLMs). For the adapted QG baselines, we prepend entities to contexts and apply fine-tuning; for LLMs, we utilize a few-shot prompting strategy.\nSummQG (Dugan et al., 2022) is a QG model enhanced by text summarization. To adapt SummQG for ECQG, it is imperative to produce entitycentric summaries. Given the absence of definitive entity-centric summaries, we employed an entity-\ncentric summarizer, CTRLsum (He et al., 2020) \u2013 pre-trained on CNN/DailyMail\u2013 to generate these summaries. For the actual question generation process, we leveraged the QG model provided by Dugan et al. (2022). Due to the unavailability of entity-centric summaries for training, we kept the summarization component fixed, while evaluating both pre-trained-only and fine-tuned QG modules. Here, fine-tuning was achieved using the generated summaries paired with their corresponding ground truth questions.\nD-S-DRIL (Zhou et al., 2021) is a BART-based model with an intermediate summarization step but sampling summaries and reconstructing questions exclusively based on the hidden states of the summary decoder. We used the model from Demszky et al. (2018) to convert QA pairs into declarative sentences. These were treated as entity-centric summaries and combined with ground truth questions for training. The summary generation and question generation processed were trained jointly, with \u03bb = 0.3 as recommended by Zhou et al. (2021).\nTegTok (Tan et al., 2022) is a knowledgeaugmented encoder-decoder model. This model incorporates task-specific knowledge during encoding, and open-world knowledge during decoding. To ensure equitable comparison, we disregarded external knowledge, focusing solely on task-specific knowledge obtained from training data.\nGPT-4 (OpenAI, 2023), as a large language model, has showcased outstanding performance across a wide range of NLP tasks. It is notably adept in multimodal zero-shot, one-shot, and fewshot contexts. As a result, GPT-4 is also adopted as a benchmark to evaluate the ECQG dataset and to compare with other methods. We conducted in-context learning on GPT-4, using both 1-shot and 5-shot prompting techniques. Investigating in-context learning of GPT-4 across varied shots could offer more insights into the impact of demonstrations. Nevertheless, given that this is not the primary focus of our study and considering the cost of GPT-4 API, we limit our evaluation to two specific few-shot scenarios."
        },
        {
            "heading": "5.1.2 Results",
            "text": "The main results are presented in Table 5. For ECQG, GenCONE notably surpasses other answeragnostic question generation models and LLMs in performance. In particular, it exceeds the performance of summarization-enhanced models like SummQG and D-S-DRIL, with an absolute gain\nof 5% to 50%. For example, SummQG achieves a 31.27% ROUGEL score and D-S-DRIL records 43.28% ROUGEL. In contrast, GenCONE attains a 46.12% ROUGEL score, marking a relative gain of 47.5% over SummQG and 6.6% over D-S-DRIL. This suggests that prior summarization-enhanced QG models may not be optimally suited for the ECQG task, aligning with our initial hypothesis. The knowledge-enhanced model, TegTok, posts a 42.39% ROUGEL score, which is a 11.12% improvement over the fine-tuned SummQG but still falls short of GenCONE by 3.73%. Furthermore, the automatic evaluation scores of most fine-tuned models surpass those of GPT-4. This is because fine-tuning allows these models to capture the inherent distribution of ECQG, hinting at significant potential to enhance GPT-4\u2019s domain adaptation to ECQG. Besides, despite a minimal performance discrepancy, GPT-4 with 5-shot prompting appears marginally less effective than its 1-shot counterpart, suggesting that increasing the shots from 1 to 5 may not enhance GPT-4\u2019s efficacy in ECQG. Overall, these findings validate that our proposed GenCONE is more adept at extracting knowledge from pre-trained models for ECQG than its contemporaries."
        },
        {
            "heading": "5.2 Comparison with Seq2Seq Models",
            "text": ""
        },
        {
            "heading": "5.2.1 Baselines",
            "text": "To further evaluate GenCONE in terms of whether it better exploits the pre-trained Seq2Seq models, we experimented with different pre-trained Seq2Seq models, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020a), and we tried both base and large versions. For all Seq2Seq models, we concatenate entity with context separated with a special token as input, and train using ground truth entity-centric questions."
        },
        {
            "heading": "5.2.2 Results",
            "text": "The results in Table 6 show that GenCONE consistently performs better than vanilla Seq2Seq models. Across all settings, GenCONE scores better on all metrics compared with the corresponding vanilla Seq2Seq model. For example, based on BARTbase, GenCONE improves Seq2Seq from 42.58% to 46.09%, with a relative gain of around 8.2%. These results further demonstrate that GenCONE can well exploit and improve significantly from pre-trained encoder-decoder models."
        },
        {
            "heading": "5.3 Ablation Study: Effect of CF/QV Modules",
            "text": ""
        },
        {
            "heading": "5.3.1 Baselines",
            "text": "To better understand the effectiveness of CF module and QV module, we conducted ablation study by removing either of them as model variants.\nGenCONE-CF is a variant of GenCONE by removing QV module, which is only equipped with CF module. The loss is thus calculated by L = LQG + \u03bb1LCF .\nGenCONE-QV is a variant of GenCONE by removing CF module, which is only equipped with QV module. Particularly, we set the focus-aware context representation the same as original context representation, i.e., HCF = HC. The loss is calculated by L = LQG + \u03bb2LQV ."
        },
        {
            "heading": "5.3.2 Results",
            "text": "The results are shown in Table 7. As we can see, either removing QV module (GenCONE-CF) or CF module (GenCONE-QV) results in a performance degradation, compared with the full model GenCONE, which shows that two modules are complementary to some degree. They can learn different knowledge and jointly contribute to improve the performance of GenCONE. When compared with Seq2Seq, either GenCONE-CF or GenCONE-QV consistently performs better, which also demon-\nstrates that both content focusing loss and question verification loss helps to improve Seq2Seq significantly, and indicates the effectiveness of both CF and QV modules in GenCONE."
        },
        {
            "heading": "5.4 Human Evaluation",
            "text": ""
        },
        {
            "heading": "5.4.1 Evaluation Setup",
            "text": "In addition to machine evaluation, we also conducted human evaluation to evaluate the quality of generated questions. We focus on three aspects of question quality: entity centricity, rationality (relevance and interpretability), and answerability. We randomly selected 100 (entity, context, question) samples generated by the Seq2Seq model and GenCONE, as well as variants of GenCONE in Section 5.3, based on T5large, and asked three students to evaluate four properties of generated questions. Students are required to answer: (1) entity centricity, whether the question is centered at the entity, i.e., asking an aspect related to entity from the context; (2) relevance, whether the question is semantically relevant to the context; (3) interpretability, whether the question makes sense in terms of context; (4) answerability, whether the question is answerable or not by the context. Each student is required to annotate agree(5), somewhat agree(4), neutral(3), somewhat disagree(2), or disagree(1). We then calculated average scores of\nthree students for all models."
        },
        {
            "heading": "5.4.2 Results",
            "text": "As shown in Table 8, our method surpasses the Seq2Seq model across all properties, indicating that GenCONE produces questions of superior centricity, rationality, and answerability. Notably, GenCONE significantly enhances question answerability. Both GenCONE variants display improvements over the Seq2Seq model: GenCONE-CF excels in rationality, while GenCONE-QV boosts answerability more effectively. Additionally, GenCONE and its variants augment entity centricity, highlighting the effectiveness of both modules in enhancing centricity. We hypothesize that the joint encoding of entity and context compels the model to discern the entity-context relationship, particularly through the integration of the main question generation module and two additional modules: content focusing and question verification. Human evaluations further underscore that our proposed GenCONE, equipped with content focusing and question verification modules, consistently crafts questions of a higher quality than those generated by Seq2Seq models."
        },
        {
            "heading": "5.5 Case Study",
            "text": "To gain an insight of how content focusing and/or question verification perform for ECQG, we show\nthree examples in Appendix A. In the first example, the question generated by Seq2Seq is general and irrelevant, and questions generated by GenCONE as well as its variants are more relevant in terms of context, which are asking more concretely. In the second example, questions generated by all models are relevant. However, the question generated by Seq2Seq is unanswerable from context, i.e., context is not sufficient to ensure it is answerable. In the third example, all models perform badly. Seq2Seq generates irrelevant questions while GenCONE generates unanswerable questions considering context. However, the question generated by GenCONE is more interpretable and makes sense. Therefore, compared with Seq2Seq, GenCONE can generate more relevant, interpretable, and answerable questions given context and entity. In addition, we further evaluated the results of GPT-3.5 on the ECQG dataset with zero-shot prompting. More details are explained in Appendix B."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduce a new task, entity-centric question generation (ECQG), motivated by realistic applications such as topic-specific learning, assisted reading, and fact checking. We also construct a largescale open-domain ECQG dataset from SQuAD. To address rationality, answerability, and centricity issues of generated questions, we propose a coherent PLM-based framework called GenCONE and design two novel modules, content focusing and question verification. Experiment results, including both automatic and human evaluations, show that GenCONE significantly and consistently outperforms baselines in terms of automatic metrics and question quality including entity centricity, rationality, and answerability.\nLimitations\nAs we construct ECQG dataset from SQuAD, which contains factoid questions and answers are short text spans, our ECQG dataset inherits these characteristics. Therefore, we focus on factoid ECQG in this paper. Future works may investigate different types of questions, e.g., highly abstractive entity-centric questions. Besides, as answers are short text spans, we use token classification for extractive QA to infer answers. Otherwise, we will need to use abstractive QA modules instead, though the idea in this paper still applies. Lastly, our model introduces many parameters and requires sufficient GPU resources to train."
        },
        {
            "heading": "Acknowledgements",
            "text": "This material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) and IBM-Illinois Discovery Accelerator Institute (IIDAI), grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies."
        },
        {
            "heading": "A Case Study of GenCONE",
            "text": "The examples are shown in Table 9."
        },
        {
            "heading": "B Case Study of LLMs",
            "text": "We further evaluated the performance of GPT-3.5 on the ECQG dataset with zero-shot prompting. By providing prompt \u201cPlease ask an entity-centric question for entity <entity tokens> from the passage and give the corresponding answer: <passage tokens>\u201d, we manually evaluated the outputs and compared them with ours and ground truth. We present examples below in Table 10.\nIn our analysis, we observed that GPT-3.5 tends to generate more intricate and open-ended questions compared with those generated by GenCONE or found in ground truth. Unlike the factoid questions predominant in our dataset, the questions generated by GPT-3.5 often require a more nuanced understanding of the context and typically cannot be answered by a text span of the input passage. Moreover, while some questions generated by GPT3.5 can be answered within the given context, others extend beyond it, requiring additional information. Therefore, although GPT-3.5 can still generate questions that focus on specific entities and appear rational, GenCONE demonstrates superior performance in terms of answerability."
        }
    ],
    "title": "Ask To The Point: Open-Domain Entity-Centric Question Generation",
    "year": 2023
}