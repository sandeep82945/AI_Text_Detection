{
    "abstractText": "Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-theart probabilistic methods\u2019 effectiveness in improving the uncertainty quality of the neural summarization models, across three largescale benchmarks with varying difficulty using our newly introduced evaluation protocol. We show that the probabilistic methods consistently improve the model\u2019s generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns of probabilistic methods widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout), cautioning the importance of choosing appropriate method for the data setting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Polina Zablotskaia"
        },
        {
            "affiliations": [],
            "name": "Du Phan"
        },
        {
            "affiliations": [],
            "name": "Joshua Maynez"
        },
        {
            "affiliations": [],
            "name": "Shashi Narayan"
        },
        {
            "affiliations": [],
            "name": "Jie Ren"
        },
        {
            "affiliations": [],
            "name": "Jeremiah Liu"
        }
    ],
    "id": "SP:9979001ae18179e92255f72e579b641aa6946af3",
    "references": [
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "Cliff: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633\u20136649.",
            "year": 2021
        },
        {
            "authors": [
                "Shrey Desai",
                "Greg Durrett."
            ],
            "title": "Calibration of pre-trained transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 295\u2013302, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Angelos Filos",
                "Sebastian Farquhar",
                "Aidan N Gomez",
                "Tim GJ Rudner",
                "Zachary Kenton",
                "Lewis Smith",
                "Milad Alizadeh",
                "Arnoud De Kroon",
                "Yarin Gal"
            ],
            "title": "A systematic comparison of bayesian deep learning robustness in diabetic retinopathy",
            "year": 2019
        },
        {
            "authors": [
                "Marina Fomicheva",
                "Shuo Sun",
                "Lisa Yankovskaya",
                "Fr\u00e9d\u00e9ric Blain",
                "Francisco Guzm\u00e1n",
                "Mark Fishel",
                "Nikolaos Aletras",
                "Vishrav Chaudhary",
                "Lucia Specia."
            ],
            "title": "Unsupervised quality estimation for neural machine translation",
            "venue": "Transactions of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani."
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "international conference on machine learning, pages 1050\u20131059. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Alexios Gidiotis",
                "Grigorios Tsoumakas."
            ],
            "title": "Should we trust this summary? bayesian abstractive summarization to the rescue",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 4119\u20134131.",
            "year": 2022
        },
        {
            "authors": [
                "Tilmann Gneiting",
                "Fadoua Balabdaoui",
                "Adrian E Raftery."
            ],
            "title": "Probabilistic forecasts, calibration and sharpness",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243\u2013 268.",
            "year": 2007
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International conference on machine learning, pages 1321\u20131330. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tomas Kocisky",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Norman P Jouppi",
                "Doe Hyun Yoon",
                "George Kurian",
                "Sheng Li",
                "Nishant Patil",
                "James Laudon",
                "Cliff Young",
                "David Patterson."
            ],
            "title": "A domain-specific supercomputer for training deep neural networks",
            "venue": "Communications of the ACM, 63(7):67\u201378.",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Jung",
                "Changhwa Lee",
                "Mallesh Pai",
                "Aaron Roth",
                "Rakesh Vohra."
            ],
            "title": "Moment multicalibration for uncertainty estimation",
            "venue": "Conference on Learning Theory, pages 2634\u20132678. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Charles Blundell"
            ],
            "title": "Simple and scalable pre",
            "year": 2017
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "ROUGE: A package for auto",
            "year": 2004
        },
        {
            "authors": [
                "Ryan McDonald"
            ],
            "title": "On faithfulness and factu",
            "year": 2020
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay Cohen",
                "Maria Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topicaware convolutional neural networks for extreme summarization",
            "venue": "2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Rahul Rahaman"
            ],
            "title": "Uncertainty quantification and deep ensembles",
            "venue": "Advances in Neural Information Processing Systems, 34:20063\u201320075.",
            "year": 2021
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "arXiv preprint arXiv:2209.15558.",
            "year": 2022
        },
        {
            "authors": [
                "Abigail See",
                "Peter J Liu",
                "Christopher D Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Matthew Snover",
                "Bonnie Dorr",
                "Richard Schwartz",
                "Linnea Micciulla",
                "John Makhoul."
            ],
            "title": "A study of translation edit rate with targeted human annotation",
            "venue": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Tech-",
            "year": 2006
        },
        {
            "authors": [
                "Shichao Sun",
                "Wenjie Li."
            ],
            "title": "Alleviating exposure bias via contrastive learning for abstractive text summarization",
            "venue": "arXiv preprint arXiv:2108.11846.",
            "year": 2021
        },
        {
            "authors": [
                "Dustin Tran",
                "Jeremiah Liu",
                "Michael W Dusenberry",
                "Du Phan",
                "Mark Collier",
                "Jie Ren",
                "Kehang Han",
                "Zi Wang",
                "Zelda Mariet",
                "Huiyi Hu"
            ],
            "title": "Plex: Towards reliability using pretrained large model extensions. arXiv preprint arXiv:2207.07411",
            "year": 2022
        },
        {
            "authors": [
                "Shuo Wang",
                "Yang Liu",
                "Chao Wang",
                "Huanbo Luan",
                "Maosong Sun."
            ],
            "title": "Improving back-translation with uncertainty-based confidence estimation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Wang",
                "Zhaopeng Tu",
                "Shuming Shi",
                "Yang Liu."
            ],
            "title": "On the inference calibration of neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070\u20133079, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Wang",
                "Zhaopeng Tu",
                "Shuming Shi",
                "Yang Liu."
            ],
            "title": "On the inference calibration of neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070\u20133079.",
            "year": 2020
        },
        {
            "authors": [
                "Yeming Wen",
                "Dustin Tran",
                "Jimmy Ba."
            ],
            "title": "Batchensemble: an alternative approach to efficient ensemble and lifelong learning",
            "venue": "arXiv preprint arXiv:2002.06715.",
            "year": 2020
        },
        {
            "authors": [
                "Yijun Xiao",
                "William Yang Wang."
            ],
            "title": "Quantifying uncertainties in natural language processing tasks",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7322\u20137329.",
            "year": 2019
        },
        {
            "authors": [
                "Jiacheng Xu",
                "Shrey Desai",
                "Greg Durrett."
            ],
            "title": "Understanding neural abstractive summarization models via uncertainty",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6275\u20136281.",
            "year": 2020
        },
        {
            "authors": [
                "Shusheng Xu",
                "Xingxing Zhang",
                "Yi Wu",
                "Furu Wei."
            ],
            "title": "Sequence level contrastive learning for text summarization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11556\u201311565.",
            "year": 2022
        },
        {
            "authors": [
                "Yao Zhao",
                "Misha Khalman",
                "Rishabh Joshi",
                "Shashi Narayan",
                "Mohammad Saleh",
                "Peter J Liu."
            ],
            "title": "Calibrating sequence likelihood improves conditional language generation",
            "venue": "arXiv preprint arXiv:2210.00045.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, autoregressive deep models for text summarization have achieved impressive performance. However, despite their success, these models often suffer from a critical flaw: they generate prediction with high confidence even when the quality of the summary is low (Xu et al., 2022). This can severely compromise the reliability and trustworthiness of the generated summaries in realworld applications. In the probabilistic forecast literature, such issue is known under the term miscalibration, i.e., the model\u2019s predictive confidence\n\u2217Equal contribution.\nis mis-aligned with its prediction quality. For example, in classification tasks, a model is said to be miscalibrated if for all test examples where it predicts with probability 0.9, the model\u2019s actual accuracy for these examples deviates far from 90% (Guo et al., 2017; Gneiting et al., 2007). Despite its practical importance, this notion of uncertainty calibration has received much less attention in the summarization literature until recently, with the proposed techniques mostly focusing on training deterministic models (Cao and Wang, 2021; Sun and Li, 2021; Zhao et al., 2022; Liu et al., 2022; Jung et al., 2021).\nIn the uncertainty literature, probabilistic deep learning has emerged as a principled approach to tackle model miscalibration while maintaining prediction quality (Nado et al., 2021). Intuitively, probabilistic DNNs generate multiple plausible predictions from its posterior predictive p\u0304m(y|x) = 1M \u2211M m=1 pm(y|x) and report the average, thereby mitigating the overconfidence of the individual model prediction. Although well-tested in classification tasks, the effectiveness of different state-of-art probabilistic methods in improving neural summarization models\u2019 uncertainty quality has been less explored. The existing study mostly focuses on a particular classic method (e.g., Monte Carlo Dropout or MCD) and tested on relatively simple datasets that doesn\u2019t fully capture the realistic usage (Gidiotis and Tsoumakas, 2022).\nIn this work, we address this by conducting a comprehensive investigation of the relative effectiveness of state-of-the-art probabilistic methods in improving the uncertainty quality of neural summarization models. We interrogate both classic approaches such as Monte Carlo Dropout (MCD) and Deep Ensemble (DE), and more recent state-of-art methods such as Batch Ensemble (BE) Spectral-normalized Gaussian process (SNGP) and their combinations that address the latency and quality caveats of the classic methods (Gal and\nGhahramani, 2016; Lakshminarayanan et al., 2017; Liu et al., 2020; Wen et al., 2020). Furthermore, we evaluate method performance across multiple benchmarks of varying difficulty to ensure the practical relevance of our result, and to uncover potential failure patterns of different approaches. Our contributions are: \u2022We adapt the various probabilistic deep learning methods to the Pre-trained Language Model (PLM) setup and conduct an extensive study on their effect on both uncertainty and prediction aspects of model performance. \u2022We propose a new evaluation protocol to measure the uncertainty calibration performance of summarization models, tailored toward domain-specific quality scores (e.g., ROUGE). \u2022 We show that using probabilistic methods generally leads to improved summarization and calibration performance, and consequently improved selective generation. We also discuss the failure patterns of the popular methods such Deep Ensembles (Lakshminarayanan et al., 2017) and Monte Carlo Dropout (Gal and Ghahramani, 2016)."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "Probabilitistic Learning for Seq2seq Models.",
            "text": "Developed primarily in the context of discriminative models, the state-of-art probabilistic approaches can be applied to large neural models without sacrificing performance (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Wen et al., 2020; Liu et al., 2020). Recently, however, initial investigations into unsupervised uncertainty estimation for structured prediction have appeared, primarily focusing on more basic approaches such as Monte Carlo Dropout (MCD) or Deep Ensemble (DE) (Xiao and Wang, 2019; Wang et al., 2019; Fomicheva et al., 2020; Malinin and Gales, 2021; Lin et al., 2022), with a few work looking into summarization tasks (Xu et al., 2020; Gidiotis and Tsoumakas, 2022). In comparison, this work focuses on an unbiased evaluation of a wide range of state-of-the-art probabilistic methods on tasks with varying difficulty, and reveals failure patterns of classic approaches such as MCD and DE."
        },
        {
            "heading": "Calibration Technique in Language Processing.",
            "text": "Guo et al. (2017) proposed improving calibration of document classifier using of temperature scaling. M\u00fcller et al. (2019) and Wang et al. (2020a) explored improving calibration in neural machine translation using label smoothing. Desai and Dur-\nrett (2020) noted that calibration methods can be used to improve the accuracy of pre-trained language models. Jung et al. (2021) proposed a novel training approach to improve calibration by minimizing a combined loss of cross-entropy and calibration. In the summarization literature, (Cao and Wang, 2021; Xu et al., 2022; Sun and Li, 2021; Zhao et al., 2022; Liu et al., 2022) explored calibrating model probability using contrastive learning approaches. Most of these techniques focus on deterministic models. They are orthogonal to and can be combined with the probabilistic approaches we explore in this work."
        },
        {
            "heading": "3 Methods",
            "text": "Probabilistic methods have been adopted to increase the reliability of pre-trained language models. Plex paper (Tran et al., 2022) provided a nice survey on the robustness of uncertainty methods on text classification tasks. In this study, we opted for the methods that are widely recognized and used. Section A.7 contains in-depth details, here we provide a general overview:"
        },
        {
            "heading": "Single-model Methods:",
            "text": "\u2022 Deterministic Baseline - we use the base T5 model 1 (Raffel et al., 2020) as the baseline model. \u2022Monte Carlo Dropout (MCD) (Gal and Ghahramani, 2016) which estimates uncertainty using the Monte Carlo average of 10 dropout samples. Those samples are generated using the same model and parameters but with different random seeds at dropout. \u2022 Batch Ensemble (BE) (Wen et al., 2020) - an ensemble method which has much lower computational costs comparing to MC Dropout and Deep Ensemble. We replace the last transformer\u2019s MLP block by a batch ensemble block with ensemble size be 5.2 \u2022 Spectral-normalized Neural Gaussian Process (SNGP) (Liu et al., 2020) - a recent stateof-the-art approach which improves uncertainty quality by transforming a neural network into an approximate Gaussian process model. The Gaussian Process last layer is able to reflect the distance between a test example and the training set, hence potentially be helpful in improving calibration. \u2022 SNGP+MCD which is the MC Dropout on top of an SNGP model;\n1All methods can be applied to larger models. 2BE requires more memory on a single machine, so we\nkeep the ensemble size below 10."
        },
        {
            "heading": "Multi-model Methods:",
            "text": "\u2022 Deep Ensemble (DE) (Lakshminarayanan et al., 2017) which trains 10 deterministic models individually and averages all. We use the same model architecture but changing the initial seeds. \u2022 Gaussian Process Ensemble (SNGP+DE) is the combination of deep ensemble and SNGP.\nFor all methods, we use the official base T5 checkpoint, which are pretrained on a large corpus like C4 (Raffel et al., 2020). We then finetune the parameters on summarization tasks. To generate prediction from the model posterior in all, we perform beam inference with respect to the model\u2019s conditional posterior mean probability, i.e., p\u0304(yt|y<t, x) = 1M \u2211M m=1 pm(yt|y<t, x), where M = 10 is the number of samples from model posterior (for the deterministic baseline and SNGP-only method M = 1). To quantify model uncertainty, we consider the length-normalized predicted log-probabilities following previous work, i.e., u(y|x) := 1T \u2211T t=1 p\u0304(yt|y<t, x) (Wu et al., 2016; Liu et al., 2022) where x is the input sequence, y is the output sequence, yt is the t-th token of y, and T is the length of the sequence, i.e. the number of tokens in y."
        },
        {
            "heading": "4 Experiments",
            "text": "For the first time, we benchmark the probabilistic uncertainty calibration methods. We use our proposed evaluation protocol, consisting of assessing the ROUGE quality improvements, measuring uncertainty calibration, which includes our newly proposed sequence-level Expected Calibration Error (ECE), assessing rank correlation and finally analysing selective generation via abstention as a measure of language model calibration. We evaluate all methods on three datasets: XSUM (Narayan et al., 2018), CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and RedditTIFUlong (Kim et al., 2019) due to their diversity in abstractiveness, lengths, domain and style (see Section A.6 for additional details). For all experiments we use beam search decoding, which was adapted to work with ensemble generation. We have also adapted SNGP and BE algorithms to work with the sequence generation and the corresponding loss (see Section A.7 for more details)."
        },
        {
            "heading": "4.1 ROUGE with Probabilistic Methods",
            "text": "We first study the effectiveness of different probabilistic methods on summary prediction by compar-\ning them with the baseline deterministic model. We use ROUGE-1/2/L (Lin, 2004) to measure general summarization quality. As shown in Table 1, we observe the consistent improvement of the ROUGE scores in probabilistic models compared to baselines. For single model methods, SNGP achieves the highest average ROUGE scores over the three datasets. Other probabilistic methods also show promising performance: SNGP+MCD is ranked the second top regarding ROUGE-1, and BE is ranked the second top regarding ROUGE-2 and the top regarding ROUGE-L. For multiple model methods, SNGP+DE improves over the deterministic DE. Comparing multiple model methods with single model methods, DE and SNGP+DE generally have higher ROUGE scores than single model methods."
        },
        {
            "heading": "4.2 Measuring Uncertainty Calibration in Summarization",
            "text": "We now study model\u2019s uncertainty calibration quality. We consider both the classic metric Expected\nCalibration Error (ECE), and also the uncertainty score\u2019s Spearman\u2019s rank correlation with domainspecific quality scores tailored for summarization (e.g., ROUGE).\nECE. In order to evaluate whether the model estimated probabilities have been more calibrated we assess the difference in expectation between confidence and accuracy using ECE metric (Naeini et al., 2015):\nECE = K\u2211 k=1 |Bk| n |conf(Bk)\u2212 acc(Bk)|,\nwhere we split the interval (0, 1] into K equalsize bins and define Bk to be the set containing the indices of examples which have predicted probability lie in the k-th bin: Bk ={ i|p\u0302i \u2208 ( k\u22121 K , k K ]} , where the average accuracy and confidence within each bin are defined as acc(Bk) = 1|Bk| \u2211 i\u2208Bk I(y\u0302i = yi) and conf(Bk) = 1 |Bk| \u2211 i\u2208Bk p\u0302i.\nIn auto-regressive prediction, y\u0302i can be a sequence or a token,3 which corresponds to two different metrics sequence-level ECE and token-level ECE respectively. When computing the sequencelevel ECE, we cast the problem into a binary classification task, where the probability p\u0302 of a predicted sequence is the production of probabilities of all tokens of that sequence. Regarding the token-level ECE, other work (e.g. Wang et al. (2020b)) uses translation edit rate (Snover et al., 2006) to relax the condition that the tokens under consideration need to be at the same position. In our work, we say that a predicted token is correct if it matches the target token at the same position in the target sequence.\n3During evaluation, we compute token probabilities from the highest scoring beam sequence.\nAs shown in Table 2, across all methods, SNGP+MCD and SNGP+DE generally leads to lower ECE in single model and multi-model methods respectively, suggesting SNGP helps to reduce ECE.\nRank Correlation with Quality Score. We investigate how the Spearman\u2019s rank correlation between the log-probabilities and ROUGE changes with calibration. Overall we see a general trend demonstrating the calibration increases the correlation, as shown in Figure 2. For the ROC-AUC scores please refer to the section A.1."
        },
        {
            "heading": "4.3 Selective Generation via Abstention",
            "text": "Selective generation refers to the procedure to selectively generate higher-quality outputs while abstain the low-quality outputs (Ren et al., 2022). It evaluates the models\u2019 uncertainty calibration, since\na well calibrated model is expected to have high uncertainty for low-quality outputs such it can be used to select examples to abstain. We use the ROUGE vs Abstention Curve 4 to compare methods: specifically, at a given abstention rate \u03b1, we remove the lowest \u03b1-fraction uncertain examples and compute the average quality of the remaining examples, as shown in Figure 1. For single model methods (solid lines), SNGP+MCD models have generally higher ROUGE scores in CNN/DM, and in regions of \u03b1 > 0.6 in XSUM and Reddit. For multi-model methods, SNGP+DE generally outperforms DE in all the datasets.\nFailure Patterns. When comparing multi-model methods with single model methods, we observe that XSUM and Reddit both have multi-model methods outperforming single model methods, but CNN/DM does not benefit from using multi-model methods. This difference can be explained by the fact that CNN/DM is an simpler task that is more extractive in nature, and a single model already performs well and relatively calibrated. In this case, using a deep ensemble can in fact lead to underconfidence (Rahaman et al., 2021). Furthermore, in Reddit dataset, MCD-family methods seem to lead to severe degradation of summarization quality. Note that Reddit is a more challenging task with much greater linguistic diversity when compared to XSUM and CNN/DailyMail, cautioning the use of MCD method in challenging test environments where a single model does not perform well."
        },
        {
            "heading": "5 Conclusion",
            "text": "We have adapted the most popular probabilistic calibration methods to the PLMs use-case and have conducted a novel and extensive study of the calibration effect on PLMs uncertainty and summarization quality. We proposed a novel evaluation protocol for the uncertainty estimation in PLMs. We demonstrated that probabilistic calibration methods can have a positive impact on the quality of generated summaries and increase reliability of the models. Our main findings can be summarized in two points:\n\u2022 When there is no time and memory constraints, our results point out that it is best to choose Deep Ensembles in combination\n4Different from the performance vs data retention curves in Filos et al. (2019), we employ log probability rather than predictive entropy as the metric for data retention.\nwith SNGP as an approach to language model calibration, since they are effective in terms of reducing the ECE and improving the quality of summarization.\n\u2022 Even when calibration methods appear to be effective in reducing ECE it may not necessarily suggest that they will be effective in improving ROUGE or other language generation quality metrics.\nWe hope to see our efforts making a significant contribution to the improvements of the reliability of PLMs, enabling future researchers to effectively leverage probabilistic calibration methods in the development and analysis of these models."
        },
        {
            "heading": "6 Limitations",
            "text": "In our paper we investigated the effect of most common and widely used probabilistic deep learning methods on Pre-trained Language Models. We observed a positive effect of calibration on the variety of metrics, however it is important to address the limitations of this work. Even though we observed the improvements on multiple metrics, we believe more work needs to be done to fully understanding the interplay between classic probabilistic deep learning methods that were traditionally applied in the context of classification and the unique set up the language models pose: autoregressive generation and mismatch between the learning and inference. Above mentioned unique properties of PLMs make it harder to align the model predictive probability with the distribution of the data. In our work we have done very basic adaptation of the existing methods to the PLM setting, but in future more work needs to be done to address these differences."
        },
        {
            "heading": "7 Ethical impact",
            "text": "Our work directly contributes to the topic of reliable deep learning. We believe our work should positively affect the scientific community, since we address one of the main problems that often occurs in the machine learning research: how to make the models more reliable and trustworthy. We hope that in long run we can arrive at a standardized benchmark set of techniques that can help the NLP community develop PLMs that are universally trustworthy."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 ROC-AUC scores",
            "text": "A.2 Spearman\u2019s rank correlation ROUGE-2 and ROUGE-L\nSpearman\u2019s rank correlation for the rest of the metrics can be found on Figure 3."
        },
        {
            "heading": "A.3 Abstention plots",
            "text": "We demonstrate the abstention plots for the rest of the metrics on Figure 4."
        },
        {
            "heading": "A.4 Experimental details",
            "text": "We ran all the experiments on the T5 base model (220 million parameters) using open-sourced T5X framework (Roberts et al., 2022). We used the TPU v3 chips (Jouppi et al., 2020) for all experiments. Reported metric results are collected from a single evaluation run, unless error bars are provided or\n5The values were selected as round numbers on the SoTA performance circa 2020, when NLG fluency was human-like: https://paperswithcode.com/sota/text-summarization-on-x-sum.\nstated otherwise. To select each model checkpoint we ran a hyperparameter sweep to find the best set of parameters. Parameters we sweeped over were: checkpoint step, leaning rate, SNGP mean field factor, number of checkpoints for the ensembles and number of training steps.\nIn all the experiments we use beam search (except where we stated \"nucleus sampling\") as a decoding method and we use beam_size= 3. For the MCD we used dropout_rate = 0.1 everywhere. Covariance matrix momentum in SNGP was set to 0.999. For the XSum the best mean field factor 10\u22124 and for CNN and Reddit it was 10\u22126."
        },
        {
            "heading": "A.5 Qualitative results",
            "text": ""
        },
        {
            "heading": "A.6 Dataset",
            "text": "XSUM (Narayan et al., 2018) consists of 227k BBC articles from 2010 to 2017 with a single sentence highly abstractive summary. The average length of each input document is 687 words, whereas the summary is 53 words long. Sometimes the summary contains information not present in the article (Maynez et al., 2020). CNN/DailyMail (Hermann et al., 2015; See et al., 2017) contains 313k articles from the CNN and Daily Mail newspapers with bullet point summaries. The summaries are on average 3-4 sentences and relatively extractive. The average length of each input document is 375 words, whereas the summary is 21 words long. RedditTIFU-long (Kim et al., 2019) contains 42k posts of informal stories from sub-reddit TIFU from 2013-Jan to 2018-Mar with author written summaries. The style and length of the summaries are very diverse. The average length of each input document is 433 words, whereas the summary is 23 words long."
        },
        {
            "heading": "A.7 Methods details",
            "text": "In this section, we delve into the methods, offering a comprehensive explanation of their individual characteristics and how their combined properties can best help in model calibration. Deep Ensembles, Monte Carlo dropout, SNGP, and Batch Ensemble are all techniques used in the field of machine learning to improve the performance and robustness of neural network models. While they share the goal of calibrating the model predictions, they differ in their underlying principles and methodologies. Let\u2019s discuss each method and highlight their fundamental differences.\nThe main idea behind Monte Carlo Dropout, Deep Ensembles and Batch Ensembles is to get an approximation of the predictive posterior of a model via computing a mean of token probabilities from several unique models. Once we have access M predictions we can compute the predictive posterior the\nfollowing way:\np\u0304(y|x,D) = Eq(\u03b8)[p(y|x,\u03b8)] \u2248 1\nM M\u2211 m=1 pm(y|x,\u03b8(m)),\u03b8(m) \u223c q(\u03b8) \u2248 p(\u03b8|D) (1)\nHere, \u03b8 are the model parameters, y is the model prediction and x is the model input (for simplicity we consider y to be a whole sentence instead of an individual token, that are dependent on the previous tokens). q(\u03b8) is the parameters prior distribution, in our case we used standard normal distribution to initialize the model parameters. Once we have an approximation of the model\u2019s predictive posterior we can estimate the expected uncertainty:\nu(p\u0304(y|x,D)) = H[p\u0304(y|x,D)] = Ep\u0304(y|x,D)[\u2212 ln p\u0304(y|x,D)] = \u2212 \u2211 y\u2208Y p\u0304(y|x,D) ln p\u0304(y|x,D) (2)\nThe uncertainty is essentially the Shannon entropy, and therefore can be easily calculated, once we have the values and probabilities across the whole space of possible outputs Y , in practice we compute everything at the token level, and Y just becomes the models\u2019 vocabulary. \u2022Monte Carlo Dropout (MCD) (Gal and Ghahramani, 2016) is a technique that leverages dropout regularization during training inference. Dropout is a regularization technique that randomly sets a fraction of neural network units to zero during training. During inference, MCD involves performing multiple forward passes with dropout enabled (but with different random seeds) and averaging the predictions (more precisely, the probabilities of all tokens in the vocabulary). By sampling multiple predictions, MCD provides a measure of model uncertainty or confidence in its predictions. In our case MCD estimates uncertainty using the Monte Carlo average of 10 dropout samples.\n\u2022 Batch Ensemble (BE) (Wen et al., 2020) - an ensemble method which has much lower computational costs comparing to MC Dropout and Deep Ensemble. Batch Ensemble is a technique that involves training multiple neural network models simultaneously within a single batch. Each model in the ensemble receives a different subset of the data batch, and the models share their weights at the end of each training iteration. By training models on different data subsets, Batch Ensemble encourages diversity and reduces overfitting. In the experiments, we replaced the last transformer\u2019s MLP block and the last dense layer by batch ensemble blocks with ensemble size be 5.\n\u2022 Deep Ensemble (DE) (Lakshminarayanan et al., 2017) which trains 10 deterministic models individually and averages all. Deep Ensembles involve training multiple neural network models independently and then combining their predictions to make a final prediction. Each model in the ensemble is typically trained with different initialization, architectures, or subsets of the training data. The main idea is that the diverse models capture different aspects of the data and, when combined, produce more accurate and robust predictions. Deep Ensembles are computationally expensive since they require training and storing multiple models.\nThe above mentioned methods are very good at increasing the representation diversity, i.e. learning the multiple modes of the underlying data distribution. However, when dealing with uncertainty it is also important that model has a distance awareness, i.e. the property of the model that allows it to quantify the distance of xtest and Xtrain in the input space ||xtest \u2212Xtrain||x. Typical Neural Networks are not distance aware. Spectral-normalized Neural Gaussian Process was proposed as a solution for this problem: \u2022 SNGP (Liu et al., 2020) - a recent state-of-the-art approach which improves uncertainty quality by transforming a neural network into an approximate Gaussian process model. The Gaussian Process last layer is able to reflect the distance between a test example and the training set, hence potentially be helpful in improving calibration. The model architecture includes a neural network that maps inputs to a lower-dimensional feature space, and a Gaussian process layer that models the target variable using the extracted features. We adapted SNGP to the sequence generation by letting all pre-logits (i.e. inputs of the last dense layer) in a sequence go through the same GP layer.\nIn summary, the fundamental differences among these techniques lie in their approaches to ensemble learning, uncertainty estimation, and distance awareness. Deep Ensembles train multiple models independently and combine their predictions, Monte Carlo dropout incorporates dropout during training and testing, Batch Ensemble trains multiple models within a single batch and shares their weights and SNGP combines neural networks with Gaussian processes. Each method offers unique advantages and can be employed based on the specific requirements of the problem at hand, our experiments showed that the best results can be achieved by combining the SNGP with Deep Ensembles, which gives us the best of both worlds, i.e. representation diversity and distance awareness. See Table 6 for the summaries of different methods properties.\nFinally, in order to compare the algorithms it is helpful to look at the total complexity:\n\u2022 SNGP, BE: time complexity O(T ), space complexity O(S),\n\u2022 MCD, SNGP+MCD: time complexity O(M \u00d7 T ), space complexity O(S),\n\u2022 DE, SNGP+DE: time complexity O(M \u00d7 T ), space complexity O(M \u00d7 S),\nwhere T and S are time and space complexity of the base model and M is the ensemble size. To reduce the memory required in DE models, we compute the predictions (i.e. the probabilities of all tokens in the vocabulary) of each model sequentially before averaging them. For that reason, the DE models have a time complexity O(M \u00d7 T ). The actual time and memory required for each method depend on the TPU topology, and they roughly align with the time and space complexity mentioned above."
        }
    ],
    "title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study",
    "year": 2023
}