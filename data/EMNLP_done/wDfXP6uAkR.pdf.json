{
    "abstractText": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within taskoriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT\u2019s capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closedsource nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code1 is provided for reproducibility.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yujie Feng"
        },
        {
            "affiliations": [],
            "name": "Zexin Lu"
        },
        {
            "affiliations": [],
            "name": "Bo Liu"
        },
        {
            "affiliations": [],
            "name": "Liming Zhan"
        },
        {
            "affiliations": [],
            "name": "Xiao-Ming Wu"
        }
    ],
    "id": "SP:ea4cc48172fc2be7d5a5eb65e2616da4ba0573fc",
    "references": [
        {
            "authors": [
                "Namo Bang",
                "Jeehyun Lee",
                "Myoung-Wan Koo"
            ],
            "title": "2023a. Task-optimized adapters for an end-to-end task-oriented dialogue system",
            "year": 2023
        },
        {
            "authors": [
                "Namo Bang",
                "Jeehyun Lee",
                "Myoung-Wan Koo."
            ],
            "title": "Task-optimized adapters for an end-toend task-oriented dialogue system",
            "venue": "arXiv preprint arXiv:2305.02468.",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Yihan Cao",
                "Siyu Li",
                "Yixin Liu",
                "Zhiling Yan",
                "Yutong Dai",
                "Philip S Yu",
                "Lichao Sun."
            ],
            "title": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
            "venue": "arXiv preprint arXiv:2303.04226.",
            "year": 2023
        },
        {
            "authors": [
                "Derek Chen",
                "Kun Qian",
                "Zhou Yu."
            ],
            "title": "Stabilized in-context learning with pre-trained language models for few shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2302.05932.",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Yue Feng",
                "Aldo Lipani",
                "Fanghua Ye",
                "Qiang Zhang",
                "Emine Yilmaz."
            ],
            "title": "Dynamic schema graph fusion network for multi-domain dialogue state tracking",
            "venue": "arXiv preprint arXiv:2204.06677.",
            "year": 2022
        },
        {
            "authors": [
                "Yue Feng",
                "Yang Wang",
                "Hang Li."
            ],
            "title": "A sequenceto-sequence approach to dialogue state tracking",
            "venue": "arXiv preprint arXiv:2011.09553.",
            "year": 2020
        },
        {
            "authors": [
                "Yujie Feng",
                "Jiangtao Wang",
                "Yasha Wang",
                "Xu Chu."
            ],
            "title": "Spatial-attention and demographicaugmented generative adversarial imputation network for population health data reconstruction",
            "venue": "IEEE Transactions on Big Data.",
            "year": 2022
        },
        {
            "authors": [
                "Yujie Feng",
                "Jiangtao Wang",
                "Yasha Wang",
                "Xu Chu."
            ],
            "title": "Towards sustainable compressive population health: A gan-based year-by-year imputation method",
            "venue": "ACM Transactions on Computing for Healthcare, 4(1):1\u201318.",
            "year": 2023
        },
        {
            "authors": [
                "Yujie Feng",
                "Jiangtao Wang",
                "Yasha Wang",
                "Sumi Helal."
            ],
            "title": "Completing missing prevalence rates for multiple chronic diseases by jointly leveraging both intra-and inter-disease population health data correlations",
            "venue": "Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Jun Gao",
                "Huan Zhao",
                "Changlong Yu",
                "Ruifeng Xu."
            ],
            "title": "Exploring the feasibility of chatgpt for event extraction",
            "venue": "arXiv preprint arXiv:2303.03836.",
            "year": 2023
        },
        {
            "authors": [
                "Jinyu Guo",
                "Kai Shuang",
                "Jijie Li",
                "Zihan Wang",
                "Yixuan Liu."
            ],
            "title": "Beyond the granularity: Multiperspective dialogue collaborative selection for dialogue state tracking",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jinyu Guo",
                "Kai Shuang",
                "Jijie Li",
                "Zihan Wang",
                "Yixuan Liu."
            ],
            "title": "Beyond the granularity: Multi-perspective dialogue collaborative selection for dialogue state tracking",
            "venue": "arXiv preprint arXiv:2205.10059.",
            "year": 2022
        },
        {
            "authors": [
                "Ting Han",
                "Ximing Liu",
                "Ryuichi Takanabu",
                "Yixin Lian",
                "Chongxuan Huang",
                "Dazhen Wan",
                "Wei Peng",
                "Minlie Huang"
            ],
            "title": "Multiwoz 2.3: A multi-domain taskoriented dialogue dataset enhanced with annotation corrections and co-reference annotation",
            "venue": "Natural",
            "year": 2021
        },
        {
            "authors": [
                "Michael Heck",
                "Nurul Lubis",
                "Benjamin Ruppik",
                "Renato Vukovic",
                "Shutong Feng",
                "Christian Geishauser",
                "Hsien-Chin Lin",
                "Carel van Niekerk",
                "Milica Ga\u0161i\u0107"
            ],
            "title": "Chatgpt for zero-shot dialogue state tracking: A solution or an opportunity",
            "year": 2023
        },
        {
            "authors": [
                "Michael Heck",
                "Carel van Niekerk",
                "Nurul Lubis",
                "Christian Geishauser",
                "Hsien-Chin Lin",
                "Marco Moresi",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "Trippy: A triple copy strategy for value independent neural dialog state tracking",
            "venue": "arXiv preprint arXiv:2005.02877.",
            "year": 2020
        },
        {
            "authors": [
                "Ehsan Hosseini-Asl",
                "Bryan McCann",
                "Chien-Sheng Wu",
                "Semih Yavuz",
                "Richard Socher."
            ],
            "title": "A simple language model for task-oriented dialogue",
            "venue": "Advances in Neural Information Processing Systems, 33:20179\u2013 20191.",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A Smith",
                "Mari Ostendorf."
            ],
            "title": "In-context learning for few-shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2203.08568.",
            "year": 2022
        },
        {
            "authors": [
                "Minlie Huang",
                "Xiaoyan Zhu",
                "Jianfeng Gao."
            ],
            "title": "Challenges in building intelligent open-domain dialog systems",
            "venue": "ACM Transactions on Information Systems (TOIS), 38(3):1\u201332.",
            "year": 2020
        },
        {
            "authors": [
                "Vojt\u011bch Hude\u010dek",
                "Ond\u0159ej Du\u0161ek"
            ],
            "title": "Are llms all you need for task-oriented dialogue? arXiv preprint arXiv:2304.06556",
            "year": 2023
        },
        {
            "authors": [
                "WX Jiao",
                "WX Wang",
                "JT Huang",
                "Xing Wang",
                "ZP Tu."
            ],
            "title": "Is chatgpt a good translator? yes with gpt-4 as the engine",
            "venue": "arXiv preprint arXiv:2301.08745.",
            "year": 2023
        },
        {
            "authors": [
                "Chia-Hsuan Lee",
                "Hao Cheng",
                "Mari Ostendorf."
            ],
            "title": "Dialogue state tracking with a language model using schema-driven prompting",
            "venue": "arXiv preprint arXiv:2109.07506.",
            "year": 2021
        },
        {
            "authors": [
                "Hwaran Lee",
                "Jinsik Lee",
                "Tae-Yoon Kim."
            ],
            "title": "Sumbt: Slot-utterance matching for universal and scalable belief tracking",
            "venue": "arXiv preprint arXiv:1907.07421.",
            "year": 2019
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Bing Liu",
                "Andrea Madotto",
                "Seungwhan Moon",
                "Paul Crook",
                "Zhenpeng Zhou",
                "Zhiguang Wang",
                "Zhou Yu",
                "Eunjoon Cho",
                "Rajen Subba"
            ],
            "title": "2021a. Zero-shot dialogue state tracking via cross-task transfer",
            "venue": "arXiv preprint arXiv:2109.04655",
            "year": 2021
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Bing Liu",
                "Seungwhan Moon",
                "Paul Crook",
                "Zhenpeng Zhou",
                "Zhiguang Wang",
                "Zhou Yu",
                "Andrea Madotto",
                "Eunjoon Cho",
                "Rajen Subba."
            ],
            "title": "Leveraging slot descriptions for zero-shot cross-domain dialogue state tracking",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Bo Liu",
                "Liming Zhan",
                "Zexin Lu",
                "Yujie Feng",
                "Lei Xue",
                "Xiao-Ming Wu"
            ],
            "title": "How good are large language models at out-of-distribution detection? arXiv preprint arXiv:2308.10261",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Ptuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602.",
            "year": 2021
        },
        {
            "authors": [
                "Mingyu Derek Ma",
                "Jiun-Yu Kao",
                "Shuyang Gao",
                "Arpit Gupta",
                "Di Jin",
                "Tagyoung Chung",
                "Nanyun Peng."
            ],
            "title": "Parameter-efficient low-resource dialogue state tracking by prompt tuning",
            "venue": "arXiv preprint arXiv:2301.10915.",
            "year": 2023
        },
        {
            "authors": [
                "Yue Ma",
                "Zengfeng Zeng",
                "Dawei Zhu",
                "Xuan Li",
                "Yiying Yang",
                "Xiaoyuan Yao",
                "Kaijie Zhou",
                "Jianping Shen."
            ],
            "title": "An end-to-end dialogue state tracking system with machine reading comprehension and wide & deep classification",
            "venue": "arXiv preprint",
            "year": 2019
        },
        {
            "authors": [
                "Jarana Manotumruksa",
                "Jeffrey Dalton",
                "Edgar Meij",
                "Emine Yilmaz."
            ],
            "title": "Similarity-based multi-domain dialogue state tracking with copy mechanisms for task-based virtual personal assistants",
            "venue": "Proceedings of the ACM Web Conference 2022, pages 2006\u20132014.",
            "year": 2022
        },
        {
            "authors": [
                "Wenbo Pan",
                "Qiguang Chen",
                "Xiao Xu",
                "Wanxiang Che",
                "Libo Qin."
            ],
            "title": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
            "venue": "arXiv preprint arXiv:2304.04256.",
            "year": 2023
        },
        {
            "authors": [
                "Gao Qixiang",
                "Guanting Dong",
                "Yutao Mou",
                "Liwen Wang",
                "Chen Zeng",
                "Daichi Guo",
                "Mingyang Sun",
                "Weiran Xu."
            ],
            "title": "Exploiting domain-slot related keywords description for few-shot cross-domain dialogue state tracking",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Lohith Ravuru",
                "Seonghan Ryu",
                "Hyungtak Choi",
                "Haehun Yang",
                "Hyeonmok Ko."
            ],
            "title": "Multi-domain dialogue state tracking by neural-retrieval augmentation",
            "venue": "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 169\u2013175.",
            "year": 2022
        },
        {
            "authors": [
                "Jamin Shin",
                "Hangyeol Yu",
                "Hyeongdon Moon",
                "Andrea Madotto",
                "Juneyoung Park."
            ],
            "title": "Dialogue summaries as dialogue states (ds2), template-guided summarization for few-shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2203.01552.",
            "year": 2022
        },
        {
            "authors": [
                "Zhoujian Sun",
                "Zhengxing Huang",
                "Nai Ding."
            ],
            "title": "On tracking dialogue state by inheriting slot values in mentioned slot pools",
            "venue": "arXiv preprint arXiv:2202.07156.",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Tan",
                "Dehai Min",
                "Yu Li",
                "Wenbo Li",
                "Nan Hu",
                "Yongrui Chen",
                "Guilin Qi."
            ],
            "title": "Evaluation of chatgpt as a question answering system for answering complex questions",
            "venue": "arXiv preprint arXiv:2303.07992.",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto."
            ],
            "title": "Alpaca: A strong, replicable instruction-following model",
            "venue": "Stanford Center for Research on Foundation Models.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Haoming Wang",
                "Wang Xin."
            ],
            "title": "How to stop an avalanche? jodem: Joint decision making through compare and contrast for dialog state tracking",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7030\u20137041.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Crosslingual summarization via chatgpt",
            "venue": "arXiv preprint arXiv:2302.14229.",
            "year": 2023
        },
        {
            "authors": [
                "Qingyue Wang",
                "Yanan Cao",
                "Piji Li",
                "Yanhe Fu",
                "Zheng Lin",
                "Li Guo."
            ],
            "title": "Slot dependency modeling for zero-shot cross-domain dialogue state tracking",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 510\u2013520.",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Wang",
                "Jing Zhao",
                "Junwei Bao",
                "Chaoqun Duan",
                "Youzheng Wu",
                "Xiaodong He."
            ],
            "title": "LUNA: Learning slot-turn alignment for dialogue state tracking",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Anjana Arunkumar",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Atharva Naik",
                "David Stap"
            ],
            "title": "Benchmarking generalization via in-context",
            "year": 2022
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Andrea Madotto",
                "Ehsan HosseiniAsl",
                "Caiming Xiong",
                "Richard Socher",
                "Pascale Fung."
            ],
            "title": "Transferable multi-domain state generator for task-oriented dialogue systems",
            "venue": "arXiv preprint arXiv:1905.08743.",
            "year": 2019
        },
        {
            "authors": [
                "Hongyan Xie",
                "Haoxiang Su",
                "Shuangyong Song",
                "Hao Huang",
                "Bo Zou",
                "Kun Deng",
                "Jianghua Lin",
                "Zhihui Zhang",
                "Xiaodong He"
            ],
            "title": "Correctable-dst: Mitigating historical context mismatch between training and inference for improved dialogue state",
            "year": 2022
        },
        {
            "authors": [
                "Jing Xu",
                "Dandan Song",
                "Chong Liu",
                "Siu Cheung Hui",
                "Fei Li",
                "Qiang Ju",
                "Xiaonan He",
                "Jian Xie."
            ],
            "title": "Dialogue state distillation network with inter-slot contrastive learning for dialogue state tracking",
            "venue": "arXiv preprint arXiv:2302.08220.",
            "year": 2023
        },
        {
            "authors": [
                "Yongxin Xu",
                "Kai Yang",
                "Chaohe Zhang",
                "Peinie Zou",
                "Zhiyuan Wang",
                "Hongxin Ding",
                "Junfeng Zhao",
                "Yasha Wang",
                "Bing Xie."
            ],
            "title": "Vecocare: Visit sequences-clinical notes joint learning for diagnosis prediction in healthcare data",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "Puhai Yang",
                "Heyan Huang",
                "Wei Wei",
                "Xian-Ling Mao."
            ],
            "title": "Toward real-life dialogue state tracking involving negative feedback utterances",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2222\u20132232.",
            "year": 2022
        },
        {
            "authors": [
                "Xianjun Yang",
                "Yan Li",
                "Xinlu Zhang",
                "Haifeng Chen",
                "Wei Cheng."
            ],
            "title": "Exploring the limits of chatgpt for query or aspect-based text summarization",
            "venue": "arXiv preprint arXiv:2302.08081.",
            "year": 2023
        },
        {
            "authors": [
                "Yuting Yang",
                "Wenqiang Lei",
                "Pei Huang",
                "Juan Cao",
                "Jintao Li",
                "Tat-Seng Chua."
            ],
            "title": "A dual prompt learning framework for few-shot dialogue state tracking",
            "venue": "Proceedings of the ACM Web Conference 2023, pages 1468\u20131477.",
            "year": 2023
        },
        {
            "authors": [
                "Fanghua Ye",
                "Jarana Manotumruksa",
                "Emine Yilmaz"
            ],
            "title": "2022a. MultiWOZ 2.4: A multi-domain taskoriented dialogue dataset with essential annotation corrections to improve state tracking evaluation",
            "venue": "In Proceedings of the 23rd Annual Meeting of the Spe-",
            "year": 2022
        },
        {
            "authors": [
                "Fanghua Ye",
                "Xi Wang",
                "Jie Huang",
                "Shenghui Li",
                "Samuel Stern",
                "Emine Yilmaz."
            ],
            "title": "Metaassist: Robust dialogue state tracking with meta learning",
            "venue": "arXiv preprint arXiv:2210.12397.",
            "year": 2022
        },
        {
            "authors": [
                "Dian Yu",
                "Mingqiu Wang",
                "Yuan Cao",
                "Laurent El Shafey",
                "Izhak Shafran",
                "Hagen Soltau."
            ],
            "title": "Knowledgegrounded dialog state tracking",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3428\u20133435, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoxue Zang",
                "Abhinav Rastogi",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Jianguo Zhang",
                "Jindong Chen"
            ],
            "title": "MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines",
            "year": 2020
        },
        {
            "authors": [
                "Jian-Guo Zhang",
                "Kazuma Hashimoto",
                "Chien-Sheng Wu",
                "Yao Wan",
                "Philip S Yu",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking",
            "venue": "arXiv preprint arXiv:1910.03544.",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey Zhao",
                "Raghav Gupta",
                "Yuan Cao",
                "Dian Yu",
                "Mingqiu Wang",
                "Harrison Lee",
                "Abhinav Rastogi",
                "Izhak Shafran",
                "Yonghui Wu."
            ],
            "title": "Descriptiondriven task-oriented dialog modeling",
            "venue": "arXiv preprint arXiv:2201.08904.",
            "year": 2022
        },
        {
            "authors": [
                "Ce Zhou",
                "Qian Li",
                "Chen Li",
                "Jun Yu",
                "Yixin Liu",
                "Guangjing Wang",
                "Kai Zhang",
                "Cheng Ji",
                "Qiben Yan",
                "Lifang He"
            ],
            "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
            "venue": "arXiv preprint arXiv:2302.09419",
            "year": 2023
        },
        {
            "authors": [
                "Yihao Zhou",
                "Guoshuai Zhao",
                "Xueming Qian."
            ],
            "title": "Dialogue state tracking based on hierarchical slot attention and contrastive learning",
            "venue": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 4737\u20134741.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Task-oriented dialogue systems have emerged as powerful tools for assisting users in accomplishing a wide range of tasks (Huang et al., 2020). These systems, such as Apple Siri and Microsoft Cortana, function as virtual personal assistants, providing support for tasks like flight reservations, appointment scheduling, and hotel bookings. Dialogue State Tracking (DST) plays a crucial role in taskoriented dialogue systems by accurately tracking\n\u2217Corresponding author. 1https://github.com/WoodScene/LDST\nthe evolving user goals and system actions during a conversation. In general, the multi-domain dialogue state is represented as a list of triplets in the form of (domain, slot, value), e.g., \u201c<restaurant, area, east>\u201d. These predefined slot pairs are extracted from the dialogue context at each turn.\nA plethora of models have been proposed to address the challenges of multi-domain DST, as documented in recent studies (Qixiang et al., 2022; Zhou et al., 2022; Feng et al., 2022b; Guo et al., 2022a; Yang et al., 2022; Ma et al., 2023; Xu et al., 2023a). These models primarily focus on effective transfer and generalization across diverse domains, addressing the crucial challenges of co-reference (Feng et al., 2022a) and error propagation problem (Wang and Xin, 2022) depicted in Figure 1. The co-reference challenge poses a significant hurdle in enhancing DST performance, as it arises from the linguistic variations in multi-turn dialogues where slots and values are often indirectly\nexpressed. Moreover, the error propagation issue emerges when the model fails to recognize and rectify errors in the previously predicted dialogue state, leading to the persistence of errors in subsequent turns. Despite significant efforts to address these issues, they persist as ongoing challenges.\nIn recent days, the emergence of large-scale pretrained language models has revolutionized the field of natural language processing (NLP). Models like ChatGPT2 have shown excellent performance, sparking significant interest in evaluating their effectiveness across different dimensions (Tan et al., 2023; Wang et al., 2023; Jiao et al., 2023; Yang et al., 2023a; Gao et al., 2023; Liu et al., 2023). Despite the significant advancements made by large language models (LLMs), their performance in multi-domain DST remains relatively unexplored. To bridge this research gap, we conduct an evaluation of ChatGPT\u2019s capabilities for DST. The evaluation unveils ChatGPT\u2019s exceptional performance in the DST task, offering valuable insights to researchers and providing useful directions for further exploration.\nWhile ChatGPT demonstrates superb performance, it has significant limitations (Zhou et al., 2023; Yang et al., 2023a; Cao et al., 2023). Firstly, it is not open source, so the underlying code and model parameters cannot be modified by users. Second, it is subject to request limitations, which can restrict its usage in high-demand scenarios. Furthermore, there are concerns regarding strong data privacy protection, as the system may collect and store user data. Lastly, ChatGPT cannot be deployed locally, limiting its availability and control. These limitations hinder the applicability and adoption of ChatGPT in various practical scenarios for building task-oriented dialogue systems.\nTo overcome the limitations of ChatGPT, we introduce LDST, a DST framework driven by LLMs but based on smaller, open-source foundation models. LDST employs a novel assembled domainslot instruction tuning method and a parameter efficient tuning technique, enabling it to achieve performance comparable to ChatGPT while utilizing a much smaller model and limited computational resources. LDST demonstrates exceptional performance across three different experimental settings, surpassing prior state-of-the-art methods by a large margin and demonstrating its remarkable adaptability and generalization capabilities. Our main\n2https://chat.openai.com\ncontributions are concluded as follows:\n\u2022 We present the first evaluation of ChatGPT in DST task, highlighting its superior performance over prior methods and providing valuable insights for advancing dialogue systems.\n\u2022 We propose LLM-driven DST (LDST) based on smaller, open-source foundation models. LDST achieves comparable performance to ChatGPT by employing an innovative assembled domain-slot instruction tuning technique.\n\u2022 We extensively evaluate LDST on three benchmark datasets across various experimental settings, revealing significant performance improvements over previous approaches. In the zero-shot scenario, LDST boosts the JGA score by 16.9%, elevating it from 65.3% to an outstanding 82.2%. In the few-shot scenario, LDST improves the JGA score by 7.5%, raising it from 47.7% to a notable 55.2%."
        },
        {
            "heading": "2 Assessing the Capabilities of ChatGPT for DST",
            "text": "In this section, we evaluate the effectiveness of ChatGPT in addressing the DST task. Before going into detail, we first formally define the problem.\nDST: Problem Formulation In task-oriented dialogue systems, a dialogue with T turns of conversations between the system and the user can be represented as {(A1, U1) , (A2, U2) . . . , (AT , UT )}, where A represents system response and U represents user input. A predefined slot set S = {S1, . . . , SJ} is given, where J is the total number of slots. The dialogue context at turn t includes previous turns of interactions, denoted as Xt = {(A1, U1) , (A2, U2) . . . , (At, Ut)}. The dialogue state at turn t is represented as a set of (slot, value) pairs, denoted as Bt = {( S1, V t 1 ) , . . . , ( SJ , V t J )} , where V tJ is the value of slot SJ . For multi-domain DST, following previous works (Lee et al., 2019), a slot is defined as the concatenation of the specific domain and the slot, e.g., \u201c<restaurant-area>\u201d. If no information is provided in the dialogue about a specific slot, the value associated with that slot is set to \u201cNONE\u201d. Essentially, the DST problem is defined as learning a dialogue state tracker F : Xt \u2192 Bt.\nLeveraging ChatGPT for DST We evaluate the performance of ChatGPT (using the gpt-3.5-turbo API service) on three multi-domain DST benchmarks, using the JGA and AGA evaluation metrics\n(for detailed descriptions of the datasets and metrics, refer to Section 4.1). As shown in Figure 2, we explore various prompt templates and utilize the MultiWOZ 2.2 dataset (Zang et al., 2020) to select the optimal prompt.\nIn Figure 2, \u201csingle return\u201d and \u201cmulti return\u201d refer to the number of slot values returned in each ChatGPT API request. \u201cSingle return\u201d involves requesting and receiving values for one slot at a time, while \u201cmulti return\u201d entails requesting and receiving values for all slots simultaneously. For instance, in the MultiWOZ 2.2 dataset which has 49 different slots, \u201cmulti return\u201d retrieves values for all 49 slots in a single request. This causes a significant increase API requests for \"single return\" but simplifies the model\u2019s task, resulting in improved performance. Conversely, \u201cmulti return\u201d reduces API requests but increases token count per request. \"No/one demo\" denotes whether an example is provided in the prompt as a demonstration to aid the model\u2019s comprehension of the task. Selecting \"one demo\" is similar to adopting the in-context learning concept. Detailed prompt template design is provided in the Appendix A.1.\nPerformance of ChatGPT As can be seen from Figure 2, the first prompt, which retrieves the value of a single slot in each request without including a demo in the input, achieves the highest AGA score. This is attributed to the inherent difficulty of the task that necessitates the model to provide multiple slot values in a single request. We have observed that ChatGPT tends to predict \u201cNONE\u201d for slots that should have a specific value. For instance, in the case of the slot \u201chotel-leaveat\u201d where\nthe expected value is \u201c14:00\u201d, ChatGPT may incorrectly predict \u201cNONE\u201d, resulting in lower prediction accuracy. Secondly, the addition of a demo to the input has a reduced effect, which may seem counter-intuitive. However, our analysis of the error results suggests that ChatGPT also analyzes the dialogue context within the demo, even when the demo and tested sample are clearly differentiated in the input. Therefore, we chose the first prompt as the best template for the subsequent evaluation.\nThe full evaluation results of ChatGPT on the three datasets3 are shown in Figure 3. Firstly, on the SGD dataset, the AGA score of ChatGPT is significantly superior than the previous SOTA method (Feng et al., 2022a), and it achieves a 7.46% ab-\n3The evaluation of the MultiWOZ 2.2 dataset were conducted between April 15th and 18th, 2023. The evaluations of MultiWOZ 2.4 occurred between June 10th and 12th, 2023. The SGD was assessed between June 14th and 17th, 2023.\nsolute imporvement in AGA score. In addition, the average improvement on the three datasets is 0.73% in JGA score and 2.34% in AGA score. Secondly, ChatGPT\u2019s performance on the MultiWOZ 2.2 dataset is slightly worse than the previous SOTA method (Bang et al., 2023a). However, through careful analysis of the errors, we found that 70% of them were due to annotation errors in the original dataset. Thus, on the MultiWOZ 2.4 dataset which has fixed the annotation errors, ChatGPT outperforms the best baseline method (Ye et al., 2022a).\nLimitations of ChatGPT In summary, ChatGPT exhibits comparable performance when solving the DST task compared to the previous SOTA methods. This highlights the ability of current LLMs to capture and comprehend complex linguistic patterns and dependencies within multi-turn dialogues. However, ChatGPT has several significant limitations that impede its applicability and adoption in various practical scenarios. Firstly, we observed that ChatGPT often provides responses with a significant amount of explanatory content, or it may not align perfectly with our expected answer format. For instance, when the ground truth value is \u201c2 pm,\u201d ChatGPT might return \u201c14:00.\u201d While both are essentially correct answers, such variations can affect the accuracy of the final metrics. And ChatGPT is not open source, which restricts the ability of developers and researchers to modify and customize the model. Secondly, ChatGPT is subject to request limitations, which may impact real-time or high-volume applications that rely on prompt and efficient responses. Furthermore, ChatGPT operates in a cloud-based environment and lacks strong data privacy protection measures, which raises concerns about the privacy and security of sensitive information shared during the dialogue sessions. Lastly, ChatGPT relies on an internet connection for operation and cannot be deployed locally."
        },
        {
            "heading": "3 Fine-tuning Smaller Foundation Models with Instructions for DST",
            "text": "To overcome the aforementioned limitations of ChatGPT, we introduce LDST, an LLMdriven DST framework that leverages fine-tuning smaller, open-source foundation models such as LLaMa (Touvron et al., 2023) with instructions specially tailored for DST. We first outline the process of constructing an instruction dataset for the multi-domain DST task. Next, we utilize a\nparameter-efficient fine-tuning (PEFT) technique to train the foundation model with the instruction dataset. PEFT enables the training of a foundation model with limited computational resources."
        },
        {
            "heading": "3.1 Instruction Tuning",
            "text": "Unlike prompt tuning, instruction tuning (Chung et al., 2022) provides more explicit and detailed guidance to the model through task-specific instructions. This allows for finer control over the model\u2019s behavior and leads to improved performance compared to prompt tuning. The core idea of instruction tuning is designing the instruction dataset, typically including instruction, input, and output fields. Usually, different instructions are assigned for different tasks. However, employing a fixed instruction template for multi-domain DST may limit the model\u2019s robustness, as emphasized by Wang et al. (2023), which highlights the crucial influence of prompt design on model performance.\nTo address this challenge, we propose a novel Assembled Domain-Slot Instruction Generation approach for the DST task. This approach generates diverse instruction samples by randomly combining different instruction and input templates, exposing the model to a rich variety of instruction types during the fine-tuning process to reduce the model\u2019s sensitivity to prompts. As shown by the provided example in Figure 4, for each sample in the original dataset, it consists of the dialogue context Xt at turn t, the current requested slot SJ and its corresponding state V tJ . The raw data is then passed through the Instruction Data Generation module to generate instruction samples. The detailed template settings for each field are introduced as follows.\nInstruction Prompt Specifically, two types of instruction templates are defined: (1) Standard Slot Tracking Instruction and (2) Customized Slot Tracking Instruction. The difference between them is that the Customized Slot Tracking Instruction provides a more specific domain-slot information. And the instruction field of each sample is randomly selected from these two templates.\nInput Prompt For the input field, the prompt template is composed of four main parts: (1) the dialogue context, (2) domain-slot description prompt, (3) Possible Value List (PVL) prompt and (4) the query prompt. The green, purple, blue and orange text in the example in Figure 4 refers to these four prompts respectively. In particular, we concatenate all sub-sequences with special segment tokens,\nsuch as the \u201c[USER]\u201d segment token used to indicate the start of a system utterance. And both the domain-slot description prompt and the PVL prompt are supplementary descriptions of the requested slot, they all derive from the given schema in original dataset (PVL prompts are only available for categorical slots). Here, to simulate the situation when the model may not get a description of the requested slot or it\u2019s possible values during the testing phase, we add these two prompt templates randomly with a 50% probability, respectively.\nOuput Prompt Finally, the output field consists of the corresponding value V tJ of the requested slot SJ . By following the aforementioned process, we obtained a newly and diverse instruction dataset for the next step of fine-tuning the model."
        },
        {
            "heading": "3.2 Parameter Efficient Tuning",
            "text": "In this part, we describe how to fine-tune the foundation model using a parameter efficient approach. LDST takes the instruction and input field from the dataset as inputs and retrieves the corresponding slot value V tJ as output:\nV tJ = Decoder(X\u0302 ) (1)\nwhere Decoder indicates that the foundation model (e.g., LLaMa) uses the Transformer-decoder framework, and X\u0302 denotes the instruction data, i.e., the combination of instruction and input fields.\nAs shown in Figure 4, to enhance the efficiency of the fine-tuning process and reduce memory requirements, we utilize Low-Rank Adaptation (LoRA) (Hu et al., 2021). LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Trans-\nformer architecture, greatly reducing the number of trainable parameters for downstream tasks. For example, in our experiment with LLaMa 7B, the number of learnable parameters is 8.4M, which is only 0.12% of the total model parameters. Denote by the trainable parameters as a weight matrix W0 \u2208 Rd\u00d7k. Unlike traditional methods that directly modify the values of W0, LoRA introduces an additional set of trainable parameters, denoted as \u2206W , which are directly injected into the original W0. We represent the update as W = W0 +\u2206W = W0 +BA, where B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7k. The rank r \u226a min(d, k). During training, W0 is frozen and does not receive any gradient updates, we only update the parameters in A and B. Note both W0 and \u2206W = BA are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For the original output h = W0x, LoRA modified forward pass yields:\nh = W0x+\u2206Wx = W0x+BAx. (2)\nFinally, the learning objective of the generation process in LDST is to minimize the negative loglikelihood of V tJ given the context Xt and slot SJ :\nL = \u2212 T\u2211 t J\u2211 j log p ( V tj | Xt,Sj ) . (3)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conducted experiments using the benchmark datasets for multi-domain task-oriented dialogue, and Table 1 gives detailed statics on these datasets.\nSchema-Guided Dialogue (SGD) SGD (Rastogi et al., 2020) is the most challenging dataset, consisting of over 16,000 conversations between a humanuser and a virtual assistant. It encompasses 26 services across 16 domains, such as events, restaurants, and media. Notably, SGD introduces unseen domains in its test set, challenging the generalization ability of the model.\nMultiWOZ 2.2 and MultiWOZ 2.4 MultiWOZ 2.4 (Ye et al., 2022a) is an updated version on top of MultiWOZ 2.2 (Zang et al., 2020) to improve DST evaluation, and the validation set and test set of MultiWOZ 2.4 have been carefully reannotated. We therefore treat it as a clean dataset for testing. We also conduct experiments on MultiWOZ 2.2 which is known to contain annotation noise. We used this noisy dataset to test the robustness of the model and to analyse the ability of the model to detect annotation errors present in the test set."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "We adopt the following evaluation metrics, consistent with previous works (Ye et al., 2022b): Joint Goal Accuracy (JGA) and Average Goal Accuracy (AGA). JGA serves as the primary metric for DST evaluation and represents the ratio of dialogue turns for which the entire state is correctly predicted. AGA is the average accuracy of the active slots in each turn. A slot becomes active if its value is mentioned in the current turn and is not inherited from previous turns."
        },
        {
            "heading": "4.3 Main Results",
            "text": "We conducted full-scale evaluations of the LLMdriven LDST model in three distinct experimental settings, where the model showed a tremendous performance improvement in both zero-shot and few-shot settings. These findings can provide valuable insights and contribute to the research community through substantial advances in the field of DST. The detailed results are as follows:\nZero-shot Cross-domain Results Following previous zero-shot settings (Wang et al., 2022c), all models are first trained on some domains and then evaluated on the test-set of the unseen domain. Here we compare with the baseline models that can predict dialogue state on unseen domains: SGDbaseline (Rastogi et al., 2020), TransferQA (Lin et al., 2021a), SDM-DST (Wang et al., 2022a), SUMBT (Lee et al., 2019), SimpleTOD (HosseiniAsl et al., 2020), T5DST (Lin et al., 2021b) and D3ST method (Zhao et al., 2022).\nTables 2 and 3 highlight the exceptional performance of our approach in zero-shot cross-domain DST. Specifically, on the SGD dataset, LDST achieves a remarkable 6.0% absolute increase in the JGA score when compared to the larger T5-XXL (11B)-based D3ST model, elevating it from 83.3% to an impressive 89.3%. Additionally, the AGA score experiences a substantial surge of 17.6%, escalating from 76.8% to a remarkable 94.4%.\nOn the MultiWOZ 2.0 dataset, we observe a significant advancement in the average JGA score, surging from 47.38% to 75.03%, reflecting an absolute improvement of 27.65%. Notably, the Payment domain in the SGD dataset displays the most prominent improvement, with the JGA metric soaring from 24.7% to an astounding 92.3%. This remarkable enhancement is attributed to the Payment domain\u2019s independence from the source domains. This significant boost not only demonstrates the powerful transfer learning capabilities of the LDST model but also emphasizes its valuable implications for the DST research community.\nFew-shot Results In the few-shot settings, we follow the multi-domain scenario from Wu et al.\n(2019), where randomly select 1%, 5%, and 10% of training dialogues to train, and evaluation is conducted on the full test set of each domain. The evaluation results on MultiWOZ 2.4 are shown in Table 4, where we compare with SOTA few-shot DST methods: DS2-BART (Shin et al., 2022), DS2T5 (Shin et al., 2022), IC-DST GPT-Neo (Hu et al., 2022), and SM2-11b (Chen et al., 2023).\nThe results indicate a clear trend: as the amount of training data increases, the performance of all models consistently improves. Notably, our LDST model stands out in this setting. At the 10% data setting, it achieved significant performance gains by elevating the JGA metric from 51.97% to 62.45%, marking an impressive 10.48% absolute improvement. Even at the 5% data setting, our approach surpassed traditional methods that were using 10% of the data. This highlights LDST\u2019s remarkable capacity to excel in learning and capturing the core aspects of the task with a smaller dataset."
        },
        {
            "heading": "Results of Fine-tuning with Full Training Data",
            "text": "We also evaluate the performance of LDST using the complete training data, and compare it with the following strong baselines, including SGDbaseline (Rastogi et al., 2020), TRADE (Wu et al., 2019), DS-DST (Zhang et al., 2019), TripPy (Heck et al., 2020), Seq2Seq-DU (Feng et al., 2020), MetaASSIST (Ye et al., 2022b), SDP-DST (Lee et al., 2021), TOATOD (Bang et al., 2023b), DiCoSDST (Guo et al., 2022b), D3ST (Zhao et al., 2022), paDST (Ma et al., 2019). And the results are shown on Table 5.\nWe initially note significant advancements in recent LLMs like ChatGPT and LLaMa. Notably, our model achieves competitive performance with ChatGPT and even surpasses it on the SGD dataset, particularly excelling in the AGA metric with a score exceeding 98%.\nThe paDST method has currently achieved\nSOTA performance on the SGD dataset (with a JGA score of 86.5%), surpassing LDST\u2019s 84.47%. However, it\u2019s important to note that paDST relies on additional techniques, which contain backtranslation between English and Chinese for data augmentation and special manual rules for model predictions. In contrast, LDST relies solely on the default SGD dataset without additional aids. Another SOTA method is D3ST, which uses T5XXL as backbone model with 11B parameters (our LDST utilizes a 7B model, for outcomes based on different foundational models and different model sizes, please consult Appendix B). D3ST surpasses LDST on the SGD dataset. However, it\u2019s noteworthy that LDST outperforms D3ST on Multiwoz 2.2 and 2.4. Additionally, our model demonstrates improved effectiveness when compared to the LLaMa backbone model, underscoring the ongoing importance of fine-tuning LLMs in current research.\nResults of Cross-dataset Transfer We further performed experiments to assess cross-dataset transfer capabilities, akin to the experiment outlined in Table 4c by Zhao et al. (2022). The JGA results are presented in Table 6, highlighting LDST\u2019s exceptional cross-dataset transfer abilities. When compared to the D3ST method, LDST showcases an average improvement of 2.7% in terms of JGA."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To validate the effectiveness of the assembled domain-slot instruction tuning, we conducted a comparison with traditional instruction tuning, which employs a fixed prompt template containing all the descriptions for the requested slot (see details in Appendix A.2). The results, as displayed in Table 7, clearly demonstrate that our method outperforms traditional instruction tuning. We observed a substantial 2.15% improvement in the JGA score and a 1.06% improvement in the AGA score.\nNext, to analyse the sensitivity of the model to different prompts during the testing phase. we designed six different prompts and evaluated their effects, the results are shown in Figure 5. The results clearly indicate that LDST demonstrates significantly higher accuracy and lower variance in test results compared to the other two baseline methods. The mean variance of our method is 0.04, in contrast to 0.78 for the LLaMa model, representing a substantial decrease of 0.74. These findings highlight that the utilization of the assembled technique for instruction tuning effectively reduces the model\u2019s sensitivity to prompts. This not only provides a more stable and efficient inference process but also enhances overall robustness."
        },
        {
            "heading": "4.5 Error Analysis",
            "text": "We analyze the types of incorrect predictions in LDST by using the 2835 incorrectly predicted samples on MultiWOZ 2.4. Firstly, 45.72% of the\nerrors are related to the values \u201cdontcare\u201d and \u201cnot mentioned\u201d. For example, in cases where the ground truth is \u201cdontcare\u201d, the model predicts \u201cnot mentioned\u201d, and vice versa. Among all 37 slots, the top five with highest error rates are \u201chotel-type\u201d (338 errors), \u201crestaurant-name\u201d (290 errors), \u201chotel area\u201d (225 errors), \u201chotel name\u201d (221 errors), and \u201cattraction name\u201d (205 errors), collectively accounting for 45.11% of the total errors. Specifically, for the \"hotel-type\" slot, 78.10% of the errors are attributed to the model frequently confusing \u201cnot mentioned\u201d with the value \u201chotel\u201d. For instance, the correct value for \u201chotel-type\u201d was \u201chotel\u201d, but the model incorrectly predicted as \u201cnot mentioned\u201d."
        },
        {
            "heading": "4.6 Effectiveness of LDST in Addressing the Main Challenges of DST",
            "text": "For the co-reference challenge, we analyze the MultiWOZ 2.3 dataset (Han et al., 2021), which includes 253 test samples annotated with coreference relationships. Our evaluation reveals that the best baseline method achieves an accuracy rate of 91.1%, whereas LDST model achieves an impressive accuracy rate of 96.4%, showcasing the significant improvement offered by our approach.\nAdditionally, we visualize the JGA score for each dialogue turn in Figure 6 to demonstrate the effectiveness in addressing error propagation. The result clearly shows that as the number of dialogue turns increases, the performance of all methods experiences a decline. However, our LDST model demonstrates a remarkable resilience to error propagation, showcasing a significantly slower decline compared to LLaMa and the best baseline method. These results emphasize the LDST model\u2019s capacity to capture and comprehend complex linguistic patterns and dependencies in multi-round conversations, making it a promising solution to mitigate the challenges associated with the DST task."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 Multi-Domain Dialogue State Tracking",
            "text": "Recent studies in multi-domain DST have extensively utilized the pre-trained language models to achieve high-performance results (Ravuru et al., 2022; Yu et al., 2022; Sun et al., 2022; Feng et al., 2021; Wang et al., 2022b; Xu et al., 2023c). For example, Xie et al. (2022) proposed a multi-stage correctable dialogue state tracking method to mitigate the error propagation phenomenon, while Wang and Xin (2022) proposed a jointly decision making and a dialogue update technique to prevent error accumulation. In addition, Wang et al. (2022a) and Manotumruksa et al. (2022) solve the challenge of co-referencing by learning correlations between slots, for example, by using a combination of slot prompts or hard copy mechanism. However, these approaches still have limitations, such as the lack of robustness in handling complex dialogue contexts and the difficulty in capturing fine-grained semantic relationships between slots and values."
        },
        {
            "heading": "5.2 LLMs for Dialogue State Tracking",
            "text": "While large language models such as GPT-3 (Brown et al., 2020) and T5 (Raffel et al., 2020) have gained significant popularity, the efficient utilization of these models remains a significant challenge. Recent advancements in parameter-efficient fine-tuning (PEFT) techniques have effectively alleviated this problem, such as LoRA(Hu et al., 2021) and Prefix Tuning(Liu et al., 2021). For instance, both Lee et al. (2021) and Yang et al. (2023b) proposed a prompt-tuning method that leverages domain-specific prompts and context information to improve the performance of DST task. Meanwhile, Ma et al. (2023) and Chen et al. (2023) introduced the prefix tuning approach, which involves modifying the input prompt by adding specific tokens at the beginning of the dialogue, aiming to enhance the efficiency of model fine-tuning. However, these methods still face challenges, where the effectiveness heavily depends on prompt design.\nRecently, Heck et al. (2023) exclusively tested ChatGPT\u2019s performance on the Multiwoz 2.1 dataset. In contrast, our evaluation covers the Multiwoz 2.2, 2.4, and SGD datasets, providing a more comprehensive assessment. While both Pan et al. (2023) and Hudec\u030cek and Du\u0161ek (2023) included results on the Multiwoz 2.2, Multiwoz 2.4, and SGD datasets, their results were relatively lower due to their use of the text-davinci-003 API. In contrast,\nwe utilized the latest gpt-3.5-turbo API, a highly capable GPT-3.5 model optimized for chat at a fraction of the cost. Consequently, we achieved new SOTA performance with ChatGPT, showcasing its significant strengths.\nWith the emergence of open-source large language models, such as LLaMa (Touvron et al., 2023), it provides a series of higher-quality backbone models with different parameters. Leveraging LLaMa and the technique of instruction tuning has proven to achieve better results (Taori et al., 2023), opening new avenues for our research."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this study, we conduct an initial examination of ChatGPT\u2019s capabilities in multi-domain DST, showcasing its superiority over previous methods. This comprehensive evaluation provides useful directions for researchers to design and enhance dialogue systems. To solve the limitations of ChatGPT, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel assembled domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings demonstrate the remarkable performance improvements achieved by LDST compared to previous methods."
        },
        {
            "heading": "Limitations",
            "text": "This work has two main limitations: (1) Subjectivity in prompt design: Prompt engineering has shown significant potential for the application of LLMs. However, the prompt designs used in our study are subjective and may not necessarily represent the optimal choices. Consequently, the effectiveness of using these prompts for model finetuning or testing may not always yield the best results. Exploring more systematic automated techniques for prompt design could enhance the overall performance of the model. (2) Input length constraints: In our study, we set the input length of the model to 512, which was determined based on statistical analysis and already contains more than 90% of the samples. While it is possible to increase the input length, doing so may result in slower training and inference times. Additionally, when the dialogue or description content becomes too long, the challenge of effectively truncating or summarizing the input arises. Further investigation\ninto handling longer input sequences without compromising model efficiency would be beneficial."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their valuable feedback and constructive comments, which greatly contributed to improve the quality of this work. This research was partially supported by the grant of HK ITF ITS/359/21FP."
        },
        {
            "heading": "A Description of Prompt Templates",
            "text": ""
        },
        {
            "heading": "A.1 Prompt Templates for ChatGPT Request",
            "text": "Initially, we noticed that the results reported in the studies by Pan et al. (2023); Hudec\u030cek and Du\u0161ek (2023) were notably lower in comparison to our results. We attribute this observation to two primary factors, as outlined below."
        },
        {
            "heading": "Mitigating the Generation of Excessively",
            "text": "Lengthy Responses ChatGPT often generated answers with excessively detailed explanations, deviating from the expected response format. For example, when asked about the \"train-leaveat\" slot, ChatGPT might respond with extensive information such as \"Monday at 05:16 for the first train and Monday at 16:16 for the last train,\" whereas the correct response should be simply \"05:16.\" To address this issue, we introduced a prompt that includes \"No explanation!\" as an instruction to ChatGPT not to provide detailed explanations. Experimental results indicated a significant improvement in answer accuracy through this approach.\nAPI Version Differences Another factor is the utilization of different API versions. The prior works all relied on the text-davinci-003 API, while we utilized a more powerful gpt-3.5-turbo API, a highly capable GPT-3.5 model optimized for chat at a fraction of the cost.\nBelow we provide specific samples for the four different prompts in Figure 2. Prompt type 1: \u201csingle return\u201d + \u201cno demo\u201d { \u201cinstruction\u201d: Now you need to perform the task of multi-domain dialogue state tracking. You need to return the value of the slot I\u2019m asking about simply based\non the content of the dialogue. No explanation!\n\u201cinput\u201d: Input dialogue: [USER] I would like to find a salon. [SYSTEM] In which city do you prefer the salon to be located? [USER] In SFO will be great. [domain] Hotels_2, it indicates A popular service for searching and booking houses for short term stay [slot] number_of_adults, it indicates Number of people for the reservation. This slot is categorical and you can only choose from the following available values: 1, 2, 3,"
        },
        {
            "heading": "4, 5. If the slot is not mentioned in the dialogue, just return NONE.",
            "text": "So the value of slot <Hotels_2-number_of_adults> is\n}\nPrompt type 2: \u201cmulti return\u201d + \u201cno demo\u201d { \u201cinstruction\u201d: Now you need to perform the task of dialogue state tracking. And the slot schema is in this list [restaurant-area, hotel-name, attraction-name, ...(the remaining slots are omitted here)], which is in a domain-slot format. You need to return the slot and its value in dict format if the value is not none, and no explanation!\n\u201cinput\u201d: Input dialogue: [USER] I would like to find a salon. [SYSTEM] In which city do you prefer the salon to be located? [USER] In SFO will be great.\nPlease return the value of slot list [restaurant-area, hotel-name, attraction-name, ...(the remaining slots are omitted here)].\n}\nPrompt type 3: \u201csingle return\u201d + \u201cone demo\u201d { \u201cinstruction\u201d: Now you need to perform the task of multi-domain dialogue state tracking. And I will show you an example and you need to return to the state of the slot I asked about.\n\u201cinput\u201d: The example is: Input dialogue: [User]: I need train reservations from norwich to cambridge [SYSTEM]: I have 133 trains matching your request. ...\nSo the value of slot <train-departure> is\nAnd your result should be Norwich.\nThe following is the dialogue you need to test:\nInput dialogue: [USER] I would like to find a salon. [SYSTEM] In which city do you prefer the salon to be located? [USER] In SFO will be great. [domain] Hotels_2, it indicates A popular service for searching and booking houses for short term stay [slot] number_of_adults, it indicates Number of people for the reservation. This slot is categorical and you can only choose from the following available values: 1, 2, 3, 4, 5. If the slot is not mentioned in the dialogue, just return NONE. So the value of slot <Hotels_2-number_of_adults> is\n}\nPrompt type 4: \u201cmulti return\u201d + \u201cone demo\u201d { \u201cinstruction\u201d: Now you need to perform the task of multi-domain dialogue state tracking. And I will show you an example and you need to return the answer strictly in the format of the example.\n\u201cinput\u201d: The example is: Input dialogue: [User]: I need train reservations from norwich to cambridge [SYSTEM]: I have 133 trains matching your request. ...\nOutput result: Train-Departure: Norwich, Train-Arrival: Cambridge, ...(the remaining slots are omitted here)\nAnd you need to test this example:\nInput dialogue: [USER] I would like to find a salon. [SYSTEM] In which city do you prefer the salon to be located? [USER]"
        },
        {
            "heading": "In SFO will be great.",
            "text": "Please return the value of slot list [restaurant-area, hotel-name, attraction-name, ...(the remaining slots are omitted here)].\n} For practical reasons related to API request costs, we conducted tests using these four prompt templates exclusively on the MultiWOZ 2.2 dataset. Subsequent evaluations on the MultiWOZ 2.4 and SGD datasets focused on the most effective tem-\nplate, i.e., \u201csingle return\u201d + \u201cno demo.\u201d"
        },
        {
            "heading": "A.2 Prompt Template for \u201cTraditional\u201d Instruction Tuning",
            "text": "Here, we present the template for traditional instruction tuning, where \"traditional\" implies the application of instruction tuning directly to the DST task with a fixed prompt template. It\u2019s important to highlight that this fixed prompt template includes all slot descriptions, such as the domain-slot description and the list of potential values. This fixed prompt is utilized during both the fine-tuning and testing phases. { \u201cinstruction\u201d: Track the state of the slot <restaurant-area> in the input dialogue.\n\u201cinput\u201d: [Dialogue context omitted...] [Domain] restaurant, [Slot] area, it indicates the area or place of the restaurant. This slot is categorical, and you can only choose from the following available values: north, east, south, west. If the slot is not mentioned in the dialogue, just return NONE. So the value of slot <restaurant-area> is\n\u201coutput\u201d: north }"
        },
        {
            "heading": "B Additional Results",
            "text": ""
        },
        {
            "heading": "B.1 Comparison of ChatGPT with Zero-shot Methods",
            "text": "Essentially, the evaluation of ChatGPT inherently belongs to the zero-shot setting. However, since we found that ChatGPT\u2019s results have been comparable to traditional fine-tuning methods, we put its results in Table 5 in the paper. Additionally, we introduce ChatGPT\u2019s results from zero-shot settings and the results are shown in table 8 and 9 below.\nThe results clearly demonstrate that ChatGPT outperforms the two strong baselines, SDM-DST and T5DST, by a huge margin. This is primarily\nbecause the evaluation is conducted in a zero-shot environment, where ChatGPT inherently holds an advantage. It\u2019s important to note that as an API service, ChatGPT cannot be tuned offline and is exclusively used for testing purposes.\nIn the zero-shot setting, the performance of traditional methods (e.g., SDM-DST and T5DST) is worse due to the lack of domain-specific training data. ChatGPT, equipped with its extensive model size and rich pre-trained knowledge, dramatically surpasses the performance of traditional methods and sets the upper bound of performance in the zero-shot setting. It\u2019s also worth mentioning that ChatGPT\u2019s performance approaches the results of traditional methods fine-tuned on the complete training dataset, which is why we include it in Table 5 for comparison\nIn contrast, our LDST, utilizing a customized instruction tuning method, effectively approaches ChatGPT\u2019s performance in the zero-shot setting, with an average performance difference of 2.4% in the JGA score."
        },
        {
            "heading": "B.2 Results with Different Foundation Models",
            "text": "We further performed evaluations using an alternative foundation model, Llama2-7B (Touvron et al., 2023), as depicted in Table 10 below.\nThe results show that LDST_LLaMa2 performed the best on SGD, attaining a JGA of 85.1% and demonstrating a performance akin to that of ChatGPT on MultiWOZ 2.4. It suggests that a stronger backbone can lead to better DST performance.\nB.3 Results with LLaMa of different sizes In order to investigate how model size influences performance, we have incorporated supplementary experimental findings involving the LLaMa-13B and -30B models on the SGD dataset. These results are presented in Table 11 below.\nThe results provide a clear indication that an increase in model size corresponds to an improvement in the JGA score. However, in practical applications, a 7B model not only offers a more suitable fit for local deployment but also showcases impressive performance.\nB.4 Inference Time Analysis The table 12 below provides the results of inference time. It\u2019s worth highlighting that we employ 8-bit quantization for the LLMs, which leads to slower inference times compared to standard 32-bit configurations.\nT5 large is the backbone model of the SDP-DST baseline method. From the table, it\u2019s clear that the inference speed decreases as the model size increases. For example, LDST_LLaMa-7B predicts an average of 90 samples per minute. When compared to the baseline method based on T5-large (770M), the speed of LDST is approximately onesixth that of the baseline."
        },
        {
            "heading": "B.5 Effect of LoRA Configurations",
            "text": "In our work, we utilized common configurations: lora_r = 8 and lora target modules=[query_proj, key_proj, value_proj, output_proj] in each selfattention module that needs to be updated.\nTo further clarify the impact of LoRA configurations on the experimental results, we performed additional analysis on the Multiwoz 2.4 dataset using 1% training set (To save training time, we set the epoch to 1). We varied the lora_r parameter with values of 1, 2, 4, 8, and 16. In addition, we experimented with two different lora_target_modules configurations: [q_proj, v_proj] and [q_proj, k_proj, v_proj, o_proj]. This resulted in 10 distinct experimental setups.\nIn these results, a smaller value of \u201clora_r\u201d indicates fewer LoRA parameters, while lora target modules determines which modules receive LoRA update matrices. Generally, updating more attention matrices yields better results, and performance improves as \u201clora_r\u201d increases. However, it\u2019s essential to note that higher \u201clora_r\u201d values might extend the model\u2019s training time. Hence, selecting appropriate hyperparameters is crucial."
        },
        {
            "heading": "B.6 Results on MultiWOZ 2.1 Dataset",
            "text": "For a comprehensive evaluation, refer to Table 14, which presents the results on the MultiWOZ 2.1 dataset, comparing ChatGPT by Heck et al. (2023) with the D3ST method by Zhao et al. (2022).\nThe results reveal that LDST\u2019s performance is marginally below that of D3ST. This could be attributed to potential noise in the test set annotations, mirroring our observations regarding the MultiWOZ 2.2 dataset.\nC Implementation Details"
        },
        {
            "heading": "C.1 Data Preprocessing and Evaluation",
            "text": "Step 1 - Standard Preprocessing In line with the approach used by Lee et al. (2021), this initial step involves the extraction of dialogue content and slot-value pairs from the raw data. For instance,\nconsider the dialogue labeled \"PMUL4398.json\" in the Multiwoz 2.2 training dataset. It comprises 6 dialogue turns between the system and the user. With Multiwoz 2.2 featuring 49 unique slots, this dialogue yields 294 (6*49) training data samples. Here is a specific example: { \u201cdialogue\u201d: [SYSTEM] What can I help you with [USER] i need a place to dine in the center thats expensive [SYSTEM] I have several options for you; do you prefer African, Asian, or British food? [USER] Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation? [domain] restaurant find places to dine and whet your appetite [slot] area area or place of the restaurant [Possible Values] centre, east, north, south, west\n\u201cstate\u201d: centre\n} In this example, the \u201cdialogue\u201d field includes the content of the dialogue (A1, U1), (A2, U2), the tracked slot <restaurant-area>, and it\u2019s description. The \u201cstate\u201d field is the value of the corresponding slot. For the slots that are not mentioned in the dialogue, the \u201cstate\u201d field is assigned to NONE.\nStep 2 - Instruction Data Generation While the preprocessing in Step 1 provided valuable data, it didn\u2019t entirely align with the required format for instruction tuning. As a result, it led to suboptimal experimental performance. To address this, we introduced an additional preprocessing stage known as the \"Instruction Data Generation Module,\" as depicted in Figure 4. This module is designed to construct more suitable prompts.\nThe aforementioned details the entirety of the preprocessing procedure, after which it can be leveraged for both model training and testing.\nEvaluation Regarding evaluation, we also utilized the code provided by Lee et al. (2021) to calculate JGA and AGA scores. A prediction was considered correct when it exactly matches the ground truth. During the testing phase, we used a consistent prompt template that included domain-slot descriptions and lists of potential values. Experimental findings showed that this template slightly outperformed others because of its provision of more comprehensive slot information."
        },
        {
            "heading": "C.2 Experimental Settings",
            "text": "During the training phase, we utilized a batch size of 128 and set the learning rate to 1e-4. The number of epochs varied depending on the specific experiment setup. For zero-shot experiments, we trained for 3 epochs. For few-shot experiments, we conducted experiments with different percentages of labeled data: 1% few-shot experiments were trained for 10 epochs, 5% few-shot experiments for 3 epochs, and 10% few-shot experiments for 2 epochs. For fine-tuning on the full dataset experiments, we used 2 epochs.\nThe model\u2019s cutoff length was set at 512, based on data analysis. This length was determined to be optimal as it covered more than 90% of the data. For samples with input lengths exceeding 512 tokens, we truncated them to fit within the cutoff length. Additionally, the parameter \"train_on_inputs\" was set to false, indicating that the model solely computed the loss based on the final output.\nRegarding the hyperparameters of the LORA module, we set the lora rank to 8, alpha to 16, dropout rate to 0.05, and selected \u201cq_proj\u201d, \u201ck_proj\u201d, \u201cv_proj\u201d and \u201co_proj\u201d as the LORA target modules. Furthermore, in order to reduce the memory usage of the model, we employed 8-bit quantization techniques to further optimize the finetuning process.\nWe would also like to offer further insights into the training time comparison of our model. In experiments involving fine-tuning on the full dataset, our model had an average training time of 8 hours. In contrast, powerful baseline methods, such as SDP-DST (Feng et al., 2023) and DiCoS-DST (Xu et al., 2023b), required approximately 60 hours for fine-tuning the T5 model based on our testing. This substantial difference in training time underscores the efficiency of our approach. And for the TOATOD (Bang et al., 2023b) method, which also utilizes the PEFT technique, the fine-tuning process only focuses on soft prompts, reducing the overall runtime to 12 hours. This runtime is comparable to our method, demonstrating the effectiveness of our approach compared to traditional methods.\nIn the case of few-shot experiments, the training time for 1% labeled data was 5 hours, 5% labeled data required 8 hours, and 10% labeled data took approximately 10 hours to train. In contrast, the runtime for zero-shot experiments averaged around 12 hours. It\u2019s worth noting that our ap-\nproach did not exhibit significant runtime improvements compared to traditional methods in these settings. However, it does illustrate that our LLMdriven approach achieves the most substantial performance improvements while still maintaining efficiency. These additional insights into the model\u2019s runtime in various experimental setups provide a comprehensive understanding of the time required for training our model and its comparison to other baseline methods."
        }
    ],
    "title": "Towards LLM-driven Dialogue State Tracking",
    "year": 2023
}