{
    "abstractText": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexis Chevalier"
        },
        {
            "affiliations": [],
            "name": "Alexander Wettig"
        },
        {
            "affiliations": [],
            "name": "Anirudh Ajith"
        },
        {
            "affiliations": [],
            "name": "Danqi Chen"
        }
    ],
    "id": "SP:cbddd05338eb0c66dd96d418e2adf8e426ee5673",
    "references": [
        {
            "authors": [
                "Joshua Ainslie",
                "Tao Lei",
                "Michiel de Jong",
                "Santiago Onta\u00f1\u00f3n",
                "Siddhartha Brahma",
                "Yury Zemlyanskiy",
                "David Uthus",
                "Mandy Guo",
                "James Lee-Thorp",
                "Yi Tay",
                "YunHsuan Sung",
                "Sumit Sanghai"
            ],
            "title": "CoLT5: Faster long-range transformers with conditional",
            "year": 2023
        },
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma"
            ],
            "title": "A general language assistant as a laboratory",
            "year": 2021
        },
        {
            "authors": [
                "Vidhisha Balachandran",
                "Bhuwan Dhingra",
                "Haitian Sun",
                "Michael Collins",
                "William Cohen."
            ],
            "title": "Investigating the effect of background knowledge on natural questions",
            "venue": "Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowl-",
            "year": 2021
        },
        {
            "authors": [
                "Luisa Bentivogli",
                "Peter Clark",
                "Ido Dagan",
                "Danilo Giampiccolo."
            ],
            "title": "The fifth PASCAL recognizing textual entailment challenge",
            "venue": "TAC.",
            "year": 2009
        },
        {
            "authors": [
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Jordan Hoffmann",
                "Trevor Cai",
                "Eliza Rutherford",
                "Katie Millican",
                "George Bm Van Den Driessche",
                "Jean-Baptiste Lespiau",
                "Bogdan Damoc",
                "Aidan Clark"
            ],
            "title": "Improving language models by retrieving from tril",
            "year": 2022
        },
        {
            "authors": [
                "Aydar Bulatov",
                "Yuri Kuratov",
                "Mikhail Burtsev."
            ],
            "title": "Recurrent memory transformer",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Tianqi Chen",
                "Bing Xu",
                "Chiyuan Zhang",
                "Carlos Guestrin."
            ],
            "title": "Training deep nets with sublinear memory cost",
            "venue": "arXiv preprint arXiv:1604.06174.",
            "year": 2016
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever."
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509.",
            "year": 2019
        },
        {
            "authors": [
                "Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and",
            "year": 2019
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini."
            ],
            "title": "The PASCAL recognising textual entailment challenge",
            "venue": "the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and",
            "year": 2005
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov."
            ],
            "title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Tri Dao",
                "Daniel Y Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher Re."
            ],
            "title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Marie-Catherine de Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The CommitmentBank: Investigating projection in naturally occurring discourse",
            "venue": "Proceedings of Sinn und Bedeutung, 23(2):107\u2013124.",
            "year": 2019
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The Pile: An 800GB dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "R Bar Haim",
                "Ido Dagan",
                "Bill Dolan",
                "Lisa Ferro",
                "Danilo Giampiccolo",
                "Bernardo Magnini",
                "Idan Szpektor."
            ],
            "title": "The second pascal recognising textual entailment challenge",
            "venue": "Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual",
            "year": 2006
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Snigdha Chaturvedi",
                "Michael Roth",
                "Shyam Upadhyay",
                "Dan Roth."
            ],
            "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
            "venue": "Proceedings of the 2018 Conference of the North American Chap-",
            "year": 2018
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations (ICLR), San Diega, CA, USA.",
            "year": 2015
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012, Proceedings of the International Conference on Knowl-",
            "year": 2012
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Vladislav Lialin",
                "Vijeta Deshpande",
                "Anna Rumshisky."
            ],
            "title": "Scaling down to scale up: A guide to parameter-efficient fine-tuning",
            "venue": "arXiv preprint arXiv:2303.15647.",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Chunting Zhou",
                "Xiang Kong",
                "Junxian He",
                "Liangke Gui",
                "Graham Neubig",
                "Jonathan May",
                "Luke Zettlemoyer."
            ],
            "title": "Mega: moving average equipped gated attention",
            "venue": "arXiv preprint arXiv:2209.10655.",
            "year": 2022
        },
        {
            "authors": [
                "Jesse Mu",
                "Xiang Lisa Li",
                "Noah Goodman."
            ],
            "title": "Learning to compress prompts with gist tokens",
            "venue": "arXiv preprint arXiv:2304.08467.",
            "year": 2023
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity",
            "venue": "Proceedings of ACL, pages 271\u2013278.",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of ACL, pages 115\u2013124.",
            "year": 2005
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados."
            ],
            "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Ofir Press",
                "Noah Smith",
                "Mike Lewis."
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jack W. Rae",
                "Anna Potapenko",
                "Siddhant M. Jayakumar",
                "Chloe Hillier",
                "Timothy P. Lillicrap."
            ],
            "title": "Compressive transformers for long-range sequence modelling",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S Gordon."
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.",
            "year": 2011
        },
        {
            "authors": [
                "Azhar",
                "Hugo Touvron",
                "Louis Martin",
                "Nicolas Usunier",
                "Thomas Scialom",
                "Gabriel Synnaeve"
            ],
            "title": "Code llama: Open foundation models for code",
            "year": 2023
        },
        {
            "authors": [
                "Devendra Sachan",
                "Mike Lewis",
                "Mandar Joshi",
                "Armen Aghajanyan",
                "Wen-tau Yih",
                "Joelle Pineau",
                "Luke Zettlemoyer."
            ],
            "title": "Improving passage retrieval with zero-shot question generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "REPLUG: Retrievalaugmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Charlie Snell",
                "Dan Klein",
                "Ruiqi Zhong."
            ],
            "title": "Learning by distilling context",
            "venue": "arXiv preprint arXiv:2209.15189.",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Donald Metzler."
            ],
            "title": "Efficient transformers: A survey",
            "venue": "ACM Comput. Surv., 55(6).",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "LLaMA: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "David Wingate",
                "Mohammad Shoeybi",
                "Taylor Sorensen."
            ],
            "title": "Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Lin Zheng",
                "Chong Wang",
                "Lingpeng Kong."
            ],
            "title": "Linear complexity randomized self-attention mechanism",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages",
            "year": 2022
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen."
            ],
            "title": "Factual probing is [MASK]: Learning vs",
            "venue": "learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Tao Lei",
                "Danqi Chen."
            ],
            "title": "Training language models with memory augmentation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657\u20135673, Abu Dhabi, United Arab Emirates. As-",
            "year": 2022
        },
        {
            "authors": [
                "Shi Ce"
            ],
            "title": "Deaflympic career as a Table tennis player including 11 gold medals. Shi Ce was eligible to compete at the National Games of China despite her deafness",
            "year": 2015
        },
        {
            "authors": [
                "Johnny Galecki"
            ],
            "title": "revel\u00f3 que abandon\u00f3 la escuela a mediados del octavo grado luego de a\u00f1os de evitar ir a clases a toda costa. Le dijo a Time Out, \"Una vez que las divisiones largas aparecieron en tercer grado, iba al ba\u00f1o por 45 minutos y nadie lo notaba, todos los d\u00edas a la misma hora del d\u00eda, s\u00f3lo para escapar de ellas.\" Puede que Galecki no tenga un cerebro matem\u00e1tico, pero siempre tuvo inteligencia callejera. \"El conocimiento es el mejor y m\u00e1s",
            "year": 2009
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Transformer-based (Vaswani et al., 2017) language models (LMs) have recently seen a sharp rise in popularity and are now receiving millions of queries, processing billions of tokens, and generating text for a wide variety of applications (Brown et al., 2020; Touvron et al., 2023; Zhang et al.,\n*AC and AW contributed equally. This work was done when AC was at the Institute for Advanced Study and visited the Princeton NLP group.\n1Our code and models are publicly available at https://github.com/princeton-nlp/AutoCompressors.\n2022). With this rise in popularity comes the challenge for researchers to make LMs more efficient, to speed up inference and to deploy LMs at scale, while increasing their versatility, thus allowing users to process more data in new ways.\nWith these goals in mind, we propose to teach pre-trained LMs the ability to compress text into summary vectors. Summary vectors are short soft prompts (Lester et al., 2021), one or two orders of magnitude shorter than the pre-compressed plain text, that are obtained from the output states of a language model. Summary vectors serve two general purposes: they can help extend the language model\u2019s context window to very long documents with minimal computational overhead, and they help speed up inference on text for which summary vectors have been pre-computed and cached.\nOur models, which we call AutoCompressors, are trained with a simple unsupervised learning objective that encourages the model to store essential information in the summary vectors. Summary vectors are produced segment by segment from long documents and are used to improve language modeling in future segments (Figure 1). Our work\nbuilds on the recently proposed RMT architecture (Bulatov et al., 2022) with a crucial difference: we introduce summary accumulation, in which summary vectors from all segments are concatenated to produce the summary of the entire document. We also train AutoCompressors with randomly segmented inputs so they can better compress contexts of variable lengths in downstream tasks. We show that these innovations improve long-range information retention and enable new ways of reasoning over multiple passages.\nAutoCompressors can be initialized with pretrained LMs to produce powerful and versatile models. We fine-tune AutoCompressors from OPT2.7B (Zhang et al., 2022) and Llama-2-7B (Touvron et al., 2023) models on sequences from 6,144 up to 30,720 tokens with a single NVIDIA A100 GPU of 80GB memory. We show that summary vectors are effective for improving perplexity over long documents and that these compression capabilities are robust to domain generalization. Our analysis suggests that AutoCompressors are able to reason over summary vectors, making them useful for a diverse set of downstream applications.\nWe apply AutoCompressors to in-context learning (ICL) by compressing up to 90 in-context demonstrations. We consider 11 classification tasks, including 7 SuperGLUE tasks (Wang et al., 2019), and we find that summary vectors outperform few-shot ICL with a comparable number of in-context tokens on 8 out of 11 tasks.\nFinally, we explore two applications where AutoCompressors can reduce inference costs by pre-computing summary vectors for large corpora. First, we adopt a setting for retrieval-augmented language modeling (Shi et al., 2023). We find that for equal sequence lengths, using summary vectors achieves 1.5\u00d7 the perplexity gains compared to plain-text passages, and outperforms retrievalaugmented methods for similar computational budgets. Secondly, we consider a zero-shot passage re-ranking task (Sachan et al., 2022). We establish that re-ranking passages based on their summary vectors achieves the best trade-off between re-ranking performance and inference throughput.\nIn summary, our main contributions are the following: (1) We introduce a method for extending LMs to long context windows under small-scale computational requirements by learning to generate summary vectors. We propose summary accumulation and training with randomized segmenting as\nkey features of AutoCompressors. (2) We show that summary vectors encode useful information for downstream tasks and can be used to reduce the inference cost of in-context learning. (3) We demonstrate the benefits of pre-computing summary vectors for large corpora and using AutoCompressors in conjunction with retrievers."
        },
        {
            "heading": "2 Related Work",
            "text": "Soft prompts Soft prompt tuning is an effective method to adapt pre-trained Transformers without updating existing parameters (Lester et al., 2021; Zhong et al., 2021; Liu et al., 2022). Newly initialized embeddings are prepended to the input sequence (the \u201csoft prompt\u201d), and optimization is performed with respect to these new parameters while the rest of the model is frozen. It is one of many parameter-efficient fine-tuning methods (Lialin et al., 2023) and is related to prefix tuning, where newly initialized parameters are prepended to the attention states instead (Li and Liang, 2021). Prompt compression Wingate et al. (2022) propose to learn a soft prompt \u03c3 to compress the information contained in a context x. Given a pretrained language model pLM, they draw continuations y \u223c pLM(\u00b7 | x) based on x and use a distillation objective to align the model\u2019s predictions conditioned on the soft prompt pLM(y | \u03c3) to the predictions conditioned on the context pLM(y | x). Wingate et al. (2022) find that soft prompts retain high-level information and facilitate controllable generation. However, the approach requires running the optimization for every new context x, with no knowledge transfer between similar contexts. In contrast, our AutoCompressors learn to predict their own soft prompts \u03c3 as a function of x. Context distillation A related line of work (Askell et al., 2021; Snell et al., 2022) aims to distill incontext information, e.g., instructions, into an unprompted student model. In concurrent work, Mu et al. (2023) teach models to compress instructions into short key-value attention prefixes. Our approach differs by learning to compress any context information, including long documents, and results in more compact soft prompts. Long-range Transformers A number of architectural modifications have been proposed to scale Transformers to longer context lengths while reducing the high memory costs of full attention. These include restricting and sparsifying the attention window (Dai et al., 2019; Child et al., 2019), ap-\nproximating the attention (Rae et al., 2020; Zheng et al., 2022; Choromanski et al., 2021), as well as introducing recurrent elements (Ma et al., 2022; Bulatov et al., 2022), conditional computation (Ainslie et al., 2023), and retrieving previous tokens from the context at the output layer (Zhong et al., 2022). See Tay et al. (2022) for a comprehensive survey of efficient long-range architectures.\nMost of these architectures typically require expensive training from scratch, or will deviate substantially from a pre-trained initialization.2 Moreover, many language models lack the inductive bias to extrapolate to longer sequences (Press et al., 2022). While AutoCompressors could in principle be trained from scratch, we show that they offer a straightforward solution for extending the context window of pre-trained models to longer sequences."
        },
        {
            "heading": "3 Method",
            "text": "We describe how we adapt a pre-trained language model to compress text into summary vectors. An overview of our architecture is shown in Figure 1.\nSummary vectors The AutoCompressor builds on the RMT architecture (Bulatov et al., 2022). We extend the input vocabulary of the base model by \u03ba special summary tokens <Sum>i and initialize \u03ba new input embeddings.3 When we append the sequence <Sum>1 . . . <Sum>\u03ba to an input, it signals to the model to output special summary vectors of the preceding context. These vectors can then be passed to the next text segment as a soft prompt of length \u03ba. Since the embedding spaces of pretrained language models can span thousands of dimensions, we expect that this mechanism has a high capacity for passing information to subsequent segments. Furthermore, a soft prompt can interpolate between many token embeddings, and therefore represent more abstract concepts than a single discrete token (Wingate et al., 2022).\nSummary accumulation We split long documents into segments S1, . . . , Sn and process them sequentially. Bulatov et al. (2022) incorporate information from previous segments by prepending the compressed summary \u03c3i\u22121 produced from Si\u22121 to the embedded inputs of Si. We propose summary\n2In our pre-liminary experiments, even fine-tuning a pretrained OPT-2.7b model with Transformer-XL-style training (Dai et al., 2019) caused optimization difficulties and deterioriated the pre-trained model quality.\n3When fine-tuning OPT models, we observe benefits with initializing the embeddings of the summary tokens with the pre-trained embedding for the end-of-sequence token </s>.\naccumulation, which allows for a direct information pathway between each segment and all segments preceding it: we concatenate the summary vectors \u03c31 . . . , \u03c3i\u22121 to form \u03c3<i and prepend \u03c3<i to Si. Note that the length of \u03c3<i is now (i\u2212 1)\u03ba, which grows linearly with the document length. Positional embeddings When using a base Transformer architecture with absolute positional embeddings, such as the OPT architecture (Zhang et al., 2022), we do not add positional embeddings to the summary tokens <Sum>i, nor to the summary vectors. This allows us to use all pre-trained position embeddings as context tokens and makes it possible to scale the model to an arbitrary number of compression steps during training. The model still preserves the order of summary tokens due to their separate token embeddings.\nIf the base Transformer uses relative positional embeddings, such as RoPE (Su et al., 2022), we apply the positional embedding to the summary tokens and vectors without any further modification."
        },
        {
            "heading": "3.1 Training Summary Vectors",
            "text": "We use a simple unsupervised training approach which encourages the model to learn to compress contexts over multiple steps. Training objective Write (xi1, . . . , ximi) for the segment Si for every i \u2264 n, where mi is the number of tokens in Si. Conditioning on the concatenated summary vectors \u03c3<i, we project the Transformer outputs with the language modeling head to obtain the next-token probabilities p(xit | xi1, . . . , xit\u22121, \u03c3<i). We minimize the crossentropy loss over the entire document:\nL = \u2212 1 N n\u2211 i=1 mi\u2211 t=1 log p(xit | xi1, . . . , xit\u22121, \u03c3<i).\nwhere N is the total number of tokens. This objective retains the pre-trained language model\u2019s abilities on the first segment S1 and it incentivizes the model to store useful information in the summary vectors, which future segments can leverage to make better token predictions.\nUnlike Wingate et al. (2022), we do not train with a knowledge distillation objective, since the pre-trained LM has a limited context window as a teacher, whereas the AutoCompressor student learns to process much longer documents. Randomized segmenting We randomly vary the lengths mi of the segments Si during training, subject to the condition that each segment fits into\nthe model\u2019s context window. This allows AutoCompressors to compress documents of different lengths and improves performance under evaluation with fixed-length segments (see Figure 2).\nBPTT with stop-gradients We employ backpropagation through time (BPTT) and gradient checkpointing (Chen et al., 2016) for each segment to reduce the size of the computational graph. In addition, we compute and cache summary vectors and stop their gradients after 2 compression steps, similar to caching past attention states in TransformerXL training (Dai et al., 2019). This assumes that for learning to compress the useful information in Si, it is sufficient to predict the tokens in the adjacent Si+1. In Figure 2, we confirm that this incurs no penalty when predicting long segments, while further reducing GPU memory requirements."
        },
        {
            "heading": "4 Language Modeling Evaluation",
            "text": "In this section, we train AutoCompressors and evaluate their long-range language modeling capabilities by sampling long sequences which we split into segments of 2,048 tokens. We fix the final segment and compress the previous n segments. We track the perplexity of the final segment when conditioning on the summary vectors for each n.\nWe conduct our main experiments and ablations with OPT models (Zhang et al., 2022) of 1.3B or 2.7B parameters, fine-tuned on 2B tokens from the Pile (Gao et al., 2020). In Section 4.1, we evaluate an AutoCompressor on sequences of 8,000 tokens and compare to an equivalent RMT model and an Extended Full Attention baseline. In Section 4.2, we fine-tune an AutoCompressor on sequences of 30,000 tokens to demonstrate the feasibility on very long sequences. Finally, in Section 4.3, we scale up AutoCompressors by fine-tuning a Llama-2-7B model on 15B tokens from RedPajama (TogetherAI, 2023). Full model hyperparameters and data information can be found in Appendix A."
        },
        {
            "heading": "4.1 Experiments on 8K-Token Sequences",
            "text": "Setting We initialize all models with the 2.7Bparameter OPT model and fine-tune on 2B tokens from 4 domains form the Pile (Gao et al., 2020). Our AutoCompressor uses \u03ba = 50 summary tokens and is fine-tuned with summary accumulation over four segments, each ranging from 1,024 to 2,048 tokens. Compressing 2,048 tokens into 50 summary vectors achieves a compression rate of 40 tokens per summary vector. We use the following\nbaselines: 1. We fine-tune an OPT-2.7B baseline on our data.\nThis model is limited to sequences of 2,048 tokens due to pre-training. 2. Extended full attention: We fine-tune OPT-2.7B on sequences of up to 4,096 tokens by extending the model\u2019s positional embeddings. We initialize the embeddings for positions [2049..4096] with the embeddings for positions [1..2048]. We are not able to extend the context beyond 4,096 tokens due to GPU memory limitations. 3. RMT-2.7B: We fine-tune an RMT model on our data with \u03ba = 50 summary vectors. We evaluate on documents of 8,192 tokens, drawn from the 4 training domains or 4 held-out domains. We generate summary vectors for up to 3 segments of 2,048 tokens, but also for single segments as short as 128 tokens. For the extended full-attention baseline we prepend the previous context tokens to the context window.\nResults We show the results in Table 1. We find that the AutoCompressor benefits from long contexts of 6,144 tokens and consistently outperforms the RMT model.\nWe also find that the AutoCompressor benefits from much shorter sequences than seen during training, unlike RMT. See also Figure 2 and Table 6 for the usefulness of randomized segmenting.\nWhile extended full attention performs the best on 4,096-long sequences, we observe a trade-off for shorter contexts where AutoCompressors achieve the best performance. We also stress that the AutoCompressor attends to at most 150 additional soft prompts during evaluation, whereas the full attention model is given an additional 2,048 tokens.\nThese trends hold for both in-domain and outof-domain evaluation. However, the gap between the AutoCompressor and the full-attention baseline increases in the out-of-domain setting, suggesting that the summary vectors generalize slightly less than pre-trained attention heads."
        },
        {
            "heading": "4.2 Experiments on 30K-Token Sequences",
            "text": "Setting We fine-tune OPT-1.3B and OPT-2.7B as AutoCompressors on 2B tokens but train on sequences of 30,720 tokens with 20 compression steps.4 We use 50 summary tokens, randomized segmenting, and stop-gradients as before. We also\n4Due to the scarcity of very long sequences in the Pile, we only train on data from the Books3 domain, and use the Gutenberg domain as out-of-domain evaluation.\nfine-tune an RMT model from OPT-1.3B, to use as a baseline. We are not able to fine-tune a 2.7Bparameter RMT baseline because the RMT method leads to an out-of-memory error.\nAll models are evaluated on the final 2,048 heldout tokens of documents of size 30,720 tokens by compressing all previous 2,048-token segments.\nResults We collect our results in Table 2. The evaluation shows that both AutoCompressor models learn to utilize the entire 28K tokens to reduce perplexity, while the RMT baseline does not benefit from doubling the number of context tokens from 14K to 28K. This shows that summary accumula-\ntion effectively captures long-range dependencies in documents. We also report the CUDA memory requirements for fine-tuning each model in Table 2. We train with one NVIDIA A100 GPU with 80GB of memory. Stopping gradients reduces CUDA memory and makes it possible to fine-tune an AutoCompressor from OPT-2.7B, while fine-tuning with RMT leads to out-of-memory at that scale."
        },
        {
            "heading": "4.3 Scaling Up AutoCompressors to Llama-2",
            "text": "Setting We fine-tune a 7B-parameter Llama-2 model as an AutoCompressor on a single GPU by freezing the model and optimizing only the summary token embeddings and the attention weights via LoRA (Hu et al., 2022). The model is trained on 15B tokens from RedPajama (TogetherAI, 2023), split into sequences of 6,144 tokens, and we use 50 summary tokens, randomized segmenting, and stop-gradients. We also fine-tune an Extended Full Attention baseline on the same dataset. The context window of the pre-trained model is extended by increasing the \u03b8 value in RoPE following (Rozi\u00e8re et al., 2023).\nWe compare both models to the pre-trained Llama-2-7B model, which has a context window of 4,096 tokens. All models are evaluated on the final 2,048 tokens of 8,192-token documents.\nResults We collect our results in Table 3. The AutoCompressor benefits from the entire context to reduce perplexity: compressing a 4,096-token context into 100 summary vectors achieves similar perplexity to the Extended Full Attention baseline with 512 plain text tokens, and compressing a 6,144-token context into 150 summary vectors further improves perplexity slightly. Moreover, we find that summary vectors preserve perplexity when short contexts are compressed.\nHowever, Llama-2 and the Extended Full At-\ntention baseline outperform the AutoCompressor when longer contexts are provided. Further research is needed to construct summary vectors that preserve all of the context information."
        },
        {
            "heading": "4.4 Analysis",
            "text": "Ablations We train OPT-2.7B models without randomized segmenting, summary accumulation, or stop gradients. The results are shown in Figure 2. We find that randomized segmenting leads to better compression of short segments, but still improves perplexity when compressing multiple 2048 token segments. As expected, summary accumulation helps improve perplexity beyond one compressed segment. We also confirm that stopping gradients does not impact performance despite reducing GPU memory requirements. In Table 2, we also show that stopping gradients helps reduce GPU memory.\nWe also train AutoCompressors with \u03ba = 20, 50, 70 or 100 summary tokens and report the heldout perplexity results in Table 7 in the Appendix. Surprisingly, we find that performance does not increase with longer soft prompts, and \u03ba = 50 performs the best overall. We hypothesize that learning a larger number of summary vectors may require a larger training budget.\nToken-level analysis We seek to better understand how summary vectors benefit individual token predictions. In Figure 5 in the Appendix, we show perplexity gains at each token position for the AutoCompressor with summary vectors and for the extended full-attention baseline.\nWe find that conditioning on summary vectors\nimproves perplexity over all 2048 token positions. We observe that the extended full attention baseline outperforms the AutoCompressor at the start of the sequence, whereas the AutoCompressor achieves the best performance towards the end of the sequence. This shows that summary vectors effectively capture long-range textual dependencies.\nIn Appendix D, we show examples of sentences and tokens which benefit the most from summary vectors. We find that summary vectors contain salient information, such as names or dates, and that the model can reason over summary vectors. This confirms that summary vectors are useful summaries of the compressed text."
        },
        {
            "heading": "5 Compressing Demonstrations for In-Context Learning",
            "text": "In this section, we study the usefulness of summary vectors for performing downstream tasks. We show that in-context demonstrations can reliably be compressed down into summary vectors to improve performance while also increasing efficiency on a diverse set of NLP benchmarks.\nEvaluation We evaluate the in-context learning abilities of the AutoCompressor based on Llama2-7B from Section 4.3 on eleven classification and multiple-choice question-answering datasets. For each dataset, we evaluate the effect of compressing 1, 2 or 3 segments of demonstrations into 50, 100 or 150 summary vectors. For each segment, we include as many demonstrations as possible until we reach 750 tokens. For SST-2, this corresponds to 30 demonstrations per segment on average. We compare this compression approach with the results obtained by prompting the model using 150 and 750 tokens\u2019 worth of plain-text demonstrations.\nWe use contextual calibration (Zhao et al., 2021) and class-balanced sampling when these techniques improve performance on a validation set. For each dataset, we report the mean accuracy and standard deviation over 7 random seeds. The detailed settings for each dataset can be found in Table 11. In Table 12 in the Appendix, we also compare the ICL performance of our OPT-2.7B based AutoCompressor models against the RMT baseline and a pre-trained OPT-2.7B, and include the performance of the pre-trained Llama-2-7B model.\nResults We show evaluation results in Table 4. Results show that summary vectors consistently improve performance over the zero-shot baseline. Furthermore, summary vectors increase accuracy\ncompared to 150 tokens worth of plain demonstrations on 8/11 tasks. On 8 tasks (AG News, SST-2, BoolQ, WiC, WSC, CB, COPA and MultiRC), summary vectors also out-perform ICL with 750 tokens\u2019 worth of plain text demonstrations. Summary vectors emerge as a strong alternative to plain text demonstrations, as they increase accuracy while reducing inference cost.\nIn Table 12 (Appendix E), we find that the OPT2.7B AutoCompressor achieves higher accuracies than the RMT baseline on 8 out of 11 tasks and that the RMT model does not benefit from multiple compression steps. This shows that summary accumulation is an effective mechanism for compressing in-context demonstrations. We also observe that our fine-tuned Llama-2 AutoCompressor has substantially worse zero-shot accuracy on some tasks compared to the Llama-2 initialization, and slightly worse ICL performance. We suspect that this is due to domain mismatch in our fine-tuning data and the Llama-2 pre-training corpus."
        },
        {
            "heading": "6 Compressing Retrieval Corpora for",
            "text": "Efficient Inference\nWe study the usefulness of pre-computing summary vectors for large collections of documents. These can be stored and later retrieved for efficient inference. Since inference is typically more expensive than storage, this approach has the potential to achieve good practical trade-offs."
        },
        {
            "heading": "6.1 Retrieval-augmented Language Modeling",
            "text": "Retrieval-augmented language models improve token predictions by retrieving information from a data store. A number of approaches have been proposed to infuse external knowledge in the input layer (Guu et al., 2020; Shi et al., 2023), intermediate layers (Borgeaud et al., 2022) or at the output layer (Khandelwal et al., 2020; Zhong et al., 2022).\nREPLUG Our case study focuses on REPLUG (Shi et al., 2023), which is a simple method for combining a pre-trained language model with an offthe-shelf retriever to improve language modeling performance. Given access to an external corpus C, REPLUG retrieves k passages D = {d1, . . . , dk} based on a segment x to score the next segment y. The overall probability for y is computed by ensembling the predictions based on different passages:\np(y | x,D) = \u2211 d\u2208D \u03bb(d, x) \u00b7 p(y | CONCAT(d, x)),\nwhere \u03bb(d, x) are the normalized similarity scores from the retriever and CONCAT(d, x) denotes concatenation of p and x. This method incurs a substantial overhead, since it requires k forward passes over sequences CONCAT(d, x, y). Fused Summaries We introduce a setting for retrieval-augmented language modeling close to fusion-in-decoder (Izacard and Grave, 2021). We concatenate the summary vectors of retrieved passages D to form the fused summary vectors, \u03c3D = CONCAT(\u03c3dk , . . . , \u03c3d1), where dk, . . . , d1 are ordered from least-to-most relevant. This resembles\nsummary accumulation as described in Section 3. We also find it useful to smooth probability scores and re-order the retrieved passages based on their summary vectors (Appendix F). Figure 3 gives an overview of our approach. Fused Passages We establish a baseline for fusing summary vectors by concatenating the plaintext passages and computing smoothed probabilities, see Appendix F. Unlike summary vectors, this method is limited by the model\u2019s context window. Experiments We evaluate the OPT-2.7B AutoCompressor introduced in Section 4.1 without any additional fine-tuning. Similar to Shi et al. (2023), we retrieve from the Pile. We use Books3, FreeLaw, GitHub, Wikipedia, Gutenberg, ArXiv, HackerNews, and YoutubeSubtitles. We index 10B tokens for each domain, which are split into passages of 512 or 50 tokens.\nWe sample segments of 256 tokens from the Pile validation data, using the first 128 tokens as context x for retrieval and the last 128 tokens y for evaluation. We use the Contriever model (Izacard et al., 2022) for retrieval, and retrieve the top 10 passages. We also deduplicate our data by removing passages that overlap with x by 64 tokens. Results Results are shown in Table 5. We find that Fused Summaries outperforms Fused Passages and REPLUG when 50-token passages are retrieved. We measure throughput empirically and show that for 10 retrieved documents, Fused Summary Vectors remains inexpensive. We note that compressing the 10B token datasets results in disk space of 5TB per domain when stored in half-precision format.5 Therefore Fused Summaries achieves a good trade-off between storage costs and throughput.\n5For comparison, storing the transformer output at every single token (e.g., in an encoder-decoder setting) would take up 51 TB, and storing all attention states would be 3,276 TB.\nMoreover, Fused Summaries outperforms REPLUG top-2 with 512-token passages and sees a 1.7x throughput increase, which shows that the model benefits from the diversity of compressed documents. However, REPLUG top-10 outperforms Fused Summaries. We leave it as future work to explore how to produce higher quality summary vectors to better utilize the compressed passages.\nWe note that fusing summary vectors is effective despite a mismatch in training since we draw independent summary vectors from separate documents. Furthermore, our AutoCompressor model is only ever trained to accumulate 3 sets of summary vectors, and yet it benefits from fusing the summary vectors of up to 10 documents."
        },
        {
            "heading": "6.2 Unsupervised Passage Re-ranking",
            "text": "Finally, we consider the case study of passage reranking, in which a fast off-the-shelf retriever like BM25 retrieves a large set of candidate passages, and a more capable re-ranker refines the ranking to increase the rank of the most relevant passages.\nMethod Sachan et al. (2022) introduce an effective method for leveraging language models as re-rankers with no additional supervision or finetuning. Given a query q and a set of candidate passages {p1, . . . , pk}, the language model scores the likelihood of the query q conditioned on the prompt \u201cPassage: {pi}. Please write a question based on this passage.\u201d for each passage pi and re-ranks the passages based on the scores.\nExperiments We consider the task of re-ranking BM25 passages on the NQ test set (Balachandran et al., 2021) and compare out-of-the-box AutoCompressors with 20 and 50 summary tokens to pretrained OPT models from 125M to 2.7B parameters. We pre-compute summary vectors for 21M passages from a Wikipedia corpus (Karpukhin et al.,\n2020), which requires 2.1TB and 5.4TB disk space in half precision for 20 and 50 summary vectors respectively. We measure the quality of the re-ranked results using Recall@20.\nResults The results are shown in Figure 4. We measure throughput for individual un-batched queries on a single NVIDIA A100 80GB GPU and assume that the latency of loading summary vectors is negligible. Although the passages are only 100 words long, resulting in low compression rates, summary vectors substantially speed up the inference, while sacrificing on performance less than smaller models. This leads to a Pareto-optimal trade-off between compute and performance and demonstrates that summary vectors often retain sufficient information from a passage to assess its relevance for a particular query."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have introduced a training strategy for adapting pre-trained LMs into AutoCompressors, which recursively compress contexts into summary vectors. Our experiments indicate that summary vectors retain important contextual information, that they can encode in-context demonstrations, and that they\ncan be used in retrieval settings. Summary vectors can also be pre-computed, cached and re-used. This offers practical efficiency gains by reducing the size of the attention window. Significant future work remains in scaling AutoCompressors to bigger models and improving the quality of summary vectors to further close the gap with full attention over long-range contexts.\nLimitations\n1. We only apply AutoCompressors to OPT models of up to 2.7B parameters and a Llama model of 7B parameters. Future work needs to establish how AutoCompressors perform for even larger models. As the summary vector dimension grows, there is promise for retaining more information per vector. 2. Our results suggest that summary vectors ignore some useful information that is accessible via full attention. Additionally, models do not always benefit from increasing the number of summary vectors. We suspect that the training signal for learning summary vectors efficiently might be limited by pre-trained models being very good at making predictions from the plaintext tokens in the current segment. Future work is needed to improve this optimization. 3. Summary accumulation still leads to quadratic complexity with increasing number of segments, albeit at a much lower rate than full attention. Future work may explore ways to combine many summary vectors more efficiently."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Mengzhou Xia, Howard Chen, Vishvak Murahari, Aatmik Gupta, Zirui Wang, Jiatong Yu, and the members of the Princeton NLP group for helpful discussion and valuable feedback. This research is supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, and a Data Science Research Award from Adobe. AC also gratefully acknowledges support from the Minerva Research Foundation."
        },
        {
            "heading": "A Models and Data",
            "text": "All models are fine-tuned from OPT models on the Pile. We conduct our experiments using a single NVIDIA A100 80GB GPU and we use Flash Attention (Dao et al., 2022) as an efficient implementation of exact attention over long sequences. We also use gradient checkpointing between compressed segments to reduce GPU memory.\nA.1 OPT Experiments on 8K Tokens We fine-tune our models on 2B tokens from the Pile. We sample 500M tokens from the following Pile subdomains: Books3, FreeLaw, GitHub and Wikipedia.\nThe following models use a learning rate of 2e-5, a batch size of 130K tokens, 1,00 warm-up steps, and the Adam optimizer (Kingma and Ba, 2015): 1. The fine-tuned OPT-2.7B baseline is fine-tuned\non documents of up to 2,048 tokens. 2. The extended full-attention baseline is fine-\ntuned on documents of up to 4,096 tokens by extending the positional embeddings of OPT-2.7B to 4,096 positions. We initialize the embeddings for positions [2049..4096] with the embeddings for positions [1..2048]. 3. The RMT baseline is fine-tuned on documents of up to 8,192 tokens. Each document is segmented into four segments of 2,048 tokens. We use \u03ba = 50 summary vectors but we do not use summary accumulation, randomized segmenting, or stop-gradients. 4. Our AutoCompressor is fine-tuned on documents of up to 6,144 tokens. Each document is randomly segmented into four segments such that the first two segments add up to 3,072 tokens. The length of each segments ranges from 1,024 to 2,048 tokens. We use \u03ba = 50 summary vectors and summary accumulation. We stop gradients every two compression steps.\nAll models are evaluated on documents sampled from the Pile with a fixed length of 8,192 tokens. We sample 610 documents from each of the following domains: Books3, FreeLaw, GitHub, Wikipedia (in-domain), and ArXiv, Gutenberg, HackerNews, YoutubeSubtitles (out-of-domain). Examples of documents from each of those domains can be found in Tables 9 and 10.\nA.2 OPT Experiments on 30K Tokens We fine-tune our models on 2 billion tokens from the Books3 subdomain of the Pile. All models are\nfine-tuned on documents of up to 30,720 tokens. We use a learning rate of 2e-5, a batch size of 130k tokens, 1,000 warm-up steps and the Adam optimizer. 1. RMT-1.3B uses \u03ba = 50 summary vectors and is\nfine-tuned without summary accumulation, randomized segmenting, or stop-gradients. Each document is split into 15 segments of 2,048 tokens Even with gradient checkpointing, attempting to fine-tune a 2.7B parameter RMT model on this dataset leads to an out-of-memory error. 2. The AutoCompressor models are fine-tuned from OPT-1.3B and 2.7B on documents of up to 30,720 tokens. Each document is split into 20 segments such that segment 2i and segment 2i+1 add up to 3,072 tokens. The length of each segment is randomly sampled between 1,024 and 2,048. We use \u03ba = 50 summary vectors with summary accumulation and we stop gradients every two compression steps.\nAll models are evaluated on documents of 30,720 tokens from the Pile. We use 1,000 documents from Books3 (in-domain) and 1,000 documents from Gutenberg (out-of-domain).\nA.3 Llama-2 Experiments on 8K Tokens\nWe fine-tune our Llama-2 models on 15B tokens from RedPajama. We sample 1B tokens from long documents in ArXiv, Books, C4, GitHub, as well as 10B tokens from CommonCrawl, 800M from Wikipedia and 70M tokens from StackExchange.\nBoth our AutoCompressor and our Extended Full Attention baseline are fine-tuned from Llama2-7B on sequences of 6,144 tokens with LoRA (Hu et al., 2022) parameter efficient fine-tuning applied to the attention heads. We use a LoRA dimension of 16 applied to the QKV- and Out-projections. We use a learning rate of 4e-4, a batch size of 200K tokens, 5,000 warm-up steps and the Adam optimizer. For the AutoCompressor, we also optimize the newly initialized summary token embeddings.\nWe train our AutoCompressor in the same way as the OPT-2.7B AutoCompressor, with \u03ba = 50, randomly segmenting each sequence into four semgents, and stopping gradients every two compression steps. The Extended Full Attention baseline is fine-tuned with a RoPE \u03b8 value of 80,000.\nWe evaluate our models on 500 sequences of 8,192 tokens from each of ArXiv, Books, C4, GitHub, StackExchange, and 5,000 sequences from CommonCrawl."
        },
        {
            "heading": "B No-context Language Modeling",
            "text": "In Table 6, we verify that our fine-tuning strategy does not significantly affect the language modeling capabilities of the OPT AutoCompressors when no summary tokens are given. We find that the AutoCompressor performs slightly better than the RMT model and significantly better than the extended full attention model when no additional context is given. Moreover, the AutoCompressor almost matches the OPT02.7B fine-tuned baseline, with perplexity increasing by less than 1%."
        },
        {
            "heading": "C AutoCompressor Ablations",
            "text": "We train OPT AutoCompressor models as in Section 4.1 while varying \u03ba = 20, 50, 70, 100. In Table 7, we report the perplexity evaluation on documents of 8192 tokens across all evaluation domains."
        },
        {
            "heading": "D Token-level AutoCompressor Analysis",
            "text": "In Figure 5, we plot the perplexity gains achieved by the OPT AutoCompressor and the extended full attention baseline from Section 4.1 over the pre-trained OPT-2.7B model. We plot the gains achieved by the AutoCompressor both without any additional context and with the summary vectors obtained from 2048 compressed tokens.\nResults show that the summary vectors help reduce perplexity over the entire 2,048-token segment. This shows that summary vectors do not only contain information which helps continue the previous sequence.\nFigure 5 also shows that the extended fullattention baseline benefits more from the additional 2,048 context tokens than the AutoCompressor at the start of the sequence, but that the AutoCompressor achieves stronger gains at the end of the sequence. This shows that summary vectors effectively capture long-range textual dependencies and that fine-tuning AutoCompressors produces more robust models than fine-tuning extended fullattention models.\nIn Tables 9 and 10, we give hand-picked examples of sequences from each evaluation domain, highlighting which tokens benefit the most from the compressed context. We compress the first 300 tokens in every document from the evaluation set and evaluate on the following 100 tokens. In the notation of Section 3.1, we measure the perplexity gain of each token as\np(x2t | x21, . . . , x2t\u22121, \u03c31) p(x2t | x21, . . . , x2t\u22121) .\nFor each example, we record the top 3-5 most improved token predictions.\nWe find that the tokens which benefit the most from the summary vectors are often interpretable. Names of characters, dates, and locations are often\ncopied through the summary vectors (see the examples for Wikipedia, FreeLaw, or HackerNews). We also find that the model is able to reason over the summary vectors, as the tokens which benefit the most are sometimes not explicitly present in the compressed context, but are closely associated with the domain of speech (see the examples for Books3, Gutenberg and YoutubeSubtitles.). Finally, we find that summary vectors are often useful for continuing the previous sentence (see the GitHub example.)\nE In-Context Learning Details\nWe evaluate on in-context examples of the following datasets: AG News (topic classification, Zhang et al. (2015)), SST-2 (sentiment analysis, Socher et al. (2013)), BoolQ (Boolean Questions, Clark et al. (2019)), WiC (Word-in-Context, word sense dismabiguation, Pilehvar and Camacho-Collados (2019)), WSC (Winograd Schema Challenge, coreference resolution, Levesque et al. (2012)), RTE (Recognizing Textual Entailment, Dagan et al. (2005); Haim et al. (2006); Bentivogli et al. (2009)), CB (CommitmentBank, de Marneffe et al. (2019)), COPA (Choice of Plausible Alternatives, Roemmele et al. (2011)), MultiRC (Multi-Sentence Reading Comprehension, Khashabi et al. (2018)), MR (Movie Reviews, Pang and Lee (2005)), Subj (Subjectivity, Pang and Lee (2004). We follow the GPT3 prompt templates (Brown et al., 2020) and detail our evaluation setting for OPT and Llama-2 in Table 11.\nIn Table 12, we compile evaluation results for OPT-2.7B, Llama-2-7B, as well as our AutoCompressor and RMT models."
        },
        {
            "heading": "F Fused Retrieval-augmented Language",
            "text": "We provide details and ablations for our proposed REPLUG alternative. Inspired by fusion-indecoder (Izacard and Grave, 2021), we fuse summary vectors or passages in a single forward pass. Fused Summary Vectors The summary vectors of retrieved passages D are concatenated in order of increasing retrieval scores to form fused summary vectors, \u03c3D = Concat[\u03c3dk , . . . , \u03c3d1 ]. This resembles summary accumulation as described in Section 3, but differs in that the retrieved summary vectors were produced independently rather than recursively. Nevertheless, we find that AutoCompressors transfer well to this setting.\nFurthermore, we find it beneficial to smooth the conditioned probabilities with the unconditioned probabilities p(y | x), and compute\np(y | x,D) = p(y | Concat[\u03c3D, x]) + p(y | x) 2 .\nWe also show that language-modeling performance improves when D is re-ordered based on the smallest \u21132 distance between the summary vectors {\u03c3(d1), . . . , \u03c3(dk)} and \u03c3x. This incurs negligible overhead since \u03c3x can be constructed during the same forward pass which computes p(y | x). The ablation for this is shown in Table 8 Fused Passages We establish a baseline for Fusing Summary Vectors by concatenating the corresponding plain-text passages D = Concat[dk, . . . , d1] and computing\np(y | x,D) = p(y | Concat[D,x]) + p(y | x) 2 .\nNote that this approach is quickly limited by the size of the pre-trained language model\u2019s context window, especially when retrieving many long passages."
        }
    ],
    "title": "Adapting Language Models to Compress Contexts",
    "year": 2023
}