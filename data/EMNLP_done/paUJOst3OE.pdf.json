{
    "abstractText": "Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuyan Chen"
        },
        {
            "affiliations": [],
            "name": "Zhihao Wen"
        },
        {
            "affiliations": [],
            "name": "Ge Fan"
        },
        {
            "affiliations": [],
            "name": "Zhengyu Chen"
        },
        {
            "affiliations": [],
            "name": "Wei Wu"
        },
        {
            "affiliations": [],
            "name": "Dayiheng Liu"
        },
        {
            "affiliations": [],
            "name": "Zhixu Li"
        },
        {
            "affiliations": [],
            "name": "Bang Liu"
        },
        {
            "affiliations": [],
            "name": "Yanghua Xiao"
        }
    ],
    "id": "SP:c70cbe5b5d3931670900601c795831da586f85dc",
    "references": [
        {
            "authors": [
                "Simran Arora",
                "Avanika Narayan",
                "Mayee F Chen",
                "Laurel J Orr",
                "Neel Guha",
                "Kush Bhatia",
                "Ines Chami",
                "Frederic Sala",
                "Christopher R\u00e9."
            ],
            "title": "Ask me anything: A simple strategy for prompting language models",
            "venue": "arXiv preprint arXiv:2210.02441.",
            "year": 2022
        },
        {
            "authors": [
                "Luisa Bentivogli",
                "Peter Clark",
                "Ido Dagan",
                "Danilo Giampiccolo."
            ],
            "title": "The fifth pascal recognizing textual entailment challenge",
            "venue": "TAC. Citeseer.",
            "year": 2009
        },
        {
            "authors": [
                "Samuel R Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "arXiv preprint arXiv:1508.05326.",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "2023. Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712",
            "year": 2023
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini."
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "Machine learning challenges workshop, pages 177\u2013190. Springer.",
            "year": 2005
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Kelvin Guu",
                "Percy Liang."
            ],
            "title": "Transforming question answering datasets into natural language inference datasets",
            "venue": "arXiv preprint arXiv:1809.02922.",
            "year": 2018
        },
        {
            "authors": [
                "Mingkai Deng",
                "Jianyu Wang",
                "Cheng-Ping Hsieh",
                "Yihan Wang",
                "Han Guo",
                "Tianmin Shu",
                "Meng Song",
                "Eric P Xing",
                "Zhiting Hu."
            ],
            "title": "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
            "venue": "arXiv preprint arXiv:2205.12548.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Bill Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "arXiv preprint arXiv:2012.15723.",
            "year": 2020
        },
        {
            "authors": [
                "Danilo Giampiccolo",
                "Bernardo Magnini",
                "Ido Dagan",
                "William B Dolan."
            ],
            "title": "The third pascal recognizing textual entailment challenge",
            "venue": "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1\u20139.",
            "year": 2007
        },
        {
            "authors": [
                "R Bar Haim",
                "Ido Dagan",
                "Bill Dolan",
                "Lisa Ferro",
                "Danilo Giampiccolo",
                "Bernardo Magnini",
                "Idan Szpektor."
            ],
            "title": "The second pascal recognising textual entailment challenge",
            "venue": "Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual",
            "year": 2006
        },
        {
            "authors": [
                "Karen Hambardzumyan",
                "Hrant Khachatrian",
                "Jonathan May."
            ],
            "title": "Warp: Word-level adversarial reprogramming",
            "venue": "arXiv preprint arXiv:2101.00121.",
            "year": 2021
        },
        {
            "authors": [
                "Yaru Hao",
                "Zewen Chi",
                "Li Dong",
                "Furu Wei."
            ],
            "title": "Optimizing prompts for text-to-image generation",
            "venue": "arXiv preprint arXiv:2212.09611.",
            "year": 2022
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.",
            "year": 2004
        },
        {
            "authors": [
                "Ellen Jiang",
                "Kristen Olson",
                "Edwin Toh",
                "Alejandra Molina",
                "Aaron Donsbach",
                "Michael Terry",
                "Carrie J Cai."
            ],
            "title": "Promptmaker: Prompt-based prototyping with large language models",
            "venue": "CHI Conference on Human Factors in Computing Systems Extended Ab-",
            "year": 2022
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190.",
            "year": 2021
        },
        {
            "authors": [
                "Zekun Li",
                "Baolin Peng",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao",
                "Xifeng Yan."
            ],
            "title": "Guiding large language models via directional stimulus prompting",
            "venue": "arXiv preprint arXiv:2302.11520.",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Huan Ma",
                "Changqing Zhang",
                "Yatao Bian",
                "Lemao Liu",
                "Zhirui Zhang",
                "Peilin Zhao",
                "Shu Zhang",
                "Huazhu Fu",
                "Qinghua Hu",
                "Bingzhe Wu."
            ],
            "title": "Fairnessguided few-shot prompting for large language models",
            "venue": "arXiv preprint arXiv:2303.13217.",
            "year": 2023
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "arXiv preprint arXiv:2104.08773.",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "arXiv preprint cs/0506075.",
            "year": 2005
        },
        {
            "authors": [
                "Bo Pang",
                "Semih Yavuz",
                "Caiming Xiong",
                "Yingbo Zhou."
            ],
            "title": "Sharpt: Shared latent space prompt tuning",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1214\u20131220.",
            "year": 2023
        },
        {
            "authors": [
                "Archiki Prasad",
                "Peter Hase",
                "Xiang Zhou",
                "Mohit Bansal."
            ],
            "title": "Grips: Gradient-free, edit-based instruction search for prompting large language models",
            "venue": "arXiv preprint arXiv:2203.07281.",
            "year": 2022
        },
        {
            "authors": [
                "Reid Pryzant",
                "Dan Iter",
                "Jerry Li",
                "Yin Tat Lee",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Automatic prompt optimization with\" gradient descent\" and beam search",
            "venue": "arXiv preprint arXiv:2305.03495.",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
            "venue": "arXiv preprint arXiv:2104.06599.",
            "year": 2021
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Rajkumar Ramamurthy",
                "Prithviraj Ammanabrolu",
                "Kiant\u00e9 Brantley",
                "Jack Hessel",
                "Rafet Sifa",
                "Christian Bauckhage",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Is reinforcement learning (not) for natural language processing?",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Stella Biderman",
                "Leo Gao",
                "Tali Bers",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze questions for few shot text classification and natural language inference",
            "venue": "arXiv preprint arXiv:2001.07676.",
            "year": 2020
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "venue": "arXiv preprint arXiv:2010.15980.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461.",
            "year": 2018
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "Gptj-6b: A 6 billion parameter autoregressive language model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Anjana Arunkumar",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Atharva Naik",
                "David Stap"
            ],
            "title": "Benchmarking generalization via in-context",
            "year": 2022
        },
        {
            "authors": [
                "Jules White",
                "Quchen Fu",
                "Sam Hays",
                "Michael Sandborn",
                "Carlos Olea",
                "Henry Gilbert",
                "Ashraf Elnashar",
                "Jesse Spencer-Smith",
                "Douglas C Schmidt."
            ],
            "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "venue": "arXiv preprint arXiv:2302.11382.",
            "year": 2023
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426.",
            "year": 2017
        },
        {
            "authors": [
                "Zheng Yuan",
                "Hongyi Yuan",
                "Chuanqi Tan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang."
            ],
            "title": "Rrhf: Rank responses to align language models with human feedback without tears",
            "venue": "arXiv preprint arXiv:2304.05302.",
            "year": 2023
        },
        {
            "authors": [
                "JD Zamfirescu-Pereira",
                "Richmond Y Wong",
                "Bjoern Hartmann",
                "Qian Yang."
            ],
            "title": "Why johnny can\u2019t prompt: how non-ai experts try (and fail) to design llm prompts",
            "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Tianjun Zhang",
                "Xuezhi Wang",
                "Denny Zhou",
                "Dale Schuurmans",
                "Joseph E Gonzalez."
            ],
            "title": "Tempera: Test-time prompt editing via reinforcement learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Yuanhang Zheng",
                "Zhixing Tan",
                "Peng Li",
                "Yang Liu."
            ],
            "title": "Black-box prompt tuning with subspace learning",
            "venue": "arXiv preprint arXiv:2305.03518.",
            "year": 2023
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen."
            ],
            "title": "Factual probing is [mask]: Learning vs",
            "venue": "learning to recall. arXiv preprint arXiv:2104.05240.",
            "year": 2021
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "arXiv preprint arXiv:2211.01910.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Advancements in Large Language Models (LLMs) have ushered in a transformative era in natural language processing, showcasing their remarkable capabilities across a wide range of tasks (OpenAI, 2023; Bubeck et al., 2023). While these models possess human-like comprehension and response abilities, their performance is heavily influenced by the quality of prompts. As can be observed in Fig. 1, answers from different LLMs vary widely when they are provided with the same task-specific prompts. Therefore, it is necessary to generate prompts that are most suitable for each LLM, thereby enhancing its performance on downstream tasks.\nA common practice towards prompt optimization is to count on human expertise (White et al., 2023; Jiang et al., 2022; Zamfirescu-Pereira et al.,\n2023). While effective, such approaches are costly and unscalable. Hence, there has been a lot of effort to streamline the prompt optimization process through automated or semi-automated ways, including prompt retrieval (Ma et al., 2023; Zhou et al., 2022), prompt generation from scratch (Pang et al., 2023), and prompt editing (Gao et al., 2020; Pryzant et al., 2023; Deng et al., 2022). For example, in prompt retrieval, Ma et al. (2023) propose a search strategy based on greedy search to identify near-optimal prompts for in-context learning; in prompt generating from scratch, Pang et al. (2023) introduce SharpT, which learns a shared latent space and generates soft prompts using a lightweight prompt generator; in prompt editing, some approaches rely on reinforcement learning or LLM-based feedback for prompt optimization (Deng et al., 2022; Zhou et al., 2022).\nHowever, the aforementioned research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. The latter, although very important, has not been studied to date in NLP. The only relevant work so far has been done on multi-modal large models, which automatically optimizing prompts using reinforcement learning to generate images based\non text (Hao et al., 2022). They underscore the concept of \u201cmodel-preferred prompts\u201d or \u201cmodelspecific prompts\u201d, emphasizing that there\u2019s a need for a systematic method to automatically align user intentions with the specific prompt preferences of each model. Therefore, in this paper, we novelly propose a Model-Adaptive Prompt Optimization (i.e. MAPO) approach for LLMs in NLP. Given the lack of effective training signals, we first establish a so-called warm-up dataset to obtain candidate prompts from an oracle LLM, and then model the prompt optimization problem with reinforcement learning. Specifically, we first generate candidate prompts and search for the optimal prompts to establish a warm-up dataset. After that, we combine Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to optimize original prompts for each specific LLM in various downstream tasks. Moreover, we make joint learning with Proximal Policy Optimization (PPO) (Schulman et al., 2017) and RRMF (note that RRMF is inspired by RRHF (Yuan et al., 2023)), to further improve the performance of RL. We conduct extensive experiments which validates the robustness and generalization of the proposed MAPO. To sum up, our main research question revolves around identifying the optimal prompt that is suited for various models. Our contributions are threefold:\n\u2022 We are the first to quantitatively show that different prompts should be adapted to different Large Language Models (LLMs) in order to enhance their performance across various NLP downstream tasks.\n\u2022 We introduce a novel approach called the Model-Adaptive Prompt Optimizer (MAPO), specifically designed to optimize the original prompts for each particular LLM in downstream tasks.\n\u2022 The experiments show that our proposed MAPO model exhibits greater robustness and generalization and also achieves superior performance in a variety of downstream tasks."
        },
        {
            "heading": "2 Empirical study",
            "text": "In this section, we conduct empirical study on three LLMs (BLOOM-7B (Scao et al., 2022), GPT-J6B (Wang and Komatsuzaki, 2021), and LLaMA7B (Scao et al., 2022)) to evaluate their separate performance on question-answering (QA), classification, and generation tasks with same task-specific prompts. We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers. We adopt F1 score, accuracy and ROUGE-L for QA, classification, and generation tasks, respectively. The visualization results are shown in the Fig. 2. From the violin plot, we observe significant variations in distribution among different LLMs in each task. For example, in the generation task, the results of all three models are distributed within the range of 0 to 0.5, but there are still differences in the specific distribution patterns. Moreover, the medians, means, and other statistical measures also differ greatly among three LLMs in each downstream task. Therefore, we consider that finding the optimal prompt for each specific LLM on each task is meaningful, as it can help enhance the LLMs\u2019 performance on various downstream tasks."
        },
        {
            "heading": "3 Methods",
            "text": "Based on the above empirical study, we present MAPO, a model-adaptive prompt optimization approach for LLMs. It takes the original prompt as input and generate an optimized prompt which makes an LLM give better outputs. The framework of MAPO is shown in Fig. 3."
        },
        {
            "heading": "3.1 Warm-up Dataset Establishment",
            "text": "We first establish a warm-up dataset as training dataset for prompt optimization.\nGenerating Candidate Prompts. The original prompts are from nine above-mentioned datasets in P3 (Sanh et al., 2021). We generate 1,000 candidate prompts per prompt using GPT-3.5 1. The\n1https://chat.openai.com/\ngenerated candidate prompts should maintain semantic meaning similar to the original prompt but may have different expressions. To achieve this, we use the following instruction as input for GPT-3.5 to generate candidates: \u201cPlease rewrite the given text \u2018original prompt\u2019 while keeping the semantic meaning unchanged.\u201d. Some candidate prompts are shown in Appendix A.\nSearching for the Optimal Prompt. To determine which candidate prompt is optimal for an original prompt, we compare the match degree, which refers to the similarity, between the outputs generated using a candidate prompt and the ground truth output. The purpose is to identify the candidate prompt that produces an output most similar to the ground truth output. When a ground-truth output is not available, the output of a stronger LLM, such as GPT-3.5, is regarded as the ground-truth. Specifically, first, we input the original prompt P and each candidate prompt into an LLM, respectively, for inference and obtain the corresponding outputs. Next, we compare the match degree with specified evaluation metrics. We adopt F1 score, accuracy, and ROUGE-L (Lin, 2004) for QA, classification, and generation tasks, respectively. Based on these metrics, we iterate the searching process and find the optimal prompt Po for an LLM in downstream tasks. The warm-up dataset consists of a collection of prompt pairs (referred to as {P, Po}), whose distribution is shown in Table 1."
        },
        {
            "heading": "3.2 Prompt Optimizer Construction",
            "text": "The prompt optimizer seeks to refine the initial prompt (P ) into an optimized prompt (Po) tailored to a particular LLM. This refinement process entails altering the structure or wording of P to produce Po, which is more suitable for the LLM in subsequent tasks."
        },
        {
            "heading": "3.2.1 Supervised Fine-tuning",
            "text": "We begin by employing the warm-up dataset to conduct supervised fine-tuning (SFT) with an LLM across multiple downstream tasks. The objective of SFT is to enhance the LLM\u2019s capacity to generate responses that align with its preferences, utilizing annotated data. Prior research conducted by Ramamurthy et al. (2022) supports the notion that employing SFT prior to reinforcement learning (RL) leads to improved outcomes. Furthermore, to differentiate between specific tasks during training, we incorporate a brief instruction preceding the input, such as \u201cThis is a... (generative/questionanswering/classification) task.\u201d."
        },
        {
            "heading": "3.2.2 Building Reward Model",
            "text": "Next, we construct a reward model to learn the effectiveness of prompts based on the preferences of different LLMs. This approach is motivated by the fact that discriminative annotation through sorting incurs significantly lower costs compared to generating annotations for answers. Initially, we obtain a ranking sequence for an LLM in a specific downstream task. We sort the outputs generated by candidate prompts {P1, P2, . . . , Pk\u22121, Pk} alongside the original prompt P , using the same evaluation metric as described in Sec. 3.1. This sorting process yields a ranking sequence {P1, P2, . . . , P, Pk\u22121, Pk}. Prompts to the left of P exhibit poorer inference results, while prompts to the right demonstrate better results. Next, we em-\nploy the ranking sequence to train a reward model. We utilize the same LLM utilized in the SFT process (referred to as \u02c6LLM) and replace the softmax layer with a linear layer to construct the reward model. The reward model takes a prompt as input and produces a scalar score indicating the quality of the prompt. We form pairwise ranking pairs by combining prompts from the ranking sequence and employ Pairwise Ranking Loss for training, as illustrated below:\nL\u03b8 = \u2212 1( k 2 )E(x,yw,yl)\u223cD [log(\u03c3(r\u03b8(x, yw)\u2212 r\u03b8(x, yl))],\n(1)\nwhere x represents the original prompt, yw and yl denote the higher-scoring and lower-scoring prompts, respectively, in the corresponding ranking pair. r\u03b8 represents the scalar output of the reward model, D is the set of ranking pairs, and K denotes the number of candidate prompts. Through this process, based on the outputs generated by \u02c6LLM with the given prompt, the reward model learns to assign higher scores (rewards) to better prompts and lower scores (rewards) to inferior prompts, thus imitating an LLM\u2019s preferences."
        },
        {
            "heading": "3.2.3 Reinforcement Learning",
            "text": "Subsequently, we employ Reinforcement Learning (RL) to further fine-tune LLMs. RL is used to adjust the bias in the reward model\u2019s scoring since the distribution of generated prompts might change during the SFT process. The primary objective of optimization is to maximize the scores of prompts generated by \u02c6LLM after SFT (referred to as the SFT model), as evaluated by the reward model. To achieve this, we utilize a combination of Proximal Policy Optimization (PPO) (Schulman et al., 2017) and RRMF algorithms (note that RRMF is inspred by RRHF (Yuan et al., 2023)) for joint learning.\nPolicy Optimization. This step aims to optimize the RL policy to improve the performance of the RL model. We first adopt the datasets shown in Table 3, which to construct environmentaction pairs. The environment refers to the original prompt, and the action represents the prompt generated by \u02c6LLM without instruct-tuning. We pass the environment-action pairs to the reward model to obtain rewards. In this process, we introduce an actor model, which is \u02c6LLM, and a frozen model, which is SFT model with its parameters frozen during the RL training process. The frozen model serves as a benchmark to evaluate whether the updated actor\nmodel has advantages over it. We then calculate the policy gradient loss (i.e., actor\u2019s loss) based on the importance ratio and reward (r+\u03b3Vnext\u2212Vcur), and calculate the value loss (i.e., critic\u2019s loss) by comparing the predicted value Vpred with the true value (r + Vnext) as follows:\nLpg = P\u03c0a(t)\nP\u03c0f(t) (r + \u03b3Vnext \u2212 Vcur), (2) Lv = \u2225Vpred \u2212 (r + Vnext)\u2225. (3)\nHere, P\u03c0a (t)P\u03c0f (t) represents the ratio of probabilities (i.e., importance ratio) of generating the same token under the actor model and the frozen model. (r + \u03b3Vnext \u2212 Vcur) represents the reward of the current step. Vpred denotes the predicted value, and (r + Vnext) denotes the true value.\nNext, we maximize the mathematical expectation of the reward model, aiming to consistently generate prompts that \u02c6LLM perceives as the best in the RL-trained SFT model (referred to as RL model). We feed prompts x generated by the SFT model based on the datasets shown in Table 3 (i.e., D) into the RL model \u03c0RL\u03d5 to obtain an optimized prompt y. y changes every time the RL model is updated. We then input (x, y) into the reward model r\u03b8 and calculate a score (i.e., reward), which represents the real-time feedback from the reward model. The loss function is defined as follows:\nLr\u03b8\u03d5 = E(x, y) \u223c D\u03c0RL\u03d5 [r\u03b8(x, y)]. (4)\nFinally, we combine the above loss functions to optimize the RL policy from multiple perspectives. The final loss function is defined as:\nL\u03c1 = \u03b11Lpg + \u03b12Lv + \u03b13L r\u03b8 \u03d5 , (5)\nwhere \u03b11, \u03b12, and \u03b13 represent the optimal weights of each function, which are determined through experiments (the same applies below).\nSFT Approximating. This step aims to maintain similarity between the RL model and the SFT model. When the RL model undergoes parameter updates, it leads to variations in the generated prompt y based on the given prompt x. If there is a significant discrepancy between the RL model and the SFT model, it can result in inaccurate estimation of scores by the reward model. To address this issue, we measure the distance between the prompts generated by the RL model and the SFT model using Kullback-Leibler (KL) divergence.\nThe objective is to minimize the KL divergence and the loss function is defined as follows:\nLSFT\u03d5 = \u2212\u03b2log(\u03c0RL\u03d5 (y|x)/\u03c0SFT (y|x)). (6)\nwhere \u03c0RL\u03d5 (y|x) and \u03c0SFT (y|x) represent prompts generated by RL model and the SFT model, respectively.\nNext, we have borrowed the idea from RRHF (Yuan et al., 2023) but adapt it to focus on \u201cmodel feedback\u201d instead of \u201chuman feedback\u201d. We name it Ranking Responses from Model Feedback (RRMF). Specifically, we calculate the likelihood probability of \u02c6LLM during SFT and align this probability with the score of the reward model. To optimize this objective, we employ supervised learning with a rank loss, defined as follows:\npi =\n\u2211 logP\u03c0(yi|x, yi)\n\u2225yi\u2225 , (7) Lr = \u2211 ri<rj max(0, pi \u2212 pj), (8)\nwhere pi is the conditional log probability which represents the reward of each optimized prompt yi. ri represents the reward model r\u03b8(x, yi). We also incorporate the cross-entropy loss introduced by RRHF (Yuan et al., 2023) to learn the generated prompts y\u2032i with the highest reward r \u2032 i as follows:\nLft = \u2212\u03a3logP\u03c0(y\u2032i|x, y\u2032i). (9)\nFinally, we combine the above loss functions for SFT approximating as follows:\nLSFT = \u03b21L SFT \u03d5 + \u03b22Lft + \u03b23Lr. (10)\nGeneralization Maintaining. This step addresses the issue of catastrophic forgetting by ensuring that an LLM performs well not only on specific tasks but also on general NLP tasks. To achieve this, we follow a similar approach as outlined in InstructGPT (Ouyang et al., 2022). We sample 10% data from general NLP tasks in GLUE (Wang et al., 2018) and the SuperGLUE benchmark (Wang et al., 2019), which are considered representative, as indicated in Table 3, during the pre-training phase. The objective of pre-training is to generate outputs that are as good as or better than the original one based on the original prompts. The original prompts are taken from Natural Instructions (Wang et al., 2022b). The loss function is as follows:\nLPre = \u03b3Ex \u223c Dpretrain[log(\u03c0RL\u03d5 (x))], (11)\nwhere Dpretrain represents the selected datasets for pre-training.\nJoint learning. Finally, we make joint learning with the above-mentioned loss functions as follows:\nL\u03d5 = \u03b31L\u03c1 + \u03b32LSFT + \u03b33LPre. (12)"
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, We conduct experiments with three popular LLMs as \u02c6LLM, respectively, including BLOOM (7B), GPT-J (6B), and LLaMA (7B), on different downstream tasks to validate the effectiveness of MAPO."
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "The experiments are executed on 4 Nvidia A100 GPUs with 80GB each, using PyTorch in Python. DeepSpeed 2 is utilized in the training process. The maximum sequence length for original prompts and optimized prompts are both set to 512 tokens. The number of epochs is set to 20 in the entire training process. We provide the detailed configuration of the hyperparameters in Appendix B. The dataset and metrics we utilize are the same as those described in Sec. 2. All results are reported on the corresponding test sets or 10% dev sets if a dataset does not have a test set. Details of all used baselines and datasets are in Appendix D and E."
        },
        {
            "heading": "4.2 Main Results",
            "text": "The main results are shown in Table 2. We observe that the performance increase evidently among all LLMs during SFT. We then utilize MAPO to make further optimization. We find the optimized prompts generated by MAPO are more adaptive in QA and generation task for BLOOM (increase by 20.5% for CloseQA and by 30.9% for Explan compared with SFT (p<0.01)) and GPT-J (increase by 21.4% for CloseQA and by 20.6% for Explan compared with SFT (p<0.01)). And the prompts are more adaptive in classification task (increase by 22.8% for News (p<0.01)) for LLaMA. These results indicate that MAPO effectively enhances the performance of various LLMs and exhibits preferences in different downstream tasks.\nTo validate the superiority of MAPO, we compare it with several SOTA prompt optimization baselines in various popular tasks based on the same setting Roberta-Large, as shown in Table 3. The reported results represent the best-performing LLM among the three LLMs, which indicates that\n2https://github.com/microsoft/DeepSpeed\nour method applies not only to LLMs but also to smaller LMs. We analyze the possible reasons as follows: MAPO employs both SFT and RL to optimize LLMs. In fact, the SFT process is not specific to LM. Fine-tuning smaller models is feasible and common, requiring fewer computational resources. RL is a widely-used algorithm across applications and model scales, and small models require less computational and storage resources, making RL more feasible on them.\nWe also test the performance of MAPO with the above-mentioned SOTA prompt optimization baselines. We use three LLMs, including BLOOM, GPT-J, and LLaMA, to replace the BART model used in Table 3 for verifying the nine datasets in Table 2, as shown in Table 4. Due to SFT in LLMs equals fine-tuning pretrained language models, we directly list the SFT results in the Fine-tuning row. Apart from Fine-tuning, we also freeze the LLMs and only modify the prompts for inference on downstream tasks. According to the experimental results, the performance of almost all baselines, except RLprompt, does not exceed that of Fine-tuning /SFT, and some even do not outperform the original LLMs. This highlights the importance of SFT\nin LLMs. When we add RL, as in the case of RLprompt, the performance on downstream tasks surpasses that of SFT, indicating the significance of RL for prompt optimization. Moreover, using our proposed MAPO method to optimize the prompt further improves performance over RLprompt, except in a very few cases, such as using BLOOM for movie classification tasks. These experimental results demonstrate that the MAPO method proposed in this study makes a substantial contribution to improving the performance and accuracy in downstream tasks.\nMoreover, we conduct experiments to evaluate the domain transfer performance of MAPO. The results are presented in Table 5 and Table 6, while the results of LLMs with original prompts are reported by Arora et al. (2022). Remarkably, we observe that each LLM, when using prompts optimized by MAPO, displays improved performance across various downstream tasks. Specifically, BLOOM exhibits the highest increase in performance compared with GPT-J and LLaMA. This experiment clearly demonstrates the significant domain transfer capability of MAPO."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "The effect of RL compared with SFT. From the experiments (Table 3, Table 5 and Table 6), we can observe that the performance improvements gained solely from using SFT are less than half of those achieved by our proposed MAPO method, both on similar tasks and general NLP tasks. This clearly indicates the effectiveness of MAPO in optimizing model-adaptive prompts.\nIn order to further demonstrate RL is necessary and how it compares to simply extending SFT with a larger warm-up dataset, we use various proportions of the warm-up dataset to progressively increase the SFT training data and then introduce RL\nTask QA CLS GEN Ad Op Cl Ne Mo QA To Su Ex\n(BLOOM) Original 13.5 25.9 6.4 92.8 90.9 99.4 29.5 46.1 5.7 F Finetuning/SFT 18.3 26.7 7.8 95.5 92.6 99.9 34.8 48.8 6.8 C Soft prompt 14.5 24.5 6.5 92.1 90.5 99.1 30.1 44.6 5.5\nBlack-Box 15.2 24.7 6.9 93.3 91.6 99.3 31.2 45.7 5.8 Autoprompt 15.7 25.0 7.1 93.6 91.9 99.4 31.6 46.0 6\nD Manual 13.7 24.6 6.8 91.8 90.9 99.0 31.5 45.1 5.7 In-Context 13.9 24.7 6.7 91.6 90.9 99.3 31.8 45.6 5.9 Instructions 15.0 24.9 6.7 92.7 91.0 99.2 30.8 45.5 5.8 GrIPS 16.6 25.3 6.8 93.1 91.2 99.4 31.8 46.7 6.2 RLprompt 19.2 26.9 8.9 97.5 94.1 99.9 35.9 49.1 8.2 TEMPERA 17.4 25.5 7.2 94.6 91.7 99.5 33.1 46.7 6.2 AMA 19.1 26.4 7.6 95.1 92.4 99.4 33.5 47.9 6.1 D MAPO 19.5 27.2 9.4 98.7 93.3 99.9 36.2 50.2 8.9 (GPT-J) Original 3.0 17.0 6.9 0.0 51.1 54 17.5 13.1 8.5 F Finetuning/SFT 9.4 20.3 8.4 5.5 52.7 56.3 21.6 16.7 10.7 C Soft prompt 5.4 16.6 6.8 2.1 51.0 54.5 17.8 13.7 8.9\nBlack-Box 7.3 17.5 7.2 2.5 51.4 54.7 18.2 14.3 9.1 Autoprompt 7.8 17.9 7.3 1.9 51.5 54.6 19.1 14.6 9.3\nD Manual 5.7 15.9 6.5 1.7 50.9 54.9 18.7 13.9 9.5 In-Context 5.6 16.5 6.7 1.6 50.7 54.6 19.2 14.0 9.3 Instructions 7.1 17.1 7.2 1.7 51.6 54.0 19.1 14.3 9.2 GrIPS 7.7 17.9 7.7 4.3 52.1 55.6 19.7 16.4 10.3 RLprompt 10.1 20.0 9.5 5.7 53.7 56.4 22.1 17.2 11.8 TEMPERA 9.5 19.3 9.1 4.8 53.2 56.1 22.3 16.8 11.4 AMA 9.6 19.1 8.8 5.0 52.9 56.3 22.4 16.5 11.6 D MAPO 11.0 21.0 10.2 6.3 53.9 56.8 23.4 17.8 12.9 (LLaMA) Original 3.2 13.3 10.8 1.1 78.7 61.6 14.3 6.6 6.9 F Finetuning/SFT 23.2 15.4 13.9 10.1 81.3 70.2 18.6 10.7 8.2 C Soft prompt 7.7 12.6 10.2 4.4 77.4 62.7 15.6 8.2 7.3\nBlack-Box 9.2 13.3 10.7 5.6 78.1 63.1 16.2 8.4 7.5 Autoprompt 10.3 13.5 11.0 7.4 78.4 65.2 16.7 9.0 7.6\nD Manual 12.3 12.7 11.1 7.5 77.5 64.8 16.2 8.3 6.7 In-Context 13.7 13.0 10.8 8.0 77.7 65.3 16.5 8.5 7.0 Instructions 16.2 13.2 11.3 7.5 78.0 65.7 17.1 9.1 7.5 GrIPS 19.3 14.7 13.4 9.3 80.6 68.6 18.9 10.7 8.3 RLprompt 24.7 15.8 14.3 11.6 81.9 71.4 19.2 11.7 8.8 TEMPERA 22.6 15.4 13.8 8.9 81.5 69.6 18.7 9.6 8.7 AMA 23.5 15.5 13.6 8.6 81.7 70.5 18.5 9.8 8.9 D MAPO 25.1 16.1 14.8 12.4 82.5 72.8 19.5 12.2 9.1\nTable 4: The performance with a frozen LLM for inference of MAPO with SOTA prompt optimizing baselines in nine tasks from P3 benchmark using LLaMA. F: Finetuning/SFT, C: Continous prompt, D: Discrete prompt.\nto it as shown in Table 17. Our findings consistently show that RL adds value to the performance beyond what is achieved by SFT alone across all proportions of the dataset. This affirms the effectiveness of RL irrespective of the SFT dataset size. However, as the proportion of the warm-up dataset increases, the margin of improvement from adding RL begins to decline. While one could hypothesize that adding RL to a very large SFT dataset might not result in as significant an improvement as it would for a smaller dataset, this observation actually underscores our method\u2019s suitability for low-resource scenarios.\nMoreover, we have tried different number of epochs to see if extended training time consistently improves SFT performance as shown in Table 18. Extending the training time does not consistently lead to performance improvements for SFT. In some instances, the performance even declines. It is important to note that we save the best-performing models in real-time during training, as the peak performance does not necessarily occur at the final epoch.\nThe effect of warm-up dataset. As shown in\nFig. 5 and Table 11, our study examines the effects of different proportions of the warm-up dataset on MAPO\u2019s performance. The results indicate that as the size of the warm-up dataset increases, performance typically improves. BLOOM is particularly sensitive, showing a pronounced growth trend. Conversely, GPT-J shows a more gradual growth. LLaMA\u2019s performance reveals an inflection around 60%, suggesting other factors also influence its performance. Even with reduced dataset sizes, the decrement in performance remains minimal, highlighting the method\u2019s suitability for low-resource tasks. We also conduct few-shot experiments on general NLP tasks with just 10% data and observe promising improvements. This underlines our method\u2019s adaptability and effectiveness in scenarios of data scarcity.\nThe effect of PPO and RRMF. To investigate the specific effects of PPO and RRMF during the RL process, we conduct separate experiments to evaluate the contributions of each component. The experimental results, depicted in Fig.6 (with details provided in Table 10 in Appendix F), clearly demonstrate the important roles played by PPO and RRMF in enhancing the performance of MAPO. We propose the following explanations for these results: PPO focuses on reducing the dissimilarity between the RL model and the SFT model. RRMF aligns the scores from the reward model with the likelihood probabilities of an LLM. Both PPO and RRMF aim to assign higher probabilities to prompts that are more adaptable to the model.\nThe effect of the Randomness. We also incorporate randomness (e.g., temperature) during the generation process of LLM. Given that our prompts do not require high creativity, we have set a lower temperature range [0-0.5] for generation, within which we aim to generate optimal prompts. To further investigate the impact of varying temperatures on the generated output, we conduct an additional set of experiments to assess the performance of the MAPO method under different randomness settings (temperature=0,0.2,0.5,0.8) as shown in Table 14, Table 12, Table 15 and Table 16. Each experiment group runs 5 times. Our findings reveal that a hightemperature setting (t=0.8) tends to produce inferior prompts that lead to less accurate outputs for a specific task. Lower temperature (t=0.2) or greedy settings (t=0) are likely to produce more accurate outputs that are closer to our optimal results. This suggests that in a task like prompt optimization,"
        },
        {
            "heading": "QA NQ 16.8 17.2 2.4 1.4 18.1 7.7 4.7",
            "text": "introducing a stable (low temperature) but slight degree of variability (non-zero temperature) yields the best results."
        },
        {
            "heading": "4.4 Case Study and Error Analysis",
            "text": "We conduct a case study to visualize the prompts optimized by MAPO, as shown in Table 7. Additional cases are included in Appendix G. We first observe that the majority of the original prompts have significant modifications after optimization through our MAPO method. Only about 10% of the generated prompt pairs remain completely unchanged. To further quantify these changes, we calculate a normalized edit distance. Given the varying lengths of different prompt pairs, we divide the edit distance by the average length of the two strings. This yields a value between 0 and 1, where 0 indicates identical strings and 1 indicates completely different strings. The average normalized edit distance for all prompt pairs stands at 0.67, demonstrating that most prompts do experience substantial modifications.\nNext, we provide a detailed examination of these modifications. In the QA task, BLOOM transforms active voice into passive voice, GPT-J utilizes the phrase \u201cthe term used\u201d and substitutes \u201crefer to\u201d\nwith \u201cdenote\u201d, while LLaMA adopts a more informal style by mentioning the \u201ccommonly used term\u201d. In the generation task, both BLOOM and GPT-J present similar prompts that emphasize topic coverage. LLaMA maintains the original sentence structure but modifies the subjects and replaces \u201cdecorate\u201d with \u201cadorn\u201d. In the classification task, all three LLMs rearrange the word order and offer additional details about the topic. Therefore, MAPO demonstrates its prompt optimization capabilities by adapting better prompts to specific tasks for different LLMs while preserving core information and adjusting tone or structure as necessary.\nHowever, there are also some errors during prompt optimization, including prompts with incomplete sentences, prompts with improper prepositions or missing necessary parts, and prompts with ambiguous meanings, etc. Therefore, there is ample room for improvement in MAPO to better adapt to different LLMs in downstream tasks."
        },
        {
            "heading": "4.5 Exploratory Analysis",
            "text": "We conduct an exploratory analysis to further investigate the patterns in optimized prompt as shown in Fig. 7, Fig. 8 and Fig. 9. We extract the three most frequent words from the original prompt and investigate their distribution in the optimized prompt for each LLM, while either retaining high-frequency words in instructions (including sentence, topics, subjects, present, statement, discussed, mentioned, included, following) or removing them.\nTaking the generation task (Fig. 7) as an example, when high-frequency words in instructions are retained, we observe that BLOOM retains a relatively higher proportion of the original prompts compared to GPT-J and LLaMA, while LLaMA retains the fewest. When these words are removed,\nwe notice that BLOOM has a higher proportion of words like \u201cman\u201d, \u201cview\u201d in its optimized prompts, which are more relative with human. GPT-J has a higher proportion of words like \u201cmatch\u201d, \u201cgrass\u201d, \u201cbathroom\u201d, \u201cwhite\u201d, which suggests it focuses on specific scenes, objects, or themes. LLaMA has a higher proportion of words like \u201croom\u201d, \u201cclose\u201d, \u201cplaying\u201d, indicating its preferences on place and experiences. The variations observed in word distribution indicate that each LLM tends to emphasize different aspects during the optimization process. Accurate conclusions need more experiments."
        },
        {
            "heading": "5 Related Work",
            "text": "LLMs\u2019 prompt optimization process involves prompt retrieval, prompt generation from scratch and prompt editing. For prompt retrieval, for example, Ma et al. (2023) adopt greedy search to identify near-optimal prompts. Zhou et al. (2022) introduce APE for automatic instruction selection, etc. For prompt generation from scratch, Pang et al. (2023) introduce SharpT, which learns a shared latent space and generates soft prompts. White et al. (2023) describe a catalog of prompt engineering techniques. Zamfirescu-Pereira et al. (2023) investigate end-user prompt engineering using a prototype LLM-based chatbot design tool. Wang et al. (2022a) present Self-Instruct for improving instruction-following capabilities of PLMs. For prompt editing, Gao et al. (2020) automatically select label words and generate templates. Pryzant et al. (2023) introduce APO based on \u201cgradients\u201d to provide critical feedback on the current prompt. Deng et al. (2022) propose RLprompt based on RL. Zhang et al. (2023) propose TEMPERA, which provides interpretable prompts for different queries. Prasad et al. (2022) introduce GrIPS, a gradientfree approach for improving task instructions for LLMs. Moreover, some research focuses on incor-\nporating additional knowledge to enhance prompt editing. For example, Li et al. (2023) propose DSP to generate \u201cdirectional stimulus\u201d of each input. Qin and Eisner (2021a) optimize a mixture of prompts using gradient descent to generate relational knowledge. Shin et al. (2020) develop Autoprompt, a gradient-guided approach to find the best tokens in the prompt. Jiang et al. (2020) propose mining-based and paraphrasing-based methods to automatically generate diverse prompts. Furthermore, some research focus on continuous prompt optimization instead of discrete prompt optimization mentioned before, such as research by Zheng et al. (2023), Hambardzumyan et al. (2021), Zhong et al. (2021), etc.\nHowever, all above-mentioned prompts optimization approaches aim to obtain task-specific prompts instead of model-specific ones. Different from theirs, we dedicate at optimizing prompts for LLMs within the NLP domain and achieve impressive performance."
        },
        {
            "heading": "6 Conclusions",
            "text": "The remarkable capabilities of LLMs have revolutionized NLP in various tasks. However, their performance heavily relies on the quality of prompts. In this work, we address the prompt optimization challenge by proposing a Model-Adaptive Prompt Optimization (MAPO) approach. Through extensive experiments, we demonstrated that MAPO can adapt different LLMs with generating modelfriendly prompts to enhance their capabilities across various downstream tasks. In future work, we aim to construct more fine-grained modeladaptive prompts that can adapt to the continuously evolving data encountered in real-world production environments. Additionally, we intend to enhance its applicability across a broad spectrum of linguistic contexts.\nLimitations It is important to acknowledge certain limitations of our approach. Firstly, the effectiveness of prompt optimization heavily relies on the availability and quality of the warm-up dataset. In cases where the dataset is limited or does not sufficiently cover the specific task, the performance gains from prompt optimization may be constrained. Additionally, MAPO requires extensive SFT and RL, which can be computationally expensive and time-consuming. This could limit the scalability of MAPO, especially when dealing with large-scale tasks or datasets. Despite these limitations, our study provides valuable insights into model-adaptive prompt optimization for LLMs and contributes to the ongoing efforts in improving the performance of these LLMs in practical applications."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported by Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), the National Natural Science Foundation of China (No.62072323, U21A20488), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), Key Projects of Industrial Foresight and Key Core Technology Research and Development in Suzhou(SYC2022009)."
        },
        {
            "heading": "A Candidate Prompts",
            "text": "For the nine datasets selected in P3, we present one prompt and the corresponding three candidate prompts for each dataset, as shown in Table 8."
        },
        {
            "heading": "B Training Details",
            "text": "We provide the training details as shown in Table 9. Other hyper-parameters are set to default."
        },
        {
            "heading": "C Computational Cost",
            "text": "While the training phase is computationally intensive, the generation phase is relatively lightweight. Specifically, once the prompt optimizing model MAPO is trained, the prompt generation process simply involves a feed forward propagation to generate the optimal prompt instead of further optimization through SFT and RL, thus significantly reducing the computational complexity. We list the computational complexity during the training and inference phase:\nTraining Phase. During the training phase, initially, a warm-up dataset is established. This involves generating candidate prompts using a model like GPT-3.5. For each original prompt, 1000 candidate prompts are generated. This leads to a time and space complexity of O(N \u00d7 M), where N is the number of original prompts, and M is the number of candidates per prompt. Subsequently, an optimal prompt is searched for, which involves comparisons among candidate prompts, yielding complexities of O(N \u00d7 M) for both time and space. Building the prompt optimizer is the next stage. Supervised fine-tuning (SFT) has a time complexity of O(E \u00d7 B \u00d7 T ), with E being the number of epochs, B the batch size, and T the number of model parameters. Its space complexity mainly arises from model parameters and gradients, which is O(T ). For building the reward\nmodel, both time and space complexities are mainly O(N \u00d7M \u00d7 logM). The reinforcement learning (RL) part requires O(E\u2032\u00d7B\u2032\u00d7T ) time, where E\u2032 is the number of epochs specific to RL, B\u2032 is the batch size in RL, and T remains the model parameters. The space complexity is O(T ). Summing these up, the total time complexity for the training phase becomes O(N \u00d7 M) + O(E \u00d7 B \u00d7 T ) + O(N\u00d7M\u00d7 logM)+O(E\u2032\u00d7B\u2032\u00d7T ). For space, it\u2019s O(N \u00d7M \u00d7 logM) +O(T ).\nInference Phase. In the inference phase, an optimized prompt is generated from an original\nprompt using the MAPO technique. The time complexity here is dominated by a single feed-forward operation, which is O(T ). There is almost negligible extra space required, making the space complexity effectively O(1) for this phase.\nWe also caclulate how long it roughly takes for a complete training run. For a LLaMA-7B model running on four A100 80GB GPUs, SFT on a highscale task (such as the News classification task with 120,000 training data) takes about 8 hours, RL takes about 12 hours, and the complete MAPO process takes roughly 20 hours in total; For a Bloom-7B\nmodel under the same hardware conditions, SFT takes about 5 hours, RL takes about 9 hours, and the total time for MAPO takes about 14 hours; For a GPT-J-6B model, SFT takes about 10 hours, RL takes about 16 hours, and the total time for MAPO takes about 26 hours."
        },
        {
            "heading": "D Baselines",
            "text": "We compared MAPO with several State-Of-TheArt (SOTA) prompt optimization baselines, including the following:\n\u2022 Finetuning (Devlin et al., 2018): Finetuning (few-shot) involves finetuning the entire language model with a classification head using a few-shot dataset.\n\u2022 Soft Prompt (Qin and Eisner, 2021b; Li and Liang, 2021): Soft Prompt Tuning utilizes continuous embeddings as a variant of parameter-efficient transfer learning, replacing discrete prompts.\n\u2022 Black-Box (Sun et al., 2022): Black-Box Tuning combines discrete and soft prompts, with the soft part trained using gradient descent and the discrete part optimized using a gradientfree tuner.\n\u2022 Autoprompt (Shin et al., 2020): Autoprompt incorporates discrete trigger tokens and updates prompts through iterative gradient search.\n\u2022 Manual (Brown et al., 2020; Schick and Sch\u00fctze, 2020; Sanh et al., 2021): Manual\nprompt achieves strong performance on various natural language understanding and natural language generation tasks without relying on training examples.\n\u2022 In-Context (Brown et al., 2020): In-Context Demonstration randomly selects a training example and concatenates it with the input query.\n\u2022 Instructions: Self-Instruction manually creates prompts for each task following Natural Instructions (Wang et al., 2022b), where the prompt is concatenated with the inputs.\n\u2022 GrIPS (Prasad et al., 2022): GrIPS performs phrase-level editing on the instructions and selects the best one.\n\u2022 RLprompt (Deng et al., 2022): RLprompt generates discrete prompts using a reinforcement learning (RL) framework.\n\u2022 TEMPERA (Zhang et al., 2023): TEMPERA is a test-time prompt editing method that uses reinforcement learning, efficiently leveraging prior knowledge and adapting to different queries, while providing an interpretable prompt for each query.\n\u2022 AMA (Arora et al., 2022): AMA recursively reformats tasks and prompts using the LLM to effectively aggregate predictions across prompts using weak supervision.\nFor a fair assessment, we adopt the same experimental setup as in LM-BFF (Gao et al., 2020) and RLPrompt (Deng et al., 2022). We take 16 training samples from each class in our training dataset for every task, making them our few-shot dataset. So, if we consider all the classes (Y), we have a total of 16 times the number of classes as our training samples. Similarly, we pick 16 samples from each class to form our validation dataset. Besides this usual setup, we also select n random examples from our training data. We call this our \u201cin-context exemplar pool\u201d. For consistency, we repeat our experiments four times using different random seeds. Afterward, we calculate the average results and note down the usual variation we see between the results. For our language model, we\u2019ve chosen to use RoBERTa large (Liu et al., 2019). We base our initial guidelines on the Natural Instructions (Mishra et al., 2021). We also ensure that the\nfirst examples we give for context are randomly picked from a set of 16. This set is different from our few-shot dataset and is also randomly picked from our main training data. By comparing MAPO with these SOTA baselines, we gain insights into the performance and effectiveness of MAPO in various downstream tasks."
        },
        {
            "heading": "E Datasets",
            "text": "We utilized nine representative datasets from P3 (Sanh et al., 2021) to establish the warm-up dataset, covering question-answering, classification, and generation tasks. The selected datasets for each task are as follows:\n\u2022 Question-Answering Task: AdverQA (https://huggingface.co/datasets/bigscience/ P3/tree/main/data/adversarial_qa_ dbidaf_question_context_answer), OpenQA (https://huggingface.co/ datasets/bigscience/P3/tree/main/data/ openbookqa_main_which_correct), CloseQA (https://huggingface.co/datasets/bigscience/ P3/tree/main/data/sciq_Direct_Question_ Closed_Book_).\n\u2022 Classification Task: News (https: //huggingface.co/datasets/bigscience/P3/ tree/main/data/ag_news_classify), Movie (https://huggingface.co/datasets/bigscience/ P3/tree/main/data/rotten_tomatoes_ Movie_Expressed_Sentiment), QASC (https://huggingface.co/datasets/bigscience/ P3/tree/main/data/qasc_is_correct_1)\n\u2022 Generation Task: Topics (https://huggingface. co/datasets/bigscience/P3/tree/main/data/ common_gen_topics_from_the_sentence), Summary (https://huggingface.co/datasets/ bigscience/P3/tree/main/data/samsum_ Sum_up_the_following_dialogue), Explan (https://huggingface.co/datasets/bigscience/ P3/tree/main/data/cos_e_v1.11_generate_ explanation_given_text).\nWe evaluate our proposed MAPO method, along with other SOTA baselines, on the following datasets for validation: SST-2 (Socher et al., 2013), Yelp Polarity (Zhang et al., 2015), MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), QNLI (Demszky et al., 2018), SNLI (Bowman et al., 2015),\nMNLI (Williams et al., 2017), MRPC (Dolan and Brockett, 2005). These datasets provide a comprehensive evaluation of MAPO\u2019s performance compared to other baselines across a range of tasks, including sentiment analysis, text classification, natural language inference, and paraphrase identification, etc.\nSpecifically, for Table 3, the training data aligns with that used by TEMPERA (Zhang et al., 2023), that is all experiments, including our own, use Roberta-large as the backbone for validating the downstream tasks. Because the setup employs a \u201cfew-shot\u201d methodology that has elaborated before, we name Table 3 as \u201cfew-shot\u201d. For Table 5 and 6, there is no training data involved; the LM performs zero-shot inference. That means all reported results occur without training on the datasets in Table 5 and 6. The purpose is to demonstrate the generalization (domain transfer) ability of our MAPO method. If one wishes to further enhance performance on these datasets, additional training with labeled data on Table 5 and 6 becomes necessary."
        },
        {
            "heading": "F Additional Experiments",
            "text": "The performance of the reward model. We plot the performance of the reward model during the training process of MAPO as shown in Fig. 4. As the training progresses, the reward model exhibits consistent growth and improvement. The consistent increase indicates that the reward model is gradually becoming more proficient in downstream tasks. It successfully adapts to its environment, leading to improved outcomes and higher task completion rates. Therefore, it can serve as a discriminator of the goodness of an optimized prompt.\nThe original capabilities maintaining ability of MAPO. We further analyze the original capabilities maintaining ability of MAPO. We use a language model trained with MAPO, which has the ability to optimize prompts but without losing its original capabilities, to modify prompts and accomplish downstream tasks. We believe that the GLUE and SuperGLUE tasks are representative, hence we use them as pre-training tasks. However, the improvements in Table 5 and 6 are not significant, possibly due to the limited scope of our pre-training tasks. Future work can explore using a broader range of datasets for pre-training, which may lead to more significant improvements in various downstream tasks.\nMoreover, for Table 3, the training and validation data for SFT, RM, and RL are different from the data used for generalization, although they all come from Table 3. This is because we consider GLUE and SuperGLUE tasks to be representative, hence we use them as pre-training tasks. Theoretically, a more diverse NLP dataset should be selected for this part, but we happened to choose this subset. To mitigate the impact on the results, we also run another test with two steps: using the optimized prompts generated by MAPO and then using the original Roberta-Large model to make inference. As shown in Table 3 (the row \u201cMAPO-w/o g\u201d), the results do not show a significant decline, with a t-test greater than 0.05. The use of data from Table 3 for generalization is merely to ensure that the prompt-optimized model retains its original capabilities for downstream tasks instead of data leakage."
        },
        {
            "heading": "G Additional Cases",
            "text": "We list more cases whose prompts have been optimized by our proposed MAPO as shown in Table 19. We make detailed analysis for the difference among LLMs as follows:\n\u2022 In SST-2, BLOOM and LLaMA both use phrases like \u201cterrific flair\u201d and \u201cremarkable skill\u201d to describe Khouri\u2019s ability, emphasizing positive sentiment. GPT-J uses the phrase \u201ctremendous artistry,\u201d highlighting the artistic aspect, but does not explicitly convey the positive sentiment as strongly as BLOOM and LLaMA.\n\u2022 In Yelp, BLOOM and LLaMA use phrases like \u201cquality of the food is commendable\u201d and \u201cservice provided is inconsistent\u201d to provide a balanced assessment. GPT-J and the original version have the same wording, emphasizing the hit-or-miss nature of the service.\n\u2022 In MR, BLOOM and LLaMA use phrases like \u201cadmirable endeavor\u201d and \u201cpraiseworthy pursuit\u201d to highlight the positive qualities of the venture. GPT-J and the original version use neutral language without explicitly conveying positive or negative sentiment.\n\u2022 In CR, BLOOM, GPT-J, and LLaMA all express confusion or potential confusion regarding the positioning of the space key on a phone. The wording in BLOOM and LLaMA suggests that using a different key for text input is more common in phones, implying a deviation from the norm.\n\u2022 In RTE, BLOOM and LLaMA emphasize the impact of the situation by using phrases like \u201csomber site\u201d and \u201cdistressing sight\u201d when describing the washed-up marine animals. GPTJ and the original version provide more neutral descriptions without explicitly conveying the emotional aspect.\n\u2022 In QNLI, BLOOM, GPT-J, and LLaMA all rephrase the sentence 2, maintaining the same overall meaning. The variations in wording are mainly stylistic, with BLOOM, GPT-J,\nand LLaMA using different synonyms to convey the same information.\n\u2022 In SNLI, BLOOM, GPT-J, and LLaMA rephrase the sentence 1 by adding additional details related to the slip and slide activity and the celebratory context. The variations in wording are mainly stylistic, enhancing the description of the baby\u2019s experience and the wetness.\n\u2022 In MNLI, BLOOM, GPT-J, and LLaMA maintain the same wording as the original sentence 1. The variations in wording occur in sentence 2, with BLOOM and GPT-J emphasizing the need for interest rates to increase, while LLaMA focuses on the importance of boosting savings.\n\u2022 In MRPC, BLOOM, GPT-J, and LLaMA all maintain the same wording as the original sentences. The variations in the rephrased sentence 1 (BLOOM and LLaMA) emphasize the 15 percent drop in revenue, while GPT-J maintains a more neutral tone."
        },
        {
            "heading": "H Additional Exploratory Analysis",
            "text": "We further analyze the distribution of the top 3 words from the original prompts in the optimized prompts of different LLMs in both the QA and\nclassification tasks as shown in Fig. 8 and Fig. 9, respectively. In the QA task, we observe minimal variations when considering whether to remove the instruction. After prompt optimization, BLOOM has a higher proportion of words like \u201ccontemporary\u201d, \u201cfrench\u201d, \u201cMethodist\u201d, \u201cplaces\u201d, \u201ceducation\u201d, \u201cpower\u201d, and \u201clife\u201d compared to the other two models. GPT-J has a higher proportion of words like \u201cchurch\u201d, \u201ctime\u201d, \u201corder\u201d, \u201cearly\u201d, and \u201cyear\u201d, indicating a focus on temporal and sequential aspects. And LLaMA has a higher proportion of words like \u201cearlier\u201d, \u201csimilar\u201d, \u201cnumber\u201d, \u201csong\u201d, and \u201cproperty\u201d compared to the other two models. In the classification task, we also observe minimal variations when considering whether to remove the instruction. After optimization, BLOOM has a higher proportion of the word \u201cyear\u201d, \u201cnew\u201d compared to the other two models. GPT-J has a higher proportion of words like \u201clargest\u201d, \u201cmusic\u201d, \u201cnational\u201d, \u201cschool\u201d and \u201cpoland\u201d. LLaMA has a higher proportion of words like \u201cincrease\u201d, \u201cgoverment\u201d, \u201cexecutive\u201d, \u201cmedical\u201d, \u201cwarsaw\u201d, and \u201cparliament\u201d compared to the other two LLMs.\nThese findings strongly suggest that each LLM exhibits unique preferences and patterns in prompt optimization across different tasks. The observed variations in word distribution clearly indicate the specific areas of focus and the semantic nuances that each LLM emphasizes during the optimization process. Additional experiments will contribute to a more comprehensive understanding of the prompt\noptimization dynamics exhibited by each LLM.\nTask Dataset BLOOM M \u2191 \u2191 \u2191(%) \u2191(%) GPT-J M (\u2191) \u2191 (\u2191(%)) \u2191(%) Coref. xwinograd 60.1 60.6 0.5 0.5 0.9 0.9 - - - - - -\nNLU\nBoolQ 67.9 68.2 0.3\n0.6\n0.4\n0.9 67.2 67.9 0.7 0.3 1 0.5 CB 77.6 78.1 0.5 0.6 83.9 84.2 0.3 0.4 COPA 74.0 75.0 1.0 1.4 84 84.2 0.2 0.2 MultiRC 59.7 60.4 0.7 1.2 63.8 64.1 0.3 0.5 ReCoRD 69.8 70.2 0.4 0.6 74.4 74.7 0.3 0.4 WiC 61.4 62.0 0.6 1.0 61 61.3 0.3 0.5 WSC 64.4 65.1 0.7 1.1 77.9 78.1 0.2 0.3\nNLI\nANLI R1 31.5 32.1 0.6\n0.5\n1.9\n1.3 37.8 38.2 0.4 0.3 1.1 0.7 ANLI R2 35.1 35.4 0.3 0.9 37.9 38.3 0.4 1.1 ANLI R3 37.1 37.8 0.7 1.9 40.9 41.1 0.2 0.5 StoryCloze 79.0 79.5 0.5 0.6 87.8 87.9 0.1 0.1\nCLS Amazon 65.2 67.7 2.5 2.3 3.8 3.3 68.2 69.4 1.2 1.2 1.8 1.6\nDBPedia 70.5 72.5 2.0 2.8 83.9 85.1 1.2 1.4\nQA\nDROP 67.9 69.9 2.0\n1.8\n2.9\n5.6 51.6 52.8 1.2 1 2.3 3.2 NQ 15.1 16.1 1.0 6.6 19.6 20.8 1.2 6.1 RealTimeQA 29.0 31.5 2.5 8.6 36 37.2 1.2 3.3 WebQs 34.8 36.3 1.5 4.3 44.1 44.6 0.5 1.1\nTask Dataset BLOOM M-0 \u2191 \u2191 \u2191(%) \u2191(%) GPT-J M-0 (\u2191) \u2191 (\u2191(%)) \u2191(%) Coref. xwinograd 60.1 60.5 0.4 0.4 0.7 0.7 - - - - - -\nNLU\nBoolQ 67.9 68.0 0.1\n0.5\n0.1\n0.7 67.2 67.8 0.6 0.3 0.9 0.4 CB 77.6 78.0 0.4 0.5 83.9 84.1 0.2 0.2 COPA 74.0 74.8 0.8 1.1 84 84.2 0.2 0.2 MultiRC 59.7 60.4 0.7 1.2 63.8 64.1 0.3 0.5 ReCoRD 69.8 70.2 0.4 0.6 74.4 74.7 0.3 0.4 WiC 61.4 61.8 0.4 0.7 61 61.3 0.3 0.5 WSC 64.4 65.0 0.6 0.9 77.9 78.1 0.2 0.3\nNLI\nANLI R1 31.5 32.0 0.5\n0.5\n1.6\n1.2 37.8 38.2 0.4 0.2 1.1 0.6 ANLI R2 35.1 35.4 0.3 0.9 37.9 38.1 0.2 0.5 ANLI R3 37.1 37.8 0.7 1.9 40.9 41.1 0.2 0.5 StoryCloze 79.0 79.4 0.4 0.5 87.8 87.9 0.1 0.1\nCLS Amazon 65.2 67.3 2.1 1.8 3.2 2.7 68.2 69 0.8 1 1.2 1.3\nDBPedia 70.5 72.0 1.5 2.1 83.9 85.1 1.2 1.4\nQA\nDROP 67.9 69.5 1.6\n1.5\n2.4\n4.6 51.6 52.3 0.7 0.7 1.4 2.3 NQ 15.1 15.8 0.7 4.6 19.6 20.4 0.8 4.1 RealTimeQA 29.0 31.2 2.2 7.6 36 37 1 2.8 WebQs 34.8 36.1 1.3 3.7 44.1 44.5 0.4 0.9\nTask Dataset BLOOM M-0.2 \u2191 \u2191 \u2191(%) \u2191(%) GPT-J M-0.2 (\u2191) \u2191 (\u2191(%)) \u2191(%) Coref. xwinograd 60.1 60.6 0.5 0.5 0.8 0.8 - - - - - -\nNLU\nBoolQ 67.9 68.1 0.2\n0.5\n0.3\n0.7 67.2 67.9 0.7 0.3 1 0.4 CB 77.6 78.1 0.5 0.6 83.9 84.2 0.3 0.4 COPA 74.0 74.6 0.6 0.8 84 84.2 0.2 0.2 MultiRC 59.7 60.3 0.6 1.0 63.8 64 0.2 0.3 ReCoRD 69.8 70.2 0.4 0.6 74.4 74.6 0.2 0.3 WiC 61.4 61.7 0.3 0.5 61 61.3 0.3 0.5 WSC 64.4 65.1 0.7 1.1 77.9 78.1 0.2 0.3\nNLI\nANLI R1 31.5 32.2 0.7\n0.5\n2.2\n1.3 37.8 38.2 0.4 0.3 1.1 0.7 ANLI R2 35.1 35.5 0.4 1.1 37.9 38.3 0.4 1.1 ANLI R3 37.1 37.6 0.5 1.3 40.9 41.1 0.2 0.5 StoryCloze 79.0 79.5 0.5 0.6 87.8 87.9 0.1 0.1\nCLS Amazon 65.2 67.1 1.9 1.6 2.9 2.4 68.2 69.1 0.9 1 1.3 1.3\nDBPedia 70.5 71.8 1.3 1.8 83.9 85 1.1 1.3\nQA\nDROP 67.9 69.3 1.4\n1.2\n2.1\n3.8 51.6 52.6 1 0.7 1.9 2.1 NQ 15.1 15.6 0.5 3.3 19.6 20.3 0.7 3.6 RealTimeQA 29.0 31.0 2.0 6.9 36 36.7 0.7 1.9 WebQs 34.8 35.8 1.0 2.9 44.1 44.5 0.4 0.9\nTask Dataset BLOOM M-0.5 \u2191 \u2191 \u2191(%) \u2191(%) GPT-J M-0.5 (\u2191) \u2191 (\u2191(%)) \u2191(%) Coref. xwinograd 60.1 60.4 0.3 0.3 0.5 0.5 - - - - - -\nNLU\nBoolQ 67.9 68.0 0.1\n0.3\n0.1\n0.5 67.2 67.6 0.4 0.2 0.6 0.2 CB 77.6 77.8 0.2 0.3 83.9 84.1 0.2 0.2 COPA 74.0 74.4 0.4 0.5 84 84.2 0.2 0.2 MultiRC 59.7 60.3 0.6 1.0 63.8 63.9 0.1 0.2 ReCoRD 69.8 70.1 0.3 0.4 74.4 74.5 0.1 0.1 WiC 61.4 61.6 0.2 0.3 61 61.2 0.2 0.3 WSC 64.4 64.8 0.4 0.6 77.9 78 0.1 0.1\nNLI\nANLI R1 31.5 32.0 0.5\n0.3\n1.6\n0.9 37.8 38.2 0.4 0.2 1.1 0.5 ANLI R2 35.1 35.3 0.2 0.6 37.9 38.1 0.2 0.5 ANLI R3 37.1 37.5 0.4 1.1 40.9 41 0.1 0.2 StoryCloze 79.0 79.3 0.3 0.4 87.8 87.9 0.1 0.1\nCLS Amazon 65.2 66.7 1.5 1.2 2.3 1.8 68.2 68.9 0.7 0.5 1 0.8\nDBPedia 70.5 71.4 0.9 1.3 83.9 84.3 0.4 0.5\nQA\nDROP 67.9 68.5 0.6\n0.8\n0.9\n2.7 51.6 52.2 0.6 0.5 1.2 1.7 NQ 15.1 15.5 0.4 2.6 19.6 20.2 0.6 3.1 RealTimeQA 29.0 30.6 1.6 5.5 36 36.6 0.6 1.7 WebQs 34.8 35.4 0.6 1.7 44.1 44.4 0.3 0.7\nTask Dataset BLOOM M-0.8 \u2191 \u2191 \u2191(%) \u2191(%) GPT-J M-0.8 (\u2191) \u2191 (\u2191(%)) \u2191(%) Coref. xwinograd 60.1 60.2 0.1 0.1 0.2 0.2 - - - - - -\nNLU\nBoolQ 67.9 68.0 0.1\n0.3\n0.1\n0.4 67.2 67.5 0.3 0.2 0.4 0.2 CB 77.6 77.8 0.2 0.3 83.9 84.1 0.2 0.2 COPA 74.0 74.3 0.3 0.4 84 84.2 0.2 0.2 MultiRC 59.7 60.3 0.6 1.0 63.8 63.9 0.1 0.2 ReCoRD 69.8 70.1 0.3 0.4 74.4 74.5 0.1 0.1 WiC 61.4 61.6 0.2 0.3 61 61.2 0.2 0.3 WSC 64.4 64.7 0.3 0.5 77.9 78 0.1 0.1\nNLI\nANLI R1 31.5 31.8 0.3\n0.3\n1.0\n0.8 37.8 38.1 0.3 0.2 0.8 0.4 ANLI R2 35.1 35.3 0.2 0.6 37.9 38 0.1 0.3 ANLI R3 37.1 37.5 0.4 1.1 40.9 41 0.1 0.2 StoryCloze 79.0 79.3 0.3 0.4 87.8 87.9 0.1 0.1\nCLS Amazon 65.2 66.5 1.3 1.1 2.0 1.6 68.2 68.8 0.6 0.4 0.9 0.7"
        }
    ],
    "title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization",
    "year": 2023
}