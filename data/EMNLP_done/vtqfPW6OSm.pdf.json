{
    "abstractText": "Tasks that model the relation between pairs of tokens in a string are a vital part of understanding natural language. Such tasks, in general, require exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime complexity in the length of the string. We show that these exhaustive comparisons can be avoided, and, moreover, the complexity of such tasks can be reduced to linear by casting the relation between tokens as a partial order over the string. Our method predicts real numbers for each token in a string in parallel and sorts the tokens accordingly, resulting in total orders of the tokens in the string. Each total order implies a set of arcs oriented from smaller to greater tokens, sorted by their predicted numbers. The intersection of total orders results in a partial order over the set of tokens in the string, which is then decoded into a directed graph representing the desired linguistic structure. Our experiments on dependency parsing and coreference resolution show that our method achieves state-of-the-art or comparable performance. Moreover, the linear complexity and parallelism of our method double the speed of graph-based coreference resolution models, and bring a 10-times speed-up over graph-based dependency parsers. https://github.com/lyutyuh/partial",
    "authors": [
        {
            "affiliations": [],
            "name": "Tianyu Liu"
        },
        {
            "affiliations": [],
            "name": "Afra Amini"
        },
        {
            "affiliations": [],
            "name": "Mrinmaya Sachan"
        },
        {
            "affiliations": [],
            "name": "Ryan Cotterell"
        }
    ],
    "id": "SP:27b84d807da9d27e674c7fc1e1259e4e166bbd51",
    "references": [
        {
            "authors": [
                "Kazimierz Adjukiewicz."
            ],
            "title": "Die syntaktische Konnexit\u00e4t",
            "venue": "Studia Philosophica, 1:1\u201327.",
            "year": 1935
        },
        {
            "authors": [
                "Afra Amini",
                "Ryan Cotterell."
            ],
            "title": "On parsing as tagging",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8884\u20138900, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Afra Amini",
                "Tianyu Liu",
                "Ryan Cotterell."
            ],
            "title": "Hexatagging: Projective dependency parsing as tagging",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1453\u20131464, Toronto, Canada.",
            "year": 2023
        },
        {
            "authors": [
                "Ben Athiwaratkun",
                "Andrew Gordon Wilson."
            ],
            "title": "On modeling hierarchical data via probabilistic order embeddings",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyung Hyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K.A. Baker",
                "P.C. Fishburn",
                "F.S. Roberts."
            ],
            "title": "Partial orders of dimension 2",
            "venue": "Networks, 2(1):11\u201328.",
            "year": 1972
        },
        {
            "authors": [
                "Srinivas Bangalore",
                "Aravind K. Joshi."
            ],
            "title": "Supertagging: An approach to almost parsing",
            "venue": "Computational Linguistics, 25(2):237\u2013265.",
            "year": 1999
        },
        {
            "authors": [
                "Yehoshua Bar-Hillel."
            ],
            "title": "A quasi-arithmetical notation for syntactic description",
            "venue": "Language, 29(1):47\u2013",
            "year": 1953
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer. arXiv:2004.05150",
            "year": 2020
        },
        {
            "authors": [
                "Jon Louis Bentley."
            ],
            "title": "Decomposable searching problems",
            "venue": "Information Processing Letters, 8(5):244\u2013251.",
            "year": 1979
        },
        {
            "authors": [
                "Jon Louis Bentley."
            ],
            "title": "Multidimensional divide-andconquer",
            "venue": "Commun. ACM, 23(4):214\u2013229.",
            "year": 1980
        },
        {
            "authors": [
                "Mark de Berg",
                "Otfried Cheong",
                "Marc van Kreveld",
                "Mark Overmars."
            ],
            "title": "Computational Geometry: Algorithms and Applications, 3rd edition",
            "venue": "SpringerVerlag TELOS, Santa Clara, CA, USA.",
            "year": 2008
        },
        {
            "authors": [
                "G. Birkhoff."
            ],
            "title": "Lattice Theory",
            "venue": "American Mathematical Society colloquium publications. American Mathematical Society.",
            "year": 1967
        },
        {
            "authors": [
                "Arthur Cayley."
            ],
            "title": "A theorem on trees",
            "venue": "Quarterly Journal of Mathematics, 23:376\u2013378.",
            "year": 1889
        },
        {
            "authors": [
                "Bernard Chazelle."
            ],
            "title": "A functional approach to data structures and its use in multidimensional searching",
            "venue": "SIAM Journal on Computing, 17(3):427\u2013462.",
            "year": 1988
        },
        {
            "authors": [
                "Bernard Chazelle."
            ],
            "title": "Lower bounds for orthogonal range searching: I",
            "venue": "The reporting case. Journal of the ACM, 37(2):200\u2013212.",
            "year": 1990
        },
        {
            "authors": [
                "Bernard Chazelle."
            ],
            "title": "Lower bounds for orthogonal range searching: Part II",
            "venue": "The arithmetic model. Journal of the ACM, 37(3):439\u2013463.",
            "year": 1990
        },
        {
            "authors": [
                "Danqi Chen",
                "Christopher Manning."
            ],
            "title": "A fast and accurate dependency parser using neural networks",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750, Doha, Qatar. Association for Com-",
            "year": 2014
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "The Minimalist Program, 20 edition",
            "venue": "The MIT Press.",
            "year": 2015
        },
        {
            "authors": [
                "Yoeng-Jin Chu",
                "Tseng-Hong Liu."
            ],
            "title": "On the shortest arborescence of a directed graph",
            "venue": "Scientia Sinica, 14:1396\u20131400.",
            "year": 1965
        },
        {
            "authors": [
                "Kenneth Ward Church."
            ],
            "title": "A stochastic parts program and noun phrase parser for unrestricted text",
            "venue": "Second Conference on Applied Natural Language Processing, pages 136\u2013143, Austin, Texas, USA. Association for Computational Linguistics.",
            "year": 1988
        },
        {
            "authors": [
                "Marie-Catherine de Marneffe",
                "Christopher D. Manning."
            ],
            "title": "Stanford typed dependencies manual",
            "venue": "Technical report, Technical report, Stanford University.",
            "year": 2008
        },
        {
            "authors": [
                "Timothy Dozat",
                "Christopher D. Manning."
            ],
            "title": "Deep biaffine attention for neural dependency parsing",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Ben Dushnik",
                "E.W. Miller."
            ],
            "title": "Partially ordered sets",
            "venue": "American Journal of Mathematics, 63(3):600\u2013 610.",
            "year": 1941
        },
        {
            "authors": [
                "Chris Dyer",
                "G\u00e1bor Melis",
                "Phil Blunsom."
            ],
            "title": "A critical analysis of biased parsers in unsupervised parsing",
            "venue": "CoRR, abs/1909.09428.",
            "year": 2019
        },
        {
            "authors": [
                "Jack Edmonds."
            ],
            "title": "Optimum branchings",
            "venue": "Journal of Research of the national Bureau of Standards B, 71(4):233\u2013240.",
            "year": 1967
        },
        {
            "authors": [
                "Jason Eisner."
            ],
            "title": "Bilexical grammars and a cubictime probabilistic parser",
            "venue": "Proceedings of the Fifth International Workshop on Parsing Technologies, pages 54\u201365, Boston/Cambridge, Massachusetts, USA. Association for Computational Linguistics.",
            "year": 1997
        },
        {
            "authors": [
                "Jason M. Eisner."
            ],
            "title": "Three new probabilistic models for dependency parsing: An exploration",
            "venue": "COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics.",
            "year": 1996
        },
        {
            "authors": [
                "Michael L. Fredman."
            ],
            "title": "New bounds on the complexity of the shortest path problem",
            "venue": "SIAM Journal on Computing, 5(1):83\u201389.",
            "year": 1976
        },
        {
            "authors": [
                "Carlos G\u00f3mez-Rodr\u00edguez",
                "Michalina Strzyz",
                "David Vilares."
            ],
            "title": "A unifying theory of transition-based and sequence labeling parsing",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3776\u20133793, Barcelona, Spain (On-",
            "year": 2020
        },
        {
            "authors": [
                "Carlos G\u00f3mez-Rodr\u00edguez",
                "David Vilares."
            ],
            "title": "Constituent parsing as sequence labeling",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1314\u20131324, Brussels, Belgium. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Carlos G\u00f3mez-Rodr\u00edguez",
                "Joakim Nivre."
            ],
            "title": "A transition-based parser for 2-planar dependency structures",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1492\u20131501, Uppsala, Sweden. Association for Com-",
            "year": 2010
        },
        {
            "authors": [
                "Carlos G\u00f3mez-Rodr\u00edguez",
                "Joakim Nivre."
            ],
            "title": "Divisible Transition Systems and Multiplanar Dependency Parsing",
            "venue": "Computational Linguistics, 39(4):799\u2013845.",
            "year": 2013
        },
        {
            "authors": [
                "F. Hausdorff."
            ],
            "title": "Grundz\u00fcge der Mengenlehre",
            "venue": "G\u00f6schens Lehrb\u00fccherei/Gruppe I: Reine und Angewandte Mathematik Series. Veit & Company.",
            "year": 1914
        },
        {
            "authors": [
                "Toshio Hiraguchi."
            ],
            "title": "On the dimension of orders",
            "venue": "The Science Reports of the Kanazawa University, 4:1\u201320.",
            "year": 1955
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long Short-Term Memory",
            "venue": "Neural Computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S. Weld",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "SpanBERT: Improving pre-training by representing and predicting spans",
            "venue": "Transactions of the Association for Computational Linguistics, 8:64\u201377.",
            "year": 2020
        },
        {
            "authors": [
                "Ben Kantor",
                "Amir Globerson."
            ],
            "title": "Coreference resolution with entity equalization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 673\u2013677, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Eliyahu Kiperwasser",
                "Miguel Ballesteros."
            ],
            "title": "Scheduled multi-task learning: From syntax to translation",
            "venue": "Transactions of the Association for Computational Linguistics, 6:225\u2013240.",
            "year": 2018
        },
        {
            "authors": [
                "Eliyahu Kiperwasser",
                "Yoav Goldberg."
            ],
            "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
            "venue": "Transactions of the Association for Computational Linguistics, 4:313\u2013 327.",
            "year": 2016
        },
        {
            "authors": [
                "Yuval Kirstain",
                "Ori Ram",
                "Omer Levy."
            ],
            "title": "Coreference resolution without span representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Kitaev",
                "Dan Klein."
            ],
            "title": "Tetra-tagging: Word-synchronous parsing with linear-time inference",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6255\u2013 6261, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Donald E. Knuth."
            ],
            "title": "On the translation of languages from left to right",
            "venue": "Information and Control, 8(6):607\u2013 639.",
            "year": 1965
        },
        {
            "authors": [
                "Donald E. Knuth."
            ],
            "title": "The Art of Computer Programming: Fundamental Algorithms, 3 edition, volume 1",
            "venue": "Addison Wesley Longman Publishing Co., Inc., USA.",
            "year": 1997
        },
        {
            "authors": [
                "Sandra K\u00fcbler",
                "Ryan McDonald",
                "Joakim Nivre."
            ],
            "title": "Dependency Parsing",
            "venue": "Springer Cham.",
            "year": 2009
        },
        {
            "authors": [
                "E.L. Lawler."
            ],
            "title": "Sequencing jobs to minimize total weighted completion time subject to precedence constraints",
            "venue": "B. Alspach, P. Hell, and D.J. Miller, editors, Algorithmic Aspects of Combinatorics, volume 2 of Annals of Discrete Mathematics, pages",
            "year": 1978
        },
        {
            "authors": [
                "Kenton Lee",
                "Luheng He",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "End-to-end neural coreference resolution",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188\u2013197, Copenhagen, Denmark. Association",
            "year": 2017
        },
        {
            "authors": [
                "Zuchao Li",
                "Jiaxun Cai",
                "Shexia He",
                "Hai Zhao."
            ],
            "title": "Seq2seq dependency parsing",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3203\u20133214, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Tianyu Liu",
                "Yuchen Jiang",
                "Ryan Cotterell",
                "Mrinmaya Sachan."
            ],
            "title": "A structured span selector",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Holbrook Mann MacNeille."
            ],
            "title": "Partially ordered sets",
            "venue": "Transactions of the American Mathematical Society, 42(3):416\u2013460.",
            "year": 1937
        },
        {
            "authors": [
                "Mitchell P. Marcus",
                "Beatrice Santorini",
                "Mary Ann Marcinkiewicz."
            ],
            "title": "Building a large annotated corpus of English: The Penn Treebank",
            "venue": "Computational Linguistics, 19(2):313\u2013330.",
            "year": 1993
        },
        {
            "authors": [
                "Peter McCullagh."
            ],
            "title": "Regression models for ordinal data",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 42(2):109\u2013142.",
            "year": 1980
        },
        {
            "authors": [
                "Ryan McDonald",
                "Fernando Pereira."
            ],
            "title": "Online learning of approximate dependency parsing algorithms",
            "venue": "11th Conference of the European Chapter of the Association for Computational Linguistics, pages 81\u201388, Trento, Italy. Association for Computa-",
            "year": 2006
        },
        {
            "authors": [
                "Ryan McDonald",
                "Fernando Pereira",
                "Kiril Ribarov",
                "Jan Haji\u010d."
            ],
            "title": "Non-projective dependency parsing using spanning tree algorithms",
            "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural",
            "year": 2005
        },
        {
            "authors": [
                "George A. Miller."
            ],
            "title": "WordNet: A lexical database for English",
            "venue": "Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "Khalil Mrini",
                "Franck Dernoncourt",
                "Quan Hung Tran",
                "Trung Bui",
                "Walter Chang",
                "Ndapa Nakashole."
            ],
            "title": "Rethinking self-attention: Towards interpretability in neural parsing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Joakim Nivre."
            ],
            "title": "An efficient algorithm for projective dependency parsing",
            "venue": "Proceedings of the Eighth International Conference on Parsing Technologies, pages 149\u2013160, Nancy, France.",
            "year": 2003
        },
        {
            "authors": [
                "Manying Zhang",
                "Hanzhi Zhu"
            ],
            "title": "Universal dependencies 2.2. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics, Charles University",
            "year": 2018
        },
        {
            "authors": [
                "Joakim Nivre",
                "Jens Nilsson."
            ],
            "title": "Pseudoprojective dependency parsing",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 99\u2013106, Ann Arbor, Michigan. Association for Computational Lin-",
            "year": 2005
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Alessandro Moschitti",
                "Nianwen Xue",
                "Olga Uryupina",
                "Yuchen Zhang."
            ],
            "title": "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes",
            "venue": "Joint Conference on EMNLP and CoNLL - Shared Task, pages 1\u201340, Jeju",
            "year": 2012
        },
        {
            "authors": [
                "John Shawe-Taylor",
                "Nello Cristianini."
            ],
            "title": "Kernel Methods for Pattern Analysis",
            "venue": "Cambridge University Press.",
            "year": 2004
        },
        {
            "authors": [
                "Yikang Shen",
                "Zhouhan Lin",
                "Chin-Wei Huang",
                "Aaron Courville."
            ],
            "title": "Neural language modeling by jointly learning syntax and lexicon",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Yikang Shen",
                "Zhouhan Lin",
                "Athul Paul Jacob",
                "Alessandro Sordoni",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "Straight to the tree: Constituency parsing with neural syntactic distance",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Compu-",
            "year": 2018
        },
        {
            "authors": [
                "N.A. Smith."
            ],
            "title": "Linguistic Structure Prediction",
            "venue": "Synthesis digital library of engineering and computer science. Morgan & Claypool.",
            "year": 2011
        },
        {
            "authors": [
                "M. Stede."
            ],
            "title": "Discourse Processing",
            "venue": "Synthesis lectures on human language technologies. Morgan & Claypool.",
            "year": 2012
        },
        {
            "authors": [
                "Mark Steedman."
            ],
            "title": "Combinatory grammars and parasitic gaps",
            "venue": "Natural Language & Linguistic Theory, 5(3):403\u2013439.",
            "year": 1987
        },
        {
            "authors": [
                "Mark Steedman."
            ],
            "title": "The Syntactic Process",
            "venue": "MIT Press, Cambridge, MA, USA.",
            "year": 2000
        },
        {
            "authors": [
                "Michalina Strzyz",
                "David Vilares",
                "Carlos G\u00f3mezRodr\u00edguez."
            ],
            "title": "Viable dependency parsing as sequence labeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Michalina Strzyz",
                "David Vilares",
                "Carlos G\u00f3mezRodr\u00edguez."
            ],
            "title": "Bracketing encodings for 2-planar dependency parsing",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 2472\u20132484, Barcelona, Spain (Online). Inter-",
            "year": 2020
        },
        {
            "authors": [
                "Edward Szpilrajn."
            ],
            "title": "Sur l\u2019extension de l\u2019ordre partiel",
            "venue": "Fundamenta Mathematicae, 16(1):386\u2013389.",
            "year": 1930
        },
        {
            "authors": [
                "Robert Endre Tarjan."
            ],
            "title": "Finding optimum branchings",
            "venue": "Networks, 7(1):25\u201335.",
            "year": 1977
        },
        {
            "authors": [
                "Ben Taskar",
                "Dan Klein",
                "Mike Collins",
                "Daphne Koller",
                "Christopher Manning."
            ],
            "title": "Max-margin parsing",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 1\u20138, Barcelona, Spain. Association for Com-",
            "year": 2004
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Samira Abnar",
                "Yikang Shen",
                "Dara Bahri",
                "Philip Pham",
                "Jinfeng Rao",
                "Liu Yang",
                "Sebastian Ruder",
                "Donald Metzler."
            ],
            "title": "Long range arena : A benchmark for efficient transformers",
            "venue": "International Conference on Learning Representa-",
            "year": 2021
        },
        {
            "authors": [
                "L. Tesni\u00e8re."
            ],
            "title": "\u00c9lements de Syntaxe Structurale",
            "venue": "C. Klincksieck.",
            "year": 1959
        },
        {
            "authors": [
                "Jacobo Valdes",
                "Robert E. Tarjan",
                "Eugene L. Lawler."
            ],
            "title": "The recognition of series parallel digraphs",
            "venue": "Proceedings of the Eleventh Annual ACM Symposium on Theory of Computing, STOC \u201979, page 1\u201312, New York, NY, USA. Association for Computing",
            "year": 1979
        },
        {
            "authors": [
                "Ivan Vendrov",
                "Ryan Kiros",
                "Sanja Fidler",
                "Raquel Urtasun."
            ],
            "title": "Order-embeddings of images and language",
            "venue": "International Conference on Learning Representations.",
            "year": 2015
        },
        {
            "authors": [
                "Xinyu Wang",
                "Kewei Tu."
            ],
            "title": "Second-order neural dependency parsing with message passing and end-to-end training",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Douglas B. West."
            ],
            "title": "Introduction to Graph Theory",
            "venue": "Pearson Modern Classics for Advanced Mathematics Series. Pearson.",
            "year": 2018
        },
        {
            "authors": [
                "Liyan Xu",
                "Jinho D. Choi."
            ],
            "title": "Revealing the myth of higher-order inference in coreference resolution",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8527\u20138533, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Naiwen Xue",
                "Fei Xia",
                "Fu-dong Chiou",
                "Marta Palmer."
            ],
            "title": "The penn chinese treebank: Phrase structure annotation of a large corpus",
            "venue": "Natural Language Engineering, 11(2):207\u2013238.",
            "year": 2005
        },
        {
            "authors": [
                "Hiroyasu Yamada",
                "Yuji Matsumoto."
            ],
            "title": "Statistical dependency analysis with support vector machines",
            "venue": "Proceedings of the Eighth International Conference on Parsing Technologies, pages 195\u2013206, Nancy, France.",
            "year": 2003
        },
        {
            "authors": [
                "Songlin Yang",
                "Kewei Tu."
            ],
            "title": "Headed-span-based projective dependency parsing",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2188\u20132200, Dublin, Ireland. Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "XLNet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran",
            "year": 2019
        },
        {
            "authors": [
                "Mihalis Yannakakis."
            ],
            "title": "The complexity of the partial order dimension problem",
            "venue": "SIAM Journal on Algebraic Discrete Methods, 3(3):351\u2013358.",
            "year": 1982
        },
        {
            "authors": [
                "Anssi Mikael Yli-Jyr\u00e4."
            ],
            "title": "Multiplanarity-a model for dependency structures in treebanks",
            "venue": "TLT 2003, Proceedings of the Second Workshop on Treebanks and Linguistic Theories. V\u00e4xj\u00f6 University Press.",
            "year": 2003
        },
        {
            "authors": [
                "Yu Zhang",
                "Zhenghua Li",
                "Min Zhang."
            ],
            "title": "Efficient second-order TreeCRF for neural dependency parsing",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295\u20133305, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Yue Zhang",
                "Stephen Clark."
            ],
            "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing",
            "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562\u2013571,",
            "year": 2008
        },
        {
            "authors": [
                "Junru Zhou",
                "Hai Zhao."
            ],
            "title": "Head-Driven Phrase Structure Grammar parsing on Penn Treebank",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2396\u2013 2408, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [],
            "title": "Hyperparameter Settings F.1 Dependency Parsing For pretrained language models, we use XLNet-large-cased11 (Yang et al., 2019) for PTB, bert-base-chinese12 for CTB, and bert-base-multilingual-cased13 for UD",
            "year": 2019
        },
        {
            "authors": [
                "Amini"
            ],
            "title": "to derive the dependency annotations from the treebank annotations using the Stanford Dependency converter v3.3.0",
            "venue": "(de Marneffe and Manning,",
            "year": 2008
        },
        {
            "authors": [
                "Zhang",
                "Clark"
            ],
            "title": "16,091 sentences for training, 803 for development, and 1,910 for testing. For UD, we follow previous work (Zhang et al., 2020; Yang and Tu, 2022) and use the standard splits of the following corpora for experiments: BG-btb, CA-ancora, CS-pdt, DE-gsd, EN-ewt, ES-ancora, FR-gsd, IT-isdt, NL-alpino, NO-rrt, RO-rrt, RU-syntagrus. Licenses",
            "venue": "testing. For CTB,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "https://github.com/lyutyuh/partial"
        },
        {
            "heading": "1 Introduction",
            "text": "Strings of tokens in natural language are not constructed arbitrarily. Indeed, which tokens co-occur within the same string is highly structured according to the rules of the language. Understanding such structures is critical to the comprehension of natural language. In natural language processing (NLP), many structured prediction tasks aim to automatically extract the underlying structure that dictates the relationship between the tokens in a string of text. Examples of such tasks include\ndependency parsing, semantic parsing, and coreference resolution. These tasks involve predicting complex and hierarchical output structures, making them inherently more challenging than their classification or regression counterparts. This paper contributes a novel and generic framework for structured prediction with empirical evidence from dependency parsing and coreference resolution.\nMany machine learning models for structured prediction score and predict graphs (McDonald et al., 2005; McDonald and Pereira, 2006), in which the vertices represent the tokens in the string and the edges represent the relations between them. One common strategy to model a graph is to decompose it into smaller subgraphs that are tractable (Taskar et al., 2004; Smith, 2011, \u00a72.2). For example, arc-factored models (Eisner, 1996) score a graph only using the score of each constituent edge. However, even with such simplification, the computational costs of arc-factored models are superlinear. The reason is that one needs to exhaustively compute scores for all possible edges in the graph, which, in general, requires at least quadratic number of computations with respect to the length of the string. Another common strategy employs weighted transition-based systems (Knuth, 1965; Yamada and Matsumoto, 2003; Nivre, 2003). They decompose structures into transitions between intermediate model states and do offer linear-time algorithms. However, in general, predicting the transitions between states cannot be parallelized, which is another worrying limitation. The authors of this paper contend the limitations of both graphbased and transition-based models are frustrating in an era when researchers are processing longer and longer texts (Tay et al., 2021).\nFrom a more abstract perspective, the mathematical and algorithmic foundation on which structured prediction models rest can be regarded as a design choice. Graph-based and transition-based modeling are both specific design choices. These design\nchoices impose substantial inductive biases by confining the class of models available to be utilized to solve the task and set limits on the efficiency of the models. In this paper, we propose a fresh design choice for structured prediction. Specifically, we propose an order-theoretic perspective to understand and model structures in NLP. Our approach can predict many structures in natural language in O(N) time where N is the length of the string and is easily parallelizable. The linear-time complexity means our method avoids comparing all O ( N2\n) token pairs. The key innovation that enables this speed-up is the following: Rather than considering structures as graphs, we view them as partial orderings of the tokens in the strings.\nConcretely, we treat structured prediction as a regression task. Because the set of real numbers R is naturally ordered by <, we use real numbers as the proxy for determining the partial order. We predict K numbers for each token and sort the tokens K times accordingly. Two tokens are partially ordered by \u227a if and only if they are ordered by < in all of the K orders above. We further provide an efficiency guarantee based on the well-established result in order theory that partial orders satisfying particular conditions can be represented as the intersection of as few as K = 2 total orders. We show that most structures in natural language, including trees, alignments, and set partitions, satisfy these conditions. This result enables us to develop a linear-time algorithm for predicting such structures. Fig. 1 gives an illustrative example of our framework applied to dependency parsing, in which the structure being modeled is a tree.\nOn dependency parsing, our experimental results\nshow that our method achieves 96.1 labeled attachment score (LAS) and 97.1 unlabeled attachment score (UAS) by using an intersection of only 2 total orders, 96.4 LAS and 97.4 UAS using an intersection of 4 total orders on the English Penn Treebank (Marcus et al., 1993). Furthermore, our method sets the new state of the art on Universal Dependencies 2.2 (Nivre et al., 2018), while being 10 times faster and more memory efficient than graph-based models. Our method also achieves 79.2 F1 score with only 4 total orders on the English OntoNotes coreference resolution benchmark (Pradhan et al., 2012), which is on par with the state of the art, while being twice as fast and using less memory."
        },
        {
            "heading": "2 Motivation",
            "text": "We now provide high-level motivation for order-theoretic structured prediction."
        },
        {
            "heading": "2.1 Linearization of Structure",
            "text": "The NLP literature abounds with linear-time structured prediction models. Many are derived from the classical shift\u2013reduce parsers (Knuth, 1965) from the compiler literature. One recent line of research has derived linear-time parsers by reducing parsing to tagging (G\u00f3mez-Rodr\u00edguez and Vilares, 2018; Strzyz et al., 2020; Kitaev and Klein, 2020; Amini et al., 2023, inter alia). In these methods, a finite set of tags C is chosen such that all structures for parsing a string can be embedded in CN for a string of length N . Tagging-based parsers often yield strong empirical performance in both constituency parsing and projective dependency parsing. A natural question is, then, why do we need another method?\nWe give two motivations. The first linguistic\nand the second mathematical. Linguistically, the underlying structures of natural language, e.g., syntax, semantics, and discourse, are often not aligned with the surface form of a sequence due to the existence of displacement (Chomsky, 2015, Chapter 1, p. 44). The strong performance of parsing-as-tagging schemes relies, in part, on there being a tight correspondence between the surface string and structure (Amini and Cotterell, 2022, Proposition 1). Mathematically, the maximum number of structures that a discrete tag sequence can represent is at most O ( |C|N ) . This set is simply not large enough to capture many structures of interest in NLP. For instance, the space of non-projective dependency trees of N tokens has a cardinality of O ( NN\u22122 ) (Cayley, 1889). Therefore, to parse non-projective dependency trees with tagging, the size of the tag set has to grow with N . However, this implies performing a classification task with an infinite number of classes."
        },
        {
            "heading": "2.2 An Illuminating Example",
            "text": "Order-theoretic approaches appear across computer science. For instance, it is well-known that a binary tree can be uniquely restored from its inorder traversal and either the pre- or postorder traversal. Consider the following binary tree. Example 2.1 (Binary Tree).\nIn a binary tree, a vertex x is a left descendant of vertex y if and only if x is visited before y in both of the in- and postorder traversal. E.g., in Ex. 2.1, a is the left descendant of d and is visited before d in both the in- and postorder traversal.\nAnother way of stating the above fact is that a binary tree can be recovered from the combination of two total orders, the one induced by the inorder traversal and the one induced by the postorder traversal. Combining these two total orders yields a partial order, i.e., left descendant, from which the left child of each vertex can be identified. This partial order is shown on the right of Ex. 2.1. See App. B and (Knuth, 1997, \u00a72.3.1, Ex. 7) for further discussion. In light of these observations, we conceive an order-theoretic treatment that constructs\na tree by predicting multiple total orders and intersecting them. In terms of computation, predicting total orders only requires labeling each node with real numbers and then sorting, the complexity of which is linear under radix sort. On the other hand, an arc-factored model necessarily computes all O ( N2 ) pair-wise scores for every pair of vertices to decide the existence of each edge. Next, we generalize the intuitions gained from this example. In \u00a73, we explore the class of graphs that can be efficiently represented with partial orders. In \u00a74, we show how to learn the ordering efficiently with neural networks."
        },
        {
            "heading": "3 Order and Structure",
            "text": "In this section, we describe an order-theoretic treatment for linguistic structure prediction. Specifically, we treat the structure to be predicted as a partially ordered set, i.e., a set equipped with a transitive relation \u227a. We begin by revisiting how linguistic structures are represented as graphs."
        },
        {
            "heading": "3.1 Linguistic Structures as Directed Graphs",
            "text": "Let \u03a3 be an alphabet, i.e., a finite set of natural language tokens, and let w = w1w2 \u00b7 \u00b7 \u00b7wN \u2208 \u03a3\u2217 be a string. Linguistic structure prediction is the task of assigning a structure, e.g., a dependency tree, to a given string w in natural language.\nA wide range of linguistic structures are built upon the relations between pairs of tokens. Many structured prediction models are thus arc-factored, i.e., they predict the arcs between a pair of tokens and then combine them back into structures, which are our focus in this work. Formally, their major goal is to model the homogeneous relation1 on the spanning node set V = {w1, w2, \u00b7 \u00b7 \u00b7 , wN} of a sentence w = w1 \u00b7 \u00b7 \u00b7wN (K\u00fcbler et al., 2009). The output space is defined by the input itself, in contrast to the external label spaces in other tasks such as classification or language generation.\nDefinition 3.1 (Structure). A structure over a string w = w1w2 \u00b7 \u00b7 \u00b7wN is a directed graph G = (V ,E), where V = {w1, w2, \u00b7 \u00b7 \u00b7 , wN}, E \u2286 V \u00d7 V is the set of arcs. A typed structure G = (V ,E,R) is a structure with E \u2286 V \u00d7V \u00d7R, where R is a finite set of relation labels.\n1A homogeneous relation on a set X is a binary relation between two elements in X . It can be equivalently represented with the set of edges in a graph in which X is the set of vertices.\nMost linguistic structures are naturally subsumed under this definition. We give two examples of linguistic structure prediction tasks.\nExample 3.2 (Dependency Parsing; K\u00fcbler et al., 2009, Def. 2.3). A dependency structure is a structure G = (V ,E,R), where E \u2286 V \u00d7 V \u00d7R, and R is the set of dependency relation types. If (x, y, r) \u2208 E, then \u2200r\u2032 \u0338= r, (x, y, r\u2032) /\u2208 E. \u25a0 Example 3.3 (Coreference Resolution). A coreference structure is a structure G = (V ,E,R), where E \u2286 V \u00d7 V \u00d7R, and R = {r, r\u2032}. The relations r, r\u2032 represent the entity mention and coreference, respectively. We have (x, y, r) \u2208 E if and only if the textual span x :y in w is a mention of an entity. (x1, x2, r\u2032)\u2208E\u2227(y1, y2, r\u2032)\u2208E if and only if the textual spans x1 :y1 and x2 :y2 corefer. \u25a0"
        },
        {
            "heading": "3.2 From Directed Graphs to Partial Orders",
            "text": "Our treatment constructs linguistic structures with techniques from order theory. The key is to cast the relation between tokens as an order, which is defined as follows.\nDefinition 3.4 (Order; Hausdorff, 1914). An order over a set V is a relation \u227a such that the following hold for all x, y, z \u2208 V :\n(a) irreflexivity: x\u2280x; (b) asymmetry: x\u227a y =\u21d2 y\u2280x; (c) transitivity: x\u227a y \u2227 y\u227a z =\u21d2 x\u227a z. Natural language exhibits structural sparsity in that each token in a string usually only interacts with very few other tokens with a particular relation. For instance, in a dependency graph, there are no direct paths between most of the word pairs. Such sparsity, from an order-theoretic point of view, can be characterized by incomparability in a partially ordered set (Birkhoff, 1967, Chapter 1, p. 2).\nBy analogy, we define the following partially ordered structure, which is a partially ordered set mathematically. Its elements are the tokens of a string, and its order encodes a linguistic structure.\nDefinition 3.5 (Partially Ordered Structure). Let G = (V ,E) be a structure. Define the following relation \u227a: For x, y \u2208 V , x\u227a y \u21d0\u21d2 (x, y) \u2208 E. We call P = (V ,E,\u227a) a partially ordered structure if \u227a satisfies Def. 3.4.\nThe essential theoretical foundation of our linguistic structure prediction framework is the classic result that partial orders can be represented by an intersection of total orders (Dushnik and Miller, 1941). It is this result that enables us to use\nreal numbers as a proxy to determine the partial ordering of tokens. Definition 3.6 (Totally Ordered Structure). A partially ordered structure P = (V ,E,\u227a) is totally ordered if \u2200x, y \u2208 V : x\u227a y \u2228 y\u227ax.\nDue to the transitivity of the ordering relation \u227a, a totally ordered structure of |V | elements always contains |E| = (|V | 2 ) relations. Given a collection of structures {(V ,Ek)}k\u2208[K] defined over the same set of vertices V , their intersection is also a structure\u2014namely (V ,\u2229k\u2208[K]Ek), where K \u2208 N, [K] def= {1, \u00b7 \u00b7 \u00b7 ,K}. The intersection of partially ordered structures remains partially ordered.\nWe now cite a famous theorem from order theory.\nTheorem 3.7 (Szpilrajn (1930)). Every partially ordered structure is contained in a totally ordered structure, i.e., for every partially ordered structure P = (V ,E,\u227a), there exists a totally ordered structure T = (V , E\u0302,\u227a) such that E \u2286 E\u0302.\nThm. 3.7 ensures that every partially ordered structure can be embedded in some totally ordered structure in the sense that the totally ordered structure contains all the relations in the partially ordered structure. More importantly, a stronger result can be shown: Partially ordered structures can always be represented as intersections of a collection of totally ordered structures. Definition 3.8 (Realizer). Let P = (V ,E,\u227a) be a partially ordered structure. A realizer RP of P is a set of totally ordered structures{ T 1, T 2, \u00b7 \u00b7 \u00b7 , T K } over V , i.e., each T k =\n(V ,Ek,\u227ak), such that E = \u22c2\nk\u2208[K]Ek. In other words, \u2200x, y \u2208 V , x\u227a y \u21d0\u21d2 \u2227 k\u2208[K] x\u227ak y. Theorem 3.9 (Dushnik and Miller, 1941, Thm. 2.32). There exists a realizer RP for every partially ordered structure P = (V ,E,\u227a).\nA corollary of the above theorem is that the complexity of a partially ordered structure can be characterized by its order dimension, which is defined as follows. Definition 3.10 (Order Dimension; Dushnik and Miller, 1941). Let P = (V ,E,\u227a) be a partially ordered structure. The order dimension DP of P is the cardinality of the smallest realizer of P ."
        },
        {
            "heading": "3.3 Efficiency Guarantees",
            "text": "In this section, we give an efficiency guarantee of order-theoretic structured prediction. These efficiency guarantees come from a series of results in\norder theory and lattice theory (Dushnik and Miller, 1941; Hiraguchi, 1955; Birkhoff, 1967, inter alia).\nFirst, it is important to note that not all partially ordered structures can be represented as an intersection of a constant number of totally ordered structures (Dushnik and Miller, 1941, Thm. 4.1).\nIn fact, testing whether the order dimension of a partial order P is at most K, \u2200K \u2265 3 is NPcomplete (Yannakakis, 1982). However, we contend that most of the linguistic structures found in natural language processing (Smith, 2011)\u2014 including trees, equivalence classes (i.e., set partitioning), and alignment (i.e., bipartite matching)\u2014 can be represented as the intersection of 2 totally ordered structures. We postulate that this is possible due to their innate sparsity, i.e., a token tends to only interact with a few other tokens. These assumptions are formalized as follows.\nAssumption 3.11 (Sparsity). A class of linguistic structures G = (V ,E) over natural language strings w \u2208 \u03a3\u2217 with N = |w| is called sparse if O(|E|) = O(N). Assumption 3.12 (Linguistic Structures are 2-dimensional). Structures in natural language can be represented as intersections of 2 totally ordered structures.\nWe justify Assumptions 3.11\u20133.12 in App. D. Empirical evidence is also provided in \u00a75, where 2-dimensional order-theoretic models are trained to tackle two linguistic structure prediction tasks with high performance."
        },
        {
            "heading": "3.4 Token-Split Structures",
            "text": "An obvious limitation of our formulation of linguistic structures as partial orders is that by Def. 3.4, partial order is transitive. In other words, x\u227a y \u2227 y\u227a z implies x\u227a z, which, however, does not hold in the structures characterized by the directed graph formalization in Def. 3.1. In addition, we note that our notation of structures generalizes to cyclic graphs. However, partially ordered structures are inherently acyclic due to the transitivity of \u227a. We now introduce the token-split structure, which enables cycles and removes redundant edges introduced by transitivity in partially ordered structures.\nDefinition 3.13 (Token-Split Structure). A tokensplit structure induced by a structure G = (V ,E) is a structure P = (V\u0302 , E\u0302,\u227a) such that (a) V\u0302 def= V r \u222a V b, where V r = {xr | x \u2208\nV }, V b = {xb | x \u2208 V };\n(b) V r \u2229 V b = \u2205; (c) E\u0302 = { (xr, yb) | (x, y) \u2208 E } .\nIn other words, a token-split structure maps the edges from the original structure, including self-loops, into a bipartite graph in which the edges are oriented from V r to V b. An example is displayed in Fig. 1b.\nGiven a token-split structure P = (V\u0302 , E\u0302,\u227a), we can recover the original structure G = (V ,E) from which P is induced using the following equation\nE={(x, y) | xr \u2208 V r\u2227 yb \u2208 V b\u2227 xr \u227a yb} (1)\nTheorem 3.14. Token-split structures are partially ordered.\nProof. See App. C.1. \u25a0\nRemark 3.15 (Conversion between Structures and Partially Ordered Structures). Thm. 3.14 and Eq. (1) ensure that we can convert back and forth between any structure under Def. 3.1 and a partially ordered structure. Specifically, they enable us to first convert a structure to a partially ordered structure, predict it order-theoretically, and then finally convert it back to a structure."
        },
        {
            "heading": "4 A Neural Parameterization",
            "text": "In this section, we describe the core technical contribution of our work. We show how to model partially ordered structures with a neural model. Specifically, we define a parameterized realizer of Def. 3.8 and an objective function for training the realizer to model the token-split structures. We also give algorithms for efficient training and decoding."
        },
        {
            "heading": "4.1 Neuralized Total Order",
            "text": "We now discuss a parameterized neural network that induces partial orders as the intersection of several total orders.\nDefinition 4.1 (Functional Realizer). A functional realizer of a partially ordered structure P = (V ,E,\u227a) is a set of mappings F\u03b8 = {f (1)\u03b8 , \u00b7 \u00b7 \u00b7 , f (K) \u03b8 }, where \u03b8 is the set of learnable parameters shared among f (k)\u03b8 , and the order dimension K \u2208 N is a hyperparameter of the realizer. The realize element f (k)\u03b8 : V \u2192 R, \u2200k \u2208 [K] maps each vertex in the input structure to a real number. We overload F\u03b8 as a mapping F\u03b8 : V \u2192 RK , defined as F\u03b8(x) def = [ f (1) \u03b8 (x), \u00b7 \u00b7 \u00b7 , f (K) \u03b8 (x) ]\u22a4 .\nThe set of real numbers R is totally ordered, in which the order is given by the < (less than) relation. Each individual f (k) \u03b8 \u2208 F\u03b8 induces a total order T k =( V , {(x, y) | x, y \u2208 V , f (k)\u03b8 (x) < f (k) \u03b8 (y)},\u227ak ) .2\nThe functional realizer assigns K total orders {T 1, T 2, \u00b7 \u00b7 \u00b7 , T K} to the input string. During decoding, an edge from x to y exists in P if and only if x\u227ak y holds in T k, \u2200k \u2208 [K].\nImplementing Def. 4.1 with neural networks is straightforward. To obtain f (k)\u03b8 (x r) and f (k)\u03b8 (x b), where xr, xb are two vertices introduced by the token-split formulation (Def. 3.13) corresponding to the same token wx in the input, we apply two linear projections on the contextualized representation of x given by a pretrained model parameterized by \u03b8.3 In total, 2K real numbers are predicted for each input token."
        },
        {
            "heading": "4.2 Learning a Functional Realizer",
            "text": "To learn the functional realizers with a gradientbased procedure, we need a differentiable objective. In a partially ordered structure P with functional realizer F\u03b8 = {f (1) \u03b8 , f (2) \u03b8 , \u00b7 \u00b7 \u00b7 , f (K) \u03b8 }, we have\nx\u227a y if and only if \u2227\nk\u2208[K] ( f (k) \u03b8 (x) < f (k) \u03b8 (y) ) .\nWe re-express this condition as follows:\nF\u03b8(x, y) def = max\nk\u2208[K]\n( f (k) \u03b8 (x)\u2212 f (k) \u03b8 (y) ) < 0 (2)\nWe call F\u03b8 a pair-wise function. On the other hand, we have x\u2280 y if and only if\u2228\nk\u2208[K] ( f (k) \u03b8 (x) \u2265 f (k) \u03b8 (y) ) . This condition can be re-expressed as F\u03b8(x, y) \u2265 0. Thus, empirically, the smaller F\u03b8(x, y) is, the more likely the relation x\u227a y exists.\nWe now define a training objective, which encourages the model to make decisions that comply with the order constraints enforced by the structures, described by Eq. (2). Given the token-split version P = (V ,E,\u227a) induced by the structure being modeled, we consider the following objective\nL(\u03b8) = log \u2211\n(x,y)\u2208V 2\\E\nexp\u2212F\u03b8(x, y)+\nlog \u2211\n(x,y)\u2208E\nexpF\u03b8(x, y) (3)\n2In this work, we assume f (k)\u03b8 is injective, i.e., \u2200x, y \u2208 V , f\n(k) \u03b8 (x) \u0338= f (k) \u03b8 (y). See \u00a78.4 for further discussion on the practicality of this assumption. 3If wx consists of more than one subword due to tokenization, we apply the projection to the representation of the last subword.\nThe first term maximizes F\u03b8(x, y) for x\u2280 y, while the second minimizes F\u03b8(x, y) for x\u227a y. Note that in the second term, we assume O(|E|) = O(N) in a linguistic structure following Assumption 3.11."
        },
        {
            "heading": "4.3 An Efficient Algorithm",
            "text": "We remark that both training and decoding of the proposed model can be regarded as performing an aggregation for every token x \u2208 V . Definition 4.2 (Aggregation). An \u2295-aggregation given a token x for a pair-wise function F\u03b8 over the set V is an operation \u2295 y\u2208V F\u03b8(x, y), where \u2295 is a commutative and associative operation over which real number addition + is distributive.\nAggregation is a common abstraction for computing the relation between a token x and every other token. The aggregation operator is associative and commutative, thus can be computed in parallel. The number of required computations is O(|V |) for na\u00efvely computing an aggregation of x.\nDuring training, we \u2295-aggregate using negative log-sum-exp, i.e., we compute \u2212 log \u2211 y exp(\u2212F\u03b8(x, y)) for all x, to compute the first term of Eq. (3). In greedy decoding, we \u2295-aggregate by computing miny F\u03b8(x, y) to find the optimal relation arc from each x. Na\u00efvely, \u2295- aggregating for every token x \u2208 V takesO ( N2 ) in total, as each aggregand has a complexity ofO(N). However, the partial order we assigned over V allows us to efficiently compute the aggregands.\nFor K = 2, we can inspect Eq. (2) to see that F\u03b8(x, y) is equal to either f (1) \u03b8 (x) \u2212 f (1) \u03b8 (y) or f (2) \u03b8 (x) \u2212 f (2) \u03b8 (y). We now define the following two subsets of V for k \u2208 {1, 2}:\nSk(x)= { y | F\u03b8(x, y) = f (k) \u03b8 (x)\u2212 f (k) \u03b8 (y) } Using this notation, we can write the following\u2295\n(x,y)\u2208V 2 F\u03b8(x, y) = \u2295 x\u2208V \u2295 y\u2208V F\u03b8(x, y) (5a)\n= \u2295 x\u2208V \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) \ufe38 \ufe37\ufe37 \ufe38\ndef =G1\n(5b)\n\u2295 \u2295 x\u2208V \u2295 y\u2208S2(x) ( f (2) \u03b8 (x)\u2212 f (2) \u03b8 (y) ) \ufe38 \ufe37\ufe37 \ufe38\ndef =G2\nWe now give an efficient algorithm to compute G1 and, by symmetry, G2. Our first observation is that,\nby distributivity, we can write G1 = \u2295 x\u2208V \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) (6a)\n= \u2295 x\u2208V ( f (1) \u03b8 (x) + \u2295 y\u2208S1(x) \u2212f (1)\u03b8 (y) )\n\ufe38 \ufe37\ufe37 \ufe38 def =G1(x)\n(6b)\nAlone, this application of dynamic programming does not reduce the complexity from O ( N2 ) to O(N) as desired because the inner\naggregand, \u2295\ny\u2208S1(x)\u2212f (1) \u03b8 (y), itself still takes\nO(N) time. However, we are able to compute\u2295 y\u2208S1(x)\u2212f (1) \u03b8 (y) in amortized O(1) time due to Fredman\u2019s (1976, Eq. 1) algebraic trick. The strategy is to sort4 the vertices of the partially ordered structure according to f (1)\u03b8 (y) \u2212 f (2) \u03b8 (y). Thus, if we have f (1) \u03b8 (y) \u2212 f (2) \u03b8 (y) < f (1) \u03b8 (x) \u2212 f (2) \u03b8 (x), simple algebra reveals that f (2)\u03b8 (x) \u2212 f (2) \u03b8 (y) <\nf (1) \u03b8 (x)\u2212 f (1) \u03b8 (y). Thus, for a given x, every vertex y that comes before x in the sorted order satisfies F\u03b8(x, y) = f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y). Aggregating in this order enables intermediate results to be reused.\nAlgorithm 1 Computing G1 when K = 2.\n1: procedure COMPUTE-G1(f (1) \u03b8 , f (2) \u03b8 , V ) 2: U \u2190 sort ( V , key=f\n(1) \u03b8 \u2212f (2) \u03b8 ) 3: G1, s1 \u2190 0,0 \u25b7 0 is the zero element of \u2295 4: for n = 1 up to N : 5: q1= f (1) \u03b8 (Un) + s1 \u25b7 q1 = G1(Un) 6: G1\u2295= q1 7: s1\u2295= \u2212f (1)\u03b8 (Un) 8: return G1\nLikewise, if we sorted in reverse, i.e., according to f (2)\u03b8 (y)\u2212 f (1) \u03b8 (y), the same manipulation yields that for a given x, every vertex y that comes before x in the reverse sorted order satisfies F\u03b8(x, y) = f (2) \u03b8 (x)\u2212 f (2) \u03b8 (y).\nThe algorithm for computing G1 is given in Algorithm 1, which has O(N) computations in total. Moreover, if parallelized, it can be run in O(logN) time. For K > 2, we speculate that the aggregation algorithm can be done in O ( KN logK\u22122N ) . We leave this to future work. See App. E.2 for further discussion. 4As before, we take the complexity of sorting to be O(N) where we can apply radix sort as implemented by Pytorch."
        },
        {
            "heading": "5 Experiments",
            "text": "We report the experimental results on two representative linguistic structure prediction problems in NLP, namely dependency parsing and coreference resolution. The graph-theoretic definitions of these tasks are given in Examples 3.2 and 3.3. We first convert the linguistic structures to partially ordered (token-split) structures as described in \u00a73.4, and then apply the neural method described in \u00a74 to model the partially ordered structures."
        },
        {
            "heading": "5.1 Dependency Parsing",
            "text": "Modeling. Orders \u227a are not typed in Def. 3.5. In other words, under Def. 3.5, all relations in a partially ordered structure are of the same type. To model dependency type labels, we apply a token-level classifier on the contextualized representation. During decoding, similar to arc-factored models for dependency parsing, we keep the head word that minimizes F\u03b8(x, y) for a given x, i.e., argminy\u2208V F\u03b8(x, y).\nFor pretrained language models, we use XLNet-large-cased5 (Yang et al., 2019) for PTB, bert-base-chinese6 for CTB, and bert-base-multilingual-cased7 for UD.\nDatasets. We conduct experiments on the English Penn Treebank (PTB; Marcus et al., 1993), the Chinese Penn Treebank (CTB; Xue et al., 2005), and the Universal Dependencies 2.2 (UD; Nivre et al., 2018). Hyperparameter settings and dataset statistics are given in Apps. F.1 and G.1.\nAccuracy. We report the experimental results in Tab. 1. The full results on UD are included in App. I.1. On PTB and UD, our method achieves state-of-the-art performance compared with O ( N3 ) (Yang and Tu, 2022), O ( N2 ) (Mrini et al., 2020), and O(N) (Amini et al., 2023) methods. Although Amini et al.\u2019s (2023) method has the same complexity as ours, it is worth noting that our method is more general since it can handle non-projective dependencies without using pseudoprojectivization (Nivre and Nilsson, 2005).\nEfficiency. We evaluate the efficiency of our method with two representative baseline models. As depicted in Tab. 2, we observe that our method with K = 2 is almost 10 times as fast as Biaff\n5https://huggingface.co/xlnet-large-cased 6https://huggingface.co/bert-base-chinese 7https://huggingface.co/\nbert-base-multilingual-cased"
        },
        {
            "heading": "32 3232 2916 493 1.7 2.9 4.5",
            "text": "(Dozat and Manning, 2017), and consumes less memory than Hexa (Amini et al., 2023), which is O(N) in space complexity. We further include some qualitative examples using K = 2 in App. J for a more intuitive picture of our method."
        },
        {
            "heading": "5.2 Coreference Resolution",
            "text": "Modeling. Our method operates in a two-stage manner to accommodate the two relations in Ex. 3.3. First, it extracts a list of entity mentions using the partial order induced by r (mention relation). In other words, x\u227a y \u21d0\u21d2 span x :y is an entity mention. Then, it models the partial order induced by r\u2032 (coreference relation) over the extracted mentions. In other words, m1\u227am2 \u21d0\u21d2 mention m1 corefers to m2. To find the optimal coreferent antecedent for each mention m, we keep m\u2032 that minimizes F\u03b8(m,m\u2032).\nThe overall complexity of the coreference resolution model is O(N), since the complexity of the encoder used (Beltagy et al., 2020) and the number of valid mentions are both O(N),\nassuming entity mentions are constituents (Liu et al., 2022). We experiment on the CoNLL-2012 English shared task dataset (OntoNotes; Pradhan et al., 2012). Hyperparameter settings and dataset statistics are given in Apps. F.2 and G.2.\nAccuracy. The experimental results are displayed in Tab. 3. Similar to the results for dependency parsing, an intersection of 2 total orders can already achieve reasonable performance on coreference resolution. This provides empirical evidence for our assertion in \u00a73.3 that most structures in NLP can be represented as the intersection of at most 2 total orders. When K = 4, the performance of our method is comparable to Kirstain et al. (2021), which uses the same pretrained encoder as ours and requires an O ( N2\n) biaffine product computation for token-pair scores.\nEfficiency. We compare the efficiency of our method with Kirstain et al.\u2019s (2021) method. It is worth noting that Kirstain et al. (2021) has already performed aggressive optimization in both the speed and memory footprint of coreference modeling. Our method is still 2 times as fast, achieving a speed of 82.8 documents per second vs. 41.9, while using less memory, especially on long documents. The full efficiency statistics are given in App. H.\n6 Related Work8"
        },
        {
            "heading": "6.1 Structured Prediction",
            "text": "Structured prediction constitutes an important part of natural language processing. It involves the modeling of interrelated variables or outputs with structural constraints. Some representative structured prediction problems are sequence tagging (Church, 1988), dependency parsing (K\u00fcbler et al., 2009), and coreference resolution (Stede, 2012).\n8More related work is included in App. A.\nStructured prediction can often be formulated as learning and inference of probabilistic graphical models (Smith, 2011, \u00a72.2). The key idea is to represent the probability distribution over the output space using a graph, in which each vertex corresponds to a random variable, and each edge corresponds to a dependence relation between two random variables."
        },
        {
            "heading": "6.2 Graph-Based Parsing",
            "text": "Graph-based parsers, or arc-factored parsers, construct graphs by scoring all possible arcs (Eisner, 1996; McDonald and Pereira, 2006) between each pair of words. At inference time, they use either maximum spanning tree (MST) finding algorithms (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977), or the projective MST algorithm (Eisner, 1996) to build the valid dependency trees with maximum score. Kiperwasser and Goldberg (2016) present a neural graph-based parser that uses the same kind of attention mechanism as Bahdanau et al. (2015) for computing arc scores. Greedy decoding that independently assigns a head word to each word (Dozat and Manning, 2017) is also widely used as an approximation to exact inference algorithms."
        },
        {
            "heading": "6.3 Tagging-Based Parsing",
            "text": "Inspired by transition-based parsers (Knuth, 1965) and Bangalore and Joshi\u2019s (1999) seminal work on supertagging, one line of work uses pretrained models to parse dependency trees by inferring tags for each word in the input sequence. Li et al. (2018) and Kiperwasser and Ballesteros (2018) predict the relative position of the dependent with respect to its head in a sequence-to-sequence manner. Strzyz et al. (2019) give a framework for analyzing similar tagging schemes. G\u00f3mez-Rodr\u00edguez et al. (2020) infer a chunk of actions in a transition-based system for each word in the sequence.\nFor non-projective dependency parsing, G\u00f3mezRodr\u00edguez and Nivre (2010, 2013) show that efficient parsers exist for 2-planar trees (Yli-Jyr\u00e4, 2003), a sub-class of non-projective trees whose arcs can be partitioned into 2 sets while arcs in the same set do not cross each other. Strzyz et al. (2020) propose an encoding scheme for 2-planar trees, enabling a tagging-based parser for such trees. As mentioned in \u00a72.1, to handle the entire set of non-projective trees, the size of the tag set has to be unrestricted, which limits the efficiency and applicability of this series of approaches of approaches."
        },
        {
            "heading": "6.4 Parsing with Syntactic Distance",
            "text": "Shen et al. (2018a,b) introduce a constituent parsing scheme which is also based on the comparison of real numbers. In this scheme, a neural model is trained to assign one real number, termed the syntactic distance, to the gap between every pair of neighboring tokens. To parse a span into two sub-constituents, the gap with the largest syntactic distance within that span is chosen as the split point. Parsing can be done by recursively performing the above splitting procedure starting from a given string. The algorithm has a runtime complexity of O(N logN), which is significantly more efficient than chart-based parsers with O ( N2 ) complexity. However, this method does not generalize easily to perform non-context-free parsing, since it cannot handle the possible discontinuity of constituents. Moreover, the recursive splitting procedure restricts the output space of parse trees to be a subset of phrase-structure trees (Dyer et al., 2019)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose an order-theoretic treatment of linguistic structured prediction. Theoretical and empirical results show that most linguistic structure prediction problems can be solved in linear time and memory by framing them as partial orderings of the tokens in the input string. We demonstrate the effectiveness of our method on dependency parsing and coreference resolution, setting the new state-of-the-art accuracy in some cases and achieving significant efficiency improvements."
        },
        {
            "heading": "8 Limitations",
            "text": ""
        },
        {
            "heading": "8.1 Decoding Algorithms",
            "text": "This work does not provide algorithms for particular structures or algorithms that ensure the wellformedness of structures, such as maximum spanning trees and projective trees. It remains to be investigated whether existing constrained decoding algorithms for arc-factored models (Chu and Liu, 1965; Edmonds, 1967; Eisner, 1997, inter alia) have their counterparts in the order-theoretic method. We would like to explore decoding algorithms for structured prediction under ordertheoretic formulation in future work."
        },
        {
            "heading": "8.2 Interpretability",
            "text": "In our method, the interactions between tokens are not directly modeled as in graph-based structured\nprediction models, which makes it more difficult to interpret the output of our model. In addition, we leave to future work the investigation of the total ordering metrics (see App. J) learned by the realizers in our method."
        },
        {
            "heading": "8.3 Hardness of Learning",
            "text": "Intuitively, it is harder to learn partial orders over strings than directly modeling the arcs in a graph, since our order-theoretic treatment has much fewer parameters when scoring token pairs. We also observed in our experiments that order-theoretic models take more training iterations to converge than arc-factored models.\nFor instance, consider the modeling of a tree structure with N nodes with N \u2212 1 arcs using partial order, which implies N \u2212 1 constraints of the form x \u227a y and N2 \u2212 2N + 1 constraints of x\u2280 y. From a theoretical perspective, K = 2 is sufficient to represent such a structure as shown in \u00a73. In other words, there always exist 2 total orders whose intersection satisfies the aforementioned N(N \u2212 1) constraints. However, it might not be easy to find such orders in practice.\nA realizer with K beyond 2 can more easily satisfy the constraints, especially of the form x\u2280 y\u2014 since there are more constraints of this form. It allows more possibilities for \u2228 k\u2208[K] f (k) \u03b8 (x) \u2265 f (k) \u03b8 (y) (i.e., more choices of k to satisfy the expression). On the other hand, using a small K might make it harder to satisfy the constraints.\nWe plan to further investigate the hardness of learning a string partial order in future work."
        },
        {
            "heading": "8.4 Precision of floating-point numbers and numerical stability",
            "text": "Our method might be affected by the finite precision of floating-point numbers and numerical instability when applied to very long strings. Although we did not encounter such issues in our experiments (N \u2264 4096 = 212), issues might arise when N > 65536 = 216 if bfloat16 or half precision is used. In such extreme cases, our assumption that \u2200k \u2208 [K], f (k)\u03b8 is injective cannot be fulfilled. Thus, not all totally ordered structures of N elements can be represented, and our method might not exhibit the desired behavior.\nEthics Statement\nWe do not believe the work presented here further amplifies biases already present in the datasets and\npretrained models. Therefore, we foresee no ethical concerns in this work."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Zhaofeng Wu, Cl\u00e9ment Guerner, and Tim Vieira for their invaluable feedback. We are grateful to the anonymous reviewers for their insightful comments and suggestions. Afra Amini is supported by ETH AI Center doctoral fellowship. MS acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1)."
        },
        {
            "heading": "A Related Work",
            "text": "A.1 Ordinal Regression Ordinal regression is a family of problems that involve ranking a set of objects. Unlike classification, the label spaces in ordinal regression exhibit some natural ordering in its elements (McCullagh, 1980). For instance, in information retrieval, a ranking model sorts a set of documents typically according to the document\u2019s relevance to the query. Practically, ordinal regression can either be tackled as either regression or classification by treating the ranks as real-values or the assignment to a particular rank value as a classification (Shawe-Taylor and Cristianini, 2004).\nA.2 Order Embeddings of Lexicons The notion of partial order has also been explored for learning word embeddings. The lexicons of natural languages exhibit hierarchical structures according to the concepts that the words represent (Miller, 1994). For instance, \u2018cat\u2019 and \u2018dog\u2019 are \u2018animal\u2019, \u2018animal\u2019 and \u2018plant\u2019 are \u2018living thing\u2019. Order embeddings (Vendrov et al., 2015; Athiwaratkun and Wilson, 2018) propose to learn such property by learning embeddings that encode such partial order on the lexicon, resulting in improved performance on downstream tasks such as image caption retrieval.\nB An Order-Theoretic Re-evaluation of \u00a72.2\nb\nc\nd\na\nf\ne g\nInorder : abcdefg\nPostorder: acbegfd\n(a) The example binary tree in \u00a72.2 and its traversal sequences.\nTheorem B.1 (A binary tree and its traversal; Knuth, 1997, \u00a72.3.1, Ex. 7). Given the inorder and either the pre- or postorder traversal of the vertices in a binary tree, the binary tree structure can be reconstructed.\nProof Sketch (order-theoretic). Without loss of generality, we explain the case of the combination of inand postorder. V denotes the set of vertices in the binary tree. First, the intersection of in- and postorder defines a partial order relation P1 = (V ,E1,\u227a1). For any 2 vertices x, y in the binary tree, x\u227a1 y if and only if x is a left descendant of y. I.e., x is either the left child or a descendant of the left child of y (see Fig. 3b). Since x is visited before visiting y in both inorder traversal and postorder traversal, if and only if x is the left descendant of y. The left child of each vertex in V can be decoded from P1 by finding\nthe child with the deepest subtree. Second, the intersection of reversed inorder and postorder defines a partial order relation P2 = (V ,E2,\u227a2). For any 2 vertices x, y in the binary tree, x\u227a2 y if and only if x is a right descendant of y (see Fig. 3c). Since x is visited before visiting y in both the reversed inorder traversal and postorder traversal, if and only if x is the right descendant of y. The right child of each vertex in V can be decoded from P2 also by finding the child with the deepest subtree. Thus, the original binary tree can be reconstructed.\n\u25a0"
        },
        {
            "heading": "C Proofs on the Partially Ordered Properties of Structures",
            "text": "C.1 Proof of Thm. 3.14 Theorem 3.14. Token-split structures are partially ordered.\nProof. We show that a token-split structure P = ( V\u0302 , E\u0302,\u227a ) satisfies all the properties of partially ordered\nstructure defined in Def. 3.4.\n1. irreflexivity: By Def. 3.13 (c), for all x \u2208 V\u0302 , x\u2280x. 2. asymmetry: Suppose that \u2203x, y, x \u0338= y, s.t. x\u227a y \u2227 y\u227ax. By Definitions 3.13 (b) and 3.13 (c),\nx, y \u2208 V r \u2229 V b = \u2205. Thus, x\u227a y =\u21d2 y\u2280x. 3. transitivity: x\u227a y \u2227 y\u227a z cannot hold by Def. 3.13 (c). Since x\u227a y implies x \u2208 V r \u2227 y \u2208 V b, while\ny\u227a z implies y \u2208 V r \u2227 x \u2208 V b, a contradiction occurs due to y \u2208 V r \u2229 V b = \u2205 by Def. 3.13 (b). x\u227a y \u2227 y\u227a z =\u21d2 x\u227a z holds since the antecedent of the proposition is always false.\nThus, token-split structures are partially ordered. \u25a0"
        },
        {
            "heading": "D Guarantees of Order Dimension of Linguistic Structures",
            "text": "We justify the guarantees of order dimension of linguistic structures. One conventional way to characterize the dimension of partial orders is from a lattice-theoretic point of view. A basic result tells us that a partial order is 2-dimensional if and only if its complete lattice embedding has a planar Hasse diagram (Baker et al., 1972). In other words, its complete lattice embedding can be drawn on a plane without any crossing edges.\nTheorem D.1 (Baker et al., 1972, Thm. 4.1). Suppose P = (V ,E,\u227a) is a partially ordered structure. Then the following are equivalent:\n(a) D(P) \u2264 2. (b) The complete lattice embedding of P has a planar Hasse diagram.\nRemark D.2. MacNeille (1937) and Birkhoff (1967, Chapter 5) introduced the construction of complete lattice embeddings for any partial order. Although it is difficult in practice to compute the complete lattice embedding for a partially ordered structure (MacNeille, 1937), Thm. D.1 can still provide an empirical characterization of the class of structures that can be efficiently represented. According to Euler\u2019s formula, the average degree of a vertex in a planar graph cannot exceed 6 (West, 2018, \u00a76.1.23), which intuitively forces the partially ordered structures that can be represented as an intersection of 2 totally ordered structures to be sparse enough\u2014thus to have planar complete lattice embeddings.\nFortunately, this is often the case in natural language. Such phenomenon is closely related to what is termed valency by Tesni\u00e8re (1959, Part 1, Book D). The number of actants (i.e., arguments) needed to implement the function of a word is a property of the word itself\u2014a constant that does not change with the context (cf. categories9 in categorial grammars (Adjukiewicz, 1935; Bar-Hillel, 1953; Steedman, 1987)). In natural language, the valency of a word is often a small constant. For instance, Steedman (2000, Chapter 3, fn. 10 and Chapter 8, p. 212) observes that the highest valency in the Dutch and English lexicon can be regarded as bounded by 4.\n9E.g., the English word \u201cgive\u201d may have the category (VP/NP)/NP, meaning that it needs two NP categories to the right to form a VP. An example is the verb phrase \u201cgive me an apple\u201d, in which \u201cme\u201d and \u201can apple\u201d are noun phrases.\nWe refer interested readers to MacNeille (1937) and Birkhoff (1967, Chapter 5) for the construction of complete lattice embeddings. Here, we give a weaker but more practical efficiency guarantee, based on a method to construct large partially ordered structures from smaller partially ordered structures.\nDefinition D.3 (Series-Parallel Partial Orders; Valdes et al., 1979). A partially ordered structure is series-parallel if it satisfies the following inductive definition: (a) A single-vertex structure with no edges is series-parallel; (b) If partially ordered structures P1 = (V 1, E1,\u227a) and P2 = (V 2, E2,\u227a) are series-parallel, so is\nthe partially ordered structures constructed by either of the following operations: i. Parallel composition: Pp = (V 1 \u222a V 2, E1 \u222a E2,\u227a).\nii. Series composition: Ps = (V 1 \u222a V 2, E1 \u222a E2 \u222a (M1\u00d7N 2),\u227a), whereM1 is the set of sinks of P1 and N 2 the set of sources of P2.10\nTheorem D.4 (Series-parallel partially ordered structures are 2-dimensional; Valdes et al., 1979). The dimension of series-parallel partially ordered structures is at most 2.\nThm. D.4 provides the guarantee that many structures in natural language processing can be represented as the intersection of 2 totally ordered structures. Since most structures of interest in NLP, such as trees and forests (thereby alignments and set partitioning), can be subsumed under series-parallel partially ordered structures, therefore have an order dimension of at most 2.\nProposition D.5 (Trees are 2-dimensional; Lawler, 1978). Directed tree partially ordered structures are series-parallel. The order dimension of tree structures is at most 2.\nProposition D.6 (Forests are 2-dimensional). Forests are series-parallel. The order dimension of forest structures is at most 2.\nProof. Forests are parallel compositions of trees. Thus, the proposition holds. \u25a0"
        },
        {
            "heading": "E Efficient Algorithm for \u2295-Aggregation",
            "text": "E.1 Correctness of Algorithm 1\nAlgorithm 1 Computing G1 when K = 2.\n1: procedure COMPUTE-G1(f (1) \u03b8 , f (2) \u03b8 , V ) 2: U \u2190 sort ( V , key=f\n(1) \u03b8 \u2212f (2) \u03b8 ) 3: G1, s1 \u2190 0,0 \u25b7 0 is the zero element of \u2295 4: for n = 1 up to N : 5: q1= f (1) \u03b8 (Un) + s1 \u25b7 q1 = G1(Un) 6: G1\u2295= q1 7: s1\u2295= \u2212f (1)\u03b8 (Un) 8: return G1\nProposition E.1. In Algorithm 1, G1 = \u2295 x\u2208V \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) . Proof. By induction, we show that upon finishing step n, s1 = \u2295\ny\u2208S1(Un+1)\u2212f (1) \u03b8 (y), G1 =\u2295 x\u2208{U1,\u00b7\u00b7\u00b7 ,Un} \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) . First, S1(Un) = {U1, \u00b7 \u00b7 \u00b7 , Un\u22121} holds as discussed\nin \u00a74.3. When n = 1, we have s1 = \u2212f (1)\u03b8 (U1), G1 = 0 = \u2295 x\u2208{U1} \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) ,\n10Sources and sinks refer to the vertices without incoming arcs and without outgoing arcs, respectively.\nsince S1(U1) = \u2205. Assume that our statements hold for n = j, when n = j + 1, it is straightforward that s1 = \u2295 y\u2208S1(Uj+2)\u2212f (1) \u03b8 (y). For G1, we have\nG1 = \u2295\nx\u2208{U1,\u00b7\u00b7\u00b7 ,Uj} \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) \u2295 f (1)\u03b8 (U j+1) + \u2295 y\u2208S1(Uj+1) \u2212f (1)\u03b8 (y)  (7a) =\n\u2295 x\u2208{U1,\u00b7\u00b7\u00b7 ,Uj} \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) \u2295 \u2295 y\u2208S1(Uj+1) ( f (1) \u03b8 (U j+1)\u2212 f (1) \u03b8 (y) ) (7b)\n= \u2295\nx\u2208{U1,\u00b7\u00b7\u00b7 ,Uj+1} \u2295 y\u2208S1(x) ( f (1) \u03b8 (x)\u2212 f (1) \u03b8 (y) ) (7c)\nThus, the claims hold for n = j + 1, establishing the induction step. \u25a0\nProposition E.2. Algorithm 1 runs in O(N) time and space. With parallel computing, Algorithm 1 runs in O(logN) span.\nProof. The sorting step in line 2 can be executed in O(N) time and space. The for loop in lines 4 to 7 runs in O(N) time and space. In total, Algorithm 1 runs in O(N) time and space. Computing s1 in each step is a prefix-sum of \u2212f (1)\u03b8 (Un), which can be done in O(logN) span with parallel computing. q1, G1 in each step can be computed in O(1) in parallel following the computation of all s1. Thus, the total span of Algorithm 1 is O(logN). \u25a0\nE.2 Order Dimension K > 2 Finding all y \u2208 V such that x\u227a y in a partial order for a given x \u2208 V requires efficiently finding all y that satisfy \u2227 k\u2208[K](f (k) \u03b8 (x) < f (k) \u03b8 (y)). We remark that this problem bears a resemblance to orthogonal range searching in a K-dimensional space (Berg et al., 2008, Chapter 5), i.e., for a given x, we aim to find all y such that (f (1)\u03b8 (y), f (2) \u03b8 (y), \u00b7 \u00b7 \u00b7 , f (K) \u03b8 (y)) is within the range (f (1) \u03b8 (x),\u221e)\u00d7 (f (2) \u03b8 (x),\u221e)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7\n(fK\u03b8 (x),\u221e). This problem can be na\u00efvely solved inO ( logK\u22121N + \u2113 ) using a range tree (Bentley, 1979, 1980; Chazelle, 1988, 1990a,b), where \u2113 is the cardinality of query results, as opposed to arc-factored models in which solving the same problem takes O(N) computations.\nFor \u2295-aggregation, a more efficient algorithm which makes use of (K\u22121)-dimensional range trees can be designed. In future work, we show that computing the complexity of \u2295-aggregation for all x \u2208 V can be further reduced to O ( KN logK\u22122N ) by applying Fredman\u2019s (1976) trick which we used in Algorithm 1. Extending the notation in \u00a74.3, the set of all vertices V can be partitioned into K subsets S1(x), \u00b7 \u00b7 \u00b7 ,SK(x) for each x \u2208 V , where Sk(x) = {y | y \u2208 V \u2227 F\u03b8(x, y) = f (k) \u03b8 (x) \u2212 f (k) \u03b8 (y)}.\u2295\ny\u2208V F\u03b8(x, y) can be decomposed into a \u2295-aggregation of K terms.\nG(x) def = \u2295 y\u2208V F\u03b8(x, y) (8a)\nG(x) = \u2295\nk\u2208[K] (\u2295 y\u2208Sk F\u03b8(x, y) )\n\ufe38 \ufe37\ufe37 \ufe38 def =Gk(x)\n(8b)\nWe leave to future work showing that computing each Gk(x) takes O ( logK\u22122N ) ."
        },
        {
            "heading": "F Hyperparameter Settings",
            "text": "F.1 Dependency Parsing For pretrained language models, we use XLNet-large-cased11 (Yang et al., 2019) for PTB, bert-base-chinese12 for CTB, and bert-base-multilingual-cased13 for UD. We set the dimension of POS tag embedding to 256 for all experiments. On top of concatenated pretrained representations\n11https://huggingface.co/xlnet-large-cased 12https://huggingface.co/bert-base-chinese 13https://huggingface.co/bert-base-multilingual-cased\nand POS embedding, we use a 3-layer BiLSTM (Hochreiter and Schmidhuber, 1997) with a hidden size of 768 for base-sized models (bert-base-chinese on CTB and bert-multilingual-cased on UD) and 1024 for large-sized models (xlnet-large-cased on PTB). We apply dropout with a rate of 0.33 to the concatenated embedding layer, between LSTM layers, and before the linear projection layer of the realizer. We employ AdamW (Loshchilov and Hutter, 2019) with a learning rate of 2e\u22125 for pretrained LMs and 1e\u22124 for POS embedding, BiLSTM, and linear projection during training. The gradient clipping threshold is set to 1.0. The batch size for training is 32. The number of training epochs is 50.\nF.2 Coreference Resolution We use longformer-large-cased14 (Beltagy et al., 2020) as the pretrained encoder. We use the same hyperparameter settings as Kirstain et al. (2021). We use AdamW with a learning rate of 1e\u22125 for pretrained LM and 3e\u22124 for the linear projection during training, with 5600 linear warmup steps. Training documents are batched into batches with maximum 5000 tokens in total. The number of training epochs is 129."
        },
        {
            "heading": "G Datasets",
            "text": "G.1 Dependency Parsing Preprocessing. We follow previous work (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) to derive the dependency annotations from the treebank annotations using the Stanford Dependency converter v3.3.0 (de Marneffe and Manning, 2008). During evaluation, punctuations are omitted. Following Amini et al. (2023), we provide gold part-of-speech tags to the model during training and decoding.\nSplits. The dataset splits are consistent with previous work. For PTB, we follow the standard split of Marcus et al. (1993), resulting in 39,832 sentences for training, 1,700 for development, and 2,416 for testing. For CTB, we follow the split of Zhang and Clark (2008), resulting in 16,091 sentences for training, 803 for development, and 1,910 for testing. For UD, we follow previous work (Zhang et al., 2020; Yang and Tu, 2022) and use the standard splits of the following corpora for experiments: BG-btb, CA-ancora, CS-pdt, DE-gsd, EN-ewt, ES-ancora, FR-gsd, IT-isdt, NL-alpino, NO-rrt, RO-rrt, RU-syntagrus.\nLicenses. The PTB and CTB datasets are licensed under LDC User Agreement. The UD dataset is licensed under the Universal Dependencies License Agreement.\nG.2 Coreference Resolution Preprocessing. We experiment on the CoNLL-2012 English shared task dataset (OntoNotes; Pradhan et al., 2012). We follow the preprocessing procedure of (Kirstain et al., 2021). During training and decoding, the speaker information is provided to the model.\nSplits. The OntoNotes dataset contains 2,802 documents for training, 343 for validation, and 348 for testing. We use this official split following previous work (Lee et al., 2017; Kirstain et al., 2021).\nLicenses. The OntoNotes dataset is licensed under LDC User Agreement."
        },
        {
            "heading": "H Efficiency Evaluation",
            "text": "H.1 Dependency Parsing For efficiency evaluation, BERT-large-cased15 is used as the pretrained encoder for our method with K = 2, hexatagger (Hexa; Amini et al., 2023), and biaffine model (Biaff). We use the English PTB test set and truncate or pad the input sentences to the control length. The results are averaged over 3 random runs on the same server with one Nvidia A100-80GB GPU. The other experimental settings are kept the same (i.e., the version of PyTorch and transformers, FP32 precision, batching).\nH.2 Coreference Resolution 14https://huggingface.co/allenai/longformer-large-4096 15https://huggingface.co/bert-large-cased 16https://huggingface.co/allenai/longformer-base-4096\nWe compare the efficiency of our order-theoretic method with baseline coreference resolution model. The full results are given in Tab. 4. On the OntoNotes coreference resolution benchmark, our method is twice as fast as Kirstain et al.\u2019s (2021) model while using less memory, especially on long documents. It is worth noting that Kirstain et al. (2021) has already performed aggressive optimization in both the speed and memory footprint of coreference modeling. I.e., they abandon the computation for textual span representations and entity-pair representations, and use biaffine scorers to compute coreference scores."
        },
        {
            "heading": "I Additional Experimental Results",
            "text": "I.1 Dependency Parsing We report additional experimental results on the UD dependency parsing dataset in Tab. 5. On average, our model has state-of-the-art performance and outperforms all other baseline models on 5 languages."
        },
        {
            "heading": "J Qualitative Examples",
            "text": "We present some qualitative examples from the PTB development set and one non-projective example using our method with a 2-dimensional realizer, with their ground truth annotations on the right in Figures 4\u20139. For a more intuitive and compact exhibition, we plot the 2 total orders output by our model in a 2-dimensional plane. Each axis corresponds to one of the 2 orders. The relation x\u227a y encoded by\u2227\nk\u2208{1,2} f (k) \u03b8 (x) < f (k) \u03b8 (y) is equivalent to x being located below and to the left of y.\nTokens in V r and V b are represented by and , respectively. The line segments between and are the extracted dependency relations. In each of the plots, every (token in V r) except for the root is connected to a (token in V b), which indicates is the modifier of . The roots (about, moving, ready, had, adds, bought represented by ) are not connected to any other word."
        }
    ],
    "title": "Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective",
    "year": 2023
}