{
    "abstractText": "The majority of existing work on sign language recognition encodes signed videos without explicitly acknowledging the phonological attributes of signs. Given that handshape is a vital parameter in sign languages, we explore the potential of handshape-aware sign language recognition. We augment the PHOENIX14T dataset with gloss-level handshape labels, resulting in the new PHOENIX14T-HS dataset. Two unique methods are proposed for handshape-inclusive sign language recognition: a single-encoder network and a dualencoder network, complemented by a training strategy that simultaneously optimizes both the CTC loss and frame-level crossentropy loss. The proposed methodology consistently outperforms the baseline performance. The dataset and code can be accessed at https://github.com/Este1le/slr_handshape.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Kevin Duh"
        }
    ],
    "id": "SP:ab528fcf5c53f38ae82c37162dd8a5cb089a6ecd",
    "references": [
        {
            "authors": [
                "Necati Cihan Camgoz",
                "Simon Hadfield",
                "Oscar Koller",
                "Hermann Ney",
                "Richard Bowden."
            ],
            "title": "Neural sign language translation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7784\u20137793.",
            "year": 2018
        },
        {
            "authors": [
                "Necati Cihan Camgoz",
                "Oscar Koller",
                "Simon Hadfield",
                "Richard Bowden."
            ],
            "title": "Sign language transformers: Joint end-to-end sign language recognition and translation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yutong Chen",
                "Fangyun Wei",
                "Xiao Sun",
                "Zhirong Wu",
                "Stephen Lin."
            ],
            "title": "A simple multi-modality transfer learning baseline for sign language translation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "Yutong Chen",
                "Ronglai Zuo",
                "Fangyun Wei",
                "Yu Wu",
                "Shujie Liu",
                "Brian Mak."
            ],
            "title": "Two-stream network for sign language recognition and translation",
            "venue": "arXiv preprint arXiv:2211.01367.",
            "year": 2022
        },
        {
            "authors": [
                "Ka Leong Cheng",
                "Zhaoyang Yang",
                "Qifeng Chen",
                "Yu-Wing Tai."
            ],
            "title": "Fully convolutional networks for continuous sign language recognition",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Necati Cihan Camgoz",
                "Simon Hadfield",
                "Oscar Koller",
                "Richard Bowden."
            ],
            "title": "Subunets: End-to-end hand shape and continuous sign language recognition",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 3056\u20133065.",
            "year": 2017
        },
        {
            "authors": [
                "Runpeng Cui",
                "Hu Liu",
                "Changshui Zhang."
            ],
            "title": "A deep neural framework for continuous sign language recognition by iterative training",
            "venue": "IEEE Transactions on Multimedia, 21(7):1880\u20131891.",
            "year": 2019
        },
        {
            "authors": [
                "Taylor Fahey",
                "Allison Hilger"
            ],
            "title": "Impact of manual american sign language parameters on intelligibility",
            "year": 2022
        },
        {
            "authors": [
                "Jens Forster",
                "Christoph Schmidt",
                "Oscar Koller",
                "Martin Bellgardt",
                "Hermann Ney."
            ],
            "title": "Extensions of the sign language recognition and translation corpus rwth-phoenix-weather",
            "venue": "LREC, pages 1911\u20131916.",
            "year": 2014
        },
        {
            "authors": [
                "Aiming Hao",
                "Yuecong Min",
                "Xilin Chen."
            ],
            "title": "Selfmutual distillation learning for continuous sign language recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11303\u201311312.",
            "year": 2021
        },
        {
            "authors": [
                "Joseph C Hill",
                "Diane C Lillo-Martin",
                "Sandra K Wood."
            ],
            "title": "Sign languages: Structures and contexts",
            "venue": "Routledge.",
            "year": 2018
        },
        {
            "authors": [
                "Will Kay",
                "Jo\u00e3o Carreira",
                "Karen Simonyan",
                "Brian Zhang",
                "Chloe Hillier",
                "Sudheendra Vijayanarasimhan",
                "Fabio Viola",
                "Tim Green",
                "Trevor Back",
                "Paul Natsev",
                "Mustafa Suleyman",
                "Andrew Zisserman."
            ],
            "title": "The kinetics human action video dataset",
            "venue": "CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Oscar Koller",
                "Necati Cihan Camgoz",
                "Hermann Ney",
                "Richard Bowden."
            ],
            "title": "Weakly supervised learning with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language videos",
            "venue": "IEEE transactions on pattern analysis and machine intelli-",
            "year": 2019
        },
        {
            "authors": [
                "Oscar Koller",
                "Jens Forster",
                "Hermann Ney."
            ],
            "title": "Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers",
            "venue": "Computer Vision and Image Understanding, 141:108\u2013125.",
            "year": 2015
        },
        {
            "authors": [
                "Oscar Koller",
                "Hermann Ney",
                "Richard Bowden."
            ],
            "title": "May the force be with you: Force-aligned signwriting for automatic subunit annotation of corpora",
            "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG),",
            "year": 2013
        },
        {
            "authors": [
                "Oscar Koller",
                "Hermann Ney",
                "Richard Bowden."
            ],
            "title": "Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages",
            "year": 2016
        },
        {
            "authors": [
                "Oscar Koller",
                "Sepehr Zargaran",
                "Hermann Ney."
            ],
            "title": "Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4297\u20134305.",
            "year": 2017
        },
        {
            "authors": [
                "Harlan L Lane",
                "Robert Hoffmeister",
                "Benjamin J Bahan."
            ],
            "title": "A journey into the deaf-world",
            "venue": "Dawn Sign Press.",
            "year": 1996
        },
        {
            "authors": [
                "Dongxu Li",
                "Cristian Rodriguez",
                "Xin Yu",
                "Hongdong Li."
            ],
            "title": "Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison",
            "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yuecong Min",
                "Aiming Hao",
                "Xiujuan Chai",
                "Xilin Chen."
            ],
            "title": "Visual alignment constraint for continuous sign language recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11542\u201311551.",
            "year": 2021
        },
        {
            "authors": [
                "Zhe Niu",
                "Brian Mak."
            ],
            "title": "Stochastic fine-grained labeling of multi-state sign glosses for continuous sign language recognition",
            "venue": "Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVI 16,",
            "year": 2020
        },
        {
            "authors": [
                "Katerina Papadimitriou",
                "Gerasimos Potamianos."
            ],
            "title": "Multimodal sign language recognition via temporal deformable convolutional sequence learning",
            "venue": "Interspeech, pages 2752\u20132756.",
            "year": 2020
        },
        {
            "authors": [
                "Ilias Papastratis",
                "Kosmas Dimitropoulos",
                "Dimitrios Konstantinidis",
                "Petros Daras."
            ],
            "title": "Continuous sign language recognition through cross-modal alignment of video and text embeddings in a joint-latent space",
            "venue": "IEEE Access, 8:91170\u201391180.",
            "year": 2020
        },
        {
            "authors": [
                "Junfu Pu",
                "Wengang Zhou",
                "Hezhen Hu",
                "Houqiang Li."
            ],
            "title": "Boosting continuous sign language recognition via cross modality augmentation",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, pages 1497\u20131505.",
            "year": 2020
        },
        {
            "authors": [
                "Valerie Sutton",
                "DAC."
            ],
            "title": "Sign writing",
            "venue": "Deaf Action Committee (DAC). Technical Report.",
            "year": 2000
        },
        {
            "authors": [
                "Saining Xie",
                "Chen Sun",
                "Jonathan Huang",
                "Zhuowen Tu",
                "Kevin Murphy."
            ],
            "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pages 305\u2013321.",
            "year": 2018
        },
        {
            "authors": [
                "Kayo Yin",
                "Amit Moryossef",
                "Julie Hochgesang",
                "Yoav Goldberg",
                "Malihe Alikhani."
            ],
            "title": "Including signed languages in natural language processing",
            "venue": "arXiv preprint arXiv:2105.05222.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhou",
                "Wengang Zhou",
                "Weizhen Qi",
                "Junfu Pu",
                "Houqiang Li."
            ],
            "title": "Improving sign language translation with monolingual data by sign back-translation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1316\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhou",
                "Wengang Zhou",
                "Yun Zhou",
                "Houqiang Li."
            ],
            "title": "Spatial-temporal multi-cue network for sign language recognition and translation",
            "venue": "IEEE Transactions on Multimedia, 24:768\u2013779.",
            "year": 2021
        },
        {
            "authors": [
                "Ronglai Zuo",
                "Brian Mak."
            ],
            "title": "C2slr: Consistencyenhanced continuous sign language recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5131\u2013 5140.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Sign languages are primarily the languages of Deaf people. They are the center of the Deaf culture and the daily lives of the Deaf community. 1 In the U.S., estimates suggest that between 500,000 to two million people communicate using American Sign Language (ASL), making it the fifth mostused minority language in the country after Spanish, Italian, German, and French (Lane et al., 1996). Natural sign languages, which develop independently and possess unique grammatical structures distinct from surrounding spoken languages, are just as crucial to include in the field of natural language processing (NLP) as any other language, as Yin et al. (2021) advocates.\nOne direction in sign language processing (SLP) is sign language recognition (SLR), a task of recognizing and translating signs into glosses, the writ-\n1Deaf sociolinguist Barbara Kannapell: \"It is our language in every sense of the word. We create it, we keep it alive, and it keeps us and our traditions alive.\" And further, \"To reject ASL is to reject the Deaf person.\"\nten representations of signs typically denoted by spoken language words. Among the array of SLR products, sign gloves, and wearable devices using sensors to track hand movements, are widespread. However, these devices have faced criticism from the Deaf community, primarily due to the social stigma associated with wearing them. This feedback has motivated us to explore video-based SLR, an alternative approach that utilizes cameras to record signs and feed them into the system as video inputs. By doing so, we aspire to foster a more inclusive society, preserving the valuable sign languages serving as the heart of the Deaf culture, and thereby facilitating improved communications between the Deaf and hearing communities.\nSigns can be defined by five parameters: handshape, orientation, location, movement, and nonmanual markers such as facial expressions. Signs that differ in only one of these parameters can form minimal pairs. An example of a handshape minimal pair in ASL is illustrated in Figure 1. As reported by Fahey and Hilger (2022), among all parameters, handshape minimal pairs are identified with the lowest accuracy \u2013 only 20%, compared to palm orientation (40%), location (47%), and movement (87%). This indicates the complexity in-\n2Images clipped from https://babysignlanguage.com/.\nvolved in distinguishing handshapes, underscoring their importance in correctly interpreting signs.\nThe majority of existing research on SLR does not incorporate phonological features such as handshapes into their system designs, with only a few exceptions (Koller et al., 2016; Cihan Camgoz et al., 2017; Koller et al., 2019). Typically, signs are interpreted as a cohesive whole, meaning that an SLR model is expected to correctly recognize all five parameters simultaneously to accurately identify a sign. This constitutes a major distinction between spoken and sign languages \u2013 the former is linear, while the latter incorporates both linearity and simultaneity (Hill et al., 2018). This uniqueness introduces considerable challenges to SLR tasks.\nThe limited interest in integrating handshapes into SLR systems can be attributed largely to the absence of handshape annotations in existing Continuous SLR (CSLR3) datasets. In response to this, we have extended one of the most widely used SLR datasets, PHOENIX14T, with handshape annotations, sourced from online dictionaries and manual labeling, thus creating the PHOENIX14T-HS dataset. Our hope is that this will facilitate more research into handshape-aware SLR.\nMoreover, we introduce two handshapeinclusive4 SLR networks (Figure 2), designed with either single or dual-encoder architectures. These proposed models extend the basic SLR network, which doesn\u2019t include handshape information in gloss prediction. Thus, any existing SLR can adopt\n3CSLR refers to the recognition of sign language at the sentence level, as opposed to Isolated SLR, which operates at the word level. Our work focuses on CSLR due to its broader practical application and a higher level of complexity.\n4We use handshape-aware to denote SLR that incorporates handshape information during training, while handshapeinclusive pertains to the deliberate inclusion of handshape predictions within SLR.\nour approach, underscoring the adaptability of our methods.\nWe set a benchmark on the PHOENIX14T-HS dataset with the proposed methods. Our models outperform previous state-of-the-art (SOTA) singlemodality SLR networks, which utilize only RGB videos as input and were trained on PHOENIX14T."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 SLR",
            "text": "In recent developments of CSLR, a predominant methodology has emerged that employs a hybrid model. The model is usually composed of three essential components: a visual encoder, which extracts the spatial features from each frame of the sign video; a sequence encoder, responsible for learning the temporal information; and an alignment module which monotonically aligns frames to glosses. The visual encoder component could be built with various architectures, including 2DCNNs (Koller et al., 2019; Cheng et al., 2020; Min et al., 2021), 3D-CNNs (Chen et al., 2022a,b), or 2D-CNNs followed by 1D-CNNs (Papastratis et al., 2020; Pu et al., 2020; Zhou et al., 2021a,b). The sequence encoder can be implemented using LSTMs (Cui et al., 2019; Pu et al., 2020; Zhou et al., 2021b), Transformer encoders (Niu and Mak, 2020; Camgoz et al., 2020; Zuo and Mak, 2022; Chen et al., 2022b), or 1D-CNNs (Cheng et al., 2020). In terms of the alignment module, research attention has been redirected from HMM (Koller et al., 2017, 2019) to connectionist temporal classification (CTC) (Hao et al., 2021; Zhou et al., 2021b; Zuo and Mak, 2022; Chen et al., 2022b).\nVarious approaches have been proposed to improve SLR system performance.\nMulti-stream network The multi-stream networks use multiple parallel encoders to extract features from distinct input streams. In addition to the RGB stream, Cui et al. (2019) incorporate an optical flow stream, while Zhou et al. (2021b); Chen et al. (2022b) use key points. Koller et al. (2019) and Papadimitriou and Potamianos (2020) introduce two extract encoders for hand and mouth encoding, directing the system\u2019s focus towards critical image areas.\nCross-entropy loss Training objectives beyond CTC loss can also be employed. Cheng et al. (2020); Hao et al. (2021) train their models to also minimize the frame-level cross-entropy loss, with frame-level labels derived from the CTC decoder\u2019s most probable alignment."
        },
        {
            "heading": "2.2 Handshape-inclusive Datasets",
            "text": "Currently, datasets frequently employed for the continuous SLR task, such as RWTH-PHOENIXWeather 2014T (Camgoz et al., 2018) and CSL Daily (Zhou et al., 2021a) generally lack handshape annotations, except for RWTH-PHOENIXWeather 2014 (Koller et al., 2015), which is extended by Forster et al. (2014) with handshape and orientation labels. The annotating process involved initially labeling the orientations frame-byframe, followed by clustering within each orientation, and then manually assigning a handshape label to each cluster. Additionally, a subset of 2k signs is annotated using the SignWriting (Sutton and DAC, 2000) annotation system. To facilitate handshape recognition, Koller et al. (2016) introduced the 1-Million-Hands dataset, comprising 1 million cropped hand images from sign videos, each labeled with a handshape. The dataset consists of two vocabulary-level datasets in Danish and New Zealand sign language, where handshapes are provided in the lexicon, and a continuous SLR dataset, PHOENIX14, annotated with SignWriting. It also includes 3k manually labeled handshape test images."
        },
        {
            "heading": "2.3 Handshape-aware SLR",
            "text": "Research on leveraging handshape labels to support SLR has been relatively scarce. Koller et al. (2016) applied the statistical modelling from Koller et al. (2015) and incorporated a stacked fusion with features from the 1-Million-Hands model and full frames. While Cihan Camgoz et al. (2017) and Koller et al. (2019) utilized a multi-stream sys-\ntem, where two separate streams are built to predict handshapes and glosses, respectively. These two streams are then merged and trained for gloss recognition. The aforementioned studies are all carried out on the PHOENIX14 dataset, made possible by the efforts of Forster et al. (2014), which extended the dataset with handshape labels. Our work instead focuses on the PHOENIX14T dataset."
        },
        {
            "heading": "3 Datasets",
            "text": "We have enriched the SLR dataset PHOENIX14T by incorporating handshape labels derived from the SignWriting dictionary and manual labeling. In the subsequent sections, we initially present the original PHOENIX14T dataset (3.1) and the SignWriting dictionary (3.2), followed by a detailed description of the updated PHOENIX14T dataset (PHOENIX14T-HS), now featuring handshape labels (3.3)."
        },
        {
            "heading": "3.1 PHOENIX14T",
            "text": "PHOENIX14T (Camgoz et al., 2018) is one of the few predominantly utilized datasets for SLR tasks nowadays. This dataset consists of German sign language (DGS) aired by the German public TV station PHOENIX in the context of weather forecasts. The corpus comprises DGS videos from 9 different signers, glosses annotated by deaf experts, and translations into spoken German language. Key statistics of the dataset are detailed in Table.1.\nPHOENIX14T (Camgoz et al., 2018), an extension of PHOENIX14 (Koller et al., 2015), features redefined sentence segmentations and a slightly reduced vocabulary compared to its predecessor. Despite Forster et al. (2014) having expanded PHOENIX14 with handshape labels, their extended dataset is not publicly accessible and only includes labels for the right hand. In contrast, our annotated data will be released publicly, encompassing handshapes for both hands."
        },
        {
            "heading": "3.2 SignWriting",
            "text": "The SignWriting dictionary (Sutton and DAC, 2000; Koller et al., 2013) publicly accessible, useredited sign language dataset, encompassing more than 80 distinct sign languages. Adhering to the International SignWriting Alphabet, which prescribes a standard set of icon bases, users represent signs via abstract illustrations of handshapes, facial expressions, orientations, and movements. These depictions can be encoded into XML format and\n{name: train/01April_2010_Thursday_heute-6703, signer: Signer04, gloss: MORGEN TEMPERATUR ACHT BIS DREIZEHN MAXIMAL DREIZEHN, handshape-right: [[1], [f], [3], [index], [3], [b_thumb], [3]], handshape-left: [[], [f], [5], [], [], [b_thumb], []]}. 5\nconverted into textual descriptions. We utilized the SignWriting parser6 provided by Koller et al. (2013) to extract handshapes for both hands from the original SignWriting dictionary.\n6https://github.com/huerlima/signwriting-parser 6Note that one-handed signs do not have handshape labels for the left hand, which is the non-dominant hand for this signer."
        },
        {
            "heading": "3.3 Handshape-extended PHOENIX14T (PHOENIX14T-HS)",
            "text": "There are 17,947 entries for DGS in the SignWriting dictionary. However, 314 signs/glosses in the PHOENIX14T dataset are either not included or lack handshape annotations in the dictionary (Table.1). This implies that 4,366 of the 7,096 samples in the train set contain signs devoid of handshape labels. We thus manually labeled these 314 signs.\nThis results in the following annotation steps:\n1. Look up the SignWriting dictionary. 2. Manually label handshapes for signs not\npresent in SignWriting.\nThe author, who has a competent understanding of ASL and Sign Language Linguistics, yet lacks formal training in DGS, annotated by simultaneously watching the corresponding sign video to ensure alignment. The task proves particularly demanding when consecutive gaps\u2013signs missing handshapes\u2013emerge. To delineate the boundaries of these signs, the author resorted to online DGS dictionaries (not SignWriting). The entire manual annotation process took around 30 hours.\nOur method contrasts with that of Koller et al. (2016), which applied frame-level handshape annotations. We have instead adopted gloss-level handshape annotations. While the frame-level approach is more detailed, Koller et al. (2016) reported a significant number of blurred frames, making the task of frame-by-frame labeling both challenging and time-intensive. Moreover, given that sign language recognition is essentially a gloss-level recognition task, our aim is to maintain consistency in granularity when integrating handshape recognition as an additional task within the framework.\nWe adopt the Danish taxonomy for handshape labels as in (Koller et al., 2016), which includes 60 unique handshapes. The application results in the PHOENIX14T-HS dataset, from where an example is shown in Figure 3. Given that all 9 signers in PHOENIX14T are right-hand dominant, it is appropriate to employ the default annotations from SignWriting without necessitating side-switching.\nFigure 4 presents the frequency distribution of the top 10 most prevalent handshapes for each hand in the PHOENIX14T dataset. It is important to note that a single sign may comprise multiple handshapes. In fact, 13.5% of the signs in the dataset incorporate more than one handshape for the right hand, whereas the left hand employs more than one handshape in 5% of the signs.\nLimitations We would like to note certain limitations in the proposed PHOENIX14T-HS dataset. While approximately one-third of the signs were manually labeled with handshapes, the remaining two-thirds were labeled using the user-generated SignWriting dictionary. As a result, these handshape labels may contain noise and should not be seen as curated. When dealing with sign variants, i.e. multiple entries for a single sign, our selection process was random and thus may not necessarily correspond with the sign video.\nMoreover, individual signers possess unique signing preferences, leading them to opt for different sign variants. Furthermore, the signers might deviate from the dictionary-form signs, resulting in discrepancies between the real-world usage and the standardization form. In terms of our labeling process, we omitted handshape labels during the initial and final moments of each video, when the signers\u2019 hands are in a resting position. Finally,\nwe did not account for co-articulation, the transition phase between two consecutive signs, in our handshape labeling."
        },
        {
            "heading": "4 Methods",
            "text": "The task of SLR can be defined as follows. Given an input sign video V = (v1, ..., vT ) with T frames, the goal is to learn a network to predict a sequence of glosses G = (g1, ..., gL) with L words, monotonically aligned to T frames, where T \u2265 L.\nIn this section, we start by describing the vanilla SLR network, where handshapes are not provided or learned during training in Section 4.1. We then introduce the two handshape-inclusive network architectures employed in this study in Section 4.2. Specifically, these networks are designed to predict sign glosses and handshapes concurrently. Finally, we elaborate on our chosen training and pretraining strategy in Section 4.3 and 4.4."
        },
        {
            "heading": "4.1 Vanilla SLR networks",
            "text": "The architecture of the vanilla SLR network is illustrated in Figure 5. Similar to Chen et al. (2022a) and Chen et al. (2022b), we use an S3D (Xie et al., 2018) as the video encoder, followed by a head network, where only the first four blocks of S3D are included to extract dense temporal representations. Then, a gloss classification layer and a CTC decoder are attached to generate sequences of gloss predictions."
        },
        {
            "heading": "4.2 Handshape-inclusive SLR networks",
            "text": "Figure 2 depicts our proposal of two handshapeinclusive SLR network variants, which are expansions upon the vanilla network. Both variants explicitly utilize handshape information by training\nthe model to predict glosses and handshapes concurrently. The key distinction between the two variants lies in the employment of either a single encoder or dual encoders.\nModel I. In comparison to the vanilla network, this model forwards the S3D feature to two additional heads, each tasked with predicting the handshapes for the left and right hand respectively. The loss for this model is computed as follows:\nLModelI = L G CTC + \u03bb LLLCTC + \u03bbRLRCTC , (1)\nwhere LGCTC represents the CTC loss of the gloss predictor. LLCTC and LGCTC denote the CTC losses for the left and right handshape predictors, weighted by \u03bbL and \u03bbR.\nModel II. This model employs dual encoders, each dedicated to encoding the representations for glosses and handshapes independently. While both encoders receive the same input (sign videos) and share the same architecture, they are trained with different target labels (gloss vs. handshape). We also incorporate a joint head, which combines the visual representation learned by both encoders to generate gloss predictions. The architecture of this joint head mirrors that of the gloss head and the handshape head. Therefore, the loss for this model is computed as follows:\nLModelII = L G CTC+LJCTC+\u03bbLLLCTC+\u03bbRLRCTC , (2) where LJCTC denotes the CTC loss of the joint gloss predictor.\nFor this model, we also adopt a late ensemble strategy. This involves averaging the gloss probabilities predicted by both the gloss head and the joint head. The averaged probabilities are then fed into a CTC decoder, producing the gloss sequence."
        },
        {
            "heading": "4.3 Training strategy",
            "text": "The CTC loss is computed by taking the negative logarithm of the probability of the correct path, which corresponds to the true transcription. It is a relatively coarse-grained metric because it operates at the gloss level, not requiring temporal boundaries of glosses. Given that handshape prediction could potentially operate on a frame level, it stands to reason for us to compute the loss at this level as well. However, as the PHOENIX14T-HS dataset does not provide temporal segmentations, we opt to estimate these with gloss probabilities7 generated\n7For Model II, they are the averaged probabilities.\nby our models. First, we extract the best path for glosses from a CTC decoder and fill in the blanks with neighboring glosses. After this, if a particular gloss has only one associated handshape, we assign that handshape to all frames within the extent of that gloss. If there is more than one handshape, we gather the handshape probabilities produced by the handshape classifiers within that segment and feed them into a CTC decoder to determine the optimal handshape labels for the frames within that gloss\u2019s range8. Finally, we calculate the crossentropy loss between the pseudo-labels and the handshape probabilities. This enables more finegrained frame-level supervision.\nThe loss function then becomes:\nLSLR = LModelI|II +\u03bb L CELLCE+\u03bbRCELRCE , (3)\nwhere LLCE and LRCE are cross-entropy loss for left and right hand weighted by \u03bbLCE and \u03bb R CE respectively."
        },
        {
            "heading": "4.4 Pretraining",
            "text": "Given that our target dataset is relatively sparse, it\u2019s crucial to pretrain the model to ensure a solid initialization. We first pretrain the S3D encoder on the action recognition dataset, Kinetics-400 (Kay et al., 2017), consisting of 3 million video clips. Following this, we further pretrain on a word-level ASL dataset, WLASL (Li et al., 2020), which includes 21 thousand videos."
        },
        {
            "heading": "5 Experiments",
            "text": "In this section, we present the performance of our top-performing model (Section 5.1) and further conduct ablation study (Section 5.2) to analyze the crucial components of our implementations."
        },
        {
            "heading": "5.1 Best model",
            "text": "Our highest-performing system utilizes the dualencoder architecture of Model II. After initial pretraining on Kinetics-400 and WLASL datasets, we freeze the parameters of the first three blocks of the S3D. For the hyperparameters, we set \u03bbL and \u03bbR to 1, while \u03bbLCE and \u03bb R CE are set to 0.05. The initial learning rate is 0.001. Adam is used as the optimizer.\nA comparison of our premier system (HS-SLR) with leading SLR methods on the PHOENIX14T\n8For the left hand, when the sign does not have corresponding handshape, we label it with the special token <pad>.\ndataset is shown in Table 2. While we do not surpass the existing records, our system ranks as the top performer among all single-modality models. It is worth noting that the extension of multi-modality models into handshape-inclusive models, such as introducing handshape heads or an additional handshape encoder to the original networks, could potentially enhance the SOTA performance further.9"
        },
        {
            "heading": "5.2 Ablation study",
            "text": ""
        },
        {
            "heading": "5.2.1 Model variants",
            "text": "In our analysis, we contrast our suggested model variants, Model I and Model II (discussed in Section 4.2), with the Vanilla SLR network (described in Section 4.1). Additionally, we compare models that feature solely a right handshape head against those equipped with two heads, one for each hand. An extended variant, Model II+, which adds two handshape heads to the gloss encoder, is also considered in our experimentation.\nAs demonstrated in Table 3, Model II outperforms Model I and Model II+. The performance differs marginally between models with handshape heads for both heads versus those with a single right-hand head.\n9Due to computational resource constraints, we are currently unable to fit such models into our GPU devices. Future work may explore the expansion of multi-modality models to include handshape-inclusive models."
        },
        {
            "heading": "5.2.2 Pretraining for gloss encoder",
            "text": "We delve into optimal pretraining strategies for the S3D encoder that\u2019s coupled with a gloss head. We conduct experiments using Model I, as shown in Table 4. We contrast the efficacy of four distinct pretraining methodologies: (1) pretraining solely on Kinetics-400; (2) sequential pretraining, first on Kinetics-400, followed by WLASL; (3) tripletiered pretraining on Kinetics-400, then WLASL, and finally on handshape prediction by attaching two handshape heads while deactivating the gloss head; and (4) a similar three-stage process, but focusing on gloss prediction in the final step."
        },
        {
            "heading": "5.2.3 Pretraining for handshape encoder",
            "text": "Table 5 outlines various pretraining strategies adopted for the handshape encoder in Model II. The results pertain to right handshape predictions on the PHOENIX14T-HS dataset. Both Kinetics and WLASL are employed for gloss predictions, as they lack handshape annotations. We also test the 1-Million-Hands dataset (Koller et al., 2016) for pretraining purposes. This dataset comprises a million images cropped from sign videos, each\nlabeled with handshapes. To adapt these images for S3D, we duplicate each image 16 times, creating a \u2018static\u2019 video. Furthermore, we experiment with two input formats: the full frame and the righthand clip. As indicated in Table 5, both pretraining and full-frame input significantly outperform their counterparts."
        },
        {
            "heading": "5.2.4 Frozen parameters",
            "text": "We evaluate the impact of freezing varying numbers of blocks within the pretrained S3D encoders in Model II. The results are presented in Table 6."
        },
        {
            "heading": "5.2.5 Cross-entropy loss",
            "text": "In Table 7, we investigate the computation of cross-entropy loss on handshape predictions utilizing pseudo-labels obtained via two methods: Ensemble and HS. The former pertains to pseudolabels gathered as outlined in Section 4.3, while the latter relies on CTC decoder-applied handshape probabilities from the handshape head to produce pseudo-labels. We also examine a hybrid approach (Ensemble, HS), which sums the losses from both methods. In addition, we tune the weights \u03bbLCE and \u03bbRCE in Equation 3, setting them to the same value."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this work, we introduce the concept of handshape-aware SLR, enriching this area of re-\nsearch by offering a handshape-enriched dataset, PHOENIX14T-HS, and proposing two distinctive handshape-inclusive SLR methods. Out methodologies maintain orthogonality with existing SLR architectures, delivering top performance among single-modality SLR models. Our goal is to draw increased attention from the research community toward the integration of sign language\u2019s phonological features within SLR systems. Furthermore, we invite researchers and practitioners in NLP to contribute to the relatively nascent and challenging research area of SLP, thus fostering a richer understanding from linguistic and language modeling perspectives.\nIn future work, we would like to explore three primary avenues that show promise for further exploration: (1) Extension of multimodal SLR models. This involves expanding multi-modality SLR models, which use inputs of various modalities like RGB, human body key points, and optical flow, to become handshape-aware. This approach holds potential as different streams capture distinct aspects of sign videos, supplying the system with a richer information set. (2) Contrastive learning. Rather than using handshape labels as supervision, they can be employed to generate negative examples for contrastive learning. This can be achieved by acquiring the gloss segmentation from the CTC decoder and replacing the sign in the positive examples with its counterpart in the handshape minimal pair. The resulting negative examples would be particularly challenging for the model to distinguish, thereby aiding in the development of better representations. (3) Data augmentation. Alternatively, to create negative examples for contrastive learning, the data volume could be increased using the same method that generates negative examples for contrastive learning.\nLimitations\nNoisy labels: As highlighted in Section 3, the handshape labels we create might be noisy, since two-thirds of them are from a user-edited online dictionary. Additionally, these labels may not correspond perfectly to the sign videos due to the variations among signers and specific signs.\nSingle annotator: Finding DGS experts to serve as our annotators proved challenging. Also, obtaining multiple annotators and achieving interannotator agreements proved to be difficult.\nSingle parameter: The dataset used in our study does not account for other sign language parameters including orientation, location, movement, and facial expressions. Moreover, these parameters are not explicitly incorporated as subsidiary tasks within our SLR methodologies.\nSingle dataset: We only extend a single dataset with handshape labels. It remains to be seen whether the methods we propose will prove equally effective on other datasets, featuring different sign languages, domains, or sizes."
        }
    ],
    "title": "Handshape-Aware Sign Language Recognition: Extended Datasets and Exploration of Handshape-Inclusive Method",
    "year": 2023
}