{
    "abstractText": "Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffusion LM, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffusion LM can achieve better generation quality than the state-of-the-art diffusion models with better efficiency. Code is available at https://github. com/SALT-NLP/Masked_Diffusioin_LM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaao Chen"
        },
        {
            "affiliations": [],
            "name": "Aston Zhang"
        },
        {
            "affiliations": [],
            "name": "Mu Li"
        },
        {
            "affiliations": [],
            "name": "Alex Smola"
        },
        {
            "affiliations": [],
            "name": "Diyi Yang"
        }
    ],
    "id": "SP:1d5ca4dc9de7860a64021e0a003839fb18dd6bb8",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Daniel D. Johnson",
                "Jonathan Ho",
                "Daniel Tarlow",
                "Rianne van den Berg"
            ],
            "title": "Structured denoising diffusion models in discrete state-spaces",
            "year": 2021
        },
        {
            "authors": [
                "Yang",
                "Xiaodong Liu",
                "Yu Wang",
                "Songhao Piao",
                "Jianfeng Gao",
                "Ming Zhou"
            ],
            "title": "Unilmv2: Pseudomasked language models for unified language model pre-training",
            "venue": "arXiv preprint arXiv:2002.12804",
            "year": 2020
        },
        {
            "authors": [
                "Christian Bentz",
                "Dimitrios Alikaniotis"
            ],
            "title": "The word entropy of natural languages",
            "year": 2016
        },
        {
            "authors": [
                "Jiaao Chen",
                "Diyi Yang."
            ],
            "title": "Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4106\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jiaao Chen",
                "Diyi Yang."
            ],
            "title": "Structure-aware abstractive conversation summarization via discourse and action graphs",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Jiaao Chen",
                "Diyi Yang."
            ],
            "title": "Controllable conversation generation with conversation structures via diffusion models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7238\u2013 7251.",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Andrea Madotto",
                "Janice Lan",
                "Jane Hung",
                "Eric Frank",
                "Piero Molino",
                "Jason Yosinski",
                "Rosanne Liu."
            ],
            "title": "Plug and play language models: A simple approach to controlled text generation",
            "venue": "International Conference on Learning Representa-",
            "year": 2020
        },
        {
            "authors": [
                "Danilo Dess\u00ed",
                "Rim Helaoui",
                "Vivek Kumar",
                "Diego Reforgiato Recupero",
                "Daniele Riboni"
            ],
            "title": "Tf-idf vs word embeddings for morbidity identification in clinical notes: An initial study",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT.",
            "year": 2019
        },
        {
            "authors": [
                "Bryan Eikema",
                "Wilker Aziz"
            ],
            "title": "Sampling-based approximations to minimum bayes risk decoding for neural machine translation",
            "year": 2021
        },
        {
            "authors": [
                "Zhujin Gao",
                "Junliang Guo",
                "Xu Tan",
                "Yongxin Zhu",
                "Fang Zhang",
                "Jiang Bian",
                "Linli Xu."
            ],
            "title": "Difformer: Empowering diffusion model on embedding space for text generation",
            "venue": "arXiv preprint arXiv:2212.09412.",
            "year": 2022
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Omer Levy",
                "Yinhan Liu",
                "Luke Zettlemoyer."
            ],
            "title": "Mask-predict: Parallel decoding of conditional masked language models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Shansan Gong",
                "Mukai Li",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Lingpeng Kong"
            ],
            "title": "Diffuseq: Sequence to sequence text generation with diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Jiatao Gu",
                "James Bradbury",
                "Caiming Xiong",
                "Victor O.K. Li",
                "Richard Socher."
            ],
            "title": "Non-autoregressive neural machine translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Zhengfu He",
                "Tianxiang Sun",
                "Kuanning Wang",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "title": "Diffusionbert: Improving generative masked language models with diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "year": 2020
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Alexey A. Gritsenko",
                "Jasmijn Bastings",
                "Ben Poole",
                "Rianne van den Berg",
                "Tim Salimans"
            ],
            "title": "Autoregressive diffusion models",
            "year": 2021
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Didrik Nielsen",
                "Priyank Jaini",
                "Patrick Forr\u00e9",
                "Max Welling"
            ],
            "title": "2021b. Argmax flows and multinomial diffusion: Learning categorical distributions",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S. Weld",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "Spanbert: Improving pre-training by representing and predicting spans",
            "venue": "Transactions of the Association for Computational Linguistics, 8:64\u201377.",
            "year": 2019
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "year": 2020
        },
        {
            "authors": [
                "Terry K Koo",
                "Mae Y Li."
            ],
            "title": "A guideline of selecting and reporting intraclass correlation coefficients for reliability research",
            "venue": "Journal of chiropractic medicine, 15(2):155\u2013163.",
            "year": 2016
        },
        {
            "authors": [
                "Ben Krause",
                "Akhilesh Deepak Gotmare",
                "Bryan McCann",
                "Nitish Shirish Keskar",
                "Shafiq Joty",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "GeDi: Generative discriminator guided sequence generation",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Shankar Kumar",
                "William Byrne."
            ],
            "title": "Minimum Bayes-risk decoding for statistical machine translation",
            "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2004
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Diffusionlm improves controllable text generation",
            "year": 2022
        },
        {
            "authors": [
                "Alisa Liu",
                "Maarten Sap",
                "Ximing Lu",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "year": 2021
        },
        {
            "authors": [
                "Jekaterina Novikova",
                "Ond\u0159ej Du\u0161ek",
                "Verena Rieser."
            ],
            "title": "The E2E dataset: New challenges for endto-end generation",
            "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201\u2013206, Saarbr\u00fccken, Germany. Association",
            "year": 2017
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "year": 2022
        },
        {
            "authors": [
                "Machel Reid",
                "Vincent J Hellendoorn",
                "Graham Neubig."
            ],
            "title": "Diffuser: Discrete diffusion via edit-based reconstruction",
            "venue": "arXiv preprint arXiv:2210.16886.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Ren",
                "Jinglin Liu",
                "Xu Tan",
                "Zhou Zhao",
                "Sheng Zhao",
                "Tie-Yan Liu"
            ],
            "title": "A study of nonautoregressive model for sequence generation",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Mohammad Norouzi."
            ],
            "title": "Non-autoregressive machine translation with latent alignments",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Nikolay Savinov",
                "Junyoung Chung",
                "Mikolaj Binkowski",
                "Erich Elsen",
                "Aaron van den Oord."
            ],
            "title": "Stepunrolled denoising autoencoders for text generation",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli."
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon."
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Yu Sun",
                "Shuohuan Wang",
                "Yukun Li",
                "Shikun Feng",
                "Xuyi Chen",
                "Han Zhang",
                "Xin Tian",
                "Danxiang Zhu",
                "Hao Tian",
                "Hua Wu."
            ],
            "title": "Ernie: Enhanced representation through knowledge integration",
            "venue": "arXiv preprint arXiv:1904.09223.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Yang",
                "Dan Klein."
            ],
            "title": "FUDGE: Controlled text generation with future discriminators",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2021
        },
        {
            "authors": [
                "Ling Yang",
                "Zhilong Zhang",
                "Yang Song",
                "Shenda Hong",
                "Runsheng Xu",
                "Yue Zhao",
                "Yingxia Shao",
                "Wentao Zhang",
                "Bin Cui",
                "Ming-Hsuan Yang"
            ],
            "title": "Diffusion models: A comprehensive survey of methods and applications",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, pages 5754\u20135764.",
            "year": 2019
        },
        {
            "authors": [
                "Jiasheng Ye",
                "Zaixiang Zheng",
                "Yu Bao",
                "Lihua Qian",
                "Mingxuan Wang."
            ],
            "title": "Dinoiser: Diffused conditional sequence learning by manipulating noises",
            "venue": "arXiv preprint arXiv:2302.10025.",
            "year": 2023
        },
        {
            "authors": [
                "Lin Zheng",
                "Jianbo Yuan",
                "Lei Yu",
                "Lingpeng Kong."
            ],
            "title": "A reparameterized discrete diffusion model for text generation",
            "venue": "arXiv preprint arXiv:2302.05737.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffusion LM, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffusion LM can achieve better generation quality than the state-of-the-art diffusion models with better efficiency. Code is available at https://github. com/SALT-NLP/Masked_Diffusioin_LM."
        },
        {
            "heading": "1 Introduction",
            "text": "We present a novel diffusion method for modeling languages, Masked-Diffusion LM (language model), which uses strategic soft-masking informed by linguistic features to corrupt both the discrete and continuous space, and then iteratively denoise them back by predicting the categorical distribution. Specifically, a strategic soft-masking process is designed that gradually adds perturbation to the input text in an order from harder or\n\u2217Correspondence to Jiaao Chen <jiaaochen@gatech.edu> and Aston Zhang <az@astonzhang.com>.\nmore informative words to simpler or less informative words through soft-masking. As a result, the models are encouraged to recover and generate the text following an easy-first-generation nature (Dieleman et al., 2022) to improve the generation structure and quality with more flexibility. Also, during the diffusion process, we directly predict the discrete token with cross-entropy loss that maps the continuous space to discrete textual space to stabilize the intermediate diffusion steps. Through our proposed Masked-Diffusion LM, the applicationspecific performance metrics as well as training efficiency are significantly improved over current diffusion language models based on experiments.\nOur work is inspired by recent advances in diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Yang et al., 2022; Ramesh et al., 2022; Rombach et al., 2022) that are introduced as a new generative modeling approach based on iterative denoising and have achieved high-quality generations for visual and audio modalities (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Nichol and Dhariwal, 2021; Kong et al., 2020).\nAlthough these approaches have received growing attention and achieved impressive success, applying diffusion models to textual domain is still challenging and under-explored due to the discrete nature of the text (e.g., one-hot vectors) compared to continuous data like images (e.g., RGB values) (Li et al., 2022). A few prior works (Li et al., 2022; Gong et al., 2022; He et al., 2022; Austin et al., 2021; Hoogeboom et al., 2021b) that explore using diffusion models on textual data can be divided into two lines. The first is to extend diffusion models to discrete state spaces (Austin et al., 2021; Hoogeboom et al., 2021b,a). The second is to perform the diffusion process and its reverse process in the continuous domain and bridge the continuous and the discrete domain through embedding and rounding (Li et al., 2022; He et al., 2022), for example,\nDiffusion-LM (Li et al., 2022). Despite the improvements, most previous works fail to leverage the linguistic features (e.g., words in sentences are with different importance) to noise the input textual data and recover it back in a more suitable way. Besides, they usually neglect or fail to adapt large pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020), which is an unmissable treasure in the NLP community: their adopted k-nearest-neighbor rounding technique that maps continuous space to discrete space cannot handle high-dimensional data in a stable and efficient way (Li et al., 2022). As a result, a corruption process tailored for languages and the objective that allows efficient and straightforward discrete and continuous space transformation is in great need. Our Masked-Diffusion LM realizes this extension.\nTo demonstrate the effectiveness of our introduced Masked-Diffusion LM, we perform experiments on E2E dataset (Novikova et al., 2017) and 5 controllable generation tasks (Li et al., 2022) including Semantic Content, Parts-of-speech, Syntax Tree, Syntax Spans, and Length. We observe that our Masked-Diffusion LM can (i) achieve the stateof-the-art performances compared to recent baseline models, and (ii) allow more efficient training and inference compared to previous Diffusion-LM.\nTo summarize, our contributions are: (1)We introduce a strategic masking noise strategy guided by linguistic features to corrupt the textual data in diffusion models for modeling languages. (2) We use linear layers and cross-entropy objectives to bridge the continuous and discrete spaces in the diffusion process for efficiency and stability. (3) We conduct experiments on different controllable generation tasks to demonstrate the effectiveness of our proposed methods compared to previous diffusion language models."
        },
        {
            "heading": "2 Related Work",
            "text": "Diffusion Models for Language There has been growing attention in deep generative diffusion models, which is a latent variable generative method based on iterative denoising (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021). Through a forward and diffusion process, diffusion models have shown state-of-the-art sample quality on generating in the continuous domain such as producing\nimages and audio (Ramesh et al., 2022; Rombach et al., 2022; Kong et al., 2020; Savinov et al., 2022). Despite their huge success, it is still challenging and under-explored to adapt diffusion models to discrete domains like languages. A few recent works have modified the diffusion models for textual data. For example, discrete forward processes, such as categorical transition kernels (Hoogeboom et al., 2021a; Ye et al., 2023), uniform transition kernels, and absorbing kernels (Hoogeboom et al., 2021b), have been introduced. However, replacing continuous diffusion with a discrete corruption process affords some flexibility (Dieleman et al., 2022; Zheng et al., 2023; Reid et al., 2022). Other works have also made efforts to model text in the continuous embedding space and applied Gaussian noise uniformly to every token (Li et al., 2022; He et al., 2022; Chen and Yang, 2023), which is closer to the settings in previous works of diffusion models. However, they neglect the inherent linguistic features in the text (e.g., different words are playing different roles in sentences) so the generated text often lacks coherence (He et al., 2022). Besides, the k-nearest-neighbor rounding technique (Li et al., 2022; Gao et al., 2022) holds up the decoding and convergence speed especially when the vocabulary is large or the hidden dimension is high, thus limiting the potential of combining large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020). To alleviate these issues, in our work, we introduce a linguistic-informed soft-masking process to corrupt the discrete and continuous space with structures, and then use linear projections and cross-entropy objectives to directly map the latent variables to textual data for better efficiency and generating better text.\nNon-Autoregressive Text Generation Most language models (Chowdhery et al., 2022; Brown et al., 2020) and text generation models (Vaswani et al., 2017a; Eikema and Aziz, 2021; Chen and Yang, 2020, 2021) follow a left-to-right autoregressive manner. However, the fixed generation order prevents the models\u2019 flexibility in editing former text based on later generation results, especially for global controllable generation settings. To overcome the limitations, non-autoregressive text modeling has been proposed (Ghazvininejad et al., 2019; Ren et al., 2020; Gu et al., 2018; Sa-\nharia et al., 2020; Savinov et al., 2022) through masked language models (Ghazvininejad et al., 2019), iterative sequence alignment (Saharia et al., 2020), insertion and deletion (Gu et al., 2018), or unrolling the generation path (Savinov et al., 2022). Our Masked-Diffusion LM achieves the non-autoregressive generation through gradually recovering the intermediate latent variables in a planned sequence from the forward process.\nPlug-and-Play Controllable Generation Our work is also closely related to the line of research about plug-and-play controllable generation methods (Yang and Klein, 2021; Dathathri et al., 2020; Krause et al., 2021; Liu et al., 2021), which modify the outputs based on extra guidance such as classifiers without changing or fine-tuning the pretrained language models. Dathathri et al. (2020) used gradients to edit the autoregressive language model\u2019s hidden representations to fulfill the control guidance. Yang and Klein (2021) proposed to reweight the predicted token from the language models while (Krause et al., 2021; Liu et al., 2021) further fine-tuned a smaller LM to reweight the token predictions. In this work, we apply the gradient-based plug-and-play approach to our Masked-Diffusion LM for controllable generation by making classifier-guided gradient updates to the intermediate latent variables during the diffusion."
        },
        {
            "heading": "3 Method: the Masked-Diffusion LM",
            "text": "In this section, we describe our introduced MaskedDiffusion LM. The overall diagram is shown in Figure 1 and Algorithm 1,2. Different from the recent diffusion models for languages, e.g., Diffusion-LM (Li et al., 2022), which are based on continuous diffusion models, we propose to make corruptions in both discrete and continuous space to help modeling the textual data. Specifically, we formulate a novel corruption process as an alternative to Gaussian diffusion (in Section 3.2) and we directly map continuous vectors to discrete inputs in every diffusion step with cross-entropy objectives (in Section 3.3). Moreover, our approach could easily integrate pre-trained language models (in Section 3.4)."
        },
        {
            "heading": "3.1 Embedding",
            "text": "For the input sentence d with l tokens d = w\u03021:l, we first map the discrete tokens to the continuous space and form the initial latent variable, X0, through a learnable embedding layer or an encoder e(.):\nX0 = w1:l = e(w1:l). (1)\nThis bridges the discrete space and continuous space. We will then add designed soft-masked noise to the tokens\u2019 representations in the later diffusion models."
        },
        {
            "heading": "3.2 Forward Process with Soft-Masking",
            "text": "Different words in sentences play different roles. As a result, when corrupting the sentences and recovering the sentences, words with various importance should be treated differently. Thus, in this work, instead of evenly adding Gaussian noise to all the token embeddings like in Diffusion-LM (Li et al., 2022), we add soft-masked noise to different tokens in the input text in different stages to corrupt the text gradually with structures. Intuitively, more important words would be perturbed with soft-masks in an earlier stage so that the model could be encouraged to generate them in the later phase to follow the easy-first-generation nature of language planning and generation.\nIn this work, we consider the following aspects to measure and define the importance of words in one sentence:\nWord Relevancy We use the tf-idf weights (Dess\u00ed et al., 2020), wtf-idf, of the word as one way to measure the relevance of word w in one sentence d:\nwtf-idf(w, d) = fw,d\u2211\nw\u2032\u2208d fw\u2032,d\nlog N\n1 + |{d \u2208 D : w \u2208 d}| ,\n(2)\nwhere the fw,d is the number of times that word w occurs in sentence d, N is the number of sentences in the corpus, and D is the set of sentences, and |{d \u2208 D : w \u2208 d}| is number of sentences where the word t appears. A higher weight for word w in sentence d in tf\u2013idf means that the word might be more important in the sentence.\nEntropy We also consider measuring the amount of information with entropy H (Bentz and Alikaniotis, 2016; He et al., 2022) in the word w to reflect the importance of that word:\nH(w) = \u2212p (w) log (p (w)) (3)\nwhere p (w) = fw\u2211V j=1 fj represents the probability of word w and f is the word Reluency in the corpus. A word with lower entropy indicates that the word might contain less information and thus be\nless important compared to the words with higher entropy.\nIn practice, we combine these two measures (with normalization) to decide the importance I of the word w in one sentence d by:\nI(w) = xtf-idf(w, d)\u2211\nw\u2032\u2208dwtf-idf(w \u2032, d)\n+ H(w)\u2211\nw\u2032\u2208dH(w \u2032) .\n(4) Based on the introduced importance I of the words in a sentence, we first divide these words into m buckets {W1:m}. The buckets with lower indices include words with higher importance. We will add soft-masked noise to words with higher importance before words with lower importance. By doing this, models could learn to generate the easier words first and then generate harder words in the reversed denoising process for better generation quality. Specifically, at every step t, we will add a small amount of Gaussian noise to the hidden representation of the word wi in bucket W| tm\nT |:\nq(wi,t+1|wi,t) = N(wi,t+1; \u221a (1\u2212 \u03b2t)wi,t, \u03b2tI),\n(5)\nwhere \u03b2t is the amount of noise added at diffusion step t.\nWe further apply a square-root noise schedule following Li et al. (2022) to gradually increase \u03b2t:\n\u03b2t = 1\u2212 \u221a t/T + s, (6)\nAlgorithm 1 Forward Process Input A sentence X = [x0, . . . , xn]. Output Corrupted hidden representations HT = [h0, . . . , hn].\n1: Encode the sentence into hidden representations via an encodere(.): H0 = e(X). 2: for t = 1, . . . ,K do 3: Add soft-masking noise to H based\non the importance of tokens (from higherimportance to lower-importance): Ht+1 = soft-masking(Ht)\n4: end for\nwhere s is a small constant that corresponds to the starting noise level. Thus, less noise would be added to harder words to stabilize the training. By performing the above noising steps, initial latent variable X0 is gradually corrputed to a series of noisy latent variables X1:T ."
        },
        {
            "heading": "3.3 Diffusion Process",
            "text": "After the forward process to corrupt the input tokens in sentences d into latent variables X1:T , we then gradually denoise XT back to X0 through diffusion steps, X\u0302t\u22121 = p(X\u0302t|\u03b8), where \u03b8 is the learned parameter to model the state transition. In practice, we model the transition with Transformers (Vaswani et al., 2017b).\nAfter every diffusion step t \u2208 (0, T ], instead of minimizing the distance between the hidden rep-\nAlgorithm 2 Diffusion Process Input Corrupted hidden representations H = [h0, . . . , hn]. Output A sentence X = [x0, . . . , xn].\n1: Utilize a transition network f(.) to recover the last state: Ht\u22121 = f(Ht) 2: Utilize a linear layers to map hidden representations to actual tokens Xt\u22121 = g(Ht\u22121) 3: Compute the loss Lt and update the transition network. 4: Do the above steps until it recovers the sentence.\nresentations of X\u0302t\u22121 and X0 (Li et al., 2022), we first directly map the continuous space to discrete space using a learnable linear layer f(.) and then minimize a weighted cross entropy between the predicted sentence and (i) the original sentence d and (ii) the masked sentence d\u0302 at time step t\u2212 1:\nLt = \u03b3tCE(f(X\u0302t\u22121), d; \u03b8) + (1\u2212 \u03b3t)CE(f(X\u0302t\u22121), d\u0302; \u03b8), t \u2208 (0, T ]\nHere, \u03b3t = T\u2212tT . In other words, we put higher weights on the masked tokens that are masked in this time step during the forward process and put lower weights to the other tokens. So the models are learned to generate the corresponding masked tokens first at every time step."
        },
        {
            "heading": "3.4 Adapting Pre-trained Language Models",
            "text": "Our introduced Masked-Diffusion LM also allows the use of large pre-trained language model (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020). In this work, we use BERT (Devlin et al., 2019) as an example. To combine the prior knowledge in large language models, it is straightforward to directly replace the embedding layer e(.) with the pre-trained model and use the pre-trained model to get the hidden representations of input tokens as the initial state in diffusion models. We use the final linear layers in pre-trained models to predict the tokens. For efficiency, in our experiments, when using pre-trained models, we freeze the parameters in them and only learn the transition model \u03b8 in our Masked-Diffusion LM."
        },
        {
            "heading": "4 Controllable Text Generation with Masked-Diffusion LM",
            "text": "In this section, we illustrate how we apply our Masked-Diffusion LM to fulfill controllable text generation. Inspired by recent plug-and-play methods (Yang and Klein, 2021; Dathathri et al., 2020; Krause et al., 2021; Liu et al., 2021), we conduct controls c from external modules (e.g., classifiers) directly on the latent variables Xt in every intermediate step t \u2208 [0, T ] in our Masked-Diffusion LM:\np (X0:T | c) = T\u220f t=1 p (Xt\u22121 | Xt, c) . (7)\nWe follow the conditional independence assumption (Yang and Klein, 2021; Dathathri et al., 2020; Krause et al., 2021; Liu et al., 2021) and decompose the above joint probability into a sequence of control task at every time step t:\np (Xt\u22121 | Xt, c) \u221d p (Xt\u22121 | Xt) \u00b7 p(c | Xt\u22121, Xt) = p (Xt\u22121 | Xt) \u00b7 p(c | Xt\u22121).\n(8) As a result, for the t-th step, we run gradient\nupdates on Xt to generate Xt\u22121:\n\u2207Xt\u22121 log p (Xt\u22121 | Xt, c) = \u03bb\u2207Xt\u22121 log p (Xt\u22121 | Xt) +\u2207Xt\u22121 log p (c | Xt\u22121) ,\n(9) where both log p(Xt\u22121|Xt) and log p(c|Xt\u22121) are differentiable: the first term is parametrized by the transition Transformers, \u03b8, in Masked-Diffusion LM, and the second term is parametrized by extra neural network classifiers. Note that the extra classifiers are trained with the diffusion latent variables as input to allow direct gradient updates on the latent space. Note that \u03bb is a fluency regularization hyper-parameter to balance the fluency (gradient updates from Masked-Diffusion LM) and control (gradient updates from classifiers) in order to further improve the generation quality.\nFor the decoding strategy, following Li et al. (2022), the Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004) is used to aggregate and select the sample that has the lowest expected loss under the specified loss function from the MaskedDiffusion LM."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "In this work, we train our Masked-Diffusion LM on the E2E datasets (Novikova et al., 2017), which\nconsists of 50K restaurant reviews together with the labels in terms of food type, price, and customer ratings.\nFollowing Li et al. (2022), we conduct 5 control tasks to evaluate the learned Masked-Diffusion language model:\n\u2022 Semantic Content. For a given field (e.g., food) and value (e.g., Japanese), sentences that covers field=value need to be generated. We evaluate the accuracy of the generated sentence by examine the exact match rate of \u201cvalue\u201d (word mention).\n\u2022 Parts-of-speech. For a given sequence of parts-of-speech (POS) tags (e.g., Noun Verb Determiner Noun), the models need to produce the sentence with the same length and follow the exact given POS tag sequence (e.g., Birds eat the warms). We evaluate the accuracy of the generation by checking the wordlevel POS tag exact match (under an oracle POS tagger).\n\u2022 Syntax Tree. For a given syntactic parse tree, the generated sentence should have the same parse tree. We evaluate the accuracy by first parsing the generated sentence with an off-theshelf parser and report the F1 scores compared to the given parse.\n\u2022 Syntax Spans. For a given (span, syntactic category) pair (e.g., (2, 5, VP)), the parse tree\nof the generated sentence should match the given syntactic category over the given spans. We evaluate the accuracy of the sentence by the exact match rate of the given spans.\n\u2022 Length. For a given target length (e.g., 20), the models need to generate a sentence within \u00b12 of the given target. We evaluate the accuracy by the match rate of the sentence lengths.\nFor every control task, we sample 200 control targets c from the validation splits, and we generate 50 samples for each control target. The first four tasks rely on a classifier to guide the diffusion, and the last one task is classifier free. To further evaluate the fluency of the generated sentences from models, we use a teacher LM (i.e., a carefully fine-tuned GPT-2 model) and report the perplexity of generated text under the teacher LM. A lower perplexity indicates better sample quality and fluency."
        },
        {
            "heading": "5.2 Baselines",
            "text": "We compare our Masked-Diffusion LM with the following state-of-the-art baselines on controllable generation tasks:\n\u2022 PPLM (Dathathri et al., 2020) runs gradient ascent on the pre-trained language models\u2019 hidden representations to increase the classifier probabilities and language model probabilities.\n\u2022 FUDGE (Yang and Klein, 2021) reweights the predicted tokens from the pre-trained language models by a discriminator which takes in a prefix sequence and predicts whether the complete sequence would satisfy the constraint.\n\u2022 Diffusion-LM (Li et al., 2022) learns an embedding to map discrete text into the continuous space where it performs Gaussian\ndiffusion process. Also, a rounding step is designed to map the embeddings back into discrete texts. For every control task, the Diffusion-LM infuses the controlling signals in every diffusion step."
        },
        {
            "heading": "5.3 Experimental Setting",
            "text": "We use a Transformer with 80M parameters to parameterize our Masked-Diffusion LM, with a sequence length n = 64, diffusion steps T = 500, and a square-root noise schedule. For MaskedDiffusion LM, we set the hidden dimension to 128. We set the number of word buckets m = 3. When combining pre-trained models, we incorporate BERT-base (Devlin et al., 2019) with about 110M parameters. We use BERT to encode the input text into vectors with dimension of 768 and freeze the parameters in BERT. We learn Masked-Diffusion LM with the AdamW optimizer (Loshchilov and Hutter, 2019) for 20,000 steps with learning rate of 3e-4, dropout probability of 0.1, and batch size of 32. We use a linear warmup schedule starting with 1,000 warmup steps. All experiments are conducted on NVIDIA A100 Tensor Core GPUs. We use 4 GPUs for training and a single GPU for sampling."
        },
        {
            "heading": "5.4 Results",
            "text": "We show the main results on five controllable generation tasks in Table 1. When the diffusion process is engaged, the performances on all the controlled generation tasks receives significant boosts (e.g., 81.2 of Diffusion-LM vs. 69.9 if FUDUGE on Semantic Content task), suggesting the superiority of the diffusion model on controllable generation tasks. While the previous Diffusion-LM can not be well combined with large language model like BERT (e.g., a 5% drop on Semantic Content accuracy), largely due to the fact that their way (rounding) to bridge continuous space and discrete space suffers from significantly higher dimensions. Compared to Diffusion-LM, our proposed MaskedDiffusion LM consistently outperforms the previous models in all tasks (e.g., a 1.7% improvement on the POS task), indicating the effectiveness of our introduced linguistic-informed noise forward process. Also, when combined with large language models like BERT, our method significantly outperforms the previous methods, demonstrating that our approach can be well aligned with pre-trained models.\nEfficiency We also display the training cost and inference cost in Table 2. Compared to the previous Diffusion-LM, our method requires significantly less training time to converge and needs less inference time to generate sentences. This is because our introduced noise process is more stable and suitable for modeling languages. Besides, the objectives we introduced are more efficient than the rounding techniques in previous work.\nHuman Evaluation We then conduct human evaluation to evaluate the generated conversations qualitatively. We ask native speakers of English from Amazon Mechanical Turk to rank the quality of 50 generated sentences (randomly sampled) from different models for every control task. Specifically, annotators need to rank different system outputs based on the (i) fluency (whether the\ngiven sentence is readable and fluent) and (ii) the controllability (whether the given sentence match the given control conditions). To increase annotation quality, we require turkers to have a 98% approval rate with over 10,000 approved tasks for their previous work. The pay rate was $0.15 per hit. Every example is assessed by 3 annotators, and the rank for every sentence is aggregated by majority voting. The Intra-Class Correlation (ICC1k) was 0.63, indicating moderate agreement (Koo and Li, 2016). The results are shown in Table 3. As it shows, our proposed Masked-Diffusion LM and its variation with BERT received the best average ranks, suggesting the effectiveness of our proposed diffusion modeling strategy for languages."
        },
        {
            "heading": "5.5 Ablation Studies",
            "text": "We then perform ablation studies to demonstrate the effectiveness of our introduced linguisticinformed noise and the cross entropy objectives.\nNoise Strategy We first demonstrate the performances on Semantic Content task of MaskedDiffusion LM with different types of noise strategy in Table 4. Gaussian adds Gaussian noise to all the tokens in the input sentence in the forward process following Li et al. (2022). We also compare different masking noise strategies: (i) Random Mask, where the soft-mask is added to tokens in a random\norder. (ii) Mask with POS, where the soft-mask perturbs the tokens in an order (noun \u2192 verb \u2192 other words) based on POS tags. Our introduced noise strategy (Mask with Entropy and Reluency) shows significantly better performances on semantic content generation. This indicates that our introduced noise strategy that considers the linguistic features in sentences is providing more appropriate perturbation to the textual data for the diffusion process.\nObjectives We further show the impact of different objectives in Table 5. We compare our used cross entropy objectives with the L2 object that is used in Li et al. (2022) where they minimize the distance between latent intermediate variables and the initial latent variable instead of directly predicting the text. We observe that cross entropy objectives slightly perform better than L2 when the pre-trained model is not used. After combining with large language models, CE-BERT significantly outperforms the L2-BERT, indicating the effectiveness of our introduced objectives in terms of incorporating large language models."
        },
        {
            "heading": "5.6 Case Studies",
            "text": "We also include some examples of intermediate steps of Masked-Diffusion LM in Table 6. In the denoising diffusion process, easy words are generated first. For example, \u201cis\u201d, \u201can\u201d, and \u201crestaurant\u201d.\nWith more diffusion steps, sentences are enriched with more informative words such as \u201cMill\u201d and \u201cIndian\u201d. It shows that our Masked-Diffusion LM encourages the generation to follow an easy-first order for stable and better generation quality."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we present a novel diffusion model for language, Masked-Diffusion LM, which corrupts the discrete text with a linguistic-informed softmasking strategy and then iteratively denoises them back by directly predicting the text. Specifically, we gradually soft-mask the tokens in the sentence following an order from more informative words to less informative words in the forward process. This satisfies the flexibility for diffusion models, as well as encourages the easy-first-generation nature in the denoising process for better generation quality. Also, we directly predict the discrete token during the diffusion process with the cross-entropy loss to stabilize the intermediate diffusion steps and make our approach orthogonal to large pre-trained language models. Experiments on E2E dataset and five controllable generation tasks including Semantic Content, Parts-of-speech, Syntax Tree, Syntax Spans, and Length show that our MaskedDiffusion LM can (i) achieve the state-of-the-art performances compared to recent baseline models and (ii) allow more efficient training and inference compared to the previous Diffusion-LM."
        },
        {
            "heading": "7 Limitations",
            "text": "In this work, we mainly leverage linguistic softmasking such as word relevancy and word entropy. We encourage future work to explore how to incorporate other linguistic structures to design the nosing process. And we mainly test with smaller models like simple transformer models as well as BERT-based models. Future work might test with larger pre-trained models to evaluate whether diffusion methods would work better or not. Also, we focused on controllable generation to evaluate the models. Future work may study different downstream tasks."
        }
    ],
    "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
    "year": 2023
}