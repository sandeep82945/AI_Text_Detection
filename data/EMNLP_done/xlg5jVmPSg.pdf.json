{
    "abstractText": "Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziqiao Ma"
        },
        {
            "affiliations": [],
            "name": "Jacob Sansom"
        },
        {
            "affiliations": [],
            "name": "Run Peng"
        },
        {
            "affiliations": [],
            "name": "Joyce Chai"
        }
    ],
    "id": "SP:feb2f1a059be9af1642f7a56385a987f4f5420a6",
    "references": [
        {
            "authors": [
                "Arjun R Akula",
                "Keze Wang",
                "Changsong Liu",
                "Sari SabaSadiya",
                "Hongjing Lu",
                "Sinisa Todorovic",
                "Joyce Chai",
                "Song-Chun Zhu."
            ],
            "title": "Cx-tom: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models",
            "venue": "Iscience,",
            "year": 2022
        },
        {
            "authors": [
                "Stefano V Albrecht",
                "Peter Stone"
            ],
            "title": "Autonomous agents modelling other agents: A comprehensive",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Language models as agent models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769\u20135779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "James N Aronson",
                "Claire Golomb."
            ],
            "title": "Preschoolers\u2019 understanding of pretense and presumption of congruity between action and representation",
            "venue": "Developmental Psychology, 35(6):1414.",
            "year": 1999
        },
        {
            "authors": [
                "Jaan Aru",
                "Aqeel Labash",
                "Oriol Corcoll",
                "Raul Vicente."
            ],
            "title": "Mind the gap: Challenges of deep learning approaches to theory of mind",
            "venue": "Artificial Intelligence Review, pages 1\u201316.",
            "year": 2023
        },
        {
            "authors": [
                "Yuwei Bao",
                "Sayan Ghosh",
                "Joyce Chai."
            ],
            "title": "Learning to mediate disparities towards pragmatic communication",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL).",
            "year": 2022
        },
        {
            "authors": [
                "Cristian-Paul Bara",
                "Ziqiao Ma",
                "Yingzhuo Yu",
                "Julie Shah",
                "Joyce Chai."
            ],
            "title": "Towards collaborative plan acquisition through theory of mind modeling in situated dialogue",
            "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intel-",
            "year": 2023
        },
        {
            "authors": [
                "Cristian-Paul Bara",
                "CH-Wang Sky",
                "Joyce Chai."
            ],
            "title": "Mindcraft: Theory of mind modeling for situated dialogue in collaborative tasks",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1112\u20131125.",
            "year": 2021
        },
        {
            "authors": [
                "Simon Baron-Cohen."
            ],
            "title": "Mindblindness: An essay on autism and theory of mind",
            "venue": "MIT press.",
            "year": 1997
        },
        {
            "authors": [
                "Simon Baron-Cohen",
                "Alan M Leslie",
                "Uta Frith"
            ],
            "title": "Does the autistic child have a \u201ctheory of mind\u201d? Cognition, 21(1):37\u201346",
            "year": 1985
        },
        {
            "authors": [
                "Simon Baron-Cohen",
                "Michelle O\u2019riordan",
                "Valerie Stone",
                "Rosie Jones",
                "Kate Plaisted"
            ],
            "title": "Recognition of faux pas by normally developing children and children with asperger syndrome or high-functioning autism",
            "venue": "Journal of autism and developmental disor-",
            "year": 1999
        },
        {
            "authors": [
                "Cindy Beaudoin",
                "\u00c9lizabel Leblanc",
                "Charlotte Gagner",
                "Miriam H Beauchamp."
            ],
            "title": "Systematic review and inventory of theory of mind measures for young children",
            "venue": "Frontiers in psychology, 10:2905.",
            "year": 2020
        },
        {
            "authors": [
                "Mark Bennett",
                "Linda Galpert."
            ],
            "title": "Children\u2019s understanding of multiple desires",
            "venue": "International Journal of Behavioral Development, 16(1):15\u201333.",
            "year": 1993
        },
        {
            "authors": [
                "Sarah-Jayne Blakemore",
                "Jean Decety."
            ],
            "title": "From the perception of action to the understanding of intention",
            "venue": "Nature reviews neuroscience, 2(8):561\u2013567.",
            "year": 2001
        },
        {
            "authors": [
                "Paul Bloom."
            ],
            "title": "How children learn the meanings of words",
            "venue": "MIT press.",
            "year": 2002
        },
        {
            "authors": [
                "Helene Borke"
            ],
            "title": "Interpersonal perception of young children: Egocentrism or empathy",
            "venue": "Developmental psychology,",
            "year": 1971
        },
        {
            "authors": [
                "Michael Bratman."
            ],
            "title": "Intention, plans, and practical reason",
            "venue": "The University of Chicago Press.",
            "year": 1987
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Peter Carruthers."
            ],
            "title": "Perceiving mental states",
            "venue": "Consciousness and cognition, 36:498\u2013507.",
            "year": 2015
        },
        {
            "authors": [
                "Fulvia Castelli."
            ],
            "title": "The valley task: Understanding intention from goal-directed motion in typical development and autism",
            "venue": "British journal of developmental psychology, 24(4):655\u2013668.",
            "year": 2006
        },
        {
            "authors": [
                "Arjun Chandrasekaran",
                "Deshraj Yadav",
                "Prithvijit Chattopadhyay",
                "Viraj Prabhu",
                "Devi Parikh."
            ],
            "title": "It takes two to tango: Towards theory of ai\u2019s mind",
            "venue": "arXiv preprint arXiv:1704.00717.",
            "year": 2017
        },
        {
            "authors": [
                "Maxime Chevalier-Boisvert",
                "Lucas Willems",
                "Suman Pal"
            ],
            "title": "Minimalistic gridworld environment for gymnasium",
            "year": 2018
        },
        {
            "authors": [
                "Michael Cohen"
            ],
            "title": "Exploring roberta\u2019s theory of mind through textual entailment",
            "year": 2021
        },
        {
            "authors": [
                "Philip R Cohen",
                "Hector J Levesque."
            ],
            "title": "Intention is choice with commitment",
            "venue": "Artificial intelligence, 42(2-3):213\u2013261.",
            "year": 1990
        },
        {
            "authors": [
                "Cristina Colonnesi",
                "Carolien Rieffe",
                "Willem Koops",
                "Paola Perucchini."
            ],
            "title": "Precursors of a theory of mind: A longitudinal study",
            "venue": "British Journal of Developmental Psychology, 26(4):561\u2013577.",
            "year": 2008
        },
        {
            "authors": [
                "Antonio R Damasio."
            ],
            "title": "Emotions and feelings",
            "venue": "Feelings and emotions: The Amsterdam symposium, volume 5, pages 49\u201357. Cambridge University Press Cambridge.",
            "year": 2004
        },
        {
            "authors": [
                "Susanne A Denham."
            ],
            "title": "Social cognition, prosocial behavior, and emotion in preschoolers: Contextual validation",
            "venue": "Child development, pages 194\u2013201.",
            "year": 1986
        },
        {
            "authors": [
                "Daniel Dennett."
            ],
            "title": "Do animals have beliefs",
            "venue": "Comparative approaches to cognitive science, 111.",
            "year": 1995
        },
        {
            "authors": [
                "Daniel C Dennett."
            ],
            "title": "Pr\u00e9cis of the intentional stance",
            "venue": "Behavioral and brain sciences, 11(3):495\u2013505.",
            "year": 1988
        },
        {
            "authors": [
                "Jesse Dodge",
                "Maarten Sap",
                "Ana Marasovi\u0107",
                "William Agnew",
                "Gabriel Ilharco",
                "Dirk Groeneveld",
                "Margaret Mitchell",
                "Matt Gardner."
            ],
            "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
            "venue": "Proceedings of the 2021",
            "year": 2021
        },
        {
            "authors": [
                "Fred I Dretske."
            ],
            "title": "Simple seeing",
            "venue": "Body, mind, and method, pages 1\u201315. Springer.",
            "year": 1979
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ximing Lu",
                "Melanie Sclar",
                "Xiang Lorraine Li",
                "Liwei Jian",
                "Bill Yuchen Lin",
                "Peter West",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jena D Hwang"
            ],
            "title": "Faith and fate: Limits of transformers on compositionality",
            "venue": "arXiv preprint arXiv:2305.18654",
            "year": 2023
        },
        {
            "authors": [
                "Jacquelynne S Eccles",
                "Allan Wigfield."
            ],
            "title": "Motivational beliefs, values, and goals",
            "venue": "Annual review of psychology, 53(1):109\u2013132.",
            "year": 2002
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Carl Vondrick",
                "Antonio Torralba"
            ],
            "title": "Who is mistaken? arXiv preprint arXiv:1612.01175",
            "year": 2016
        },
        {
            "authors": [
                "Nico H Frijda"
            ],
            "title": "The emotions",
            "venue": "Cambridge University Press.",
            "year": 1986
        },
        {
            "authors": [
                "Kanishk Gandhi",
                "Gala Stojnic",
                "Brenden M Lake",
                "Moira R Dillon."
            ],
            "title": "Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others",
            "venue": "Advances in Neural Information Processing Systems, 34:9963\u20139976.",
            "year": 2021
        },
        {
            "authors": [
                "Edmund L Gettier"
            ],
            "title": "Is justified true belief knowledge",
            "venue": "Causal Theories of Mind,",
            "year": 2000
        },
        {
            "authors": [
                "Rachel Giora."
            ],
            "title": "On our mind: Salience, context, and figurative language",
            "venue": "Oxford University Press.",
            "year": 2003
        },
        {
            "authors": [
                "Alison Gopnik",
                "Henry M Wellman."
            ],
            "title": "Why the child\u2019s theory of mind really is a theory",
            "venue": "Mind & Language, 7(1\u20132):145\u2013171.",
            "year": 1992
        },
        {
            "authors": [
                "Andrew Gordon."
            ],
            "title": "Commonsense interpretation of triangle behavior",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 30.",
            "year": 2016
        },
        {
            "authors": [
                "Erin Grant",
                "Aida Nematzadeh",
                "Thomas L Griffiths"
            ],
            "title": "How can memory-augmented neural networks pass a false-belief task? In CogSci",
            "year": 2017
        },
        {
            "authors": [
                "David Gunning."
            ],
            "title": "Machine common sense concept paper",
            "venue": "arXiv preprint arXiv:1810.07528.",
            "year": 2018
        },
        {
            "authors": [
                "Julie Hadwin",
                "Simon Baron-Cohen",
                "Patricia Howlin",
                "Katie Hill"
            ],
            "title": "Does teaching theory of mind have an effect on the ability to develop conversation in children with autism",
            "venue": "Journal of autism and developmental disorders,",
            "year": 1997
        },
        {
            "authors": [
                "Thilo Hagendorff"
            ],
            "title": "Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods",
            "year": 2023
        },
        {
            "authors": [
                "Francesca GE Happ\u00e9."
            ],
            "title": "An advanced test of theory of mind: Understanding of story characters\u2019 thoughts and feelings by able autistic, mentally handicapped, and normal children and adults",
            "venue": "Journal of autism and Developmental disorders, 24(2):129\u2013154.",
            "year": 1994
        },
        {
            "authors": [
                "Paul L Harris",
                "Kara Donnelly",
                "Gabrielle R Guz",
                "Rosemary Pitt-Watson."
            ],
            "title": "Children\u2019s understanding of the distinction between real and apparent emotion",
            "venue": "Child development, pages 895\u2013909.",
            "year": 1986
        },
        {
            "authors": [
                "Peter Hase",
                "Mona Diab",
                "Asli Celikyilmaz",
                "Xian Li",
                "Zornitsa Kozareva",
                "Veselin Stoyanov",
                "Mohit Bansal",
                "Srinivasan Iyer."
            ],
            "title": "Methods for measuring, updating, and visualizing factual beliefs in language models",
            "venue": "Proceedings of the 17th Conference of",
            "year": 2023
        },
        {
            "authors": [
                "Yinghui He",
                "Yufan Wu",
                "Yilin Jia",
                "Rada Mihalcea",
                "Yulong Chen",
                "Naihao Deng."
            ],
            "title": "Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2023
        },
        {
            "authors": [
                "Mark K Ho",
                "Rebecca Saxe",
                "Fiery Cushman."
            ],
            "title": "Planning with theory of mind",
            "venue": "Trends in Cognitive Sciences.",
            "year": 2022
        },
        {
            "authors": [
                "Bart Holterman",
                "Kees van Deemter"
            ],
            "title": "Does chatgpt have theory of mind? arXiv preprint arXiv:2305.14020",
            "year": 2023
        },
        {
            "authors": [
                "Jennifer Hu",
                "Sammy Floyd",
                "Olessia Jouravlev",
                "Evelina Fedorenko",
                "Edward Gibson."
            ],
            "title": "A finegrained comparison of pragmatic language understanding in humans and language models",
            "venue": "arXiv preprint arXiv:2212.06801.",
            "year": 2022
        },
        {
            "authors": [
                "Unnat Jain",
                "Luca Weihs",
                "Eric Kolve",
                "Mohammad Rastegari",
                "Svetlana Lazebnik",
                "Ali Farhadi",
                "Alexander G Schwing",
                "Aniruddha Kembhavi."
            ],
            "title": "Two body problem: Collaborative visual task completion",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2019
        },
        {
            "authors": [
                "Ao Jia",
                "Yu He",
                "Yazhou Zhang",
                "Sagar Uprety",
                "Dawei Song",
                "Christina Lioma."
            ],
            "title": "Beyond emotion: A multi-modal dataset for human desire understanding",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "David J Kavanagh",
                "Jackie Andrade",
                "Jon May."
            ],
            "title": "Imaginary relish and exquisite torture: the elaborated intrusion theory of desire",
            "venue": "Psychological review, 112(2):446.",
            "year": 2005
        },
        {
            "authors": [
                "Casey Kennington."
            ],
            "title": "Understanding intention for machine theory of mind: a position paper",
            "venue": "2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), pages 450\u2013453. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Ariel Knafo",
                "Carolyn Zahn-Waxler",
                "Maayan Davidov",
                "Carol Van Hulle",
                "JoAnn L Robinson",
                "Soo Hyun Rhee."
            ],
            "title": "Empathy in early childhood: Genetic, environmental, and affective contributions",
            "venue": "Annals of the New York Academy of Sciences, 1167(1):103\u2013",
            "year": 2009
        },
        {
            "authors": [
                "Michal Kosinski"
            ],
            "title": "Theory of mind may have spontaneously emerged in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Nicole C Kr\u00e4mer",
                "Astrid von der P\u00fctten",
                "Sabrina Eimler."
            ],
            "title": "Human-agent and human-robot interaction theory: Similarities to and differences from human-human interaction",
            "venue": "Human-computer interaction: The agency perspective, pages 215\u2013240.",
            "year": 2012
        },
        {
            "authors": [
                "Matthew Le",
                "Y-Lan Boureau",
                "Maximilian Nickel."
            ],
            "title": "Revisiting the evaluation of theory of mind through question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Courtland Leer",
                "Vincent Trost",
                "Vineeth Voruganti."
            ],
            "title": "Violation of expectation via metacognitive prompting reduces theory of mind prediction error in large language models",
            "venue": "arXiv preprint arXiv:2310.06983.",
            "year": 2023
        },
        {
            "authors": [
                "Belinda Z Li",
                "Maxwell Nye",
                "Jacob Andreas."
            ],
            "title": "Implicit representations of meaning in neural language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Guohao Li",
                "Hasan Abed Al Kader Hammoud",
                "Hani Itani",
                "Dmitrii Khizbullin",
                "Bernard Ghanem."
            ],
            "title": "Camel: Communicative agents for\" mind\" exploration of large scale language model society",
            "venue": "arXiv preprint arXiv:2303.17760.",
            "year": 2023
        },
        {
            "authors": [
                "Shuang Li",
                "Xavier Puig",
                "Chris Paxton",
                "Yilun Du",
                "Clinton Wang",
                "Linxi Fan",
                "Tao Chen",
                "De-An Huang",
                "Ekin Aky\u00fcrek",
                "Anima Anandkumar"
            ],
            "title": "Pretrained language models for interactive decisionmaking",
            "venue": "Advances in Neural Information Processing",
            "year": 2022
        },
        {
            "authors": [
                "Andy Liu",
                "Hao Zhu",
                "Emmy Liu",
                "Yonatan Bisk",
                "Graham Neubig."
            ],
            "title": "Computational language acquisition with theory of mind",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Inbal Magar",
                "Roy Schwartz."
            ],
            "title": "Data contamination: From memorization to exploitation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165.",
            "year": 2022
        },
        {
            "authors": [
                "Bertram F Malle",
                "Joshua Knobe."
            ],
            "title": "The distinction between desire and intention: A folk-conceptual analysis",
            "venue": "Intentions and intentionality: Foundations of social cognition, 45:67.",
            "year": 2001
        },
        {
            "authors": [
                "Gary Marcus",
                "Ernest Davis"
            ],
            "title": "How not to test gpt-3",
            "year": 2023
        },
        {
            "authors": [
                "Zenaida S Masangkay",
                "Kathleen A McCluskey",
                "Curtis W McIntyre",
                "Judith Sims-Knight",
                "Brian E Vaughn",
                "John H Flavell."
            ],
            "title": "The early development of inferences about the visual percepts of others",
            "venue": "Child development, pages 357\u2013366.",
            "year": 1974
        },
        {
            "authors": [
                "Andrew N Meltzoff."
            ],
            "title": "Understanding the intentions of others: re-enactment of intended acts by 18-monthold children",
            "venue": "Developmental psychology, 31(5):838.",
            "year": 1995
        },
        {
            "authors": [
                "Melanie Mitchell",
                "David C Krakauer."
            ],
            "title": "The debate over understanding in ai\u2019s large language models",
            "venue": "Proceedings of the National Academy of Sciences, 120(13):e2215907120.",
            "year": 2023
        },
        {
            "authors": [
                "Shima Rahimi Moghaddam",
                "Christopher J Honey."
            ],
            "title": "Boosting theory-of-mind performance in large language models via prompting",
            "venue": "arXiv preprint arXiv:2304.11490.",
            "year": 2023
        },
        {
            "authors": [
                "Henrike Moll",
                "Cornelia Koring",
                "Malinda Carpenter",
                "Michael Tomasello."
            ],
            "title": "Infants determine others\u2019 focus of attention by pragmatics and exclusion",
            "venue": "Journal of Cognition and Development, 7(3):411\u2013430.",
            "year": 2006
        },
        {
            "authors": [
                "Aida Nematzadeh",
                "Kaylee Burns",
                "Erin Grant",
                "Alison Gopnik",
                "Tom Griffiths."
            ],
            "title": "Evaluating theory of mind in question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392\u20132400, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Osterhaus",
                "Sandra L Bosacki."
            ],
            "title": "Looking for the lighthouse: A systematic review of advanced theory-of-mind tests beyond preschool",
            "venue": "Developmental Review, 64:101021.",
            "year": 2022
        },
        {
            "authors": [
                "Gon\u00e7alo Pereira",
                "Rui Prada",
                "Pedro A Santos."
            ],
            "title": "Integrating social power into the decision-making of cognitive agents",
            "venue": "Artificial Intelligence, 241:1\u201344.",
            "year": 2016
        },
        {
            "authors": [
                "Josef Perner",
                "Susan R Leekam",
                "Heinz Wimmer."
            ],
            "title": "Three-year-olds\u2019 difficulty with false belief: The case for a conceptual deficit",
            "venue": "British journal of developmental psychology, 5(2):125\u2013137.",
            "year": 1987
        },
        {
            "authors": [
                "Josef Perner",
                "Heinz Wimmer."
            ],
            "title": "john thinks that mary thinks that",
            "venue": ". . \u201d attribution of second-order beliefs by 5-to 10-year-old children. Journal of experimental child psychology, 39(3):437\u2013471.",
            "year": 1985
        },
        {
            "authors": [
                "Ann T Phillips",
                "Henry M Wellman",
                "Elizabeth S Spelke."
            ],
            "title": "Infants\u2019 ability to connect gaze and emotional expression to intentional action",
            "venue": "Cognition, 85(1):53\u201378.",
            "year": 2002
        },
        {
            "authors": [
                "F. Pons",
                "P. Harris."
            ],
            "title": "Test of Emotion Comprehension: TEC",
            "venue": "University of Oxford.",
            "year": 2000
        },
        {
            "authors": [
                "David Premack",
                "Guy Woodruff"
            ],
            "title": "Does the chimpanzee have a theory of mind? Behavioral and Brain Sciences, 1(4):515\u2013526",
            "year": 1978
        },
        {
            "authors": [
                "Liang Qiu",
                "Yizhou Zhao",
                "Yuan Liang",
                "Pan Lu",
                "Weiyan Shi",
                "Zhou Yu",
                "Song-chun Zhu."
            ],
            "title": "Towards socially intelligent agents with mental state transition and human value",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse",
            "year": 2022
        },
        {
            "authors": [
                "Neil Rabinowitz",
                "Frank Perbet",
                "Francis Song",
                "Chiyuan Zhang",
                "SM Ali Eslami",
                "Matthew Botvinick."
            ],
            "title": "Machine theory of mind",
            "venue": "International conference on machine learning, pages 4218\u20134227. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Inioluwa Deborah Raji",
                "Emily Denton",
                "Emily M Bender",
                "Alex Hanna",
                "Amandalynne Paullada."
            ],
            "title": "Ai and the everything in the whole wide world benchmark",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Betty M Repacholi",
                "Alison Gopnik."
            ],
            "title": "Early reasoning about desires: evidence from 14-and 18month-olds",
            "venue": "Developmental psychology, 33(1):12.",
            "year": 1997
        },
        {
            "authors": [
                "Ted K Ruffman",
                "David R Olson."
            ],
            "title": "Children\u2019s ascriptions of knowledge to others",
            "venue": "Developmental Psychology, 25(4):601.",
            "year": 1989
        },
        {
            "authors": [
                "Laura Ruis",
                "Akbir Khan",
                "Stella Biderman",
                "Sara Hooker",
                "Tim Rockt\u00e4schel",
                "Edward Grefenstette."
            ],
            "title": "Large language models are not zero-shot communicators",
            "venue": "arXiv preprint arXiv:2210.14986.",
            "year": 2022
        },
        {
            "authors": [
                "Tessa Rusch",
                "Saurabh Steixner-Kumar",
                "Prashant Doshi",
                "Michael Spezio",
                "Jan Gl\u00e4scher."
            ],
            "title": "Theory of mind and decision science: towards a typology of tasks and computational models",
            "venue": "Neuropsychologia, 146:107488.",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Daniel Fried",
                "Yejin Choi."
            ],
            "title": "Neural theory-of-mind? on the limits of social intelligence in large LMs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762\u20133780, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social IQa: Commonsense reasoning about social interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Roger C Schank",
                "Robert P Abelson."
            ],
            "title": "Scripts, plans, goals, and understanding: An inquiry into human knowledge structures",
            "venue": "Psychology press.",
            "year": 2013
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Melanie Sclar",
                "Sachin Kumar",
                "Peter West",
                "Alane Suhr",
                "Yejin Choi",
                "Yulia Tsvetkov"
            ],
            "title": "Minding language models\u2019 (lack of) theory of mind: A plug-andplay multi-character belief tracker",
            "year": 2023
        },
        {
            "authors": [
                "Melanie Sclar",
                "Graham Neubig",
                "Yonatan Bisk."
            ],
            "title": "Symmetric machine theory of mind",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162, pages 19450\u201319466.",
            "year": 2022
        },
        {
            "authors": [
                "Natalie Shapira",
                "Mosh Levy",
                "Seyed Hossein Alavi",
                "Xuhui Zhou",
                "Yejin Choi",
                "Yoav Goldberg",
                "Maarten Sap",
                "Vered Shwartz"
            ],
            "title": "Clever hans or neural theory of mind? stress testing social reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Natalie Shapira",
                "Guy Zwirn",
                "Yoav Goldberg."
            ],
            "title": "How well do large language models perform on faux pas tests",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Tianmin Shu",
                "Abhishek Bhandwaldar",
                "Chuang Gan",
                "Kevin Smith",
                "Shari Liu",
                "Dan Gutfreund",
                "Elizabeth Spelke",
                "Joshua Tenenbaum",
                "Tomer Ullman."
            ],
            "title": "Agent: A benchmark for core psychological reasoning",
            "venue": "International Conference on Machine Learn-",
            "year": 2021
        },
        {
            "authors": [
                "Damien Sileo",
                "Antoine Lernould."
            ],
            "title": "Mindgames: Targeting theory of mind in large language models with dynamic epistemic modal logic",
            "venue": "arXiv preprint arXiv:2305.03353.",
            "year": 2023
        },
        {
            "authors": [
                "Patricia A Smiley."
            ],
            "title": "Intention understanding and partner-sensitive behaviors in young children\u2019s peer interactions",
            "venue": "Social Development, 10(3):330\u2013354.",
            "year": 2001
        },
        {
            "authors": [
                "Caoimhe Harrington Stack",
                "Effat Farhana",
                "Xinyu Shen",
                "Simeng Zhao",
                "Angela Maliakal."
            ],
            "title": "Framework for a multi-dimensional test of theory of mind for humans and ai systems",
            "venue": "The Tenth Annual Conference on Advances in Cognitive Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Shane Storks",
                "Qiaozi Gao",
                "Yichi Zhang",
                "Joyce Chai."
            ],
            "title": "Tiered reasoning for intuitive physics: Toward verifiable commonsense language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4902\u20134918.",
            "year": 2021
        },
        {
            "authors": [
                "Kate Sullivan",
                "Ellen Winner",
                "Natalie Hopfield."
            ],
            "title": "How children tell a lie from a joke: The role of second-order mental state attributions",
            "venue": "British journal of developmental psychology, 13(2):191\u2013204.",
            "year": 1995
        },
        {
            "authors": [
                "J Swettenham"
            ],
            "title": "Can children be taught to understand false belief using computers? child psychology & psychiatry",
            "venue": "allied disciplines,",
            "year": 1996
        },
        {
            "authors": [
                "Ece Takmaz",
                "Nicolo\u2019 Brandizzi",
                "Mario Giulianelli",
                "Sandro Pezzelle",
                "Raquel Fern\u00e1ndez"
            ],
            "title": "Speaking the language of your listener: Audience-aware adaptation via plug-and-play theory of mind",
            "venue": "In Findings of the Association",
            "year": 2023
        },
        {
            "authors": [
                "Michael Tomasello."
            ],
            "title": "Constructing a language: A usage-based theory of language acquisition",
            "venue": "Harvard university press.",
            "year": 2005
        },
        {
            "authors": [
                "Jennifer Tracey",
                "Owen Rambow",
                "Claire Cardie",
                "Adam Dalton",
                "Hoa Trang Dang",
                "Mona Diab",
                "Bonnie Dorr",
                "Louise Guthrie",
                "Magdalena Markowska",
                "Smaranda Muresan"
            ],
            "title": "2022. Best: The belief and sentiment corpus",
            "venue": "In Proceedings of the Thirteenth Language",
            "year": 2022
        },
        {
            "authors": [
                "Sean Trott",
                "Cameron Jones",
                "Tyler Chang",
                "James Michaelov",
                "Benjamin Bergen"
            ],
            "title": "Do large language models know what humans know? arXiv preprint arXiv:2209.01515",
            "year": 2022
        },
        {
            "authors": [
                "Tomer Ullman"
            ],
            "title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "year": 2023
        },
        {
            "authors": [
                "Qiaosi Wang",
                "Koustuv Saha",
                "Eric Gregori",
                "David Joyner",
                "Ashok Goel."
            ],
            "title": "Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant",
            "venue": "Proceedings of the 2021 CHI Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Henry M Wellman",
                "Jacqueline D Woolley."
            ],
            "title": "From simple desires to ordinary beliefs: The early development of everyday psychology",
            "venue": "Cognition, 35(3):245\u2013275.",
            "year": 1990
        },
        {
            "authors": [
                "Jincenzi Wu",
                "Zhuang Chen",
                "Jiawen Deng",
                "Sahand Sabour",
                "Minlie Huang."
            ],
            "title": "Coke: A cognitive knowledge graph for machine theory of mind",
            "venue": "arXiv preprint arXiv:2305.05390.",
            "year": 2023
        },
        {
            "authors": [
                "Wako Yoshida",
                "Ray J Dolan",
                "Karl J Friston."
            ],
            "title": "Game theory of mind",
            "venue": "PLoS computational biology, 4(12):e1000254.",
            "year": 2008
        },
        {
            "authors": [
                "Mo Yu",
                "Yisi Sang",
                "Kangsheng Pu",
                "Zekai Wei",
                "Han Wang",
                "Jing Li",
                "Yue Yu",
                "Jie Zhou."
            ],
            "title": "Few-shot character understanding in movies as an assessment to meta-learning of theory-of-mind",
            "venue": "arXiv preprint arXiv:2211.04684.",
            "year": 2022
        },
        {
            "authors": [
                "Jamil Zaki",
                "Niall Bolger",
                "Kevin Ochsner."
            ],
            "title": "Unpacking the informational bases of empathic accuracy",
            "venue": "Emotion, 9(4):478.",
            "year": 2009
        },
        {
            "authors": [
                "Chen Zhang",
                "Joyce Y Chai."
            ],
            "title": "Towards conversation entailment: An empirical investigation",
            "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756\u2013 766.",
            "year": 2010
        },
        {
            "authors": [
                "Haode Zhang",
                "Yuwei Zhang",
                "Li Ming Zhan",
                "Jiaxin Chen",
                "Guangyuan Shi",
                "Xiao Ming Wu",
                "Albert YS Lam."
            ],
            "title": "Effectiveness of pre-training for few-shot intent classification",
            "venue": "2021 Findings of the Association for Computational Linguistics, Findings of",
            "year": 2021
        },
        {
            "authors": [
                "Pei Zhou",
                "Aman Madaan",
                "Srividya Pranavi Potharaju",
                "Aditya Gupta",
                "Kevin R McKee",
                "Ari Holtzman",
                "Jay Pujara",
                "Xiang Ren",
                "Swaroop Mishra",
                "Aida Nematzadeh"
            ],
            "title": "2023a. How far are large language models from agents with theory-of-mind",
            "year": 2023
        },
        {
            "authors": [
                "Pei Zhou",
                "Andrew Zhu",
                "Jennifer Hu",
                "Jay Pujara",
                "Xiang Ren",
                "Chris Callison-Burch",
                "Yejin Choi",
                "Prithviraj Ammanabrolu."
            ],
            "title": "I cast detect thoughts: Learning to converse and guide with intents and theoryof-mind in dungeons and dragons",
            "venue": "Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Hao Zhu",
                "Graham Neubig",
                "Yonatan Bisk."
            ],
            "title": "Few-shot language coordination by modeling theory of mind",
            "venue": "International Conference on Machine Learning, pages 12901\u201312911. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhu",
                "Graham Neubig",
                "Yonatan Bisk."
            ],
            "title": "Few-shot language coordination by modeling theory of mind",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages",
            "year": 2021
        },
        {
            "authors": [
                "Beaudoin"
            ],
            "title": "2020)\u2019s category of non-literal",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The term theory of mind (ToM, sometimes also referred to as mentalization or mindreading) was first introduced by Premack and Woodruff (1978) as agents\u2019 ability to impute mental states to themselves and others. Many aspects of human cognition and social reasoning rely on ToM modeling of others\u2019 mental states (Gopnik and Wellman, 1992; Baron-Cohen, 1997; Gunning, 2018). This is crucial for understanding and predicting others\u2019 actions (Dennett, 1988), planning over others\u2019 beliefs and next actions (Ho et al., 2022), and various\nforms of reasoning and decision-making (Pereira et al., 2016; Rusch et al., 2020). Inspired by human ToM, AI researchers have made explicit and implicit efforts to develop a machine ToM for social intelligence: AI agents that engage in social interactions with humans (Kr\u00e4mer et al., 2012; Kennington, 2022) and other agents (Albrecht and Stone, 2018). Likewise, a machine ToM can enhance agents\u2019 capacity for interactions (Wang et al., 2021), explainable decision-making (Akula et al., 2022), dialogue communication (Qiu et al., 2022; Takmaz et al., 2023), and collaborative task planning (Bara et al., 2023).\nMachine ToM has received an increasing amount of attention in the research community, especially as the field is reshaped by large language models (LLMs) such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023). This highlights an ongoing debate and discussion on whether a machine ToM has emerged in LLMs. While LLMs have demonstrated some capability of inferring communicative intentions, beliefs, and desires (Andreas, 2022; Kosinski, 2023; Bubeck et al., 2023), researchers also reported concerns regarding a lack of robust agency in LLMs for complex social and belief reasoning tasks (Sap et al., 2022; Shapira et al., 2023a) and in-context pragmatic communication (Ruis et al., 2022). Emerged or not emerged, that remains a question (or may not even be the central question to ask). In our view, existing evaluation protocols do not fully resolve this debate. Most current benchmarks focus only on a (few) aspect(s) of ToM, in the form of written stories, and are prone to data contamination, shortcuts, and spurious correlations (Trott et al., 2022; Aru et al., 2023; Shapira et al., 2023a). Prior to embarking on extensive data collection for new ToM benchmarks, it is crucial to address two key questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM?\nTo embrace the transformation brought by LLMs and explore their full potential in understanding and modeling ToM, this position paper calls for a holistic investigation that taxonomizes ToM using the Abilities in Theory of Mind Space (ATOMS) framework (Beaudoin et al., 2020). After a review of existing benchmarks under this framework, we put forward a situated evaluation of ToM, one that treats LLMs as agents who are physically situated in environments and socially situated in interactions with humans. We hope this paper will offer an intuitive means to identify research priorities and to help gain a deeper understanding of, as well as to effectively utilize, LLMs in ToM modeling for AI agents in the future."
        },
        {
            "heading": "2 Large Language Models as Theory of Mind Agents",
            "text": "Since the advent of pre-trained language models, the research community has questioned whether they possess intrinsic mental states to represent the environment (Li et al., 2021; Storks et al., 2021; Hase et al., 2023) and comprehend the mental states of others (Sap et al., 2019; Zhang et al., 2021) through the textual description (observation) of behavioral cues. The relatively recent breakthroughs of LLMs have created many discussions and debates, primarily concerning the extent to which LLMs possess various capabilities required for a machine ToM. In this section, we first survey recent research presenting evidence and counter-evidence for the emergence of ToM in LLMs. We conclude the discussion with the limitations of current evaluation protocols."
        },
        {
            "heading": "2.1 Do Machine ToM Emerge in LLMs?",
            "text": "Evidence for emergent ToM in LLMs. Prior to the rise of large language models, there has been growing evidence and acknowledgment of a narrow and limited sense of agency in smaller language models. Andreas (2022) argues that language models have the capacity to predict relations between agents\u2019 observations, mental states, actions, and utterances, as they infer approximate representations of beliefs, desires, and intentions of agents mentioned in the context. These representations have a causal influence on the generated text, similar to an intentional agent\u2019s state influencing its communicative actions under a Belief-Desire-Intention (BDI) agent model (Bratman, 1987). Amidst the excitement surrounding the release of GPT-4 (Ope-\nnAI, 2023), researchers have searched for evidence of an emergent ToM in LLMs. Kosinski (2023) presents 20 case studies each of the unexpected contents task (Perner et al., 1987) and the unexpected transfer (Sally-Anne) task (Baron-Cohen et al., 1985). With direct comparisons to children\u2019s performance, the findings have been cited as potential evidence for a spontaneous emergence of ToM in LLMs. Bubeck et al. (2023) present a similar behavioral study with 10 cases of belief, emotion, and intention understanding, concluding that GPT4 has an advanced level of ToM after qualitative comparison with predecessors. Other case studies have also shown aspects of machine ToM (Li et al., 2023; Holterman and van Deemter, 2023). Limitations of ToM capabilities in LLMs. The above findings contradict the conclusions drawn in Sap et al. (2022)\u2019s earlier study, which shows a clear lack of ToM in GPT-3 (Brown et al., 2020) on SOCIALIQA (Sap et al., 2019) and TOMI (Le et al., 2019) benchmarks. As a potential account, there has been criticism that the cognitive inquiries are anecdotal and inadequate for evaluating ToM in LLMs (Marcus and Davis, 2023; Mitchell and Krakauer, 2023; Shapira et al., 2023a). Following the same evaluation protocol, Ullman (2023) demonstrates that simple adversarial alternatives to Kosinski (2023) can fail LLMs. To further understand if the most recent variants of LLMs possess a robust ToM, Shapira et al. (2023a) present a comprehensive evaluation over 6 tasks and 3 probing methods, showing that a robust machine ToM is absent even in GPT-4 and that LLMs are prone to shortcuts and spurious correlations. Based on the ongoing debate, it can be concluded that, while LLMs exhibit some level of sensitivity at understanding others\u2019 mental states, this capability is limited and falls short of achieving robust humanlevel ToM (Trott et al., 2022; Shapira et al., 2023a)."
        },
        {
            "heading": "2.2 Roadblocks in ToM Evaluation in LLMs",
            "text": "Given the pressing need for a robust machine ToM in LLMs and large-scale ToM benchmarks, researchers echo several difficulties in the evaluation protocol. Presently, ToM benchmarks suffer from three primary issues summarized as follows. Limited aspects of ToM. The evaluation of machine ToM lacks consistency in the literature due to the ambiguity surrounding the specific mental states being targeted. Existing benchmarks often focus on limited numbers of mental states, such as the intention (Yoshida et al., 2008), belief (Grant\net al., 2017), emotion (Sap et al., 2019), and knowledge (Bara et al., 2021) of another agent. While all of these are necessary building blocks of machine ToM, we echo Shapira et al. (2023a)\u2019s concern that the ToM capability of LLMs may have been overclaimed based on evaluations from only a specific aspect of ToM. To give a comprehensive assessment of a holistic machine ToM, a taxonomy is essential to enable researchers to effectively position their work with different focuses and priorities, which may be orthogonal to each other. Data contamination. Data contamination refers to the lack of a verifiable train-test split that is typically established to test the ability of machine learning models to generalize (Magar and Schwartz, 2022). LLMs typically learn from internet-scale data, potentially giving them access during training to the data used to test them (Bubeck et al., 2023; Hagendorff, 2023). For ToM evaluation specifically, the training corpora of LLMs may contain research papers detailing these psychological studies. Many past studies used identical or slightly altered language prompts to test LLMs, leading to potential contamination issues (Ullman, 2023). To critically evaluate the performance of LLMs on ToM tasks, researchers must have access to the datasets used to train them (Dodge et al., 2021), which are unfortunately not available. Shortcuts and spurious correlations. The availability of shortcuts and spurious features has triggered many concerns that a model may leverage them to perform highly on a benchmark without robustly acquiring the desired skill (Sclar et al., 2023; Ullman, 2023; Shapira et al., 2023a). Recent findings suggest that LLMs tend to learn surface-level statistical correlations in compositional tasks, potentially leading to an illusion of systematic learning (Dziri et al., 2023). In all likelihood, LLMs are capable of learning ToM shortcuts in a similar manner."
        },
        {
            "heading": "3 Towards A Holistic Landscape of Machine Theory of Mind",
            "text": ""
        },
        {
            "heading": "3.1 Abilities in Theory of Mind Space (ATOMS) Framework",
            "text": "The evaluation of machine ToM lacks clarity and consistency across various literature, primarily due to the ambiguity surrounding the specific mental states being targeted. This ambiguity is not unique to the field of AI but is rooted in the complicated cognitive underpinnings of ToM. At the core of\nthis ambiguity is the latent nature of mental states, the subject has privileged access to them while others can only infer the existence of these mental states based on observable behaviors or expressions (Dretske, 1979; Blakemore and Decety, 2001; Zaki et al., 2009). Thus, it is impossible to directly access and assess the mental states of a human, and ToM must be tested indirectly through humans\u2019 ability to understand the relationship between mental states and behaviors, especially by predicting how agents behave based on their mental states (Swettenham, 1996; Phillips et al., 2002).\nWhile the exact definition of ToM remains a central debate, the AI community can benefit from looking at what psychologists have viewed as an initial step. In this paper, we follow Beaudoin et al. (2020)\u2019s taxonomy of ToM sub-domains, i.e., the Abilities in Theory of Mind Space (ATOMS). As shown in Figure 1, the space consists of 7 categories of mental states, including beliefs, intentions, desires, emotions, knowledge, percepts, and non-literal communication. We selected this taxonomy because it was derived from a comprehensive meta-analysis of ToM studies. The meta-analysis focused on young children aged 0-5 years at the early stage of cognitive development, such that the setups are simpler and more comparable, avoiding complicated physical and social engagements that cannot be trivially deployed on LLMs.\nBeliefs. Beliefs are informational states that people judge to be true, usually decoupled from motivational states (Dennett, 1995; Eccles and Wigfield, 2002). Beliefs, the most studied mental states in the field of ToM, are usually tested in the form of false belief tasks, including the unexpected contents test (Perner et al., 1987), the unexpected transfer (Sally-Anne) Test (Baron-Cohen et al., 1985), the second-order false belief (Icecream Van) Test (Perner and Wimmer, 1985). Researchers also studied their connection to actions and emotions (Swettenham, 1996).\nIntentions. Intentions are choices with commitment, usually associated with concrete actions towards a goal (Cohen and Levesque, 1990). As a critical component of ToM, Kennington (2022) has called for a more explicit treatment of intentions. Intentions have been extensively explored in psychology tests, e.g., behavioral re-enactment (Meltzoff, 1995), action prediction (Phillips et al., 2002), intention explanation (Smiley, 2001), and intention attribution to abstract figures (Castelli, 2006).\nAbilities in Theory of Mind Space\n(ATOMS)\nPercepts\nPercept-Action Link\nVisual Perspective Taking\nAuditory Perspective Taking\nDesires\nMultiple Desires\nDiscrepant Desires\nDesire-Action Contradiction\nDesire-Influenced Action/Emotions\nKnowledge\nPercepts-Knowledge Link\nKnowledge-Attention Link\nInformation-Knowledge Link\nKnowledge-Pretend Play Link\nEmotions\nMoral Emotions\nMixed Emotions\nHidden Emotions\nEmotion Regulation\nDiscrepant Emotions\nComprehensive Measures\nTypical Emotional Reactions\nAtypical Emotional Reactions\nNon-literal Communication\nHumor\nWhite Lies\nIrony/Sarcasm\nEgocentric Lies\nInvoluntary Lies\nFaux-Pas/Social Gaffe\nComprehensive Measures\nBeliefs\nIdentity False Beliefs\nSecond-Order Beliefs\nContent False Beliefs\nLocation False Beliefs\nComprehensive Measures\nBelief-Based Action/Emotions\nIntentions\nIntention Attribution\nIntention Explanation\nPrediction of Actions\nDiscrepant Intentions\nCompletion of Failed Actions\nFigure 1: The ATOMS framework of Beaudoin et al. (2020), which identified 7 categories of mental states through meta-analysis of ToM studies for children.\nBeliefInten tionDesir e Emot ion know ledgeperce pt Non-l iteral\nComm .\n0\n2\n4\n6\n8\n10\n# Av\nai la\nbl e\nBe nc\nhm ar\nks\n10 11\n4 5\n3\n5\n3\nFigure 2: Number of available benchmarks for each mental state in ATOMS.\nSymmetricity\nSocial Situatedness"
        },
        {
            "heading": "Physical",
            "text": "Situatedness\nInput Modality\nsymmetric 3\nasymmetric 18\nperceiver 6 interactor 5\nNone 10\nperceiver 12\ninteractor 3 None 6\ntextual 12\nenvironment 4 visual 5\nFigure 3: A comparison of benchmark settings.\nDesires. Desires are motivational states that do not necessarily imply commitment, though they are usually emotionally charged and affect actions (Malle and Knobe, 2001; Kavanagh et al., 2005). Typical studies along this line include the Yummy-Yucky Task (Repacholi and Gopnik, 1997) for discrepant preferences from different individuals, the multiple desires within one individual (Bennett and Galpert, 1993), and the relationship between desires and emotions/actions (Wellman and Woolley, 1990; Colonnesi et al., 2008). Emotions. Emotions are mental states associated with an individual\u2019s feelings and affective experiences, which could impact beliefs and behaviors (Frijda et al., 1986; Damasio, 2004). Most ToM studies on emotions focus on typical (Knafo et al., 2009) and atypical (Denham, 1986) emotional reactions to situations. Other studies also encompass affective perspective taking (Borke, 1971), understanding hidden emotions (Harris et al., 1986), and morally related emotions (Pons and Harris, 2000).\nKnowledge. Many controversies revolve around the definition of knowledge as justified true beliefs (Gettier, 2000). In the context of AI, knowledge typically consists of information and organized representations of the world, which can be used to simplify understanding and address intricate reasoning and planning (Schank and Abelson, 2013). ToM studies usually involve understanding the absence of knowledge (Aronson and Golomb, 1999) as well as the connection between knowledge and perception (Ruffman and Olson, 1989) and attention (Moll et al., 2006).\nPercepts. Humans are situated in the physical and social environments. To enable AI agents to operate in the world and communicate with humans, the sensory and social aspects of perception are crucial in a machine ToM. Along this line, psychological studies have investigated the perceptual perspective taking (Masangkay et al., 1974) and understanding the influence of limited perception on actions (Hadwin et al., 1997).\nNon-literal communications. Being able to understand non-literal and figurative communication helps humans to perform pragmatic inference and reason about hidden words behind their written meanings (Giora, 2003). Non-literal communication has been recognized as an advanced ToM capability, spanning a wide spectrum of humor and deceptions (Happ\u00e9, 1994), sarcasm (Sullivan et al., 1995), and faux-pas (social gaffe) situations (BaronCohen et al., 1999)."
        },
        {
            "heading": "3.2 A Taxonomized Review of Benchmarks",
            "text": "The ATOMS framework can serve as an intuitive reference for researchers to identify their research priorities and situate their work better in the landscape of literature. We further take the initiative to provide a systematic review of existing benchmarks for machine ToM under the umbrella of ATOMS.1 Although there are independent research initiatives on certain ToM facets like intention classification, emotion modeling, and aspects of non-literal communications, we primarily focus on those that explicitly target ToM or inferences of latent mental states. Besides the ToM dimensions in ATOMS, we further characterize the benchmarks on their task formulation, input modalities, physical and social situatedness, and symmetricity (whether the tested agent is co-situated and engaged in mutual interac-\n1We maintain a repository for relevant literature at https: //github.com/Mars-tin/awesome-theory-of-mind.\ntions with other ToM agents). We summarize our review in Table 1 and discuss our observations and under-explored aspects of ToM evaluation."
        },
        {
            "heading": "Many aspects of ToM are under-explored. As",
            "text": "shown in Figure 2, we notice an overwhelming research focus on the intention and belief aspects of machine ToM. Several other aspects of ToM have not received enough attention. While the field of NLP has thoroughly explored different facets of emotion and non-literal communication, e.g., in the context of dialogue systems, ToM has rarely been explicitly mentioned as motivation. More connections and integrative efforts are clearly needed.\nLack of clear targeted mental states. Explicitly mentioning the Sally-Anne Test (Baron-Cohen et al., 1985) as inspiration, Grant et al. (2017) developed the predecessor of TOMI (Le et al., 2019). Similarly, Nematzadeh et al. (2018) cited the Icecream Van Test (Perner and Wimmer, 1985) as motivation and the FAUXPAS-EAI (Shapira et al., 2023b) benchmark followed the study of BaronCohen et al. (1999). While these benchmarks are cognitively grounded and target one particular aspect of ToM, the majority often incorporate multiple mental states without clear descriptions, which could make it challenging to measure the actual progress (Raji et al., 2021).\nLack of situatedness in a physical and social environment. Figure 6 illustrates the configurations of benchmarks. Each bar in the chart represents a distinct benchmark characteristic, and each segment within the bar illustrates the proportion of benchmarks with one specific setting. An immediate observation is a noticeable lack of benchmarks that encompass both physical and social environments, which highlights an existing research disparity in the field. We notice that many existing benchmarks are story-based, which verbalize the agent\u2019s perception of the environment and the behaviors of other agents in the form of story episodes, usually with language templates. The semantics of the environment are given by high-level events (e.g., Sally entered the kitchen). Many aspects of physical and social situatedness are overlooked in these benchmarks, e.g., spatial relations, the task and motivation of agents, and their action trajectories.\nLack of engagement in environment. We point out that existing benchmarks primarily adopt a passive observer role to test language agents. Yet the crucial aspects of interaction and engagement between the agent and other entities involved have\nbeen overlooked. Among all the benchmarks we reviewed, only three of them treat the tested model as an active agent, one that perceives the physical and social context, reasons about others\u2019 mental states, communicates with other agents, and interacts with the environment to complete pre-defined tasks (Sclar et al., 2022; Bara et al., 2021, 2023)."
        },
        {
            "heading": "4 Towards A Situated Theory of Mind",
            "text": ""
        },
        {
            "heading": "4.1 Why A Situated ToM?",
            "text": "There have been concerns that cognitive inquiries are inadequate for gaining insight into understanding ToM for LLMs (Mitchell and Krakauer, 2023; Shapira et al., 2023a). However, we believe that the primary problem lies in using story-based probing as proxies for psychological tests, which situate human subjects in specific physical or social environments and record their responses to various cues. We, therefore, call for a situated evaluation of ToM, in which the tested LLMs are treated like agents who are physically situated in environments and socially situated in interactions with others.\nSituated evaluation covers more aspects of ToM. Although it is possible to frame the situations as narratives and cover all mental states using text-only benchmarks, certain aspects of ToM can only be effectively studied within specific physical or social environment (Carruthers, 2015). This is because humans have the ability to infer the mental states of others through various modalities such as visual perception, actions, attention (gazes or gestures), and speech (Stack et al., 2022). For instance, studying perceptual disparities can be challenging with text-only datasets, as they often reduce complex scenarios to rule-based manipulations over negations in the prompts (Sileo and Lernould, 2023). Benchmarks that are not situated also face challenges when it comes to implementing coordination between agents, e.g., aligning intentions towards joint actions (Jain et al., 2019) and pragmatic generation (Zhu et al., 2021a; Bao et al., 2022).\nSituated evaluation mitigates data contamination. A situated ToM evaluation can mitigate data contamination, as researchers can design scenarios in simulated settings that are unlikely to be part of the LLM\u2019s training data. Carefully designed benchmarks can also incorporate seen and unseen environments to assess generalization to new tasks and new environments, fundamentally addressing the issue of data contamination (Gandhi et al., 2021).\nSituated evaluation mitigates shortcuts. By employing situated evaluation, the risk of taking shortcuts can be mitigated. Many of the existing ToM benchmarks are either limited in scale or adopt text templates to verbalize a (few) predefined scenario(s) and prompt LLMs for answers, giving answers away from syntactic structures and positional information (Le et al., 2019; Sclar et al., 2023). In a situated setting, on the contrary, we rely on simulated environments to manipulate evaluation data at scale, so that the environment, the states, and the action traces in the environment can be randomized to avoid the statistical spurious correlations. While situated evaluation can mitigate shortcuts, it does not eliminate the issue completely. For example, Aru et al. (2023) have reported that shortcuts can emerge in grid world setups if the design is not careful enough and randomness is limited. We emphasize that careful design and consideration are still required to curate any ToM benchmark."
        },
        {
            "heading": "4.2 A Preliminary Exploration in Grid World",
            "text": "In this section, we present a proof-of-concept study on a situated evaluation of ToM on LLMs. We choose to conduct our pilot study in MiniGrid (Chevalier-Boisvert et al., 2018), a simple and commonly used environment for ToM studies in the machine learning community (Rabinowitz et al., 2018; Sclar et al., 2022). Through basic grid world representation, we can create tasks to challenge LLMs to reason about many aspects of physical and social situatedness, e.g., spatial relations, partial observability, agent\u2019s action trajectories, and from there, their beliefs, intent, emotions, etc. This is in stark contrast to existing story-based ToM benchmarks, which only contain high-level event episodes. We demonstrate that a diverse range of challenging ToM tests, covering all mental states from ATOMS, can be effectively created in a situated manner using a simple 2D grid world.\nEnvironment and Task Setups We introduced 9 different ToM evaluation tasks for each mental state under ATOMS, and 1 reality-checking task to test LLMs\u2019 understanding of the world. It is important to acknowledge that our experiment serves as a proof of concept and does not aim to cover the entire spectrum of machine ToM, as our case studies are far from being exhaustive or systematic.\n\u2022 Reality Check: Given the sequence of actions, predict the closest object at the end of the trajectory. The task is designed to test LLMs\u2019 under-\nstanding of relocations in the grid world. \u2022 Short-term Intention: Given an incomplete tra-\njectory and a goal, predict the next action. \u2022 Long-term Intention: Given an incomplete tra-\njectory and a list of subgoals, predict the next subgoal that the agent is planning to achieve.\n\u2022 Desire: Given a complete trajectory, predict if the agent demonstrates a preference for objects.\n\u2022 Percepts: Given a complete trajectory, predict if the agent has a partial or full observation.\n\u2022 Belief: The classic unexpected transfer task with possible first and second order false belief.\n\u2022 Non-literal Communication: Given a trajectory and a statement from the agent, judge whether the agent is being deceptive.\n\u2022 Knowledge: Given a trajectory, predict the object whose location is unknown to the agent.\n\u2022 Emotion: The classic perception-emotion link test, where emotions are evoked in response to witnessing an emotionally stimulating situation.\nWe detail two case studies and leave examples of each task in Appendix A. Case Study 1: Beliefs. Our belief experiments emulate the classic unexpected transfer tasks (Baron-Cohen et al., 1985; Perner and Wimmer, 1985). As is shown in Figure 4, we simulate this disparity of belief state and world state in MiniGrid. The first-order belief task features a main room with three connected side rooms, two agents named Red and Green, and a ball. Each instance of the belief experiment begins with Green placing the ball in Room#2 while Red watches. Red then enters a separate Room#1 and shuts the door. While Red is inside of this closed room, Green transfers the ball to Room#3. Red presumably holds a false belief about the location of the ball, believing it is in Room#2 though it is now in Room#3. Similarly, we implement the second-order belief task to test an incorrect belief that one agent holds about the belief of another. After Green has finished transferring the ball, it navigates to the room originally\nReality Check\nShort Intent\nLong Intent\nDesire Percept 1st belief 2nd belief Non-Literal Comm.\nKnowledge Emotion0.0\n0.2\n0.4\n0.6\n0.8 1.0 Ac cu ra cy Rnd.\nN/A\nModels Avg. Accuracy GPT-3.5 Zero Shot GPT-3.5 One Shot GPT-3.5 Zero Shot CoT GPT-4 Zero Shot GPT-4 One Shot 54.1% 50.1% 51.2% 60.9% 65.7%\nFigure 6: The LLMs\u2019 performance across the 10 tasks is illustrated. Each bar shows how one LLM performed with a specific prompting method. Overall, the tasks are tough for all the LLMs tested. While one-shot and CoT prompting can help in certain cases, their effectiveness is not consistent across the board.\ncontaining the ball and shuts the door. Red then navigates to the room now containing the ball and sees the true location of the ball. Still, Green presumably possesses a false belief about Red\u2019s belief. In both tasks, LLMs are queried with two versions of the world: a false one with the ball in the original room, and a true one with the ball in the third room (its actual location). LLMs must correctly respond that the agents hold a false belief. Case Study 2: Emotions. While the belief tasks highlight the importance of physical situatedness, we further demonstrate that social interactions can be simulated in the grid world. As is shown in Figure 5, We design morally related events that stimulate emotions (e.g., fear, appreciation). In this task, LLMs are queried to predict the emotional response of Agent-White, who either directly witnesses or is ignorant of this event. LLMs must correctly respond that the agent holds an emotional reaction only if it observes the event. Experiment Setups. For each task, we create 100 instances following a prompt template that consists of [environment description], [agent description], [observability statement], [task statement], [actions sequences], [QA]. We select GPT-4 (gpt-4-0314) and ChatGPT (gpt-3.5-turbo-0613) for evaluation on the 9 tasks.2 Following prior work (Hu et al., 2022; Shapira et al., 2023a), we adopt MC-probing for LLMs that don\u2019t produce probabilities, which directly instructs LLMs to generate only the letter corresponding to the answer. Besides zero-shot evaluation, we also explored one-shot learning and Chain-of-Thought (CoT) prompting (Wei et al., 2022). More details are available in Appendix B.\n2We use the ChatCompletion.create function from openai package.\nResults and Discussion. We observe that LLMs exhibit some level of sensitivity for some mental states. Especially, GPT-4 scores up to 91% zeroshot accuracy and 96% one-shot accuracy in the long-term intention task. However, we also highlight the shortcomings of LLMs in some mental states of ATOMS to varying degrees, especially, in terms of predicting preferences, perception limitations, missing knowledge, and higher-order beliefs. These findings align with previous research (Sap et al., 2022; Trott et al., 2022; Shapira et al., 2023a), further confirming that LLMs are not yet reliable and comprehensive ToM agents. From the realitychecking task, we observe that GPT-3.5 reaches 78% accuracy with CoT prompting and GPT-4 significantly surpasses its predecessors with 83% zeroshot accuracy and 95% one-shot accuracy. Solving this reality check by no means implies that LLMs have a general perception ability of the real world, but that as a proof of concept, they demonstrate a certain (but still limited) level of situated awareness within the context of a basic abstract grid world. This implies that researchers can begin utilizing them as powerful building blocks for situated agents in complex ToM tasks. We note that it is always possible to come up with more challenging reality-checking questions to expose the limitations of LLMs, or to provide more guided prompts to assist LLMs in successfully completing ToM tasks. Undoubtedly, further research is required along this exciting yet challenging trajectory to advance ToM in LLMs and AI agents built upon LLMs."
        },
        {
            "heading": "5 Discussions and Action Items",
            "text": ""
        },
        {
            "heading": "5.1 The Scope of Machine Theory of Mind",
            "text": "Be specific about the mental states studied. Existing benchmarks often lack a clear target mental\nstate, making it challenging to interpret the results and measure the actual progress. To mitigate the risk of overestimating LLMs\u2019 ToM capabilities, it is recommended that future benchmark developers provide specific details regarding the targeted mental state(s) they intend to assess. Broaden the Scope of Machine ToM. A breadth of mental states and their sub-domains have already been covered by AI benchmarks (Table 1). We observed an overwhelming emphasis on the benchmarks and modeling of beliefs and intentions, while other aspects have received insufficient attention. Still, there are considerably many blank spaces in the landscape of machine ToM, especially for more complicated forms of knowledge, desires, perspective-tasking, and emotional experiences beyond typical social situations."
        },
        {
            "heading": "5.2 Design New Theory of Mind Benchmarks",
            "text": "Avoid shortcuts and spurious correlations. The evaluation of LLMs itself presents significant challenges, not only in the case of ToM. Existing benchmarks suffer from issues such as data leakage and spurious correlations. Especially, shortcut solutions have been consistently reported in recent years (Le et al., 2019; Shapira et al., 2023a; Aru et al., 2023). We are in pressing need of new benchmarks with scalable sizes, high-quality human annotations, and privately held-out sets for evaluation. Avoid unfair evaluations from prompting. Previous work has shown that CoT prompting can improve the performance of LLMs in ToM tasks (Li et al., 2023; Moghaddam and Honey, 2023; Shapira et al., 2023a). Various recent prompting mechanisms have also been developed to improve LLM\u2019s capability on ToM tasks (Zhou et al., 2023a; Leer et al., 2023). In the evaluation of LLMs\u2019 ToM capabilities, we recommend the careful documentation of prompts used and the avoidance of implicit human guidance to ensure a fair comparison. Move on to a situated ToM. We call for a situated evaluation of ToM, in which the tested LLMs are treated like agents who are physically situated in environments and socially situated in interactions with others. A situated setup covers a wider range of ToM aspects. With carefully designed benchmarks with diverse environments and unseen test sets, a situated setup can help address data contamination issues and assess generalization to new tasks and environments. Furthermore, a situated setup allows for more complicated evaluation protocols than simple inference and QA tasks.\nConsider a mutual and symmetric ToM. ToM is symmetric and mutual in nature, as it originally imputes the mental states of self and others. Prior research is largely limited to passive observer roles (Grant et al., 2017; Nematzadeh et al., 2018; Le et al., 2019; Rabinowitz et al., 2018) or speaker in a speaker-listener relationship (Zhu et al., 2021b; Zhou et al., 2023b). We encourage more studies on how humans and agents build and maintain common ground with a human ToM and a machine ToM through situated communication (Bara et al., 2021; Sclar et al., 2022). Besides, more research is needed to understand if LLMs possess early forms of intrinsic mental states given observation cues of the world. While we need to develop machines that impute the mental states of humans, humans should also develop a theory of AI\u2019s mind (ToAIM) (Chandrasekaran et al., 2017) by understanding the strengths, weaknesses, beliefs, and quirks of these black box language models."
        },
        {
            "heading": "5.3 Neural Language Acquisition and ToM",
            "text": "Both psychological studies (Bloom, 2002; Tomasello, 2005) and computational simulations (Liu et al., 2023) have demonstrated the effectiveness of ToM, especially intention, in language acquisition. Instead of concentrating on eliciting ToM in LLMs, we should contemplate whether certain ToM elements should be inherently present in LLMs or perhaps introduced alongside language pretraining. More research is needed to understand the connection between neural word acquisition and ToM development in machines."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this position paper, we survey and summarize the ongoing debate regarding the presence of a machine ToM within LLMs, and identify the inadequate evaluation protocols as the roadblock. Many benchmarks focus only on a few aspects of ToM, and are prone to shortcuts. To mediate this issue, we follow the ATOMS framework to offer a holistic review of existing benchmarks and identify underexplored aspects of ToM. We further call for a situated evaluation of ToM, one that is physically situated in environments and socially situated in interactions with humans. We hope this work can facilitate future research towards LLMs as ToM agents, and offer an intuitive means for researchers to position their work in the landscape of ToM."
        },
        {
            "heading": "Ethical Statement",
            "text": "The dataset created in this study includes instances that are synthetically generated from planners and RL algorithms, as well as ones created by humans. Human subjects research is approved by the University of Michigan Health Sciences and Behavioral Sciences Institutional Review Board (IRB-HSBS) under eResearch ID HUM00234647. The text generated by LLMs could potentially contain harmful, toxic, or offensive content. The authors have ensured that the data does not contain personally identifiable information or offensive content."
        },
        {
            "heading": "Limitations",
            "text": "Our current benchmark only covers 100 instances for each task, adding up to only 1000 instances. Our experiment serves as a proof of concept and does not aim to cover the entire spectrum of machine ToM, as our case studies are far from being exhaustive or systematic. In the future, we plan to create a more systematic benchmark with a larger scale and various forms of evaluation. Additionally, it is worth noting that the ATOMS framework is derived from human ToM studies conducted with children under the age of 5. Consequently, this framework primarily focuses on the early developmental stages of ToM, capturing the naive and potentially rudimentary aspects of ToM. For more advanced ToM capability, we point to some recent frameworks proposed by Osterhaus and Bosacki (2022) and Stack et al. (2022)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by NSF IIS1949634, NSF SES-2128623, and by the Automotive Research Center at the University of Michigan. Without implying any agreement with the contents as presented in this work, the authors extend their appreciation to Susan Gelman for her valuable feedback. The authors would like to thank all anonymous reviewers for their valuable feedback."
        },
        {
            "heading": "A Task Settings and Data Collection",
            "text": "In this section, we provide an in-depth explanation of the ten tasks outlined in section 4.2. Task 0 serves as a \"reality check\" to assess LLMs\u2019 grasp of the physical world, particularly relocations within the grid world. Tasks 1 through 9 each emphasize distinct facets of ToM. All these tasks utilize MiniGrid, a streamlined 2D grid-world environment. (Chevalier-Boisvert et al., 2018)."
        },
        {
            "heading": "Task 0: Reality Check",
            "text": "In this scenario, the agent is tasked with visiting 0-2 of the objects in the grid world. The agent has full observation in the world for efficient navigation. At least 2 objects (12 maximum) are placed in the environment. To test an LLM\u2019s ability to understand the physical actions taken by the agent, we ask it about the distance between the agent and various objects after it accomplishes a number of actions. The action planner is either a shortest path planner towards specified objects or a random action generator.\nAfter the agent has finished 10 random actions, or right after it has visited one object with an optimal action planner, the task-related question will be generated with the following format:\nAfter having taken these actions, which item is the agent closer to?\nA. <object1.color> <object1.name>\nB. <object2.color> <object2.name>\nHere object1 and object2 both exist in the environment, and one of them is guaranteed to be the target object if there is such a goal. There are always two options for this task.\nAlong with the task-related question, the prompt includes a description of the grid world environment, the action space of the agent (only going forward, turn left / right in this task), a board-like depiction of the initial state, the list of actions taken\nby the agent, and the agent\u2019s location and face direction for each step. For more details on prompting, please refer to section B.\nThe data for Task 0 are autonomously generated using seeds and a shortest-path planner.\nTask 1: Short Term Intention\nIn this scenario, the agent is tasked with visiting either of the two objects in the grid world. The LLM is not provided with the goal object. Rather, it must determine the goal object by examining and understanding the agent\u2019s trajectory. In this task, the agent has full observation in the world, and there are exactly two objects placed. The object types and colors are randomly generated. The size of the room can be randomly sampled from the range 6 by 6 to 12 by 12.\nTo test an LLM\u2019s understanding of short-term intention, we ask it to predict the next action of the agent given its previous trajectory.\nThe agent\u2019s trajectory halts at a random step, with the exception of the precise moment when the optimal paths to the two objects diverge. This \"cutting point\" is set as an exception and also serves as the mean for the normal distribution from which the stopping point is sampled.\nBy restricting the cutting point in this manner, we guarantee the trajectory included in the prompt to be optimal for reaching the potential goal objects. This reduces the ambiguities of our experiments, and thus improves the significance of our results.\nThe task-related question has the following format:\nWhich action will the agent take next?\nA. left B. right C. forward\nThe LLM should be able to choose which action would be next were the agent to continue its optimal trajectory to the goal object. The data for Task 1 are\nautonomously generated using seeds and a shortestpath planner.\nTask 2: Long Term Intention\nIn this scenario, the agent needs to complete the following subgoals in as few steps as possible: 1) Locate and pick up a key; 2) Locate and go through a door; 3) Navigate to the object in the new room. There are two rooms in this setting, which are connected by a locked door. The key of the door is always in the room in which the agent is initially located. The object is always placed in the other room.\nWe provide an LLM with a subset of the agent\u2019s trajectory and ask it which subgoal the agent is currently trying to complete.\nThe task-related question has the following format:"
        },
        {
            "heading": "Based on the agent\u2019s trajectory thus",
            "text": "far, which subgoal is the agent currently trying to complete?\nA. Locate and pick up a key B. Locate and go through a door"
        },
        {
            "heading": "C. Navigate to the object in the new room",
            "text": "The data for Task 2 are autonomously generated using seeds and a shortest-path planner.\nTask 3: Desire In this scenario, the agent is required to pick up three objects as soon as possible. There are 2 types of objects in the world (e.g., blue balls and white balls), 3 of each. The agent may or may not have a preference for one object type (we stratify the data such that in 50% of the episodes the agent has\na preferred object type and in the other 50% the agent lacks a preference). We also deduct from the final reward given to the agent when it takes a large number of steps to finish picking up three objects.\nThis task tests whether an LLM is able to determine the desire of the agent (for one object type or the other) by examining its trajectory. We prepared the following question for this task:\nWhich object does the agent prefer?\nA. white ball B. blue ball C. no preference\nThe data for Task 3 are autonomously generated using seeds and Reinforcement Learning. We use the PPO algorithm (Schulman et al., 2017) to train the model. In the scenario wherein a preference is present, the preferred object type yields 10 times more reward than the non-preferred one. In the scenario wherein the preference is absent, both object types yield identical rewards.\nTask 4: Percept In this scenario, the agent is instructed to navigate in two rooms and reach the goal in the other room as fast as possible. In contrast with previous task settings, the agent has either a very limited visual range (3 x 3 grid in the front), or an \u201cinfinitely\u201d large visual range (for practical purposes, the visual range is actually a 101 x 101 grid). Naturally, an agent with a smaller viewing range will\nmake more mistakes (e.g., navigating to a dead end) while trying to reach the goal object. Obstacles are randomly placed in each room to block the agent\u2019s view.\nLLMs are expected to only look at the trajectory of the agent in an environment, and determine whether the agent has a limited view range or a nearly full view range. The question format is as follows:"
        },
        {
            "heading": "Based on the agent\u2019s actions, what is",
            "text": "the most likely perceptual range that the agent possesses? The perceptual range refers to the number of cells that an agent can see in front of it.\nA. 3 blocks B. infinitely many blocks\nWe manually collected 100 trials in total for both situations.\nTask 5 & 6: First and Second Order Belief In this scenario, there are two agents in the environment. Both of them are initially in the main room (on the left side of the whole grid world; see Figure 4). On the right side, there are three small rooms. Agents can freely go in and out of each room. They can see everything inside the current room and can see the other rooms through the door if it is open.\nThis task reproduces the unexpected transfer (Sally-Anne) test (Baron-Cohen et al., 1985; Perner and Wimmer, 1985), with both first-order and second-order belief checking. In the first-order belief task, one agent does not see the second agent\ntransfer a ball from one room to another. Presumably, therefore, the agent falsely believes the ball to be in its previous location. The second-order belief task extends the first-order belief task by enabling the agent with the false belief to see the ball in its new location. The other agent, however, does not witness the first agent rectifying its false belief, so it presumably holds a second-order false belief (about the belief state of the first agent). By varying the observations that each agent makes (e.g., whether or not each agent sees the transfer of the ball from one room to the another) as well as the order of these events, this task setting allows us to check an LLM\u2019s first-order and second-order belief capabilities.\nRather than asking the LLM directly where to find the object, we provide two board-like belief states as options. We then query the LLM about which board-like state the agent is more likely to believe.\nData for tasks 5 & 6 are collected via rule-based planners with several scenarios.\nTask 7: Non-Literal Communication\nThis task focuses specifically on one form of non-literal communication: lying. Within this task, the LLM is told explicitly that there is an agent tasked with navigating to all of the objects within the grid world. In each instance of the task, however, the agent only visits a subset of the objects in the environment. The LLM is subsequently told that the agent has claimed success in visiting a particular object. This object is randomly selected so that sometimes it is an object that has actually been visited and other times it is not.\nThe question format is as follows:"
        },
        {
            "heading": "Based on the agent\u2019s actions, is it",
            "text": "telling the truth?\nA. yes B. no\nTo successfully complete this task, the LLM must combine its knowledge about the physical occurrences taking place within the grid world with its knowledge about lying, a vital component in Beaudoin et al. (2020)\u2019s category of non-literal communication.\nTask 8: Knowledge\nThis task requires the agent to pick up two objects that exist in the environment. Both two objects are known to be placed separately in two of the four rooms.\nIn this scenario, the agent is informed of the location of one object, while the location of the second object remains unknown. The agent is instructed to collect the objects in a specific sequence. Ideally, if the agent knows an object\u2019s location, it should proceed directly to the appropriate room. Otherwise, it will have to search for the object in the yet-to-be-explored rooms.\nWe include the agent\u2019s entire trajectory in the prompt. We then ask the LLM to determine if the agent knows the position of one object before. The question format is presented as follows:\nBased on the agent\u2019s actions, does\nit know the position of <color> <name> before?\nA. Yes\nB. No\nThe data for Task 8 are autonomously generated using seeds and a rule-based planner.\nTask 9: Emotions This task (see Figure 5) requires LLMs to infer the emotions of agents in a situated context from their physical behaviors. Specifically, it involves variations on a theme involving a small lake of water. In every variation, one agent pushes another agent into the lake. In some of these variations, an observer is privy to the situation. The prompt then asks the LLM about this observer\u2019s feelings towards both the victim and the perpetrator. Presumably, the observer should experience sympathy for the victim and anger (or a similarly negative emotion) for the perpetrator."
        },
        {
            "heading": "How would <observer.name> most likely",
            "text": "feel about <pusher.name>?\nA. no strong emotion\nB. angry\nIn other variations, a helper comes along and pulls the victim out of the lake. Presumably, the observer should feel positive emotions (e.g. respect, gratitude) for this helper. The question format is as follows:"
        },
        {
            "heading": "How would <observer.name> most likely",
            "text": "feel about <helper.name>?\nA. no strong emotion\nB. respectful"
        },
        {
            "heading": "B Prompting and Reproducibility",
            "text": "In this pilot study, our data curation follows a uniform structure across all tasks similar to prior work (Li et al., 2022), deviating only slightly to account for task-specific circumstances.3\nEnvironment Description Each prompt begins with a description of the two-dimensional world wherein the task will take place. Specifically, our prompting code provides LLMs with the dimensions of the game board and a method to reference specific cells (column-first Cartesian coordinates).\n3The data for this pilot study is available at https:// huggingface.co/datasets/sled-umich/2D-ATOMS.\nEach prompt subsequently itemizes the various objects that are situated in the world, along with their coordinates and attributes. Although this prompting structure could be easily adapted to handle many different types of attributes, we focused only on color for the sake of simplicity. Additionally, this section assigns each object a \u201clabel\u201d: a single letter that the prompt uses to represent the object in a printed grid representation."
        },
        {
            "heading": "Agent Description, Observability, and Task",
            "text": "The next section of each prompt is a detailed description of the agent(s) occupying the grid world. In the example below, which was taken from task 1, only one agent occupies the grid world. In multiagent tasks, this section details the various properties of all agents. Whether single or multi-agent, however, the basic properties are the same. The prompt first details the position and direction of the agent. It is located in a specific cell, and it always faces up, down, left, or right. The prompt then specifies the various actions that an agent is capable of taking (e.g. \u201cforward\u201d, \u201cleft\u201d, and \u201copen\u201d). The agent\u2019s labels are then specified. Within the printed grid representation, the agent is always represented as a V-like shape depending on the direction that it is facing. For instance, the prompting tool uses < to represent an agent that is facing left. Finally, the prompt specifies two key attributes of the agent: (1) its level of observability (e.g. whether or not it can see into adjacent rooms) and (2) its goal. Sometimes these two descriptors are heavily modified, restricted, or removed altogether so that they do not interfere with the task. For instance, when testing LLMs for percepts, the prompt does not specify the visual range of the agent.\nAction Sequences Following the agent description section, the prompt prints out a board representation, a multi-line sequence of plain text that appears two-dimensional when printed out. Here, the various objects and agents are depicted in position by their associated labels. Additionally, the configuration of the walls is specified by a perimeter of W\u2019s. Next, the prompt specifies a sequence of actions that take place over the course of the episode being considered by the LLM. In the episode depicted below, the agent navigates part of the way to a red box. These actions always specify the new position and orientation of the agent.\nQuestions and Answer Candidates Finally, each prompt contains a question that the LLM must answer. These questions are the most task-specific\nportion of the prompt, so their contents vary, however, they are always multiple-choice. Additionally, they always contain a set of instructions below heeding the LLM to return only a single letter in its response.\nSample Prompt for Task 1\nThis is a grid-like 2D world The grid world consists of 6 rows and 6 columns, 0-based We use (i,j) to represent the i-th column (from left to right) and j-th row (from top to bottom).\nThe following is a list of objects in this world. Each line starts with the object's position and is followed by its attributes (2, 3): key, grey; represented by this label: G (4, 4): box, red; represented by this label: H\nWalls are depicted using the symbol W\nThere is an agent at (2, 2) facing left\nThe agent can take the following actions: - left: makes the agent face left of where it is currently facing - right: makes the agent face right of where it is currently facing - forward: makes the agent move one step in the direction it is currently facing - open: makes the agent open a door that it is in front of - pickup: makes the agent pick up the object that it is in front of - drop: makes the agent drop an item that it is holding - stay: makes the agent stay where it currently is for a timestep\nThe agent is represented by the following labels depending on which direction it is facing: - Facing left: < - Facing up: ^ - Facing right: > - Facing down: v\nThe agent has full observability, meaning it can see the entire world\nThe agent has been instructed to navigate to one of the two objects in the environment, although you do not know which\nThis is the starting state of the board: ```\n0 1 2 3 4 5 0 | W W W W W W 1 | W O O O O W 2 | W O < O O W 3 | W O G O O W 4 | W O O O H W 5 | W W W W W W ``` This list contains a sequence of actions taken by the agent (Step 1) The agent took action left and is now at (2, 2) facing down (Step 2) The agent took action left and is now at (2, 2) facing right (Step 3) The agent took action forward and is now at (3, 2) facing right (Step 4) The agent took action forward and is now at (4, 2) facing right (Step 5) The agent took action right and is now at (4, 2) facing down\nWhich action will the agent take next? A: left B: right C: forward\nPlease ONLY respond using the letter corresponding to your answer Do not generate any text other than the letter"
        }
    ],
    "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models",
    "year": 2023
}