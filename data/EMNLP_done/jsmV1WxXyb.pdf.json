{
    "abstractText": "Recent studies have shown that many natural language understanding and reasoning datasets contain statistical cues that can be exploited by NLP models, resulting in an overestimation of their capabilities. Existing methods, such as \u201chypothesis-only\u201d tests and CheckList, are limited in identifying these cues and evaluating model weaknesses. We introduce ICQ (I-SeeCue), a lightweight, general statistical profiling framework that automatically identifies potential biases in multiple-choice NLU datasets without requiring additional test cases. ICQ assesses the extent to which models exploit these biases through black-box testing, addressing the limitations of current methods. In this work, we conduct a comprehensive evaluation of statistical biases in 10 popular NLU datasets and 4 models, confirming prior findings, revealing new insights, and offering an online demonstration system to encourage users to assess their own datasets and models. Furthermore, we present a case study on investigating ChatGPT\u2019s bias, providing valuable recommendations for practical applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shanshan Huang"
        },
        {
            "affiliations": [],
            "name": "Shanghai Jiao Tong"
        },
        {
            "affiliations": [],
            "name": "Kenny Q. Zhu"
        }
    ],
    "id": "SP:e3afa512a98b5025660bb31cf9ad5abba7efa65f",
    "references": [
        {
            "authors": [
                "Rachel KE Bellamy",
                "Kuntal Dey",
                "Michael Hind",
                "Samuel C Hoffman",
                "Stephanie Houde",
                "Kalapriya Kannan",
                "Pranay Lohia",
                "Jacquelyn Martino",
                "Sameep Mehta",
                "Aleksandra Mojsilovic"
            ],
            "title": "2018. Ai fairness 360: An extensible toolkit for detecting",
            "year": 2018
        },
        {
            "authors": [
                "Samuel Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "EMNLP, pages 632\u2013642.",
            "year": 2015
        },
        {
            "authors": [
                "Ronan Le Bras",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Rowan Zellers",
                "Matthew E Peters",
                "Ashish Sabharwal",
                "Yejin Choi."
            ],
            "title": "Adversarial filters of dataset biases",
            "venue": "arXiv preprint arXiv:2002.04108.",
            "year": 2020
        },
        {
            "authors": [
                "Qian Chen",
                "Xiaodan Zhu",
                "Zhenhua Ling",
                "Si Wei",
                "Hui Jiang",
                "Diana Inkpen."
            ],
            "title": "Enhanced lstm for natural language inference",
            "venue": "arXiv preprint arXiv:1609.06038.",
            "year": 2016
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Clark",
                "Mark Yatskar",
                "Luke Zettlemoyer."
            ],
            "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases",
            "venue": "EMNLP-IJCNLP, pages 4060\u20134073.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Ivan Habernal",
                "Henning Wachsmuth",
                "Iryna Gurevych",
                "Benno Stein."
            ],
            "title": "The argument reasoning comprehension task: Identification and reconstruction of implicit warrants",
            "venue": "arXiv preprint arXiv:1708.01425.",
            "year": 2017
        },
        {
            "authors": [
                "He He",
                "Sheng Zha",
                "Haohan Wang."
            ],
            "title": "Unlearn dataset bias in natural language inference by fitting the residual",
            "venue": "EMNLP-IJCNLP, page 132.",
            "year": 2019
        },
        {
            "authors": [
                "Armand Joulin",
                "\u00c9douard Grave",
                "Piotr Bojanowski",
                "Tom\u00e1\u0161 Mikolov."
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "ECAL, pages 427\u2013431.",
            "year": 2017
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "In ICML 2022 Workshop on Knowledge Retrieval and Language Models",
            "year": 2022
        },
        {
            "authors": [
                "Guokun Lai",
                "Qizhe Xie",
                "Hanxiao Liu",
                "Yiming Yang",
                "Eduard Hovy."
            ],
            "title": "RACE: Large-scale reading comprehension dataset from examinations",
            "venue": "EMNLP, pages 785\u2013794.",
            "year": 2017
        },
        {
            "authors": [
                "J. Lin."
            ],
            "title": "Divergence measures based on the shannon entropy",
            "venue": "IEEE Transactions on Information Theory, 37(1):145\u2013151.",
            "year": 1991
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "ACL, pages 3428\u20133448.",
            "year": 2019
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "NAACL-HLT, pages 839\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Aakanksha Naik",
                "Abhilasha Ravichander",
                "Norman Sadeh",
                "Carolyn Rose",
                "Graham Neubig."
            ],
            "title": "Stress test evaluation for natural language inference",
            "venue": "COLING, pages 2340\u20132353.",
            "year": 2018
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Marco T\u00falio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with checklist",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S Gordon."
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "AAAI Spring.",
            "year": 2011
        },
        {
            "authors": [
                "Ivan Sanchez",
                "Jeff Mitchell",
                "Sebastian Riedel."
            ],
            "title": "Behavior analysis of nli models: Uncovering the influence of three factors on robustness",
            "venue": "NAACLHLT, pages 1975\u20131985.",
            "year": 2018
        },
        {
            "authors": [
                "Tal Schuster",
                "Darsh J Shah",
                "Yun Jie Serene Yeo",
                "Daniel Filizzola",
                "Enrico Santus",
                "Regina Barzilay."
            ],
            "title": "Towards debiasing fact verification models",
            "venue": "arXiv preprint arXiv:1908.05267.",
            "year": 2019
        },
        {
            "authors": [
                "Rishi Sharma",
                "James Allen",
                "Omid Bakhshandeh",
                "Nasrin Mostafazadeh."
            ],
            "title": "Tackling the story ending biases in the story cloze test",
            "venue": "ACL, pages 752\u2013757.",
            "year": 2018
        },
        {
            "authors": [
                "Siddarth Srinivasan",
                "Richa Arora",
                "Mark Riedl."
            ],
            "title": "A simple and effective approach to the story cloze test",
            "venue": "NAACL-HLT, pages 92\u201396.",
            "year": 2018
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "EMNLP, page 353.",
            "year": 2018
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Yadollah Yaghoobzadeh",
                "Remi Tachet",
                "TJ Hazen",
                "Alessandro Sordoni."
            ],
            "title": "Robust natural language inference models with example forgetting",
            "venue": "arXiv preprint arXiv:1911.03861.",
            "year": 2019
        },
        {
            "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Yanfei Dong",
                "Jiashi Feng."
            ],
            "title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "venue": "arXiv preprint arXiv:2002.04326.",
            "year": 2020
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Roy Schwartz",
                "Yejin Choi."
            ],
            "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
            "venue": "EMNLP, pages 93\u2013104.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Deep neural models have made remarkable strides in a broad spectrum of natural language understanding (NLU) tasks (Bowman et al., 2015; Wang et al., 2018; Mostafazadeh et al., 2016; Roemmele et al., 2011; Zellers et al., 2018). These tasks often employ a multiple-choice framework, as illustrated in Example 1. However, the inherent sensitivity of these models to minute variations calls for a robust and precise evaluation mechanism (Jurafsky et al., 2020).\nExample 1 Natural language inference in the SNLI dataset, with the correct answer bolded.\nPremise: A swimmer playing in the surf watches a low flying airplane headed inland.\n\u2217The corresponding author.\nHypothesis: Someone is swimming in the sea.\nLabel: a) Entailment. b) Contradiction. c) Neutral.\nIn tasks akin to Example 1, humans typically rely on the logical relationship between the premise and hypothesis. Contrarily, some NLP models might bypass this logical reasoning, focusing instead on the biases embedded within the dataset and, more specifically, within the hypotheses (Naik et al., 2018; Schuster et al., 2019). These biases\u2014such as sentiment or shallow n-grams\u2014could provide misleading cues for correct predictions.\nWe refer to these biases as \u201cartificial spurious cues\u201d when they pervade both the training and test datasets, maintaining a similar distribution over predictions. An example of such a cue is a model\u2019s disproportionate dependence on the word \u201csomeone\u201d in Example 1. These cues, when absent or altered, can significantly impair a model\u2019s performance, underlining the importance of identifying them to enhance model robustness in the future.\nTo tackle the issue of cues, it\u2019s crucial to distinguish between cues embedded in the dataset and those learned by the model. Conventional bias\ndetection and mitigation tools, such as the AI Fairness 360 toolkit (Bellamy et al., 2018), primarily target dataset biases, inadequately addressing those learned by models during training.\nWhile existing methods like \u201chypothesis-only\u201d tests and CheckList can uncover model vulnerabilities, they\u2019re not expressly designed to identify model-learned cues. \u201cHypothesis-only\u201d tests can highlight dataset issues where the hypothesis alone can provide a correct answer but fail to realistically depict the model\u2019s capabilities as they don\u2019t evaluate the model using the full data context used during both training and prediction.\nDrawing from the tenets of black-box testing in software engineering, CheckList scrutinizes model weaknesses without detailed knowledge of the model\u2019s internal architecture. It achieves this by delivering additional stress test cases premised on predefined linguistic features. However, CheckList\u2019s dependence on meticulously designed templates limits its scope, and it also falls short of illuminating the knowledge the model has actually gleaned from the data.\nTo address these limitations, we introduce ICQ (\u201cI-see-cue\u201d), a resilient statistical analysis framework 1 engineered to identify model-learned cues. Diverging from traditional methods, ICQ identifies biases in multiple-choice NLU datasets without necessitating additional test cases. Employing blackbox testing, ICQ assesses how models utilize these biases, delivering a comprehensive understanding of the bias in NLU tasks.\nWe authenticate ICQ\u2019s efficacy by deploying it on various NLU datasets to probe potential cues learned by models during training. ICQ facilitates an in-depth understanding of how models like ChatGPT 2 learn potential cues, and it offers illustrative examples to guide the selection of suitable prompts, providing invaluable guidance for model optimization.\nIn summary, this paper contributes the following:\n\u2022 We unveil ICQ, a lightweight yet potent method for identifying statistical biases and cues in NLU datasets, proposing simple and efficient tests to quantitatively and visually evaluate whether a model leverages spurious cues in its predictions.\n1The code and dataset are available at https:// github.com/flora336/icq\n2https://chat.openai.com/\n\u2022 We execute a comprehensive evaluation of statistical bias issues across ten popular NLU datasets and four models, corroborating previous findings and unveiling new insights. We also offer an online demonstration system to showcase the results and invite users to evaluate their own datasets and models.\n\u2022 Through a case study, we delve into how ChatGPT learns potential biases, offering valuable recommendations for its practical applications."
        },
        {
            "heading": "2 Preliminary",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "We define an instance x of an NLU task dataset X as\nx = (p, h, l) \u2208 X, (1)\nwhere p is the context against which to do the reasoning (p corresponds to \u201cpremise\u201d in Example 1); h is the hypothesis given the context p; l \u2208 L is the label that depicts the type of relation between p and h. The size of the relation set L varies with tasks."
        },
        {
            "heading": "2.2 Linguistic Features",
            "text": "As demonstrated in previous work (Naik et al., 2018; Jurafsky et al., 2020), we consider the following linguistic features:\nWord: The existence of a specific word in the premise or hypothesis of a dataset instance.\nSentiment: The sentiment value of an instance, calculated as the sum of sentiment polarities of individual words.\nTense: The tense feature (past, present, or future) of an instance, determined by the POS tag of the root verb.\nNegation: The existence of negative words (e.g.,\u201cno\u201d, \u201cnot\u201d, or\u201cnever\u201d) in an instance, determined by dependency parsing.\nOverlap: The existence of at least one word (excluding stop words) that occurs in both the premise and hypothesis.\nNER: The presence of named entities (e.g., PER, ORG, LOC, TIME, or CARDINAL) in an instance, detected using the NLTK NER toolkit.\nTypos: The presence of at least one typo in an instance, identified using a pretrained spelling model.\nFor multiple-choice datasets, all features except Overlap are applied exclusively to hypotheses."
        },
        {
            "heading": "3 Approach",
            "text": "The ICQ framework, depicted in Figure 2, consists of three phases: data extraction, cue discovery, and model probing. In the data extraction phase, instances containing a specific linguistic feature f are extracted from the dataset. The cue discovery phase identifies potential cues among pre-defined features. Finally, the model probing phase conducts two tests: the \u201caccuracy test\u201d and \u201cdistribution test\u201d. We will discuss these phases in more detail below."
        },
        {
            "heading": "3.1 Data Extraction Phase",
            "text": "After defining the linguistic features, our system\u2019s fundamental step is constructing a data extractor for each feature value f . An extractor processes a dataset and retrieves a set of instances associated with the specified feature value. More specifically, for a specific feature, if an instance contains that feature, then that instance will be singled out as a part of the subset bearing that feature."
        },
        {
            "heading": "3.2 Cue Discovery Phase",
            "text": "For each feature f , we apply its extractor to both the training and test data of dataset X , denoted as R and S in Figure 2. This results in clustered subsets of training instances (Rf ) and test instances (Sf ). A feature is considered a possible cue for a dataset only if it is present in both the training and test data.\nThe bias of the label distribution for an extracted set is computed using the mean squared error\n(MSE) and Jensen-Shannon Divergence (JSD) (Lin, 1991). The cueness score indicates the extent to which dataset X is biased against a feature f .\nMSE(F ) = 1 |L| \u2211 i (yi \u2212 yi)2 (2)\nHere, yi represents the number of instances with label li in the extracted dataset F , and yi is the mean number of instances for each label. A larger MSE(F ) implies a more pointed label distribution and greater bias. If the extracted training set (Rf ) and the extracted test set (Sf ) exhibit similar biases, the JSD between their distributions will be small:\nJSD = 1\n2 (Q(Rf ) \u2225 A) +\n1 2 (Q(Sf ) \u2225 A) , (3)\nwhere A = 12 (Q(Rf ) +Q(Sf )). The function Q() denotes the label distribution of the extracted dataset. We define the cueness score as:\ncue(f,X) = MSE(Rf )\nexp(JSD(Rf , Sf )) (4)\nThis cueness score represents the degree to which a dataset X is biased against a feature f ."
        },
        {
            "heading": "3.3 Model Probing Phase",
            "text": "In the previous section, we established that a dataset X could be influenced by a cue f . However, a model trained on this dataset may not necessarily exploit that cue, as models depend on both data and architecture. In this section, we introduce a framework to probe any model instance trained on the biased dataset, assessing if it utilizes cue f and to what degree. We achieve this through two tests: the accuracy test and the distribution test."
        },
        {
            "heading": "3.3.1 Accuracy Test",
            "text": "The accuracy test examines the model\u2019s performance on data subsets with and without specific features. By comparing the model\u2019s accuracy on these two subsets, we can understand the model\u2019s generalization capabilities under different conditions. If the model\u2019s performance shows a noticeable improvement on the subset containing a specific feature, it may suggest that the model has exploited that feature for prediction.\nIn the accuracy test, we assess the prediction accuracies of the model M on the extracted test set (with feature f ) and on the remaining test set (without feature f ), denoted as acc(Sf ) and acc(Snf ),\nrespectively. The accuracy test computes the difference between these two accuracies:\n\u2206Acc(f) = acc(Sf )\u2212 acc(Snf ) (5)\nA positive or negative value of \u2206Acc indicates the direction of the model\u2019s performance change when comparing its accuracy on data subsets with and without specific features. A positive value suggests that the model exploits the feature for prediction, while a negative value implies struggles to generalize or detrimental sensitivity to the feature.\nThe magnitude of the absolute value of \u2206Acc reflects the degree to which a model\u2019s performance is affected by the presence or absence of specific features in the data subsets. A larger absolute value indicates a stronger reliance on or sensitivity to the feature, whereas a smaller absolute value suggests a more robust model that is less affected by the presence or absence of the feature."
        },
        {
            "heading": "3.3.2 Distribution Test",
            "text": "The distribution test is a visual examination that focuses on how changes in specific feature distributions within datasets affect a model\u2019s predictive performance.\nFirst, we create a \u201cstress dataset\u201d Sf by \u201cflattening\u201d the label distribution in Sf . We achieve this by removing random instances from all labels except for the one with the smallest number of instances, stopping when a balance is reached. In other words, we retain the minimum number of instances present in each label, while randomly discarding the excess instances. This approach effectively eliminates bias in the extracted test set, and challenges the model.\nNext, we apply the model to the stress test set and obtain prediction results. We then compare the label distribution of the prediction results on the stress test set with the label distribution of the extracted training data (Rf ). The rationale is that if the extracted training data contains a cue, its label distribution will be skewed towards a specific label. If the model exploits this cue, it will prefer to predict that label as much as possible, amplifying the skewness of the distribution, despite the input test set being neutralized. We aim to observe such amplification in the output distribution to identify the model\u2019s weaknesses.\nIn summary, the accuracy test and distribution test are related in terms of assessing a model\u2019s sensitivity to specific features, but they emphasize different aspects. Distribution testing focuses on\nthe impact of feature distribution changes on model performance, while accuracy testing evaluates the model\u2019s performance on data subsets with and without specific features. By combining these two testing methods, a model\u2019s sensitivity to particular features can be more comprehensively assessed. If both tests determine that the model is sensitive to a certain feature, we can have a higher degree of confidence in this conclusion."
        },
        {
            "heading": "4 Evaluation",
            "text": "We first present the experimental setup, followed by results on cue discovery, model probing, and analysis. The entire framework is implemented in an online demo."
        },
        {
            "heading": "4.1 Setup",
            "text": "We evaluate this framework on 10 popular NLR datasets in Table 1 and 4 well-known models, namely FASTText (FT) (Joulin et al., 2017), ESIM (ES) (Chen et al., 2016), BERT (BT) (Devlin et al., 2018) and RoBERTA (RB) (Liu et al., 2019) on these datasets. All these datasets except for SWAG (Zellers et al., 2018) and RECLOR (Yu et al., 2020) are collected through crowdsourcing. SWAG is generated from an LSTM-based language model. Specifications of the datasets are listed in Table 1.\nThese datasets can mainly be classified into two types of tasks. SNLI, QNLI, and MNLI (Williams et al., 2018) are classification tasks, while ROC, COPA (Roemmele et al., 2011), SWAG, RACE (Lai et al., 2017), RECLOR, ARCT (Habernal et al., 2017) and ARCT_adv (Schuster et al., 2019) are multiple-choice reasoning tasks. Features appearing a minimum of five times in training\nor testing sets are considered cues."
        },
        {
            "heading": "4.2 Cues in Datasets",
            "text": "In this section, we showcase the cues identified in each dataset using the cueness metric, as described in Section 3.\nWe first filter the training and test data for each dataset using all the features defined in this paper. The left half of Table 2 displays the top 5 cues discovered for each of the 10 datasets, along with their cueness scores. ARCT_adv, an adversarial dataset, is intentionally well-balanced. Consequently, we only found one cue, OVERLAP, with a very low cueness score. This is unsurprising since OVERLAP is the only \u201csecond-order\u201d feature in our list of linguistic features that considers tokens in both the premise and hypothesis, and likely evaded data manipulation by the creator.\nMostly, the top 5 cues discovered are word features. However, besides OVERLAP, we also see NEGATION and TYPO appearing in the lists. In fact, SENTIMENT and NER features would have emerged if we expanded the list to the top 10. Interestingly, several features previously reported as biased by other works, such as \u201cnot\u201d and NEGATION in ARCT, \u201cno\u201d in MNLI and SNLI, and \u201clike\u201d in ROC, are also found. Particularly in MNLI, all five discovered cues are related to negatively toned words, suggesting significant human artifacts in this dataset that can lead to model fragility.\nAdditionally, we observe that some word cues are indicative of certain syntactic, semantic, or sentiment patterns in the questions. For example, \u201cbecause\u201d in SNLI implies a cause-effect structure; \u201clike\u201d in ROC indicates positive sentiment; \u201cprobably\u201d and \u201cmay\u201d in RACE suggest uncertainty, and so on. These features can serve as clues for revising datasets."
        },
        {
            "heading": "4.3 Biases in Models",
            "text": "To investigate whether a model is affected by a specific cue or feature in a dataset, we train four models on their original training sets and evaluate them using accuracy and distribution tests.\nAccuracy Test: The results are presented in Table 2. As mentioned in Section 3.3.1, a positive or negative \u2206 value indicates the direction of a model\u2019s performance change when comparing its accuracy on data subsets with and without specific features. The absolute value of \u2206 reflects the degree to which the model\u2019s performance is\ninfluenced by these features, with larger values suggesting stronger reliance or sensitivity and smaller values indicating a more robust model.\nThe bottom of Table 2 shows that, across all 10 datasets, the sum of the absolute values of \u2206 follows the order: RoBERTA < BERT < ESIM < FastText. This is consistent with earlier hypothesisonly tests and the community\u2019s common perception of these popular models. However, examining individual datasets and features reveals a more nuanced situation. For instance, FastText tends to pick up individual word cues rather than semantic cues, while more complex models such as BERT and RoBERTA appear more sensitive to structural features like NEGATION and SENTIMENT, which are actually classes of words. This is well explained by FastText\u2019s design, which focuses more on modeling words than syntactic or semantic structures.\nInterestingly, FastText exhibits a strong negative correlation with TYPO. We speculate that FastText might have been trained with a more orthodox vocabulary, making it less tolerant of typos in the text.\nDistribution Test: We highlight three interesting findings in Figure 3. The bars for the four models represent the distribution percentage based on each predicted label. Rf denotes the extracted training data distribution with a specific feature. We observe that all models on the cue \u201cno\u201d in MNLI achieve positive \u2206 in Table 2, particularly FastText. Consistent with the \u201cAccuracy Test,\u201d we find that the prediction label distribution skewness\nis amplified in Figure 3 for FastText and ESIM. With the \u201cno\u201d cue, they prefer to predict \u201cContradiction\u201d even more than the ground truth in the training data. In contrast, BERT and RoBERTA moderately follow the training data. While the cue \u201cno\u201d is effective at influencing the models, the cue \u201cabove\u201d is not as successful. Figure 3 shows that the distribution of predicted results for ESIM in ARCT is entirely opposite to the training data, explaining the \u2206 = \u22128.43 in Table 2 and demonstrating that models may not exploit a cue even if it is present in the data. Similarly, \u201cspeaking\u201d in BERT and RoBERTA can also explain their low \u2206 values, which are not shown in Table 2.\nThe example of the cue \u201cthrew\u201d presents an out-\nlier for BERT, as the distribution test result is inconsistent with the accuracy test: the accuracy deviation is very high for BERT, but its prediction distribution is flat. We have not encountered many such contradictory cases. However, when they do occur, as in this example, we give BERT the benefit of the doubt that it might not have exploited the cue \u201cthrew\u201d."
        },
        {
            "heading": "5 Case Study",
            "text": "Recently, ChatGPT, a large language model (LLM) released by OpenAI, has garnered significant interest from the NLP community. ChatGPT, a GPT-\nx 3 series model, is trained through Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) similarly to InstructGPT (Ouyang et al., 2022). In this section, we investigate if zeroshot ChatGPT is influenced by bias features, using a case study focused on the word \u201cno\u201d in the MNLI dataset. We aim to compare the effectiveness of different prompts and select the best one for mitigating bias based on a single bias feature."
        },
        {
            "heading": "5.1 Dataset",
            "text": "We selected test instances from the MNLI dataset to study the influence of the word \u201cno\u201d on ChatGPT\u2019s performance. The original test set has Contradiction: 3240, Entailment: 3463, and Neutral: 3129 instances. Instances containing \u201cno\u201d are distributed as follows: Contradiction:229, Entailment: 38, and Neutral: 46.\nFor the accuracy test, we used all 313 instances with \u201cno\u201d and an equal number of instances without \u201cno\u201d, randomly chosen from the remaining test set. This ensures a balanced evaluation of ChatGPT\u2019s performance.\nFor the distribution test, we selected 38 instances per label containing \u201cno\u201d, resulting in a total of 114 instances."
        },
        {
            "heading": "5.2 Prompts",
            "text": "We have four prompts in Figure 4: The first prompt is proposed by ChatGPT itself. We ask, \u201cWhat is the best prompt for the MNLI task according to you?\u201d. ChatGPT returns prompt 1 for us. The sec-\n3Currently, x is either 3.5 or 4, and in the subsequent experimental process, the ChatGPT we use is based on GPT3.5.\nond prompt is inspired by previous work (Qin et al., 2023). The third and fourth prompts are created by adding \u201cLet\u2019s think step by step\u201d (Kojima et al.) to prompt 1 and prompt 2 with the \u201cchain of thought\u201d (CoT) thinking, respectively. This modification has been shown to significantly improve the performance of InstructGPT on reasoning tasks (Ouyang et al., 2022)."
        },
        {
            "heading": "5.3 ICQ Results",
            "text": "We evaluated the model\u2019s accuracy using different prompts on instances with and without the word \u201cno.\u201d The results are shown in Table 3. P1 demonstrates a negative \u2206 Acc, indicating difficulty in generalizing when \u201cno\u201d is present. P2 exhibits a positive \u2206 Acc, suggesting better generalization.\nAdding \u201cCoT\u201d to both prompts reduces bias risk. P1 + CoT shows the most significant improvement in Acc (\u201cno\u201d), but P2 + CoT has the smallest absolute \u2206 Acc, indicating the least sensitivity to \u201cno\u201d and the lowest bias risk among the tested prompts.\nBesides, we analyzed the model\u2019s prediction distribution Figure 5 for the different prompts on the stress test set containing the word \u201cno\u201d with balanced label distribution. Distribution test results reveal imbalances in P1 and P2 prediction distributions, with P1 and P2 leaning towards predicting contradictions. Adding \u201cCoT\u201d mitigates these imbalances, leading to more balanced distributions. P2 + CoT presents the most balanced distribution among the labels, supporting its lowest bias risk.\nIn conclusion, our case study, particularly when focusing on the feature \u201cno\u201d, indicates that zeroshot ChatGPT can be influenced by bias features. The choice of prompt significantly affects its performance. When assessing the feature \u201cno\u201d, P2 + CoT showed the lowest bias risk across both tests. The \u201cCoT\u201d strategy appears effective in reducing bias risk for this specific feature. However, it\u2019s important to note that our findings, particularly regarding the feature \u201cno\u201d, suggest that ChatGPT\u2019s self-recommended prompt (P1) might not always be optimal. This underscores the importance of human intervention and ongoing exploration to optimize performance and minimize bias risks. Future studies and conclusions would benefit from a more nuanced and feature-specific analysis."
        },
        {
            "heading": "6 Related Work",
            "text": "Our work is related to three research directions: spurious features analysis, bias calculation, and dataset filtering.\nSpurious features analysis has been increasingly studied recently. Much work (Sharma et al., 2018; Srinivasan et al., 2018; Zellers et al., 2018) has observed that some NLP models can surprisingly get good results on natural language understanding questions in MCQ form without even looking at the stems of the questions. Such tests are called \u201chypothesis-only\u201d tests in some works. Further, some research (Sanchez et al., 2018) discovered that these models suffer from insensitivity to certain small but semantically significant alterations in the hypotheses, leading to speculations that the hypothesis-only performance is due to simple statistical correlations between words in the hypothesis and the labels. Spurious features can be\nclassified into lexicalized and unlexicalized (Bowman et al., 2015): lexicalized features mainly contain indicators of n-gram tokens and cross-ngram tokens, while unlexicalized features involve word overlap, sentence length, and BLEU score between the premise and the hypothesis. (Naik et al., 2018) refined the lexicalized classification to Negation, Numerical Reasoning, Spelling Error. (McCoy et al., 2019) refined the word overlap features to Lexical overlap, Subsequence, and Constituent which also considers the syntactical structure overlap. (Sanchez et al., 2018) provided unseen tokens an extra lexicalized feature.\nBias calculation is concerned with methods to quantify the severity of the cues. Some work (Clark et al., 2019; He et al., 2019; Yaghoobzadeh et al., 2019) attempted to encode the cue feature implicitly by hypothesis-only training or by extracting features associated with a certain label from the embeddings. Other methods compute the bias by statistical metrics. For example, (Yu et al., 2020) used the probability of seeing a word conditioned on a specific label to rank the words by their biasness. LMI (Schuster et al., 2019) was also used to evaluate cues and re-weight in some models. However, these works did not give the reason to use these metrics, one way or the other. Separately, (Ribeiro et al., 2020) gave a test data augmentation method, without assessing the degree of bias.\nDataset filtering is one way of achieving higher quality in datasets by reducing artifacts. In fact, datasets such as SWAG and RECLOR evaluated in this paper were produced using variants of this filter approach which iteratively perturb the data instances until a target model can no longer fit the resulting dataset well. Some methods (Yaghoobzadeh et al., 2019), instead of preprocessing the data by removing biases, leave out samples with biases in the middle of training according to the decision made between epoch to epoch. (Bras et al., 2020) investigated model-based reduction of dataset cues and designed an algorithm using iterative training. Any model can be used in this framework. Although such an approach is more general and more efficient than human annotating, it heavily depends on the models. Unfortunately, different models may catch different cues. Thus, such methods may not be complete."
        },
        {
            "heading": "7 Conclusion",
            "text": "Our lightweight framework, ICQ, identifies biases and cues in multiple-choice NLU datasets, illuminating model behaviors from a statistical perspective. Extensive experimentation on diverse tasks validates ICQ\u2019s efficiency in revealing dataset and model biases. Using a case study on ChatGPT, we explore its cues, offering practical guidance. ICQ advances our understanding and optimization of large language models, promoting the creation of robust, unbiased AI systems."
        }
    ],
    "title": "Statistically Profiling Biases in Natural Language Reasoning Datasets and Models",
    "year": 2023
}