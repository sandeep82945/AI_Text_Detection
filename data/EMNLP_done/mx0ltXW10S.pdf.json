{
    "abstractText": "As a precious cultural heritage of human beings, classical Chinese poetry has a very unique writing style and often contains special words that rarely appear in general Chinese texts, posting critical challenges for natural language processing. Little effort has been made in the literature for processing texts from classical Chinese poetry. This study fills in this gap with TopWORDS-Poetry, an unsupervised method that can achieve reliable text segmentation and word discovery for classical Chinese poetry simultaneously without pre-given vocabulary or training corpus. Experimental studies confirm that TopWORDS-Poetry can successfully recognize unique poetry words, such as named entities and literary allusions, from metrical poems of\u300a\u5168\u5510\u8bd7\u300b(Complete Tang Poetry) and segment these poetry lines into sequences of meaningful words with high quality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Changzai Pan"
        },
        {
            "affiliations": [],
            "name": "Feiyue Li"
        },
        {
            "affiliations": [],
            "name": "Ke Deng"
        }
    ],
    "id": "SP:783aa8f440d7f5af6cb981367f8d40f9d92b31b0",
    "references": [
        {
            "authors": [
                "Manex Agirrezabal",
                "I\u00f1aki Alegria",
                "Mans Hulden."
            ],
            "title": "Machine learning for metrical analysis of English poetry",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 772\u2013781, Osaka,",
            "year": 2016
        },
        {
            "authors": [
                "Munef Abdullah Ahmed",
                "Stefan Trausan-Matu."
            ],
            "title": "Using natural language processing for analyzing Arabic poetry rhythm",
            "venue": "2017 16th RoEduNet Conference: Networking in Education and Research (RoEduNet), pages 1\u20135. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Patrick J Burns",
                "James A Brofos",
                "Kyle Li",
                "Pramit Chaudhuri",
                "Joseph P Dexter."
            ],
            "title": "Profiling of intertextuality in Latin literature using word embeddings",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Wanxiang Che",
                "Yunlong Feng",
                "Libo Qin",
                "Ting Liu."
            ],
            "title": "N-LTP: An open-source neural language technology platform for Chinese",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Y Chen."
            ],
            "title": "Metrical structure: evidence from Chinese poetry",
            "venue": "Linguistic Inquiry, 10(3):371\u2013 420.",
            "year": 1979
        },
        {
            "authors": [
                "Arthur P Dempster",
                "Nan M Laird",
                "Donald B Rubin."
            ],
            "title": "Maximum likelihood from incomplete data via the EM algorithm",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1\u201322.",
            "year": 1977
        },
        {
            "authors": [
                "Ke Deng",
                "Peter K. Bol",
                "Kate J. Li",
                "Jun S. Liu."
            ],
            "title": "On the unsupervised analysis of domain-specific Chinese texts",
            "venue": "Proceedings of the National Academy of Sciences, 113(22):6154\u20136159.",
            "year": 2016
        },
        {
            "authors": [
                "Bradley Efron",
                "Robert J Tibshirani."
            ],
            "title": "An introduction to the bootstrap",
            "venue": "CRC press.",
            "year": 1994
        },
        {
            "authors": [
                "Alex Chengyu Fang",
                "Fengju Lo",
                "Cheuk Kit Chinn."
            ],
            "title": "Adapting NLP and corpus analysis techniques to structured imagery analysis in classical Chinese poetry",
            "venue": "Proceedings of the Workshop on Adaptation of Language Resources and Technology to New",
            "year": 2009
        },
        {
            "authors": [
                "Shengli Feng."
            ],
            "title": "Prosodic syntax in Chinese: theory and facts",
            "venue": "Routledge.",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Gelman",
                "John B Carlin",
                "Hal S Stern",
                "David B Dunson",
                "Aki Vehtari",
                "Donald B Rubin."
            ],
            "title": "Bayesian data analysis",
            "venue": "CRC press.",
            "year": 2013
        },
        {
            "authors": [
                "Sharon Goldwater",
                "Thomas L. Griffiths",
                "Mark Johnson."
            ],
            "title": "A Bayesian framework for word segmentation: Exploring the effects of context",
            "venue": "Cognition, 112(1):21\u201354.",
            "year": 2009
        },
        {
            "authors": [
                "Irving John Good",
                "Ian Hacking",
                "RC Jeffrey",
                "H\u00e5kan T\u00f6rnebohm."
            ],
            "title": "The estimation of probabilities: An essay on modern Bayesian methods",
            "venue": "Synthese, 16(2).",
            "year": 1966
        },
        {
            "authors": [
                "Xia Li",
                "Bin Wu",
                "Bailing Zhang."
            ],
            "title": "Unknown word detection in Song poetry",
            "venue": "2016 IEEE First International Conference on Data Science in Cyberspace (DSC), pages 544\u2013549. IEEE.",
            "year": 2016
        },
        {
            "authors": [
                "Chao-Lin Liu."
            ],
            "title": "Onto word segmentation of the complete Tang poems",
            "venue": "arXiv preprint arXiv:1908.10621.",
            "year": 2019
        },
        {
            "authors": [
                "James JY Liu."
            ],
            "title": "The art of Chinese poetry",
            "venue": "University of Chicago Press.",
            "year": 1966
        },
        {
            "authors": [
                "Ruixuan Luo",
                "Jingjing Xu",
                "Yi Zhang",
                "Xuancheng Ren",
                "Xu Sun."
            ],
            "title": "PKUSEG: A toolkit for multi-domain Chinese word segmentation",
            "venue": "CoRR, abs/1906.11455.",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Manning",
                "Mihai Surdeanu",
                "John Bauer",
                "Jenny Finkel",
                "Steven Bethard",
                "David McClosky."
            ],
            "title": "The Stanford CoreNLP natural language processing toolkit",
            "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Daichi Mochihashi",
                "Takeshi Yamada",
                "Naonori Ueda."
            ],
            "title": "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint",
            "year": 2009
        },
        {
            "authors": [
                "Maria Moritz",
                "Andreas Wiederhold",
                "Barbara Pavlek",
                "Yuri Bizzoni",
                "Marco B\u00fcchler."
            ],
            "title": "Non-literal text reuse in historical texts: An approach to identify reuse transformations and its application to bible reuse",
            "venue": "Proceedings of the 2016 Conference on",
            "year": 2016
        },
        {
            "authors": [
                "Changzai Pan",
                "Maosong Sun",
                "Ke Deng."
            ],
            "title": "TopWORDS-seg: Simultaneous text segmentation and word discovery for open-domain Chinese texts via Bayesian inference",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Pearl",
                "Sharon Goldwater",
                "Mark Steyvers."
            ],
            "title": "Online learning mechanisms for Bayesian models of word segmentation",
            "venue": "Research on Language and Computation, 8:107\u2013132.",
            "year": 2010
        },
        {
            "authors": [
                "Donald Sturgeon."
            ],
            "title": "Unsupervised identification of text reuse in early Chinese literature",
            "venue": "Digital Scholarship in the Humanities, 33(3):670\u2013684.",
            "year": 2018
        },
        {
            "authors": [
                "Junyi Sun"
            ],
            "title": "Jieba Chinese word segmentation tool",
            "year": 2012
        },
        {
            "authors": [
                "Maosong Sun",
                "Xinxiong Chen",
                "Kaixu Zhang",
                "Zhipeng Guo",
                "Zhiyuan Liu"
            ],
            "title": "THULAC: An efficient lexical analyzer for Chinese",
            "year": 2016
        },
        {
            "authors": [
                "Wai-lim Yip."
            ],
            "title": "Chinese poetry, revised: an anthology of major modes and genres",
            "venue": "Duke University Press.",
            "year": 1997
        },
        {
            "authors": [
                "Zheng Yuan",
                "Yuanhao Liu",
                "Qiuyang Yin",
                "Boyao Li",
                "Xiaobin Feng",
                "Guoming Zhang",
                "Sheng Yu."
            ],
            "title": "Unsupervised multi-granular Chinese word segmentation and term discovery via graph partition",
            "venue": "Journal of Biomedical Informatics, 110:103542.",
            "year": 2020
        },
        {
            "authors": [
                "Pan"
            ],
            "title": "Formula (3), however, specifies for a mixture prior distribution with multiple components to reflect the fact that each poetry line can potentially follow multiple segmentation patterns",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Expressing complicated thoughts and feelings in refined language, poetry is the most shining pearl on the crown of human literature. As a special literary style, poetry has unique characteristics in rhythm, metre and language use, making it a critical challenge to process poetry texts via algorithms (Agirrezabal et al., 2016; Fang et al., 2009; Ahmed and Trausan-Matu, 2017).\nRising in the Spring and Autumn Period (770- 476 B.C.) and flourishing in the Tang Dynasty (618- 907 A.D.), classical Chinese poetry has a long history and profound cultural heritage. Composed of nearly 50,000 poems by over 2000 poets of the Tang Dynasty,\u300a\u5168\u5510\u8bd7\u300b(Complete Tang Poetry, or CTP) displayed in Fig 1 (a) is the ideal object for studying classical Chinese poetry. Fig 1 (b) and (c) visualize the 200 poets with the most poems and the 300 most frequent Chinese characters in CTP.\nAmong the nearly 50,000 poems in CTP, 34,227 are metrical poems that have recognizable patterns\n* Corresponding author.\nof syllables. Containing 1.3 million Chinese characters in total, the collection of metrical poems in CTP (referred to as P hereinafter) is the most essence of Tang poetry and our focus in this study. A typical metrical poem in CTP is composed of either 4 or 8 lines, each of which contains either 5 or 7 Chinese characters, and can be classified into 4 categories P4\u00d75, P4\u00d77, P8\u00d75 and P8\u00d77, where Pm\u00d7n represents the poems with m lines and n Chinese characters in each line. In P , there are 3,225 poems in P4\u00d75, 9,820 poems in P4\u00d77, 13,536 poems in P8\u00d75, and 7,646 poems in P8\u00d77. Fig 1 (d)-(g) show 4 famous poems (each for one of the 4 categories) by the famous poet Du Fu (\u675c\u752b, 712-770 A.D.) of the Tang Dynasty, with the word boundaries (highlighted by symbol \u2018/\u2019) manually labelled by domain experts. For the majority of poems in CTP, however, no word boundaries are labelled yet.\nThe absence of word boundaries posts critical challenges in literary study of classical Chinese poetry at a large scale, and leads to a great appeal to develop efficient algorithms to achieve reliable text segmentation and word discovery for poetry texts(Li et al., 2016; Liu, 2019). However, such a task is very challenging, because classical Chinese poetry is composed of special domain-specific texts with many unique features.\nFirst, classical Chinese poetry has a very unique writing style and often contains special words that rarely appear in general Chinese texts, such as unique named entities and literary allusions. Moreover, to express complicated thoughts and feelings with few words, poets were often forced to compress long phrases into shorter \u201cpseudo words\u201d for putting them into short poetry lines. Although these \u201cpseudo words\u201d are phrases instead of words from the grammatical point of view, many of them are treated as \u201cpoetry words\u201d in practice because they have specific meanings and function as words (Liu, 1966; Li et al., 2016). Due to these facts, word discovery becomes a critical problem in processing\nclassical Chinese poetry.\nSecond, to keep proper rhythm, text segmentation of classical Chinese poetry lines follows a few major segmentation patterns. Previous studies have revealed that 5-character poetry lines mainly follow two segmentation patterns 2-1-2 and 2-2-1, while 7-character poetry lines mainly follow another two segmentation patterns 2-2-1-2 and 2-2-2-1 (Chen, 1979; Yip, 1997; Feng, 2019; Liu, 2019). In this study, we refer to these 4 major segmentation patterns as \u03f1212, \u03f1221, \u03f12212 and \u03f12221, respectively. Apparently, \u03f12212 and \u03f1212 correspond to the same segmentation pattern for poetry lines of different lengths, and form a general segmentation pattern \u03f1\u221712. Similarly, \u03f12221 and \u03f1221 form another general segmentation pattern \u03f1\u221721. To investigate the\nproportion of these segmentation patterns in P , we randomly sampled 1,000 poems from P (referred to as P1000) by stratification sampling (i.e., 250 poems for each of the 4 poem types in P), and manually segmented them as the demonstrating examples shown in Fig 1 (d)-(g). Figure 1 (h) summarizes the obtained results, confirming that most poetry lines indeed precisely follow these 4 segmentation patterns.\nThird, on the other hand, however, about 10% of poetry lines violate these major segmentation patterns due to appearance of long words containing more than 2 Chinese characters, such as named entities and literary allusions. For example, in the 4 poems shown in Fig 1 (d)-(g), there are 5 poetry lines that violate the 4 major segmentation\npatterns because of long words with 3 Chinese characters. Due to this phenomenon, text segmentation becomes a non-trivial issue.\nThese unique features of poetry texts make it inappropriate to directly apply popular tools for processing general Chinese texts based on supervised learning, such as Jieba (Sun, 2012), StanfordNLP (Manning et al., 2014), THULAC (Sun et al., 2016), PKUSEG (Luo et al., 2019), and LTP (Che et al., 2021), to process classical Chinese poetry. As we will show in the result Section, due to the lack of training for poetry texts, these methods suffer from serious performance degradation in text segmentation, and tend to miss many poetry words because of the limited ability in discovering unregistered words. And, it is not realistic to tune these tools for this specific task, because no qualified training corpus that contains enough well-segmented classical Chinese poetry lines is publicly available yet, to the best of our knowledge. Some methods based on unsupervised learning, e.g., TopWORDS (Deng et al., 2016), GTS (Yuan et al., 2020) and others (Goldwater et al., 2009; Mochihashi et al., 2009; Pearl et al., 2010), are strong in discovering unregistered words, but suffer from poor performance on text segmentation in the current scenario because they do not take the special features of poetry texts into consideration.\nAs pointed out by Pan et al. (2022), when processing domain-specific Chinese texts like poetry that potentially contain many unregistered words, word discovery and text segmentation become two critically important tasks entangled with each other. Most existing methods for Chinese text segmentation work under the support of a pre-loaded vocabulary, either explicitly given apriori or implicitly encoded in a pre-trained large language model. If the pre-loaded vocabulary covers the target corpus well, these methods typically perform reasonably well; once the underlying vocabulary of the target corpus exceeds the pre-given vocabulary seriously, however, we often observe significant performance degeneration of these methods, as demonstrated in Pan et al. (2022). Because classical Chinese poetry contains many unique named entities and literary allusions that never appear in general texts and follow specific segmentation templates, methods without considering these features tend to miss many poetry words and segment poetry lines in an inappropriate manner.\nIn the literature, few methods have been pro-\nposed for processing classical Chinese poetry. Li et al. (2016) proposed to recognize meaningful words in classical Chinese poetry by filtering out fake words from word candidates via a modified support vector machine trained by manually labelled training samples. This method relies heavily on manually labelled samples to capture features of poetry words, and completely ignores the segmentation patterns of poetry lines. Liu (2019) proposed to formulate the text segmentation problem of a poetry line into a pattern selection problem under the assumption that every poetry line must precisely follow one of the two general segmentation patterns \u03f1\u221721 or \u03f1\u221712, and achieve text segmentation by selecting the plausible pattern based on only the last three characters of each poetry line via weighted point-wise mutual information. Although such a method takes good advantage of the segmentation patterns and performs reasonably well, it oversimplifies the problem via a restrictive assumption and tends to miss all named entities and literary allusions with more than 2 Chinese characters.\nTo better resolve this challenging problem, we need an algorithm that can wisely utilize the segmentation patterns of poetry lines, is flexible enough to deal with situations where the regular segmentation patterns are violated, and has strong word discovery ability to recognize special poetry words with little training information. In this study, we fill in this gap with TopWORDS-Poetry, an algorithm with all these desired features. Extending the Bayesian framework of TopWORDS-Seg proposed by Pan et al. (2022) to a more sophisticated hierarchical Bayesian model to reflect the constraints on text segmentation due to the major segmentation patterns, TopWORDS-Poetry is able to achieve effective text segmentation and word discovery simultaneously for classical Chinese poems with metrical patterns.\nAn experiment study on metrical poems in CTP confirms that TopWORDS-Poetry can correctly segment these poetry texts, with various named entities, literary allusions and special poetry words effectively recognized. Compared to existing methods in the literature, TopWORDS-Poetry is superior on processing classical Chinese poetry with a significant performance margin. The outputs of TopWORDS-Poetry may promote the applications of AI approaches in the study of classical Chinese literature and inspire more research efforts to study classical Chinese poetry via the paradigm of digital\nhumanity."
        },
        {
            "heading": "2 TopWORDS-Poetry",
            "text": "Proposed by Pan et al. (2022), TopWORDS-Seg is an effective method for processing open-domain Chinese texts that can achieve high-quality text segmentation and word discovery simultaneously. Modelling the target texts by the uni-gram language model with an over-complete initial dictionary D obtained by enumerating all qualified word candidates, TopWORDS-Seg utilizes the word boundaries suggested by a pre-given text segmenter (e.g., PKUSEG) as prior information to guide usage frequency estimation and pruning of word candidates in D under a Bayesian framework, and finally achieves text segmentation based on the pruned word dictionary with respect to the prior word boundaries.\nAs demonstrated by Pan et al. (2022), the Bayesian framework of TopWORDS-Seg integrates the advantages of a strong word discoverer TopWORDS and an effective text segmenter PKUSEG, leading to a stronger word discoverer and a more robust text segmenter for open-domain Chinese texts.\nIn this section, we extend the Bayesian framework of TopWORDS-Seg with a more flexible way to utilize prior information on text segmentation. Instead of using the word boundaries suggested by a pre-given text segmenter, e.g., PKUSEG, as prior information to guide learning, we encode the constraints on text segmentation due to various segmentation patterns (i.e., \u03f1\u221712 and \u03f1\u221721) into a more sophisticated prior distribution, coming up with a hierarchical Bayesian model to infer the unknown segmentation pattern of each poetry line."
        },
        {
            "heading": "2.1 The Bayesian Framework",
            "text": "Following the notations in Pan et al. (2022), we let T = {T1, \u00b7 \u00b7 \u00b7 , Tn} be the unsegmented Chinese poetry lines of interest, A = {a1, \u00b7 \u00b7 \u00b7 , aM} be the set of Chinese characters that appears in T , and DT be the underlying vocabulary of T . For each poetry line Tj of length Lj , let Bj = (bj1, . . . , bjLj ) be its word boundary profile, where bjl = 1 means there is a word boundary behind the lth character of Tj , and bjl = 0 otherwise. Apparently, Bj determines how Tj is segmented. Our goal is to learn the unknown vocabulary DT and word boundary profiles B = {B1, \u00b7 \u00b7 \u00b7 , Bn} from T .\nTo discover DT , we first construct an over-\ncomplete initial dictionary D = {w1, w2, . . . , wN} by enumerating all qualified word candidates in T . This can be achieved in three steps: first, we enumerate all possible sub-strings in T whose length \u2264 \u03c4L and frequency \u2265 \u03c4F , as suggested by TopWORDS, to form a collection of word candidates denoted as DE ; second, we segment each poetry line Tj according to the two general segmentation patterns \u03f1\u221721 and \u03f1\u221712 respectively to get a collection of Tj-specific word candidates Dj , and assemble them into an additional collection of word candidates DP = \u222anj=1Dj ; finally, we unite DE and DP to get D = DE \u222a DP . Throughout this paper, we assume that the initial dictionary D generated in this way covers the underlying vocabulary DT of interest, i.e., DT \u2286 D.\nDefine \u03b8 = {\u03b8w}w\u2208D, where \u03b8w is the usage probability of word candidate w in T . Under the uni-gram language model with D as the vocabulary, we have the following likelihood function for poetry lines in T when D, B and \u03b8 are given:\nP(T | D;\u03b8,B) = n\u220f j=1 P(Tj | D;\u03b8, Bj)\n= n\u220f j=1 \u220f w\u2208D \u03b8 nw(Tj ,Bj) w , (1)\nwhere nw(Tj , Bj) is the number of appearance of w in Tj under segmentation Bj .\nTo learn the unknown model parameters (\u03b8,B) via Bayesian inference, we follow Pan et al. (2022) to adopt the following independent prior:\n\u03c0(\u03b8,B) = \u03c0(\u03b8) \u00b7 \u03c0(B) = \u03c0(\u03b8) \u00b7 n\u220f j=1 \u03c0(Bj),\nwhere \u03c0(\u03b8) = Dirichlet(\u03b8 | \u03b1) (2)\nis the same conjugate prior as in TopWORDS-Seg, with \u03b1 = {\u03b1w}w\u2208D being the parameter vector of a n-dimensional Dirichlet distribution. Here, we choose to specify \u03b1w = 1 for all w \u2208 D, leading to a non-informative plat prior distribution for \u03b8, as suggested by Pan et al. (2022). The prior distribution of B, i.e., \u03c0(B) = \u220fn j=1 \u03c0(Bj), however, needs to be properly specified to fit the special features of poetry lines.\nDifferent from a general Chinese sentence, whose segmentation usually does not follow obvious patterns, a poetry line in this study typically follows a few major segmentation patterns as we\nhave pointed out previously. To be concrete, let PL be the collections of segmentation patterns for poetry lines of length L, and B\u03f1 = (b\u03f11, \u00b7 \u00b7 \u00b7 , b\u03f1L) be the word boundary profile of a segmentation pattern \u03f1 \u2208 PL, where b\u03f1,l = 1 if \u03f1 puts a word boundary behind the l-th Chinese character of a poetry line, and 0 otherwise. In this study, we have two collections of segmentation patterns\nP5 = {\u03f1221, \u03f1212} and P7 = {\u03f12221, \u03f12212},\nwith the following word boundary profiles:\nB\u03f1212 = (0, 1, 1, 0, 1), B\u03f12212 = (0, 1, 0, 1, 1, 0, 1),\nB\u03f1221 = (0, 1, 0, 1, 1), B\u03f12221 = (0, 1, 0, 1, 0, 1, 1).\nTo reflect the impact of these segmentation patterns on text segmentation, we use the following mixture distribution as the prior distribution of Bj :\n\u03c0(Bj) = \u2211\n\u03f1\u2208PLj\n\u03bb\u03f1 \u00b7 \u03c0(Bj |\u03f1), (3)\nwhere \u03bbPLj = {\u03bb\u03f1}\u03f1\u2208PLj is a probability vector defined over segmentation patterns in PLj , and\n\u03c0(Bj |\u03f1) = Lj\u220f l=1 Binary(bjl|\u03c1\u03f1l)\nwith \u03c1\u03f1l being a \u03f1-specific probability value determined byB\u03f1, the word boundary profile of segmentation pattern \u03f1. Here, we specify\n\u03c1\u03f1l = { (1\u2212 \u03ba) \u00b7 b\u03f1l + \u03ba \u00b7 \u03c1, l < L\u03f1, 1, l = L\u03f1, (4)\nwhere \u03c1 \u2208 (0, 1) is the probability of placing a word boundary at each position randomly by a pseudo segmenter, and \u03ba \u2208 (0, 1) is a smoothing parameter to mix the segmentation pattern \u03f1 and the pseudo segmenter. In this study, we set \u03c1 = 0.5 by default, and leave \u03ba as a tunable parameter. Apparently, a \u03ba closer to 0 means a stronger prior preference to follow the segmentation pattern \u03f1. Such a prior setting sufficiently considers the constraints in segmenting poetry lines due to the segmentation patterns, while gives us the flexibility to violate these constraints, leading to an ideal framework for processing classical Chinese poetry.\nBecause there are two collections of segmentation patterns, i.e., P5 and P7, in this study, we need two groups of parameters \u03bbP5 and \u03bbP7 to reflect the prior proportion of different segmentation\npatterns in P5 and P7, respectively. In case that the proportion of segmentation patterns in P is precisely known, we can simply specify \u03bbP to the known proportion vector to reflect the prior knowledge. In practice, however, we usually do not have such knowledge. In such a scenario, a reasonable strategy is to specify the following conjugate prior distribution for \u03bbP5 and \u03bbP7 ,\n\u03c0(\u03bbP5) = Dirichlet(\u03bbP5 | \u03b2P5), (5) \u03c0(\u03bbP7) = Dirichlet(\u03bbP7 | \u03b2P7), (6)\nand infer them along the learning process. Such a Bayesian framework leads to a hierarchical Bayesian model (Good et al., 1966; Gelman et al., 2013) for Bj\u2019s with (\u03b2P5 ,\u03b2P7 , \u03ba) as hyperparameters. Here, we choose to specify \u03b2P5 and \u03b2P7 with constant vectors whose elements all equal to 1.\nFinally, we come up with the following joint prior distribution for \u03b8 = {\u03b8w}w\u2208D, \u03bb = (\u03bbP5 ,\u03bbP7) and B = {Bj}1\u2264j\u2264n:\n\u03c0(\u03b8,\u03bb,B) = \u03c0(\u03b8) \u00b7 \u03c0(\u03bb) \u00b7 n\u220f j=1 \u03c0(Bj | \u03bbPLj ),\nwhere \u03c0(\u03bb) = \u03c0(\u03bbP5) \u00b7 \u03c0(\u03bbP7),\nwith \u03c0(\u03b8), \u03c0(Bj |\u03bbPLj ) and \u03c0(\u03bbP) defined in Eq. (2)-(6), respectively. Accordingly, we have the following posterior distribution of (\u03b8,\u03bb,B) given T and D based on the Bayes theorem:\nf(\u03b8,\u03bb,B | D, T ) \u221d \u03c0(\u03b8,\u03bb,B) \u00b7P(T | D;\u03b8,B). (7)"
        },
        {
            "heading": "2.2 Word Discovery and Text Segmentation",
            "text": "Based on the posterior distribution in Eq. (7), word discovery and text segmentation can be achieved by following the strategy recommended by Deng et al. (2016).\nFirst, we maximize the marginal posterior distribution below with B integrated out\nf(\u03b8,\u03bb | D, T ) = \u222b f(\u03b8,\u03bb,B | D, T )dB\nby the EM algorithm (Dempster et al., 1977) to get the Maximum a Posteriori (MAP) estimate of (\u03b8,\u03bb) below:\n(\u03b8\u0302, \u03bb\u0302) = argmax (\u03b8,\u03bb)\nf(\u03b8,\u03bb | D, T ). (8)\nSecond, we get the significance score \u03c8w of each word candidate w \u2208 D by calculating the likelihood ratio between the full model with all word candidates in D and the reduced model with w removed from D:\n\u03c8w = log ( \u220fn j=1P(Tj | D; \u03b8\u0302, \u03bb\u0302)\u220fn\nj=1P(Tj | D; \u03b8\u0302[w=0], \u03bb\u0302)\n) , (9)\nwhere \u03b8\u0302[w=0] is a modification of \u03b8\u0302 by setting \u03b8\u0302w = 0 with all the other elements of \u03b8\u0302 unchanged, and\nP(Tj | D; \u03b8\u0302, \u03bb\u0302) = \u2211 Bj\u2208Bj \u03c0(Bj | \u03bb\u0302) \u00b7P(Tj | D; \u03b8\u0302, Bj),\n= \u2211 Bj\u2208Bj \u2211 \u03f1\u2208PLj \u03bb\u0302\u03f1 \u00b7 \u03c0(Bj | \u03f1) \u00b7P(Tj | D; \u03b8\u0302, Bj).\nBecause a larger \u03c8w means that word candidate w is more important for fitting the poetry texts and thus more likely to be a meaningful word, we achieve word discovery by keeping only word candidates whose \u03c8w \u2265 \u03c4\u03c8. Since \u22122\u03c8w \u223c \u03c72 asymptotically under the null hypothesis that the reduced model with w removed is the true model, we set the threshold \u03c4\u03c8 to the (1\u2212 0.05N )-quantile of \u03c72 distribution by default, based on the Bonferroni correction principle for multiple hypothesis testing.\nThird, based on the MAP estimate (\u03b8\u0302, \u03bb\u0302) of the pruned word dictionary D obtained in the previous step, we can segment Tj according to\nB\u0302j = max B\u2208Bj\nP(B | D, Tj ; \u03b8\u0302, \u03bb\u0302), (10)\nwhere Bj stands for the set of all possible word boundary profiles of Tj . A more robust strategy recommended by Deng et al. (2016) is to calculate\n\u03b3jl = \u2211 B\u2208Bj bjl \u00b7P(Bj | D, Tj ; \u03b8\u0302, \u03bb\u0302), (11)\nand put a word boundary behind the lth position of Tj if \u03b3jl is larger than a pre-given threshold \u03c4S ."
        },
        {
            "heading": "2.3 Computational Issues",
            "text": "Although TopWORDS-Poetry follows a similar strategy as in TopWORDS-Seg to achieve word discovery and text segmentation, it involves more computational challenges due to the more complicated prior distribution \u03c0(Bj), which is a mixture distribution with unknown parameter \u03bbPLj .\nTo ease the computation challenges, we propose to introduce an additional group of auxiliary latent variables Z = {Zj}1\u2264j\u2264n with Zj standing for the underlying segmentation pattern of poetry line Tj . With the help of Z, we can expand the target posterior distribution f(\u03b8,\u03bb,B | D, T ) in Eq. (A.1) to a higher dimensional distribution f(\u03b8,\u03bb,Z,B | D, T ), which takes f(\u03b8,\u03bb,B | D, T ) as its marginal distribution and is computationally more friendly. and thus implement computations based on the expanded posterior distribution instead of the original one. We leave all these computation details to Appendix A-C, with intuitions behind some key formulas explained in Appendix D."
        },
        {
            "heading": "2.4 TopWORDS-Poetry Pipeline",
            "text": "Assembling the above ingredients, we come up with the TopWORDS-Poetry algorithm composed of four stages: a dictionary initialization stage to generate the initial word dictionary D, a prior specification stage to specify prior distributions based on pre-given segmentation patterns and hyperparameters, a word discovery stage to estimate (\u03b8,\u03bb) and prune the initial word dictionary D into a more concise final dictionary DF , and a text segmentation stage to segment poetry lines based on the refined model. Figure 2 illustrates the algorithm pipeline of TopWORDS-Poetry.\nIn this pipeline, \u03c4L, \u03c4F and \u03ba are three control parameters to be specified by the users. Since words with more than 3 Chinese characters are very rare in classical Chinese poetry and it is not realistic to discover rare words that appear only once, we set \u03c4L = 3 and \u03c4F = 2 by default. Following the strategy proposed by Pan et al. (2022), we choose a larger \u03ba in the word discovery stage (referred to as \u03bad) to encourage the discovery of unregistered words, and a smaller \u03ba in the text segmentation stage (referred to as \u03bas) to show more respect to the segmentation patterns. In this study, we find that TopWORDS-Poetry works well by setting \u03bad = 0.5 and \u03bas = 0.001, and thus use these values as the default setting.\nNote that TopWORDS-Peotry is an unsupervised method in nature, because it only takes a set of segmentation patterns (i.e., P5 and P7) and a few hyper-parameters as inputs, and does not need any training corpus or pre-given vocabulary."
        },
        {
            "heading": "2.5 Considering Correlation within a Couplet",
            "text": "In the TopWORDS-Poetry algorithm, we assume that the choice of segmentation pattern is completely independent across different poetry lines. In practice, however, a metrical poem with 4 or 8 lines is composed of 2 or 4 couplets, each of which contains two poetry lines with positively correlated segmentation patterns (see Fig E1 for detailed evidence). Such a fact means that we can slightly modify TopWORDS-Poetry for a more efficient algorithm by modelling the correlation accordingly. Hereinafter, we refer to the modified TopWORDSPoetry algorithm as TopWORDS-Poetry\u2217."
        },
        {
            "heading": "3 Experimental Study",
            "text": "We applied TopWORDS-Poetry on the poetry texts in P , the collection of all metrical poems in CTP, under the default setting of \u03c4L = 3, \u03c4F = 2, \u03c1 = 0.5, \u03bad = 0.5, \u03bas = 0.001, \u03c4\u03c8 = 0.05 and \u03c4S = 0.5. For comparison purpose, we also applied 6 competing methods to the same data set, including Jieba (Sun, 2012), StanfordNLP (Manning et al., 2014), THULAC (Sun et al., 2016), PKUSEG (Luo et al., 2019), LTP (Che et al., 2021), TopWORDS (Deng et al., 2016) and TopWORDS-Seg under their default settings.\nUnfortunately, however, no executable program is available for either the modified SVM method (referred to as ModSVM) for discovering poetry words by Li et al. (2016) or the pattern selection method (referred to as SelectPattern) for segmenting poetry texts by Liu (2019). Although it is straightforward to implement SelectPattern by ourselves, it is non-trivial to implement ModSVM due to many subtle details. Therefore, we only include SelectPattern in the experimental study. Moreover, Liu (2019) also proposed a baseline method (referred to as RandomPattern here) that randomly\npicks up a segmentation pattern from {\u03f1\u221712, \u03f1\u221721} for each poetry line. We also include RandomPattern in our experiment.\nFurthermore, we also evaluate the performance of GPT-style methods on segmenting classical Chinese poetry by asking ChatGPT (model version: gpt-35-turbo) to segment all poetry lines in the testing dataset P1000. Appendix F reports the prompt designing for calling ChatGPT for this task in detail."
        },
        {
            "heading": "3.1 Performance Evaluation Criteria",
            "text": "Because no gold standard is publicly available for text segmentation of poems in CTP, we utilize the manually segmented poems in P1000 as the test data for performance evaluation. Randomly sampled from P , the pool of all metrical poems in CTP, via stratification sampling, P1000 covers 250 poems for each of the 4 poem types of interest and is a group of representative samples from P . Let Ttest be the indices of all poetry lines in P1000, Btest = {Bj}j\u2208Ttest be the true word boundary profiles of poetry lines in Ttest, and Vtest be the true vocabulary of Ttest obtained by collecting all words in the segmented poetry lines in Ttest.\nFor each competing method, let Vd be the discovered vocabulary based on all poetry lines in T , B\u0302 = {B\u0302j}j\u2208T be the predicted word boundary profiles, and Vs be the collection of words that appear in the segmented poetry lines in Ttest based on B\u0302test = {B\u0302j}j\u2208Ttest . We measure its performance on word discovery by discovery recall R(t,l)d and segmentation recall R(t,l)s defined as following:\nR (t,l) d =\n|V (t,l)test \u2229 V (t,l) d |\n|V (t,l)test | , R(t,l)s =\n|V (t,l)test \u2229 V (t,l) s |\n|V (t,l)test | ,\nhttps://chat.openai.com/\nwhere t and l are natural numbers, and\nV (t,l) = {w \u2208 V : freq(w) \u2265 t, len(w) \u2265 l},\nis a subset of V composed of selected words, with freq(w) being the raw frequency of string w in T and len(w) being the length of w. Rd and Rs evaluate word discovery from different perspectives, and provide us a more comprehensive view on this task in this application.\nFor test data Ttest, let Fs be the F1 score of B\u0302test with respect to Btest. We measure the performance on text segmentation for Ttest by Fs. In practice, by using poetry lines in P1000 and its subsets (e.g., P4\u00d75250 , P 8\u00d75 250 , P 4\u00d77 250 and P 8\u00d77 250 , which cover different types of poems in it) as the test data, we can evaluate text segmentation performance at the overall as well as more detailed levels."
        },
        {
            "heading": "3.2 Results",
            "text": "It took 80 minutes in a cluster with 10 CPU cores (Platinum82, 2.5GHz) with a RAM of 28GB to finish all computations involved in this experiment. Table 1 compares the performance of proposed\nmethods on word discovery and text segmentation for the CTP data with 10 competing methods, from which we can see: (1) tools for processing general Chinese texts, such as LTP, Jieba and so on, perform poorly in this application; (2) TopWORDS recognizes most long words but at the price of relative low performance on short words, and cannot segment poetry lines well, with TopWORDS-Seg performing worse in most dimensions because the prior information provided by PKUSEG is misleading; (3) SelectPattern achieves good performance on segmenting poetry lines in general, but misses all long words with more than 2 Chinese characters and all irregular patterns for text segmentation\nbeyond the major patterns; (4) The RS and FS scores of ChatGPT concentrate in a narrow region of around 70%, leaving a wide performance margin (>10%) with respect to TopWORDS-Poetry. (5) TopWORDS-Poetry and TopWORDS-Poetry\u2217, however, both achieve balanced performance on word discovery and text segmentation that outperforms all competing methods, with TopWORDSPoetry\u2217 being slightly superior in most dimensions.\nThese results confirm that TopWORDS-Poetry is an effective word discoverer for poetry texts. Although R(2,3)s of TopWORDS-Poetry, which is 62.74%, is much smaller than 89.49% reported by TopWORDS, it is the third largest value in the column and outperforms SelectPattern with a huge margin. We note that R(2,3)s = 62.74% means that TopWORDS-Poetry may miss 1/3 long words in Ttest in the segmented texts. But, considering that R\n(2,3) d = 83.76% for TopWORDS-Poetry, we still capture the majority of these long words in the discovered word dictionary.\nThe results of TopWORDS-Poetry reported in Table 1 are under the default setting where hyperparameter \u03bas is set to \u03bas = 0.001. In this setting, TopWORDS-Poetry reports a relatively low R(2,3)s score (\u223c60%), with respect to the highR(2,2)s score (\u223c89%), because the low parameter \u03bas = 0.001 discourages long words in text segmentation. In practice, however, we can adjust the granularity of poetry line segmentation in TopWORDS-Poetry by specifying hyper-parameter \u03bas differently: a larger \u03bas tends to reserve more long words in text segmentation, and thus leads to a higher chance to break through the restrictive segmentation templates. Therefore, if we could accept a little bit of decrease in the FS scores from the current \u223c85% to \u223c80%, we can get a R(2,3)s that is as large as 80% by specifying \u03bas = 0.1 instead.\nTable 2 reports the segmentation patterns recognized by TopWORDS-Poetry from T , confirming that TopWORDS-Poetry can successfully recognize rich segmentation patterns and correctly estimate their proportion. Figure 3 (a)-(c) visualize the most significant names, addresses and literary allusions with more than 2 Chinese characters discovered by TopWORDS-Poetry, suggesting that TopWORDS-Poetry indeed captures a lot of meaningful words that are often missed by most competing methods. These results confirm the superiority of the proposed method over competing methods and its ability as a powerful tool for processing\nclassical Chinese poetry. Additionally, we also report in Appendix G more detailed performance evaluation results on word discovery by word types (e.g., name, address, and literary allusion), where labelled words in test dataset P1000 are manually classified into different categories. Moreover, to investigate whether the advantage of TopWORDS-Poetry over the other methods is statistically significant, we further calculated the performance margin of TopWORDS-Poetry\u2217 between the second best competing methods in every performance evaluation dimension and tested the statistical significance of the obtained performance margin. Detailed results are reported in Appendix H."
        },
        {
            "heading": "4 Conclusions and Discussions",
            "text": "In this paper, we proposed TopWORDS-Poetry, a powerful unsupervised method for processing classical Chinese poetry that can segment poetry lines wisely with respect to a few major segmentation patterns, and recognize unique poetry words such as name entities and literary allusions successfully with little training information. Relying on a hierarchical Bayesian model to utilize prior information about known segmentation patterns of poetry lines, and rigorous statistical principles to achieve parameter estimation and model selection, TopWORDSPoetry leads to an elegant solution to challenging NLP problems in an interpretable way.\nWe hope this proposed method can serve as a useful tool for scholars in the field of Chinese literary or digital humanity to conduct in-depth study on classical Chinese poetry via distant reading. For example, the literary allusions discovered from CTP by TopWORDS-Poetry may lead to very exciting research in near future, which may extend the research efforts discussed by Moritz et al. (2016); Sturgeon (2018); Burns et al. (2021). We also hope this work can promote more and more interactions between literary scholars and NLPers, and applications of NLP technology in cultural education and dissemination, helping more people to better enjoy the charm of Chinese traditional culture.\nLimitations\nThe proposed method relies on regular segmentation patterns in poetry lines to guide text segmentation and thus may fail to process poetry lines without such regular segmentation patterns. Moreover, the proposed method tends to miss rare words\nthat appear only one or two times in the target corpus due to its nature as an unsupervised approach, and may result in improper segmentation for poetry lines containing rare named entities."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research is partially supported by the National Scientific and Technological Innovation 2030 Major Project (No: 2020AAA0106501), the Guo Qiang Institute of Tsinghua University, and the Beijing Natural Science Foundation (Z190021). Changzai Pan was supported by China Scholarship Council during the early stage of this work. The authors would like to thank Miss Yafei Han, Miss Jiafei Song, Miss Linyue Zhang, Mr Yanggang Zhang, Mr Ning Cheng, Miss Xiting Song and other members in Ke Deng\u2019s lab and Feiyue Li\u2019s lab for their great help on generating manually labelled data for performance evaluation."
        },
        {
            "heading": "A EM Algorithm for Estimating (\u03b8,\u03bb)",
            "text": "By adding latent variables Z, the target posterior distribution f(\u03b8,\u03bb,B | D, T ) can be extended to f(\u03b8,\u03bb,Z,B | D, T ) as following:\nf(\u03b8,\u03bb,Z,B | D, T )\n= \u03c0(\u03b8) \u00b7 \u03c0(\u03bb) \u00b7 n\u220f j=1 ( \u03bbZj \u00b7 \u03c0(Bj | Zj)\n\u00b7P(T | D;\u03b8,B) ) . (A.1)\nGiven (\u03b8(t),\u03bb(t)), the current estimation of\n(\u03b8,\u03bb), the E-step computes the Q-function below:\nQ(\u03b8,\u03bb;\u03b8(t),\u03bb(t)) =E ( log ( P(\u03b8,\u03bb,B,Z | T ,D) )\u2223\u2223D, T ;\u03b8(t),\u03bb(t)) =C +\n\u2211 w\u2208D ( log \u03b8w \u00b7 nw(\u03b8(t),\u03bb(t)) ) +\n\u2211 \u03f1\u2208P5\u222aP7 ( log \u03bb\u03f1 \u00b7 n\u03f1(\u03b8(t),\u03bb(t)) ) ,\nwhere C is a constant that does not change with \u03b8,\nnw(\u03b8 (t),\u03bb(t)) = n\u2211 j=1 nw,j(\u03b8 (t),\u03bb(t)),\nn\u03f1(\u03b8 (t),\u03bb(t)) = n\u2211 j=1 n\u03f1,j(\u03b8 (t),\u03bb(t)),\nnw,j(\u03b8 (t),\u03bb(t))\n= E(nw(Tj , Bj) | D, Tj ;\u03b8(t),\u03bb(t)) (A.2) = \u2211 Bj\u2208Bj nw(Tj , Bj)P(Bj | D, Tj ;\u03b8(t),\u03bb(t)),\nn\u03f1,j(\u03b8 (t),\u03bb(t))\n= P(Zj = \u03f1 | D, Tj ;\u03b8(t),\u03bb(t)) (A.3) \u221d \u03bb(t)\u03f1 \u2211 Bj\u2208Bj \u03c0(Bj | \u03f1) \u00b7P(Tj | D;\u03b8(t), Bj).\nThe M-step updates (\u03b8(t),\u03bb(t)) by maximizing Q(\u03b8,\u03bb;\u03b8(t),\u03bb(t)) with respect to (\u03b8,\u03bb), leading to the updating function below:\n\u03b8(t+1)w = nw(\u03b8 (t),\u03bb(t))\u2211 w\u2208D nw(\u03b8 (t),\u03bb(t)) , \u2200 w \u2208 D; (A.4)\n\u03bb(t+1)\u03f1 = n\u03f1(\u03b8 (t),\u03bb(t))\u2211 \u03f1\u2208PL\u03f1 n\u03f1(\u03b8 (t),\u03bb(t)) , \u2200\u03f1 \u2208 PL.(A.5)"
        },
        {
            "heading": "B Calculation of Significance Score",
            "text": "In order to compute\u03c8w efficiently through dynamic programming, we have\n\u03c8w = \u2212 n\u2211 j=1 log (1\u2212 rwj) ,\nwhere rwj = P ( w \u223c Bj | Tj ,D; \u03b8\u0302, \u03bb\u0302 ) (B.1)\n= \u2211 Bj\u2208Bj I (w \u223c Bj) \u00b7P(Bj | Tj ,D; \u03b8\u0302, \u03bb\u0302),\nwith notation \u201cw \u223c Bj\u201d meaning that word candidate w appears in the segmented version of Tj based on Bj ."
        },
        {
            "heading": "C Efficient Computation via Dynamic Programming",
            "text": "To implement the TopWORDS-Poetry algorithm, we need to calculate nw,j in (A.2), n\u03f1,j in (A.3), rw,j in (B.1) and \u03b3jl in (11) for \u2200 Tj \u2208 T .\nFor a specific Tj = T = a1 \u00b7 \u00b7 \u00b7 aL, by defining T[t:s] = at \u00b7 \u00b7 \u00b7 as, we can shown that nw,j , n\u03f1,j , rwj and \u03b3jl, which are all functions of Tj , have the following recursive formula:\nnw(T ) = 1\np(T ) \u2211 \u03f1\u2208PLj \u03bb\u03f1 \u00b7 n\u03f1w(T ),\nn\u03f1w(T ) = \u2211\n1\u2264t<s\u2264L\n[ p(\u03f1)(T[<t]) \u00b7 p(\u03f1)(T[>s])\n\u00b7 \u03b8w \u00b7 \u220f t\u2264l<s (1\u2212 \u03c1\u03f1l) \u00b7 \u03c1\u03f1s \u00b7 I(T[t:s]=w) ] ,\nn\u03f1(T ) = \u03bb\u03f1 \u00b7 p(\u03f1)(T )\np(T ) ,\nrw(T ) = 1\np(T ) \u2211 \u03f1\u2208PLj \u03bb\u03f1 \u00b7 r\u03f1w(T ),\nr\u03f1w(T ) = \u03c4L\u2211 t=1 [ r\u03f1w(T[>t]) \u00b7 I(T[1:t] \u0338=w) + I(T[1:t]=w) ] \u00b7 \u03b8T[\u2264t] \u00b7\n\u220f 1\u2264l<t (1\u2212 \u03c1\u03f1l) \u00b7 \u03c1\u03f1t \u00b7 p(\u03f1)(T[>t]) p(\u03f1)(T ) ,\n\u03b3l(T ) =\n\u2211 \u03f1\u2208PLj \u03bb\u03f1 \u00b7 p(T[\u2264l]) \u00b7 p(T[>l])\np(T ) ,\nwhere\np(\u03f1)(T[t:s]) = \u2211\nB\u2208B[t:s]\nP(T[t:s] | D;B,\u03b8) \u00b7 \u03c0(B | \u03c1\u03f1j),\np(T ) = \u2211 \u03f1\u2208C \u03bb\u03f1 \u00b7 p(\u03f1)(T ),\nwith B[t:s] being the collection of all possible word boundary profiles of Tj[t:s] .\nWe can compute p(\u03f1)(T[<t]) and p(\u03f1)(T[>t]) in linear time via dynamic programming to implement\nall computation issues in the following recursion:\np(\u03f1)(T[<t]) = \u2211\n1\u2264s\u2264min(t\u22121,\u03c4L)\n[ p(\u03f1)(T[<t\u2212s])\n\u00b7\u03b8T[t\u2212s:t\u22121] \u00b7 \u220f\nt\u2212s\u2264l<t\u22121 (1\u2212 \u03c1\u03f1l) \u00b7 \u03c1(\u03f1),t\u22121\n] ,\np(\u03f1)(T[>t]) = \u2211\n1\u2264s\u2264min(L\u2212t,\u03c4L)\n[ p(\u03f1)(T[>t+s])\n\u00b7\u03b8T[t+1:t+s] \u00b7 \u220f\nt+1\u2264l<t+s (1\u2212 \u03c1\u03f1l) \u00b7 \u03c1\u03f1,t+s\n] ,\nD Insights behind the Mathematical Formulation\nThe mathematical formulation of TopWORDSPoetry is composed of four parts. The first part, which contains just formula (1), defines the basic Unigram Language Model (ULM) which serves as the backbone of the TopWORDS-series methods. Formula (2)-(6) form the second part, which defines the informative prior distribution for word boundary based on the potential segmentation patterns of poetry lines. The third part composed by formula (7)-(11) provides detailed results about Bayesian inference of the unknown model parameters and word boundary prediction under the ULM and the specified prior. And, the fourth part composed by formula (A.1)-(B.1) in the Appendix provides more computational details about the EM algorithm and significant score.\nAmong these four parts, part 1, 3 and 4 are all standard mathematical formulas about ULM, Bayesian inference and EM algorithm with fast calculation via dynamic programming, which are very similar to their counterparts in the TopWORDSSeg algorithm proposed by Pan et al. (2022). The design of prior distribution in the second part, i.e., formula (2)-(6), however, makes a key difference and plays an essential role in this study. Among these formulas, the conjugate Dirichlet prior in formula (2) is a natural choice that has been adopted by Pan et al. (2022). Formula (3), however, specifies for a mixture prior distribution with multiple components to reflect the fact that each poetry line can potentially follow multiple segmentation patterns. Formula (4) corresponds to the fact that a segmentation pattern may not be exactly followed and we have a chance to break through the pattern template by placing a word boundary at each position randomly. Formula (5)- (6), on the other hand, specify the prior distribution for the relative\nweights of the mixture components defined in formula (3). And, formula (7), at last, assembles all these piecewise prior distributions into a joint prior distribution to guide the learning."
        },
        {
            "heading": "E More Details on TopWORDS-Poetry\u2217",
            "text": "Let (T2i\u22121, T2i) be one couplet composed of two poetry lines. We can estimate the transition probability P(T2i | T2i\u22121) based on couplets in training data P1000. Figure E1 visualizes the estimated transition probability matrices, suggesting there exists a positive correlation between the usage of segmentation patterns within a couplet.\n(a)\n(b)\nFigure E1: Transition probability matrix between patterns within one couplet on T1000 . (a) Transition probability matrix for 5-character poem couplets. (b)Transition probability matrix for 7-character poem couplets.\nThese facts suggest that we can further improve TopWORDS-Poetry by modelling such correlation. In practice, this can achieved conveniently by merging two poetry lines in the same couplet into a longer concatenated line with 10 or 14 Chinese characters, and extending P5 and P7 to the following larger collections of longer segmentation\npatterns:\nP10 ={\u03f1212|212, \u03f1212|221, \u03f1221|212, \u03f1221|221}, P14 ={\u03f12212|2212, \u03f12212|2221, \u03f12221|2212,\n\u03f12221|2221}. (E.1)\nSegmentation patterns such as \u03f1212|212 means that the two poetry lines in one couplet have the same pattern 2-1-2, while \u03f1212|221 means that the two poetry lines have different patterns. Let \u03bb10 and \u03bb14 be the proportion vectors of P10 and P10, respectively. Replacing P5 and P7 by P10 and P10, we can implement TopWORDSPoetry\u2217 by following exactly the same pipeline as in TopWORDS-Poetry, except for fixing \u03c1\u03f1,L\u03f1/2 = 1 for any \u03f1 \u2208 P10 \u222a P14 because there is always a word boundary between the two poetry lines in a couplet."
        },
        {
            "heading": "F Details on Calling ChatGPT",
            "text": "To segment classical Chinese poetry lines by ChatGPT, we asked ChatGPT (model version: gpt35-turbo) to segment all poetry lines in the testing dataset P1000 under the parameter setting where temperature = 0.0, max_tokens = 800, top_p = 0.95, frequency_penalty = 0, presence_penalty = 0 (all other parameters take the default values) with the prompt template in Figure F1.\nPrompt template:\n\u201c\u5bf9<>\u4e2d\u7684\u53e5\u5b50\u8fdb\u884c\u5206\u8bcd\uff0c\u7528\u201d/\u201c\u9694\u5f00\uff0c\u4e0d\u9700\u8981\u63d0\u4f9b\u4efb \u4f55\u89e3\u91ca\u3002 \u8f93\u5165\uff1a<\u529f\u84cb\u4e09\u5206\u570b\u3002\u540d\u6210\u516b\u9663\u5716\u3002\u6c5f\u6d41\u77f3\u4e0d\u8f49\u3002\u907a \u6068\u5931\u541e\u5433\u3002> \u56de\u7b54\uff1a\u529f\u84cb/\u4e09\u5206\u570b\u3002\u540d\u6210/\u516b\u9663\u5716\u3002\u6c5f\u6d41/\u77f3/\u4e0d/\u8f49\u3002 \u907a\u6068/\u5931/\u541e\u5433\u3002 \u8f93\u5165\uff1a<[\u76ee\u6807\u6587\u672c]>\u201d\nEnglish translation:\n\u201cPlease do text segmentation for the sentences in <>, separated by \u201d/\u201c, without providing any explanation. Input: < His deeds covered a kingdom split in three, his fame completed the Plan of the Eight Formations. The River flows on, the stones do not turn, a remnant bitterness at his failure to swallow Wu.> Answer: His deeds covered / a kingdom split in three /, his fame completed / the Plan of the Eight Formations. The River flows on /, the stones / do not / turn /, a remnant bitterness / at his failure / to swallow Wu. Input:<[Target Text]>\u201d\nFigure F1: The prompt template for calling ChatGPT and its English translated version."
        },
        {
            "heading": "G Performance Evaluation on Word Discovery by Word Type",
            "text": "Performance evaluations on word discovery reported in Table 1 are pooled results for all word types including both common words and technical words, e.g., names, addresses and literary allusions. Here, we would like to report more detailed results for different types of technical words. For this purpose, we manually picked up all names, addresses and literary allusions in the 1000 segmented poems in the P1000, and summarized in Table G1 the performance of different approaches on discovering technical words of different types. From this table, we can see that TopWORDS-Poetry is effective in discovering all types of technical words, particularly for literary allusions that are often missed by competing methods.\nMethod Name Adress Allusion\nEntity Count 1303 924 723\nLTP 20.64% 22.29% 19.78% StanfordNLP 50.42% 52.71% 44.67%\nTHULAC 61.17% 63.31% 59.06% PKUSEG 63.70% 64.94% 60.86%\nJieba 62.55% 64.18% 55.19%\nTopWORDS 84.96% 84.20% 82.99% TopWORDS-Seg 73.68% 76.95% 72.48%\nRandomPattern 80.05% 76.84% 69.85% SelectPattern 86.19% 85.50% 78.56%\nChatGPT 80.97% 83.23% 78.98%\nTopWORDS-Poetry 87.18% 88.64% 88.38% TopWORDS-Poetry\u2217 87.72% 88.96% 88.38%\nTable G1: Performance of different methods on discovering technical words of different types."
        },
        {
            "heading": "H Does TopWORDS-Poetry Perform Significantly Better?",
            "text": "To check whether TopWORDS-Poetry performs significantly better than competing methods, we conducted an additional experimental study based on the idea of bootstrapping (Efron and Tibshirani, 1994). For this purpose, we generated 20 pseudo testing datasets by randomly picking up 80% elements of P1000 without replacement. For each of these pseudo testing datasets, one version of the multi-dimensional performance measurements was calculated, resulting in an empirical distribution of performance measurements for every involved method. Table H1 summarizes the mean\nand standard deviation of the bootstrap distribution of each performance measurement for every involved method.\nIf TopWORDS-Poetry works significantly better than competing methods, we would like to observe an obvious shift in its empirical distributions with respect to the competing methods, which can be tested by a standard paired 2-sample t-test. The last two rows of Table H1 reports the performance margins between TopWORDS-Poetry\u2217 (the more robust member of the TopWORDS-Poetry family) and the best competing method outside the TopWORDS-Poetry family, and the p-values of the corresponding paired 2-sample t-test. Because most of these p-values are highly significant (<0.001), we are comfortable to claim that TopWORDS-Poetry indeed works better than existing methods in most performance evaluation dimensions. Although such a strategy is not very rigorous, it provides us with strong empirical evidence to gain useful insights about the statistical significance of TopWORDS-Poetry with respect to competing methods.\nW or\nd di\nsc ov\ner y\n(R (t ,l )\nd ,R\n(t ,l ) s )\nTe xt\nse gm\nen ta\ntio n\n(F S ) M et ho d |V d | R (2 ,2 ) d R (2 ,3 ) d |V s | R (2 ,2 ) s R (2 ,3 ) s P 1 0 0 0 P 4 \u00d7 5 2 5 0 P 8 \u00d7 5 2 5 0\nP 4 \u00d7 7 2 5 0\nP 8 \u00d7 5 2 5 0\nLT P\n39 K\n60 .3\n9% \u00b1\n0. 19\n% 58\n.4 5%\n\u00b1 1.\n47 %\n20 88\n\u00b1 26\n19 .0\n8% \u00b1\n0. 12\n% 29\n.9 4%\n\u00b1 1.\n05 %\n20 .0\n5% \u00b1\n0. 15\n% 21\n.1 8%\n\u00b1 0.\n80 %\n18 .7\n8% \u00b1\n0. 31\n% 21\n.8 3%\n\u00b1 0.\n33 %\n19 .6\n7% \u00b1\n0. 30 % St an fo rd N L P 77 K 77 .5 4% \u00b1 0. 23 % 63 .7 1% \u00b1 1. 90 % 49 21 \u00b1 24 43 .1 9% \u00b1 0. 25 % 44 .6 1% \u00b1 1. 53 % 39 .4 5% \u00b1 0. 18 % 40 .3 3% \u00b1 0. 70 % 40 .1 6% \u00b1 0. 41 % 41 .1 8% \u00b1 0. 48 % 37 .7 4% \u00b1 0. 32 % T H U L A C 76 K 77 .6 9% \u00b1 0. 21 % 43 .6 2% \u00b1 1. 40 % 55 02 \u00b1 25 52 .8 8% \u00b1 0. 22 % 34 .2 2% \u00b1 1. 27 % 49 .1 1% \u00b1 0. 15 % 50 .0 8% \u00b1 0. 79 % 50 .1 6% \u00b1 0. 31 % 49 .4 3% \u00b1 0. 40 % 47 .8 4% \u00b1 0. 23 % PK U SE G 83 K 82 .0 8% \u00b1 0. 15 % 48 .3 6% \u00b1 1. 49 % 63 18 \u00b1 24 58 .8 8% \u00b1 0. 23 % 36 .1 6% \u00b1 1. 30 % 52 .2 3% \u00b1 0. 17 % 52 .4 8% \u00b1 0. 57 % 52 .9 2% \u00b1 0. 41 % 52 .9 2% \u00b1 0. 41 % 51 .3 0% \u00b1 0. 36 % Ji eb a 71 K 78 .6 1% \u00b1 0. 15 % 43 .8 0% \u00b1 1. 27 % 56 77 \u00b1 22 57 .2 9% \u00b1 0. 21 % 37 .7 8% \u00b1 1. 37 % 53 .6 6% \u00b1 0. 14 % 56 .5 0% \u00b1 0. 54 % 52 .5 9% \u00b1 0. 36 % 54 .3 1% \u00b1 0. 31 % 53 .0 8% \u00b1 0. 29 % To pW O R D S 54 K 78 .7 0% \u00b1 0. 25 % 90 .1 5% \u00b1 0. 94 % 70 80 \u00b1 27 71 .5 3% \u00b1 0. 29 % 89 .8 4% \u00b1 1. 01 % 63 .6 8% \u00b1 0. 21 % 63 .3 1% \u00b1 0. 46 % 62 .3 4% \u00b1 0. 32 % 64 .9 9% \u00b1 0. 50 % 64 .1 7% \u00b1 0. 32 % To pW O R D SSe g 86 K 83 .5 1% \u00b1 0. 14 % 54 .0 5% \u00b1 1. 51 % 71 06 \u00b1 19 67 .8 8% \u00b1 0. 25 % 45 .2 0% \u00b1 1. 54 % 60 .7 5% \u00b1 0. 20 % 60 .0 1% \u00b1 0. 50 % 61 .3 4% \u00b1 0. 31 % 60 .5 3% \u00b1 0. 43 % 60 .7 2% \u00b1 0. 40 % R an do m Pa tte rn 96 K 95 .2 4% \u00b1 0. 11 % 0. 00 % \u00b1 0. 00 % 80 15 \u00b1 24 81 .2 6% \u00b1 0. 23 % 0. 00 % \u00b1 0. 00 % 69 .1 1% \u00b1 0. 22 % 63 .7 8% \u00b1 0. 47 % 64 .7 4% \u00b1 0. 40 % 72 .7 2% \u00b1 0. 55 % 72 .5 6% \u00b1 0. 23 % Se le ct Pa tte rn 73 K 94 .3 3% \u00b1 0. 13 % 0. 00 % \u00b1 0. 00 % 76 88 \u00b1 21 88 .8 9% \u00b1 0. 14 % 0. 00 % \u00b1 0. 00 % 84 .1 1% \u00b1 0. 17 % 81 .1 4% \u00b1 0. 53 % 81 .5 3% \u00b1 0. 27 % 86 .4 8% \u00b1 0. 33 % 85 .9 7% \u00b1 0. 26 %\nTo pW\nO R\nD S-\nPo et\nry 82\nK 96\n.3 2%\n\u00b1 0.\n06 %\n84 .4\n7% \u00b1\n1. 08\n% 78\n03 \u00b1\n21 89\n.0 9%\n\u00b1 0.\n20 %\n63 .2\n8% \u00b1\n1. 64\n% 84\n.5 0%\n\u00b1 0.\n14 %\n81 .5\n3% \u00b1\n0. 46\n% 81\n.9 1%\n\u00b1 0.\n26 %\n86 .4\n7% \u00b1\n0. 42\n% 86\n.5 4%\n\u00b1 0.\n20 %\nTo pW\nO R\nD S-\nPo et\nry \u2217\n82 K\n96 .3\n9% \u00b1\n0. 06\n% 83\n.5 7%\n\u00b1 1.\n24 %\n78 28\n\u00b1 24\n89 .4\n7% \u00b1\n0. 17\n% 57\n.0 7%\n\u00b1 1.\n57 %\n84 .7\n3% \u00b1\n0. 17\n% 80\n.7 2%\n\u00b1 0.\n45 %\n82 .6\n7% \u00b1\n0. 30\n% 86\n.8 2%\n\u00b1 0.\n42 %\n86 .6\n9% \u00b1\n0. 21\n%\nPe rf\nor m\nan ce\nm ar\ngi n\n- 1.\n15 -6\n.5 8\n- 0.\n58 -3\n2. 77\n0. 62\n-0 .4\n2 1.\n14 0.\n34 0. 73 p -v al ue - 9 .5 7 \u00d7 10 \u2212 2 3 2. 22 \u00d7 10 \u2212 1 9 - 4 .1 0 \u00d7 10 \u2212 1 3 1 .1 5 \u00d7 10 \u2212 2 8 1. 06 \u00d7 10 \u2212 1 2 2 .2 5 \u00d7 10 \u2212 3 2. 8 0 \u00d7 1 0\u2212 1 7 7 .8 5 \u00d7 1 0\u2212 6 6 .2 7 \u00d7\n1 0\u2212\n1 2\nTa bl\ne H\n1: Pe\nrf or\nm an\nce of\nTo pW\nO R\nD S-\nPo et\nry fo\nrc la\nss ic\nal C\nhi ne\nse po\net ry\nby bo\not st\nra pp\nin g\non w\nor d\ndi sc\nov er\ny an\nd te\nxt se\ngm en\nta tio\nn co\nm pa\nre d\nto co\nm pe\ntin g\nm et\nho ds\n."
        }
    ],
    "title": "TopWORDS-Poetry: Simultaneous Text Segmentation and Word Discovery for Classical Chinese Poetry via Bayesian Inference",
    "year": 2023
}