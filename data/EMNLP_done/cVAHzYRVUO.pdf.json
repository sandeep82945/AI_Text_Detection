{
    "abstractText": "The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entityindependent features such as graph structure information and relationship information to inference. However, the neighborhood of these new entities is often too sparse to obtain enough information to build these features effectively. In this work, we propose a knowledge graph inductive inference method that fuses ontology information. Based on the enclosing subgraph, we bring in feature embeddings of concepts corresponding to entities to learn the semantic information implicit in the ontology. Considering that the ontology information of entities may be missing, we build a type constraint regular loss to explicitly model the semantic connections between entities and concepts, and thus capture the missing concepts of entities. Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K165, and improves the MRR metrics by 15.4% and 44.1%, respectively, when compared with the state-of-the-art methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wentao Zhou"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        },
        {
            "affiliations": [],
            "name": "Tao Gui"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Xuanjing Huang"
        }
    ],
    "id": "SP:22207b394a9ad8ee6f8418a1773faf72f2717f19",
    "references": [
        {
            "authors": [
                "Mehdi Ali",
                "Max Berrendorf",
                "Mikhail Galkin",
                "Veronika Thost",
                "Tengfei Ma",
                "Volker Tresp",
                "Jens Lehmann."
            ],
            "title": "Improving inductive link prediction using hyper-relational facts",
            "venue": "The Semantic Web \u2013 ISWC 2021, pages 74\u201392, Cham. Springer International",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDuran",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems, volume 26. Curran Associates,",
            "year": 2013
        },
        {
            "authors": [
                "Jiajun Chen",
                "Huarui He",
                "Feng Wu",
                "Jie Wang."
            ],
            "title": "Topology-aware correlations between relations for inductive link prediction in knowledge graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6271\u20136278.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Daza",
                "Michael Cochez",
                "Paul Groth."
            ],
            "title": "Inductive entity representations from text via link prediction",
            "venue": "Proceedings of the Web Conference 2021, WWW \u201921, page 798\u2013808, New York, NY, USA. Association for Computing Machinery.",
            "year": 2021
        },
        {
            "authors": [
                "Jing Dong",
                "Binbin Gu",
                "Jianfeng Qu",
                "An Liu",
                "Lei Zhao",
                "Zhigang Chen",
                "Zhixu Li."
            ],
            "title": "Hyperjoie: Two-view hyperbolic knowledge graph embedding with entities and concepts jointly",
            "venue": "Web Information Systems Engineering \u2013 WISE 2021:",
            "year": 2021
        },
        {
            "authors": [
                "Genet Asefa Gesese",
                "Harald Sack",
                "Mehwish Alam."
            ],
            "title": "Raild: Towards leveraging relation features for inductive link prediction in knowledge graphs",
            "venue": "Proceedings of the 11th International Joint Conference on Knowledge Graphs, IJCKG \u201922,",
            "year": 2023
        },
        {
            "authors": [
                "Junheng Hao",
                "Muhao Chen",
                "Wenchao Yu",
                "Yizhou Sun",
                "Wei Wang."
            ],
            "title": "Universal representation learning of knowledge bases by jointly embedding instances and ontological concepts",
            "venue": "Proceedings of the 25th ACM SIGKDD International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Xiao Huang",
                "Jingyuan Zhang",
                "Dingcheng Li",
                "Ping Li."
            ],
            "title": "Knowledge graph embedding based question answering",
            "venue": "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM \u201919, page 105\u2013113, New York,",
            "year": 2019
        },
        {
            "authors": [
                "S\u00f6ren Auer"
            ],
            "title": "Dbpedia\u2013a largescale, multilingual knowledge base extracted from wikipedia",
            "venue": "Semantic web,",
            "year": 2015
        },
        {
            "authors": [
                "Zhao Li",
                "Xin Liu",
                "Xin Wang",
                "Pengkai Liu",
                "Yuxin Shen."
            ],
            "title": "Transo: a knowledgedriven representation learning method with ontology information constraints",
            "venue": "World Wide Web, 26(1):297\u2013 319.",
            "year": 2023
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yang Liu",
                "Xuan Zhu."
            ],
            "title": "Learning entity and relation embeddings for knowledge graph completion",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 29.",
            "year": 2015
        },
        {
            "authors": [
                "Xiyang Liu",
                "Huobin Tan",
                "Qinghong Chen",
                "Guangyan Lin."
            ],
            "title": "Ragat: Relation aware graph attention network for knowledge graph completion",
            "venue": "IEEE Access, 9:20840\u201320849.",
            "year": 2021
        },
        {
            "authors": [
                "Xin Lv",
                "Lei Hou",
                "Juanzi Li",
                "Zhiyuan Liu."
            ],
            "title": "Differentiating concepts and instances for knowledge graph embedding",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1971\u20131979, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Farzaneh Mahdisoltani",
                "Joanna Biega",
                "Fabian Suchanek."
            ],
            "title": "Yago3: A knowledge base from multilingual wikipedias",
            "venue": "7th biennial conference on innovative data systems research. CIDR Conference.",
            "year": 2014
        },
        {
            "authors": [
                "Sijie Mai",
                "Shuangjia Zheng",
                "Yuedong Yang",
                "Haifeng Hu."
            ],
            "title": "Communicative message passing for inductive relation reasoning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 4294\u20134302.",
            "year": 2021
        },
        {
            "authors": [
                "Maximilian Nickel",
                "Lorenzo Rosasco",
                "Tomaso Poggio."
            ],
            "title": "Holographic embeddings of knowledge graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 30.",
            "year": 2016
        },
        {
            "authors": [
                "Chao Ren",
                "Le Zhang",
                "Lintao Fang",
                "Tong Xu",
                "Zhefeng Wang",
                "Senchao Yuan",
                "Enhong Chen."
            ],
            "title": "Ontological concept structure aware knowledge transfer for inductive knowledge graph embedding",
            "venue": "2021 International Joint Conference on Neural",
            "year": 2021
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N. Kipf",
                "Peter Bloem",
                "Rianne van den Berg",
                "Ivan Titov",
                "Max Welling."
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "venue": "The Semantic Web, pages 593\u2013607, Cham. Springer International Publishing.",
            "year": 2018
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang."
            ],
            "title": "Rotate: Knowledge graph embedding by relational rotation in complex space",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Komal Teru",
                "Etienne Denis",
                "Will Hamilton."
            ],
            "title": "Inductive relation prediction by subgraph reasoning",
            "venue": "Proceedings of the 37th International Conference",
            "year": 2020
        },
        {
            "authors": [
                "Rakshit Trivedi",
                "Hanjun Dai",
                "Yichen Wang",
                "Le Song."
            ],
            "title": "Know-evolve: Deep temporal reasoning for dynamic knowledge graphs",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings",
            "year": 2017
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "Eric Gaussier",
                "Guillaume Bouchard."
            ],
            "title": "Complex embeddings for simple link prediction",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of",
            "year": 2016
        },
        {
            "authors": [
                "Shikhar Vashishth",
                "Soumya Sanyal",
                "Vikram Nitin",
                "Partha Talukdar."
            ],
            "title": "Composition-based multirelational graph convolutional networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107."
            ],
            "title": "Wikidata: A new platform for collaborative data collection",
            "venue": "Proceedings of the 21st International Conference on World Wide Web, WWW \u201912 Companion, page 1063\u20131064, New York, NY, USA. Association for Computing Machinery.",
            "year": 2012
        },
        {
            "authors": [
                "Hongwei Wang",
                "Fuzheng Zhang",
                "Jialin Wang",
                "Miao Zhao",
                "Wenjie Li",
                "Xing Xie",
                "Minyi Guo."
            ],
            "title": "Ripplenet: Propagating user preferences on the knowledge graph for recommender systems",
            "venue": "Proceedings of the 27th ACM International Conference",
            "year": 2018
        },
        {
            "authors": [
                "Zhen Wang",
                "Jianwen Zhang",
                "Jianlin Feng",
                "Zheng Chen."
            ],
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 28.",
            "year": 2014
        },
        {
            "authors": [
                "Ruobing Xie",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Representation learning of knowledge graphs with hierarchical types",
            "venue": "Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence, IJCAI\u201916, page 2965\u20132971. AAAI",
            "year": 2016
        },
        {
            "authors": [
                "Keyulu Xu",
                "Chengtao Li",
                "Yonglong Tian",
                "Tomohiro Sonobe",
                "Ken-ichi Kawarabayashi",
                "Stefanie Jegelka."
            ],
            "title": "Representation learning on graphs with jumping knowledge networks",
            "venue": "Proceedings of the 35th International Conference on Machine",
            "year": 2018
        },
        {
            "authors": [
                "Xiaohan Xu",
                "Peng Zhang",
                "Yongquan He",
                "Chengpeng Chao",
                "Chaoyang Yan"
            ],
            "title": "Subgraph neighboring relations infomax for inductive link prediction on knowledge graphs",
            "year": 2022
        },
        {
            "authors": [
                "Bishan Yang",
                "Wen tau Yih",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng"
            ],
            "title": "Embedding entities and relations for learning and inference in knowledge bases",
            "year": 2015
        },
        {
            "authors": [
                "Zhilin Yang",
                "William W. Cohen",
                "Ruslan Salakhutdinov."
            ],
            "title": "Revisiting semi-supervised learning with graph embeddings",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48,",
            "year": 2016
        },
        {
            "authors": [
                "Muhan Zhang",
                "Yixin Chen."
            ],
            "title": "Link prediction based on graph neural networks",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
            "year": 2018
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "ERNIE: Enhanced language representation with informative entities",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge graphs (KGs) store a large amount of structured real-world knowledge through a set of triples, and they have been widely used in many domains, such as natural language processing (Zhang et al., 2019), recommendation systems (Wang et al., 2018), and question answering (Huang et al., 2019). However, even the most knowledge-rich KGs suffer from incompleteness, such as DBpedia (Lehmann et al., 2015), YAGO (Mahdisoltani et al., 2014), and WikiData (Vrandec\u030cic\u0301, 2012). To complete the KGs, knowledge graph inference aims to predict the missing links between entities in KGs.\n*Equal Contributions. \u2020Corresponding authors.\nIn past studies, most methods (Bordes et al., 2013; Yang et al., 2015) mainly learn the specific embeddings of entities and relations and predict missing links by various mapping operations. Since the embeddings in such methods depends on specific entities, it requires that the entities in the graph are fixed, which is referred to as the transductive setting (Yang et al., 2016). However, in fact, new emerging entities are continuously added to real-world KGs over time, such as new users and products in e-commerce knowledge graphs and new molecules in biomedical knowledge graphs (Trivedi et al., 2017). Works adopting the transductive setting often require expensive retraining to make predictions for these added entities. As the amount of data increases this overhead will become unaffordable (Schlichtkrull et al., 2018). To deal with this problem, inductive inference gains the ability to extend what is learned from training entities to unknown entities by learning entity-independent semantic information (Teru et al., 2020).\nHowever, most of the existing inductive infer-\nence work (Chen et al., 2021; Mai et al., 2021) has only focused on various structural features in KGs (e.g., enclosing subgraphs induced by paths between nodes), ignoring the important ontology information. In general, newly emerging entities tend to lack sufficient neighbor relationships, leaving them without much contextual information to refer to in terms of structure (As shown in Test graph A in fig. 1) (Xu et al., 2022). As an abstract description of the real world, various concepts in ontology provide the basic type information for the affiliated entities, which can help the model achieve inductive link prediction. For example, in Test Graph B, despite lacking the neighbor relationship, with the help of the type information provided by the concepts \"Person\" and \"City\" for \"J.Butler\" and \"Miami\" respectively, we can predict that the relation between them is most likely to be \"lives_in\". In fact, the ontology information of entities also suffers from missing problems, so it is not easy to use ontology information effectively.\nTo solve the above problem, we propose a knowledge graph inductive inference method combining ontology information. Specifically, based on the enclosing subgraph, we bring in feature embeddings of corresponding concepts at the node initialization of entities to obtain the semantic information in the ontology. To deal with the problem of missing ontology information, we build a type constraint regular loss that captures the missing concepts of entities by explicitly modeling the semantic associations between entities and concepts. In addition, we train the link prediction on the ontology triples. The final training is performed using a joint strategy. Experimental results show that the method significantly outperforms ChatGPT as well as stateof-the-art inductive baselines. Our codes are publicly available at GitHub.*\nOur contributions can be summarized in the following three points: (1) we propose a knowledge graph inductive inference method combining ontology information, which effectively improves the inductive inference performance on newly emerging entities; (2) we build typeconstrained regular loss to alleviate the problem of missing ontology information; (3) we achieve a remarkable improvement on two benchmark datasets, demonstrating the effectiveness of using ontology information to enhance the effectiveness\n*https://github.com/chasers-of-Qs/OEILP\nof inductive link prediction."
        },
        {
            "heading": "2 Related Works",
            "text": "Transductive Link Prediction: Most existing knowledge graph inference methods are embedding-based transductive learning. These methods can be broadly classified into: 1) translation-based(Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Sun et al., 2019); 2) semantic matching-based, (Yang et al., 2015; Trouillon et al., 2016; Nickel et al., 2016); 3) GNN-based(Schlichtkrull et al., 2018; Vashishth et al., 2020; Liu et al., 2021). The difference between them mainly lies in how to design the score function. Inductive Link Prediction: As inductive inference models have the ability to extend from known entities to unknown entities, these methods(Ali et al., 2021; Gesese et al., 2023) show great potential for link prediction tasks on new entities. BLP(Daza et al., 2021) learns the embedding representations of entities based on the architecture of the pre-trained language model to obtain the required generalization capabilities. GraIL(Teru et al., 2020) suggests modeling enclosing subgraphs around the target triple for the first time, based on the graph neural network framework. TACT(Chen et al., 2021) models the semantic correlation between relations as several topological patterns and uses a relational correlation network (RCN) to learn the importance of different patterns for inductive link prediction. SNRI(Xu et al., 2022) enhances the processing for sparse subgraphs by exploiting full neighbor relationships and by applying mutual information (MI) maximization to knowledge graphs. Ontology Enhanced Inference: Incorporating ontology information through various methods(Xie et al., 2016; Ren et al., 2021), it helps models learn richer semantic information. TransC (Lv et al., 2018) models the embedding of concepts as a sphere and assumes that the embedding corresponding to the entity belonging to the concept should be in this sphere. Considering the limitation of relation information in the ontology, TransO (Li et al., 2023) computes the weights of the type mapping matrix based on domain and range. JOIE (Hao et al., 2019) proposes the first approach of jointly embedding entity and ontology knowledge graphs to build a unified representation learning framework from multiple levels. These\nmethods are all transductive link predictions on the original KGs. Inductive link prediction for unknown entities is still an urgent but underresearched task."
        },
        {
            "heading": "3 Approach",
            "text": "We start with task formulation. Unlike transductive link prediction which predicts unknown triples on a fixed entity graph, for inductive link prediction, the goal is to predict unknown triples (u, rt, v) in the unseen entity graph by learning from the seen entity graph. The entities in these two graphs are disjoint. Specifically, given the seen entity graph Gek and the migratable information (e.g., ontology graph Gc and type links between entities and concepts Tt), we optimize the score function f(s, r, t) at training time, where the score reflects the likelihood of the existence of target relation r between target nodes s and t. In testing, combined with the migratable information, we use the score function f to predict the unknown triples (u, rt, v) in the unseen entity graph Geu ."
        },
        {
            "heading": "3.1 Method Overview",
            "text": "In this work, we propose an inductive link prediction method that combines entity and ontology information. The method brings in ontology information to enhance inductive inference on\nunknown entities and effectively captures the missing ontology information of entities by type constraint. As shown in fig. 2, the whole training consists of three steps:\nStep \u2460: This part aims to integrate ontology information. First, we extract the enclosing subgraph around the target node and obtain the position feature hposi of nodes in the subgraph. Second, we obtain the type feature htypei from the embedding of the concept corresponding to the entity by the attention approach. Finally, we concatenate the position feature hposi with the type feature htypei as the initial feature of the node. In this way, we integrate the semantic information of the ontology into the entity.\nStep \u2461: This step aims to optimize the score function fe(u, rt, v) used for prediction. For the representations of nodes and relations in the subgraph, we use a graph neural network (GNN) to update them. Then, we optimize the score function fe(u, rt, v) obtained with these representations. Since the node representations contain type features, the model can use ontology information to help complete inductive link prediction.\nStep \u2462: This step aims to construct type constraint and optimize ontology embedding. We construct a type constraint regular loss ft by modeling the relationship between entity embeddings and\nconcept embeddings. The constraint requires that the embedding of the entity h should be close to the embedding of its corresponding concept c after mapping to the feature space of the ontology. Based on this constraint, the model gains the ability to capture the missing ontology information of the entity. In addition, we train the link prediction task on the ontology triples to optimize the embedding representation of the ontology in order to enhance the semantic information obtained by the model from the ontology.\nFinally, based on the above steps, the model learns from the total loss L. Next, we will describe the technical details of each step in detail."
        },
        {
            "heading": "3.2 Ontology Information Feature Embedding",
            "text": "Subgraph Extraction: We extract the enclosing subgraph around the target triple (u, rt, v) from GraIL (Teru et al., 2020). First, we obtain the sets Nk(u) and Nk(v) of nodes in the corresponding k-hop neighborhoods from the two target nodes u and v, respectively. Then, the duplicate nodes are removed by taking the intersection set Nk(u)\u2229 Nk(v). Finally, the nodes isolated from any node or at a distance greater than k are cut off to obtain the enclosing subgraph. Node Initialization: We use the position feature hposi of the node and the type feature h type i as the initial embedding of the node features to ensure that the node features do not contain any node attributes. First, we obtain the position feature hposi of the node in subgraph by double radius vertex labeling (Zhang and Chen, 2018) scheme:\nhposi = [one-hot(d(i, u))\u2295 one-hot(d(i, v))], (1)\nwhere d(i, u) denotes the shortest distance from node i to u without passing through v. Second, we obtain the type feature htypei from the embedding of the concept corresponding to the entity by the attention approach:\nh type i = \u03c31( \u2211 cj\u2208Ci \u03b1jW1cj + b1) (2)\n\u03b1j = softmax(cj , c) = exp(cTj c)\u2211\nck\u2208Ci exp(ck Tc)\n, (3)\nwhere Ci is the set of concepts corresponding to node hi, c denotes the type relation between entities and concepts, \u03b1j reflects the importance of the type information in each concept cj under the type\nrelation c, and \u03c31 is the sigmoid function. Finally, by connecting the position feature hposi and type feature htypei , we obtain the initial embedding of the node h0i :\nh0i = [h type i \u2295 h pos i ]. (4)\nWe think that using ontology type information to guide the node initialization helps the model to learn the semantic information implicit in the ontology."
        },
        {
            "heading": "3.3 Subgraph Neural Network",
            "text": "We input the enclosing subgraph G(u,rt,v) into the GNN to update the embedding of the nodes. We define the update function based on the architecture of R-GCN (Schlichtkrull et al., 2018):\nhkt = ReLU(W k selfh k\u22121 t + a k t ), (5)\nwhere akt denotes the neighbor feature aggregation function. Inspired by CompGCN (Vashishth et al., 2020) and edge attention, we define akt as:\nakt = R\u2211\nr=1 \u2211 s\u2208Nr(t) \u03b1krrtstW k r \u03d5(e k\u22121 r , h k\u22121 s ) (6)\n\u03b1krrtst = \u03c32(W k 2 s+ b k 2) (7)\ns = ReLU(W k3 [h k\u22121 s \u2295 hk\u22121t \u2295 ek\u22121r \u2295 ek\u22121rt ]\n+ bk3), (8)\nwhere Nr(t) denotes the direct outgoing neighbors of node t under relation r, \u03b1krrtst is the edge attention weight of edge (s, r, t) at layer k, and \u03c32 is the sigmoid function. \u03d5(ek\u22121r , h k\u22121 s ) is the fusion operation on the features of the neighboring nodes and relations. We set it as the subtraction \u03d5(e,h) = h\u2212 e(Vashishth et al., 2020). In order to keep the same embedding space for nodes and relations, we also update the relation embedding:\nekr = W k rele k\u22121 r , (9)\nIn addition, we also use the JK-connection mechanism(Xu et al., 2018) on the representation of nodes and relations, and this approach makes the performance of the model robust to the number of layers of the GNN. The representation of the subgraph G(u,rt,v) is obtained by average pooling of all node representations:\nhLG(u,rt,v) =\n1 |V| \u2211 i\u2208V hLi , (10)\nFinally, we obtain the score of the target triple (u, rt, v):\nfe(u, rt, v) = W T [hLG(u,rt,v) \u2295 hLu \u2295 hLv \u2295 eLrt ], (11)\nwhere hLu , h L v , and e L rt denote the embedding of target nodes u, v and target relation r. We obtain the negative triples used in loss functions below by replacing the head or tail entity with uniformly sampled random entities. The margin-based loss function in the subgraph is:\nLent = \u2211\n(u,rt,v)\u2208Ge\nmax(0, fe(u\u2032, rt, v\u2032)\n\u2212 fe(u, rt, v) + \u03b31). (12)\nwhere (u, rt, v) and (u\u2032, rt, v\u2032) denote positive and negative samples, respectively, and \u03b31 is the margin hyperparameter."
        },
        {
            "heading": "3.4 Type Constraint and Ontology Training",
            "text": "We explicitly model the relationship between entity embeddings and concept embeddings. Specifically, this requires that the embedding of the entity h should be close to the embedding of its corresponding concept c after mapping to the ontology embedding space. This way builds a type constraint ft in the form of regularization:\nft(u, v, Cu,v) = 1 |Cu,v| \u2211\ncw\u2208Cu,v\n\u2225cw \u2212 cu,v\u22252 (13)\ncu,v = \u03c33(W4hu,v + b4), (14)\nwhere hu,v is the embedding of the target node u or v, cu,v is the embedding of the entity embedding after mapping to the ontology embedding space, cw is the embedding of the corresponding concept of node u or v, and \u03c33 is the tanh function. We use the same negative sampling method as before. Thus, the margin-based type constraint regular loss is:\nLtype = \u2211\n(u,rt,v)\u2208Ge\nmax(0, ft(u, v, Cu,v)\n\u2212ft(u, v, Cu,v \u2032) + \u03b32). (15)\nwhere (u, v, Cu,v) and (u, v, Cu,v \u2032) denote positive and negative samples. Inspired by(Hao et al., 2019; Dong et al., 2021), learning meta-relations between concepts\u2020 will enhance the semantic\n\u2020Meta-relations include hierarchical relations between concepts and other general meta-relations\ninformation obtained by the model from the ontology. Therefore, we train the link prediction task on the ontology triplet (uc,mt, vc), and the corresponding score function is:\nfo(uc,mt, vc) = \u2225huc +mt \u2212 hvc\u22252 , (16)\nwhere huc and hvc denote the embedding of concepts, and mt denotes the embedding of metarelations between concepts. The margin-based loss function obtained by ontology training is:\nLonto = \u2211\n(uc,mt,vc)\u2208Gc\nmax(0, fo(uc,mt, vc)\n\u2212fo(u\u2032c,mt, v\u2032c) + \u03b33). (17)\nwhere (uc,mt, vc) and (u\u2032c,mt, v \u2032 c) denote positive and negative samples."
        },
        {
            "heading": "3.5 Joint Training Strategy",
            "text": "We combine all the losses to obtain the total loss L. The overall training objective is as follows:\nL = Lent + \u03b1Lont + \u03c9Ltype. (18)\nwhere \u03b1 and \u03c9 are weighting hyperparameters. With the joint training strategy, our model can better utilize the ontology information to enhance the inductive inference on unknown entities, while effectively capturing the missing ontology information of entities."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "The KG benchmark datasets with ontology information, YAGO26K-906 and DB111K-174 (Hao et al., 2019), are originally developed for transductive inference prediction. To facilitate the inductive test, we build two new datasets, YAGO21K-610 and DB45K-165, based on the two original benchmark datasets. Both datasets are built from entity triples,\nontology triples, and type links. In the entity triples, the entities in the test set do not appear in the train set and valid set, while the relations in both the test set and valid set are included in the train set. We train on the train graph and test on the test graph. In addition, to achieve ontology training, we randomly divide the ontology triples into a train set, a valid set, and a test set in the ratio of 80%/10%/10%. tab. 1 provides the complete statistics for both datasets.\nPlease refer to Appendix A.1 for the detailed generation process of the datasets."
        },
        {
            "heading": "4.2 Compared Methods",
            "text": "To evaluate the effectiveness of our proposed approach, we compare our method with several state-of-the-art baseline methods. In addition, large language models (e.g., ChatGPT, GPT-4, etc.) have recently achieved impressive performance on several natural language processing tasks. Therefore, we also compare it with ChatGPT. GraIL(Teru et al., 2020). The earliest method for inductive inference based on graph neural networks. It uses locally enclosing subgraphs and entity-independent node labels to represent node embeddings. TACT(Chen et al., 2021). An inductive inference method based on relational correlation networks. The method categorizes all relation pairs into several topological patterns and uses the topology of the knowledge graph to learn the semantic correlation between relations. SNRI(Xu et al., 2022). An inductive inference method based on graph neural networks. It utilizes complete neighborhood relations in terms of both neighborhood relation features of node features and neighborhood relation paths of sparse subgraphs and also models the neighborhood relations using mutual information maximization."
        },
        {
            "heading": "4.3 Metrics",
            "text": "As in the previous work (Teru et al., 2020; Xu et al., 2022), for the test triples (h, r, t), we combine head (or tail) entities and relations with 50 candidate tail (or head) entities (including the original tail (or head) entities) obtained by random sampling to get positive and negative triples and rank all triples based on their scores. We use three metrics widely used in link prediction for evaluation. (1) MRR: The average of the inverse of the ranking of the correct entities in all tested samples. (2) Hits@1: The ratio of correct entities ranked first in all test samples. (3) Hits@10: The ratio of correct entities ranked within the top ten for all test samples."
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "We extracted 3-hop enclosing subgraphs. In the training process, we use Adam as the optimizer, the learning rate is set to 0.01, and the batch size is set to 16. We use a three-layer GNN, and the dimensions of all feature embeddings are 32 except for the dimensions of the type feature embedding which is 24. The margins in the loss function are set to 10,10,5, and the weighting hyperparameters \u03b1 and \u03c9 are set to 1. The maximum number of training epochs is 30. All experiments are conducted with Python 3.8.12 and PyTorch 1.11.0, using a GeForce GTX 2080Ti with 12GB RAM."
        },
        {
            "heading": "5 Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "In this section, we make inductive link predictions for the proposed method and several comparative methods, and the results are shown in tab. 2. Since we evaluate on a newly constructed benchmark dataset, we re-implement GraIL, TACT, and SNRI under our evaluation metrics. To make a fair comparison, we keep the hyperparameters the\nsame as their original papers. Similarly, we test ChatGPT with the same test data. The relevant details of the prompts are shown in Appendix A.2. From the results, we can observe that our method significantly outperforms the state-of-the-art baseline methods and ChatGPT. Specifically, some baseline methods (e.g., GraIL, TACT) only focus on various structural features in the knowledge graph, which makes them unable to handle sparse subgraphs effectively. And when the subgraph is empty (i.e., only the target triple exists), its performance even drops to the same as random guesses. In contrast, our approach effectively alleviates this problem by incorporating ontology information and achieves an overall performance improvement (We provide more results to support this claim in sec. 5.2). Although approaches like SNRI attempt to deal with the sparse subgraph problem by importing global information, our proposed method still outperforms them. Moreover, although ChatGPT outperforms some of the baseline methods (or even all of them) in some metrics, our method achieves better results compared to it.\nIn addition, we also make transductive link predictions for the proposed method and several baseline methods for further comparison. For detailed experimental results, please refer to Appendix A.3."
        },
        {
            "heading": "5.2 Result Analysis",
            "text": "In this section, we analyze the sources of performance improvement of our approach. First, we counted all the test triples, and the results are shown in tab. 3. We can observe that the prediction performance for entities with ontology type information outperforms that for entities lacking ontology information on both datasets.\nMeanwhile, the prediction performance for tail entities outperforms that for head entities on the YAGO21K-610 dataset, while the prediction performance for both is similar on the DB45K165 dataset. In addition, the proportion of head entities lacking type information is higher than that in tail entities on the YAGO21K-610 dataset, while the proportions are close on the DB45K-165 dataset. Therefore, we believe that the integration of ontology information improves the prediction performance of our model.\nFurthermore, we counted the prediction performance for target nodes with different numbers of neighboring nodes. For the target nodes, the more neighboring nodes they have, the more paths may exist between the nodes and thus the more dense enclosing subgraphs are extracted. Therefore, by analyzing the performance improvement of the prediction of target nodes with different numbers of neighboring nodes by ontology information, we investigate the effect of ontology information on subgraphs with different densities, and the results are shown in fig. 3.From the results, we can see that integrating ontology information improves the prediction performance of target nodes with all different numbers of neighboring nodes, and the lower the number of neighboring nodes, the greater the performance improvement. This indicates that our method can improve the prediction for all enclosing subgraphs, and effectively alleviate the problem of poor prediction performance for sparse subgraphs at the same time."
        },
        {
            "heading": "5.3 Type Prediction",
            "text": "Our previous analysis of the experimental results demonstrates that ontology information can effectively improve prediction performance. In fact,\nontology information also suffers from missing information, for example, on the test set of the YAGO21K-610 dataset, only 1725 entities have type information and more than half of them miss ontology information. Since the semantic links between entities and concepts are explicitly modeled in our method, we can use our method to predict type information for the test triples. Specifically, without providing ontology information, we predict the concept corresponding to the target node based on the enclosing subgraph of the target triple, and the prediction results are shown in tab. 4. From the results, we can see that our method can effectively predict the type information of entities, which helps to alleviate the problem of missing ontology information."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "To investigate the contribution of each component of our approach, we conduct ablation experiments on two datasets, and the experimental results are shown in tab. 5. First, like our analysis, the type information provided by the ontology plays a very important role in the inductive link prediction. When this module is removed, the prediction effect degrades to equal the baseline method, and the performance is severely compromised. Moreover, ontology training and type constraint improve the performance of the model in terms of optimizing the embedding representation of ontology and explicitly modeling the semantic connection between instances and concepts, respectively, and removing either of these modules leads to a degradation of performance. And when both modules are removed at the same time, the performance will be further degraded."
        },
        {
            "heading": "5.5 Hyper-parameter Analysis",
            "text": "As the core structure that is relied on when predicting, the size of the enclosing subgraph implies how much semantic information the target node can obtain from the structure. We conducted experiments on two datasets to investigate the effect of hop (which directly responds to the\nsize of the enclosing subgraph) on inductive prediction. From fig. 4, we can obtain the following observations. With the hop gradually increasing from 1, the prediction performance of the model keeps improving. This implies that too few neighbor nodes cannot provide enough semantic information for prediction(We provide more results in Appendix A.4 to support this claim). And when hop exceeds a certain threshold (i.e., 3), the performance starts to decrease. This indicates that the subgraph contains the critical structure needed for prediction after reaching a certain size and continuing to increase the size of the subgraph will continuously increase the number of noise nodes, which will lead to performance degradation."
        },
        {
            "heading": "5.6 Error Analysis",
            "text": "What are the remaining errors in our research? For the inductive prediction of various subgraphs with ontology information, our method has brought different degrees of performance improvement. Moreover, our method can capture the missing ontology information of the target nodes through the rich semantic information in the subgraphs. However, for those enclosing subgraphs that lack ontology information and are extremely sparse,\nour method has difficulty in giving correct type predictions due to the lack of available structural information, making the inductive prediction on such subgraphs not effectively improved. In fact, link prediction on extremely sparse enclosing subgraphs is difficult for all inductive prediction methods."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this work, we propose a knowledge graph inductive inference method that combines ontology information. The approach integrates semantic information of ontology by using type information for the initialization of node features. We construct a type-constrained regular loss, which effectively captures the missing ontology information of entities. At the same time, ontology training helps the model to enhance the semantic information obtained from the ontology. Experimental results show that our approach achieves state-of-the-art inductive link prediction.\nLimitations\nAlthough we demonstrate the effectiveness of ontology training for improving model performance, we only use the simplest methods to model ontology graphs. Using some richer and more effective methods (e.g., using pre-trained embedding representations or building detailed hierarchies) to learn better embedding representations of the ontology can help make the model further achieve superior performance. In addition, in all experiments, we set all margin and weighting hyperparameters as fixed hyperparameters, which may make our model not achieve its best performance. A dynamic optimization of these hyperparameters may be a better choice. We leave these limitations to our future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by National Natural Science Foundation of China (No.62076069,62206057,61976056), Shanghai Rising-Star Program (23QA1400200), and Natural Science Foundation of Shanghai (23ZR1403500)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Dataset Generation\nTo generate the train set, we randomly select a triple from the whole entity triples and treat it as the initial train graph, and keep adding new edges and nodes to the graph by the edges connected to the train graph. To ensure that the entities in the test set do not appear in the train set, we first remove the train set from the entire graph and then use the same approach to generate the test set, while requiring that the relations in the test set must appear in the train set. Finally, we generate the valid set using the same way as we generated the train set, and in addition, to ensure the inductive setting, all entities in the valid set do not appear in the test set. The triples in the train set, valid set, and test set do not intersect each other.\nA.2 Prompt details\nOur prompt includes a one-shot example, task instruction, relevant information, questions and candidate entities. The prompt templates are shown in tab. 6.\nWhere one-shot example is used to standardize the format of the output. <Information> indicates the neighbor information and type information of the test triples, <Question> is the link prediction for the test triples, and <Possible answer> denotes the candidate entities.\nA.3 Transductive Prediction\nFB15K-237 is the entity KG benchmark dataset widely used in many recent works, and GraIL has proposed four versions of variant datasets based on it. We used some of these variant datasets to compare our approach with several baseline methods. Since FB15K-237 contains only entity triples, we supplemented the type information for the dataset used based on WikiData, and the statistics of the supplemented dataset are shown in tab. 7. In addition, due to the lack of carefully built ontology triples, our method uses the version without the ontology training module (w/o OT) for transductive link prediction. The results are shown in tab. 8. As can be seen from the results, our method outperforms the baseline methods even for transductive link prediction.\nA.4 The Impact of Neighboring Nodes\nWe studied the effect of different enclosing subgraph sizes on the prediction during the test, and the experimental results are shown in fig. 5. It can be seen that the smaller the enclosing subgraph extracted for the test triple, the worse its prediction performance is without changing any other conditions. The direct result of a smaller enclosing subgraph is that the target node has fewer neighboring nodes available. Therefore, we argue that too few neighbor nodes will impair the semantic information captured by the model from the structure and thus lead to a decrease in prediction performance.\nA.5 Ontology Prediction\nAs a kind of knowledge graph, the ontology graph also suffers from the problem of incompleteness. Like other jointly trained methods, our proposed method can conveniently complete the ontology\ngraph while achieving the prediction of entities. We compared our method with the jointly trained baseline method (Hao et al., 2019) and the results are shown in tab. 9. It can be seen that even though we only use the simplest model to train ontology triples, our method still outperforms the state-ofthe-art baseline method, where JOIE builds the hierarchy of ontology in detail and uses a more advanced training model."
        }
    ],
    "title": "Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information",
    "year": 2023
}