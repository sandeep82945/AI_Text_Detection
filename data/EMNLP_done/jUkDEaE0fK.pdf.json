{
    "abstractText": "With the rapid development of large language models (LLMs), it is highly demanded that LLMs can be adopted to make decisions to enable the artificial general intelligence. Most approaches leverage manually crafted examples to prompt the LLMs to imitate the decision process of human. However, designing optimal prompts is difficult and the patterned prompts can hardly be generalized to more complex environments. In this paper, we propose a novel model named Large Decision Model with Memory (LDM), which leverages a dynamic memory mechanism to construct dynamic prompts, guiding the LLMs in making proper decisions according to the faced state. LDM consists of two stages: memory formation and memory refinement. In the former stage, human behaviors are decomposed into state-action tuples utilizing the powerful summarizing ability of LLMs. Then, these tuples are stored in the memory, whose indices are generated by the LLMs, to facilitate the retrieval of the most relevant subset of memorized tuples based on the current state. In the latter stage, our LDM employs tree exploration to discover more suitable decision processes and enrich the memory by adding valuable state-action tuples. The dynamic circle of exploration and memory enhancement provides LDM a better understanding of the global environment. Extensive experiments conducted in two interactive environments have shown that our LDM outperforms the baselines in terms of both score and success rate, which demonstrates its effectiveness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingjin Wang"
        },
        {
            "affiliations": [],
            "name": "Linjing Li"
        },
        {
            "affiliations": [],
            "name": "Daniel Dajun Zeng"
        }
    ],
    "id": "SP:677b4b17054670e57d04e638f24be94c86737751",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "year": 2022
        },
        {
            "authors": [
                "Kai Arulkumaran",
                "Marc Peter Deisenroth",
                "Miles Brundage",
                "Anil Anthony Bharath."
            ],
            "title": "Deep reinforcement learning: A brief survey",
            "venue": "IEEE Signal Processing Magazine, 34(6):26\u201338.",
            "year": 2017
        },
        {
            "authors": [
                "Boyuan Chen",
                "Fei Xia",
                "Brian Ichter",
                "Kanishka Rao",
                "Keerthana Gopalakrishnan",
                "Michael S Ryoo",
                "Austin Stone",
                "Daniel Kappler."
            ],
            "title": "Open-vocabulary queryable scene representations for real world planning",
            "venue": "arXiv preprint arXiv:2209.09874.",
            "year": 2022
        },
        {
            "authors": [
                "Liting Chen",
                "Lu Wang",
                "Hang Dong",
                "Yali Du",
                "Jie Yan",
                "Fangkai Yang",
                "Shuang Li",
                "Pu Zhao",
                "Si Qin",
                "Saravan Rajmohan"
            ],
            "title": "Introspective tips: Large language model for in-context decision making",
            "venue": "arXiv preprint arXiv:2305.11598",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Marc-Alexandre C\u00f4t\u00e9",
                "Akos K\u00e1d\u00e1r",
                "Xingdi Yuan",
                "Ben Kybartas",
                "Tavian Barnes",
                "Emery Fine",
                "James Moore",
                "Matthew Hausknecht",
                "Layla El Asri",
                "Mahmoud Adada"
            ],
            "title": "Textworld: A learning environment for text-based games",
            "year": 2019
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Hangfeng He",
                "Hongming Zhang",
                "Dan Roth."
            ],
            "title": "Rethinking with retrieval: Faithful large language model inference",
            "venue": "arXiv preprint arXiv:2301.00303.",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch."
            ],
            "title": "Language models as zeroshot planners: Extracting actionable knowledge for embodied agents",
            "venue": "International Conference on Machine Learning, pages 9118\u20139147. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Fei Xia",
                "Ted Xiao",
                "Harris Chan",
                "Jacky Liang",
                "Pete Florence",
                "Andy Zeng",
                "Jonathan Tompson",
                "Igor Mordatch",
                "Yevgen Chebotar"
            ],
            "title": "2022b. Inner monologue: Embodied reasoning through planning with language models",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Hussein",
                "Mohamed Medhat Gaber",
                "Eyad Elyan",
                "Chrisina Jayne."
            ],
            "title": "Imitation learning: A survey of learning methods",
            "venue": "ACM Computing Surveys (CSUR), 50(2):1\u201335.",
            "year": 2017
        },
        {
            "authors": [
                "Omar Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Minsoo Kim",
                "Yeonjoon Jung",
                "Dohyeon Lee",
                "Seungwon Hwang."
            ],
            "title": "PLM-based world models for text-based games",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1324\u20131341, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Shuang Li",
                "Xavier Puig",
                "Chris Paxton",
                "Yilun Du",
                "Clinton Wang",
                "Linxi Fan",
                "Tao Chen",
                "De-An Huang",
                "Ekin Aky\u00fcrek",
                "Anima Anandkumar"
            ],
            "title": "Pretrained language models for interactive decisionmaking",
            "venue": "Advances in Neural Information Processing",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651",
            "year": 2023
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u00ec",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz"
            ],
            "title": "Augmented language models: a survey",
            "venue": "arXiv preprint arXiv:2302.07842",
            "year": 2023
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders"
            ],
            "title": "Webgpt: Browser-assisted questionanswering with human feedback",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Joon Sung Park",
                "Joseph C O\u2019Brien",
                "Carrie J Cai",
                "Meredith Ringel Morris",
                "Percy Liang",
                "Michael S Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human",
            "year": 2023
        },
        {
            "authors": [
                "Debjit Paul",
                "Mete Ismayilzada",
                "Maxime Peyrard",
                "Beatriz Borges",
                "Antoine Bosselut",
                "Robert West",
                "Boi Faltings."
            ],
            "title": "Refiner: Reasoning feedback on intermediate representations",
            "venue": "arXiv preprint arXiv:2304.01904.",
            "year": 2023
        },
        {
            "authors": [
                "Reid Pryzant",
                "Dan Iter",
                "Jerry Li",
                "Yin Tat Lee",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Automatic prompt optimization with\" gradient descent\" and beam search",
            "venue": "arXiv preprint arXiv:2305.03495.",
            "year": 2023
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580.",
            "year": 2023
        },
        {
            "authors": [
                "Noah Shinn",
                "Beck Labash",
                "Ashwin Gopinath."
            ],
            "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "venue": "arXiv preprint arXiv:2303.11366.",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Xingdi Yuan",
                "Marc-Alexandre C\u00f4t\u00e9",
                "Yonatan Bisk",
                "Adam Trischler",
                "Matthew Hausknecht."
            ],
            "title": "Alfworld: Aligning text and embodied environments for interactive learning",
            "venue": "arXiv preprint arXiv:2010.03768.",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "venue": "arXiv preprint arXiv:2212.10509.",
            "year": 2022
        },
        {
            "authors": [
                "Sai Vemprala",
                "Rogerio Bonatti",
                "Arthur Bucker",
                "Ashish Kapoor."
            ],
            "title": "Chatgpt for robotics: Design principles and model abilities",
            "venue": "Microsoft Auton. Syst. Robot. Res, 2:20.",
            "year": 2023
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Chenfei Wu",
                "Shengming Yin",
                "Weizhen Qi",
                "Xiaodong Wang",
                "Zecheng Tang",
                "Nan Duan."
            ],
            "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
            "venue": "arXiv preprint arXiv:2303.04671.",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Howard Chen",
                "John Yang",
                "Karthik R Narasimhan."
            ],
            "title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Rohan Rao",
                "Matthew Hausknecht",
                "Karthik Narasimhan."
            ],
            "title": "Keep CALM and explore: Language models for action generation in textbased games",
            "venue": "Proceedings of the 2020 Conference",
            "year": 2020
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629.",
            "year": 2022
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Lianghong Guo",
                "Qiqi Gao",
                "He Ye",
                "Yanlin Wang"
            ],
            "title": "Memorybank: Enhancing large language models with long-term memory",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "employs tree exploration to discover more suitable decision processes and enrich the memory by adding valuable state-action tuples. The dynamic circle of exploration and memory enhancement provides LDM2 a better understanding of the global environment. Extensive experiments conducted in two interactive environments have shown that our LDM2 outperforms the baselines in terms of both score and success rate, which demonstrates its effectiveness."
        },
        {
            "heading": "1 Introduction",
            "text": "The rapid development of large language models (LLMs) has led to remarkable revolution in the field of natural language processing (NLP). LLMs, such as Llama (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023), have achieved impressive results in a variety of tasks,\n\u2217*Corresponding author\nincluding text/code generation (Chen et al., 2021), question answering (Zhou et al., 2022), and reasoning (Wei et al., 2022b), to name a few. Besides NLP tasks, LLMs are also employed as policy agents to accomplish decision-making tasks (Kim et al., 2022; Mialon et al., 2023). Nowadays, most approaches adopt the standard prompting paradigm which uses manually crafted in-context examples to prompt the LLMs (Sanh et al., 2022; Wei et al., 2022a) making decisions. However, the standard prompting is not suitable for decisionmaking tasks, as it restricts the LLMs to merely imitate the provided examples, making the generated decisions highly context-sensitive. Worse still, standard prompting cannot generate admissible decisions in complex environments (Liu et al., 2021; Dong et al., 2022). In certain cases, the LLMs may fall into confusion when new situations are greatly different from the examples. In more challenging situations, even humans are unable to provide complete solution examples. Meanwhile, once the prompt examples are fixed, the LLMs can no longer\nlearn from the feedback of the environment, thus cannot further improve the performance.\nIn this paper, we proposed Large Decision Model with Memory (LDM2), a framework that enhances the standard LLMs with dynamic updating memory. The memory mechanism maintains the most valuable state-action tuples when imitating human decisions. As shown in Fig. 1, the prompts generated by the proposed LDM2 focus on providing LLMs with sufficient information to guide decision making in the current situation, rather than relying solely on the simple examples in the entire process of decisions. In order to obtain sufficient state-action tuples in the memory, LDM2 incorporates a memory formation stage that is analogous to the traditional imitation learning (Hussein et al., 2017). In this stage, we take human trajectories as training data and instruct the LLMs to produce numerous standard state-action tuples including the task goals, observations, historical information, and actions. These standard state-action tuples are preserved to form the initial memory. In the inference phrase, we retrieve the most similar state-action tuples from the memory with current observation and then construct the prompt to inspire the LLMs dynamically. These retrieved state-action tuples 1) inform the LLMs which actions would be taken by human in the current state and 2) help the LLMs understand the global environment.\nDifferentiate from traditional imitation learning (Hussein et al., 2017), LDM2 is equipped with a dynamic memory refinement stage to enhance the memory with valuable state-action tuples. First, We conduct tree exploration to generate all potential decision processes and evaluate them according to the environment rewards. Then, we add the stateaction tuple corresponding to the best decision process into the memory. This exploration-evaluationadding circle mimics the traditional reinforcement learning framework (Arulkumaran et al., 2017; Yao et al., 2020). The refinement stage not only expands the action space of the LLMs, but also enable the LLMs to deal with new situations not covered by the initial memory.\nWe evaluate the proposed LDM2 in two interactive environments: WebShop and ALFworld. LDM2 outperforms the standard few shots prompting methods and other methods prompted with verbal reasoning. We further analyze the successful examples in both tasks and find that LDM2 has a more diverse action space compared with meth-\nods using fixed examples prompt, this advantage empowers the LLMs to handle unseen or complex situations. Additionally, We conduct ablation experiments to evaluate the memory refinement mechanism. The results demonstrate the effectiveness of adding highly rewarded state-action tuples into the memory. Our main contributions can be summarized as:\n\u2022 We propose a novel paradigm that leverages a two-stage memory mechanism to dynamically prompt the standard LLMs for decisionmaking tasks.\n\u2022 We make full use of the standard LLMs in the memory formation stage to produce stateaction tuples and generate the corresponding indices.\n\u2022 We adopt tree exploration to generate potential decision processes and instruct the LLMs to identify the most valuable state-action tuples to enhance the memory."
        },
        {
            "heading": "2 Related work",
            "text": "Our paper is closely related to the following three research directions: LLMs for decision-making, feedback for LLMs, and memory and retrieval for LLMs. In this section, we briefly review the literature on these research.\nLLMs for decision-making Powerful LLMs are able to act as policy models to make decisions in interactive environments. Li et al. (2022) constructed a general framework for decision-making that uses LLMs to encode observations, goals, and history and then generate actions. Demonstration of example prompts and utilization of high-level function libraries are employed to explore innovative strategies (Huang et al., 2022a; Liang et al., 2022; Wu et al., 2023; Nakano et al., 2021; Shen et al., 2023). Prompting structure with pre-defined functions, behaviors, and examples are leveraged to ground LLMs to generate robotic actions (Ahn et al., 2022; Huang et al., 2022b; Vemprala et al., 2023). However, these methods use manually crafted examples to prompt the LLMs which results in decisionmaking in a fixed direction. Our LDM2 leverages dynamic state-action tuples as prompt to improve the effectiveness of decisions.\nFeedback for LLMs Recent techniques have emerged that focus on establishing closed-loop systems which are capable of utilizing the scalar or\ntextual feedback from environment or human to update the LLMs (Christiano et al., 2017; Ouyang et al., 2022; Chen et al., 2022; Bai et al., 2022). Madaan et al. (2023) and Pryzant et al. (2023) employ an iterative framework for self-refinement to optimize the prompt of LLMs based on the feedback of self-evaluation. REFINER (Paul et al., 2023) fine-tunes another critic model to provide intermediate feedback within trajectories to improve the reasoning response. ReAct (Yao et al., 2022b) prompts LLMs with both verbal reasoning traces and actions which guides the models to perform dynamic reasoning according to environmental feedback. Reflexion (Shinn et al., 2023) converts binary or scalar feedback from the environment into verbal feedback which is then added in the prompt of the next episode. Introspective Tips (Chen et al., 2023) learns tips from the action trajectories and environmental feedback to empower the LLM agents with self-optimizing capabilities. These approaches mainly aim to leverage reward feedback to augment the prompt of LLMs, however, LDM2 adds highly rewarded state-action tuples into the memory which achieves dynamic learning ability. Memory for LLMs Memory could store information perceived from the environment and leverages the recorded memories to facilitate future actions. Generative Agents (Park et al., 2023) maintain a memory stream to record the experience including observations and behaviors. Reflexion (Shinn et al., 2023) stores experiential feedback in natural language within a sliding window. Voyager (Wang et al., 2023) employs natural language descriptions to represent skills within the Minecraft game, which are directly stored in memory. MemoryBank (Zhong et al., 2023) encodes the memory segment into embedding vector which could enhance memory retrieval and reading efficiency. Knowledge base is also used as the memory to retrieve relevant information and construct the taskrelated augmented prompts (He et al., 2022; Trivedi et al., 2022; Khattab et al., 2022). Our LDM2 also constructs a memory to record vast state-action tuples, which forms the retrieval indices list through instructing the LLMs to cluster different goals and observations."
        },
        {
            "heading": "3 Methodology",
            "text": "In this paper, we consider a general setup of LLMs as policy models to accomplish decision-making tasks in an interactive environment. In the follow-\ning, LLMA(\u00b7) denotes employing a LLM to perform the A function/operation."
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "We leverage N human decision trajectories T = {t1, t2, \u00b7 \u00b7 \u00b7 , tN} as the training data, where each trajectory ti = { oi1, a i 1, \u00b7 \u00b7 \u00b7 , oiTi , a i Ti } has a task goal gi, Ti is the length of this trajectory, oi\u03c4 is the observation at time step \u03c4 , ai\u03c4 is the human action when faced with oi\u03c4 , 1 \u2264 \u03c4 \u2264 Ti. The set T of trajectories can be further processed into a memory M consisting of standard state-action tuples:\nM = {\u2329\ngi, hi\u03c4 , o i \u03c4 \u2192 ai\u03c4 \u232a |1 \u2264 i \u2264 N, 1 \u2264 \u03c4 \u2264 Ti } ,\n(1) where hi\u03c4 (to be further elaborated in the next subsection) represents historical information about the observations and actions before time step \u03c4 . The memory M provides the LLMs with a sufficient set of state-action tuples to help generate proper actions in various situations. Based on M, the LLMs can interactively explore the environment. Given a new task goal gj , the LLMs receive the current observation oj\u03c4 at time step \u03c4 , the historical information hj\u03c4 and the context prompt p j \u03c4 to generate an action aj\u03c4 :\naj\u03c4 = LLM(g j , hj\u03c4 , o j \u03c4 , p j \u03c4 ), (2)\nwhere the context prompt pj\u03c4 is a subset of memory M that can be retrieved from M according to the task goal gj and the current observation oj\u03c4 :\npj\u03c4 = LLMretrieve(M|gj , oj\u03c4 ). (3)\nIn order to accomplish the goal gj , the context prompt tells the LLMs about actions taken by human in the current state, invoking the LLMs to comprehend the environment. Subsequently, the LLMs generate the complete decision process P j and get the final reward r:\nP j = { gj ; oj1, a j 1, \u00b7 \u00b7 \u00b7 , o j T , a j T } . (4)\nDue to the context length, an intrinsic limitation of LLMs, we have to partition all those N trajectories into n batch data with size B, where N = nB. Each batch data Tb = { t(b\u22121)B+1, t(b\u22121)B+2, \u00b7 \u00b7 \u00b7 , tBb } , b = 1, 2, \u00b7 \u00b7 \u00b7 , n, is processed to form a batch memory Mb by the same procedure described above. Accordingly, we construct n independent batch memory M1,M2, \u00b7 \u00b7 \u00b7 ,Mn. Each batch memory Mb\nassists the LLMs completing a whole decision process P jb w.r.t. the goal g\nj . We require the LLMs to choose the optimal process as the final decision:\nP jfinal = LLMchoose ( P j1 , P j 2 , \u00b7 \u00b7 \u00b7 , P j n ) , (5)\nwhere P jb is the decision process based on the batch memory Mb, b = 1, 2, \u00b7 \u00b7 \u00b7 , n."
        },
        {
            "heading": "3.2 Memory Formation",
            "text": "Previous imitation learning methods enable a policy agent to mimic expert behavior through updating the parameters of language models. However, in the new prompt paradigm of LLMs (Liu et al., 2023), we need to integrate human cognition into the context prompt while freezing the parameters of LLMs. Our LDM2 leverages the memory to store vast state-action tuples and constructs dynamic context prompt based on the current observation, which imitates the human decisions.\nMemory Structure and Format As introduced in the last subsection, the memory M consists of a large number of standard state-action tuples. This subsection depicts how to construct the memory M. Given a human trajectory ti ={ oi1, a i 1, \u00b7 \u00b7 \u00b7 , oiTi , a i Ti } , it can be divided into Ti standard state-action tuples. In addition to the current observation oi\u03c4 , the past decision process {oi1, ai1, \u00b7 \u00b7 \u00b7 , oi\u03c4\u22121, ai\u03c4\u22121} and the task goal gi are the crucial factors that LLMs must consider when making decision at time step \u03c4 . However, the raw data of the past process is relatively long as the prompt context. Therefore, we instruct the LLMs to summarize them into the brief historical information:\nhi\u03c4 = LLMsummary(o i 1, a i 1, \u00b7 \u00b7 \u00b7 , oi\u03c4\u22121, ai\u03c4\u22121; gi).\n(6) The instruction of this summary process require the LLMs to briefly describe the past experiences and assess the progress of tasks in the current state. The historical information also provide LLMs agent with planning information, indicating the decision stage it has reached and assisting the agent in making proper decisions. The complete prompt of the summary process is listed in the appendix A.\nTo sum up, as indicted in Eq. (1), a standard stateaction tuple of human trajectory ti at time step \u03c4 contains four elements: task goal gi, agent history hi\u03c4 , current observation o i \u03c4 , and the current action aj\u03c4 demonstrated in Eq. (2). As shown in Fig. 2,\nthe blue blocks in the left represents the obtained state-action tuples from the human trajectories.\nMemory Index To efficiently store and retrieve tuples from the memory M, we construct the following index system including two types of indices: goal index and observation index.\nFirst, we cluster the goals of different tasks in each batch memory to form its goal index, which is achieved by instructing the LLMs to generate highlevel types of received information and classify each task goal to the corresponding type:\nindexbg = LLMcluster ( g1b , g 2 b , \u00b7 \u00b7 \u00b7 , gBb ) , (7)\nwhere indexbg is the goal index of batch memory Mb, b = 1, 2, \u00b7 \u00b7 \u00b7 , n and g\u2113b is the goal of trajectory t(b\u22121)B+\u2113, \u2113 = 1, 2, \u00b7 \u00b7 \u00b7 , B. Then, we cluster the observations of each goal type by instructing the LLMs to classify all observations into a high-level type to form the observation index:\nindexbko = LLMcluster ( obk1 , \u00b7 \u00b7 \u00b7 , obkZbk ) , (8)\nwhere indexbko is the observation index of goal type k in the batch memory Mb and Zbk denotes the total quantity of observations in goal type k.\nIn the inference phrase, the LLMs agent firstly leverage the goal index to retrieve the similar tasks and then use the observation index to find the similar situations with the current state. The complete prompt of the cluster process is listed in appendix A. As shown in Fig. 2, the green and yellow blocks are the classified data based on the goals and observations. Compared with traditional clustering methods, LLMs based clustering could work with text-based input and generate text-based output instead of numerical data representations, which is more flexible and effective to capture complex semantic relationships in text-rich decision environment."
        },
        {
            "heading": "3.3 Memory Refinement",
            "text": "The above memory formation stage follows the imitation learning paradigm, which provides LDM2 a initial policy. To improve the policy dynamically, we adopt tree exploration, which mimics online reinforcement learning, to enhance the memory by adding the most valuable state-action tuples into M.\nTree Exploration We leverage the tree exploration to generate more possible decision processes\nthrough splitting more leaf nodes at each parent nodes. For task goal gi, at each time step \u03c4 , we instruct the LLMs to provide some possible actions based on the current observation and the memory M, and we prompt the LLMs to assign a confidence score to each action (the complete prompt is listed in appendix A).\naj1\u03c4 , c j1 \u03c4 , ..., a jz \u03c4 , c jz \u03c4 = LLM(g j , hj\u03c4 , o j \u03c4 , p j \u03c4 ), (9)\nIf the action distribution of retrieved state-action tuples in each node is highly concentrated, the LLMs will select the majority action and proceed to the next state. Otherwise, the LLMs will retain all admissible actions and explore a subtree for each action. Meanwhile, we maintain the confidence of each exploration path, which is the product of confidence score of all nodes along the path. To avoid the exponential growth of exploration paths, we only retain top-n (n=4) confidence paths for the the next step of exploration and prune the valueless exploration paths. Finally, we obtain top-n (n=4) decision processes and get the final rewards at the leaf nodes.\nMemory Enhancement The tree exploration generates a set of decision process P i1, P i 2, \u00b7 \u00b7 \u00b7 with reward r1, r2, \u00b7 \u00b7 \u00b7 , respectively. The process with the maximal reward is the best decision process P i\u22c6, and the process which is formed by instructing the LLMs to generate one best decision process based on the memory M is the ground decision process P ig . If P i \u22c6 have a higher reward than P i g , we then instruct the LLMs to compare these processes and find the key decision step in the P i\u22c6. Then we treat the subtrajectory after the key steps as the valuable data to enhance the memory:\n{oi\u03c4\u22c6 , ai\u03c4\u22c6 , \u00b7 \u00b7 \u00b7 , oiTi , a i Ti} = LLMcompare(P i \u22c6, P i g), (10) where \u03c4\u22c6 is the key step in the best decision process given by the LLMs.\nThe obtained valuable subtrajectory is converted into standard state-action tuples using the method depicted in the above sections. For each pair, we leverage the goal index and observation index to find the corresponding categories and directly add this tuples into this subset of the memory. Adding these new valuable state-action tuples into the memory M changes the distribution of the action space. As shown in the right of Fig. 2, the LLMs conduct tree exploration to generate more decision processes. The best and ground processes in the\nleaf nodes are converted to state-action tuples to enhance the memory M which are marked by the red arrows."
        },
        {
            "heading": "4 Experiment Setup",
            "text": ""
        },
        {
            "heading": "4.1 Tasks and Datasets",
            "text": "We evaluate LDM2 on two language-based interactive decision-making tasks: ALFWorld and WebShop. Both are complex environments with various observations and actions that are difficult to be addressed through fixed examples prompt.\nWebShop WebShop (Yao et al., 2022a) is a simulated e-commerce website environment with realworld products and crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and make actions to find, customize, and purchase the required product. The performance is evaluated by average score and sucess rate, the former the percentage of desired attributes covered by the chosen product averaged across all episodes, and the latter is the percentage of episodes where the chosen product satisfies all requirements on 500 test instructions.\nALFworld ALFworld (Shridhar et al., 2020) is a suite of text-based environments which require the agent to accomplish multi-step tasks in a variety of interactive environments based on TextWorld (C\u00f4t\u00e9 et al., 2019). ALFworld includes six types of goals (e.g. picking specified objects and putting in designated place, examining objects by specific instructions and manipulating the objects through specific means). The agent needs to navigate and interact with a simulated household to determine the actions. We conduct experiments on 134 test games, the result is scored by the success rate that is the percentage of episodes which achieves the given goals."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare LDM2 with two prompt-based approaches using complete examples and traditional imitation learning methods trained with annotated trajectories. In all experiments, we employ GPT3.5 (gpt-3.5-turbo) as the workhorse LLM.\nStandard We directly leverage few-shot successful example decision processes as context to prompt the LLMs for both tasks.\nReAct The prompts in the ReAct (Yao et al., 2022b) include not only the observations and actions, but also verbal reasoning traces to guide the LLMs perform dynamic reasoning in the decision process.\nReflexion Reflexion (Shinn et al., 2023) is an extension of ReAct, which uses self-reflect to convert binary or scalar feedback from the environment into verbal feedback and repeats the same task many times based on the reflection.\nImitation Learning BUTLER (Shridhar et al., 2020) is an imitation learning agent for ALFWorld tasks trained on a large amount of human trajectories. WebShop (Yao et al., 2022a) finetunes multiple language models to learn how to search and choose from various shopping processes."
        },
        {
            "heading": "4.3 Training Setup",
            "text": "In the LDM2 memory formation stage, we use 500 human shopping trajectories to construct the WebShop training memory and 200 expert trajectories for each task in the ALFWorld to form the memory. We set the batch size as 100 for WebShop and 50 for ALFWorld to construct multiple independent batch memories for both tasks. In the LDM2 memory refinement stage, we utilize 100 new instructions to explore the WebShop environment and 10 new goals for each task to explore the ALFWorld environment."
        },
        {
            "heading": "5 Results and Analyses",
            "text": ""
        },
        {
            "heading": "5.1 WebShop",
            "text": "As shown by Tab. 1, our LDM2 outperforms all baselines in both score and success rate, which indicates the effectiveness of leveraging state-action\ntuples as dynamic prompts to instruct LLMs to make decisions. The traditional Imitation Learning method finetunes a language model with medium size, which results in poorer performance compared with the prompt-based method using LLMs. Also, the poor performance of the standard prompting method validates that fixed prompt of complete examples is not suitable for decision-making tasks. Standard prompting methods may select products in the same order as demonstrated in the examples, instead of selecting the proper products according to the instructions. Additionally, they may generate actions that similar to the examples but not executable in the current environment. Though ReAct adds dynamic reasoning into the prompt, it may make incorrect reasoning when encountered with different situations. For instance, the ReAct prompts include thought process of clicking corresponding attribute options, thus it may imitate the given reasoning process to click one option even\nwhen no option exists in the current situation.\nAnalysis on the Initial Memory The result of experiment on initial memory outperforms other baselines, which demonstrates the effectiveness of using initial memory to provide sufficient stateaction tuples for the LLMs. State-action tuple examples of the current situation assist the LLMs in understanding the patterns of the environment instead of merely imitating the actions of existing examples. As shown in the upper left of Tab. 3, ReAct first selects one of the most plausible product from the list of products, but the product detail is not fully aligned with the instruction. However, the LLMs still imitate the examples to purchase this one. The proposed LDM2 also select this product, but it clicks the \u201cPrev\u201d button to search another one after finding that the detail is not matched. The reason is that the retrieved memory in this state includes same situations in which the products are not the best choice, and a click on \u201cPrev\u201d to find another product is illustrated in the examples.\nAnalysis of the Refined Memory The performance of model equipped with the refined memory is better than the one with only the initial memory, which shows the effectiveness of updating the memory by exploring more valuable decision processes. The nodes which generate multiple actions in the tree exploration are mainly the products selection nodes. As shown in the upper right of Tab. 3, based on the initial memory, LDM2 selects the most plausible products, but these products do not include the specified options (size 11 women), which results in a lower reward. However, LDM2 with the refined memory explores more possible products and finds the product matching all options. In addition, the tree exploration can expand and enrich\nthe initial memory to help the model revise actions in case the state-action tuples in the initial memory are not sufficient."
        },
        {
            "heading": "5.2 ALFWorld",
            "text": "According to Tab. 2, LDM2 also outperforms all baselines evaluated on tasks in ALFWorld. The prompt-based LLMs outperform the traditional deep learning method. In ALFWorld, finding the desired object and performing correct operations often involves many steps, which results in a loss of the current state\u2019s tracking when received prompt with long examples. Despite its dynamic reasoning ability, ReAct still generates incorrect actions due to the long decision process and unseen situations.\nAnalysis on the Initial Memory The imitation learning has a better performance than other baselines, which shows LDM2 can form a valid initial memory for ALFWorld tasks. In the memory formation stage, the LLMs not only cluster the household items into many high-level types like furniture, kitchen ware, and electronic devices, but also cluster the observations as kitchen room, bathroom, and bedroom. The state-action tuples in different subsets of the memory guide LDM2 to go to the most likely place to find the desired object, take the desired object, manipulate the object correctly, and then put the object in the designated place. As\nshown in the bottom left of Tab. 3, ReAct takes first sopabar from cabinet and then falls into confusion, as it does not know where to find the second item. Based on the memory, the human experience teaches LDM2 to go to the most likely place (countertop) instead of exploring all places, thus finds the items efficiently.\nAnalysis on the Refined Memory The nodes split in the ALFWorld are the most possible selection nodes. In the memory refinement stage, LDM2 generates some possible places to explore the environment. LDM2 finds more convenient and fast ways to complete the goals and adds these tuples into the memory. Meanwhile, in some cases where there are no analogous situations to the test task in the memory, the tree exploration process can assist the LLMs in exploring the common appearing places of the unseen items. As shown in the bottom right of Tab. 3, LDM2 fails to find the desired item in the new environment based on the past human experience, but the tree exploration help find the item by providing more possible places."
        },
        {
            "heading": "5.3 Analysis of Training Data",
            "text": "As the batch data represents human experience, we expect that the performance of LDM2 will increase as the data size increases. Thus we conduct experiment to find the relationship between score/success\nrate and the size of the batch data. As shown by Fig. 3, the score/success rate increases in both tasks as the size of batch data increases, which demonstrates that more human trajectories can enhance the LLMs\u2019 ability to make more proper actions in the current state. In WebShop, more data means more types of products to help the LLMs learn what to search or click. In ALFWorld, more data provides the LLMs more information about where the desired objects may appear and how to manipulate them correctly."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper proposed LDM2, which enhances the standard LLMs with dynamic updating memory to maintain the most valuable state-action tuples to imitate human decision. LDM2 consists of two stage: memory formation and memory refinement. In the formation stage, we take human trajectories as training data and instruct the LLMs to produce numerous standard state-action tuples. These standard state-action tuples are preserved to form the initial memory. LDM2 is equipped with dynamic memory refinement stage to enhance the memory through adding valuable state-action tuples. We conduct tree exploration to generate all potential decision processes and add the state-action tuple corresponding to the higher reward decision process into the memory. Experiments on two interactive environments illustrated that LDM2 outperforms the standard few shots prompting methods and the ablation study verified the effectiveness of the memory formation and refinement mechanism.\nLimitations\nTo fully leverage the capabilities of LDM2, we need to collect a certain amount of high-quality human trajectories, which may be difficult and infeasible in some environments. Hence, we need to stimulate the LLMs\u2019 own reasoning and understanding ability when having few data to interact with the environments.\nMeanwhile, LDM2 bears a higher time cost compared with other standard prompting methods. In the inference phrase, LDM2 needs to retrieve relevant state-action tuples in the memory, which results in a significant lower action generation speed. However, the computational cost of our method is comparable with other methods, because the prompt length of each timestep of our method is shorter and only the current state needs to be con-\nsidered, but other methods must record the whole past experience which will increase as time goes by. The amount of inference tokens of our method and existing methods are roughly equal."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the Strategic Priority Research Program of Chinese Academy of Sciences under Grant #XDA27030100 and the National Natural Science Foundation of China under Grants #72293573, #72293575 and #62206282."
        },
        {
            "heading": "A Prompt Details",
            "text": "All the prompts used in this paper to instruct the LLMs are shown in Tab. 4."
        },
        {
            "heading": "B Example Process",
            "text": "Tab. 5 and Tab. 6 provide an example decision process in the WebShop task produced by the proposed LDM2. Due to the long decision-making process, we omit some parts of prompts.\nPrompt1: 1. Instruction: i need a pair of shoes with rubber soles. remember to get size seven and a half womens, and price lower than 40.00 dollars. Past: Action: search[shoes, rubber soles, size 7.5, women\u2019s less than 40] 2. Instruction: can you find some carol wright mens lounge pants in a 5xl? i want the ones with a draw string closure that are charcoal colored, and price lower than 40.00 dollars Past: Action: search[5xl carol wright men\u2019s lounge pants charcoal color] 3. Instruction: i need some black ankle strap flats that are in a size 9 wide, and price lower than 60.00 dollars Past: Action: search[black ankle strap flats in a \"size 9 wide\"] 4. Instruction: i need a fleece jacket for the winter that is warm and gray, and price lower than 40.00 dollars Past: Action: search[winter fleece jacket gray warm] 5. Instruction: i want a extra large yellow mens loose fit shirt, and price lower than 40.00 dollars Past: Action: search[extra large yellow men\u2019s loose fit shirt] Observation1: Instruction: Find me long lasting, moisture wicking, loose fit men\u2019s shorts with elastic waistband, quality materials, polyester cotton, short sleeve with color: black, and size: 4x-large, and price lower than 50.00 dollars. Action1: search[long lasting moisture wicking loose fit men\u2019s shorts with elastic waistband black 4x-large polyester cotton] Prompt2: 1. Past: Searched for 5XL Carol Wright men\u2019s lounge pants in charcoal color. The interface is: [Back to Search] Page 1 (Total results: 50) [Next >] [B08B6D39FM] Carol Wright Gifts Men\u2019s Comfy Lounge Pant 14.99 to 17.99 [B075RCSFJ5] Carol Wright Gifts Men\u2019s Fleece Lounge Pants by Cozee Corner 14.99 to 17.99 [B003LUVGVI] Carol Wright Gifts Women\u2019s Flats | Comfortable Flats for Women | Women\u2019s Dress Flats 21.99 to 32.99 [B08BJ9VXVY] Carol Wright Gifts Comfy Slip-On 21.99 \u00b7 \u00b7 \u00b7 Action: click[b08b6d39fm] 2. Past: Searching for women\u2019s shoes with rubber soles in size 7.5 for less than 40. The interface is: [Back to Search] Page 1 (Total results: 50) [Next >]\n[B0751HY5BG] ASICS Men\u2019s Gel-Lyte Gymnastics Shoes 35.34 to 77.99 [B09L5YX71C] Foggs Japan Slippers, Cozislides Original, CloudFeet Ultra-Soft Slippers, EVA Universal Quickdrying Thickened Non-slip Sandals 100.0 [B09PQXCB3V] ZSpzx Women\u2019s Fashion Sneakers Lace Up Casual Round Toe Arch Support Comfortable Low Top Sneakers Glitter Classic Leather Cushion Foam Comfort Loafers Dress Shoes Ladies Walking Flat Shoes 24.99 to 32.8 \u00b7 \u00b7 \u00b7 Action: click[b0751hy5bg] \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation2: Past: Searched for black 4x-large polyester cotton loose fit men\u2019s shorts with elastic waistband that have long lasting moisture wicking. The interface is: [Back to Search] Page 1 (Total results: 50) [Next >] [B09Q5ZHRVM] Yinimo Mens Gym Shorts Fashionable Loose Beach Drawstring Turkey Print Beach Cool Shorts 19.19 [B09QQP3356] HAUKLIE Men\u2019s Sports Waffle Ribbed Polo Shirts Summer Short Sleeve Cotton Muscle Quarter-Zip Henley T-Shirt Tunics Tops 10.99 [B09ND8P2QR] InterestPrint Deer and Stars Men\u2019s Loungewear Pajama Sets - Long Sleeve Tee and Jogger Pant 43.59 Action2: click[B09Q5ZHRVM] Prompt3: 1.Past: Searched for black ankle strap flats in size 9 wide and clicked on b09r2dhbd3. The interface is: [Back to Search] [< Prev] color [aa04-black] [aa04-brown] [aa04-khaki] [aa03-beige] [aa03-black] [aa02-yellow] [aa-01 beige] [aa-01 brown] size [5.5] [6] [6.5] [6.5-7] [7] [7.5] [8] [8.5] [9] [9.5-10] [10] [10.5] [11] [11.5] Women Flats Sandals for Summer,Fashion Casual Flat Shoes Solid Color Denim Zipper Sandals Buckle Strap Open Toe Flip Flops Price: 7.07 to 8.23 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] Action: click[aa03-black] 2.Past: Searched for black ankle strap flats in size 9 wide, clicked on b09r2dhbd3, and then clicked on aa03-black.\nThe interface is: [Back to Search] [< Prev] color [aa04-black] [aa04-brown] [aa04-khaki] [aa03-beige] [aa03-black] [aa02-yellow] [aa-01 beige] [aa-01 brown] size [5.5] [6] [6.5] [6.5-7] [7] [7.5] [8] [8.5] [9] [9.5-10] [10] [10.5] [11] [11.5] Women Flats Sandals for Summer,Fashion Casual Flat Shoes Solid Color Denim Zipper Sandals Buckle Strap Open Toe Flip Flops Price: 7.07 to 8.23 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] Action: click[9] \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation3: Past: Searched for and clicked on black 4x-large polyester cotton men\u2019s shorts with elastic waistband that have long lasting moisture wicking and a loose fit. The interface is: [Back to Search] [< Prev] color [black] [blue] [red] size [small] [medium] [large] [x-large] [xx-large] [3x-large] [4x-large] [5x-large] Yinimo Mens Gym Shorts Fashionable Loose Beach Drawstring Turkey Print Beach Cool Shorts Price: 19.19 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] Action3: click[black] Prompt4: \u00b7 \u00b7 \u00b7 2.Past: Searched for black ankle strap flats in size 9 wide, clicked on b09r2dhbd3, and then clicked on aa03-black. The interface is: [Back to Search] [< Prev] color [aa04-black] [aa04-brown] [aa04-khaki] [aa03-beige] [aa03-black] [aa02-yellow] [aa-01 beige] [aa-01 brown] size [5.5] [6] [6.5] [6.5-7] [7] [7.5] [8] [8.5] [9] [9.5-10] [10] [10.5] [11] [11.5] Women Flats Sandals for Summer,Fashion Casual Flat Shoes Solid Color Denim Zipper Sandals Buckle Strap Open Toe Flip Flops Price: 7.07 to 8.23 Rating: N.A. [Description] [Features] [Reviews] [Buy Now]\nAction: click[9] 3.Past: Searched for black ankle strap flats in size 9 wide, clicked on b09r2dhbd3, then clicked on aa03-black and finally clicked on size 9. The interface is: [Back to Search] [< Prev] color [aa04-black] [aa04-brown] [aa04-khaki] [aa03-beige] [aa03-black] [aa02-yellow] [aa-01 beige] [aa-01 brown] size [5.5] [6] [6.5] [6.5-7] [7] [7.5] [8] [8.5] [9] [9.5-10] [10] [10.5] [11] [11.5] Women Flats Sandals for Summer,Fashion Casual Flat Shoes Solid Color Denim Zipper Sandals Buckle Strap Open Toe Flip Flops Price: 7.07 to 8.23 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] Action: click[buy now] \u00b7 \u00b7 \u00b7 Observation4: Past: Searched for and clicked on black, long-lasting, moisture-wicking, loose-fit men\u2019s shorts with elastic waistband in 4X-large size made of polyester cotton. The interface is: [Back to Search] [< Prev] color [black] [blue] [red] size [small] [medium] [large] [x-large] [xx-large] [3x-large] [4x-large] [5x-large] Yinimo Mens Gym Shorts Fashionable Loose Beach Drawstring Turkey Print Beach Cool Shorts Price: 19.19 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] You have clicked black. Action4: click[4x-large] Prompt5: \u00b7 \u00b7 \u00b7 1.Past: Past: Searched for black ankle strap flats in size 9 wide, clicked on b09r2dhbd3, then clicked on aa03-black and finally clicked on size 9. The interface is: [Back to Search] [< Prev] color [aa04-black] [aa04-brown] [aa04-khaki] [aa03-beige] [aa03-black] [aa02-yellow] [aa-01 beige] [aa-01 brown] size [5.5] [6] [6.5] [6.5-7] [7] [7.5] [8] [8.5] [9] [9.5-10] [10] [10.5] [11] [11.5] Women Flats Sandals for Summer,Fashion Casual Flat Shoes Solid Color Denim Zipper Sandals Buckle Strap Open Toe Flip Flops Price: 7.07 to 8.23 Rating: N.A.\n[Description] [Features] [Reviews] [Buy Now] Action: click[buy now] 2.Past: Searched for 5XL Carol Wright men\u2019s lounge pants in charcoal color, clicked on item code B08B6D39FM, selected size 5X-large, and chose the color charcoal. The interface is: [Back to Search] [< Prev] size [small] [medium] [large] [x-large] [xx-large] [3x-large] [4x-large] [5x-large ] color [black] [charcoal] [gray] [heather blue] [navy] [plaid] [olive] Carol Wright Gifts Men\u2019s Comfy Lounge Pant Price: 14.99 to 17.99 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] Action: click[buy now] \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation5: Past: Searched for and clicked on black 4x-large polyester cotton moisture wicking loose fit men\u2019s shorts with elastic waistband (ASIN: B09Q5ZHRVM). The interface is: [Back to Search] [< Prev] color [black] [blue] [red] size [small] [medium] [large] [x-large] [xx-large] [3x-large] [4x-large] [5x-large] Yinimo Mens Gym Shorts Fashionable Loose Beach Drawstring Turkey Print Beach Cool Shorts Price: 19.19 Rating: N.A. [Description] [Features] [Reviews] [Buy Now] You have clicked black. You have clicked 4x-large. Action5: click[Buy Now]\nTable 5: An Example decision process in the WebShop.\nGoals: cool some apple and put it in diningtable. You are in the middle of a room. Looking quickly around you, you see a cabinet 1, a cabinet 10, a cabinet 11, a cabinet 2, a cabinet 3, a cabinet 4, a cabinet 5, a cabinet 6, a cabinet 7, a cabinet 8, a cabinet 9, a coffeemachine 1, a countertop 1, a countertop 2, a diningtable 1, a drawer 1, a drawer 2, a drawer 3, a fridge 1, a garbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 1, a stoveburner 2, a stoveburner 3, a stoveburner 4, and a toaster 1. Prompt1: 1. Observation: You are in the middle of a room. Looking quickly around you, you see nothing. Evaluation:Past: Subgoal 1: - Incomplete Subgoal 2: Cool - Incomplete Subgoal 3: - Incomplete Summary: Unknown. More information is needed to provide a summary of the past process. Action: go to sinkbasin 1 2. Observation: You arrive at loc 12. On the sinkbasin 1, you see a apple 2, a mug 2, and a potato 1. Evaluation: Past: Subgoal 1: Take apples from somewhere - Incomplete Subgoal 2: Cool apples - Incomplete Subgoal 3: Put apples into dining table - Incomplete Summary: Looked and went to sink basin 1. Action: take apple 2 from sinkbasin 1 3. Observation: You arrive at loc 8. On the sinkbasin 1, you see a egg 1, a mug 1, and a potato 2. Evaluation:Past: Subgoal 1: Take apple from somewhere - Incomplete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked and went to sink basin 1. Action: go to microwave 1 4. Observation: You arrive at loc 10. The microwave 1 is closed. Evaluation:Past: Subgoal 1: Take apple from somewhere - Incomplete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked and then went to sink basin 1 and microwave 1. Action: open microwave 1 5. Observation: You close the microwave 1. Evaluation:Past: subgoal 1: take apple from somewhere - incomplete subgoal 2: cool apple - incomplete subgoal 3: put apple into countertop - incomplete Summary: Looked and went to sink basin 1, then went to microwave 1, opened and closed it. Action: go to garbagecan 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation1: Observation: You are in the middle of a room. Looking quickly around you, you see nothing. Evaluation: Subgoal 1: take apple from somewhere -Incomplete Subgoal 2: cool apple -Incomplete Subgoal 3: put apple into countertop -Incomplete Summary: Unknown. More information is needed to provide a summary of the past process. Action1: go to garbagecan 1 Prompt2: 1. Observation: You are in the middle of a room. Looking quickly around you, you see nothing. Evaluation:Past: Subgoal 1: - Incomplete Subgoal 2: Cool - Incomplete Subgoal 3: - Incomplete Summary: Unknown. More information is needed to provide a summary of the past process. Action: go to sinkbasin 1 2. Observation: You arrive at loc 12. On the sinkbasin 1, you see a apple 2, a mug 2, and a potato 1. Evaluation: Past: Subgoal 1: Take apples from somewhere - Incomplete Subgoal 2: Cool apples - Incomplete Subgoal 3: Put apples into dining table - Incomplete Summary: Looked and went to sink basin 1. Action: take apple 2 from sinkbasin 1\n3. Observation: You arrive at loc 8. On the sinkbasin 1, you see a egg 1, a mug 1, and a potato 2. Evaluation:Past: Subgoal 1: Take apple from somewhere - Incomplete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked and went to sink basin 1. Action: go to microwave 1 4. Observation: You arrive at loc 10. The microwave 1 is closed. Evaluation:Past: Subgoal 1: Take apple from somewhere - Incomplete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked and then went to sink basin 1 and microwave 1. Action: open microwave 1 5. Observation: You close the microwave 1. Evaluation:Past: subgoal 1: take apple from somewhere - incomplete subgoal 2: cool apple - incomplete subgoal 3: put apple into countertop - incomplete Summary: Looked and went to sink basin 1, then went to microwave 1, opened and closed it. Action: go to garbagecan 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation2: Observation: On the garbagecan 1, you see a apple 1, and a tomato 1. Evaluation: Subgoal 1: take apple from somewhere -Incomplete Subgoal 2: cool apple -Incomplete Subgoal 3: put apple into countertop -Incomplete Summary: Looked and then went to garbagecan 1. Action2: take apple 1 from garbagecan 1 Prompt3: 1. Observation: You pick up the apple 2 from the sinkbasin 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into dining table - Incomplete Summary: Took apple 2 from sinkbasin 1. Action: go to fridge 1 2. Observation: You arrive at loc 16. The fridge 1 is closed. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into dining table - Incomplete Summary: Looked, went to sinkbasin 1, took apple 2 from sinkbasin 1, and went to fridge 1. Action: cool apple 2 with fridge 1 3. Observation: You pick up the apple 1 from the garbagecan 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked around, went to sinkbasin 1, went to microwave 1, opened and closed microwave 1, went to garbagecan 1, took apple 1 from garbagecan 1. Action: go to fridge 1 4. Observation: You arrive at loc 1. The fridge 1 is closed. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked around, went to sinkbasin 1, went to microwave 1, opened and closed microwave 1, went to garbagecan 1, took apple 1 from garbagecan 1, and went to fridge 1. Action: cool apple 1 with fridge 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nObservation3: Observation: You pick up the apple 1 from the garbagecan 1. Evaluation: Subgoal 1: take apple from somewhere -complete Subgoal 2: cool apple -Incomplete Subgoal 3: put apple into countertop -Incomplete Summary: Looked for apple, found it in garbage can 1. Action3: go to fridge 1 Prompt4: 1. Observation: You pick up the apple 2 from the sinkbasin 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into dining table - Incomplete Summary: Took apple 2 from sinkbasin 1. Action: go to fridge 1 2. Observation: You arrive at loc 16. The fridge 1 is closed. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into dining table - Incomplete Summary: Looked, went to sinkbasin 1, took apple 2 from sinkbasin 1, and went to fridge 1. Action: cool apple 2 with fridge 1 3. Observation: You pick up the apple 1 from the garbagecan 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked around, went to sinkbasin 1, went to microwave 1, opened and closed microwave 1, went to garbagecan 1, took apple 1 from garbagecan 1. Action: go to fridge 1 4. Observation: You arrive at loc 1. The fridge 1 is closed. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Incomplete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked around, went to sinkbasin 1, went to microwave 1, opened and closed microwave 1, went to garbagecan 1, took apple 1 from garbagecan 1, and went to fridge 1. Action: cool apple 1 with fridge 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation4: Observation: The fridge 1 is closed. Evaluation: Subgoal 1: take apple from somewhere -complete Subgoal 2: cool apple -Incomplete Subgoal 3: put apple into countertop -Incomplete Summary: Looked for apple, found it in garbage can 1 and took it. Action4: cool apple 1 with fridge 1 Prompt5: 1. Observation: You cool the apple 2 using the fridge 1. Evaluation:Past: subgoal 1: take apple 2 from somewhere - complete subgoal 2: cool apple 2 - complete subgoal 3: put apple 2 into diningtable - incomplete Summary: Looked for apple 2, took it from sinkbasin 1, and cooled it with fridge 1. Action: go to diningtable 1 2. Observation: You arrive at loc 15. On the diningtable 1, you see a apple 3, a apple 4, a bowl 1, a bowl 2, a butterknife 1, a egg 3, a fork 2, a fork 3, a knife 2, a knife 3, a lettuce 1, a lettuce 2, a lettuce 3, a spatula 3, and a spoon 2.\nEvaluation:Past: Subgoal 1: Take apple 2 from somewhere - Complete Subgoal 2: Cool apple 2 - Complete Subgoal 3: Put apple 2 into diningtable 1 - Complete Summary: Looked for apple 2, took it from sinkbasin 1, cooled it in fridge 1, and placed it on diningtable 1. Action: put apple 2 in/on diningtable 1 3. Observation: You cool the apple 1 using the fridge 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Complete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked for an apple, went to sinkbasin 1, went to microwave 1, opened and closed microwave 1, went to garbagecan 1, took apple 1 from garbagecan 1, went to fridge 1, cooled apple 1 with fridge 1. Action: go to countertop 1 4. Observation: You arrive at loc 15. On the countertop 1, you see a bowl 1, a bread 1, a butterknife 1, a cup 1, a dishsponge 1, a fork 2, a knife 2, a ladle 2, a pan 1, a potato 1, a soapbottle 1, a spatula 1, a spatula 2, a spatula 3, and a spoon 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Complete Subgoal 3: Put apple into countertop - Complete Summary: Looked for an apple, found one in the garbage, cooled it in the fridge, and placed it on the countertop. Action: put apple 1 in/on countertop 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation5: Observation: You cool the apple 1 using the fridge 1. Evaluation: Subgoal 1: take apple from somewhere -complete Subgoal 2: cool apple -complete Subgoal 3: put apple into countertop -Incomplete Summary: Looked for apple 1, took it from garbage can 1, and cooled it with fridge 1. Action5: go to countertop 1 Prompt6: 1. Observation: You cool the apple 2 using the fridge 1. Evaluation:Past: subgoal 1: take apple 2 from somewhere - complete subgoal 2: cool apple 2 - complete subgoal 3: put apple 2 into diningtable - incomplete Summary: Looked for apple 2, took it from sinkbasin 1, and cooled it with fridge 1. Action: go to diningtable 1 2. Observation: You arrive at loc 15. On the diningtable 1, you see a apple 3, a apple 4, a bowl 1, a bowl 2, a butterknife 1, a egg 3, a fork 2, a fork 3, a knife 2, a knife 3, a lettuce 1, a lettuce 2, a lettuce 3, a spatula 3, and a spoon 2. Evaluation:Past: Subgoal 1: Take apple 2 from somewhere - Complete Subgoal 2: Cool apple 2 - Complete Subgoal 3: Put apple 2 into diningtable 1 - Complete Summary: Looked for apple 2, took it from sinkbasin 1, cooled it in fridge 1, and placed it on diningtable 1. Action: put apple 2 in/on diningtable 1 3. Observation: You cool the apple 1 using the fridge 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Complete Subgoal 3: Put apple into countertop - Incomplete Summary: Looked for an apple, went to sinkbasin 1, went to microwave 1, opened and closed microwave 1, went to garbagecan 1, took apple 1 from garbagecan 1, went to fridge 1, cooled apple 1 with fridge 1. Action: go to countertop 1\n4. Observation: You arrive at loc 15. On the countertop 1, you see a bowl 1, a bread 1, a butterknife 1, a cup 1, a dishsponge 1, a fork 2, a knife 2, a ladle 2, a pan 1, a potato 1, a soapbottle 1, a spatula 1, a spatula 2, a spatula 3, and a spoon 1. Evaluation:Past: Subgoal 1: Take apple from somewhere - Complete Subgoal 2: Cool apple - Complete Subgoal 3: Put apple into countertop - Complete Summary: Looked for an apple, found one in the garbage, cooled it in the fridge, and placed it on the countertop. Action: put apple 1 in/on countertop 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Observation6: Observation: You arrive at loc 15. On the countertop 1, you see a bowl 1, a bread 1, a butterknife 1, a cup 1, a dishsponge 1, a fork 2, a knife 2, a ladle 2, a pan 1, a potato 1, a soapbottle 1, a spatula 1, a spatula 2, a spatula 3, and a spoon 1. Evaluation: Subgoal 1: take apple from somewhere -complete Subgoal 2: cool apple -complete Subgoal 3: put apple into countertop -complete Summary: Looked for apple 1, took it from garbage can 1, cooled it in fridge 1, and placed it on countertop 1. Action6: put apple 1 in/on diningtable 1\nTable 6: An Example decision process in the ALFWorld."
        }
    ],
    "title": "LDM: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement",
    "year": 2023
}