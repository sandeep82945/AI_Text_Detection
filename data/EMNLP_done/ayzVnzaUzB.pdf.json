{
    "abstractText": "Though majority vote among annotators is typically used for ground truth labels in machine learning, annotator disagreement in tasks such as hate speech detection may reflect systematic differences in opinion across groups, not noise. Thus, a crucial problem in hate speech detection is determining if a statement is offensive to the demographic group that it targets, when that group may be a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to predict the ratings of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators\u2019 ratings and by 33% at predicting variance among annotators, which provides a metric for model uncertainty downstream. We find that annotators\u2019 ratings can be predicted using their demographic information as well as opinions on online content, and that non-invasive questions on annotators\u2019 online experiences minimize the need to collect demographic information when predicting annotators\u2019 opinions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Eve Fleisig"
        },
        {
            "affiliations": [],
            "name": "Rediet Abebe"
        },
        {
            "affiliations": [],
            "name": "Dan Klein"
        }
    ],
    "id": "SP:7e924fd9db5c07f0f1461791d6b16830c820c98c",
    "references": [
        {
            "authors": [
                "Aida Mostafazadeh Davani",
                "Mark D\u00edaz",
                "Vinodkumar Prabhakaran."
            ],
            "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
            "venue": "Transactions of the Association for Computational Linguistics, 10:92\u2013110.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Philip Dawid",
                "Allan M Skene."
            ],
            "title": "Maximum likelihood estimation of observer errorrates using the em algorithm",
            "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1):20\u201328.",
            "year": 1979
        },
        {
            "authors": [
                "Eve Fleisig",
                "Aubrie Amstutz",
                "Chad Atalla",
                "Su Lin Blodgett",
                "Hal Daum\u00e9 III",
                "Alexandra Olteanu",
                "Emily Sheng",
                "Dan Vann",
                "Hanna Wallach."
            ],
            "title": "FairPrism: Evaluating fairness-related harms in text generation",
            "venue": "Proceedings of the 61st Annual Meeting",
            "year": 2023
        },
        {
            "authors": [
                "Tommaso Fornaciari",
                "Alexandra Uma",
                "Silviu Paun",
                "Barbara Plank",
                "Dirk Hovy",
                "Massimo Poesio."
            ],
            "title": "Beyond black & white: Leveraging annotator disagreement via soft-label multi-task learning",
            "venue": "Proceedings of the 2021 Conference of the North Amer-",
            "year": 2021
        },
        {
            "authors": [
                "Mitchell L. Gordon",
                "Michelle S. Lam",
                "Joon Sung Park",
                "Kayur Patel",
                "Jeff Hancock",
                "Tatsunori Hashimoto",
                "Michael S. Bernstein."
            ],
            "title": "Jury learning: Integrating dissenting voices into machine learning models",
            "venue": "Proceedings of the 2022 CHI Conference on Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Kumar",
                "Patrick Gage",
                "Sunny Consolvo",
                "Joshua Mason",
                "Elie Bursztein",
                "Zakir Durumeric",
                "Kurt Thomas",
                "Michael Bailey."
            ],
            "title": "Designing toxic content classification for a diversity of perspectives",
            "venue": "SOUPS. Usenix.",
            "year": 2021
        },
        {
            "authors": [
                "Savannah Larimore",
                "Ian Kennedy",
                "Breon Haskett",
                "Alina Arseniev-Koehler"
            ],
            "title": "Reconsidering annotator disagreement about racist language: Noise or signal",
            "venue": "In Proceedings of the Ninth International Workshop on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Jennimaria Palomaki",
                "Olivia Rhinehart",
                "Michael Tseng."
            ],
            "title": "A case for a range of acceptable annotations",
            "venue": "SAD/CrowdBias@HCOMP.",
            "year": 2018
        },
        {
            "authors": [
                "Desmond Upton Patton",
                "Philipp Blandfort",
                "William R. Frey",
                "Michael B. Gaskell",
                "Svebor Karaman."
            ],
            "title": "Annotating social media data from vulnerable populations: Evaluating disagreement between domain experts and graduate student annotators",
            "venue": "Hawaii",
            "year": 2019
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Tom Kwiatkowski."
            ],
            "title": "Inherent disagreements in human textual inferences",
            "venue": "Transactions of the Association for Computational Linguistics, 7:677\u2013694.",
            "year": 2019
        },
        {
            "authors": [
                "Vinodkumar Prabhakaran",
                "Aida Mostafazadeh Davani",
                "Mark Diaz."
            ],
            "title": "On releasing annotator-level labels and information in datasets",
            "venue": "Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR)",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2018
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "The risk of racial bias in hate speech detection",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668\u20131678, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Laura Vianna",
                "Xuhui Zhou",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Ruyuan Wan",
                "Jaehyung Kim",
                "Dongyeop Kang."
            ],
            "title": "Everyone\u2019s voice matters: Quantifying annotation disagreement using demographic information",
            "venue": "arXiv preprint arXiv:2301.05036.",
            "year": 2023
        },
        {
            "authors": [
                "Zeerak Waseem."
            ],
            "title": "Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter",
            "venue": "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138\u2013 142, Austin, Texas. Association for Computational",
            "year": 2016
        },
        {
            "authors": [
                "Gordon"
            ],
            "title": "2022, who also use a test set",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "For many machine learning tasks in which the ground truth is clear, having multiple people label examples, then averaging their judgments, is an effective strategy: if nearly all annotators agree on a label, the rest were likely inattentive. However, the assumption that disagreement is noise may no longer hold if the task is more subjective, in the sense that the ground truth label varies by person. For example, the same text may truly be offensive to some people and not to others (Figure 1). Recent work has questioned several assumptions of majority-vote aggregated labels: that there is\n\u2020Corresponding author: efleisig@berkeley.edu\na single, fixed ground truth; that the aggregated judgments of a group of annotators will accurately capture this ground truth; that disagreement among annotators is noise, not signal; that such disagreement is not systematic; and that aggregation is equally suited to very straightforward tasks and more nuanced ones (Larimore et al., 2021; Palomaki et al., 2018; Pavlick and Kwiatkowski, 2019; Prabhakaran et al., 2021; Sap et al., 2022; Waseem, 2016).\nIn tasks such as hate speech detection, annotator disagreement often results not from noise, but from differences among populations, e.g., demographic groups or political parties (Larimore et al., 2021; Sap et al., 2022). Thus, training with aggregated annotations can cause the opinions of minoritized groups with greater expertise or personal experience to be overlooked (Prabhakaran et al., 2021).\nIn particular, annotators who are members of the group being targeted by a possibly offensive statement often differ in their opinions from the majority of annotators who rate the text. Because these annotators may have relevant lived experi-\nence or greater familiarity with the context of the statement, understanding when and why their opinions differ from the majority can help to capture cases where the majority is wrong or opinions are truly divided over whether the example is offensive.\nWe construct a model that predicts individual annotators\u2019 ratings on the offensiveness of text, as well as the potential target group(s)1 of the text, and combines this information to predict the ratings of target group members on the offensiveness of the example. Compared to a baseline that does not model individual annotators, we raise performance by 22% at predicting individual annotators\u2019 ratings, 33% at predicting variance among annotators, and 22% at predicting target group members\u2019 aggregate ratings. We find that annotator survey responses and demographic features allow ratings to be modeled effectively, and that survey questions on annotators\u2019 online experiences allow annotators\u2019 ratings to be predicted while minimizing intrusive collection of demographic information."
        },
        {
            "heading": "2 Motivation and Related Work",
            "text": "Our work draws on recent studies that investigate the causes of disagreement among annotators. Pavlick and Kwiatkowski (2019) and Palomaki et al. (2018) note that disagreement among human annotations is not necessarily noise because there is a plausible range of human judgments for some tasks, rather than a single ground truth. Waseem (2016) finds that less experienced annotators are more likely to label items as hate speech than more experienced ones. When evaluating gang-related tweets, domain experts\u2019 familiarity with language use in the community resulted in more informed judgments compared to those of annotators without relevant background (Patton et al., 2019). Annotator perception of racism varies with annotator race and political views (Larimore et al., 2021; Sap et al., 2022) and awareness of speaker race and dialect also affects annotation, such that annotators were less likely to find text in African-American English (AAE) offensive if they were told that the text was in AAE or likely written by a Black author (Sap et al., 2019).\nIn addition, aggregation obscures crucial differences in opinion among annotators. For example, aggregated labels align more with the opinions of\n1Throughout the paper, we use \u201ctarget group\u201d or \u201ctarget groups\u201d to mean the demographic groups to which a text may cause harm, i.e., the groups that are the subject of the text\u2019s stereotyping, demeaning, or otherwise hateful content.\nWhite annotators than those of Black annotators because Black annotators are underrepresented in typical annotation pools (Prabhakaran et al., 2021).\nThis motivates our first objective, identifying crucial examples where target group members disagree. Disagreement among annotators may stem from a variety of sources, including level of experience, political background, and background experience with the topic on which they are annotating. That is, only some disagreement stems from the annotators who disagree being better-informed annotators or key stakeholders. We argue that if a statement may stereotype, demean, or otherwise harm a demographic group, members of that demographic group are typically well-suited to determining whether the statement is harmful: both from a normative standpoint, as they would be the group most affected, and because they likely have the most relevant lived experience regarding the harms that the group faces.\nHowever, majority vote aggregation often obscures the opinion of the demographic being targeted by a statement, and so explicitly modeling the opinion of the target group provides a useful source of information for downstream decisions. Thus, finding cases where members of the group targeted by a harmful statement disagree with the majority opinion is crucial to determining whether the majority opinion is actually a useful label on a particular example, and provides a vital source of information for downstream decisions on whether content should be filtered. We address this by constructing a model that predicts the target group(s) of a statement, then predicts the individual ratings of annotators who are target group members (Section 3.1).\nDrawing on earlier approaches such as Dawid and Skene (1979), newer work has used disagreement between annotators as a source of information. Fornaciari et al. (2021) use probability distributions over annotator labels as an auxiliary task to reduce model overfitting. Davani et al. (2022) use multitask models that predict each annotator\u2019s rating of an example, then aggregate these separate predictions to produce a final majority-vote decision as well as a measure of uncertainty based on the variance among predicted individual annotators\u2019 ratings of the example. Wan et al. (2023) directly predict the degree of disagreement among annotators on an example using annotator demographics, and simulate potential annotators to predict variance in\nthe degree of disagreement. Gordon et al. (2022) predict individual annotator judgments based on ratings linked to individual annotators and annotators\u2019 demographic information. For each example, their system returns a distribution and overall verdict from a \u201cjury\u201d of annotators with demographic characteristics chosen by an evaluator in an interactive system.\nThese approaches motivate our second objective, using disagreement as an independent source of information. Previous work, such as Gordon et al. (2022), uses human-in-the-loop supervision to find alternatives to aggregated majority vote. Since human judgments depend on the person\u2019s background and level of experience with the language usage and content of a text, not all evaluators may be able to correctly identify the target group to select an appropriate jury; their own background may also influence their jury selection (e.g., through confirmation bias) and thus the final system judgment. Gordon et al. (2022) investigate the benefits of the interactive approach; we extend on this work to examine the question of how to account for annotator disagreement without human-in-the-loop supervision. Our model directly identifies the target group(s) of a statement and predicts the ratings of target group members, providing an independent source of information about possible errors by the majority of annotators. Our model\u2019s predictions can be used as a source of information by human evaluators in conjunction with humanin-the-loop approaches, or function independently (Section 3.2).\nOur third objective considers how to minimize collection of annotators\u2019 demographic or identifiable information. Using individual annotator IDs that map annotators to their ratings permits fine-grained prediction of individual annotator ratings. However, this information is often unavailable (e.g., in the data used by Davani et al., 2022) and its collection may raise privacy concerns for sensitive tasks. Our method uses demographic information and survey responses regarding online preferences to predict annotator ratings without the need to individually link responses to annotators (Section 4.1).\nIn addition, approaches to predicting annotator opinions that rely on extensive collection of demographic information also raise privacy concerns, particularly when asking about characteristics that are the basis for discrimination in some\nenvironments (e.g., sexual orientation). Thus, an open question is whether annotator ratings may be predicted while using less invasive questions and minimizing unnecessary demographic data collection. We find that survey information about online preferences provides an extremely useful alternate source of information for predicting annotator ratings, while remaining less invasive and thus easier to obtain than extensive demographic questions (Section 4.4)."
        },
        {
            "heading": "3 Approach",
            "text": "Our approach consists of two connected modules that work in parallel. Given information about the annotator and the text itself, the rating prediction module predicts the rating given by each annotator who labeled a piece of text (Figure 2, in red). For this prediction task, we used a RoBERTa-based module (Liu et al., 2019) initially fine-tuned on the Jigsaw toxicity dataset, which we then fine-tuned on the hate speech detection dataset from Kumar et al. (2021). The target group prediction module predicts the demographic group(s) harmed by the input text (Figure 2, in blue). For this prediction task, we fine-tuned a GPT-2 based module on the text examples and annotated target groups from the Social Bias Frames dataset (Sap et al., 2020). At test time, our model predicts the target group for the input text, then predicts the rating that the members of the target group would give to that text (Figure 2, in purple)."
        },
        {
            "heading": "3.1 Individual Rating Prediction Module",
            "text": "Motivated by the question of whether information about participants besides directly tracking their judgments (e.g. with IDs) can help to predict participant judgments, we experimented with inclusion of two kinds of annotator information: (1) demographic information and (2) responses to a survey indicating annotator preferences and experiences with online content.\nWe obtained this information from Kumar et al. (2021)\u2019s dataset, in which each example is labeled by five annotators and each annotator labeled 20 examples. For each annotator label, demographic information is provided on the annotator\u2019s race, gender, importance of religion, LGBT status, education, parental status, and political stance. The annotators also completed a brief survey on their preferences regarding online content, including the types of online content they consume (news sites,\nsocial media, forums, email, and/or messaging); their opinion on how technology impacts people\u2019s lives; whether they have seen or been personally targeted by toxic content; and their opinion on whether toxic content is a problem.\nFor these predictions, the input is formatted as:\ns1 . . . sn [SEP] d1 . . . dn [SEP] w1 . . . wn\nwhere s1 . . . sn is a template string describing the annotator\u2019s survey responses (examples in Appendix A), d1 . . . dn is a template string containing the annotator\u2019s demographic information (e.g., \u201cThe reader is a 55-64 year old white female who has a bachelor\u2019s degree, is politically independent, is a parent, and thinks religion is very important. The reader is straight and cisgender\u201d), w1 . . . wn is the text being rated, and [SEP] is a separator token. We use a template string instead of categorical variables in order to best take advantage of the model\u2019s language pretraining objective (e.g., underlying associations about the experiences of different demographic groups).\nThe model is then trained on the regression task of predicting every annotator\u2019s rating (from 0=not at all offensive to 4=very offensive) on each example that they annotated. We use MSE loss with each annotator\u2019s rating on an example treated as a separate training point. To compare with the information provided by individual annotator IDs, we additionally trained versions of the model where the input is prepended with an assigned ID corresponding to a unique set of user responses.2\n2The original responses in the dataset do not provide IDs, but we found that fewer than 9% of the unique sets of survey and demographic responses corresponded to more than one annotator, so we assigned IDs to unique sets of responses.\nAt test time, we evaluated performance of this module on the Kumar et al. (2021) dataset using the mean absolute error (MAE) of predicting individual annotators\u2019 ratings and MAE of predicting aggregate ratings for an utterance (following Gordon et al., 2022, who evaluated on the same dataset). We also evaluated the MAE of predicting the variance among individual annotator ratings, which provides a measure of uncertainty for model predictions (Davani et al., 2022): if there is high variance among the labels that different annotators are predicted to assign to a piece of text, this can signal that there is low model confidence about whether the text is harmful, or perhaps enough disagreement among the general population that the text should be rerouted to manual content moderation.\nFor each of these metrics, we hypothesized that modeling individual annotators\u2019 ratings with added survey and demographic information would reduce error relative to the baseline, which predicts annotators\u2019 aggregate ratings of a piece of text using the average rating as the ground truth (the typical task setup for hate speech detection)."
        },
        {
            "heading": "3.2 Target Group Prediction Module",
            "text": "The second module predicts the target group(s) of the input text.3 Following Sap et al. (2020), we used a GPT-2 based module that takes in as input:\nw1 . . . wn [SEP] t1 . . . tn 3In theory, it is possible to train both modules simultaneously, end-to-end. We trained them separately because at the time of writing, to our knowledge, there was no single dataset that contained sufficiently extensive information on annotators\u2019 demographic information, ratings from individual annotators, and labels for the predicted target group of the text.\nwhere w1 . . . wn is the text being rated and t1 . . . tn is a comma-separated list of target groups. During training, this module predicts each token in the sequence conditioned on the previous tokens, using GPT-2\u2019s standard language modeling objective. This model was fine-tuned on the text examples and target groups from the Social Bias Frames dataset of hate speech (Sap et al., 2020).\nThis module generates a free-text string of predicted target groups, used instead of a categorical prediction because there is a long tail of many specific target groups. After the model produces this free-text string, it standardizes the word forms of the list of target groups using a mapping from a list of word forms or variants of a demographic group (e.g., \u201cHispanic people\u201d, \u201cLatinx folks\u201d) to one standardized variant (\"Hispanic\"), taken from the list of standardized demographics in Fleisig et al. (2023). Our model then uses string matching between this list of target groups and the template strings describing annotator demographics to find the set of annotators who are part of any of these target groups.\nTo provide information about the opinion of the group potentially being harmed by a text example at test time, the model combines the outputs of the target group prediction module and the rating prediction module by (1) predicting the target group of a text example and (2) predicting the individual ratings for all annotators in the predicted target group (Figure 2, in purple). To evaluate model performance, given a text example x, we examine the predicted individual ratings for all annotators in the predicted target group of x who provided labels for that example.\nAppendices A and B contain additional details on model training for reproducibility."
        },
        {
            "heading": "4 Results",
            "text": "The key hypotheses we aimed to test were: Can we predict the ratings of individual annotators, using only their demographic information and information about their opinions on online content, without a specific mapping from each annotator to the statements they annotated? To measure this, we examine how well the individual rating prediction module predicts individual annotators\u2019 information given demographic information and/or survey responses, compared to a baseline that does not use information about the annotators (Section 4.1).\nCan we minimize unnecessary collection of in-\nformation about annotators, while still accurately predicting their ratings? To examine this, we train models that omit some survey and demographic information and measure their performance to investigate what information contributes most to the accuracy of model predictions (Section 4.4).\nMost importantly, given a statement that may target a specific demographic group, can we predict the ratings of target group members on whether the statement is harmful? We evaluate the full model\u2019s performance at predicting the ratings of target group members on a statement (predicting the target group + predicting the ratings of that group\u2019s members). To measure performance on this task, we examine the model error at predicting the ratings given by annotators who labeled examples where they themselves were members of the group targeted by the example (Section 4.3)."
        },
        {
            "heading": "4.1 Individual Rating Prediction Module",
            "text": "We evaluated the performance of our rating prediction module with different information about the annotators provided as input (combinations of demographic information, survey responses, and annotator IDs). Table 1 gives the mean absolute error on predicting annotators\u2019 individual ratings, the aggregate ratings for all annotators who labeled an example, and the variance between annotators labeling an example. We found that both demographic information and survey responses are useful predictors of annotator ratings, as adding either demographic or survey information results in a 10% improvement at predicting individual annotators\u2019 ratings. Combining both sources of information results in the most accurate predictions: the model that includes both survey and demographic information raises performance over the baseline by 22% at predicting individual annotators\u2019 ratings and 33% at predicting variance among all the annotators who rated an example.\nOur best model for individual rating prediction also improves performance at predicting aggregate ratings (by taking the average of the predicted individual annotators\u2019 ratings), raising performance by 16% over the baseline. Because this model has access to demographic and survey information about the individual annotators who labeled a response, but the baseline has no information about which subset of annotators labeled an example, this result is unsurprising. However, it further supports the hypothesis that even when predicting the aggregate\nrating of several annotators on an example, the aggregate rating of that small subgroup of annotators is sufficiently dependent on the subgroup\u2019s backgrounds and opinions that taking these into account substantially improves accuracy over predicting the rating of the \u201caverage\u201d annotator.\nBy contrast, inclusion of annotator IDs as a feature may in fact hinder our model\u2019s performance. This may be due to the fact that the language model objective does not take full advantage of categorical features and is better suited to textual features. This hypothesis is borne out by previous work on the same dataset: our model achieves lower error than previous work when using only demographic features4 and the input text (individual annotator MAE = 0.77, compared to 0.81 in Gordon et al., 2022), but not when using demographic features and annotator IDs (individual annotator MAE = 0.80, compared to 0.61 in Gordon et al.). Thus, compared to previous work, this approach appears to make better use of text features, but not of categorical features without meaningful text representations."
        },
        {
            "heading": "4.2 Target Group Prediction Module",
            "text": "To evaluate the performance of the target group prediction module on the Kumar et al. (2021) dataset, which does not list the demographic groups targeted by the examples, we manually annotated 100 examples from the dataset\u2019s test set with the groups targeted by the text. We then measured the word movers\u2019 distance between the predicted text and the freeform text that annotators provided for the\n4We exclude queer and trans status from the demographic information here for even comparisons with previous work, which did not include these features.\ngroup(s) targeted by the example, the accuracy on exactly matching the list of target groups, and accuracy on partially matching the list of target groups. The model achieves a word movers\u2019 distance of 0.370 on the dataset, exact match accuracy of 58%, and partial match accuracy of 81% (error analysis in Appendix C)."
        },
        {
            "heading": "4.3 Full Model Performance",
            "text": "To evaluate the overall performance of the model (the target group prediction and rating modules together), we had the target group module predict the target groups of the examples in the test set, then had the rating prediction module predict the ratings of members of the target group who rated that example. This is a harder task than predicting individual ratings across the entire dataset, since target group members tend to belong to groups already underrepresented in the annotator pool, such that there is less training data about their behavior.\nWe considered whether the model can accurately estimate the ratings of target group members, which is necessary to flag cases where the consensus among target group members differs from that of the majority, by defining a target offense error metric. Given a set of examples X , where each example xi has a target demographic group5 ti, we defined the target offense error as the mean absolute error across all xi \u2208 X of the members of ti who annotated example xi, where the average rating of the members of ti who annotated example xi is at least 1 on a 0-5 scale (i.e., they found the example at least somewhat offensive).\nWe also evaluated the performance of the com-\n5When there are multiple target groups, we define ti as the union of those groups.\nbined model at predicting the individual and aggregated ratings among annotators who are members of ti, as well as the variance among members of ti when two or more of them labeled xi (Table 2). We found that the model achieves an MAE of 0.59 at predicting the average rating of the target group (22% improvement over the baseline), 0.73 at predicting individual annotator ratings for the target group (18% improvement), and target offense error of 0.8 (17% improvement). The model also has an MAE of 1.22 at predicting the variance among target group members (28% improvement). This suggests that the model not only captures differences in opinion between target group members and non-members, but also variation in opinion between members of the target group. This helps to prevent group members from being modeled as a monolith and, by providing a sign of disagreement among experienced stakeholders, provides a particularly useful measure of model uncertainty in hate speech detection.\nTo understand model performance on annotators from different demographics and text targeting different demographics (see also Appendix C), we first examined the five annotator demographic groups with the highest and lowest error rates. We found that the model performs best at predicting the ratings of annotators who are conservative, nonbinary, Native American or Alaska Native, Native Hawaiian or Pacific Islanders, or had less than a high school degree (individual rating MAE under .66). The model performed worst at predicting the ratings of annotators who are liberal, politically independent, transgender, had a doctoral or professional degree, or for whom religion was somewhat important (individual rating MAE over 1.24).\nIn addition, to understand model performance on text targeting different groups, we examined the five target groups with the highest and lowest error\nrates. We found that individual rating MAE was lowest (under 0.35) at predicting ratings on text targeting people who were racist, Syrian, Brazilian, teenagers, or millennials, and individual rating MAE was highest (over 1.52) at predicting ratings on text targeting people who were well-educated, non-violent, Russian, Australian, or non-believers. To understand effects on text targeting intersectional groups or multiple groups, we examined the performance for different numbers of target groups and found that individual rating MAE changed by no more than 0.02 for text targeting up to four target groups.\nOverall, we found that the model is able to combine its prediction of the target group of a statement with predictions of the rating of individual annotators in order to estimate the ratings of target group members on relevant examples, and that it does so best when provided with both demographic information about the annotators and responses to survey questions about their online experiences. We also found that the model accurately predicts target group ratings in the key cases where target group members find that the statement is offensive."
        },
        {
            "heading": "4.4 Ablations",
            "text": "The effectiveness of information on annotator demographics and their preferences regarding online content for rating prediction suggests that predicting individual ratings is feasible even when tracking individual annotators\u2019 ratings across questions is not possible or excessively obtrusive. However, an additional concern is that asking for some types of demographic information (e.g., sexual orientation, gender identity) or asking for many finegrained demographic attributes can infringe on annotator privacy, making such data collection both harder to recruit for and ethically fraught. For these reasons, we trained models that omit some of the survey and demographic information to investigate which are most necessary to effectively predict annotator ratings.\nBy training models on single features, we found that opinions on whether toxic comments are a problem in online settings are the most useful features in isolation for predicting annotator ratings, followed by annotator race (Table 3). Using forward feature selection, we also found that a model trained on only three features\u2013annotators\u2019 race, their opinion on whether toxic posts are a problem, and the types of social media they use\u2013\napproximately matches the performance of the model trained with annotators\u2019 full demographic information at predicting annotator ratings (individual rating MAE=0.79). This result suggests that future studies could use more targeted information collection to predict individual annotator ratings, reducing privacy risks while minimizing effects on model quality. Another potential implication is that for studies with limited ability to recruit a perfectly representative population of annotators, it may be most important to prioritize recruiting a representative pool of annotators along the axes that have the greatest effect on annotator ratings, such as race.\nDemographic information is useful both to predict individual ratings, and to identify annotators who are relevant stakeholders for a particular information. For the latter purpose, there is an inherent tradeoff between certainty that an annotator is a member of a relevant demographic group, and protecting privacy by not collecting that demographic information. However, the use of non-demographic survey questions may allow practitioners to more\nfinely tune this tradeoff: survey questions that correlate with demographics (e.g. types of social media used may correlate with age) give partial information without guaranteeing that any single annotator is a member of a specific group. In other words, non-demographic survey questions could function a mechanism to tune the tradeoff between privacy protection and representation. A caveat is that such proxy variables may introduce skews or other issues into data collection, an important issue for future work. Future extensions of this research could also consider whether other specific survey questions are more useful for predicting ratings, allowing for accurate predictions while avoiding unnecessary collection of annotator information."
        },
        {
            "heading": "5 Conclusion",
            "text": "We presented a model that predicts individual annotator ratings on the offensiveness of text, which we applied to automatically flagging examples where the opinion of the predicted target group differs from that of the majority. We found that our model raises performance over the baseline by 22% at predicting individual annotators\u2019 ratings, 33% at predicting variance among annotators, and 22% at predicting target group members\u2019 aggregate ratings. Annotator survey responses and demographic features allow ratings to be modeled without the need for identifying annotator IDs to be tracked, and we found that a model given only annotators\u2019 race, opinion on online toxic content, and social media usage performs comparably to one trained with annotators\u2019 full demographic information. This suggests that questions about online preferences provide a future avenue for minimizing collection of demographic information when modeling annotator ratings.\nOur findings provide a method of modeling individual annotator ratings that can support research on the downstream use of information about annotator disagreement. Predicting the ratings of target group members allows hate speech detection systems to flag examples where the target group disagrees with the majority as potentially mislabeled or as examples that should be routed to human content moderation. Understanding the variance among annotators provides an estimate of model confidence for content moderation or filtering, facilitating a rapid method of spotting cases where a model is uncertain.\nIn addition, since opinions within a demographic\ngroup are not monolithic, accurately predicting the variance among target group members is especially useful: text that generates disagreement among people who are typically experienced stakeholders may require particularly attentive care. Potential applications of this work that have yet to be explored include improving resource allocation for annotation by identifying the labelers whose judgments it would be most useful to have for a given piece of text, and routing content moderation decisions to experts in particular areas.\nLimitations\nThis work was conducted on English text that primarily represents varieties of English used in the U.S.; the annotators who labeled the Kumar et al. (2021) dataset were from the United States and the annotators who labeled the Social Bias Frames dataset (Sap et al., 2020) were from the U.S. and Canada. It remains unclear how to generalize this work to groups that face severe harms (e.g., legal discrimination) in the U.S. and Canada or groups that may not face severe harms in the U.S. and Canada, but do face severe harms in other countries. For example, in a country where a group faces serious persecution, it may be unsafe for members of that group from that country to identify themselves and annotate data. In addition, statements that pit perspectives from multiple groups against each other (e.g., text stating that members of one group hurt another group) may be more difficult to analyze under this framework when multiple potential harms towards different groups are at play. Future work could extend this paradigm to uncovering disagreements between multiple groups and refining approaches to intersectional groups.\nEthical Considerations\nPredicting opinions of annotators from different demographic groups runs the risk of imputing an opinion to a group as a monolith instead of understanding the diversity of opinions within that group. Such modeling is no substitute for adequate representation of minoritized groups in data collection. To avoid risks of tokenism, we encourage that individual annotator modeling be used as an aid rather than a replacement for more diverse and participatory data collection. For example, the model can be used to highlight examples for which it would be useful to collect more data from a particular community or delegate decisions to community stake-\nholders rather than automated decision making. In addition, improving methods for intersectional demographic groups is an important area for future work.\nModeling individual annotators raises new privacy concerns, since sufficiently detailed collection of annotator information may render the annotators re-identifiable. We discuss our findings on some approaches to minimizing privacy concerns in Section 4.4, and as previous work has noted, investigation into techniques such as differential privacy may help to minimize these risks (Gordon et al., 2022).\nThese questions intertwine with issues of how to collect demographic information, since coarsegrained questionnaires may erase the experiences of more specific or intersectional subgroups, as well as the convergent expertise of different demographic groups that have had overlapping experiences. Further research into effective and inclusive data collection could unlock better ways of understanding these complex experiences."
        },
        {
            "heading": "Acknowledgments",
            "text": "Many thanks to Deepak Kumar for providing access to the data used in this paper and to Maarten Sap for data and discussions that inspired this research. Thank you to the Berkeley NLP group and recommender systems discussion group for their feedback on this research, and extra thanks to Nicholas Tomlin, Kevin Yang, and Ruiqi Zhong for their especially helpful feedback on earlier drafts."
        },
        {
            "heading": "B Further Model and Dataset Details",
            "text": "We divided the dataset from Kumar et al. (2021) into a train set of 97,620 examples and validation and test sets of 5,000 examples each (following Gordon et al., 2022, who also use a test set of 5,000 examples for this dataset). We used the Social Bias\nFrames dataset\u2019s existing split into a training set of 35,424 examples, validation set of 4,666 examples, and test set of 4,691 examples (Sap et al., 2020).\nThe model uses RoBERTa-base, which has 123 million parameters, and GPT2-large, which has 1.5 billion parameters, for a total of approximately 1.623 billion parameters. No hyperparameter search was conducted; the recommended hyperparameters for RoBERTa-base and GPT2-large were used (Liu et al., 2019; Radford et al., 2018).\nFor each training run, the model was trained on two NVIDIA Quadro RTX 8000 GPUs for approximately 12 hours."
        },
        {
            "heading": "C Results: Details",
            "text": "Table 4 provides the validation results for the metrics in Table 1. Tables 5 and 6 provide details on the demographic groups for which the model performs best and worst.\nOverall, each annotator labeled 20 examples on average (stddev=9). For the final level of categories in the ablations in Section 4.4, there are a mean of 31 annotators in each possible bucket (stddev=142).\nExamining patterns of error in the target model, we found that the model was most likely to miss a target group when the text was actually targeting people on the basis of race (33% of missed target groups), followed by religion (23%). In cases where the model returned a target group that was not represented in the example, the model most often predicted that the text was targeting people on the basis of race (25% of false positive predictions) or gender (18%)."
        }
    ],
    "title": "When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks",
    "year": 2023
}