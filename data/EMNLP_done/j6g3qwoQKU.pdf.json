{
    "abstractText": "Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenkai Ma"
        },
        {
            "affiliations": [],
            "name": "Xinya Du"
        }
    ],
    "id": "SP:6840b746e539719cee0fc47c2b274d324d0a3e55",
    "references": [
        {
            "authors": [
                "Maciej Besta",
                "Nils Blach",
                "Ales Kubicek",
                "Robert Gerstenberger",
                "Lukas Gianinazzi",
                "Joanna Gajda",
                "Tomasz Lehmann",
                "Michal Podstawski",
                "Hubert Niewiadomski",
                "Piotr Nyczyk"
            ],
            "title": "Graph of thoughts: Solving elaborate problems with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Conan Doyle."
            ],
            "title": "The Sign of Four",
            "venue": "Spencer Blackett.",
            "year": 1890
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Event extraction by answering (almost) natural questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 671\u2013683, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yu Fei",
                "Yifan Hou",
                "Zeming Chen",
                "Antoine Bosselut."
            ],
            "title": "Mitigating label biases for in-context learning",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14014\u201314031, Toronto, Canada.",
            "year": 2023
        },
        {
            "authors": [
                "Rusins Freivalds",
                "Marek Karpinski",
                "Carl H. Smith",
                "Rolf Wiehagen."
            ],
            "title": "Learning by the process of elimination",
            "venue": "Inf. Comput., 176:37\u201350.",
            "year": 2002
        },
        {
            "authors": [
                "Ari Holtzman",
                "Peter West",
                "Vered Shwartz",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Surface form competition: Why the highest probability answer isn\u2019t always right",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Xinxi Lyu",
                "Sewon Min",
                "Iz Beltagy",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Z-ICL: Zero-shot in-context learning with pseudo-demonstrations",
            "venue": "ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models.",
            "year": 2023
        },
        {
            "authors": [
                "Nikolay Malkin",
                "Zhen Wang",
                "Nebojsa Jojic."
            ],
            "title": "Coherence boosting: When your pretrained language model is not paying enough attention",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Noisy channel language model prompting for few-shot text classification",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Robinson",
                "David Wingate."
            ],
            "title": "Leveraging large language models for multiple choice question answering",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social IQa: Commonsense reasoning about social interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Minji Seo",
                "YeonJoon Jung",
                "Seungtaek Choi",
                "Seungwon Hwang",
                "Bei Liu."
            ],
            "title": "Debiasing event understanding for visual commonsense tasks",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 782\u2013787, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "KaShun Shum",
                "Shizhe Diao",
                "Tong Zhang."
            ],
            "title": "Automatic prompt augmentation and selection with chain-of-thought from labeled data",
            "venue": "arXiv preprint arXiv:2302.12822.",
            "year": 2023
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabili",
            "year": 2023
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou",
                "Jason Wei"
            ],
            "title": "Challenging BIG-bench tasks and whether chain-of-thought can solve them",
            "year": 2023
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "PeiFeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren."
            ],
            "title": "PINTO: Faithful language reasoning using prompt-generated rationales",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint arXiv:2305.10601.",
            "year": 2023
        },
        {
            "authors": [
                "Xi Ye",
                "Qiaochu Chen",
                "Isil Dillig",
                "Greg Durrett."
            ],
            "title": "Satisfiability-aided language models using declarative prompting",
            "venue": "arXiv preprint arXiv:2305.09656.",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Holtzman"
            ],
            "title": "2021) and Robinson and Wingate (2023) to construct task-agnostic prompts, and use them to wrap questions. We use one instance from SIQA. The question x is \"Kendall was searching for ring with their eyes",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "How often have I said to you that when you have eliminated the impossible whatever remains, however improbable, must be the truth? (Doyle, 1890)\nIn natural language processing, many reasoning tasks are multiple choice-based, in which a model chooses the best option from several options, given a question. Current LMs exhibit remarkable performance on diverse reasoning tasks, like commonsense reasoning (Wang et al., 2023a; Holtzman et al., 2021), logical reasoning (Ye et al., 2023), and arithmetic reasoning (Shum et al., 2023).\nThere are two types of in-context learning methods for multiple choice reasoning: scoring and prompting. Given the question, scoring methods score each option, and select the highest-scored\n1Code is available at https://github.com/KasMasVan/ PoE.\noption. One common score is language modeling likelihood (P (yi|x), Brown et al., 2020), and there are also many alternatives (Zhao et al., 2021; Holtzman et al., 2021; Fei et al., 2023; Min et al., 2022; Malkin et al., 2022; Robinson and Wingate, 2023; Lyu et al., 2023). Tailored for powerful LLMs (Chowdhery et al., 2022; Chen et al., 2021), prompting methods (Wei et al., 2022; Wang et al., 2023b; Kojima et al., 2022; Besta et al., 2023; Yao et al., 2023) append all options to the question, and prompt the model to generate a sequence, then extract the option from the sequence.\nOne limitation of these two types of approaches is that they both treat each option equally, i.e., they either consider each option independently, or consider all options jointly. On the contrary, when humans solve a multiple choice reasoning task, they often eliminate some wrong options, then choose from the remaining ones. This so-called process of elimination is widely used in college exams and can be stronger than usual Gold style learning (Freivalds et al., 2002). Therefore, we assume that language models can similarly benefit from this elimination process, i.e., eliminating wrong options and choosing the best option are two types of reasoning that can be disentangled into two steps.\nTo this end, we present the Process of Elimination (POE), a two-step scoring method for multiple choice reasoning, as shown in Figure 1. In the first step, POE scores each option, then eliminates some wrong options based on their scores. In the second step, it masks these wrong options, then chooses the best one from the remaining options. We conduct experiments on 8 reasoning tasks that cover diverse domains. POE achieves the best zero-shot performance on most tasks. A following analysis shows that it favors logical reasoning tasks. We also measure the effect of masks, and find our method applicable to few-shot settings and compatible with LLMs like ChatGPT (Ouyang et al., 2022).\nOur contributions are twofold: (1) We present POE, a two-step scoring method for multiple choice reasoning; (2) Through comprehensive experiments and analysis, we demonstrate the effectiveness and generalizability of POE."
        },
        {
            "heading": "2 Method",
            "text": "Problem Setting. A multiple choice reasoning task instance includes a question x, several options Y = {y1, ..., yn}, and the correct option y. There are two kinds of in-context learning approaches to this problem: scoring and prompting.\nScoring uses an LM to compute a score for each option yi, and chooses the highest-scored option:\nsi = score(x, yi), (1)\ny\u0302 = argmax i si. (2)\nA commonly-used score is language modeling likelihood (P (yi|x), Brown et al., 2020), More recent scores include average log probability (Brown et al., 2020), calibrated log probability (Holtzman et al., 2021), and channel (Min et al., 2022). As shown in Equation 1, most scoring methods consider each option in isolation, except multiple choice prompting (Robinson and Wingate, 2023).\nIn contrast, prompting methods provide all the options in the input (Wei et al., 2022; Wang et al., 2023b; Kojima et al., 2022). The model then generates raw output from the input. Finally, these methods extract the option from the raw output:\nraw output = generate(x, Y ), (3)\ny\u0302 = extract(raw output). (4)\nPOE. Our method is a two-step scoring method that considers all options but treats them differently.\nWe also implement a prompting-based variation of POE for LLM (Section 5).\nThe first step of POE is elimination, in which it eliminates some wrong options. POE starts by scoring each option. Then, unlike common scoring methods that choose the highest-scored option, it uses the scores to eliminate some options with low scores. In particular, POE computes the average score of all options, and eliminates options whose score is below average, i.e., Ywrong:\nYwrong = {yi|si < avg(s1, ..., si)}. (5)\nThe intuition behind this elimination strategy (\"Below Average\") is that the scores of wrong options deviate from others, and are thus easy to identify, which we verify in Appendix B.2. We compare other elimination strategies in Section 5.2.\nThe second step of POE is prediction, which chooses the best answer that is not in Ywrong. Specifically, POE computes binary masks mi for options:\nmi = { 0, if yi \u2208 Ywrong 1, otherwise\n(6)\nmask = [m1, ...,mn]. (7)\nFor these options, POE enforces the masks.2 In particular (shown in Figure 1), it uses a template T to first wrap the question with all options and their symbols like \"A\", then append an instruction to the question which asks the model to neglect masked options, and finally replace eliminated options with a special text sequence \"[MASK]\":\nxmask = T (x, Y,mask). (8)\nThen, POE scores each option, and chooses the highest-scored option, during which the scores of eliminated options are set to negative infinity:\nsmask,i = { score(xmask, yi), if yi /\u2208 Ywrong \u2212inf, otherwise (9)\ny\u0302 = argmax i smask,i. (10)\n2We don\u2019t remove wrong options because that requires extra work like relabeling remaining options (e.g., \"ADE\" to \"ABC\"), but brings negligible difference to performance."
        },
        {
            "heading": "3 Experiment Setup",
            "text": "Data. We consider 8 multiple choice reasoning tasks to cover diverse domains. We include three traditional reasoning tasks: ANLI (Nie et al., 2020), CommonsenseQA (CQA, Talmor et al., 2019), and Social IQa (SIQA, Sap et al., 2019). We also include five BIG-bench tasks (Srivastava et al., 2023), with the first two from BIG-Bench Hard (Suzgun et al., 2023): Logical Deduction (LD), Disambiguation_QA (DQA), Conceptual Combinations (CC), Strange Stories (SS), and Symbol Interpretation Task (SIT). For all tasks, we use their test sets if available, otherwise their development sets. To reduce cost, we sample 100 instances from each task. We present more task information and preprocessing in Appendix A.\nModel. We use FLAN-T5-XL (3B) (Chung et al., 2022) in both steps of POE, because it is an economical and performant instruction-tuned model. In Section 5, we also apply POE to LLMs like ChatGPT (Ouyang et al., 2022).3\nMethods. We consider 5 scoring baselines: language modeling (LM, baseline in Zhao et al., 2021), average language modeling (AVG, Brown et al., 2020), calibration (Holtzman et al., 2021), channel (Min et al., 2022), and multiple choice prompting (MCP, Robinson and Wingate, 2023). For POE, we use MCP in both steps. We discuss other possible scoring methods and elimination strategies for POE in Section 5.2, and present input and output samples for all methods in Appendix C.4\n3We do not use recent larger instruction-following models based on LLaMA (Touvron et al., 2023), because 1) they are not optimized for traditional NLP tasks, and 2) they evolve quickly, which makes it hard for a fair comparison.\n4We do not include prompting baselines like Chain-of-\nSettings. We evaluate POE in the zero-shot setting. We use accuracy as the metric, and average results over 5 random seeds. We consider other settings in Section 5.3 and 5.4."
        },
        {
            "heading": "4 Results",
            "text": "In Table 1, we compare POE with other baselines on 8 reasoning tasks, and present the accuracy and standard deviation. Our method achieves the best or second-best performance on all 8 tasks. We notice that multiple choice prompting (MCP) is a strong baseline that consistently outperforms other baselines. Nevertheless, POE beats MCP on 5 tasks, and is comparable on the remaining three. Since POE uses MCP in both steps, the result demonstrates the effectiveness of the former, especially elimination. In addition, POE is exceptionally performant on CC, beating the second-best method by 12% absolute. This corroborates our hypothesis on two types of reasoning for multiple choice tasks, because while MCP sometimes fails to simultaneously eliminate wrong options and choose the right option, POE disentangles the two jobs in two steps, and thus achieves better performance. We present a case study in Appendix B.2."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 When Is POE Better than MCP?",
            "text": "In Table 1, we find MCP to be the best baseline, and sometimes even beats POE. This finding leads us to wonder: when (on which tasks) can we expect POE to dramatically outperform MCP?\nThought Prompting (Wei et al., 2022), because they are tailored for very large (e.g., 175B) models.\nWe start by calculating their performance gaps (POE - MCP) on all 8 tasks. We find the gaps on LD (13.8%) and CC (12%) are much larger than those of other tasks (less than 3%). Furthermore, we find both LD and CC are logical reasoning tasks, while other tasks rely more on commonsense or social reasoning.5 We thus hypothesize that POE is better than MCP on tasks that mainly require logical reasoning, while not being consistently performant on social and commonsense reasoning. One explanation is that logical reasoning is a general skill, whereas social or commonsense reasoning relies on specific knowledge, some of which may be absent in certain language models.\nTo verify this hypothesis, we compare POE and MCP in the zero-shot setting on 8 new tasks from BIG-Bench, including 4 logical reasoning tasks: Logical Arguments (LA), Identify Math Theorems (IMT), Code Description (CLD), Reasoning about Colored Objects (RACO); and 4 control tasks that are based on commonsense or social reasoning: Counterfactual Conditionals (CAI), The Essential, the Excessive, and the Extraneous (EIE), RiddleSense (RS), Identify Odd Metaphor (IOM). We present task information in Appendix A.\nAs shown in Table 2, we find POE consistently and dramatically outperforms MCP on all 4 logical reasoning tasks (top 4 tasks). We find the largest performance gap on LA (18.8%), a task of GREstyle logical questions. Human test-takers commonly use elimination-based methods to solve such GRE questions, which further supports the motiva-\n5SIT is another logical reasoning task, but it also requires visual reasoning, which is not ideal for language models.\ntion of POE. We also find it underperforms MCP on 4 control tasks (bottom 4 tasks), which means our method is not consistent on commonsense or social reasoning tasks. These findings support our hypothesis that compared to MCP, POE is most performant on logical reasoning tasks."
        },
        {
            "heading": "5.2 What Makes Good Masks?",
            "text": "As shown in Section 2, scoring methods and elimination strategies jointly determine which options to eliminate (Ywrong), and create masks accordingly. In this section, we analyze the effect of using different configurations of these two factors.6 We define masking accuracy (Accmask) as the ratio of instances that have their correct option kept after elimination. Accmask represents the upper bound of POE, which we aim to maximize to reduce error propagation (Du and Cardie, 2020). We measure two quantities: Accmask and final accuracy (Acc). We consider four scoring methods (Calibration, Channel, LM, MCP) and two elimination strategies (\"Below Average\" and \"Lowest\", the latter of which means eliminating the option with the lowest score). We average zero-shot results over all 8 tasks in Table 1.\nAs shown in Figure 2, the best scoring method is MCP, and the two elimination strategies are comparable to each other. We find that higher Accmask leads to higher Acc. The best configuration of scoring method and elimination strategy (MCP, Lowest) leads to 91.9% Accmask.7 This means a moderatesized LM (FLAN-T5-XL) can eliminate wrong op-\n6We also tried mask tokens other than \"[MASK]\", but did not find one consistently better than others. So we don\u2019t tune this token to avoid unnecessary costs.\n7We use (MCP, Below Average) in the main experiment, because this configuration makes POE more separable on tasks that it performs well, i.e., the gap between POE and the second-best baseline is larger.\ntions while keeping the correct ones most of the time. The corresponding 68.7% Acc means a large performance gap, which suggests a more powerful LM like FLAN-T5-XXL for prediction. We also find Ywrong and masks to make POE more interpretable, because they provide intermediate reasoning outputs. Enforcing masks in prediction also makes our method faithful and factual."
        },
        {
            "heading": "5.3 Does POE Work with LLMs?",
            "text": "To measure whether POE is compatible with LLMs, we implement a prompting-based variation of POE and apply it to ChatGPT (gpt-3.5-turbo-0613). We then measure its zero-shot performance on 8 original tasks in Table 1 and 4 logical reasoning tasks in Table 2. For POE, we only use ChatGPT in the second step (prediction), as the base LM suffices for masking (Section 5.2). Concretely, we prompt ChatGPT to complete xmask, and extract the last occurrence of any option symbol as the prediction. We compare POE to MCP, which is similarly modified to work with ChatGPT.\nWe present the result in Table 3. The result from ChatGPT is consistent with those from FLAN-T5XL (Table 1): POE beats MCP on 5 out of the 8 original tasks, and performs well on 4 logical reasoning tasks. These findings suggest our method also works with LLMs. Nevertheless, we find ChatGPT underperforms FLAN-T5-XL on some tasks, which requires further experiments and is beyond\nthe scope of this work."
        },
        {
            "heading": "5.4 Does POE Work in Few-Shot Settings?",
            "text": "To measure POE in the few-shot settings, we compare its zero-shot and three-short performance with MCP on ANLI, CC, and LD. We build three-shot demonstrations by randomly sampling from the training sets of ANLI and test sets of CC and LD.8\nWe present the result in Figure 3. For ANLI, POE underperforms MCP in both settings, but their performance gap drops from 5.3% (zero-shot) to 1.5% (three-shot). For LD and CC, POE outperforms MCP in both settings, but their performance gap similarly diminishes in the three-shot setting. Comparing POE with MCP, We find three-shot POE to be less performant than its zero-shot counterpart, and possible reasons include: 1) we did not optimize prompts or demonstrations for threeshot POE; 2) the instructions we use in zero-shot POE are already powerful, and three-shot demonstrations may introduce some noise. Still, our findings suggest POE is applicable to few-shot settings, which we will continue to explore in future work."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present POE, a two-step scoring method for multiple choice reasoning. POE eliminates some wrong options in the first step, and chooses from the remaining options in the second step. POE performs well on reasoning (especially logical reasoning) tasks in the zero-shot setting, and also works with LLMs or in the few-shot setting. In the future, we will improve the generalizability of our method, and use fine-tuning to better enforce the masks.\n8For CC and LD, we run experiments on the remaining data after sampling.\nLimitations\nThe first limitation of our work is the enforcement of the masks. Although we replace eliminated options with \"[MASK]\", and also replace their scores afterward, the model still considers these options in prediction. It would be better if the model does not consider these options at all. A second limitation is that we do not fully explore the components of POE: We fix the prompts, which may not be optimal. A third limitation of our work is the scope of the LLM experiment: We only test in the zeroshot setting. It would be better if we also run fewshot LLM experiments, and incorporate complex prompting methods like Chain-of-Thought Prompting (Wei et al., 2022). A fourth limitation is that this work considers only one modality, i.e., language. It would be interesting to extend POE to tasks involving other modalities, like vision (Seo et al., 2022).\nEthics Statement\nThe tasks and models in this work are publicly available. They could contain bias, and should be used with discretion."
        },
        {
            "heading": "A Task Information and Pre-Processing",
            "text": "All tasks we use are expressed in English. ANLI is originally a classification task, and we convert it to multiple choice by mapping labels to options, i.e., \"0\" to \"entailment\", \"1\" to \"neutral\", and \"2\" to \"contradiction\". For BIG-bench tasks, we remove instances whose number of options deviate from others. For ANLI and CC, we aggregate instances from all subtasks; For LD, we use the subtask with\n5 options; For SS, we use the multiple choice subtask, and treat options with scores of 0.5 as wrong. We provide other details in Table 4. The first three tasks can be accessed on Hugging Face.9 The last thirteen tasks can be accessed on BIG-bench.10"
        },
        {
            "heading": "B Additional Analysis",
            "text": "B.1 Different Number of Options\nSince our method eliminates wrong options in the first step, we assume the number of options affects performance. Consequently, we run experiments on three subtasks of logical deduction (LD (3), LD (5), LD (7)), which have 3, 5, and 7 options respectively. The other experiment settings are consistent with the main experiment (Section 3).\nWe present the result in Table 5. We find most methods (including POE) perform worse as the number of options increases. This conforms to intuition, because more options require more reasoning steps, and the questions get harder. Nevertheless, POE is the best on all subtasks. This means although POE does not counterintuitively perform better on harder questions, it works well on questions of different difficulties, and thus applies to a variety of multiple choice reasoning tasks.\n9https://huggingface.co/ 10https://github.com/google/BIG-bench\nB.2 Case Study\nIn Figure 4, we present a case study to show why POE works. We compare POE with MCP on one instance from CC. The question invents the word \"wajey\" with a definition, and forms a surprising and uncommon conceptual combination between \"wajey\" and \"grape\". The correct option is A.\nWe find MCP unable to solve this task, and chooses C. Taking a closer look at MCP scores (lower is better) for each option, we find that MCP assigns high scores to options B and D, two seemingly wrong options. This verifies our hypothesis that some wrong options\u2019 scores deviate from others, and can thus be eliminated. Nevertheless, MCP is distracted by option C, and we assume this\nis caused by the correlation between \"grape\" and \"Muscat\", the latter of which is a kind of grape. This instance shows although MCP can eliminate some wrong options, it struggles to simultaneously choose the right option.\nFor POE, since it uses MCP in the first step, it masks options B and D. We examine the final POE scores for each option, and find that it correctly chooses option A. In addition, we find the POE score for option A is lower than its MCP score, and conversely for option C. This means our method is not distracted by option C. This case study verifies our assumption that eliminating wrong options and choosing the best option are two types of reasoning that can be disentangled into two steps, and that such disentanglement is beneficial.\nC Input and Output Samples for All Methods\nWe do not optimize prompts, because our goal is to evaluate POE with a fixed prompt format. Therefore, we follow Holtzman et al. (2021) and Robinson and Wingate (2023) to construct task-agnostic prompts, and use them to wrap questions. We use one instance from SIQA. The question x is \"Kendall was searching for ring with their eyes closed. They hit something. Why did Kendall do this?\". The options Y are \"kendall who has searching his ring\" (y1), \"kendall who has wanted to close their eyes\" (y2), and \"find the rings\" (y3).\nWe present input and output samples for all methods for this instance in Table 6. Calibration computes two scores (logP (yi|x)) and (logP (yi|text)), so we present samples for each of them. POE uses different prompts for each step, so we present them separately."
        }
    ],
    "title": "POE: Process of Elimination for Multiple Choice Reasoning",
    "year": 2023
}