{
    "abstractText": "Large Language Models (LLMs) have shown impressive abilities in various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory-ofThought, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaonan Li"
        },
        {
            "affiliations": [],
            "name": "Xipeng Qiu"
        }
    ],
    "id": "SP:01e29881bfe87f01f7552a683eb901f8498b6511",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Chunting Zhou",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad."
            ],
            "title": "Incontext examples selection for machine translation",
            "venue": "CoRR, abs/2212.02437.",
            "year": 2022
        },
        {
            "authors": [
                "aoyu Feng",
                "Vlad Fienber",
                "Markus Freitag",
                "Xavier Garcia",
                "Sebastian Gehrmann",
                "Lucas Gonzalez"
            ],
            "title": "Palm 2 technical report",
            "year": 2023
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "FAccT",
            "year": 2021
        },
        {
            "authors": [
                "Zekai Chen",
                "Mariann Micsinai Balan",
                "Kevin Brown."
            ],
            "title": "Language models are few-shot learners for prognostic prediction",
            "venue": "CoRR, abs/2302.12692.",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "CoRR, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei."
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "CoRR, abs/2210.11416.",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Rajarshi Das",
                "Manzil Zaheer",
                "Dung Thai",
                "Ameya Godbole",
                "Ethan Perez",
                "Jay Yoon Lee",
                "Lizhen Tan",
                "Lazaros Polymenakos",
                "Andrew McCallum."
            ],
            "title": "Casebased reasoning for natural language queries over knowledge bases",
            "venue": "Proceedings of the 2021 Confer-",
            "year": 2021
        },
        {
            "authors": [
                "A.J. Dijksterhuis",
                "Loran F Nordgren."
            ],
            "title": "A theory of unconscious thought",
            "venue": "Perspectives on Psychological Science, 1:109 \u2013 95.",
            "year": 2006
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui"
            ],
            "title": "A survey for in-context learning",
            "year": 2022
        },
        {
            "authors": [
                "Dheeru Dua",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Gabriel Stanovsky",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "venue": "Proceedings of the 2019 Conference of the North American",
            "year": 2019
        },
        {
            "authors": [
                "J. Dunlosky",
                "J. Metcalfe."
            ],
            "title": "Metacognition",
            "venue": "SAGE Publications.",
            "year": 2008
        },
        {
            "authors": [
                "Stan Franklin",
                "Bernard Baars",
                "Uma Ramamurthy",
                "Matthew Ventura."
            ],
            "title": "The role of consciousness in memory",
            "venue": "Brains, Minds Media, 1.",
            "year": 2005
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot."
            ],
            "title": "Specializing smaller language models towards multi-step reasoning",
            "venue": "CoRR, abs/2301.12726.",
            "year": 2023
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot."
            ],
            "title": "Specializing smaller language models towards multi-step reasoning",
            "venue": "CoRR, abs/2301.12726.",
            "year": 2023
        },
        {
            "authors": [
                "Jiahui Gao",
                "Renjie Pi",
                "Yong Lin",
                "Hang Xu",
                "Jiacheng Ye",
                "Zhiyong Wu",
                "Xiaodan Liang",
                "Zhenguo Li",
                "Lingpeng Kong."
            ],
            "title": "Zerogen+: Self-guided highquality data generation in efficient zero-shot learning",
            "venue": "CoRR, abs/2205.12679.",
            "year": 2022
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "CoRR, abs/2212.10071.",
            "year": 2022
        },
        {
            "authors": [
                "Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre."
            ],
            "title": "Training compute-optimal large language models",
            "venue": "CoRR, abs/2203.15556.",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Thomas Scialom",
                "Omer Levy",
                "Timo Schick."
            ],
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "venue": "CoRR, abs/2212.09689.",
            "year": 2022
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A. Smith",
                "Mari Ostendorf."
            ],
            "title": "Incontext learning for few-shot dialogue state tracking",
            "venue": "CoRR, abs/2203.08568.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Shixiang Shane Gu",
                "Le Hou",
                "Yuexin Wu",
                "Xuezhi Wang",
                "Hongkun Yu",
                "Jiawei Han."
            ],
            "title": "Large language models can self-improve",
            "venue": "CoRR, abs/2210.11610.",
            "year": 2022
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "CoRR, abs/2212.10403.",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mann",
                "Sam McCandlish",
                "Chris Olah",
                "Jared Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "CoRR, abs/2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "CoRR, abs/2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew K. Lampinen",
                "Ishita Dasgupta",
                "Stephanie C.Y. Chan",
                "Kory W. Mathewson",
                "Mh Tessler",
                "Antonia Creswell",
                "James L. McClelland",
                "Jane Wang",
                "Felix Hill"
            ],
            "title": "Can language models learn from explanations in context",
            "venue": "In Findings of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Itay Levy",
                "Ben Bogin",
                "Jonathan Berant."
            ],
            "title": "Diverse demonstrations improve in-context compositional generalization",
            "venue": "CoRR, abs/2212.06800.",
            "year": 2022
        },
        {
            "authors": [
                "Junlong Li",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Self-prompting large language models for opendomain QA",
            "venue": "CoRR, abs/2212.08635.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaonan Li",
                "Kai Lv",
                "Hang Yan",
                "Tianyang Lin",
                "Wei Zhu",
                "Yuan Ni",
                "Guotong Xie",
                "Xiaoling Wang",
                "Xipeng Qiu."
            ],
            "title": "Unified demonstration retriever for incontext learning",
            "venue": "CoRR, abs/2305.04320.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaonan Li",
                "Xipeng Qiu."
            ],
            "title": "Finding supporting examples for in-context learning",
            "venue": "CoRR, abs/2302.13539.",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Lin",
                "Shubhendu Trivedi",
                "Jimeng Sun."
            ],
            "title": "Generating with confidence: Uncertainty quantification for black-box large language models",
            "venue": "CoRR, abs/2305.19187.",
            "year": 2023
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration",
            "year": 2022
        },
        {
            "authors": [
                "Weijie Liu",
                "Peng Zhou",
                "Zhiruo Wang",
                "Zhe Zhao",
                "Haotang Deng",
                "Qi Ju."
            ],
            "title": "Fastbert: a selfdistilling BERT with adaptive inference time",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Jieyi Long."
            ],
            "title": "Large language model guided tree-ofthought",
            "venue": "CoRR, abs/2305.08291.",
            "year": 2023
        },
        {
            "authors": [
                "Xinxi Lyu",
                "Sewon Min",
                "Iz Beltagy",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Z-ICL: zero-shot incontext learning with pseudo-demonstrations",
            "venue": "CoRR, abs/2212.09865.",
            "year": 2022
        },
        {
            "authors": [
                "Lucie Charlotte Magister",
                "Jonathan Mallinson",
                "Jakub Ad\u00e1mek",
                "Eric Malmi",
                "Aliaksei Severyn."
            ],
            "title": "Teaching small language models to reason",
            "venue": "CoRR, abs/2212.08410.",
            "year": 2022
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark J.F. Gales."
            ],
            "title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "venue": "CoRR, abs/2303.08896.",
            "year": 2023
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A. Smith",
                "Mike Lewis."
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "CoRR, abs/2210.03350.",
            "year": 2022
        },
        {
            "authors": [
                "Xipeng Qiu",
                "Tianxiang Sun",
                "Yige Xu",
                "Yunfan Shao",
                "Ning Dai",
                "Xuanjing Huang."
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "CoRR, abs/2003.08271.",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Stephen E. Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of",
            "year": 2022
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault F\u00e9vry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2022
        },
        {
            "authors": [
                "Daniel L. Schacter",
                "Donna Rose Addis."
            ],
            "title": "The cognitive neuroscience of constructive memory: Remembering the past and imagining the future",
            "venue": "Jon Driver, Patrick Haggard, and Tim Shallice, editors, Mental Processes in the Human Brain. Oxford",
            "year": 2007
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "CoRR, abs/2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Zhihong Shao",
                "Yeyun Gong",
                "Yelong Shen",
                "Minlie Huang",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Synthetic prompting: Generating chain-of-thought demonstrations for large language models",
            "venue": "CoRR, abs/2302.00618.",
            "year": 2023
        },
        {
            "authors": [
                "Peng Shi",
                "Rui Zhang",
                "He Bai",
                "Jimmy Lin."
            ],
            "title": "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
            "venue": "CoRR, abs/2210.13693.",
            "year": 2022
        },
        {
            "authors": [
                "Peng Shi",
                "Rui Zhang",
                "He Bai",
                "Jimmy Lin."
            ],
            "title": "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
            "venue": "CoRR, abs/2210.13693.",
            "year": 2022
        },
        {
            "authors": [
                "Hongjin Su",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu."
            ],
            "title": "Selective annotation makes language models better few-shot learners",
            "venue": "CoRR, abs/2209.01975.",
            "year": 2022
        },
        {
            "authors": [
                "Hongjin Su",
                "Weijia Shi",
                "Jungo Kasai",
                "Yizhong Wang",
                "Yushi Hu",
                "Mari Ostendorf",
                "Wen-tau Yih",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Tao Yu."
            ],
            "title": "One embedder, any task: Instruction-finetuned text embeddings",
            "venue": "CoRR, abs/2212.09741.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q. Tran",
                "Xavier Garcia",
                "Dara Bahri",
                "Tal Schuster",
                "Huaixiu Steven Zheng",
                "Neil Houlsby",
                "Donald Metzler."
            ],
            "title": "Unifying language learning paradigms",
            "venue": "CoRR, abs/2205.05131.",
            "year": 2022
        },
        {
            "authors": [
                "Endel Tulving."
            ],
            "title": "Episodic memory: From mind to brain",
            "venue": "Annual review of psychology, 53:1\u201325.",
            "year": 2002
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Shuailong Liang",
                "Yue Zhang",
                "Xiaonan Li",
                "Tian Gao."
            ],
            "title": "Does it make sense? and why? a pilot study for sense making and explanation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim."
            ],
            "title": "Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models",
            "venue": "CoRR, abs/2305.04091.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou."
            ],
            "title": "Rationaleaugmented ensembles in language models",
            "venue": "CoRR, abs/2207.00747.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou."
            ],
            "title": "Selfconsistency improves chain of thought reasoning in language models",
            "venue": "CoRR, abs/2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "CoRR, abs/2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022c. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Weng",
                "Minjun Zhu",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Large language models are reasoners with self-verification",
            "venue": "CoRR, abs/2212.09561.",
            "year": 2022
        },
        {
            "authors": [
                "Ji Xin",
                "Raphael Tang",
                "Jaejun Lee",
                "Yaoliang Yu",
                "Jimmy Lin."
            ],
            "title": "Deebert: Dynamic early exiting for accelerating BERT inference",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
            "year": 2020
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Jiahui Gao",
                "Qintong Li",
                "Hang Xu",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "Zerogen: Efficient zero-shot learning via dataset generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Jiahui Gao",
                "Zhiyong Wu",
                "Jiangtao Feng",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "Progen: Progressive zero-shot dataset generation via in-context feedback",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Zhiyong Wu",
                "Jiangtao Feng",
                "Tao Yu",
                "Lingpeng Kong."
            ],
            "title": "Compositional exemplars for in-context learning",
            "venue": "CoRR, abs/2302.05698.",
            "year": 2023
        },
        {
            "authors": [
                "Kang Min Yoo",
                "Junyeob Kim",
                "Hyuhng Joon Kim",
                "Hyunsoo Cho",
                "Hwiyeol Jo",
                "Sang-Woo Lee",
                "Sang-goo Lee",
                "Taeuk Kim"
            ],
            "title": "Ground-truth labels matter",
            "year": 2022
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Noah D. Goodman."
            ],
            "title": "Star: Bootstrapping reasoning with reasoning",
            "venue": "CoRR, abs/2203.14465.",
            "year": 2022
        },
        {
            "authors": [
                "har",
                "Tianlu Wang",
                "Luke Zettlemoyer"
            ],
            "title": "2022a. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "CoRR, abs/2210.03493.",
            "year": 2022
        },
        {
            "authors": [
                "Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen."
            ],
            "title": "A survey of large language models",
            "venue": "CoRR, abs/2303.18223.",
            "year": 2023
        },
        {
            "authors": [
                "Chuanyang Zheng",
                "Zhengying Liu",
                "Enze Xie",
                "Zhenguo Li",
                "Yu Li."
            ],
            "title": "Progressive-hint prompting improves reasoning in large language models",
            "venue": "CoRR, abs/2304.09797.",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "CoRR, abs/2205.10625.",
            "year": 2022
        },
        {
            "authors": [
                "Chen et al",
                "Ouyang"
            ],
            "title": "2022) to evaluate MoT\u2019s generality across different LLMs. We show the results in Table 5. We observe that MoT consistently outperforms baselines on these two LLMs, which shows the effectiveness",
            "year": 2022
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "2022b) and Wang et al. (2022a), respectively. For the left datasets that have no publicly released manual CoT demonstrations, we randomly select questions from the training set and use ChatGPT",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Few-shot CoT prompts for AQuA",
            "year": 2022
        },
        {
            "authors": [
                "EEC in"
            ],
            "title": "1958 - 1951 = 7",
            "venue": "So the answer is 7.",
            "year": 1958
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Few-shot CoT prompts for NLI tasks, three subsets of ANLI",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "wilting in a vase (D) plants sprouting, blooming and wilting A: The sun can affect the growing of living things, like plants. The answer is (D). Table 11: Few-shot CoT examples for OBQA",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Few-shot CoT examples for BoolQ",
            "year": 2022
        },
        {
            "authors": [
                "Q: On June"
            ],
            "title": "2017, the following claim was made: David Lloyd George lost every bid to become prime minister",
            "year": 2017
        },
        {
            "authors": [
                "A: David Lloyd George"
            ],
            "title": "Prime Minister of the United Kingdom from 1916 to 1922. He also served as the Chancellor of the Exchequer and the Minister of Munitions before becoming Prime Minister",
            "year": 1922
        },
        {
            "authors": [
                "A: George Harrison"
            ],
            "title": "married his first wife, model",
            "venue": "Pattie Boyd, on January",
            "year": 1966
        },
        {
            "authors": [
                "Q: On June"
            ],
            "title": "2017, the following claim was made: Woodrow Wilson did not live during World War I",
            "year": 2017
        },
        {
            "authors": [
                "A: Woodrow"
            ],
            "title": "Wilson was the President of the United States during World War I, serving from 1913 to 1921",
            "year": 1921
        },
        {
            "authors": [
                "David Lloyd"
            ],
            "title": "George lost every bid to become prime minister. Was this claim true or false? A: David Lloyd George served as the Prime Minister of the United Kingdom from 1916 to 1922. He also served as the Chancellor of the Exchequer and the Minister of Munitions before becoming Prime Minister",
            "venue": "Q: On June 2017,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have demonstrated surprising abilities on a wide range of Natural Language Processing (NLP) tasks (Chen et al., 2023; Zhang et al., 2022a; Chowdhery et al., 2022; Tay et al., 2022; OpenAI, 2023; Hoffmann et al., 2022; Touvron et al., 2023; Mialon et al., 2023; Zhao et al., 2023; Qiu et al., 2020). Notably, new abilities emerge in LLMs as they are scaled to hundreds of billions of parameters, like in-context few-shot learning (Chen et al., 2023; Dong et al., 2022), simple digit operation and factual knowledge query (Wei et al., 2022b). Especially, the general reasoning ability of the LLM has impressed the NLP community and relevant techniques have achieved a series of new state-of-the-art (Wei et al., 2022c; Kojima et al., 2022; Lampinen et al., 2022;\nWang et al., 2022b; Huang and Chang, 2022). Specifically, Wei et al. (2022c) and Kojima et al. (2022) propose few-shot CoT and zero-shot CoT, which elicit LLM\u2019s reasoning by few-shot demonstrations and simple yet effective \u201cLet\u2019s think step by step\u201d prompting, respectively. Based on them, Wang et al. (2022b); Press et al. (2022); Zhou et al. (2022); Wang et al. (2023); Weng et al. (2022) further propose self-consistency, self-ask, least-tomost, plan-and-solve, etc., to achieve more complicated reasoning in various specialized scenarios.\nDespite the impressive abilities of the LLM pretrained on the large corpus, fundamentally improving the LLM\u2019s performance beyond few-shot / zero-shot baselines highly depends on either highquality annotated datasets or costly fine-tuning of LLMs. In general, these methods can be divided into three categories: 1. Annotated Datasets + Fine-tuning: Wei et al. (2022a) and Sanh et al. (2022) propose FLAN and T0 respectively to enhance the LLM\u2019s zero-shot ability by tens of curated NLP benchmark datasets. Based on FLAN, Chung et al. (2022) scale up its training in terms of model size and the number of tasks, and demonstrate that the added CoT examples with rationales improve the LLM\u2019s reasoning abilities. InstructGPT (Ouyang et al., 2022) improves the GPT3\u2019s instruction-following ability by fine-tuning on many diverse crowd-sourced instruction-answer\npairs. 2. Retrieving Annotated Data: Liu et al. (2022), Su et al. (2022a) and Agrawal et al. (2022) use SentenceBERT (Reimers and Gurevych, 2019) or BM25 (Robertson and Zaragoza, 2009) to retrieve relevant examples from the annotated dataset, to improve LLM\u2019s in-context learning. Rubin et al. (2022) and Shi et al. (2022a) leverage annotated datasets to train retrievers by the LM-feedback to retrieve helpful demonstrations for the test example. 3. Fine-tuning with LLM-generated data: Zelikman et al. (2022) let the LM generate rationales for annotated dataset and train itself to enhance the reasoning ability. Magister et al. (2022),Ho et al. (2022) and Fu et al. (2023a) use the reasoning paths generated by large LM to improve the small LM\u2019s reasoning capability. More recently, Huang et al. (2022) demonstrate the effectiveness of self-training on PaLM (Chowdhery et al., 2022).\nAs annotating high-quality data, especially rationales in CoT data, is expensive, fine-tuning LLM requires extremely powerful computational resources and results in high computational costs. Methods above that rely on fine-tuning also face two challenges: 1. Since the most powerful LLMs, e.g., GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022; Anil et al., 2023), are only publicly available through the inference API, it is not feasible for most of the research community to improve them by these methods. 2. Fine-tuning LLM for specific capability enhancement is costly and not environmentally friendly. As the LLM has massive parameters, fine-tuning them will lead to substantial costs of model storage and deployment. Further studies show that fine-tuning the LLM with specialized data may significantly decrease its general abilities (Fu et al., 2023b).\nWhile considerable efforts were dedicated to collecting high-quality annotated datasets and finetuning the LLM, which is costly and may decrease its general ability, on the contrary, humans can improve their own reasoning abilities through the metacognition process (Dunlosky and Metcalfe, 2008) and the memory mechanisms (Tulving, 2002), and preserve their general abilities. For example, memory helps humans improve themselves in terms of decision-making, reasoning, judgment, etc (Tulving, 2002). Inspired by this, we propose MoT, shown in Figure 1, a pre-think-then-recall framework to let the LLM self-improve through Memory-of-Thoughts, without supervised data and parameter updates. In the pre-thinking stage, the LLM thinks on the unlabeled dataset and saves the\nthoughts as external memory. In the test stage, the LLM recalls relevant memory to help reason and answer the given test question. Since we focus on the overall framework and aim to demonstrate its generality and extensibility, we use simple components to instantiate these two stages. Specifically, we use the simple Few-Shot-CoT (Wei et al., 2022c) with multiple-path decoding strategy (Wang et al., 2022b) in the pre-thinking stage and propose answer-entropy to filter out uncertain thoughts. For memory recall, we propose LLM-retrieval, which lets the LLM itself retrieve relevant memory to help answer the test question. Compared with typical semantic retrievers like SBERT (Reimers and Gurevych, 2019), LLM-retrieval can better capture the deep connection of complicated logic and reasoning than semantic embeddings.\nWe summarize our contribution as follows:\n\u2022 To the best of our knowledge, the proposed framework is the first to let LLM improve its own reasoning abilities based on the memory mechanism, without parameter updates and annotated datasets.\n\u2022 We conduct comprehensive experiments on extensive datasets and the results show that MoT can help ChatGPT improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning and natural language inference without parameter updates and annotated datasets. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.\n\u2022 We release the code and generated CoT reasoning paths to facilitate future research1. In this paper, we instantiate the proposed framework with simple components and demonstrate its effectiveness. We hope that MoT can inspire researchers of the important design choices about making the LLM self-improve with memory mechanisms and pave the way for further improvements."
        },
        {
            "heading": "2 Background: Chain of Thought",
            "text": "The large language model has shown impressive reasoning abilities on various tasks. Chain-ofThoughts (CoT) prompting (Wei et al., 2022c; Kojima et al., 2022) is the most prevailing way to\n1https://github.com/LeeSureman/MoT\nlet the LLM reason, i.e., generate a series of intermediate reasoning steps that lead to the final answer. As shown in Figure 2, Few-Shot-CoT (Wei et al., 2022c; Lampinen et al., 2022) provides a few demonstrations with rationales, i.e., question/rationale/answer pairs, and prompts the LLM to generate the rationale that leads to the final answer. Zero-Shot-CoT (Kojima et al., 2022) adds the prompt, \u201cLet\u2019s think step by step\u201d, after the test question and elicits the LLM\u2019s reasoning. Specifically, the Few-Shot-CoT gets the answer as:\ns = LLM(d1, d2, \u00b7 \u00b7 \u00b7 , qtest) (1) a = Parse-Answer(s), (2)\nwhere di = [xi, ri, ai] is the i th demonstration and consists of the input, rationale and answer. FewShot-CoT first decodes s from the LLM given the few-shot CoT demonstrations, and parses s to get the final answer. Since the demonstration is typically in the format: \u201c[input] [rationale] The answer is [answer]\u201d, the answer can be easily parsed from s by the trigger \u201cThe answer is\u201d (Wei et al., 2022c). Similarly, Zero-Shot-CoT uses answer triggers, e.g., \u201cTherefore, the answer is\u201d, to extract the final answer from the zero-shot reasoning path generated by LLM (Kojima et al., 2022)."
        },
        {
            "heading": "3 Method",
            "text": "We show the overview of our framework in Figure 3. In this paper, we mainly focus on making LLM self-improve in the typical few-shot CoT scenario, where we are given a frozen large language model and an unlabeled dataset with a few CoT demonstrations (Wei et al., 2022c; Huang et al., 2022). We further demonstrate MoT\u2019s effectiveness in zero-shot scenarios in section 4.3. Our framework is divided into two stages: 1. Pre-Think Before the test stage, the LLM thinks over the unlabeled dataset and keeps the high-confidence reasoning paths as memory. 2. Recall In the test stage,\ngiven a test question, we propose LLM-retrieval to let the LLM retrieve relevant memory to help itself reason and answer it. Our method does not depend on high-quality labeled datasets and costly finetuning of LLM, and it is feasible when the LLM is frozen or only available through the inference API. Since we let the LLM think over the unlabeled dataset, save the self-generated thoughts as external memory and retrieve relevant memory for itself to help reasoning, we consider our method as making the LLM self-improve with Memory-of-Thought. We introduce these two stages below."
        },
        {
            "heading": "3.1 Pre-Thinking",
            "text": ""
        },
        {
            "heading": "3.1.1 Let LLM Think before Test Stage",
            "text": "In this stage, we let the LLM think over the unlabeled dataset and save the resultant question/rationale/answer pairs as external memory. Since we focus on the overall framework and aim to demonstrate its generality and extensibility, we instantiate the \u201cthinking\u201d mechanism here as the simple Few-Shot-CoT (Wei et al., 2022c) with multiple-path decoding strategy (Wang et al., 2022b) in this paper. Specifically, for each example x from the unlabeled dataset X , we let the LLM sample n reasoning paths and answers with temperature T > 0, denoted as [r1, r2 \u00b7 \u00b7 \u00b7 , rn] and [a1, a2 \u00b7 \u00b7 \u00b7 , an]. Then we use majority-voting to select the most consistent answer, a\u0303 = argmaxai \u2211n j=1 1(ai = aj), and keep the reasoning path, which leads to a\u0303, as memory. Since we only consider the thought that leads to the most consistent answer, the retained thoughts can be more accurate (Wang et al., 2022b) and better help the test stage. For simplicity and to save memory size, we randomly select one reasoning path of the final answer for each unlabeled example and see saving multiple thoughts for one question as future work."
        },
        {
            "heading": "3.1.2 High-Confidence Thought Filtering",
            "text": "Since the most consistent answer does not necessarily lead to the correct answer and incorrect demonstrations can cause inferior performance (Yoo et al., 2022; Lyu et al., 2022), we further propose to filter the thoughts by uncertainty. Inspired by Liu et al. (2020); Xin et al. (2020), we propose the answerentropy u(\u00b7) to filter out high-uncertainty thoughts:\nA\u2217 = unique({ai}ni=1) (3) p(a\u2217i ) = \u2211n\nj=1 1(a\u2217i = aj)/n (4)\nu(a\u2217i ) = \u2212 \u2211|A\u2217|\ni=1 p(a\u2217i ) log p(a \u2217 i ) (5)\nwhere A\u2217 = {a\u22171, a\u22172 \u00b7 \u00b7 \u00b7 } is the set of answers. u(\u00b7) indicates the answer uncertainty, and the higher u(\u00b7) is, the more uncertain the LLM is. We filter out thoughts whose uncertainty is higher than \u03c4 and \u03c4 is a pre-defined threshold. In the exploratory experiment (Figure 4), we find the thought with lower uncertainty is more likely to be correct: the stricter the filtering is, the more accurate the remaining thoughts are. Hence the answerentropy can filter out noisy thoughts and lead to more accurate thoughts for recalling. Compared with the filtering in Huang et al. (2022), which uses the number of consistent paths (max probability, abbreviated as Max-P) as metric, answer-entropy leads to slightly higher accuracy, under the same filter-out ratio.\nAfter filtering, we obtain the pool of memoryof-thoughts, M = {mi}|M |i=1, where mi is the concatenation of corresponding input, reasoning path and answer (see Figure 3). M consists of the highquality thoughts of LLM on various questions and\nthus contains crucial and valuable information for the LLM to answer the test question. For the coherence in the subsequent content, we will refer to mi as \u201cmemory\u201d or \u201cthought\u201d."
        },
        {
            "heading": "3.2 Recalling",
            "text": "In the test stage, the relevant memory is retrieved from the memory pool M , to help the LLM answer the given test question, qtest. Although semantic embedders, e.g., SBERT (Reimers and Gurevych, 2019) are capable of retrieving semantically relevant examples for ICL (Liu et al., 2022), for reasoning tasks, it is challenging for them to fully capture the deep logical connections between qtest and helpful memory, as a single vector can not directly reflect the intricate logic and reasoning path. Since the LLM, e.g., ChatGPT, has shown impressively powerful and general natural language understanding capability and a certain level of selfawareness (Kadavath et al., 2022), we propose LLM-retrieval to let the LLM retrieve helpful memory for itself.\nAs the LLM has a limitation of the max length, it is infeasible to let the LLM directly select among the entire memory pool. Inspired by human\u2019s memory recall process, where we usually first unconsciously filter the relevant memories and then consciously evaluate them (Schacter and Addis, 2007; Franklin et al., 2005), we divide LLM-retrieval into two stages: 1. filter out semantically irrelevant memory and get memory candidates; 2. let the LLM choose from memory candidates.\nSince the diversity of demonstrations has been shown important for LLMs(Ye et al., 2023; Levy et al., 2022; Li and Qiu, 2023), we follow Li et al. (2022) to conduct memory retrieval with diversity-\nbased clustering, i.e., we partition the entire memory pool into l clusters, {M (1),M (2) \u00b7 \u00b7 \u00b7 ,M (l)}, and retrieve one memory from each cluster separately. Specifically, for each cluster M (i), we first use an off-the-shelf semantic embedder, e.g., SBERT (Reimers and Gurevych, 2019), to filter out semantically irrelevant memory and get memory candidates as follows:\nM (i)c = top-km\u2208M(i)(sim(qtest,m)), (6)\nwhere sim(\u00b7, \u00b7) is the cosine similarity of semantic embeddings. M (i)c are the i-th cluster\u2019s candidates and contain k memories.\nThen we further let the LLM select the most helpful memory from each cluster as follows:\nm(i) = LLM(qtest,M (i) c , Pretrieval), (7)\nwhere Pretrieval is the prompt for the LLM to retrieve helpful memory. We concatenate the test question qtest, memory candidates M (i) c and Pretrieval by a specialized template. The resulting input for LLM is like: \u201cReferences: [M (i)c,1,M (i) c,2 \u00b7 \u00b7 \u00b7 ,M (i) c,k] Target Question: [qtest] which one reference would be the most helpful for you to answer the target question?\u201d.\nIn this manner, we can utilize the LLM\u2019s powerful natural language understanding ability to select the most helpful memory of M (i)c for itself. Since these retrieved memories are from diverse memory clusters, they can be not only helpful for qtest but also comprehensive, thus facilitating the LLM to answer the test question. Meanwhile, the semantical-filtering can filter out semantically irrelevant memories in advance, thus significantly helps save the number of LLM calls.\nIn exploratory experiments, we find that providing only memory candidates\u2019 questions for LLMretrieval almost does not affect the retrieval result. Hence, for each M (i)c , we only provide its question for the LLM to select, which can significantly save the inference cost of the LLM. To make LLM better understand the goal of retrieving helpful memory and make its output easy-parsing with a pre-defined format, we append extra instructions like \u201cYou must end in the format like \"The most helpful question is question [idx].\u201d to the input. We show the complete input of LLM-retrieval in Appendix A."
        },
        {
            "heading": "3.3 Inference",
            "text": "Given a test question qtest, the LLM can think and then output the answer based on the retrieved memory, m(1),m(2) \u00b7 \u00b7 \u00b7m(k). Specifically, we let the LLM reason in the manner of Few-Shot-CoT:\ns = LLM(m(1),m(2), \u00b7 \u00b7 \u00b7 ,m(k), qtest) (8) a = Parse-Answer(s) (9)\nIn this paper, we focus on the overall framework and instantiate it with simple components, i.e., FewShot-CoT, simple uncertainty filtering and LLMretrieval. We further analyze the orthogonality of MoT and different CoT methods in section 4.3. We leave exploring more implementations, e.g., letting the LLM itself filter out uncertain thoughts (Weng et al., 2022; Long, 2023), as the future work."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Dataset We conduct experiments on ten datasets, across four task families: Arithmetic reasoning: AQuA (Ling et al., 2017) and DROP (Dua et al., 2019); Natural Language Inference: Adversarial NLI subsets (Nie et al., 2020), including ANLI-A1, ANLI-A2 and ANLI-A3, which cover varying difficulty levels; Commonsense Reasoning: OBQA (Mihaylov et al., 2018) and ComV (Commonsense Validation) (Wang et al., 2019); Factual Reasoning: BoolQ (Clark et al., 2019), FactCK (Fact Checker) and WikiQA (Srivastava et al., 2022). We list dataset overview, statistics, split and evaluation metrics in Appendix B. Method Comparison Since we focus on whether MoT can help the LLM self-improve, we compare MoT with baselines on the same LLM, ChatGPT (GPT-3.5-Turbo-0301), including zero-shot/fewshot CoT and zero-shot/few-shot direct prompting. To analyze the effect of rationales and thinking in MoT, we additionally compare MoT with its two variants: 1) MoT (no rationale), which removes rationales in the retrieved memory and thus lets the LLM directly output the answer, which can be seen as the few-shot direct version of MoT; 2) MoT (no thinking), which keeps rationales in the retrieved memory but forces the LLM to directly answer the question without CoT. Specifically, we add \u201cThe answer is\u201d as the LLM\u2019s output prefix to prompt the LLM directly output the answer. Through these two variants, we can analyze the effect of rationales and the thinking in MoT, respectively. Additionally, we conduct experiments of MoT under annotated datasets, MoT (with gold), to see its potential improvement space, where we use the gold labels to filter out incorrect memory. Thus, MoT will not be degraded by the incorrect answer.\nImplementation Details We use the public OpenAI language model of \u201cgpt-3.5-turbo-0301\u201d un-\nless otherwise specified and the experiments on \u201ctext-davinci-002/003\u201d (Appendix C) show consistent trends. For recalling, we use SBERT (\u201callmpnet-base-v2\u201d) (Reimers and Gurevych, 2019) for semantic filtering. And we set the number of clusters l (also the demonstration quantity) and the number of each cluster\u2019s memory candidates k as 4 and 10, respectively. We further analyze the number of demonstrations in Appendix D. In the test stage, for the stability of results, we use greedy decoding to generate the output, unless otherwise specified. We list the full implementation details and few-shot demonstrations in Appendix E."
        },
        {
            "heading": "4.2 Main Results",
            "text": "We show the results in Table 1. We see that MoT significantly outperforms baselines on most datasets, which shows MoT\u2019s best comprehensive reasoning capability on a series of NLP tasks. Specifically, MoT exceeds Few-Shot-CoT and Zero-Shot-CoT by 3.7 and 9.1 points respectively, and this directly demonstrates that MoT can make the LLM improve itself by memory-of-thoughts, without annotated dataset and parameter updates. Notably, Zero-Shot-CoT shows impressive performance on ChatGPT and outperforms Few-ShotCoT on several datasets, e.g., AQuA, DROP, ANLI1 and ANLI-2, which indicates the potential unnecessity of irrelevant CoT demonstration for the LLM with powerful zero-shot reasoning ability. Meanwhile, MoT surpasses Zero-Shot-CoT consistently on all datasets and this indicates the helpfulness of retrieved memory.\nAs for MoT\u2019s two variants, they also show better overall performance than Zero-Shot and FewShot. Meanwhile, despite directly outputting the answer, they outperform Zero-Shot-CoT and FewShot-CoT on several datasets, e.g., OBQA, BoolQ and FactCK. This is analogous to a common phenomenon in human beings: when recalling relevant\nmemory, we can perform well by intuition, without conscious reasoning (Dijksterhuis and Nordgren, 2006; Todd et al., 1999). Additionally, although MoT (no thinking) is provided with the rationales while MoT (no rationale) is not, they show generally similar performance, which indicates that explicit reasoning is necessary for the LLM to fully leverage the retrieved memory. In short, both relevant memory and explicit reasoning are essential for MoT to consistently achieve improvements on extensive datasets.\nAdditionally, MoT (with gold) shows better performance than MoT, which indicates the potential improvements when MoT applies more advanced CoT methods (Wang et al., 2023; Zhou et al., 2022; Zheng et al., 2023; Long, 2023) and verification methods (Weng et al., 2022; Manakul et al., 2023)."
        },
        {
            "heading": "4.3 Analyses",
            "text": "Multiple-Decoding Performance In this section, we evaluate MoT under self-consistency strategy (Wang et al., 2022b) which decodes multiple times and uses majority-voting to get the final answer. We compare MoT with baselines across varying sampling times and temperatures on OBQA, BoolQ and WikiQA, and the results are shown in Table 2. We see that MoT consistently outperforms Zero-Shot-CoT and Few-Shot-CoT across different decoding temperatures and sampling times, which indicates the generality and stability of MoT. We notice that the improvements slightly diminish when using more sampling times. This is similar to the phenomenon in human beings: the more carefully we think about a question, the less our previous preparation matters.\nThe Effect of LLM-retrieval To evaluate the effect of LLM-retrieval for MoT, we conduct experiments with varying retrieval methods on DROP, ANLI-A3, BoolQ and WikiQA, shown in Table 3. Besides the SBERT (\u201call-mpnet-base-v2\u201d, abbreviated as MPNet) (Reimers and Gurevych, 2019) used in MoT, we further compare two other semantic embedders, SBERT (\u201call-MiniLM-L6-v2\u201d, abbreviated as MiniLM) (Reimers and Gurevych, 2019) and Instructor-base (Su et al., 2022b) which is trained by 330 diverse tasks and supports various scenarios. We observe that using only MPNet for memory retrieval also brings significant improvements over Few-Shot-CoT, which shows MoT\u2019s usability under the limited LLM-API budget. After using the LLM to retrieve memory, the performance gets further improvements, which directly demonstrates the effectiveness of LLM-retrieval. Additionally, we see that LLM-retrieval outperforms all compared semantic embedders, which shows that the LLM can better capture the complicated reasoning logic than semantic embeddings.\nThe Effect of Filtering To evaluate the effect of memory filtering in MoT, we plot the performance\ncurve over different filtering thresholds on DROP, ANLI-A3, OBQA and WikiQA. Specifically, we tune the filtering threshold of answer-entropy uniformly and observe the corresponding performance. The results are shown in Figure 5. We find that the MoT without filtering significantly degrades and slightly underperforms Few-shot-COT on some datasets, e.g., OBQA and FactCK, which indicates that the incorrect memory can deteriorate the LLM\u2019s reasoning and thus our filtering strategy is necessary. Meanwhile, most filtering thresholds consistently lead to improvements over baselines, which demonstrates that the improvements of MoT exhibit insensitivity to the hyper-parameter of filtering thresholds in general.\nLimited Memory-Size Performance In realworld scenarios, the number of unlabeled examples or the size of available external memory space may be limited, and these can both lead to the limited memory-size. In this section, we evaluate MoT under different memory sizes. Specifically, we conduct experiments on the randomly sampled subsets with different proportions and plot the corresponding performance curve in Figure 6. We observe that MoT can consistently lead to performance improvements. Even under 10 percent of the original memory pool, MoT can still outperform Zero-ShotCoT and Few-Shot-CoT. These show the usability of MoT when unlabeled examples or available ex-\nternal memory space are limited.\nTransferability across Different COT Methods In this section, we evaluate the performance of MoT on two additional COT methods: ZeroShot-COT (Kojima et al., 2022) and Plan-andSolve (Wang et al., 2023). Compared with ZeroShot-COT which uses \u201cLet\u2019s think step by step\u201d to elicit LLM\u2019s reasoning, Plan-and-Solve uses a specialized prompt to let the LLM first devise a plan to divide the entire task into sub-tasks and then solve them based on the plan, and thus it can accomplish more complicated reasoning (Wang et al., 2023). For these two CoT methods, we use them to generate the pool of memory-of-thoughts at prethinking stage, respectively. At the test stage, we retrieve thoughts from the corresponding memory pool, concatenate them with the test question, and then use the corresponding prompt, e.g., \u201cLet\u2019s think step by step\u201d for Zero-Shot-COT, to elicit the LLM\u2019s reasoning. Results on DROP, ANLI-A1, ANLI-A3 and OBQA are shown in Table 4. We observe that MoT leads to consistent improvements, which shows its stability and generality across various CoT methods. Moreover, since these two CoT methods do not rely on manual CoT demonstrations, these results also demonstrate the effectiveness of MoT when the manual CoT demonstration is not available. Meanwhile, when using the more advanced CoT method, Plan-and-Solve, MoT\u2019s performance gets further improvements, which shows its potential in the future where the more powerful CoT method is proposed."
        },
        {
            "heading": "5 Related Work",
            "text": "Model Augmentation by LLM-generated Data In this section, we introduce previous methods that use the data generated by LLMs for model augmentation. Ye et al. (2022a); Gao et al. (2022); Ye et al. (2022b) propose ZeroGen, ProGen and ZeroGen+ to use the LLM to generate the dataset to enhance small models, e.g., LSTM. Fu et al. (2023a); Mag-\nister et al. (2022); Ho et al. (2022) leverage LLM to generate reasoning paths and teach small LMs to reason. Wang et al. (2022c) and Honovich et al. (2022) leverage the LLM to generate instruction data and improve the instruction-following capability of the LLM. Schick et al. (2023) propose ToolFormer, which learns how to use various tools by self-generated data. Zelikman et al. (2022) and Huang et al. (2022) leverage the LLM to generate reasoning paths and improve itself using labeled and unlabeled datasets, respectively. Different from these methods that depend on expensive fine-tuning, MoT can make the LLM selfimprove with memory-of-thoughts and does not depend on parameter updates and is compatible with API-accessing LLM. Recently, Zhang et al. (2022b); Shao et al. (2023) automatically generate COT demonstrations by the LLM itself. Li et al. (2022) leverage the LLM to generate the knowledge base and improve its ability of open-domain QA. These methods can be seen as the specialized case of MoT, with task-level memory selection or task-specialized memory building.\nDemonstration Retrieval for LLM In this section, we introduce previous demonstration retrieval methods for ICL, which mainly retrieve relevant input/output pairs, from an annotated dataset, for the LM to predict the test example. Liu et al. (2022) propose to leverage a dense semantic embedder to retrieve relevant examples to improve ICL. Agrawal et al. (2022) leverage BM25 to retrieve examples for machine translation\u2019s ICL. Das et al. (2021) and Hu et al. (2022) design specialized target similarities to train demonstration retrievers on ICL of knowledge-based question answering and dialogue state tracking respectively. Rubin et al. (2022); Shi et al. (2022b) use the LM\u2019s feedback to train the demonstration retriever for semantic parsing. Lyu et al. (2022) retrieve relevant examples with random labels and propose heuristic methods to reduce the negative effect of false labels. Recently, Li et al. (2023) propose UDR, a unified demonstration retriever for various NLP tasks, which is trained by the unified LM-feedback on about 40 annotated datasets. While most of these methods depend on high-quality annotated datasets and only explore in-context learning without rationales, MoT can make the LLM self-improve without annotated datasets and parameter updates, and to the best of our knowledge, we are the first to explore demonstration retrieval in the challenging\nand complicated reasoning scenarios and demonstrate MoT\u2019s effectiveness."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose MoT, a framework that let the LLM self-improve via Memory-of-Thought, without annotated datasets and parameter updates. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.\nLimitations MoT mainly has the following limitations:\n\u2022 Although we propose the answer-entropy to filter out uncertain thoughts, the remaining thoughts can still contain certain mistakes. We will explore more methods of false thought filtering (Lin et al., 2023) in the future.\n\u2022 In this paper, we employ a simple strategy to utilize the relevant memory, i.e., concatenate it with the test input qtest and thus help the LLM answer qtest. We will explore more strategies to utilize the retrieved memory, e.g., retrieving the memory to verify the current reasoning path for qtest.\n\u2022 On the one hand, in this paper, we make the first step to let the LLM self-improve based on the memory mechanism. The conducted experiments are still in a safe setting, i.e., a specific unlabeled dataset, and the LLM cannot access the internet and control external tools. Hence we think our method and experiment are still safe enough, which will not cause serious impact and unrecoverable consequences on society. On the other hand, large language models have shown various kinds of bias (Bender et al., 2021). Since we let the LLM generate thoughts/memory to help itself, the LLM might suffer from the generated biased content. We see LLM debias as an important future research topic."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027)."
        },
        {
            "heading": "A The Example of LLM-Retrieval",
            "text": "We show the LLM-retrieval example in Table 6."
        },
        {
            "heading": "B Dataset Details",
            "text": "Overview We conduct experiments on ten datasets, including four task families:\n\u2022 Arithmetic reasoning: AQuA (Ling et al., 2017): A multi-choice dataset of arithmetic questions covering various topics and difficulty levels, and DROP (Dua et al., 2019): A reading comprehension dataset that needs discrete reasoning;\n\u2022 Natural Language Inference: Adversarial NLI subsets (Nie et al., 2020), including ANLI-A1, ANLI-A2 and ANLI-A3, which cover varying difficulty levels respectively;\n\u2022 Commonsense Reasoning: OBQA (OpenBookQA) (Mihaylov et al., 2018): Commonsense-related questions which require the facts and their applications to novel situations, and ComV (Commonsense Validation) (Wang et al., 2019): A dataset that requires for identifying the sentence that does not make sense from two sentences of similar wording;\n\u2022 Factual Reasoning: BoolQ (Clark et al., 2019), FactCK (Fact Checker) (Srivastava et al., 2022): A dataset that tests the ability to evaluate the authenticity of factual claims covering Wikipedia, COVID-19 and Politics. WikiQA (Srivastava et al., 2022): question answering fron randomly-sampled Wikidata fact triples.\nSplit, Evaluation Metric and Statistics For AQuA, DROP, ANLI-A1, ANLI-A2, ANLI-A3, ComV and OBQA, we use their official test set for evaluation. For BoolQ, we follow Wang et al. (2022a) to use the validation set for evaluation, since its test set is not publicly available. For FactCK and WikiQA, we manually split them into a train/test split, and use the questions of the training set as unlabeled dataset, since there is not split version of them released. Limited by the budget, for the DROP dataset, we only use the half of its unlabeled dataset (the questions of training set) for the LLM to pre-think. For the classification or multi-choice datasets, we use the accuracy as evaluation metric. For the abstractive QA dataset\nincluding DROP and WikiQA, we use the F1-score as evaluation metric. For DROP, since its one test example has multiple annoated answer, we follow its original paper (Dua et al., 2019) to take a max over all annotated answers. Limited by budget, for those evaluation datasets that are larger than 1000, we randomly sample a subset of 1000 examples for evaluation. We list the overall dataset satistiscs, the size of memory after filtering and evaluation metrics in Table 7."
        },
        {
            "heading": "C Performance on Different LLMs",
            "text": "We conduct experiments on Text-Davinci-002 and Text-Davinci-003 (Chen et al., 2023; Ouyang et al., 2022) to evaluate MoT\u2019s generality across different LLMs. We show the results in Table 5. We observe that MoT consistently outperforms baselines on these two LLMs, which shows the effectiveness of MoT does not rely on one specific LLM and it can bring further improvements in the future where the more strong LLM is proposed."
        },
        {
            "heading": "D The Impact of Demonstration Quantity",
            "text": "We compare MoT and Few-Shot-CoT under varying numbers of demonstrations and the results are shown in Figure 7. We see that MoT consistently outperforms Few-Shot-CoT across varying amounts of demonstrations, which shows the stability of MoT. Additionally, the results show that the demonstrations in retrieved memory are more helpful and informative than manual demonstrations in Few-Shot-CoT: specifically, with 1 or 2 demonstrations, MoT can outperform Few-Shot-CoT with 4 demonstrations on OBQA and DROP.\nE Implementation Details\nWe use the public OpenAI language model of \u201cgpt3.5-turbo-0301\u201d unless otherwise specified and the experiments (Appendix C) on \u201ctext-davinci002/003\u201d show consistent improvements. Due to the limitation of LLM-API budget, we heuristically set the hyper-parameters in the pre-thinking stage, including the generation temperature T , the number of decoded reasoning paths n. For the filtering threshold \u03c4 and the number of memory clusters l, we conduct exploratory experiments on OBQA and find that \u03c4 = {0.2, 0.3, 0.4} and l = {3, 4, 5} lead to similar performance. Thus we set \u03c4 = 0.3 and l = 4, respectively. For pre-thinking, we use the temperature T = 1.2 to encourage more diverse reasoning paths, and use n = 16 reasoning path sampling times, unless otherwise specified. For memory recall, we use SBERT (\u201call-mpnet-basev2\u201d) (Reimers and Gurevych, 2019) for semantic filtering. Limited by the LLM\u2019s max input length, we fix the the number of each cluster\u2019s memory candidates as 10 for each dataset. In the test stage, for the stability of results, we use greedy decoding to generate the output, unless otherwise specified. For simplicity, we separately run MoT on each dataset and regard cross-dataset memory recall as future work. Baselines\u2019 points are from our implementation, and share the same templates, answer parsing and evaluation as MoT.\nFor AQuA, OpenBookQA, BoolQ, DROP, ANLI-A1, ANLI-A2 and ANLI-A3, We use the same few-shot CoT examples as those in Wei et al. (2022c), Zhou et al. (2022), Wang et al. (2022b) and Wang et al. (2022a), respectively. For the left datasets that have no publicly released manual CoT demonstrations, we randomly select questions from the training set and use ChatGPT to generate reasoning paths and get their few-shot CoT examples. We list the used Few-Shot-CoT examples in Ta-\nble 8, 9, 10, 12, 11, 13, 14 and 15."
        },
        {
            "heading": "F Ethics Statement",
            "text": "In this paper we make the first step to let the LLM self-improve based on the memory mechanism. The conducted experiments are still in a safe setting, i.e., a specific unlabeled dataset, and the LLM cannot access the internet and control external tools. Hence we think our method and experiment are still safe enough, which will not cause serious impact and unrecoverable consequences on society.\nLLM Input\nI will provide you with a target question and 10 reference questions. I need you to choose a reference question from \"Reference Questions\", whose question, train of thought or answer would be most helpful for you to answer the target question. Please note that the following reference QA pairs are presented in a random order without any prioritization.\nTarget Question: Machine A puts out a yo-yo every 6 minutes. Machine B puts out a yo-yo every 9 minutes. After how many minutes will they have produced 10 yo-yos? Answer Choices: (A) 24 minutes (B) 32 minutes (C) 36 minutes (D) 64 minutes (E) 72 minutes\nReference Questions: 1. Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 5 items in the same time Machine Z produces 2 items. If machine Y takes 9 minutes to produce a batch of items, how many minutes does it take for machine Z to produce the same number of items? Answer Choices: (A) 6 (B) 9 (C) 9 1/2 (D) 22.5 (E) 13 1/2 2. Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 30 items in the same time Machine Z produces 38 items. If machine Y takes 19 minutes to produce a batch of items, how many minutes does it take for machine Z to produce the same number of items? Answer Choices: (A) 6 (B) 9 (C) 9 1/2 (D) 15 (E) 13 1/2 3. Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 30 items in the same time Machine Z produces 24 items. If machine Y takes 36 minutes to produce a batch of items, how many minutes does it take for machine Z to produce the same number of items? Answer Choices: (A) 60 (B) 90 (C) 9 1/2 (D) 45 (E) 13 1/2 4. Q: Working alone at its constant rate, machine A produces x boxes in 10 minutes and working alone at its constant rate, machine B produces 2x boxes in 5 minutes. How many minutes does it take machines A and B, working simultaneously at their respective constant rates, to produce 10x boxes? Answer Choices: (A) 13 minutes (B) 14 minutes (C) 15 minutes (D) 16 minutes (E) 20 minutes 5. Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 23 items in the same time Machine Z produces 21 items. If machine Y takes 21 minutes to produce a batch of items, how many minutes does it take for machine Z to produce the same number of items? Answer Choices: (A) 6 (B) 9 (C) 9 1/2 (D) 12 (E) 23 6. Q: Machines X and Y produce bottles at their respective constant rates. Machine X produces k bottles in 6 hours and machine Y produces k bottles in 12 hours. How many hours does it take machines X and Y , working simultaneously , to produce 12k bottles? Answer Choices: (A) 8 (B) 12 (C) 15 (D) 48 (E) 24 7. Q: Machines X and Y produce bottles at their respective constant rates. Machine X produces k bottles in 6 hours and machine Y produces k bottles in 3 hours. How many hours does it take machines X and Y , working simultaneously , to produce 12k bottles? Answer Choices: (A) 4 (B) 8 (C) 12 (D) 18 (E) 4 8. Q: Machines X and Y produce bottles at their respective constant rates. Machine X produces k bottles in 4 hours and machine Y produces k bottles in 5 hours. How many hours does it take machines X and Y , working simultaneously , to produce 10k bottles? Answer Choices: (A) 8 2/3 (B) 12 5/3 (C) 15 (D) 18 (E) 22 2/9 9. Q: Working alone at its constant rate, machine A produces x boxes in 10 minutes and working alone at its constant rate, machine B produces 2x boxes in 5 minutes. How many minutes does it take machines A and B, working simultaneously at their respective constant rates, to produce 6x boxes? Answer Choices: (A) 3 minutes (B) 4 minutes (C) 5 minutes (D) 6 minutes (E) 12 minutes 10. Q: Machine A can make 350 widgets in 1 hour, and machine B can make 250 widgets in 1 hour. If both machines work together, how much time will it take them to make a total of 900 widgets? Answer Choices: (A) 1 hour and 20 minutes (B) 1 hour and 24 minutes (C) 1 hour and 30 minutes (D) 1 hour and 36 minutes (E) 1 hour and 40 minutes\nWhich one of the above reference questions is the most helpful question for you to answer the target question? You must choose exactly one reference question to you answer the target question. Your response must end in this format: \"The most helpful question is question [index].\". For example, if question 5 is your answer, you must end in \"The most helpful question is question 5.\"\nLLM output\nThe most helpful question is question 10.\nTable 6: The example of LLM-Retrieval.\nQ: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64 A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (A).\nQ: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Answer Choices: (A) 1/2 (B) 3/2 (C) 5/2 (D) 4/2 (E) 7/2 A: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means 44a / 3 = 22. So a is equal to 3/2. The answer is (B).\nQ: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance? Answer Choices: (A) 53 km (B) 55 km (C) 52 km (D) 60 km (E) 50 km A: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (E).\nQ: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (A) 1156 (B) 1392 (C) 1480 (D) 1562 (E) 1788 A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (B).\nTable 8: Few-shot CoT prompts for AQuA, from Wang et al. (2022b).\nQ: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the shortest touchdown run and the longest touchdown pass combine for? A: All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21 yards.\nQ: In the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older. How many more percent are under the age of 18 compared to the 18 to 24 group? A: According to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8.\nQ: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states. How many years did the European Coal and Steel Community exist? A: According to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7.\nQ: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith. How many scoreless quarters were there? A: The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.\nTable 9: Few-shot CoT examples for DROP, from Zhou et al. (2022).\nQ: Poison causes harm to which of the following? (A) a Tree (B) a robot (C) a house (D) a car A: Poison will harm living things, only a tree is a living thing. The answer is (A).\nQ: As you look deeper into a Marbel you can see (A) the future (B) minut defects (C) colors (D) the other side A: Marbel is not transparent, so you can not see the other side. Marbel does not necessarily have multiple colors. You will see minut defects. The answer is (B).\nQ: When food is reduced in the stomach (A) the mind needs time to digest (B) take a second to digest what I said (C) nutrients are being deconstructed (D) reader\u2019s digest is a body of works A: The food is being deconstructed in the stomach during digestion. The answer is (C).\nQ: The sun is responsible for (A) puppies learning new tricks (B) children growing up and getting old (C) flowers wilting in a vase (D) plants sprouting, blooming and wilting A: The sun can affect the growing of living things, like plants. The answer is (D).\nTable 11: Few-shot CoT examples for OBQA, from Wang et al. (2022a).\nQ: Which one of the following statements is against common sense? (A) Roses buds eat caterpillars (B) The caterpillar eats the rose bud A: Statement (A) is against common sense as it goes against the natural food chain and the known behavior of roses. Roses are plants and cannot eat or consume other organisms, including caterpillars. The answer is A.\nQ: Which one of the following statements is against common sense? (A) He threw his house into the trash bin (B) He threw his food waste into the trash A: It is not physically possible to throw a house into a trash bin. Statement (A) goes against the laws of physics and is therefore illogical. The answer is A.\nQ: Which one of the following statements is against common sense? (A) Because his car was damaged, he received RMB 1000 from electricity company (B) Because his car was damaged, he received RMB 1000 from insurance company A: It does not make logical sense for an electricity company to compensate someone for car damage. It is more reasonable for an insurance company to provide compensation for car damage. The answer is A.\nQ: Which one of the following statements is against common sense? (A) Because his car was damaged, he received RMB 1000 from electricity company (B) Because his car was damaged, he received RMB 1000 from insurance company A: It does not make logical sense for an electricity company to compensate someone for car damage. It is more reasonable for an insurance company to provide compensation for car damage. The answer is A.\nTable 12: Few-shot CoT examples for ComV.\nQ: does system of a down have 2 singers? A: System of a Down currently consists of Serj Tankian, Daron Malakian, Shavo Odadjian and John Dolmayan. Serj and Daron do vocals, so the band does have two singers. The answer is yes.\nQ: do iran and afghanistan speak the same language? A: Iran and Afghanistan both speak the Indo-European language Persian. The answer is yes.\nQ: is a cello and a bass the same thing? A: The cello is played sitting down with the instrument between the knees, whereas the double bass is played standing or sitting on a stool. The answer is no. Q: can you use oyster card at epsom station? A: Epsom railway station serves the town of Epsom in Surrey and is not in the London Oyster card zone. The answer is no.\nTable 13: Few-shot CoT examples for BoolQ, from Wang et al. (2022a).\nQ: On June 2017, the following claim was made: David Lloyd George lost every bid to become prime minister. Was this claim true or false? A: David Lloyd George served as the Prime Minister of the United Kingdom from 1916 to 1922. He also served as the Chancellor of the Exchequer and the Minister of Munitions before becoming Prime Minister. Therefore, the claim that he lost every bid to become Prime Minister is false. The answer is false.\nQ: On June 2017, the following claim was made: In 1966, George Harrison got married for the first time. Was this claim true or false? A: George Harrison married his first wife, model Pattie Boyd, on January 21, 1966. The answer is true.\nQ: On June 2017, the following claim was made: Woodrow Wilson did not live during World War I. Was this claim true or false? A: Woodrow Wilson was the President of the United States during World War I, serving from 1913 to 1921. The answer is false.\nQ: On April 17 2008, the following claim was made: Hillary Clinton has taken over $800,000 from lobbyists. Was this claim true or false? A: According to OpenSecrets.org, a nonpartisan research group that tracks money in politics, Hillary Clinton received over $800,000 in campaign contributions from lobbyists during her 2008 presidential campaign. The answer is true.\nTable 14: Few-shot CoT examples for Fact Checker.\nQ: On June 2017, the following claim was made: David Lloyd George lost every bid to become prime minister. Was this claim true or false? A: David Lloyd George served as the Prime Minister of the United Kingdom from 1916 to 1922. He also served as the Chancellor of the Exchequer and the Minister of Munitions before becoming Prime Minister. Therefore, the claim that he lost every bid to become Prime Minister is false. The answer is false.\nQ: The native language of Aaron Swartz is? A: Aaron Swartz was born in Chicago, Illinois, United States. Therefore, his native language is most likely English, as it is the primary language spoken in the United States. The answer is English.\nQ: The religion of Prajadhipok is? A: Prajadhipok was a Buddhist, as Buddhism is the predominant religion in Thailand, where he was the last absolute monarch before the country became a constitutional monarchy. The answer is Buddhism.\nQ: The country of Valletta is? A: Valletta is the capital city of Malta, which is a small island nation located in the Mediterranean Sea. The answer is Malta.\nQ: The sport played by Garry Kasparov is? A: Garry Kasparov is a former world chess champion, therefore the sport played by him is chess. The answer is chess.\nTable 15: Few-shot CoT examples for WikiQA."
        }
    ],
    "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
    "year": 2023
}