{
    "abstractText": "Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10\u22126% of the Transformer\u2019s FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-ofthe art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries.",
    "authors": [
        {
            "affiliations": [],
            "name": "Livio Baldini Soares"
        },
        {
            "affiliations": [],
            "name": "Daniel Gillick"
        },
        {
            "affiliations": [],
            "name": "Jeremy R. Cole"
        },
        {
            "affiliations": [],
            "name": "Tom Kwiatkowski"
        }
    ],
    "id": "SP:a3fb0f230e4d3ca1e8fc346c181b151d7b7fbbe0",
    "references": [
        {
            "authors": [
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "Electra: Pretraining text encoders as discriminators rather than generators",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
            "year": 2020
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "Context-aware term weighting for first stage passage retrieval",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201920, page 1533\u20131536, New",
            "year": 2020
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Vincent Y. Zhao",
                "Ji Ma",
                "Yi Luan",
                "Jianmo Ni",
                "Jing Lu",
                "Anton Bakalov",
                "Kelvin Guu",
                "Keith B. Hall",
                "Ming-Wei Chang"
            ],
            "title": "Promptagator: Few-shot dense retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Thibault Formal",
                "Carlos Lassance",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "From distillation to hard negative sampling: Making sparse neural ir models more effective",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Thibault Formal",
                "Carlos Lassance",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "Splade v2: Sparse lexical and expansion model for information retrieval",
            "venue": "CoRR, abs/2109.10086.",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Formal",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "Splade: Sparse lexical and expansion model for first stage ranking",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Unsupervised corpus aware language model pre-training for dense passage retrieval",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2843\u20132853,",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "COIL: Revisit exact lexical match in information retrieval with contextualized inverted list",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Jiatao Gu",
                "James Bradbury",
                "Caiming Xiong",
                "Victor O.K. Li",
                "Richard Socher."
            ],
            "title": "Nonautoregressive neural machine translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Kailash A. Hambarde",
                "Hugo Proenca"
            ],
            "title": "Information retrieval: Recent advances and beyond",
            "year": 2023
        },
        {
            "authors": [
                "Kai Hui",
                "Honglei Zhuang",
                "Tao Chen",
                "Zhen Qin",
                "Jing Lu",
                "Dara Bahri",
                "Ji Ma",
                "Jai Gupta",
                "Cicero Nogueira dos Santos",
                "Yi Tay",
                "Donald Metzler."
            ],
            "title": "ED2LM: Encoder-decoder to language model for faster document re-ranking inference",
            "venue": "Find-",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave"
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "year": 2021
        },
        {
            "authors": [
                "Kalervo J\u00e4rvelin",
                "Jaana Kek\u00e4l\u00e4inen."
            ],
            "title": "Cumulated gain-based evaluation of ir techniques",
            "venue": "ACM Trans. Inf. Syst., 20(4):422\u2013446.",
            "year": 2002
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR",
            "year": 2020
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Carlos Lassance",
                "St\u00e9phane Clinchant."
            ],
            "title": "An efficiency study for splade models",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201922, page 2220\u20132226, New York, NY, USA.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Lee",
                "Elman Mansimov",
                "Kyunghyun Cho."
            ],
            "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1173\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Zhuyun Dai",
                "Sai Meher Karthik Duddu",
                "Tao Lei",
                "Iftekhar Naim",
                "Ming-Wei Chang",
                "Vincent Y. Zhao"
            ],
            "title": "Rethinking the role of token retrieval in multi-vector retrieval",
            "year": 2023
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Lewis",
                "Yuxiang Wu",
                "Linqing Liu",
                "Pasquale Minervini",
                "Heinrich K\u00fcttler",
                "Aleksandra Piktus",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "PAQ: 65 million probably-asked questions and what you can do with them",
            "venue": "Transactions of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma"
            ],
            "title": "A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques",
            "year": 2021
        },
        {
            "authors": [
                "Antonio Mallia",
                "Omar Khattab",
                "Torsten Suel",
                "Nicola Tonellotto."
            ],
            "title": "Learning passage impacts for inverted indexes",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR",
            "year": 2021
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "MS MARCO: A human generated machine reading comprehension dataset",
            "venue": "Proceedings of the Workshop on Cognitive Computation: Inte-",
            "year": 2016
        },
        {
            "authors": [
                "Jianmo Ni",
                "Chen Qu",
                "Jing Lu",
                "Zhuyun Dai",
                "Gustavo Hern\u00e1ndez \u00c1brego",
                "Ji Ma",
                "Vincent Y. Zhao",
                "Yi Luan",
                "Keith B. Hall",
                "Ming-Wei Chang",
                "Yinfei Yang."
            ],
            "title": "Large dual encoders are generalizable retrievers",
            "venue": "CoRR, abs/2112.07899.",
            "year": 2021
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Zhiying Jiang",
                "Ronak Pradeep",
                "Jimmy Lin."
            ],
            "title": "Document ranking with a pretrained sequence-to-sequence model",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 708\u2013718, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Jimmy Lin",
                "AI Epistemic."
            ],
            "title": "From doc2query to doctttttquery",
            "venue": "Online preprint, 6.",
            "year": 2019
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Wei Yang",
                "Jimmy Lin",
                "Kyunghyun Cho."
            ],
            "title": "Document expansion by query prediction",
            "venue": "arXiv preprint arXiv:1904.08375.",
            "year": 2019
        },
        {
            "authors": [
                "Rodrigo Frassetto Nogueira",
                "Wei Yang",
                "Jimmy Lin",
                "Kyunghyun Cho."
            ],
            "title": "Document expansion by query prediction",
            "venue": "CoRR, abs/1904.08375.",
            "year": 2019
        },
        {
            "authors": [
                "Cicero Nogueira dos Santos",
                "Xiaofei Ma",
                "Ramesh Nallapati",
                "Zhiheng Huang",
                "Bing Xiang."
            ],
            "title": "Beyond [CLS] through ranking by generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Yingqi Qu",
                "Yuchen Ding",
                "Jing Liu",
                "Kai Liu",
                "Ruiyang Ren",
                "Wayne Xin Zhao",
                "Daxiang Dong",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering",
            "venue": "Proceed-",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "Journal of Machine Learning Re-",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The Probabilistic Relevance Framework: BM25 and Beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Stephen E. Robertson",
                "Steve Walker."
            ],
            "title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.",
            "year": 1994
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "ColBERTv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "NAACL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "Thirty-fifth Conference on Neural Information Processing Sys-",
            "year": 2021
        },
        {
            "authors": [
                "Hassabis."
            ],
            "title": "Parallel WaveNet: Fast highfidelity speech synthesis",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3918\u20133926. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Lee Xiong",
                "Chenyan Xiong",
                "Ye Li",
                "Kwok-Fung Tang",
                "Jialin Liu",
                "Paul N. Bennett",
                "Junaid Ahmed",
                "Arnold Overwijk."
            ],
            "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
            "venue": "CoRR, abs/2007.00808.",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Xiaopeng Lu",
                "Kyusong Lee."
            ],
            "title": "SPARTA: Efficient open-domain question answering via sparse transformer matching retrieval",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Guoqing Zheng",
                "Jamie Callan."
            ],
            "title": "Learning to reweight terms with distributed representations",
            "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201915, page 575\u2013584, New",
            "year": 2015
        },
        {
            "authors": [
                "Giulio Zhou",
                "Jacob Devlin."
            ],
            "title": "Multi-vector attention models for deep re-ranking",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5452\u20135456, Online and Punta Cana, Dominican Republic. Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Shengyao Zhuang",
                "Guido Zuccon"
            ],
            "title": "Fast passage re-ranking with contextualized exact term matching and efficient passage expansion",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "We attempt to answer the following question: to what extent can the computationally-intensive inference in modern neural retrieval systems be pushed entirely to indexing time?\nNeural networks have revolutionized information retrieval, both with powerful reranking models that cross-attend to query and document, and with dual-encoder models that map queries and documents to a shared vector space, leveraging approximate nearest neighbor search for top-k retrieval. The strongest systems typically use a dual-encoder for retrieval followed by a cross-attention reranker to improve the ordering. However, both these components tend to be built on increasingly large Transformers (Ni et al., 2021; Nogueira dos Santos et al., 2020; Izacard et al., 2021; Hui et al., 2022) and thus rely on dedicated accelerators to process queries\nquickly at serving time. In many application settings, this may be impractical or costly, and as we will show, potentially unnecessary.\nIn particular, we explore a retrieval paradigm where documents are indexed by predicted query token scores. As a result, scoring a query-document pair (q, d) simply involves looking up the scores for the tokens in q associated with d in the index. While the scores are predicted by a neural network, the lookup itself involves no neural network inference so can be faster than other approaches. However, this also means that there can be no crossattention between a specific query and document or even a globally learned semantic vector space. Given these shortcomings, it is unclear that such a model, which offloads all neural network computation to indexing time, can be a practical alternative to its more expensive neural counterparts.\nIn addition, while large pre-trained language models have been shown to generalize well over a number of language and retrieval tasks (Chowdhery et al., 2022; Raffel et al., 2020; Brown et al., 2020; Nogueira et al., 2019b; Ni et al., 2021), a key challenge is that they have universally adopted a sequence-to-sequence architecture which is not obviously compatible with precomputing query scores. Naive approaches are either computationally infeasible (scoring all possible queries), or rely on sampling a small, incomplete set of samples (such as in Lewis et al. 2021).\nTo overcome this challenge, we introduce a novel use of non-autoregressive decoder architecture that is compatible with existing Transfomerbased language models (whether Encoder-Decoder or Decoder-only, Chowdhery et al. 2022). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems (Ni et al., 2021; Izacard et al., 2021; Formal et al., 2021a). We call the retrieval system based on this proposed\nmodel NAIL (Non-Autoregressive Indexing with Language models). We summarize our contributions as follows: 1. We advance prior work on learned sparse re-\ntrieval by leveraging pretrained LMs with a novel non-autoregressive decoder.\n2. We describe a range of experiments using the BEIR benchmark (Thakur et al., 2021) that explore the performance and efficiency of our model as a reranker and as a retriever. As a reranker, NAIL can recover 86% of the performance of a large cross-attention reranker (Nogueira et al., 2020), while requiring 10\u22126% of the inference-time FLOPS. As a retriever, NAIL has an extremely high upper bound for recall\u2014exceeding the performance of all other retrievers in the zero-shot setting. Finally, by using BM25 as a retriever and NAIL as a reranker, we can match state-of-the-art dualencoders (Ni et al., 2021; Izacard et al., 2021) with 10\u22124% of the inference-time FLOPS.\n3. We propose our model as a preferred solution when significant compute is available at indexing time, but not on-demand at serving time, and we provide a cost analysis that illustrates when our approach could be preferred to previous work that harnesses LLMs."
        },
        {
            "heading": "2 Related work",
            "text": "There has been much work in information retrieval leveraging neural networks, which we cannot adequately cover in this paper. For a comprehensive overview, we refer the reader to the survey by Hambarde and Proenca 2023. Here, we focus on methods that minimize the use of expensive neural methods at query inference time (typically methods of sparse retrieval) and on those that leverage LLMs.\nLM-based Term Weighting Bag-of-words models, such as TF-IDF and BM25 (Robertson and Zaragoza, 2009), use term weighting based on corpus statistics to determine relevance of document terms to query terms. Our work can be seen as a way to construct document term weights that are\nboth (1) unconditional with respect to the query, and (2) indexed using lexicalized features (specifically, we use a vector of token scores). As a result, this type of document representation can be precomputed (at indexing time) and does not require expensive computation at query-time. Prior work on leveraging language models to produce such lexicalized term weighting can be roughly divided into two groups: those with just document-side encoders, and those with query-side and documentside encoders.\nExamples of the first group include DeepCT (Dai and Callan, 2020), DeepTR (Zheng and Callan, 2015), and DeepImpact (Mallia et al., 2021), Tilde v2 (Zhuang and Zuccon, 2021), and Spladedoc (Formal et al., 2021a). These systems are examples of the model paradigm we are exploring, in which all neural network computation happens at indexing time. Our work can be seen as an attempt to update these systems (which use word2vec embeddings or encoder-only language models) to modern encoder-decoder architectures. Splade-doc is the most recent (and performant) of these, so is in many cases the most useful point of comparison for our work. We include results for the best version of Splade-doc (Lassance and Clinchant, 2022).\nExamples of the second group include SPARTA (Zhao et al., 2021), ColBERT (Khattab and Zaharia, 2020), ColBERT v2 (Santhanam et al., 2022), COIL (Gao et al., 2021), Splade (Formal et al., 2021b), and Splade v2 (Formal et al., 2021a). These sparse dual-encoders have proven themselves competitive with dense dual-encoders, and have some advantages like improved interpretability. We demonstrate comparable performance without the need for any query-side encoder.\nLM-based Document Expansion Another way to improve retrieval indices using language models is document expansion. This consists of augmenting the terms in a document that do not occur in its original text, but are likely to be useful for retrieval. When used in combination with a lexicalized retrieval index, document expansion can be implemented without additional query-\ntime computational requirements. Recent examples of LM-based document expansion systems include Doc2Query (Nogueira et al., 2019c) and Doc2Query-T5 (Nogueira et al., 2019a).\nOther forms of document expansion include the Probably asked questions database (Lewis et al., 2021) which, via an expensive offline system, uses a generative language model to produce lists of questions for every document in the corpus.\nWe agree with Lin and Ma (2021) that document expansion typically improves the quality of retrieval systems, irrespective of representation used. Our approach, however, makes no assumptions about which terms should be used to index a document, allowing the model to score all tokens in the vocabulary.\nNon-autoregressive decoders Non-autoregressive sequence-to-sequence models have been previously proposed and studied, particularly in the context of machine translation (Gu et al., 2018; van den Oord et al., 2018; Lee et al., 2018), motivated by the computational complexity of standard auto-regressive decoding, which requires a decode step per generated token. Non-autoregressive decoding breaks the inter-step dependency and thus provides two computational benefits: (1) a single step through the decoder can produce outputs for more than one position, and (2) computation can be easily parallelized since are is no time-wise dependencies between computations.\nWhile these systems use non-autoregressive decoding to perform iterative generation of text, we know of no existing work that uses nonautoregressive decoding to produce document representations or for retrieval purposes.\n3 NAIL Model\nA major goal of this work is to investigate retrieval methods that forego neural computation and the need for specialized accelerator hardware at query time. As such, we focus on a method that uses a large neural model to precompute the required representations of the retrieval items (documents) ahead of time. Then, at retrieval time, the method performs only basic featurization (e.g., tokenization) of the queries.\nSpecifically, we investigate query-document scoring functions that score the compatibility of a query-document pair with the inner-product of separate featurizations of the query \u03c6q(q) and document \u03c6d(d).\nscore(q, d) = \u3008\u03c6q(q), \u03c6d(d)\u3009 (1)\nThis form is familiar from both traditional lexicalized retrieval and from more recent work on dense retrieval. In lexicalized retrieval, (e.g., TF-IDF and BM25) (Robertson and Zaragoza, 2009; Robertson and Walker, 1994), \u03c6q and \u03c6d assign non-zero scores to sub-strings of q and d. On the other hand, in dense retrieval (Karpukhin et al., 2020; Ni et al., 2021; Izacard et al., 2021), \u03c6q and \u03c6d are neural networks that map q and d to dense vectors. Note that this formulation does not allow for deeper interactions between d and q, such as cross-encoder scorers, as these cannot be computed efficiently and without an accelerator at query time.\nWe investigate an alternative formulation of Equation 1 than either traditional lexicalized retrieval or dense retrieval. In this formulation, \u03c6d can be an arbitrarily complex neural network, but \u03c6q must be a sparse featurization that can be quickly computed on commodity CPUs. This way, it is possible to push all costly neural network inference to indexing time, and avoid the need for accelerators at serving-time. For this paper, we choose \u03c6q to be a simple tokenizer, but we believe that our results could also extend to more complex sparse featurizations."
        },
        {
            "heading": "3.1 Independent prediction of query tokens",
            "text": "Given the choice of \u03c6q described above, we need to learn a function \u03c6d that can assign high scores to tokens that are are likely to occur in a query associated with the input document and low scores to tokens that are unlikely to appear in such a query. This goal differs from related work on query prediction for document expansion (Nogueira et al., 2019b; Lewis et al., 2021) where only a few likely query terms are added to the set of document terms.\nInstead of aiming to predict a small number of queries that are related to d, we aim to predict a featurization of d that can be used to score any query. Given that an important motivation of this work is to make use of large pretrained language models, we must also investigate how best to adapt the sequence-to-sequence generative architecture that most such models have adopted. In particular, the Transformer-based language models adopt an autoregressive decoding strategy, where the model predicts a single token position at a time, conditioned on the output of previous predictions. A naive decoding strategy, of decoding every possible target query ahead of time, is not computationally feasible, requiring 32k16 = 1072 decode steps (or more generally, |V|l, where V is the vocabulary and l is the length of the query).\nHow do we generate document representations, using a sequence-to-sequence architecture, in a computationally efficient way?\nTo do this, while also making use of pre-trained Transformer language models, we modify the decoder stack to support independent predictions of the output tokens (also known in the literature as non-autoregressive decoding, Lee et al. 2018; Gu et al. 2018). In addition, we modify the output of the model so that instead of generating a token sequence, it generates a sequence of scores over the vocabulary. We use this predicted sequence of vector of scores over the vocabulary as a representation of the document d in our system.\nOur model architecture is illustrated in Figure 1. In this model, each output token is predicted independently from other output tokens, and is conditioned only on input sequence and positional information. This allows the model to produce output for all positions in parallel. In addition, because the output representation is no longer a single token, but scores over the entire vocabulary, we can obtain a representation for scoring any possible query q in a single step of the decoder.\nThe NAIL model is based on the T5 architecture (Raffel et al., 2020) and, for the experiments in Section 5, we start with pre-trained T5 checkpoints. There are several ways to use such a model to predict feature scores. NAIL uses the T5 vocabulary as its featurization, consisting of 32,000 tokens. In order to quickly score all 32,000 tokens, we modify the baseline model in two ways: 1. The standard encoder-decoder model proceeds\nauto-regressively, predicting the next token\nbased on the previous predicted tokens. Each output token additionally conditions on a relative position embedding based on the current decode position. Here, instead there are a fixed number of decode positions which all proceed simultaneously, conditioning only on the input and a fixed position embedding.\n2. In both the standard T5 model and our adaptation of it, each token position outputs a distribution over the entire output vocabulary. Normally, this produces a single sequence of tokens by sampling or taking the maximum probability token at each position. Here, we instead pool over all positions, taking the maximum token score produced at any position.\nA simpler alternative would be to have the model decode for only a single position and then use the produced distribution as the scores for each token. However, we found that the model was able to represent a more diverse and better-performing distribution of query tokens when it could distribute their predictions over multiple output positions."
        },
        {
            "heading": "3.2 Contrastive training",
            "text": "Similar to previous work that has trained dual encoders for retrieval, we utilize negative training examples in order to do contrastive learning. In particular, we assume training data of the form D = {(q0, d+0 ,d \u2212 0 ), . . . , (qn, d + n ,d \u2212 n )} made up of triples that associate a query qi with a positive passage d+i and a set of k negative passages d\u2212i = {d \u2212 i:0, . . . , d \u2212 i:k}. The negative passages are typically related to the query but are worse retrievals than the positive passages.\nWe train NAIL by assembling D into batches of m examples and calculating an in-batch softmax that includes both positive and negative passages from the batch (Ni et al., 2021). Let a single batch of m examples be\nbi = ((qi\u2217m, d + i\u2217m, d \u2212 i\u2217m)), . . . ,\n(qi\u2217m+m\u22121, d + i\u2217m+m\u22121,d \u2212 i\u2217m+m\u22121))\n(2)\nand let di be all of the positive and negative candidate passages in this batch. The per-example loss for a query q and positive passage d+ drawn from batch bi is\nL = \u2212\u3008\u03c6q(qi), \u03c6d(d+)\u3009+ log \u2211 d\u2032\u2208di exp(\u3008\u03c6q(qi), \u03c6d(d\u2032)\u3009) (3) and we train the model to incrementally minimize the per-batch loss, summed over all m examples in the batch. Note that the number of explicit negative passages can vary under this setup, as the positive passages for other queries serve as implicit negative passages for every other query. More details about the training setup are given in the following section."
        },
        {
            "heading": "4 Model Training and Experiments",
            "text": "To train the NAIL model, we have empirically found it beneficial to perform two stages of training (1) a pre-training stage the uses self-supervised tasks over a large, unlabeled text corpus, and (2) a finetuning stage that relies on question-answering data via explicit hard negatives. We present the details of each of the training steps in Sections 4.1 and 4.2.\nOur model is implemented within the T5X framework (Roberts et al., 2022) and we initialize model weights with published T5.1.1 checkpoints (Raffel et al., 2020). Unless otherwise noted, the NAIL model size used in the experiments is XL, with roughly 3 billion parameters. We saw no further gains from increasing parameters further.\nTo be compatible with T5 checkpoints, we also adopt the T5 vocabulary and attendant SentencePiece tokenizer (Kudo and Richardson, 2018). The vocabulary consists of 32,000 tokens extracted from a English-focused split of Common Crawl."
        },
        {
            "heading": "4.1 Pre-training",
            "text": "For pretraining, we combine two related selfsupervision tasks for retrieval: inverse cloze and independent cropping (Lee et al., 2019; Izacard et al., 2021). Both of these tasks take in a passage from a document and generate a pair of spans of text, forming a positive example. One of the generated spans serves as a pseudo-query and the other as a\npseudo-passage. In independent cropping, two contiguous spans of text are sampled from the passage. As the spans are selected independently, overlaps between them are possible. For the inverse cloze task, a contiguous span is initially selected from the passage, forming a pseudo-query. The second span encompasses the remainder of the passage with the sub-sequence selected in the first span omitted.\nIn both tasks, we use the C4 corpus (Raffel et al., 2020), a cleaned version of Common Crawl\u2019s web crawl corpus. In each training batch, half of the examples are from the independent cropping task and half are from the inverse cloze task. In addition, each target has a single correct corresponding input, and all other inputs serve as negatives.\nWe found this pre-training to be very important to calibrate language model scores to lexical retrieval scores. One possible reason is that while highly frequent words (stop words) typically have a high score in LMs, they are known to be insignificant or harmful in ranking retrievals independent of the context or inputs in which they occur. Additional discussion of the need for pre-training can be found in Appendix B.2. We run pre-training for 500k steps on batches of 2048 items, the largest size we are able to fit into accelerator memory."
        },
        {
            "heading": "4.2 Fine-tuning",
            "text": "We finetune our model on the MS-MARCO dataset (Nguyen et al., 2016). It consists of roughly 500,000 queries, each with a corresponding set of gold passages (typically one per query) along with a set of 1,000 negative passages produced by running a BM25 system over the full corpus of 8.8M passages. We construct training examples using the gold passage as positive, along with a sample of the BM25 candidate passages as hard negatives.\nWe investigate a variable number of MSMARCO hard negatives and find that more hard negatives improves MS-MARCO performance but worsens BEIR performance. More details can be found in Appendix B.1. Similar to pre-training, each batch consists of 2048 total passages."
        },
        {
            "heading": "4.3 Evaluation Methodology",
            "text": "For evaluation, we focus on the public, readilyavailable, datasets available in the BEIR (Thakur et al., 2021) suite and which have baseline numbers present in the leaderboard, which totals 12 distinct datasets. We specifically target BEIR since it contains a heterogeneous set of retrieval datasets, and equally importantly, evaluates these datasets in zero-shot setting. While neural models have made\nhuge gains over BM25 on in-domain data, BEIR shows that a variety of neural retrievers underperform relative to BM25 on out-of-domain data.\nBEIR results are typically presented as two separate tasks, where most systems are only evaluated on either the reranking variant or the full retrieval variant. In the full retrieval variant, systems must retrieve over the provided corpus of document passages, which range from a few thousand to a few million, and they are evaluated on their recall@100 and their nDCG@10 (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002), providing a view into their ability to retrieve the gold passages into the top 100 and the ordering of the top ten passages, respectively. In the reranking variant, models do not have to do retrieval, and the recall@100 is fixed to the performance of an off-the-shelf BM25 system, so only nDCG@10 is reported."
        },
        {
            "heading": "5 Experimental Evaluation",
            "text": "We compare NAIL to other systems that have published results on BEIR. To compare with some sparse systems that have not been evaluated on BEIR datasets, we also make of use the MSMARCO passage ranking task. We focus on answering the following questions: \u2022 How does NAIL perfom as a reranker, particu-\nlarly when compared to much more expensive neural reranker systems?\n\u2022 How does NAIL compare to recent term weighting retrieval systems that use language models?\n\u2022 How does NAIL compare with a similarly trained dual-encoder system that uses an expensive query-side encoder? Further experimental work is also presented in appendices, including: qualitative analysis (Appendix A), sensitivity to hard-negatives in the batch loss (Appendix B.1), effects of ablating pre-training or fine-tuning (Appendix B.2), and analysis of sparsifying document representations to make them more efficient for indexing (Appendix C)."
        },
        {
            "heading": "5.1 Reranking",
            "text": "In the reranking BEIR task, each system must rerank the 100 passages returned by an off-theshelve BM25 system.\nBaselines In this section we divide approaches into two types of systems: lexical-based approaches and cross-encoders. In the cross-encoder category, we compare to MonoT5-3B (Nogueira\net al., 2020) and MiniLM-L6 1. MiniLM-L6 is a BERT-based models trained on MS-MARCO using a cross-encoder classifier. MonoT5-3B uses a T5-based model fine-tuned on MS-MARCO, using a generative loss for reranking.\nResults Table 2 shows the reranking results. The baseline comparison for NAIL\u2019s performance here is BM25 alone: using BM25 without a reranker is the only other method that does not need to run a neural network for each query. We see that NAIL improves over BM25 fairly consistently. The improvement on MS-MARCO, which has in-domain training data, is especially striking. On BEIR, NAIL improves performance on 10 out of the 12 datasets increasing the average score by over 5 points.\nWhile cross-encoder models are more powerful, they are also more expensive. Cross-encoder models must run inference on all 100 documents for each query. Thus, NAIL uses 8 to 9 orders of magnitude fewer FLOPS than cross encoder models, corresponding to almost 1 trillion fewer FLOPS for a single query. Moreover, NAIL significantly closes the gap between the BM25 baseline and the top performing cross-encoder rerankers, capturing 86% of the gains on MS MARCO and 45% of the gains on the broader suite of BEIR tasks. Thus, it presents an attractive alternative to expensive\n1huggingface.co/cross-encoder/ms-marco-MiniLM-L-6v2\nrerankers when compute is limited."
        },
        {
            "heading": "5.2 Full Corpus Retrieval",
            "text": "In the full corpus retrieval task, each system must retrieve and rank from each dataset\u2019s corpus.\nBecause NAIL is very cheap to run as a reranker, it is reasonable to compare the BM25+NAIL results from Section 5.1 to direct retrieval systems that do not include a reranking step, but typically consume many orders of magnitude more FLOPs at query time. Table 3 presents this comparison.\nAs NAIL could be used to populate an inverted index, we investigate how well NAIL works when scoring all candidates in the corpus, which is an upper-bound for a NAIL-only retrieval system. These results are presented as NAIL-exh in Table 3.\nWe later present a brief investigation into the effect of sparsification of the NAIL output, to further understand the potential for using NAIL to populate a sparse inverted index for retrieval.\nBaselines For full retrieval, we compare NAIL to lexical-based and dual-encoder systems.\nGTR-XXL (Ni et al., 2021) is one of the largest and best performing dual-encoder systems publicly available. It is pre-trained on a large, non-public, corpus of 2 billion QA pairs scraped from the web, and fine-tuned on MS-MARCO. Contriever is a dual-encoder system which employs novel selfsupervised pretraining task (Izacard et al., 2021) and is fine-tuned on MS-MARCO; we describe it in more detail in Section 5.4.\nSPLADE v2 (Formal et al., 2021a) develops query and document encoders to produce sparse representations, differing from dense dual-encoders systems. The query and document representations in SPLADE v2 are used for slightly different objectives. The query encoder is used to perform query expansion, and the document encoder is used to produce sparse representations for indexing. This system is trained via distillation of a cross-encoder reranker, and finally fine-tuned on MS-MARCO.\nColbert v2 adopts a late interaction model that produces multi-vector representations for both documents and passages. In this model, per-token affinity between query and document tokens are scored using per-token representations. It is trained via distillation of a cross-encoder reranker.\nBesides BM25 and NAIL, SPLADE-doc+ is the only other retriever that does not require neural network inference at query time. This model is a variant of SPLADE v2 where the query encoder is dropped, and only the document encoder is\nused (Lassance and Clinchant, 2022). As with SPLADE v2, SPLADE-doc+ is trained using distillation of cross-encoder reranker, with additional fine-tuning on MS-MARCO.\nResults Table 3 shows the results for nDCG@10 and recall@100 on BEIR full corpus retrieval for all systems that report it. We stratify the results into two sets, (1) MS-MARCO, which with the exception of BM25, is used as a training dataset, and (2) the average over all the other BEIR datasets, which are evaluated as zero-shot.\nOn the out-of-domain BEIR tasks, BM25+NAIL beats all but one of the neural retrieval systems, despite not encoding the query with a neural network and being limited in recall to BM25. Additionally, we note that NAIL-exh outperforms all other retrieval systems according to the recall@100 metric, suggesting potential for a NAIL-based retriever that uses NAIL to populate an inverted index. However, given the lower nDCG@10 than BM25+NAIL, this may only be worthwhile to implement if combined with a different reranker. Note that while recall@100 is highest for NAIL on the out-of-domain BEIR tasks, NAIL does worse than other models like GTR-XXL on the in-domain MSMARCO task. This is in part due to the training recipes used by other work to optimize for MS-MARCO performance, including model distillation and large nonpublic corpora of QA pairs.\nSPLADE-doc also does not require a query-time encoder. We observe that NAIL lags on the indomain evaluation but outperforms SPLADE-doc on both metrics of the zero-shot datasets in BEIR. As with many of the other retrievers, SPLADEdoc was distilled from a cross-attention reranker teacher, which may account for this in-domain gain in performance (Gao and Callan, 2022; Formal et al., 2022)."
        },
        {
            "heading": "5.3 Comparison to Term Weighting Models",
            "text": "In this work we are primarily interested in the zeroshot multi-domain retrieval task represented by BEIR. However Table 4 also contains a comparison to other recent systems that use LMs to compute term weights, using the in-domain MS-MARCO passage retrieval task that they focused on. For NAIL, we report both the version which uses BM25 retrievals (in that case, the recall metric is derived from the BM25 system) and the system described in the previous section which uses exhaustive scoring. The results demonstrate that both NAIL-exh and BM25+NAIL outperform the other term weight-\ning models presented on the MRR@10 metric for the MS-MARCO passage ranking task. With respect to recall, NAIL-exh clearly improves over the previous systems. Exhaustive scoring is much more expensive than the other systems shown; however, given the sparsification results shown in Figure 3, we believe a sparse version of NAIL would be competitive with the models presented."
        },
        {
            "heading": "5.4 Comparison to Contriever",
            "text": "There are several confounding factors in comparing the systems presented in Tables 2 and 3. As mentioned, each system uses different training recipes and training data while also having slightly different architectures. Training techniques presented in the baselines presented in this work include unsupervised pretraining, hard negative mining, and distillation from a cross-attention teacher. These factors can make it difficult to pinpoint the cause of the variance in performance across models.\nHowever, NAIL and Contriever (Izacard et al., 2021) share training recipes to a large extent, having both a similar pretraining stage followed by fine-tuning on MS-MARCO. Contriever is a recently introduced dual-encoder model that inspired the pretraining task in this work. However, architecturally, NAIL and Contriever are quite different. NAIL\u2019s query representation is not learned and is tied to the fixed set of vocabulary terms; this approach is potentially less powerful than a fully learned dense representation.\nThe summary of the comparison is available in\nTable 8 (Appendix E). We observe that on the BEIR reranking task, NAIL matches both the in-domain and zero-shot performance of the Contriever model, despite lacking a query time neural network. Without using BM25 for initial retrievals, both methods perform slightly worse on nDCG@10 for the zeroshot BEIR tasks, but they remain comparable."
        },
        {
            "heading": "5.5 Performance versus query-time FLOPS",
            "text": "We have motivated this work by asking how much can we leverage large language models at indexing time while making query time computational costs small enough for a commodity CPU. As the results in this section show, there are tradeoffs between reranking improvements and computational costs. To illustrate this tradeoff, we present results of percentage nDGC@10 improvement over BM25 versus query-time FLOPS in Figure 4 (Appendix D). In general, we think lexicalized approaches like NAIL provide an interesting point on this curve, where much higher performance than BM25 can be achieved for only a small amount more compute. Note that Lassance and Clinchant (2022) discuss smaller versions of Splade; see Table 1 for the approximate reduction."
        },
        {
            "heading": "6 Concluding Remarks",
            "text": "We introduce a new model for sparse, lexicalized retrieval, called NAIL that adapts expensive pretrained sequence-to-sequence language models for document indexing. The main elements of NAIL are (1) the use of a non-autoregressive decoder, (2)\nthe use of vocabulary based representation for documents and queries, (3) a self-supervised training approach that is critical for good performance.\nWith NAIL, we focus on offloading all neural computation to indexing time, allowing serving to operate cheaply and without the use of accelerators. Evaluating retrieval on BEIR, we show that the NAIL approach is as effective as recent dualencoder systems and captures up to 86% of the performance gains of a cross-attention model on MS-MARCO while being able to serve requests on commodity CPUs.\nLimitations\nThe work presented in this paper contains several limitations. In this section we focus on limitations relating to (1) the choice of document representation (vocabulary-based vector of weights) and (2) empirical analysis using BEIR suite of datasets.\nAs described in Section 4, we inherit the vocabulary from the T5 models as basis for our document representation. This choice limits the applicability of NAIL in various ways:\n1. The vocabulary is derived from an Englishfocused portion of the web. As a consequence, there are very few non-English word pieces encoded in the vocabulary, such as Unicode and other scripts. We expect this will have a significant, but unknown, impact on applying our system to non-English text.\n2. In order to better support multi-lingual retrieval, we expect that the vocabulary of the model will need to be extended. For example, the multi-lingual T5, (mT5, Xue et al. 2021) contains 250 thousand items, an almost 8-fold increase compared to T5. This work does not study what the impact of vocabulary size increase can be on the quality of document representations and subsequently, on retrieval performance.\n3. Unlike learned dense representations, our vocabulary-based representations may have more limited representational power. Recent work demonstrate that even in the case of learned dense representations, multiple representations can improve model performance (Lee et al., 2023; Zhou and Devlin, 2021). This work also does not evaluate the upper-bound on such vocabulary-based representations.\nWe believe the BEIR suite of datasets presents an improvement over prior text-based retrieval for QA, particularly focusing on a wider range of datasets and in zero-shot setting. Nonetheless, BEIR does not cover some domains in which NAIL may be under-perform. Beyond multi-linguality discussed above, we do not know how our model behaves when needing to reason about numbers or programming, or other domains of text which typically do not tokenize well.\nThis paper demonstrates that NAIL is competitive with other model expensive and complex neural retrieval systems. However, we do not present a highly optimized implementation of NAIL as a standalone retriever. An efficient implementation based on an inverted index is needed before NAIL can be used for end-to-end retrieval in high-traffic applications. Further work in sparsification of document representations (see Appendix C) is not explored in this work and is likely needed to achieve efficient indexing."
        },
        {
            "heading": "A Qualitative Analysis",
            "text": "In this section, we present a qualitative analysis of the tokens that score highest according to the NAIL model for a given input. We choose the Natural Questions (NQ) subset of the BEIR benchmark for this analysis, as the queries tend to be complete questions that are easily interpretable. Table 5 shows the percentage of NAIL\u2019s top predicted tokens that appear in the passage input to the NAIL model along with the gold query that is paired with this passage in the NQ development set. Figure 2 presents the top predicted terms for a randomly sampled set of passages.\nAlmost all of the tokens in both the input passages and the unseen query are present in NAIL\u2019s top 1000 predictions (Table 5). However, tuning towards MS-MARCO significantly increases the number of query tokens predicted in the top 100 and 1000 positions, while simultaneously reducing the number of passage tokens predicted. This is unsurprising: the fine-tuning stage represents a domain shift from the pre-training task, which is predicting document tokens, toward predicting query tokens. One indication of this shift is the increase in the prevalence of \u2019wh\u2019 words (what, who, where) in the top terms from the finetuned model in Figure 2.\nFigure 2 also illustrates some other interesting shifts in NAIL\u2019s output during fine-tuning. For example, in Example (3) the pre-trained model predicts many dates associated with the Eagles (e.g., album release years). These are likely to occur in adjacent passages in the same document as the input passage, so they are good predictions for the pre-training task (Section 4.1). However, they are very unlikely to occur in queries associated with the input passage, and thus they are replaced in the finetuned predictions with terms that are more likely to occur in queries targeting the passage (\u2019sang\u2019, \u2019sing\u2019, \u2019wrote\u2019, \u2019who\u2019, \u2019released\u2019).\nFigure 2 also illustrates NAIL\u2019s ability to predict the type of query that is likely to be paired with a given passage. Passages containing definitions, such as the one presented in Example (1), are\nhighly associated with the wh-word \u2019what\u2019. On the other hand, passages about individuals or groups of individuals (Examples (3) and (4)) are more highly associated with \u2019who\u2019.\nFinally, the predicted terms in Figure 2 contain a lot of small surface-form variations of the same root word, with different segmentations and capitalizations treated separately by the query tokenizer. For example, the tokens \u2019chic\u2019, \u2019chi\u2019, \u2019CHI\u2019, \u2019Ch\u2019, \u2019ch\u2019, \u2019CH\u2019 in Example (2) are all probably coming from different forms of the word \u2019Chicago\u2019 presented in different contexts. This redundancy illustrates a drawback of our featurization: unlike neural models, it does not abstract over diverse surface forms. Future work could examine more efficient and discriminative featurizations than the tokenization used in this work."
        },
        {
            "heading": "B Alternate training recipes",
            "text": "Our primary goal has been to determine the extent to which the performance of an expensive neural network can be captured in a fast, sparse, featurization for general purpose retrieval. Subsequently, we have prioritized a training recipe that is aligned with previous work and well suited to the multidomain BEIR task. However, the performance of learned retrievers as rerankers is very sensitive to the exact nature of the training recipe, and in this section we present analyses of the choices we made, and the associated trade-offs on BEIR and MSMARCO performance.\nB.1 Hard-negative selection for fine-tuning\nOne key choice in contrastive learning is the distribution of negative examples used in Equation 3. This is commonly a combination of hard negatives, which are chosen to be challenging for a single example, and batch negatives, which are drawn from the distribution of all positive and hard-negative candidates across training examples (Karpukhin et al., 2020; Xiong et al., 2020; Qu et al., 2021).\nnDCG@10 Finetuned Pretrained only Pretrained + only only Finetuned\nMSMARCO 0.367 0.212 0.377 BEIR 0.422 0.416 0.465\nTable 7: Effect of pretraining on NAIL for the BEIR reranking task. The BEIR nDCG@10 metric corresponds to average score of datasets excluding MSMARCO.\nOur pretraining task (described in Section 4.1) does not use hard negatives; however, the MSMARCO fine-tuning task includes hard negatives created by running BM25 retrieval over the set of candidate passages. Table 6 shows how BEIR and MS-MARCO results change as we change the number of MS-MARCO hard-negatives that we sample during fine tuning. As this number increases, the MS-MARCO performance also increases until it matches the performance of the cross-attention rerankers in Table 2 when 63 hard negatives are sampled for each training example. However, increasing the number of MS-MARCO hard negatives also hurts BEIR performance.\nB.2 Effects of pretraining and fine-tuning The training recipe, presented in Section 4.1, has two stages beyond the language model training from Raffel et al. (2020). Table 7 shows that both stages benefit both the BEIR and MSMARCO results. However, NAIL still yields a nice improvement over BM25 across the BEIR tasks using only the pre-training task. This is encouraging because these data are heuristically generated rather than relying on human relevance labels, so they can be trivially applied to new domains. The MS-MARCO results are unsurprisingly more dependent on finetuning on MS-MARCO. Pre-trained NAIL does not outperform BM25 on MS-MARCO without finetuning. More sophisticated methods of synthetic training data generation, such as Promptagator(Dai et al., 2022), could also help improve NAIL further, but we leave this to future work."
        },
        {
            "heading": "C Sparsification",
            "text": "To further explore the potential for using NAIL for full retrieval, we experiment with a naive approach to sparsifying NAIL document representations. Specifically, we simply order tokens by their scores and keep the top-k scoring tokens.\nFigure 3 demonstrates the effect on the recall@100 metric of reducing the number of terms per document from the original vocabulary of 32\nthousand tokens down to 100 tokens. For both MSMARCO and other BEIR datasets, recall@100 falls considerably when using only the top 100 tokens. Nonetheless, with only two thousand tokens we are able to maintain the same level of performance for MS-MARCO and roughly 97% of the recall performance on BEIR. This observation, along with the results in Table 3, suggest that NAIL could be used to populate an efficient inverted index for retrieval, with little loss of recall. Such an index could serve as a more powerful alternative to BM25. We leave this to future work."
        },
        {
            "heading": "D Performance versus query-time FLOPS",
            "text": "Figure 4 illustrates different systems with varying tradeoff between computational cost and retrieval performance. See Section 5.5 for the discussion on this figure."
        },
        {
            "heading": "E Comparison to Contriever",
            "text": "Table 8 compares the reranking performance of the Contriever system with NAIL. See Section 5.4 for the discussion on this comparison."
        }
    ],
    "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
    "year": 2023
}