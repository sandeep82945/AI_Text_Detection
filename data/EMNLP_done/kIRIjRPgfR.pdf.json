{
    "abstractText": "Multi-aspect controllable text generation aims to generate fluent sentences that possess multiple desired attributes simultaneously. Traditional methods either require expensive iteration / searching within the discrete text space during the decoding stage, or train separate controllers for each aspect, resulting in a degradation of text quality due to the discrepancy between different aspects. To address these limitations, we introduce a novel approach for Multi-aspect control, namely MacLaSa, that estimates compact Latent space for multiple aspects, and performs efficient Sampling with a fast sampler. To eliminate the domain discrepancies between different aspects, we first utilize a variational autoencoder (VAE) network to map text sequences from various data sources into close latent representations. The estimated latent space enables the formulation of joint energy-based models and the plugging in of arbitrary attribute discriminators to achieve multiaspect control. Afterwards, we draw latent samples with a fast sampler based on ordinary differential equations and feed sampled examples to the VAE decoder to produce target text sequences. Experimental results demonstrate that MacLaSa outperforms strong baselines on both attribute relevance and textual quality while maintaining a high inference speed.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanxing Ding"
        },
        {
            "affiliations": [],
            "name": "Liang Pang"
        },
        {
            "affiliations": [],
            "name": "Zihao Wei"
        },
        {
            "affiliations": [],
            "name": "Huawei Shen"
        },
        {
            "affiliations": [],
            "name": "Xueqi Cheng"
        },
        {
            "affiliations": [],
            "name": "Tat-Seng Chua"
        }
    ],
    "id": "SP:5ca948a7a9a0f47e59fad61708f6cf995a2b7116",
    "references": [
        {
            "authors": [
                "Sumanta Bhattacharyya",
                "Amirmohammad Rooshenas",
                "Subhajit Naskar",
                "Simeng Sun",
                "Mohit Iyyer",
                "Andrew McCallum."
            ],
            "title": "Energy-based reranking: Improving neural machine translation using energybased models",
            "venue": "Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Luke Vilnis",
                "Oriol Vinyals",
                "Andrew M. Dai",
                "Rafal J\u00f3zefowicz",
                "Samy Bengio."
            ],
            "title": "Generating sentences from a continuous space",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL",
            "year": 2016
        },
        {
            "authors": [
                "Alvin Chan",
                "Yew-Soon Ong",
                "Bill Pung",
                "Aston Zhang",
                "Jie Fu."
            ],
            "title": "Cocon: A self-supervised approach for controlled text generation",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Tian Qi Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David Duvenaud."
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Andrea Madotto",
                "Janice Lan",
                "Jane Hung",
                "Eric Frank",
                "Piero Molino",
                "Jason Yosinski",
                "Rosanne Liu."
            ],
            "title": "Plug and play language models: A simple approach to controlled text generation",
            "venue": "8th International Conference on Learning Represen-",
            "year": 2020
        },
        {
            "authors": [
                "Jessica Ficler",
                "Yoav Goldberg."
            ],
            "title": "Controlling linguistic style aspects in neural language generation",
            "venue": "CoRR, abs/1707.02633.",
            "year": 2017
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron C. Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neu-",
            "year": 2014
        },
        {
            "authors": [
                "Yuxuan Gu",
                "Xiaocheng Feng",
                "Sicheng Ma",
                "Jiaming Wu",
                "Heng Gong",
                "Bing Qin."
            ],
            "title": "Improving controllable text generation with position-aware weighted decoding",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland,",
            "year": 2022
        },
        {
            "authors": [
                "Yuxuan Gu",
                "Xiaocheng Feng",
                "Sicheng Ma",
                "Lingyuan Zhang",
                "Heng Gong",
                "Bing Qin."
            ],
            "title": "Controllable text generation via probability density estimation in the latent space",
            "venue": "arXiv preprint arXiv:2212.08307.",
            "year": 2022
        },
        {
            "authors": [
                "Yuxuan Gu",
                "Xiaocheng Feng",
                "Sicheng Ma",
                "Lingyuan Zhang",
                "Heng Gong",
                "Bing Qin."
            ],
            "title": "A distributional lens for multi-aspect controllable text generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Skyler Hallinan",
                "Alisa Liu",
                "Yejin Choi",
                "Maarten Sap."
            ],
            "title": "Detoxifying text with marco: Controllable revision with experts and anti-experts",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Changying Hao",
                "Liang Pang",
                "Yanyan Lan",
                "Yan Wang",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Sketch and customize: A counterfactual story generator",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Zhiting Hu",
                "Li Erran Li."
            ],
            "title": "A causal lens for controllable text generation",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual.",
            "year": 2021
        },
        {
            "authors": [
                "Zhiting Hu",
                "Zichao Yang",
                "Xiaodan Liang",
                "Ruslan Salakhutdinov",
                "Eric P Xing."
            ],
            "title": "Toward controlled generation of text",
            "venue": "International conference on machine learning, pages 1587\u20131596. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Xuancheng Huang",
                "Zijun Liu",
                "Maosong Sun Peng Li",
                "Tao Li",
                "Yang Liu."
            ],
            "title": "An extensible plugand-play method for multi-aspect controllable text generation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R. Varshney",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "CTRL: A conditional transformer language model for controllable generation",
            "venue": "CoRR, abs/1909.05858.",
            "year": 2019
        },
        {
            "authors": [
                "Ben Krause",
                "Akhilesh Deepak Gotmare",
                "Bryan McCann",
                "Nitish Shirish Keskar",
                "Shafiq R. Joty",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "Gedi: Generative discriminator guided sequence generation",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Sachin Kumar",
                "Eric Malmi",
                "Aliaksei Severyn",
                "Yulia Tsvetkov."
            ],
            "title": "Controlled text generation as continuous optimization with multiple constraints",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information",
            "year": 2021
        },
        {
            "authors": [
                "Sachin Kumar",
                "Biswajit Paria",
                "Yulia Tsvetkov."
            ],
            "title": "Constrained sampling from language models via langevin dynamics in embedding spaces",
            "venue": "CoRR, abs/2205.12558.",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Lample",
                "Myle Ott",
                "Alexis Conneau",
                "Ludovic Denoyer",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Phrasebased & neural unsupervised machine translation",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "year": 2018
        },
        {
            "authors": [
                "Chunyuan Li",
                "Xiang Gao",
                "Yuan Li",
                "Baolin Peng",
                "Xiujun Li",
                "Yizhe Zhang",
                "Jianfeng Gao."
            ],
            "title": "Optimus: Organizing sentences via pre-trained modeling of a latent space",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Alisa Liu",
                "Maarten Sap",
                "Ximing Lu",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dexperts: Decoding-time controlled text generation with experts and anti-experts",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Guangyi Liu",
                "Zeyu Feng",
                "Yuan Gao",
                "Zichao Yang",
                "Xiaodan Liang",
                "Junwei Bao",
                "Xiaodong He",
                "Shuguang Cui",
                "Zhen Li",
                "Zhiting Hu."
            ],
            "title": "Composable text controls in latent space with odes",
            "venue": "arXiv preprint arXiv:2208.00638.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Fatemehsadat Mireshghallah",
                "Kartik Goyal",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Mix and match: Learningfree controllable text generationusing energy language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Weili Nie",
                "Arash Vahdat",
                "Anima Anandkumar."
            ],
            "title": "Controllable and compositional generation with latent-space energy-based models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing",
            "year": 2021
        },
        {
            "authors": [
                "Jing Qian",
                "Li Dong",
                "Yelong Shen",
                "Furu Wei",
                "Weizhu Chen."
            ],
            "title": "Controllable natural language generation with contrastive prefixes",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Lianhui Qin",
                "Sean Welleck",
                "Daniel Khashabi",
                "Yejin Choi."
            ],
            "title": "COLD decoding: Energy-based constrained text generation with langevin dynamics",
            "venue": "CoRR.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole."
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "9th International Conference on Learning Representations, ICLR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Fei Xiao",
                "Liang Pang",
                "Yanyan Lan",
                "Yan Wang",
                "Huawei Shen",
                "Xueqi Cheng."
            ],
            "title": "Transductive learning for unsupervised text style transfer",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Yang",
                "Dan Klein."
            ],
            "title": "FUDGE: controlled text generation with future discriminators",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Yang",
                "Dayiheng Liu",
                "Wenqiang Lei",
                "Baosong Yang",
                "Mingfeng Xue",
                "Boxing Chen",
                "Jun Xie."
            ],
            "title": "Tailor: A prompt-based approach to attributebased controlled text generation",
            "venue": "CoRR.",
            "year": 2022
        },
        {
            "authors": [
                "Dian Yu",
                "Zhou Yu",
                "Kenji Sagae."
            ],
            "title": "Attribute alignment: Controlling text generation from pretrained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Hanqing Zhang",
                "Haolin Song",
                "Shaoyu Li",
                "Ming Zhou",
                "Dawei Song."
            ],
            "title": "A survey of controllable text generation using transformer-based pre-trained language models",
            "venue": "CoRR, abs/2201.05337.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Mengjie Zhao",
                "Tao Lin",
                "Fei Mi",
                "Martin Jaggi",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Masking as an efficient alternative to finetuning for pretrained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Attribute-based controllable generation aims to generate text that exhibits desired attributes in certain aspects (Zhang et al., 2022). Early work focused on single-aspect control tasks and involved re-training or fine-tuning language models (LMs) using well-labeled data, which resulted in good performance (Keskar et al., 2019; Chan et al., 2021; Hao et al., 2021; Hu et al., 2017; Ficler and Goldberg, 2017; Xiao et al., 2021). Recent studies focus on a more challenging and practical setting,\n\u2217Corresponding author\nmulti-aspect controllable text generation1 (Kumar et al., 2021; Qian et al., 2022; Qin et al., 2022). For instance, a dialogue system may require the control of emotions, persona, politeness, etc, at the same time. However, training multi-aspect controllers directly is difficult due to the limited availability of sentences with multi-attribute annotations. Thus, recent works focus on training separate single-aspect discriminators or controllers for each aspect and combining them for multi-aspect controllable text generation (Mireshghallah et al., 2022; Qian et al., 2022).\nAs illustrated in Figure 1, recent works on multi-aspect controllable text generation task can be primarily categorized into two types. Firstly,\n1The aspect can be sentiment or topic, and sentiment may have two attributes: positive and negative.\noptimization-based methods either apply extra attribute classifiers to adjust the conditional probability distributions of language model at every generation step (Dathathri et al., 2020; Krause et al., 2021; Yang and Klein, 2021), or regard the decoding process as an optimization objective and search for optimal soft-representations that satisfy multi-objective constraints (Kumar et al., 2021, 2022; Qin et al., 2022; Mireshghallah et al., 2022). However, from a distributional perspective, optimization-based methods often conduct complicated gradient-descent iterations or searching in the distorted text space, and the discrete nature makes it difficult to find high-quality texts, leading to poor linguistic quality and slow inference speeds. Secondly, prefix-based methods are introduced to guide conditional generation using lightweight continuous task-specific vectors (Qian et al., 2022; Yang et al., 2022). They typically train single-aspect prefixes separately and suffer from text quality degeneration when combining them for multi-aspect control due to the mutual interference between multiple prefixes. As depicted in Figure 1, prefix-based methods combine multiple prefixes to obtain the interpolation or average of these distribution centers appraised by prefixes. However, there could be a mismatch between interpolation points and target intersection regions when the distribution centers of different aspects are far away, leading to the degradation of textual fluency. Therefore, an ideal method for multi-aspect controllable generation should enhance controllability and textual quality, while enabling rapid inference speeds.\nIn this paper, we introduce a new technique for multi-aspect controllable text generation, dubbed MacLaSa, which estimates a compact space containing latent representations of various attributes and performs effective sampling using a fast sampler based on ordinary differential equations (ODEs). To eliminate the domain discrepancies between different aspects, we initially employ a VAE encoder network to map attribute-related sentences into latent representations and penalize the distance between each pair of aspect distribution centers. The acquired compact latent space aids in formulating joint latent-space energy-based models (EBMs) and allows us to integrate arbitrary attribute discriminators to satisfy multi-aspect combinations. Subsequently, we utilize an efficient ODE-based sampler (Song et al., 2021; Nie et al., 2021) to draw latent samples possessing desired attributes\nfrom the distribution formed by multiple attribute classifiers. Ultimately, the selected latent vectors are input into a VAE decoder to generate target text sequences. In short, our approach improves controllability and textual quality by estimating a compact latent space to mitigate mutual interference among various aspects, and the fast ODE-based sampler contributes to efficient sampling.\nWe conduct experiments on the multi-aspect control task with two attributes from the sentiment aspect and four attributes from the topic aspect, with datasets IMDb movie reviews (Maas et al., 2011) and AGNews (Zhang et al., 2015), respectively. Experimental results of both automatic and human evaluation demonstrate that our method achieves encouraging improvements in attribute relevance and text quality compared to previous strong baselines. Our work also exhibits significant advantages in inference speed over existing baselines2."
        },
        {
            "heading": "2 Related Work",
            "text": "In this section, we discuss the related work on multi-aspect control. Recent researches on multiaspect can be divided into two types: optimizationbased methods and prefix-based methods.\nOptimization-based Methods Existing efforts on multi-aspect control typically combine many attribute controllers in the decoding stage to bias the language model for desired directions. Weighteddecoding methods focus on decomposing conditional probability through Bayesian factorization into a language model and a classifier (Dathathri et al., 2020; Krause et al., 2021; Yang and Klein, 2021; Liu et al., 2021; Gu et al., 2022a; Hallinan et al., 2023). Other approaches define controllable text generation as a multi-objective optimization problem and find the optimal soft-representation sequences by specific sampling schemes or other gradient-based samplers (Lample et al., 2018; Bhattacharyya et al., 2021; Mireshghallah et al., 2022; Qin et al., 2022; Kumar et al., 2021, 2022). These optimization-based methods often require complicated iteration / search in the high-dimensional text space, leading to slow inference speed.\nPrefix-based Methods Recent work leverages the learned continuous task-specific vectors, which are called prefixes, as a lightweight alternative to guide the language model to generate desired\n2Our code is available at https://github.com/ TrustedLLM/MacLaSa\nattribute text (Li and Liang, 2021; Yu et al., 2021; Zhao et al., 2020; Qian et al., 2022; Yang et al., 2022; Huang et al., 2023). Contrastive Prefixes (Qian et al., 2022) utilize the opposite relationship between different attributes to help to train single-aspect prefixes and combine them for multi-aspect control. Tailor (Yang et al., 2022) provides a multi-aspect prefix mask and a re-indexing position-ids sequence to bridge the gap between single and multi-aspect control. Nevertheless, these learned controllers in prefix-based methods may prefer different language habits, resulting in textual quality degeneration when combining them for multi-aspect control.\nThere is also a line of work that manipulates latent variables in the latent space (Gu et al., 2022c,b; Liu et al., 2022). Gu et al. (2022c) map attributerelated sentences to the latent space and then designs a heuristic searching algorithm to approach intersection regions of the different attributes for generation. Despite their efficiency, they still suffer from the unstable controllability due to the rare intersections of different attributes. LatentOps (Liu et al., 2022) executes composable control operations within the low-dimensional continuous latent space. However, it does not adequately consider the discrepancy between various aspects, resulting in suboptimal performance when controlling multiple attributes simultaneously."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first present the task definition of multi-aspect controllable text generation (\u00a73.1). Next, we describe how to build the compact latent space (\u00a73.2), how to define the joint EBMs on the latent space (\u00a73.3), and how to sample from the EBMs to generate the final results (\u00a73.4).\nThe overall structure of MacLaSa is illustrated in Figure 2. Our approach primarily relies on the variational autoencoder architecture for manipulating latent spaces. To weaken the mutual interference among different aspects, we initially employ the VAE encoder to estimate a continuous lowdimensional latent space, incorporating additional losses to ensure its compactness. Subsequently, we establish joint latent-space energy-based models, which allow us to integrate multiple constraint functions for guiding sophisticated multi-aspect control. Finally, we utilize a fast ODE-based sampler to draw samples from the EBMs and input them into the VAE decoder to generate the desired\nmulti-aspect sequences."
        },
        {
            "heading": "3.1 Task Definition",
            "text": "First, we present the task definition of multi-aspect controllable text generation. Suppose we have N aspects, represented by A = {A1, \u00b7 \u00b7 \u00b7 , AN}, where each aspect An contains |An| attributes, given by {a1n, \u00b7 \u00b7 \u00b7 , a |An| n }. The goal of multi-aspect control is to generate sentences that possess multiple attributes a = {a\u22171, \u00b7 \u00b7 \u00b7 , a\u2217N} simultaneously. For instance, we may expect our model to produce a sentence with attribute a21 (from aspect A1) and attribute a42 (from aspect A2).\nOur training samples are organized and labeled according to their corresponding aspects and attributes. Sjn denotes the index set of sentences with attribute ajn. As a result, we have Sn = \u22c3|An| j=1 S j n, which represents the index set containing all sentences within aspect An. Likewise, S = \u22c3N n=1 Sn signifies the indices encompassing our entire training dataset. We use x to represent an arbitrary sentence and z to indicate its latent representation.\nIt is worth noting that our training corpus contains only single-aspect labeled sentences, making it infeasible to directly train a multi-aspect controllable text generative model."
        },
        {
            "heading": "3.2 Building Latent Space",
            "text": "To estimate a compact, continuous latent space that outlines the latent distribution of interest and facilitates subsequent sampling processes, we utilize a VAE network equipped with pre-trained language models to encode any single-aspect sentence x to its hidden representation z using z = Encoder\u03d5 (x). The encoded latent representations constitute the estimated attribute space.\nWe expect the latent space to be sufficiently compact while ensuring that latent representations from various aspects maintain their semantic meanings. To accomplish this, we propose the following three training objectives:\nELBO Loss LE We adopt the basic Evidence Lower Bound (ELBO) objective to learn a smooth latent space and force the decoder to map any given latent vector z into its original text x: LE = \u2212Eq\u03d5(z|x)[log p\u03b8(x|z)] + KL(q\u03d5(z|x)\u2225pprior (z)),\n(1)\nwhere pprior (z) is a standard Gaussian distribution as the prior, and KL(\u00b7\u2225\u00b7) is the Kullack-Leibler divergency. The first term encourages z to encode more relevant content information for reconstruct-\ning the original text x with the VAE decoder p\u03b8. The KL divergence forces the variational distribution q\u03d5(z|x) to match the prior.\nClassification Loss LC We propose the classification loss LC to force the mapped representations to preserve their original attribute information and help the model to distinguish representations of different attributes from the same aspect. We introduce independent classification layers for each aspect and train them by minimizing the negative log-likelihood of the corresponding attribute ajn:\nLC = \u2212 N\u2211\nn=1 |An|\u2211 j=1 \u2211 i\u2208Sjn log p\u03c0n ( ajn | zi ) , (2)\nwhere p\u03c0n is a classifier that distinguish attributes {a\u2217n} from aspect An with parameter \u03c0n.\nAspect Discrepancy Loss LD To reduce the distribution discrepancy between different aspects, we introduce the aspect discrepancy loss (Gu et al., 2022c) to penalize the distance between distribution centers of each two aspects:\nLD = \u2211\n1\u2264n1<n2\u2264N\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211\ni\u2208Sn1\nzi |Sn1 |\n\u2212 \u2211\nj\u2208Sn2\nzj |Sn2 | \u2225\u2225\u2225\u2225\u2225\u2225 2 , (3)\nwhich calculates the Euclidean distance between two distribution centers. In practice, we use a batchlevel approximation by taking the average representations of each aspect in each mini-batch as the estimated center and calculating the distances to\ncenters of other aspects. Minimizing LD allows the model to reduce the discrepancy between different aspects, and helps to eliminate the mutual interference among them.\nTotally, our learning objective is: L = w1LE + w2LC + w3LD. (4)\nWe update parameters \u03d5, \u03b8 and {\u03c0n} for the encoder, decoder, and the classifier layers."
        },
        {
            "heading": "3.3 Formulating Joint Latent-Space EBMs",
            "text": "In order to satisfy the requirement of controlling multiple attributes simultaneously, we leverage the compositionality of EBMs and formulate the joint distribution for the latent representations and target attribute by incorporating any constraint(e.g., attribute classifiers) into the energy function E(\u00b7).\nTo begin with, we define the following joint distribution on both the latent representation z and desired attributes a as:\np(z,a) := pprior(z)p(a|z) = pprior(z) \u00b7 e\u2212E(a|z)/Z, (5) where Z = \u222b e\u2212E(a|z)dz is the normalization term, pprior(z) is the Gaussian prior distribution, and p(a|z) follows a Boltzmann distribution. In this work, We assume the target attributes are independent with each others. We then formulate E(a|z) as the energy-based models that can combine arbitrary attribute classifiers based on our needs:\nE(a|z) = N\u2211\nn=1\n\u03bbnEn (a \u2217 n|z) . (6)\n\u03bbn \u2208 R is the balanced weight to balance the performance among attributes from different aspects. The energy function En (a\u2217n|z) is defined as the negative log probability of target attribute ajn:\nEn (a \u2217 n|z) = \u2212fn(z) [ ajn ] + log \u2211 k exp ( fn(z) [ akn ]) ,\n(7)\nwhere fn(z) is the multi-class attribute classifier trained on the frozen latent space, and fn(z)[a\u2217n] is the output unnormalized logits for attribute a\u2217n. After the training of VAE, we fix the entire VAE encoder and map the input text with attribute annotations into the latent space, then ask the classifier to predict target attribute label given the latent vector. Training attribute classifiers fn(z) in the frozen low-dimensional latent space is efficient, which enables us to plug in different attribute classifiers to guide complex multi-aspect control."
        },
        {
            "heading": "3.4 Sampling from EBMs with ODE",
            "text": "After the acquisition of the joint distribution p(z,a), we would like to draw latent representations z given the target attribute values a. To ensure high-quality and efficient sampling, we adopt a fast ODE-based sampler to draw samples from the energy based models.\nPrior work (Song et al., 2021) shows that controllable generation p(x|a) can be achieved by solving the following ordinary differential equation (ODE):\ndx = \u22121 2 \u03b2(t) [x+\u2207x log pt(x,a)] dt, (8)\nwhere \u03b2(t) is a time-variant diffusion coefficient that has the form \u03b2(t) = \u03b2min + (\u03b2max \u2212 \u03b2min) t. t is the timestep from T to 0, and pt(x,a) denotes the join distribution of data and attribute at time t.\nIn our work, we adapt the ODE from Eq.(8) into the low-dimensional latent space, which gives:\ndz = \u22121 2 \u03b2(t) [z +\u2207z log pt(z,a)] dt\n= \u22121 2 \u03b2(t) [z \u2212\u2207zEt(a|z) +\u2207z log pt(z)] dt.\n(9)\nNote that pt(z) = N (0, I) is time-invariant for t \u2208 [0, T ]. Since the classifier fn(z) in Eq.(7) is fixed, Et(a|z) is also time-invariant and we have Et(a|z) = E(a|z). The above ODE becomes:\ndz = \u22121 2 \u03b2(t)\n[ z \u2212\u2207zE(a|z)\u2212 1\n2 \u2207z\u2225z\u222522\n] dt\n= 1\n2 \u03b2(t)\u2207zE (a|z) dt\n= 1\n2 \u03b2(t) \u2211 n \u2207z\u03bbnEn (a\u2217n|z) dt.\n(10)\nNow we can easily sample latent samples by\ndrawing z(T ) \u223c N (0, I) and solving the Eq.(10) with a differential neural ODE solver3 (Chen et al., 2018) to obtain z(0). Then z(0) is fed to the VAE decoder p\u03b8 to produce target text sequences that possess multiple attributes simultaneously.\nTo narrow the inevitable gap between the prior distribution pprior (z) and the learned VAE posterior q\u03d5(z|x) on Z , following previous work (Li et al., 2020; Hu and Li, 2021; Liu et al., 2022), we fit a simple single-layer generative adversarial network (GAN) (Goodfellow et al., 2014), pGAN(z), on the learned latent space and draw z(T ) from pGAN(z). We study the impact of pGAN in \u00a74.5."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we demonstrate the effectiveness of our proposed MacLaSa in the multi-aspect control setting through both automatic and human evaluations. Additionally, we provide further analysis and visualization on efficiency, and case studies."
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "Datasets We conduct experiments for controlling two aspects: sentiment and topic, simultaneously. We adopt the IMDb movie reviews (positive and negative) (Maas et al., 2011) for sentiment control and AGNews dataset (World, Sports, Business and Sci./Tech) (Zhang et al., 2015) for topic control. Following previous work (Qian et al., 2022; Gu et al., 2022c), we randomly sample 20k sentences from each dataset for each attribute to train our method. For evaluation, consistent with previous work (Dathathri et al., 2020; Krause et al., 2021; Yang and Klein, 2021; Liu et al., 2021; Gu et al., 2022a), we choose the same 15 attribute-unrelated prompts and ask the model to complete 50 sentences with the desired attributes starting with each prompt.\nMacLaSa Settings For the proposed MacLaSa, we employ BERT-base and GPT-2 medium to initialize the encoder and decoder networks in VAE, respectively. The dimension of the latent space is 128. We also apply a cyclical schedule for KL weight and a KL thresholding scheme to alleviate the notorious KL vanishing issue (Bowman et al., 2016). During the training stage, we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a learning rate of 8e-5. The number of training epochs is 50. We also randomly select 10k / 1k\n3https://github.com/rtqichen/torchdiffeq\nexamples to train / validate attributes classifiers in the latent-space EBMs. In our experiments, w1, w2 and w3 are set to 1. During the inference stage, we set \u03b2min = 0.1 and \u03b2max = 20 for the time-variant diffusion coefficient \u03b2t. We also manually tune the weight \u03bbn of different attributes to balance them. All experiments are conducted on a single NVIDIA V100 32GB GPU."
        },
        {
            "heading": "4.2 Baseline Models",
            "text": "We compare with three types of baseline models: (1) optimization-based methods: PPLM (Dathathri et al., 2020) back-propagates gradients of extra attribute classifiers to guide conditional generation at every decoding step. DEXPERTS (Liu et al., 2021) reweights the predictions of language models based on expert (and anti-expert) opinions for effective attribute control. MaRCo (Hallinan et al., 2023) achieves controllable generation using likelihoods under a expert LM and a anti-expert LM to find candidate words to mask and replace. Mix&Match (Mireshghallah et al., 2022) uses a Metropolis-Hastings sampling scheme to draw samplers from an energy-based model that combines multiple attribute discriminators. (2) Prefix-based methods: Contrastive Prefixes (abbreviated as Contrastive) (Qian et al., 2022) trains prefixes for each aspect while the combination of them can achieve multi-aspect control. We also compare with recent approaches that manipulate the latent space, including: LatentOps (Liu et al., 2022) performs composable text operations in the low-dimensional latent space, and Distribution (Gu et al., 2022c) searches for the intersection areas of multiple attribute distributions for generation."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "Automatic Evaluations We adopt three automatic evaluations metrics to measure the performance on the two-aspect control task. Correctness evaluates the success rate of controlling the two aspects simultaneously. We finetune two RoBERTaLarge (Liu et al., 2019) discriminators on the IMDb dataset for sentiment aspect, and the AGNews dataset for topic aspect. We use the two attribute discriminators to compute the fraction of sentences that contain pre-specified attributes. Perplexity (PPL) is an automatic metric of text fluency. We feed generated test sentences to a GPT2-Large model and report the perplexity score. Distinctness (Li et al., 2016) is a n-gram-based metric for evaluating textual diversity, we report Distinct-1\nand Distinct-2 in our paper.\nHuman Evaluations In addition to automatic evaluations, we conduct human evaluations to compare our method\u2019s performance with that of the baseline models. We enlist four annotators with high-level language skills to carry out the human evaluation. Annotators are instructed to assess attribute relevance, fluency, and diversity on a scale of 1-5, with 1 denoting \"very low\" and 5 representing \"very high.\" Moreover, we direct the annotators not to consider linguistic quality when evaluating attribute alignment and vice versa. We randomly select 800 generated sentences (100 for each combination) and shuffle them for evaluation with each method. The scores are then averaged to derive the final human evaluation results."
        },
        {
            "heading": "4.4 Main Results",
            "text": "Automatic Evaluations We conduct experiments in the two-aspect control setting and compare our method with several strong baselines. The results of automatic evaluation are depicted in Table 1. We calculate the average correctness scores of eight attribute combinations as the final results for each method. We also report the standard deviations, which stand for the stability of models among different runs. Moreover, we assess the average inference time required to generate a single sentence for each method.\nWe note that existing baselines excel in individual evaluation metrics but struggle to concurrently achieve good controllability and superior linguistic quality, which is essential for multi-aspect control. PPLM and MaRCo can generate fluent sentences but fall short in attribute accuracy. In contrast, Mix&Match demonstrates strong attribute controllability, yet the text quality is subpar. Moreover, optimization-based methods, including PPLM and Mix&Match, exhibit severe slow inference speeds due to their complex iterations or searching in the high-dimensional text space. The Contrastive method attains a high correctness score in multiaspect control by training separate continuous prefix vectors for each aspect. However, the mutual interference of different prefixes results in diminished text quality. LatentOps has average performance over baseline models. The Distribution method generates highly fluent texts with good attribute correctness scores but lacks textual diversity.\nMacLaSa showcases a notable performance boost in average correctness scores, achieving an\n11.62% improvement compared to the strongest baseline. This result highlights our superiority in multi-aspect controllability. Additionally, MacLaSa displays good linguistic quality compared to previous method, emphasizing the benefits of learning a compact latent space. Our approach also exhibits substantial advantages in generation efficiency. Compared to the parameterefficient prefix-based Contrastive method, our method demonstrates a remarkable 5.9\u00d7 faster in inference speeds. In summary, MacLaSa surpasses existing baselines in attribute correctness and textual quality while keeping high inference speeds.\nHuman Evaluations The human evaluation results for the multi-aspect control task can be found in Table 2. The inter-annotator agreement is 0.32 in Fleiss\u2019 \u03ba, indicating a fair agreement. Generally, the human judgment on attribute correctness aligns well with the results of the automatic evaluation. Our method excels in attribute control, achieving a correctness score of 3.54. Contrary to the automatic\nevaluation results, annotators favor our approach as it delivers the highest text quality among the baselines. Overall, our model demonstrates superior performance in both attribute correctness and textual quality.\nBoth automatic and human evaluations demonstrate that our proposed MacLaSa outperforms other baseline models in terms of attribute correctness and linguistic quality, while maintaining a high inference speed."
        },
        {
            "heading": "4.5 Analysis",
            "text": "Effects of VAE Losses We conduct an ablation study to verify the effects of the classification loss LC and aspect discrepancy loss LD. The results are shown in Table 1. Removing LC causes the latent space to collapse completely. The correctness scores drop drastically as the model can hardly distinguish between representations of different attributes within the same aspect. Removing LD degrades attribute correctness since we cannot alleviate domain gaps between different data sources. Interestingly, without LD, the distance between sam-\nple points from different aspects increases, leading our model to generate sentences mapped from sparser regions. This results in a minor decrease in fluency while slightly increasing diversity.\nEffects of Samplers To demonstrate the superiority of our ODE-based sampler, we compare it with other standard samplers. For fair comparison, we fix the parameters of VAE and choose different samplers for multi-aspect control text generation. We first implement a random sampler by directly drawing samples from the latent space using pGAN(described in \u00a73.4). We also compared it with a gradient-based sampler using Langevin Dynamics (Kumar et al., 2022; Qin et al., 2022). The automatic evaluation results are shown in Table 3. Random sampling directly from the latent space can only generate representations with single attributes, highlighting the necessity of using a specific sampler. While the LD-based sampler can generate high-quality sentences, it sacrifices attribute alignment, resulting in low attribute relevance. This may be because LD is sensitive and unrobust to hyperparameters (Nie et al., 2021). In contrast, our ODE-based sampler outperforms LD in terms of attribute alignment and textual diversity.\nTo investigate the impact of pGAN, we conduct experiments by removing the GAN network and directly drawing latent representations from the standard Gaussian distribution N (0, I). As shown in Table 3, without the GAN, our model cannot accurately estimate the attribute space, resulting in decreased attribute relevance and textual quality.\nVisualization of Latent Space To provide an intuitive impression of the estimated latent space,\nwe use the t-SNE technique (Van der Maaten and Hinton, 2008) to visualize part of our estimated latent space with four attributes: positive, negative, world and sports in Figure 3. As shown, (1) attribute distributions within the same aspect are well separated due to the classification loss LC that helps our model distinguish mutually exclusive attributes. (2) The distribution centers of sentiment and topic aspects are close to each other because we introduced LD to penalize the distance between them to eliminate domain gaps, which helps generating high-quality multi-aspect sentences. We also notice that the combination of negative-world is tighter than that of negative-sports because world news often covers negative events such as war, disease, and famine. This observation aligns with our experimental results in Appendix A."
        },
        {
            "heading": "4.6 Case Study",
            "text": "To better understand the benefits of learning a compact latent space for generative models, we randomly present generated examples in Table 4. When generating sentences with the attribute combination negative and sports, the Contrastive method can generate attribute-related words like \"tragic\" and \"NBA\"; however, the semantic coherence of the sentences is insufficient. This observa-\ntion is consistent with the results of both automatic and human evaluations (see \u00a7 4.4). One possible explanation is that the prefixes used for sentiment and topic control are trained independently, causing the two learned prefixes to exhibit different language habits and leading to incoherent expressions when combined for multi-aspect control. Conversely, the Distribution method can generate fluent sentences that display multiple attributes but struggles with varying expressions. For instance, Distribution tends to use the word \"bad\" to convey negative emotions, and its sentence structure is often repetitive, such as \"The [noun] was bad in the first round of \". Our proposed MacLaSa can generate numerous attribute-related content, such as \"there is little hope\" and \"world championship leader\", in a fluent manner. By minimizing the discrepancy between sentiment and topic representations in the latent space, we merge high-quality representations related to attribute information, resulting in more coherent expression."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this study, we introduce a novel method, namely MacLaSa, for multi-aspect controllable text generation that estimates a compact, low-dimensional latent space and employs a fast ODE-based sampler for efficient sampling. Our experiments on the two-aspect control task demonstrate the effectiveness and efficiency of our approach. Additionally, we carry out in-depth analytical experiments to emphasize the impact of each module and visualize the estimated latent space. In the future, we aim to expand our work by incorporating arbitrary attribute discriminators into the diffusion process using a plug-and-play approach. Furthermore, we plan to explore more powerful models to enhance the linguistic quality of generated sentences.\nLimitations\nOne of the limitations of the current MacLaSa approach is that when a new aspect or attribute is introduced, the entire VAE framework needs to be retrained to accommodate the unseen attributes. This retraining process can often be time-consuming and computationally expensive, posing a significant challenge in dynamic environments where new aspects may frequently emerge.\nMoreover, due to the notorious KL vanishing issue, the training process of the VAE framework is not stable and requires a significant amount of\nskill and experience to address. The KL vanishing problem refers to the situation where, during the training process, the KL divergence term may approach zero. This can lead to a poorly constrained latent space, resulting in the model generating samples that lack diversity and are not representative of the true data distribution. To tackle this issue, we adopt several techniques, which are described in \u00a7 4.1.\nEthics Statement\nWe honor and support the EMNLP code of Ethics. The paper focuses on controlled text generation, which aims to generate text with desired aspects. We recognize that controlled text generation may be misused to generate harmful texts, e.g., fake news. However, our method can also help to eliminate toxic information in pre-trained language models by introducing specific attribute classifiers. Overall, it is meaningful to continue research into this work based on predecessors. Besides, the datasets used in this paper are all from previously published work and do not involve privacy or ethical issues."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key R&D Program of China (2022YFB3103700, 2022YFB3103704), the National Natural Science Foundation of China (NSFC) under Grants No. 62276248, and the Youth Innovation Promotion Association CAS under Grants No. 2023111. Liang Pang is also supported by Beijing Academy of Artificial Intelligence (BAAI)."
        },
        {
            "heading": "A Detailed Results of Multi-aspect Control",
            "text": "We exhibit the detailed results of eight combinations (two sentiment attributes \u00d7 four topic attributes) on multi-aspect control in Table 5. We compare with three types of baselines: (1) optimization-based methods, PPLM and Mix&Match. (2) prefix-based method Contrastive, and (3) methods that manipulate the latent space, for example, LatentOps and the Distribution method. For automatic evaluation metrics, we finetune two RoBERTa-Large (Liu et al., 2019) discriminators to assess the attribute accuracy scores for both sentiment and topic aspects simultaneously. Perplexity (PPL) is employed to gauge the linguistic quality of the generated sentences. Additionally, we compute the Distinctness score to appraise the textual diversity, reporting Distinct-1 and Distinct2 in our paper. We also report the standard deviations, which stand for the stability of models among different runs.\nWe observe that PPLM demonstrates strong controllability in specific combinations, such as the Positive-Sci./Tech pairing. However, the performance of each combination varies significantly, resulting in subpar average results. This phenomenon also exists for DEXPERTS and MaRCo. While Mix&Match and the Contrastive method excel at attribute alignment, their linguistic quality leaves much to be desired. We postulate that this is due to Mix&Match employing a Metropolis-Hastings sampling scheme for high-dimensional text space sampling, which is hindered by the discrete nature of text space and prevents smooth text generation. The Contrastive method posits that contrasting relationships between individual attributes within each aspect aid in training attribute controllers, but it neglects the differences between aspects, compromising overall textual quality. Regarding the two latent space manipulation methods, LatentOps exhibits moderate performance in both attribute relevance and textual quality, while the Distribution method generates fluent sentences with the desired attributes but lacks diversity.\nOur method attains a remarkable average accuracy of 59.18% across the eight combinations, boasting a 11.62% improvement compared to the most powerful baseline and showcasing the exceptional controllability of our approach. Additionally, our technique excels in both linguistic quality and textual diversity. MacLaSa delivers well-rounded\nperformance concerning attribute alignment, linguistic quality, and diversity. We also evaluate the inference speed for sentence generation, with the results displayed in Table 1. The experimental findings indicate that MacLaSa maintains a high inference speed as well."
        },
        {
            "heading": "B Distribution of Attribute Space",
            "text": "In Figure 4, we employ the t-SNE technique to project hidden representations from four attributes into 2D for visualization: positive, negative, business, and sci./tech. This offers insight into a portion of the estimated latent space. We observe that, on one hand, the two sentiment attributes are distinctly separated due to the classification loss LC , which also applies to the topic aspects. Conversely, the distribution centers of the two aspects are situated closely together, as a result of the aspect discrepancy loss penalty LD. Overall, the observed attribute space distribution aligns with our expectations."
        }
    ],
    "title": "MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space",
    "year": 2023
}