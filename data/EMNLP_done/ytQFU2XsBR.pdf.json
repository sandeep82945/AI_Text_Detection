{
    "abstractText": "Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to selfconsistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%.1",
    "authors": [
        {
            "affiliations": [],
            "name": "James Xu Zhao"
        },
        {
            "affiliations": [],
            "name": "Yuxi Xie"
        },
        {
            "affiliations": [],
            "name": "Kenji Kawaguchi"
        },
        {
            "affiliations": [],
            "name": "Junxian He"
        },
        {
            "affiliations": [],
            "name": "Michael Qizhe Xie"
        }
    ],
    "id": "SP:47435963cb631e3cb4fd004b829b2b38b00e0f62",
    "references": [
        {
            "authors": [
                "Leo Breiman."
            ],
            "title": "Bagging predictors",
            "venue": "Machine learning, 24:123\u2013140.",
            "year": 1996
        },
        {
            "authors": [
                "Leo Breiman."
            ],
            "title": "Random forests",
            "venue": "Machine learning, 45:5\u201332.",
            "year": 2001
        },
        {
            "authors": [
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "ArXiv, abs/2107.03374.",
            "year": 2021
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin."
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013 794.",
            "year": 2016
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen."
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "ArXiv, abs/2211.12588.",
            "year": 2022
        },
        {
            "authors": [
                "Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "ArXiv, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "ArXiv, abs/2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins."
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "ArXiv, abs/2205.09712.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805.",
            "year": 2019
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire."
            ],
            "title": "A decisiontheoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences, 55(1):119\u2013139.",
            "year": 1997
        },
        {
            "authors": [
                "Yao Fu",
                "Hao-Chun Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot."
            ],
            "title": "Complexity-based prompting for multi-step reasoning",
            "venue": "ArXiv, abs/2210.00720.",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Pal: Program-aided language models",
            "venue": "ArXiv, abs/2211.10435.",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International Conference on Machine Learning.",
            "year": 2017
        },
        {
            "authors": [
                "Tin Kam Ho."
            ],
            "title": "Random decision forests",
            "venue": "Proceedings of 3rd international conference on document analysis and recognition, volume 1, pages 278\u2013 282. IEEE.",
            "year": 1995
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "J. Araki",
                "Haibo Ding",
                "Graham Neubig."
            ],
            "title": "How can we know when language models know? on the calibration of language models for question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 9:962\u2013977.",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Mann",
                "Sam McCandlish",
                "Christopher Olah",
                "Jared Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "ArXiv, abs/2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Seyed Mehran Kazemi",
                "Najoung Kim",
                "Deepti Bhatia",
                "Xinyuan Xu",
                "Deepak Ramachandran."
            ],
            "title": "Lambada: Backward chaining for automated reasoning in natural language",
            "venue": "ArXiv, abs/2212.13894.",
            "year": 2022
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "Tie-Yan Liu."
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "ArXiv, abs/2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Rik Koncel-Kedziorski",
                "Subhro Roy",
                "Aida Amini",
                "Nate Kushman",
                "Hannaneh Hajishirzi."
            ],
            "title": "Mawps: A math word problem repository",
            "venue": "North American Chapter of the Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Clark."
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "ArXiv, abs/2303.17651.",
            "year": 2023
        },
        {
            "authors": [
                "Shen-Yun Miao",
                "Chao-Chun Liang",
                "Keh-Yih Su."
            ],
            "title": "A diverse corpus for evaluating and developing english math word problem solvers",
            "venue": "ArXiv, abs/2106.15772.",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Arkil Patel",
                "S. Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are nlp models really able to solve simple math word problems? In North American Chapter of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Debjit Paul",
                "Mete Ismayilzada",
                "Maxime Peyrard",
                "Beatriz Borges",
                "Antoine Bosselut",
                "Robert West",
                "Boi Faltings."
            ],
            "title": "Refiner: Reasoning feedback on intermediate representations",
            "venue": "ArXiv, abs/2304.01904.",
            "year": 2023
        },
        {
            "authors": [
                "Noah Shinn",
                "Beck Labash",
                "Ashwin Gopinath."
            ],
            "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "venue": "ArXiv, abs/2303.11366.",
            "year": 2023
        },
        {
            "authors": [
                "Yasaman Bahri",
                "Ye Ji Choi",
                "Yichi Yang",
                "Yiding Hao",
                "Yifu Chen",
                "Yonatan Belinkov",
                "Yu Hou",
                "Yuntao Bai",
                "Zachary Seid",
                "Zhao Xinran",
                "Zhuoye Zhao",
                "Zi Fu Wang",
                "Zijie J. Wang",
                "Zirui Wang",
                "Ziyi Wu",
                "Sahib Singh",
                "Uri Shaham"
            ],
            "title": "Beyond the imi",
            "year": 2022
        },
        {
            "authors": [
                "lien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Uesato",
                "Nate Kushman",
                "Ramana Kumar",
                "Francis Song",
                "Noah Siegel",
                "L. Wang",
                "Antonia Creswell",
                "Geoffrey Irving",
                "Irina Higgins."
            ],
            "title": "Solving math word problems with process- and outcomebased feedback",
            "venue": "ArXiv, abs/2211.14275.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Huai hsin Chi",
                "Denny Zhou."
            ],
            "title": "Rationale-augmented ensembles in language models",
            "venue": "ArXiv, abs/2207.00747.",
            "year": 2022
        },
        {
            "authors": [
                "Yidong Wang",
                "Zhuohao Yu",
                "Zhengran Zeng",
                "Linyi Yang",
                "Cunxiang Wang",
                "Hao Chen",
                "Chaoya Jiang",
                "Rui Xie",
                "Jindong Wang",
                "Xingxu Xie",
                "Wei Ye",
                "Shi-Bo Zhang",
                "Yue Zhang"
            ],
            "title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Huai hsin Chi",
                "F. Xia",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "ArXiv, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yuxi Xie",
                "Kenji Kawaguchi",
                "Yiran Zhao",
                "Xu Zhao",
                "MinYen Kan",
                "Junxian He",
                "Qizhe Xie."
            ],
            "title": "Decomposition enhances reasoning via self-evaluation guided decoding",
            "venue": "ArXiv, abs/2305.00633.",
            "year": 2023
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley."
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "venue": "ArXiv, abs/2304.01196.",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L. Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "ArXiv, abs/2305.10601.",
            "year": 2023
        },
        {
            "authors": [
                "Tony Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning.",
            "year": 2021
        },
        {
            "authors": [
                "Chuanyang Zheng",
                "Zhengying Liu",
                "Enze Xie",
                "Zhenguo Li",
                "Yu Li."
            ],
            "title": "Progressive-hint prompting improves reasoning in large language models",
            "venue": "ArXiv, abs/2304.09797.",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric P. Xing",
                "Haotong Zhang",
                "Joseph Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot",
            "year": 2023
        },
        {
            "authors": [
                "Aojun Zhou",
                "Ke Wang",
                "Zimu Lu",
                "Weikang Shi",
                "Sichun Luo",
                "Zipeng Qin",
                "Shaoqing Lu",
                "Anya Jia",
                "Linqi Song",
                "Mingjie Zhan",
                "Hongsheng Li."
            ],
            "title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Scharli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Huai hsin Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "ArXiv,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have made impressive progresses in numerous fields (Devlin et al., 2019; Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Bubeck et al., 2023; Wei et al., 2022a) and are often powerful enough to solve problems through a single unified method. While convenient, this approach tends to ignore the distinct structures and variations among the problems, which would benefit from using different methods. On the other hand, in human society, individuals with different skill sets excel in various roles, leading to a thriving world as a whole.\nIn the case of reasoning, Chain-of-Thought (CoT) (Wei et al., 2022b) and Program-Aided Language Models (PAL) (Gao et al., 2022; Chen et al.,\n1Our code, data and prompts are available at https:// github.com/XuZhao0/Model-Selection-Reasoning\n2022) have emerged as two effective methods that offer different strengths and weaknesses. Essentially, CoT decomposes a reasoning problem into a series of intermediate steps using natural language, making it more general, flexible, and comprehensible. On the other hand, PAL offers solutions via Python functions, with its step-by-step programming code ensuring a more rigorous and structured methodology. The external Python interpreter further guarantees the computation precision. Intuitively, combining the strengths of both models and selecting between them based on their solutions to a specific problem would lead to enhanced performance. However, without access to the ground truth, choosing a better method itself becomes a machine learning problem.\nIn order to select among multiple solutions, previous studies have suggested training a ranker (Uesato et al., 2022). While training a dedicated model generally results in improved accuracy, it can also be somewhat cumbersome and entail significant costs. Conversely, large language models (LLMs) have demonstrated good calibration and have been used to assess the accuracy of their own outputs (Guo et al., 2017; Shinn et al., 2023; Xie et al., 2023). In light of this, we propose leveraging the in-context learning capabilities of LLMs for model selection. We direct LLMs to choose between two distinct reasoning models and elucidate their selection rationale.\nWe present a detailed theoretical analysis to validate our approach. The analysis highlights two primary factors impacting the effectiveness of our method: (1) the significance of the difference between the two models, i.e. the distinct distributions of their respective probabilities of correctly solving problems, and (2) the probability of selecting the correct model. Particularly, a higher overall performance can be attained when there is a substantial difference between the models being considered, and when there is a high probability of the correct\nmodel being selected. Furthermore, our analysis affirms that even without an exceptional model selector, we can still achieve improvement in certain cases. This reinforces our decision to simply employ an LLM for model selection.\nWe evaluate our method across eight reasoning tasks, with CoT and PAL serving as the baseline methods. Our method consistently attains performance improvement, when employing Codex (Chen et al., 2021), ChatGPT, and GPT4 as backbone LLMs. Its broad applicability is further underscored by performance gains with open-source Llama 2 models (Touvron et al., 2023). In the context of multiple sample aggregations, our approach significantly reduces the computation costs while achieving notable performance enhancements. Moreover, our approach attains new state-of-the-art accuracies of 96.8% and 93.7% on GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021), respectively."
        },
        {
            "heading": "2 Automatic Model Selection with Large Language Models",
            "text": "In this study, we examine reasoning tasks using two baseline models: CoT and PAL. To tackle complex reasoning tasks, CoT leverages an LLM to generate several intermediate reasoning steps before arriving at a final answer. Due to its reliance on natural language, the reasoning steps are clearly ex-\nplained. Furthermore, natural language supports a broad range of reasoning that may involve common sense and confidence in the reasoning steps. But it might struggle with intricate logical relationships. In contrast, PAL takes a structured and accurate approach, breaking down reasoning problems using Python code. However, the deterministic program format of PAL constrains its generalizability and flexibility."
        },
        {
            "heading": "2.1 Case Study on the Differences between CoT and PAL",
            "text": "To delve deeper into the distinct strengths of CoT and PAL, we analyze 100 instances from GSM8K where CoT and PAL yield different results: 50 cases where CoT is correct and PAL is wrong, and\nanother 50 where the opposite is true. All the CoT and PAL solutions are generated using ChatGPT. Then we divide the error reasons into 5 categories. Examples of each category can be found in Appendix E.\n\u2022 Computation Precision: The ability to perform accurate numerical computations.\n\u2022 Logical Decomposition: Breaking down complex problems into manageable parts and handling logical relationships well.\n\u2022 Problem-Solving Flexibility: The adaptability in addressing problems using different reasoning paradigms, such as forward and backward reasoning.\n\u2022 Semantic Understanding: Grasping and interpreting the problem accurately without overlooking crucial information.\n\u2022 Others: This encompasses miscellaneous error reasons such as redundant calculations and ambiguous questions.\nFigure 2 depicts the error distribution across these categories. It reveals notable differences between CoT and PAL. PAL, with its external Python interpreter, ensures computation accuracy. It also excels in logical decomposition, effectively breaking down problems and employing variables to map logical relationships. On the other hand, CoT is more versatile and flexible, allowing for both forward and backward logical reasoning, while PAL is less adept at backward reasoning scenarios. CoT\u2019s strength also lies in its superior grasp of natural language, aiding in semantic understanding. This analysis highlights the distinct proficiencies of both models in reasoning, reinforcing our hypothesis."
        },
        {
            "heading": "2.2 Method",
            "text": "Given the distinct advantages of CoT and PAL, it would be beneficial to combine the strengths of both. We propose a systematic approach that combines these models through model selection. Specifically, it contains two stages: solution generation and model selection.\nFor a given reasoning question Q, an LLM is prompted to generate reasoning chains for CoT, symbolized as CCoT along with its answer ACoT . Simultaneously, the LLM is prompted to derive reasoning chains for PAL, denoted as CPAL. Employing a Python interpreter, we then get its\nanswer APAL. When ACoT and APAL yield different results, we move to the model selection phase. The LLM is prompted again, provided with (Q,CCoT , ACoT , CPAL), and tasked to select the better method, alongside generating a brief explanation. Specifically for Llama 2 selection prompts, the LLM is prompted with (Q,CCoT , ACoT , CPAL, APAL) pairs. This selection result, together with the explanation, is represented as S. Ultimately, based on S, the final answer is derived as Afinal \u2208 {ACoT , APAL}.\nDuring the selection phase, LLMs leverage their in-context learning abilities for model selection, i.e., we present the LLM with a few in-context exemplar pairs (Q,CCoT , ACoT , CPAL, S). An example of the proposed method is illustrated in Figure 1, excluding the few-shot examples to save space. Specifically, we provide the LLM with two distinct reasoning solutions in a multiple-choice question format. We expect the LLM to identify errors or gaps within the chains and determine the correct method. Note that there are very few instances where the LLM fails to make a choice; in these cases, we randomly select a method. The effect and qualitative analysis of the generated explanation can be found in Appendix C.\nOwing to the in-context learning capabilities of LLMs, we find that they exhibit reasonable accuracy in selecting the appropriate method. Furthermore, as our subsequent theoretical analysis in Section 2.3 reveals, a highly accurate method selection is not actually necessary for the algorithm to perform well. Our empirical findings corroborate this, showing that even if there are situations where the model selection is poor, the overall improvement remains substantial."
        },
        {
            "heading": "2.3 Theoretical Analysis",
            "text": "In this section, we conduct a theoretical analysis to determine under which condition the proposed method could work (and fail).\nQuantifying error rates Let us denote the error rates of the two base methods, m1 and m2, by err1 and err2, respectively. Without loss of generality, let m1 be a better base method in the overall performance: i.e., err1 \u2264 err2. For a given question x, we define \u03c1x as the probability of choosing a more accurate method, either m1 or m2, for the given x using the proposed approach. Define R(x) = p(correct | x,m2) \u2212 p(correct | x,m1) where p(correct | x,mi) represents the probabil-\nity of outputting a correct prediction given input x with method mi. Then we can quantify the final error rate err of the proposed method as follows: Proposition 1. For any methods m1 and m2 with any combining probability function \u03c1x,\nerr = err1 \u2212 Ex [|R(x)| (\u03c1x \u2212 1{R(x) < 0})]\nWe refer readers to Appendix B for the full proof. Proposition 1 decomposes the possible improvement (or deterioration) over base methods in terms of R(x) and \u03c1x. It quantitatively shows when and how we can expect improvement (or deterioration) based on these two factors. For example, to improve over the best base method m1, Proposition 1 suggests us to choose another base method m2 such that |R(x)| is not too small and \u03c1x is high when |R(x)| is large. In other words, it discourages us from choosing too similar methods as base methods, because for similar methods, |R(x)| tends to be small and it is challenging to increase \u03c1x even when |R(x)| is large due to the similarity. This provides a theoretical motivation for us to use CoT and PAL, instead of combining CoT with another CoT.\nOn the accuracy of selection Define \u03c1 to be the overall probability of selecting a better method: \u03c1 = Ex[\u03c1x]. Theorem 1 shows that \u03c1 can be much worse than that of a random guess to achieve the improvement over the base methods m1 and m2; i.e., err < err1 and err1 \u2264 err2 can happen with \u03c1 < 0.5: Theorem 1. For any \u03f5 > 0, there exist data distribution over x, two base methods (m1,m2), and combining probability (\u03c1x) such that err < err1, err1 \u2264 err2, and \u03c1 < \u03f5.\nWe provide a stronger version of Theorem 1 in Appendix A (that implies Theorem 1) and its proof in Appendix B.\nTheorem 1 supports our proposition that, despite not training a new model for the selection process and with the in-context learning limited to a few-shot prompt, it is possible to achieve improvement, even if we do not achieve \u03c1x > 0.5 in some instances. Further elaborated in Theorem 2, our analysis indicates that even with a weak selector, improvements are attainable, especially when the base models exhibit comparable overall performance. This theoretical analysis offers deeper insights into the conditions and strategies for effective performance improvement with the proposed methodology."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Setup",
            "text": "Datasets and backbones We conduct experiments on eight datasets that span a range of arithmetic and symbolic reasoning tasks. 7 of these datasets, including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), SingleOP, SingleEQ, AddSub and MultiArith (Koncel-Kedziorski et al., 2016), are about arithmetic reasoning, while Date Understanding (Srivastava et al., 2022) focuses on symbolic reasoning. To comprehensively evaluate the effectiveness of our approach, we employ three proprietary LLMs as backbone systems: Codex (code-davinci-002), ChatGPT (gpt-3.5-turbo-0301) and GPT-4 (gpt-4-0314). We also evaluate our method with open-source Llama 2 models on GSM8K.2 Unless stated otherwise, we always utilize the same LLM for both base models as well as for the model selector.\nPrompt design To effectively exploit the incontext learning abilities of the LLMs, we create a set of few-shot examples by manually creating an error in one model\u2019s reasoning chain. Few-shot examples in our prompts are based on those in previous works. For each backbone LLM, we use a fixed prompt for all the arithmetic benchmarks without specific customizations. Examples of the prompts can be found in Appendix D.\nHyperparameters For the results derived using greedy decoding, we use a temperature of 0 during both solution generation and model selection. For experiments with self-consistency, we follow the settings in prior works by setting the temperature at 0.5 for CoT and 0.8 for PAL, during solution generation. During model selection, the temperature is set to 0 for a deterministic result. We refrain from further hyperparameter tuning to emphasize the simplicity and efficacy of our method."
        },
        {
            "heading": "3.2 Main Results",
            "text": "The results of our experiments with greedy decoding are shown in Table 1. First, we find that our proposed method effectively and robustly enhances performance in most settings across various datasets of different difficulties and with different backbone LLMs, simply by combining two base models. For example, with GPT-4, we achieve\n2Codex results are obtained in February and March, ChatGPT in April and May, GPT-4 in May and June 9-13, Llama 2 in September 2023.\nan accuracy of 95.6% on GSM8K and 93.7% on SVAMP without self-consistency.\nSecond, our results show a considerable improvement even when one of the base models performs much worse than the other. For instance, we observe a significant 3.2% improvement over PAL\u2019s 71.5% accuracy on GSM8K, even though CoT has a lower accuracy of 64.4%.\nThird, our model\u2019s general applicability is further underscored by its 1.9% improvement on the symbolic date understanding task when utilizing Codex as the backbone. In fact, even if the accuracy difference between two base models is as large as 13% on this task, our proposed method still improves the accuracy from 77.5% to 79.4%. Additionally, our method also achieves respectable performance gains with both ChatGPT and GPT-4 on this task. It serves as evidence of our method\u2019s applicability to a spectrum of reasoning tasks, extending beyond mathematical reasoning.\nExperiments with self-consistency We aim to investigate the relationship between selfconsistency (Wang et al., 2022b) and model se-\n3https://openai.com/pricing Oct. 2023\nlection with multiple samples, and whether they complement each other. The results of CoT and PAL with self-consistency are obtained by sampling multiple times with the temperature at 0.5 and 0.8 respectively. For our method with selfconsistency, we execute our algorithm repeatedly, and aggregate results across all samples through majority voting, arriving at the final answer. As demonstrated in Table 2, we achieve substantial improvements over the self-consistency results of both CoT and PAL.\nEmploying ChatGPT, we attain a pronounced performance improvement with only 5 samples, reflecting a 2.8% leap in accuracy. With GPT-4, even though both base models already score around 95%, integrating them with our method leads to a 96.5% accuracy on GSM8K. Furthermore, we establish the new state-of-the-art results on GSM8K at 96.8% using 15 samples."
        },
        {
            "heading": "3.3 Computation Cost Comparison",
            "text": "We extend our analysis to compare the computation costs of model selection with multiple samples versus using self-consistency alone. As depicted in Table 3, our method not only showcases notable performance improvement but also significantly reduces computation costs, underscoring its efficacy and efficiency.\nOur method consistently outperforms both CoT and PAL with SC in terms of accuracy while demanding less token computation and cost. For instance, using only 5 paths, our method achieves comparable performance to CoT@80 while only costs 29 USD, which is merely 16% of expense associated with CoT@80. Moreover, when aggregating 15 samples, our method surpasses the performance of CoT@80 at only half the cost.\nFurthermore, our method also elevates the performance ceilings. While the result of CoT with SC plateaus around 88.2% and that of PAL with SC saturates around 83.3%, our method breaks through these ceilings, achieving an impressive 89.5% accuracy using just 20 samples. This result underscores the power and potential of integrating different reasoning models."
        },
        {
            "heading": "3.4 Experiments with Llama 2 Models",
            "text": "To explore the broad applicability of our method, we conduct experiments with open-source Llama 2 7B/13B models using a subset of the GSM8K, comprising 600 data samples. We generate CoT and PAL solutions with Llama 2 7B/13B models4. For model selection, we use Llama 2 7B/13B Chat models5. The experiments are carried out by using greedy decoding without tuning hyperparameters.\nAs shown in Table 4, our method consistently yields performance enhancements when applied to Llama 2 models. Specifically, compared to the\n4https://huggingface.co/meta-llama/Llama-2-7b-hf https://huggingface.co/meta-llama/Llama-2-13b-hf 5https://huggingface.co/meta-llama/Llama-2-7b-chat-hf https://huggingface.co/meta-llama/Llama-2-13b-chat-\nhf\nbaseline results on the 13B model, our method achieves respectable 2% performance gains, improving accuracy from 33.3% to 35.3%. On the smaller 7B model, we observe the performance improvement from 15.0% to 16.2% when utilizing the 7B model as the selector. Moreover, when employing the 13B model as the selector, the performance is further improved to 16.8%. These results affirm the broad applicability of our method."
        },
        {
            "heading": "4 Analysis",
            "text": "In this section, we provide a few analyses to see when and how our method works."
        },
        {
            "heading": "4.1 Influencing Factors",
            "text": "To better understand the reasons for the performance improvement across various datasets and backbone LLMs, we present the performance improvement, and the associated influencing factors in Table 5.\nAs demonstrated in our theoretical analysis, the performance improvement is jointly determined by the difference between the two combined methods, |R(x)|, and the effectiveness of the model selection, \u03c1. The results from Table 5 showcase a high expected performance difference between CoT and PAL, and decent selection accuracy with all backbone LLMs, which clarifies the significant performance gains across the datasets.\nFirstly, the \u2206Upper Bound of CoT and PAL reflects how differently the two base models behave across questions. A larger \u2206Upper Bound signifies a larger room for potential improvement. Specifically, on GSM8K with ChatGPT, \u2206Upper Bound stands at 8.6%, even though the accuracies of CoT and PAL are closely matched at 80.8% and 79.2%, respectively. Similarly, with GPT-4, \u2206Upper Bound is 2.5% while the accuracies of the two base models are close (94.6% vs 94.0%).\nSecondly, across the eight datasets with varied complexities, each of the three LLMs exhibits commendable selection accuracy. Notably, both Codex and GPT-4 show a relatively high success selection\nrate. For example, their success rates on GSM8K surpass 70%, and on the simpler MultiArith dataset, the selection accuracies exceed 90%.\nFurthermore, as detailed in Theorem 1, performance improvements are attainable even without a high success selection rate. Our empirical results corroborate it. For instance, on the date understanding task employing ChatGPT, despite a modest success rate of 53.6%, we still achieve 1.1% performance enhancement with a large \u2206Upper Bound of 9.8%, and small overall performance difference (69.1% and 68.3%)."
        },
        {
            "heading": "4.2 Combination between Similar Methods",
            "text": "We choose CoT and PAL as our two base models due to the motivation of combining different strengths of distinct models. We conduct experiments to examine whether the performance improves when we combine two similar base models. We use two variants of CoT: CoT\u2032 where we set the temperature at 0.1, ComplexCoT (Fu et al., 2022) where we use more complex few-shot examples in\nthe prompt. Both of these methods\u2019 accuracies are similar or higher than the accuracy of PAL.\nFrom the results in Table 6, we can find that model selection between CoT and CoT\u2032, or CoT and ComplexCoT, does not lead to substantial performance gains, even though the accuracy of CoT\u2032 and ComplexCoT is on par with that of PAL. On the other hand, model selection between CoT and PAL results in consistent performance improvements. To understand the reasons behind these outcomes, we further investigate the \u2206Upper Bound and the success selection rate.\nFirstly, \u2206Upper Bound of CoT-PAL exceeds that of other combinations, CoT-CoT\u2032 and ComplexCoT-CoT, despite their employing two stronger or equivalent two base models. This observation suggests a larger absolute value of the accuracy difference per question for CoT-PAL. It indicates that CoT and PAL perform more dissimilarly than other model combinations, which represents a larger |R(x)|. As Proposition 1 highlights, without a substantial |R(x)|, it is unlikely to achieve significant performance gain since the improvement component is factored by |R(x)|.\nSecondly, the success selection rate of CoT-PAL surpasses that of other model combinations. It means that the selector is more likely to select the correct choice when one solution derives from CoT and the other from PAL. In theory, this higher success rate implies that when |R(x)| is high for a given question x, the success selection probability \u03c1x for CoT-PAL is higher than others.\nThese findings support our initial motivation and hypothesis. We choose CoT and PAL as our two\nbase models because they represent distinct reasoning approaches using natural language and programming language. We expect these models to exhibit a significant difference in errors and accuracies, indicated by a high \u2206Upper Bound. Moreover, we posit that for a particular question, the considerable difference in errors between two solutions makes it easier for LLMs to select the correct option, leading to a higher success rate compared to selecting between two similar base models like CoT-CoT\u2032. This holds true even when different prompts or temperature settings are used."
        },
        {
            "heading": "4.3 Combination between Different Backbone LLMs",
            "text": "Considering that Codex is optimized for code generation and ChatGPT performs better in natural language reasoning, we embark on experiments integrating various backbone LLMs, to harness the strengths of distinct LLMs and reasoning models.\nAs shown in Table 7, we achieve a 0.4% performance improvement with ChatGPT as the selector, despite a notable 9.3% accuracy disparity between PAL from Codex and CoT from ChatGPT. More impressively, when utilizing GPT-4 as the model selector, we get a significant 3.6% performance boost, with an 84.8% success selection rate. These results hint the potential benefits when integrating multiple models tailored for specialized domains or tasks."
        },
        {
            "heading": "4.4 The Effect of Option Order",
            "text": "To understand the influence of option order in a multiple-choice question format, we perform experiments by modifying the option order. The results in Table 8 show a performance impact from simply changing the choice order. We attribute this to the inherent bias within the LLMs. A similar phenomenon is reported by Zheng et al. (2023b).\nEach of the three backbones seems to exhibit a preference for the first option. Particularly noticeable on Codex, in which when CoT is positioned first, it is selected in 71.9% of all selections. However, RatioCoT significantly drops to 17.3% when putting CoT second. ChatGPT shows a preference for natural language reasoning as it always selects CoT solutions more frequently regardless of its position in the options. Moreover, when CoT is the second choice, RatioCoT decreases from 89.9% to 79.7%, which we speculate demonstrates its preference for the first option. GPT-4 has similar biases akin to ChatGPT, though it appears fairer. The option order also affects the success selection rate and further influences the overall performance.\nWhile Zhao et al. (2021) attempts to address the inherent bias through probability calibration, the task of mitigating the bias in the absence of underlying token probabilities remains a challenge. Therefore, despite the wide use of LLMs in data annotation and task evaluation(Xu et al., 2023; Wang et al., 2023; Chiang et al., 2023), addressing and eliminating inherent biases is a critical area that deserves further exploration."
        },
        {
            "heading": "5 Related Work",
            "text": "Ensemble learning. In machine learning, the strategy of combining various models to address a single problem is exemplified in techniques such as bagging (Breiman, 1996), boosting (Freund and Schapire, 1997; Chen and Guestrin, 2016; Ke et al., 2017), and random forest (Ho, 1995; Breiman, 2001). The underlying idea in these methods is that a group of weak learners can collectively manifest as a strong learner. This concept has also found its place in deep learning through the use of ensembles. For reasoning, self-consistency samples diverse reasoning paths and chooses the most consistent answer through majority voting (Wang et al., 2022b). Wang et al. (2022a) takes it a step further by introducing rationale-augmented ensembles, emphasizing rationale sampling in the output space. However, typically, ensembling places equal weights on models through majority voting, which may restrict the full potential of the diverse strengths that each model offers.\nReasoning. The research community has made tremendous progress in the field of reasoning. Apart from CoT (Wei et al., 2022b) and PAL (Gao et al., 2022; Chen et al., 2022), Zhou et al. (2022) simplifies complex problems by breaking them down into a series of sub-problems. Kojima et al. (2022) shows that by simply adding \"Let\u2019s think step by step\" before each answer, LLMs can be competent zero-shot reasoners. Creswell et al. (2022) alternates between selection and inference stages, generating causal reasoning steps to the final answer. Kazemi et al. (2022) proposes a backward chaining algorithm that breaks reasoning down into four sub-models. Paul et al. (2023); Xie et al. (2023); Yao et al. (2023) refine intermediate reasoning steps by leveraging another critic model or the self-evaluation ability. Recently, Zhou et al. (2023) utilizes GPT-4 Code Interpreter and code-based-self-verification, achieving a 97.0% accuracy on GSM8K, but their work is concurrent with ours. More importantly, the contributions of these methods are distinct from our approach, and the progress made by them could potentially be seamlessly integrated using our method.\nSelf-Evaluation. LLM calibration studies reveal that the probabilistic predictions made by current LLMs closely align with the actual frequencies of token occurrences, hence producing well-calibrated predictions for certain tasks (Guo et al., 2017; Kadavath et al., 2022; Jiang et al., 2020). As LLMs ex-\nhibit reliable calibration, there is a growing number of research emphasizing the use of self-evaluation for verification. Shinn et al. (2023) proposes an approach to provide an agent with dynamic memory and self-reflection capabilities. Madaan et al. (2023) proposes a method to generate outputs from LLMs and refine its previously generated output given its own feedback. Different from these works where the underlying method is the same, in this work, we are interested in combining systems with different strengths and weaknesses through selfevaluation model selection."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduce a method that effectively combines two distinct models, CoT and PAL, by using LLMs for the model selection. Our theoretical analysis supports the feasibility of such model combination, which is validated by empirical results. Our method achieves performance improvements across eight datasets with various backbone LLMs. We reduce the computation cost while attaining notable performance improvement with self-consistency. We set new SOTA results on GSM8K and SVAMP. This research represents a significant step towards tapping into the potential of diversity and collaboration among models in LLMs.\nIn our future work, we aim to expand this framework to more reasoning tasks and possibly other domains. An intriguing concept involves exploring the use of diverse system instructions to elicit varying model behaviors for model combinations. Furthermore, a genuine combination of specialized models, each with its strengths and expertise, offers a promising avenue for exploration."
        },
        {
            "heading": "7 Limitation",
            "text": "This work focuses on reasoning tasks, but we believe that extending the exploration of model selection into other domains could yield valuable insights. Further, due to the inherent bias of LLMs, our method is affected by the order of options. We encourage the community to explore solutions for bias mitigation. We also anticipate subsequent studies to identify more robust and potent model selectors. Finally, our combination strategy currently comprises only the CoT and PAL base models. Future research could investigate the incorporation of more base models with distinct strengths, with the aim of creating a more powerful synthesis that leverages their unique advantages."
        },
        {
            "heading": "A A detailed version of Theorem 1",
            "text": "In this appendix, we provide a detailed version of Theorem 1. Whereas Theorem 1 only states the existence of problem instances, Theorem 2 constructs such instances concretely: i.e., Theorem 2 implies Theorem 1. Define \u00b5x[X ] to be the distribution for the expected errors: i.e., an expected error can be written by Ex\u223c\u00b5x[X ],y,f [1[y \u0338= f(x)]] for some function f . Define S[X ] = {x \u2208 X : R(x) < 0}. Let us denote U [X ] as the uniform distribution over X . Given any X , we write n = |X |, T = |S[X ]|, \u03b1 = T/n. Assume that 1 \u2264 T < n. Theorem 2. Let \u00b5x[X ] = U [X ] and X be given such that |X | < \u221e. Let \u03f5, \u03b4 \u2208 (0, 1) and \u03bb \u2208 (0, 1] such that \u03b2 = \u03f5Tn\u2212T \u2208 (0, 1) and \u03bb \u2265 1 \u2212 \u03b2 \u03f5T (n \u2212 T \u2212 \u03b4). Let R and \u03c1x be set such that R(x) = \u2212\u03f5\nfor x \u2208 S[X ], R(x) = \u03b2 for x \u2208 X \\ S[X ], (1/T ) \u2211 x\u2208S[X ] \u03c1x = \u03bb, and (1/(n\u2212 T )) \u2211 x\u2208X\\S[X ] \u03c1x = \u03f5(T/(n\u2212 T ))(1\u2212 \u03bb)\u03b2\u22121 + \u03b4/(n\u2212 T ). Then, we have that err < err1, err1 \u2264 err2, and\n\u03c1 = 1\u2212 \u03b1+ \u03bb[2\u03b1\u2212 1] + \u03b4 n .\nIn particular, when \u03b1 \u2265 0.5, we have \u03c1 \u2192 0 as \u03b1 \u2192 1 and (\u03b4/(n\u2212T )) \u2192 0 (with \u03bb = 1\u2212 \u03b2T\u03f5(n\u2212T\u2212\u03b4)); when \u03b1 < 0.5, we have \u03c1 \u2192 0 as \u03b1 \u2192 0 and (\u03b4/n) \u2192 0 (with \u03bb = 1).\nThe proof of Theorem 2 is presented in Appendix B. Theorem 2 shows that the overall success probability of the selection process can be much worse than a random guess to achieve the improvement over the base methods m1 and m2; i.e., err < err1 and err1 \u2264 err2 can happen with \u03c1 < 0.5. Indeed, it is possible to have \u03c1 \u2192 0 with the improvement (err < err1 and err1 \u2264 err2) when the size of X is large: when \u03b1 \u2265 0.5, we can choose \u03bb = 1\u2212 \u03b2T\u03f5(n\u2212 T \u2212 \u03b4) with which err < err1, err1 \u2264 err2, and \u03c1 \u2192 0 as \u03b1 \u2192 1 and (\u03b4/(n\u2212 T )) \u2192 0. When \u03b1 < 0.5, we can choose \u03bb = 1 with which err < err1, err1 \u2264 err2, and \u03c1 \u2192 0 as \u03b1 \u2192 0 and (\u03b4/n) \u2192 0. This supports our proposition that, despite not training a new model for the selection process and with the in-context learning limited to a few-shot prompt, it is possible to achieve improvement, even if we do not achieve \u03c1x > 0.5 in some instances.\nTheorem 2 also suggests that if the overall performance of two base methods is similar, captured by \u03f5, the overall selection process can be weak to achieve some improvement, as long as the success selection probability is relatively high when the two methods have very different expected errors (or accuracies) for a given question. In essence, Theorem 2 suggests a trade-off: we want |R(x)| to be larger when deciding which two base methods m1 and m2 to choose, implying that we prefer base methods to perform dissimilarly on X . On the other hand, if two base methods exhibit a substantial expected accuracy difference, then the selection process needs to be stronger to improve the performance (i.e., \u03c1 needs to be larger). However, if the expected accuracy difference between the two base methods is relatively small, increasing the power of the selection process is not that necessary to boost performance."
        },
        {
            "heading": "B Proofs",
            "text": "B.1 Proof of Proposition 1 Proof. Define acc = 1 \u2212 err and acci = 1 \u2212 erri for i \u2208 {1, 2}. Since expected error = E[1[incorrect prediction]] = P (incorrect prediction) = 1\u2212 P (correct prediction), we have that\nacci = p(correct|mi) = Ex[p(correct|x,mi)]\nwhere correct represents the event of the correct prediction. Similarly,\nacc = Ex\n[ 2\u2211\ni=1\np(mi|x)p(correct|x,mi)\n]\nwhere p(mi|x) represents the probability of selecting method mi given x via the proposed method. Thus,\nacc\u2212 acc1 = Ex [p(m1|x)p(correct|x,m1) + p(m2|x)p(correct|x,m2)\u2212 p(correct|x,m1)]\n= Ex [(p(m1|x)\u2212 1)p(correct|x,m1) + p(m2|x)p(correct|x,m2)] = Ex [(p(m2|x)p(correct|x,m2)\u2212 (1\u2212 p(m1|x))p(correct|x,m1)]\nSince 1\u2212 p(m1|x) = p(m2|x),\nacc\u2212 acc1 = Ex [(p(m2|x)p(correct|x,m2)\u2212 p(m2|x)p(correct|x,m1)] = Ex [(p(m2|x)R(x)] .\nHere, we notice that\np(m2|x) = { \u03c1x if R(x) \u2265 0 1\u2212 \u03c1x if R(x) < 0\n= 1{R(x) \u2265 0}\u03c1x + 1{R(x) < 0}(1\u2212 \u03c1x).\nBy plugging this into the above equation,\nacc\u2212 acc1 = Ex [(1{R(x) \u2265 0}\u03c1x + 1{R(x) < 0}(1\u2212 \u03c1x))R(x)] = Ex[R(x)1{R(x) \u2265 0}\u03c1x] + Ex[R(x)1{R(x) < 0}(1\u2212 \u03c1x)]\nSince R(x)1{R(x) \u2265 0} = |R(x)|1{R(x) \u2265 0} and R(x)1{R(x) < 0} = \u2212|R(x)|1{R(x) < 0}, we have that\nacc\u2212 acc1 = Ex[|R(x)|1{R(x) \u2265 0}\u03c1x]\u2212 Ex[|R(x)|1{R(x) < 0}(1\u2212 \u03c1x)] = Ex[|R(x)|((1{R(x) \u2265 0}+ 1{R(x) < 0})\u03c1x \u2212 1{R(x) < 0})]\nSince (1{R(x) \u2265 0}+ 1{R(x) < 0}) = 1 for any x,\nacc\u2212 acc1 = Ex[|R(x)|(\u03c1x \u2212 1{R(x) < 0})].\nB.2 Proof of Theorem 2 Proof. We first confirm that R(x) and \u03c1x define valid probabilities under the condition of this statement. For R(x), since \u03f5 \u2208 (0, 1) and \u03b2 \u2208 (0, 1), it defines valid probabilities for methods m1 and m2. For \u03c1x, since \u03bb \u2208 [0, 1], it also defines valid probabilities for the case of x \u2208 S[X ]. For the case of x \u2208 X \\ S[X ], since \u03f5(T/(n\u2212 T ))(1\u2212 \u03bb)\u03b2\u22121 + \u03b4/n \u2265 0, we need to show that \u03f5(T/(n\u2212 T ))(1\u2212 \u03bb)\u03b2\u22121 + \u03b4/n \u2264 1. That is,\n\u03f5(T/(n\u2212 T ))(1\u2212 \u03bb)\u03b2\u22121 + \u03b4/(n\u2212 T ) \u2264 1 \u21d0\u21d2\u03f5T (1\u2212 \u03bb)\u03b2\u22121 \u2264 n\u2212 T \u2212 \u03b4\n\u21d0\u21d21\u2212 \u03b2(n\u2212 T \u2212 \u03b4) \u03f5T \u2264 \u03bb,\nwhich is satisfied by the condition on \u03bb that \u03bb \u2265 1\u2212 \u03b2\u03f5T (n\u2212 T \u2212 \u03b4). Thus, the condition on \u03c1x defines the valid probabilities for both cases of x \u2208 S[X ] and x \u2208 X \\ S[X ].\nWe now show that err < err1. Invoking Proposition 1,\nerr = err1 \u2212 Ex [|R(x)| (\u03c1x \u2212 1{R(x) < 0})] .\nThus, we have err < err1 if Ex [|R(x)| (\u03c1x \u2212 1{R(x) < 0})] > 0. This condition can be rewritten as\nEx [|R(x)| (\u03c1x \u2212 1{R(x) < 0})] > 0\n\u21d0\u21d2 1 n \u2211 x\u2208X [|R(x)| (\u03c1x \u2212 1{R(x) < 0})] > 0\n\u21d0\u21d2 \u2211\nx\u2208X\\S[X ]\n|R(x)|\u03c1x + \u2211\nx\u2208S[X ]\n|R(x)|\u03c1x > \u2211\nx\u2208S[X ]\n|R(x)|\n\u21d0\u21d2 \u2211\nx\u2208X\\S[X ]\n|R(x)|\u03c1x + \u03f5T\u03bb > \u03f5T\n\u21d0\u21d2\u03b2 \u2211\nx\u2208X\\S[X ]\n\u03c1x > \u03f5T \u2212 \u03f5T\u03bb = \u03f5T (1\u2212 \u03bb)\n\u21d0\u21d2 \u2211\nx\u2208X\\S[X ]\n\u03c1x > \u03f5T (1\u2212 \u03bb)\n\u03b2\nThis is satisfied by the condition on \u03c1 that (1/(n \u2212 T )) \u2211\nx\u2208X\\S[X ] \u03c1x = \u03f5(T/(n \u2212 T ))(1 \u2212 \u03bb)\u03b2\u22121 + \u03b4/(n\u2212 T ) for some \u03b4 > 0: i.e., \u2211\nx\u2208X\\S[X ]\n\u03c1x = \u03f5T (1\u2212 \u03bb)\n\u03b2 + \u03b4.\nTherefore, we have that err < err1. We now show that err1 \u2264 err2. Similarly to the proof of Proposition 1, we define acci = 1\u2212 erri for i \u2208 {1, 2}. Then, the inequality err1 \u2264 err2 holds if acc1 \u2265 acc2. By using correct to represent the event of the correct prediction, this condition can be rewritten as\nacc1 \u2265 acc2 \u21d0\u21d2 \u2211 x\u2208X p(correct|x,m1)] \u2265 \u2211 x\u2208X p(correct|x,m2)]\n\u21d0\u21d20 \u2265 \u2211 x\u2208X R(x) = (n\u2212 T )\u03b2 \u2212 \u03f5T\n\u21d0\u21d2 \u03f5T n\u2212 T \u2265 \u03b2\nThis is satisfied by \u03b2 = \u03f5Tn\u2212T . Thus, we have that err1 \u2264 err2. Using these, we now compute the \u03c1 as\n\u03c1 = Ex\u223c\u00b5x[X ][\u03c1x]\n= 1\nn \u2211 x\u2208X\\S[X ] \u03c1x + 1 n \u2211 x\u2208S[X ] \u03c1x\n= 1\nn\n( \u03f5T (1\u2212 \u03bb)\n\u03b2 + \u03b4\n) + \u03b1\u03bb\n= \u03f5T (1\u2212 \u03bb)\n\u03b2n + \u03b1\u03bb+\n\u03b4\nn\n= \u03f5T (1\u2212 \u03bb)(n\u2212 T )\n\u03f5Tn + \u03b1\u03bb+\n\u03b4\nn\n= (1\u2212 \u03bb)\u2212 \u03b1(1\u2212 \u03bb) + \u03b1\u03bb+ \u03b4 n = 1\u2212 \u03b1+ \u03bb[2\u03b1\u2212 1] + \u03b4 n .\nFinally, we prove the asymptotic behavior using this equation. When \u03b1 < 0.5, by setting \u03bb = 1, we have that\n\u03c1 = 1\u2212 \u03b1+ \u03bb[2\u03b1\u2212 1] + \u03b4 n = 1\u2212 \u03b1+ 2\u03b1\u2212 1 + \u03b4 n = \u03b1+ \u03b4 n \u2192 0\nas \u03b1 \u2192 0 and (\u03b4/n) \u2192 0. When \u03b1 \u2265 0.5, by setting \u03bb = 1\u2212 \u03b2T\u03f5(n\u2212 T \u2212 \u03b4), we have that \u03c1 = 1\u2212 \u03b1+ ( 1\u2212 \u03b2\nT\u03f5 (n\u2212 T \u2212 \u03b4)\n) [2\u03b1\u2212 1] + \u03b4\nn\n= 1\u2212 \u03b1+ 2\u03b1\u2212 1\u2212 [2\u03b1\u2212 1] \u03b2 T\u03f5 (n\u2212 T \u2212 \u03b4) + \u03b4 n = \u03b1\u2212 [2\u03b1\u2212 1] \u03b2 T\u03f5 (n\u2212 T \u2212 \u03b4) + \u03b4 n\nBy defining Q = \u03b2T\u03f5(n\u2212 T \u2212 \u03b4), we have\n\u03c1 = \u03b1\u2212 [2\u03b1\u2212 1]Q+ \u03b4 n .\nHere,\nQ = \u03b2 T\u03f5 (n\u2212 T \u2212 \u03b4) = \u03f5T n\u2212 T 1 T\u03f5 (n\u2212 T \u2212 \u03b4) = 1 n\u2212 T (n\u2212 T \u2212 \u03b4) = 1\u2212 \u03b4 n\u2212 T\nThus,\n\u03c1 = \u03b1\u2212 [2\u03b1\u2212 1] ( 1\u2212 \u03b4\nn\u2212 T\n) + \u03b4\nn\n= \u03b1\u2212 2\u03b1+ 1 + \u03b42\u03b1\u2212 1 n\u2212 T + \u03b4 n\n= 1\u2212 \u03b1+ \u03b4 ( 2\u03b1\u2212 1 n\u2212 T + 1 n ) \u2192 0\nas \u03b1 = T/n \u2192 1 and (\u03b4/(n \u2212 T )) \u2192 0: e.g., by setting \u03b4 = \u03b6(n \u2212 T ) and take \u03b6 \u2192 0, with which (\u03b4/(n\u2212 T )) = \u03b6 \u2192 0."
        },
        {
            "heading": "C The Effect and Qualitative Analysis of Explanation",
            "text": "To perform model selection, we provide explanations in the few-shot examples and ask the LLMs to generate explanations after making a choice, as we expect to improve the model\u2019s selection ability by pointing out why the other choice is incorrect. To investigate the potential role of explanations in enhancing the model\u2019s selection capability, we conduct experiments on GSM8K by excluding explanations from the answer.\nThe results in Table 9 reveal that for backbone LLMs with more powerful in-context learning abilities, such as Codex and GPT-4, the inclusion of explanations within the answer seems to have negligible influence on the model\u2019s performance. In other words, Codex and GPT-4 have a relatively high success selection rate even when explanations are not given. However, for ChatGPT, which possesses relatively low few-shot in-context learning capability, the integration of explanations yields a larger improvement in the success selection rate. This observation suggests that for backbone LLMs with weaker in-context learning abilities, explanations can serve as a valuable tool to improve the success selection rate.\nQualitative studies on the explanations We are also interested in gaining an intuitive understanding of the algorithm, so we conduct some qualitative analysis on the selection explanations. As displayed in Table 10, in the first example, both ChatGPT and GPT-4 can accurately identify that the PAL solution is correct. However, ChatGPT provides an incorrect explanation, while GPT-4 offers a valid one. In the second example, which is relatively simpler, both ChatGPT and GPT-4 successfully perform accurate model selection and provide valid explanations. In the first example, we can see that GPT-4 actually possesses exceptional reasoning capabilities and provides reliable explanations.\nQuestion: Courtney attended a concert and reported that the audience was 48 in number. However, Kelly went to the same concert and said that Courtney had made the mistake of overstating the number of people in attendance by 20%. If Kelly was right, how many people really attended the concert?\n(A) Answer: Courtney reported 48 people in attendance. If this number is overstated by 20%, then we need to find the actual number of people. First, we need to find 20% of 48. 20% of 48 is (20/100) * 48 = 9.6. Now, we subtract this number from Courtney\u2019s reported number to find the actual number of people. 48 - 9.6 = 38.4. Since we cannot have a fraction of a person, we round the number to the nearest whole number. So, the actual number of people in attendance was approximately 38. So the answer is 38. (B) def solution (): courtney_report = 48 overstatement_percentage = 0.20 actual_attendance = courtney_report /(1 + overstatement_percentage) result = int(actual_attendance) return result\nChatGPT: (B) can correctly answer the math problem. Because (A) rounds up the result to the nearest whole number, which is not necessary.\nGPT-4: (B) can correctly answer the math problem. Because (A) incorrectly calculated the actual number of people by subtracting the overstatement percentage from the reported number instead of dividing it.\nQuestion: How much does it cost you for lunch today at Subway if you pay $40 for a foot-long fish sub and thrice as much for a six-inch cold-cut combo sub?"
        },
        {
            "heading": "D Examples of Prompts",
            "text": "We show examples of model selection prompts used on different tasks with different backbones. For arithmetic reasoning tasks, we employ an 8-shot example for both Codex and Llama 2, and a 5-shot example for ChatGPT and GPT-4. For the date understanding task, we use 6-shot examples for Codex, ChatGPT, and GPT-4. Few-shot examples in the prompts are based on those in previous works (Wei et al., 2022b; Gao et al., 2022; Fu et al., 2022). We only show a few examples for each case. Full prompts can be found in our code.\nMath Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left? Question: Which of the following two choices can correctly answer the math problem?\n(A)\ndef solution (): money_initial = 23 bagels = 5 bagel_cost = 3 money_spent = bagels * bagel_cost money_left = money_initial -\nmoney_spent result = money_left return result\n(B)\nAnswer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. So she has 23 - 5 = 18 dollars left. The answer is 18.\nAnswer: (A)\nAnswer: (A)\nTable 12: An example of 6-shot model selection prompts used on Date Understanding task with Codex.\nSystem: You are a helpful assistant that can identify the correct answer to the math problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. Either of them can correctly answer the math problem. You need to identify which choice can correctly answer the math problem. Here is one example how to do it,\nMath problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n(A)\nAnswer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. So she has 23 - 15 = 8 dollars left. So the answer is 8.\n(B)\ndef solution (): money_initial = 23 bagels = 5 bagel_cost = 3 money_spent = bagels + bagel_cost money_left = money_initial -\nmoney_spent result = money_left return result\nWhich of the above two choices can correctly answer the math problem?\n(A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them.\nNow it\u2019s your turn. Here is another math problem and two choices.\nMath Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n(A)\nAnswer: Michael started with 58 golf balls. Then after losing 23 on tuesday, he had 58 -23 = 35. After losing 2 more, he had 35 + 2 = 37 golf balls. So the answer is 37.\n(B)\ndef solution (): golf_balls_initial = 58 golf_balls_lost_tuesday = 23 golf_balls_lost_wednesday = 2 golf_balls_left =\ngolf_balls_initial - \\ golf_balls_lost_tuesday -\n\\golf_balls_lost_wednesday result = golf_balls_left return result\nWhich of the above two choices can correctly answer the math problem? (B) can correctly answer the math problem. Because (A) adds 2 more balls after losing 2 more on Wednesday instead of subtracting them.\nTable 13: Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with ChatGPT.\nSystem: You are a helpful assistant that can identify the correct answer to the math problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python code to answer it. Either of them can correctly answer the math problem. You need to identify which choice can correctly answer the math problem. Here is one example how to do it,\nMath problem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n(A)\nAnswer: There were originally 9 computers. For each of 4 days from monday to thursday, 5 more computers were added. So 5 * 4 = 20 computers were added. So there are 9 + 20 = 29 computers now. So the answer is 29.\n(B)\ndef solution (): computers_initial = 9 computers_added = 5 computers_total =\ncomputers_initial + computers_added\nresult = computers_total return result\nWhich of the above two choices can correctly answer the math problem? (A) can correctly answer the math problem. Because (B) missed the fact that computers were added each day from monday to thursday.\nNow it\u2019s your turn. Here is another math problem and two choices.\nMath Problem: A piece of square paper has a perimeter of 32 centimeters. Nicky\u2019s dog, Rocky, tore off 1/4 of the paper. What is the area of the remaining paper?\n(A)\nAnswer: A square has 4 equal sides. The perimeter of the square paper is 32 centimeters. So each side of the square is 32 / 4 = 8 centimeters. The area of the whole square paper is side * side = 8 * 8 = 64 square centimeters. Rocky tore off 1/4 of the paper. So The area of the remaining paper is 1/4 * 64 = 16 square centimeters. So the answer is 16.\n(B)\ndef solution (): perimeter = 32 fraction_torn = 1 / 4 area_total = (perimeter / 4) ** 2 area_remaining = (1 -\nfraction_torn) * area_total result = area_remaining return result\nWhich of the above two choices can correctly answer the math problem? (B) can correctly answer the math problem. Because (A) incorrectly calculated the area of the torn-off portion instead of the remaining portion.\nTable 14: Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with GPT-4.\nSystem: You are a helpful assistant that can identify the correct answer to the date understanding problem.\nThere are two choices to the same date understanding problem. One uses natural language to answer the question, while the other uses Python program to answer it. Either of them can correctly answer the date understanding problem. You need to identify which choice can correctly answer the problem. Here is one example how to do it,\nDate Understanding Problem: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n(A)\nAnswer: If 2015 is coming in 36 hours, then it is coming in 2 days. And 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.\n(B)\ndef solution (): # If 2015 is coming in 36 hours ,\nthen today is 36 hours before. today = datetime (2015, 1, 1) + relativedelta(hours =36) # One week from today , one_week_from_today = today + relativedelta(weeks =1) # The answer formatted with %m/%d /%Y is result = one_week_from_today. strftime('%m/%d/%Y') return result\nWhich of the above two choices can correctly answer the date understanding problem? (A) can correctly answer the date understanding problem. Because (B) incorrectly calculates the date 36 hours later instead of 36 hours before.\nNow it\u2019s your turn. Here is another date understanding problem and two choices.\nDate Understanding Problem: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?\n(A)\nAnswer: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. And today is the first monday, would be 5 days later. So today is 01/06/2019. So the answer is 01/06/2019.\n(B)\ndef solution (): # If the first day of 2019 is a\nTuesday , and today is the first Monday of 2019, then today is\n6 days later. today = datetime (2019, 1, 1) + relativedelta(days =6) # The answer formatted with %m/%d /%Y is result=today.strftime('%m/%d/%Y') return result\nWhich of the above two choices can correctly answer the date understanding problem? (B) can correctly answer the problem. Because (A) missed the fact that there are 6 days between the first day of 2019 and the first Monday of 2019.\nTable 15: Two examples of 6-shot model selection prompts used on Date Understanding with ChatGPT and GPT-4.\nSystem: You are a helpful and brilliant assistant. You are a math expert who can identify the correct answer to the math problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. *ONLY ONE* of them can correctly answer the math problem. You need to check these two solutions carefully and identify which choice can correctly answer the math problem. Here is the math problem and two solutions to it.\nMath problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n(A)\ndef solution (): money_initial = 23 bagels = 5 bagel_cost = 3 money_spent = bagels * bagel_cost money_left = money_initial -\nmoney_spent result = money_left return result\nThe result is 8.\n(B)\nAnswer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 + 3 = 8 dollars. So she has 23 - 8 = 15 dollars left. The answer is 15.\nWhich of the above two choices can correctly answer the math problem? (A) or (B)?\n(A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. *ONLY ONE* of them can correctly answer the math problem. You need to check these two solutions carefully and identify which choice can correctly answer the math problem. Here is the math problem and two solutions to it.\nMath problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n(A)\nAnswer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. She has 23 - 15 = 8 dollars left.. The answer is 8.\n(B)\ndef solution (): money_initial = 23 bagels = 5 bagel_cost = 3 money_spent = bagels + bagel_cost money_left = money_initial -\nmoney_spent result = money_left return result\nThe result is 15.\nWhich of the above two choices can correctly answer the math problem? (A) or (B)?\n(A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them.\nTable 17: One example of 8-shot model selection prompts on GSM8K with Llama 2 13B as the model selector."
        },
        {
            "heading": "E Examples of Failure Cases Comparison",
            "text": "In Section 2.1, we divide the error reasons of CoT and PAL on GSM8K into 5 categories. Detailed examples of 4 primary categories are presented in the following tables.\nQuestion: It costs $194 per meter to repave a street. Monica\u2019s street is 150 meters long. How much more does it cost to repave Lewis\u2019 street, which is 490 meters long?\nexternal Python interpreter. >65960.0 is obtained with the Python interpreter.\nQuestion:Kim has started his own housekeeping business and is calculating how much profit he will make from his clients. He already has 3 clients, but is talking to another 5 potential clients and feels confident enough to include them in his calculations. Each client\u2019s home will need 2 bottles of bleach and a pack of cloths to clean. Bottles of bleach will cost $2 each and packs of cloths will cost $5 each. These are his only expenses. He calculates that his total income each week will be $92. Profit is the difference between total income and total expenses, so how much profit, in dollars, will Lucas make each week?\nQuestion: Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget. However, she has to choose between the boots and two pairs of high heels that together cost five dollars less than the boots. If one pair of heels costs $33 and the other costs twice as much, how many dollars are the boots?"
        }
    ],
    "title": "Automatic Model Selection with Large Language Models for Reasoning",
    "year": 2023
}