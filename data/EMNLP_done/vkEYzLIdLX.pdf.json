{
    "abstractText": "We present Dolphin, a novel benchmark that addresses the need for a natural language generation (NLG) evaluation framework dedicated to the wide collection of Arabic languages and varieties. The proposed benchmark encompasses a broad range of 13 different NLG tasks, including dialogue generation, question answering, machine translation, summarization, among others. Dolphin comprises a substantial corpus of 40 diverse and representative public datasets across 50 test splits, carefully curated to reflect real-world scenarios and the linguistic richness of Arabic. It sets a new standard for evaluating the performance and generalization capabilities of Arabic and multilingual models, promising to enable researchers to push the boundaries of current methodologies. We provide an extensive analysis of Dolphin, highlighting its diversity and identifying gaps in current Arabic NLG research. We also offer a public leaderboard that is both interactive and modular and evaluate several models on our benchmark, allowing us to set strong baselines against which researchers can compare.1",
    "authors": [
        {
            "affiliations": [],
            "name": "El Moatez Billah Nagoudi\u03be"
        },
        {
            "affiliations": [],
            "name": "AbdelRahim Elmadany\u03be"
        },
        {
            "affiliations": [],
            "name": "Ahmed Oumar El-Shangiti\u03bb"
        },
        {
            "affiliations": [],
            "name": "Muhammad Abdul-Mageed\u03be"
        }
    ],
    "id": "SP:d97f84b666dc90bf14ddb9b4d3bad37b4ef93837",
    "references": [
        {
            "authors": [
                "Bashar Alhafni",
                "Nizar Habash",
                "Houda Bouamor."
            ],
            "title": "User-Centric Gender Rewriting",
            "venue": "Proceedings of the 2022 Conference of the North American 14https://alliancecan.ca 15https://arc.ubc.ca/ubc-arc-sockeye",
            "year": 2022
        },
        {
            "authors": [
                "Marwah Alian",
                "Arafat Awajan",
                "Ahmad Al-Hasan",
                "Raeda Akuzhia."
            ],
            "title": "Towards building arabic paraphrasing benchmark",
            "venue": "Proceedings of the Second International conference on Data Science E-learning and Information Systems (DATA\u2019 2019), pages 1\u20135.",
            "year": 2019
        },
        {
            "authors": [
                "Mohamed Seghir Hadj Ameur",
                "Farid Meziane",
                "Ahmed Guessoum."
            ],
            "title": "Anetac: Arabic named entity transliteration and classification dataset",
            "venue": "arXiv preprint arXiv:1907.03110.",
            "year": 2019
        },
        {
            "authors": [
                "Wissam Antoun",
                "Fady Baly",
                "Hazem Hajj."
            ],
            "title": "Arabert: Transformer-based model for arabic language understanding",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language",
            "year": 2020
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637.",
            "year": 2020
        },
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Uddin Ahmad",
                "Yuan-Fang Li",
                "Yong bin Kang",
                "Rifat Shahriyar."
            ],
            "title": "Crosssum: Beyond english-centric crosslingual abstractive text summarization for 1500+ language pairs",
            "venue": "CoRR, abs/2112.08804.",
            "year": 2021
        },
        {
            "authors": [
                "Abhik Bhattacharjee",
                "Tahmid Hasan",
                "Wasi Uddin Ahmad",
                "Rifat Shahriyar."
            ],
            "title": "BanglaNLG and BanglaT5: Benchmarks and resources for evaluating low-resource natural language generation in Bangla",
            "venue": "pages 726\u2013735.",
            "year": 2023
        },
        {
            "authors": [
                "Houda Bouamor",
                "Nizar Habash",
                "Kemal Oflazer."
            ],
            "title": "A Multidialectal Parallel Corpus of Arabic",
            "venue": "LREC, pages 1240\u20131245.",
            "year": 2014
        },
        {
            "authors": [
                "Houda Bouamor",
                "Nizar Habash",
                "Mohammad Salameh",
                "Wajdi Zaghouani",
                "Owen Rambow",
                "Dana Abdulrahim",
                "Ossama Obeid",
                "Salam Khalifa",
                "Fadhl Eryani",
                "Alexander Erdmann"
            ],
            "title": "The Madar Arabic Dialect Corpus and Lexicon",
            "venue": "Proceedings of the Eleventh",
            "year": 2018
        },
        {
            "authors": [
                "Samuel Cahyawijaya",
                "Genta Indra Winata",
                "Bryan Wilie",
                "Karissa Vincentio",
                "Xiaohong Li",
                "Adhiguna Kuncoro",
                "Sebastian Ruder",
                "Zhi Yuan Lim",
                "Syafri Bahar",
                "Masayu Leylia Khodra"
            ],
            "title": "Indonlg: Benchmark and resources for evaluating indonesian natural",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
            "venue": "arXiv preprint arXiv:1708.00055.",
            "year": 2017
        },
        {
            "authors": [
                "Yiran Chen",
                "Zhenqiao Song",
                "Xianze Wu",
                "Danqing Wang",
                "Jingjing Xu",
                "Jiaze Chen",
                "Hao Zhou",
                "Lei Li."
            ],
            "title": "MTG: A benchmark suite for multilingual text generation",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Amina Chouigui",
                "Oussama Ben Khiroun",
                "Bilel Elayeb."
            ],
            "title": "An arabic multi-source news corpus: Experimenting on single-document extractive summarization",
            "venue": "Arabian Journal for Science and Engineering, 46:3925\u20133938.",
            "year": 2021
        },
        {
            "authors": [
                "Aleksandr Chuklin",
                "Justin Zhao",
                "Mihir Kale."
            ],
            "title": "Clse: Corpus of linguistically significant entities",
            "venue": "pages 78\u201396.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2012
        },
        {
            "authors": [
                "Kareem Darwish."
            ],
            "title": "Arabizi detection and conversion to arabic",
            "venue": "arXiv preprint arXiv:1306.6755.",
            "year": 2013
        },
        {
            "authors": [
                "Long Doan",
                "Linh The Nguyen",
                "Nguyen Luong Tran",
                "Thai Hoang",
                "Dat Quoc Nguyen."
            ],
            "title": "PhoMT: A high-quality and large-scale benchmark dataset for Vietnamese-English machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods",
            "year": 2021
        },
        {
            "authors": [
                "Moussa Kamal Eddine",
                "Nadi Tomeh",
                "Nizar Habash",
                "Joseph Le Roux",
                "Michalis Vazirgiannis"
            ],
            "title": "Arabart: a pretrained arabic sequence-to-sequence model for abstractive summarization",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Eisele",
                "Yu Chen."
            ],
            "title": "MultiUN: A multilingual corpus from united nation documents",
            "venue": "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC\u201910), Valletta, Malta. European Language Resources Asso-",
            "year": 2010
        },
        {
            "authors": [
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed"
            ],
            "title": "ORCA: A Challenging Benchmark for Arabic Language Understanding",
            "year": 2023
        },
        {
            "authors": [
                "Mohamed Elmahdy",
                "Mark Hasegawa-Johnson",
                "Eiman Mustafawi."
            ],
            "title": "Development of a TV broadcasts speech recognition system for qatari Arabic",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation",
            "year": 2014
        },
        {
            "authors": [
                "Ali Fadel",
                "Ibraheem Tuffaha",
                "Bara\u2019 Al-Jawarneh",
                "Mahmoud Al-Ayyoub"
            ],
            "title": "Arabic text diacritization using deep neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Kamel Gaanoun",
                "Abdou Naira",
                "Anass Allak",
                "Imade Benelallam"
            ],
            "title": "Automatic Text Summarization for Moroccan Arabic Dialect Using an Artificial Intelligence Approach, pages 158\u2013177",
            "year": 2022
        },
        {
            "authors": [
                "Strobelt",
                "Nishant Subramani",
                "Wei Xu",
                "Diyi Yang",
                "Akhila Yerukola",
                "Jiawei Zhou."
            ],
            "title": "The GEM benchmark: Natural language generation, its evaluation and metrics",
            "venue": "pages 96\u2013120.",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Abhik Bhattacharjee",
                "Abinaya Mahendiran",
                "Alex Wang",
                "Alexandros Papangelis",
                "Aman Madaan",
                "Angelina Mcmillan-major",
                "Anna Shvets",
                "Ashish Upadhyay",
                "Bernd Bohnet"
            ],
            "title": "GEMv2: Multilingual NLG benchmarking in a single",
            "year": 2022
        },
        {
            "authors": [
                "Jian Guan",
                "Zhuoer Feng",
                "Yamei Chen",
                "Ruilin He",
                "Xiaoxi Mao",
                "Changjie Fan",
                "Minlie Huang."
            ],
            "title": "LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Tri Wahyu Guntara",
                "Alham Fikri Aji",
                "Radityo Eko Prasojo."
            ],
            "title": "Benchmarking multidomain EnglishIndonesian machine translation",
            "venue": "Proceedings of the 13th Workshop on Building and Using Comparable Corpora, pages 35\u201343, Marseille, France. Euro-",
            "year": 2020
        },
        {
            "authors": [
                "Nizar Habash",
                "David Palfreyman."
            ],
            "title": "ZAEBUC: An annotated Arabic-English bilingual writer corpus",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 79\u201388, Marseille, France. European Language Resources Association.",
            "year": 2022
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Todor Mihaylov",
                "Dimitrina Zlatkova",
                "Yoan Dinkov",
                "Ivan Koychev",
                "Preslav Nakov."
            ],
            "title": "EXAMS: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering",
            "venue": "Proceedings of the 2020",
            "year": 2020
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md Saiful Islam",
                "Kazi Samin",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar"
            ],
            "title": "Xl-sum: Large-scale multilingual abstractive summarization",
            "year": 2021
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Walaa Ismail",
                "Masun Nabhan Homsi."
            ],
            "title": "Dawqas: A dataset for arabic why question answering system",
            "venue": "Procedia Computer Science, 142:123\u2013131.",
            "year": 2018
        },
        {
            "authors": [
                "Aman Kumar",
                "Himani Shrotriya",
                "Prachi Sahu",
                "Amogh Mishra",
                "Raj Dabre",
                "Ratish Puduppully",
                "Anoop Kunchukuttan",
                "Mitesh M. Khapra",
                "Pratyush Kumar"
            ],
            "title": "IndicNLG benchmark: Multilingual datasets for diverse NLG tasks in Indic languages",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Barlas O\u011fuz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk."
            ],
            "title": "Mlqa: Evaluating cross-lingual extractive question answering",
            "venue": "arXiv preprint arXiv:1910.07475.",
            "year": 2019
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out.",
            "year": 2004
        },
        {
            "authors": [
                "Dayiheng Liu",
                "Yu Yan",
                "Yeyun Gong",
                "Weizhen Qi",
                "Hang Zhang",
                "Jian Jiao",
                "Weizhu Chen",
                "Jie Fu",
                "Linjun Shou",
                "Ming Gong",
                "Pengcheng Wang",
                "Jiusheng Chen",
                "Daxin Jiang",
                "Jiancheng Lv",
                "Ruofei Zhang",
                "Winnie Wu",
                "Ming Zhou",
                "Nan Duan"
            ],
            "title": "GLGE: A new",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Transactions",
            "year": 2020
        },
        {
            "authors": [
                "Yuval Merhav",
                "Stephen Ash."
            ],
            "title": "Design Challenges in Named Entity Transliteration",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 630\u2013640, Santa Fe, New Mexico, USA. Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Simon Mille",
                "Anya Belz",
                "Bernd Bohnet",
                "Thiago Castro Ferreira",
                "Yvette Graham",
                "Leo Wanner."
            ],
            "title": "The third multilingual surface realisation shared task (SR\u201920): Overview and evaluation results",
            "venue": "Proceedings of the Third Workshop on Multilingual Sur-",
            "year": 2020
        },
        {
            "authors": [
                "Behrang Mohit",
                "Alla Rozovskaya",
                "Nizar Habash",
                "Wajdi Zaghouani",
                "Ossama Obeid."
            ],
            "title": "The first QALB shared task on automatic text correction for Arabic",
            "venue": "Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing",
            "year": 2014
        },
        {
            "authors": [
                "Andrew Morris",
                "Viktoria Maier",
                "Phil Green"
            ],
            "title": "From wer and ril to mer and wil: improved evaluation measures for connected speech recognition",
            "year": 2004
        },
        {
            "authors": [
                "Hussein Mozannar",
                "Karl El Hajal",
                "Elie Maamary",
                "Hazem Hajj."
            ],
            "title": "Neural arabic question answering",
            "venue": "arXiv preprint arXiv:1906.05394.",
            "year": 2019
        },
        {
            "authors": [
                "Hamdy Mubarak."
            ],
            "title": "Dial2msa: A tweets corpus for converting dialectal arabic to modern standard arabic",
            "venue": "OSACT 3: The 3rd Workshop on Open-Source Arabic Corpora and Processing Tools, page 49.",
            "year": 2018
        },
        {
            "authors": [
                "banie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed."
            ],
            "title": "AraT5: Textto-text transformers for Arabic language generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed."
            ],
            "title": "TURJUMAN: A public toolkit for neural Arabic machine translation",
            "venue": "Proceedinsg of the 5th Workshop on OpenSource Arabic Corpora and Processing Tools with",
            "year": 2022
        },
        {
            "authors": [
                "Tarek Naous",
                "Wissam Antoun",
                "Reem Mahmoud",
                "Hazem Hajj."
            ],
            "title": "Empathetic BERT2BERT conversational model: Learning Arabic language generation with little data",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 164\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Tarek Naous",
                "Zahraa Bassyouni",
                "Bassel Mousi",
                "Hazem Hajj",
                "Wassim El Hajj",
                "Khaled Shaban."
            ],
            "title": "Open-domain response generation in low-resource settings using self-supervised pre-training of warmstarted transformers",
            "venue": "ACM Transactions on Asian",
            "year": 2023
        },
        {
            "authors": [
                "Tarek Naous",
                "Christian Hokayem",
                "Hazem Hajj."
            ],
            "title": "Empathy-driven arabic conversational chatbot",
            "venue": "Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 58\u201368.",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for",
            "year": 2002
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Uma Roy",
                "Noah Constant",
                "Rami Al-Rfou",
                "Aditya Barua",
                "Aaron Phillips",
                "Yinfei Yang."
            ],
            "title": "LAReQA: Language-agnostic answer retrieval from a multilingual pool",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Alla Rozovskaya",
                "Houda Bouamor",
                "Nizar Habash",
                "Wajdi Zaghouani",
                "Ossama Obeid",
                "Behrang Mohit."
            ],
            "title": "The second QALB shared task on automatic text correction for Arabic",
            "venue": "Proceedings of the Second Workshop on Arabic Natural Language Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Hassan Sajjad",
                "Ahmed Abdelali",
                "Nadir Durrani",
                "Fahim Dalvi."
            ],
            "title": "AraBench: Benchmarking dialectal Arabic-English machine translation",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5094\u20135107,",
            "year": 2020
        },
        {
            "authors": [
                "Yves Scherrer."
            ],
            "title": "TaPaCo: A corpus of sentential paraphrases for 73 languages",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6868\u20136873, Marseille, France. European Language Resources Association.",
            "year": 2020
        },
        {
            "authors": [
                "Djam\u00e9 Seddah",
                "Farah Essaidi",
                "Amal Fethi",
                "Matthieu Futeral",
                "Benjamin Muller",
                "Pedro Javier Ortiz Su\u00e1rez",
                "Beno\u00eet Sagot",
                "Abhishek Srivastava."
            ],
            "title": "Building a user-generated content North-African Arabizi treebank: Tackling hell",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyi Song",
                "Stephanie M Strassel",
                "Haejoong Lee",
                "Kevin Walker",
                "Jonathan Wright",
                "Jennifer Garland",
                "Dana Fore",
                "Brian Gainor",
                "Preston Cabe",
                "Thomas Thomas"
            ],
            "title": "Collecting natural sms and chat conversations in multiple languages: The bolt phase 2 corpus",
            "year": 2014
        },
        {
            "authors": [
                "Bashar Talafha",
                "Analle Abuammar",
                "Mahmoud AlAyyoub."
            ],
            "title": "Atar: Attention-based lstm for arabizi transliteration",
            "venue": "International Journal of Electrical and Computer Engineering, 11:2327\u20132334.",
            "year": 2021
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann."
            ],
            "title": "Parallel data, tools and interfaces in OPUS",
            "venue": "pages 2214\u20132218.",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Varab",
                "Natalie Schluter."
            ],
            "title": "MassiveSumm: a very large-scale, very multilingual, news summarisation dataset",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10150\u201310161, Online",
            "year": 2021
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyz-",
            "year": 2018
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "arXiv preprint arXiv:2010.11934.",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Yao",
                "Qingxiu Dong",
                "Jian Guan",
                "Boxi Cao",
                "Zhengyan Zhang",
                "Chaojun Xiao",
                "Xiaozhi Wang",
                "Fanchao Qi",
                "Junwei Bao",
                "Jinran Nie"
            ],
            "title": "CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark",
            "year": 2021
        },
        {
            "authors": [
                "Rabih Zbib",
                "Erika Malchiodi",
                "Jacob Devlin",
                "David Stallard",
                "Spyros Matsoukas",
                "Richard Schwartz",
                "John Makhoul",
                "Omar Zaidan",
                "Chris Callison-Burch."
            ],
            "title": "Machine translation of Arabic dialects",
            "venue": "Proceedings of the 2012 conference of the north amer-",
            "year": 2012
        },
        {
            "authors": [
                "Micha\u0142 Ziemski",
                "Marcin Junczys-Dowmunt",
                "Bruno Pouliquen"
            ],
            "title": "The united nations parallel corpus",
            "year": 2016
        },
        {
            "authors": [
                "included. A"
            ],
            "title": "Arabic Benchmarks AraBench. AraBench is an evaluation benchmark for dialectal Arabic to English machine translation (MT) introduced by (Sajjad et al., 2020)",
            "year": 2020
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2021) is a multi-task benchmark for evaluating the generalization capabilities of NLG in the English language. GLGE has eight English language generation datasets, covering four NLG tasks: data-to",
            "year": 2021
        },
        {
            "authors": [
                "text",
                "dialog",
                "table-to-text",
                "summarization"
            ],
            "title": "BanglaNLG. BanglaNLG is a benchmark designed for Bangala Bhattacharjee et al. (2023) comprising seven datasets across six NLG tasks: machine",
            "year": 2023
        },
        {
            "authors": [
                "Aryan",
                "Dravidian"
            ],
            "title": "IndicNLG involves the five following tasks: biography generation, news headline generation, sentence summarization, paraphrase generation, and question generation. MTG",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Natural language generation (NLG) systems attempt to produce coherent, contextually appropriate, and linguistically accurate human-like language. These systems have a wide range of applications in everyday life, including in recreation, education, health, etc. The recent rise of generative models has transformed these NLG systems, making them more relevant and engaging than before. Crucial to measuring the performance of NLG systems are high-quality benchmarks. In particular, they provide standardized frameworks for comparing and quantitatively assessing differ-\n1https://dolphin.dlnlp.ai/. \u22c6Equal contributions.\nent algorithms, models, and techniques. For NLG, benchmarks define specific criteria and metrics for evaluating performance, allowing for objectively gauging the strengths and limitations of different approaches and encouraging healthy competition. NLG benchmarks can also facilitate reproducibility and promote transparency across different studies, acting as a catalyst for advancement in the field.\nDespite of this significance, efforts for developing nuanced NLG benchmarks that can allow us to track and guide performance on particular languages remain limited. For Arabic, a wide collection of languages and diverse varieties, there is currently no sizeable benchmark that caters to the needs of the community. In this work, we present a large benchmark for Arabic, dubbed Dolphin, to bridge this gap. Our novel benchmark is carefully curated to represent real-world usage of Arabic at scale. Dolphin covers Classical Arabic (CA), a premodern standardized form of Arabic used for old\npoetry and religious discourse that continues to be employed for literary expression and oration, Modern Standard Arabic (MSA), a modern descendent of CA used in formal settings and in pan-Arab media, dialectal Arabic (DA), such as varieties used in everyday communication in the different Arab countries. Dolphin also encompasses text written in both Arabic and Latin scripts, the latter usually referred to as Arabizi. The benchmark is comprised of 13 different generation tasks based on 40 different datasets across 50 test splits, making it by far the largest Arabic NLG benchmark to date and among the largest for any group of languages.\nWe build Dolphin on top of exclusively public datasets, adding a number of newly developed datasets of our creation. This makes Dolphin accessible and easy to use. Our benchmark is accompanied by a modular leaderboard with a unified evaluation metric, i.e., a Dolphin score. The leaderboard is designed to serve as a central hub for tracking and showcasing the performance of NLG systems. It functions as a dynamic and transparent platform where users can submit their models to compare their results against the state-of-the-art approaches. It also encourages a culture of transparency and detailed model description.\nOverall, we make the following contributions: (1) We introduce a novel benchmark for Arabic NLG that is large, public, diverse, and inclusive. (2) We develop a dynamic leaderboard with a rich array of best design principles to facilitate the measurement of progress in the field. (3) We evaluate a wide host of Arabic and multilingual models on our benchmark, offering strong baselines. (4) We analyze our benchmark to identify gaps in existing work, hoping to help guide future directions. The rest of the paper is organized as follows: In Section 2, we provide an overview of related work. Section 3 introduces Dolphin design principles and task clusters. In Section 4, we present evaluations of the pretrained models on Dolphin, and discuss the results we acquire. We conclude in Section 5."
        },
        {
            "heading": "2 Related Works",
            "text": "Existing NLG benchmarks can be classified into three distinct categories: Arabic-specific, X-specific (where X refers to languages other than Arabic, such as English, Chinese, etc.), and multilingual benchmarks. In this section, we provide a brief overview of each category, highlighting their respective characteristics and scope. We offer more\ndetails on target languages, dataset sizes, and the breadth of tasks Dolphin covers in Appendix A. Table 1 and Figure 2 offer a summary of comparisons between Dolphin and other benchmarks. Arabic Benchmarks. Sajjad et al. (2020) introduce AraBench, a machine translation (MT) evaluation benchmark consisting of five datasets for dialectal Arabic to English translation. AraOpus20 (Nagoudi et al., 2022b) is another MT benchmark of parallel sentences between Arabic and 20 languages. Nagoudi et al. (2022a) introduce ArGen, an Arabic NLG benchmark composed of 19 datasets covering seven tasks. In comparison, Dolphin is much larger, composed of exclusively public datasets, and covers more varieties. It is also the only benchmark accompanied by a leaderboard. X-Specific Benchmarks. Liu et al. (2021) propose GLGE, a generation benchmark for English covering eight datasets across four tasks. CUGE (Yao et al., 2021) and LOT (Guan et al., 2022) are two Chinese benchmarks that cover both language understanding and generation tasks. BanglaNLG (Bhattacharjee et al., 2023) is a generation benchmark designed for Bangala comprising seven datasets across six tasks. Guntara et al. (2020) and Doan et al. (2021) present two MT benchmarks for Bahasa Indonesia and Vietnamese languages, respectively. Multi-Lingual NLG Benchmarks. The generation evaluation and metrics benchmark (GEMv1) (Gehrmann et al., 2021) is a multilingual benchmark environment for NLG. GEMv1 features 18 languages across 13 datasets spanning five tasks. Gehrmann et al. (2022) propose a second version, GEMv2, with a new set of datasets and\nmore challenging tasks. This new version supports 40 documented datasets in 51 languages. Other multilingual NLG benchmarks include CLSE (Chuklin et al., 2022), IndoNLG (Cahyawijaya et al., 2021), IndicNLG (Kumar et al., 2022), and MTG (Chen et al., 2022). As Figure 2 shows, compared to these benchmarks, Dolphin is the largest both in terms of the number of tasks and datasets. We now introduce Dolphin."
        },
        {
            "heading": "3 Dolphin Benchmark",
            "text": "Our objective is to provide a comprehensive and challenging benchmark for natural language generation that enables the assessment of language models and the tracking of progress in Arabic. To attain this objective, we develop Dolphin , considering several design principles that we will now elucidate."
        },
        {
            "heading": "3.1 Design Principles",
            "text": "Wide, diverse coverage. As our goal is to offer a demanding and diverse benchmark, we incorporate as many datasets from as many tasks as is feasible. This allows comprehensive evaluations of LMs. It also facilitates painting as complete a picture as possible of the limits of current methods across the different tasks. Reflecting this principle, our benchmark is large. It comprises 40 distinct datasets, covering 13 different task clusters. Public datasets. A major principle in choosing our datasets is public accessibility as it enables researchers to train and evaluate models without incurring expenses associated with acquiring private data. For this reason, all our 40 datasets are publicly available. Rich linguistic variability. In order to accurately reflect the multifaceted and diverse nature of Arabic languages and dialectal varieties, we strategically incorporate datasets collated from an array of sources, each corresponding to different sociological and orthographic traditions. Specifically, we construct Dolphin considering four major variants of Arabic: Arabizi (an unconventional method where Arabic is transcribed in Latin script); Classical Arabic (CA); Dialectal Arabic (DA) from a myriad of cities, countries, and regions; and Modern Standard Arabic (MSA). The heterogeneous nature of our datasets allows for a comprehensive representation of Arabic across wide linguistic nuances and orthographic traditions. Refer to Figure 1 for an illustrative depiction of the distribution of our datasets over various Arabic varieties for each specific task. Table 2 provides a quantitative description of these varieties in Dolphin. Standard evaluation metrics. Most generation tasks can be evaluated using traditional automated metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Both of these metrics evaluate the n-gram overlap between a reference text\nand the generated text. Nevertheless, in many tasks (e.g., question generation, open domain generation, title generation) there are multiple valid ways to produce a given text. In our benchmark, in addition to F1, BLEU, and ROUGE, we use several other evaluation metrics such MaxMatch (M2) (Dahlmeier and Ng, 2012) for grammatical error correction, and Character Error Rate (CER) (Morris et al., 2004) for diacritization.\nModular, interactive leaderboard. To support future research, we develop a public leaderboard that enables the evaluation of multilingual and Arabic LMs on Dolphin. Our leaderboard is interactive and provides detailed metadata about the corpora such as size, training, development, and test splits, data sources (e.g., URL, GitHub), and citations to publications. The leaderboard also offers details of language models assessed such as the number of parameters, epochs to conversion, pretraining and finetuning information, etc. We provide a screenshot from our leaderboard in Figure D.1. We now introduce each of the task clusters in Dolphin."
        },
        {
            "heading": "3.2 Task Clusters",
            "text": "Dolphin involves 50 test sets curated from 40 datasets. We arrange Dolphin into 13 task clusters, as follows: (1) machine translation, (2) codeswitching, (3) text summarisation, (4) news title generation, (5) question answering, (6) question generation, (7) transliteration, (8) paraphrasing, (9) text rewriting, (10) diacritization, (11) data-to-text, (12) dialogue generation, and (13) grammatical error correction. Appendix Table B.2 shows a summary of the data splits across datasets and task clusters in Dolphin. We present each task cluster in Dolphin next."
        },
        {
            "heading": "3.2.1 Machine Translation",
            "text": "The MT cluster is built around three tasks: (1) X \u2192 MSA. In this task, we test the ability of the models to translate from six foreign languages into MSA. We use the UN parallel corpus (Ziemski et al., 2016), a dataset covering the six official UN languages (i.e., Arabic, Chinese, English, French, Russian, and Spanish). The UN corpus consists of development and test sets only.2 For training, we randomly select 50K X-Arabic parallel sentences from the multilingual corpus MultiUN (Eisele and Chen, 2010) where X is a language from the six official languages. (2) Arabizi \u2192 X. The goal of this task is to translate from Arabizi dialectal text3 into one of two foreign languages French and English. For this, we use Darija (Outchakoucht and Es-Samaali, 2021) and NArabizi (Seddah et al., 2020). (3) Dialects \u2192 English. For this task, we focus on MT from six Arabic dialects into English using the MDP corpus (Bouamor et al., 2014). MDP is a human-translated collection of 1K sentences in Egyptian, Tunisian, Jordanian, Palestinian, and Syrian Arabic, in addition to English. For training, we use the 10K MSA-English manually translated sentences proposed by Bouamor et al. (2018) under a \u2018zero-shot\u2019 condition.4"
        },
        {
            "heading": "3.2.2 Code-Switching",
            "text": "The purpose of the code-switching (CS) task cluster is to translate Arabic dialect text that includes code-switching with a foreign language into that foreign language. For this, we create six new human-written (natural) code-switched parallel test datasets, under two tasks: (1) DIA-FR \u2192 FR. This consists of 300 code-switched Arabic-French tweets collected from Algerian, Moroccan, and Tunisian Twitter. (2) DIA-EN \u2192 EN. This is collected from Egyptian, Jordanian, and Palestinian Twitter and consists of 300 code-switched Arabic-English posts. For both of these DIA-FR and DIA-EN tasks, a human translation is performed by one native speaker from each dialect with semi-native English/French fluency. For these two tasks, we perform experiments under the zeroshot setting. That is, we use no actual code-\n24K sentences that are aligned across all official languages. 3Arabizi is the romanization of Arabic script (Darwish, 2013). In this task, we investigate the Algerian and Moroccan Arabizi.\n4Due to lexical overlap between Arabic dialects and MSA, this is not zero-shot in the strict sense of the word.\nswitched training data. Rather, we extract 50K MSA-English and MSA-French sentences from AraOPUS-20 (Nagoudi et al., 2022b) that we use for monolingual training. We then extract 50 pairs from each code-switched dialect pair for development and test on the 250 remainder sentences."
        },
        {
            "heading": "3.2.3 Text Summarization",
            "text": "For the text summarization (TS) cluster, we use the following five Arabic and multilingual (including Arabic) publicly available datasets: (1) MassiveSum (Varab and Schluter, 2021), (2) XLSum Hasan et al. (2021), (3) CrossSum (Bhattacharjee et al., 2021), (4) ANT (Chouigui et al., 2021), and (5) MarSum (Gaanoun et al., 2022)."
        },
        {
            "heading": "3.2.4 News Title Generation",
            "text": "The news title generation (NTG) task is about producing a suitable title for a given news article. That is, a title generation model is required to output a short grammatical sequence of words that are appropriate for the content of the article. For this, we use two datasets: (1) Arabic NTG (Nagoudi et al., 2022a), and (2) XLSum (Hasan et al., 2021).5"
        },
        {
            "heading": "3.2.5 Question Answering",
            "text": "For the QA cluster, we use seven publicly available QA datasets across four tasks. A summary of the QA cluster is in Appendix Table B.2. We also provide brief information about each task here. Extractive QA. We use four publicly available QA datasets: (1) The Arabic QA dataset ARCD (Mozannar et al., 2019) and the Arabic part of the following three multi-lingual QA test sets: (2) MLQA (Lewis et al., 2019), (3) XQuAD (Artetxe et al., 2020), and (4) TyDiQA (Artetxe et al., 2020). For all the extractive QA experiments, we finetune on the GoldP multilingual TyDiQAtrain (Artetxe et al., 2020) and evaluate on the test sets listed above. Retrieval QA. For this task, we use (5) LAReQA (Roy et al., 2020), a crosslingual retrieval QA dataset built by converting the extractive QA dataset XQuAD (Artetxe et al., 2020) into a retrieval task XQuAD-R. In our benchmark, we focus on the Arabic part of XQuAD-R (AraQuAD-R). Open-Domain QA. In this task, the goal is to answer fact-based questions in natural language. We\n5We note that XLSum (Hasan et al., 2021) has news articles annotated with summaries and titles. We use pairs of articles and titles to create the title generation data.\nadd (6) DAWQAS, an Arabic Why QA dataset (Ismail and Nabhan Homsi, 2018) to our QA cluster. Multi-choice QA. We also use (7) EXAMS (Hardalov et al., 2020), a cross-lingual multi-choice QA dataset that covers 26 languages (including Arabic). Since we only have this particular test set for Arabic, we follow Hardalov et al. (2020) in evaluating the models on EXAMS under a zero-shot setting.6"
        },
        {
            "heading": "3.2.6 Question Generation",
            "text": "The question generation (QG) cluster involves generating a question for a given passage (Gehrmann et al., 2021). The model is trained to generate simple questions relevant to passages along with their answers. For this cluster, we use (passage, answer, and question) triplets from five out of the seven QA question datasets described in Section 3.2.5.7"
        },
        {
            "heading": "3.2.7 Paraphrase",
            "text": "The main goal of this task is to produce for a given Arabic sentence a paraphrase with the same meaning. For this, we employ the following four datasets: (1) AraPara, a multi-domain Arabic paraphrase dataset (Nagoudi et al., 2022a), (2) ASEP, an Arabic SemEval paraphrasing dataset (Cer et al., 2017), (3) Arabic paraphrasing benchmark (APB) (Alian et al., 2019), and (4) the Arabic section of TaPaCo (Scherrer, 2020), a multilingual paraphrase corpus."
        },
        {
            "heading": "3.2.8 Transliteration",
            "text": "The task of transliteration (TS) is about converting a word or text from one writing system to another while preserving the pronunciation and sound of the original language. We create our TS component using three word-level datasets, as follows: (1) ANETA, an English-Arabic named entity transliteration and classification dataset proposed by Ameur et al. (2019). (2) ATAR (Talafha et al., 2021), a word-level parallel corpus containing human translations between Jordanian Arabizi8 and MSA. (3) NETransliteration (Merhav and Ash, 2018), a bilingual named entity (person names) transliteration dataset mined from Wikidata for English to each of Arabic, Hebrew, Japanese, Katakana, and Russian.\n6we use the multilingual part for Train and Dev, where no Arabic data is included, and blind-test on the Arabic test split.\n7We exclude the multi-choice QA EXAMS (Hardalov et al., 2020), the open-domain QA DAWQAS (Ismail and Nabhan Homsi, 2018).\n8An informal variant of Arabic spoken in Jordan"
        },
        {
            "heading": "3.2.9 Text Rewriting",
            "text": "The text rewriting (TR) cluster is about generating a text of the target style while preserving the content of the source input text. The TR cluster contains two tasks: (1) DIA \u2192 MSA. This task involves converting a text written in an Arabic dialect into MSA. For this, we use Dial2MSA (Mubarak, 2018). Dial2MSA is a parallel dialectal Arabic corpus for converting Egyptian, Maghrebi, Levantine, and Gulf dialects into MSA. (2) Gender Rewriting. We use the Arabic parallel gender corpus (APGC) proposed by Alhafni et al. (2022), where the task is to take a given input sentence written in one gender (e.g., male) to produce a target sentence that has the same meaning but employing the opposite gender (i.e., female)."
        },
        {
            "heading": "3.2.10 Diacritization",
            "text": "Arabic text diacritization (ATD) is the computational process of restoring missing diacritics or vowels to the orthographic word or a sequence of words (i.e., a sentence or a whole text). For this task, we use the Arabic diacritization dataset proposed by Fadel et al. (2019)."
        },
        {
            "heading": "3.2.11 Dialogue Response Generation",
            "text": "Dialogue response generation (DRG) is a humancomputer interaction task with the goal of automatically producing a human-like response given a dialogue context. In this cluster, we have two tasks: (1) MSA DRG. For this task, we use the Arabic empathetic chatbot (AEC) dataset (Naous et al., 2020). It contains open-domain utterances with their corresponding empathetic responses machine translated from English into MSA. (2) Dialectal DRG. We add the open-domain response generation in Arabic dialects proposed by Naous et al. (2023). Three native translators from the Levantine,\nEgyptian, and Gulf areas were asked to translate 1K utterance-response pairs from the English opendomain dialogues dataset DailyDialog (Li et al., 2017)."
        },
        {
            "heading": "3.2.12 Grammatical Error Correction",
            "text": "The task of grammatical error correction (GEC) is focused on analyzing written text, automatically pinpointing, and rectifying a variety of grammatical errors as illustrated by a typical instance of grammatical error correction and its manual rectification. In this cluster, we use three GEC datasets: (1-2) QALB. We use two datasets extracted from the QALB shared tasks from 2014 (Mohit et al., 2014) and 2015 (Rozovskaya et al., 2015). Both datasets are manually corrected collections of Arabic texts originating from online commentaries on Aljazeera articles written by native Arabic speakers (L1), as well as texts produced by learners of Arabic as a second language (L2). (3) ZAEBUC. A corpus that focuses on bilingual writers presented by Habash and Palfreyman (2022). It matches comparable texts in different languages written by the same writer on different occasions. The corpus is enhanced by adding multiple layered annotations, including manually corrected versions of the raw text, allowing us to use it for GEC."
        },
        {
            "heading": "3.2.13 Data2Text",
            "text": "The Data2Text (DT) task involves converting structured data like tables as input into descriptive texts without misrepresenting their contents, while sounding natural in writing (i.e., fluently describing this data as output). For the DT task cluster, we use the Arabic subset of the multilingual dataset MD2T proposed by Mille et al. (2020) during the third multilingual surface realization shared task. Table 3 shows examples from each task included in\nDolphin. We now introduce our strong baselines exploiting our benchmark."
        },
        {
            "heading": "3.3 Comparative Analysis with ARGEN.",
            "text": "Compared to the previous largest Arabic NLU benchmark, ARGEN (which we list in Table 1), Dolphin (Nagoudi et al., 2022a) exhibits several advantages. Specifically, we observe the following: Coverage. Dolphin boasts a significantly larger dataset pool (\u223c3X larger). In terms of the number of datasets, Dolphin comprises 40 datasets compared to only 13 datasets in ARGEN. Hence, Dolphin offers a total of 27 totally new datasets. Task clusters. Dolphin\u2019s reach also extends to a wider array of task clusters, encompassing 13 clusters as opposed to ARGEN\u2019s seven clusters. Dolphin introduces six novel tasks: Arabic text diacritization, dialogue response generation, data-totext conversion, grammatical error correction, text rewriting, and question answering. Availability. Dolphin\u2019s datasets are drawn exclusively from publicly available sources, while ARGEN involves several non-public datasets such as the machine translation datasets introduced by Zbib et al. (2012) and transliteration presented by Song et al. (2014). As such, Dolphin avoids issues ARGEN suffers from such as challenges with (i) public distribution of the data and (ii) ease of evaluation. Interactivity. Dolphin uniquely offers a benchmark leaderboard, a feature absent in ARGEN, providing real-time performance tracking and a dynamic evaluation environment."
        },
        {
            "heading": "4 Model Evaluation on Dolphin",
            "text": "In order to establish a conducive environment for meaningful comparisons on Dolphin, we offer a number of strong baselines for both finetuning and k-shot settings as described next."
        },
        {
            "heading": "4.1 Finetuned Models",
            "text": "For finetuning, we benchmark five different Arabic and multilingual models on Dolphin. These are AraT5 (Nagoudi et al., 2022a), of which we pretrain a new version that we refer to as AraT5v2, AraBART (Eddine et al., 2022), mBART (Liu et al., 2020), mT5 (Xue et al., 2020), and mT0 (Muennighoff et al., 2022). More information about these models, including our newly introduced AraT5v2, is in Appendix C. For all models, we finetune on the training data split (Train) for 20 epochs with an early stopping\nof 5 epochs, learning-rate of 5e \u2212 5, batch size of 16, and sequence length of 512.9 For all the experiments, we identify the best model on the respective development split (Dev) and blind testing on the test split (Test). We methodically evaluate each task cluster, ultimately reporting a single Dolphin score following e.g., Wang et al. (2018) and Elmadany et al. (2023). Dolphin score is simply the macro-average of the different scores across task clusters, where each task is weighted equally. Since some of our tasks are reported in metrics where lower numbers are better, we split our metric into DolphinL score (for tasks where lower \u2193 is better [i.e., CER]), and DolphinH score (for tasks where higher \u2191 is better [i.e., BLEU, F1, M2, and ROUGE]). Table 4 presents the results of all pretrained models on each task cluster of Dolphin independently using the relevant metric.\nDiscussion. As Table 4 shows, models dedicated to Arabic outperform multilingual models on tasks where higher is better (in DolphinH). We also note that AraT5v2 the new model we build on top of (Nagoudi et al., 2022a), achieves the best DolphinH and DolphinL, at 27.82 and 11.67, respectively. It is followed by AraBART with DolphinH of 26.44, where a higher score indicates better performance. Conversely, mT5 achieves a DolphinL of 12.42, which is considered better in the opposite scenario. We also note that AraT5v2 achieves the best results in 30 individual tasks out of 50, followed by AraBART and mT0, where each one excels in 11 and 8 individual tasks, respectively.10\nModel Computational Costs. We assess the computational efficiency of the Arabic and multilingual models we finetune. Figure 3 shows for each model the total time needed for convergence (under our 20 epochs constraint with a patience of 5) and the conversion epoch. AraBART is the fastest (2.07 hours), with an average of 10.58 epochs to convergence, followed by mT5, AraT5v2, mT0, and finally AraT5.\n9Except for GEC, where we use a seq length of 1, 024. 10We investigate why AraT5 achieves worst, in spite of being dedicated to Arabic, finding it to perform better with 100 epochs and a patience of 20 as Nagoudi et al. (2022a) report."
        },
        {
            "heading": "4.2 Few-Shot Evaluation.",
            "text": "We also carry out k-shot evaluations of both BLOOMZ11 (7.1B) (Muennighoff et al., 2022) and ChatGPT (gpt-3.5-turbo)12 on 12 different NLG tasks across 16 test sets extracted from Dolphin.13 To keep the cost manageable, we randomly sample a set of 200 examples from the test set of each task for evaluation. We then evaluate both models under 0-, 5-, and 10-shot settings. For all experiments, we set the temperature to zero to generate deterministic and reproducible results. We compare both models\u2019 performance to our best fully finetuned model, AraT5v2, blind-tested on the same sampled 200 examples. Discussion. Tables 5, shows that ChatGPT outperforms BLOOMZ in all the 16 NLG tasks under 0-, 5-, and 10-shot settings. The only exception is the text rewriting task in the 0-shot setting. It is worth mentioning that AraT5v2 outperforms both ChatGPT and BLOOMZ by 14 out of 16. However, ChatGPT (10-shot) achieves the highest score in both code-switching tasks, perhaps due to its multilingual pretraining data."
        },
        {
            "heading": "5 Conclusion",
            "text": "We presented Dolphin, a large and diverse benchmark for Arabic NLG composed of 40 datasets\n11BLOOMZ is finetuned on multiple tasks in 46 languages, including \u223c 1% Arabic.\n12We evaluate the version existing on March 1st, 2023. 13We only exclude the data-to-text task.\nthat are arranged in 13 tasks. Dolphin is designed to facilitate meaningful comparisons and encourage healthy competition in Arabic. We also provide an interactive leaderboard with a range of useful tools and detailed metadata to help situate future research in a rich context of information sharing. Dolphin datasets are all publicly available, which should facilitate the adoption and further development of the benchmark. In the future, we intend to build on top of Dolphin by extending it to more tasks and Arabic varieties."
        },
        {
            "heading": "6 Limitations",
            "text": "In spite of the diversity, wide-coverage, highquality datasets, accessibility, and challenging nature of Dolphin, it is not without limitations. In particular, we identify the following limitations.\n1. Coverage of Arabic Varieties. While we make efforts to incorporate tasks from all Arabic varieties, it is important to note that there is a lack of available downstream datasets from countries such as Djibouti, Mauritania, and Yemen. Consequently, these varieties are not currently included in Dolphin. We hope that the community will develop resources representing all Arab countries, including these, across the various tasks. We also hope that future versions of our benchmark will have extended dialectal coverage in ways that enhance its representation of the Arabic language and help foster technological inclusion.\n2. Machine-Translated Datasets. Dolphin includes two machine-translated\ndata, AEC (Naous et al., 2021) and AraPara (Nagoudi et al., 2022a)). While these datasets increase task coverage in Dolphin, the MT process may inadvertently introduce some biases. For example, MT can result in a narrow representation of language patterns and structures, leading to a limited understanding of the complexities and nuances of different languages. Additionally, benchmark datasets may not adequately capture the wide range of domains, genres, and styles that exist in real-world translation scenarios. This can limit the generalizability of models trained on such data, as they may struggle to handle unfamiliar or specialized content. We hope that future versions of Dolphin will involve real-world data that further complement (or even substitute) these translated datasets.\n3. Automated Evaluation. Although all NLP depends heavily on automated evaluation to speed up model development, automated methods have their limitations, especially for some tasks. That is, in addition to automated evaluation, some tasks may need human evaluation. In particular, we believe human evaluation can play a crucial role in NLG tasks such as open-domain dialogue generation. For example, it can capture the nuanced aspects of dialogue quality, such as coherence, relevance, and appropriateness. In addition, human evaluation can allow for a comprehensive assessment of the generated dialogues, taking into account contextual understanding, fluency, and overall user experience. This feedback is invaluable in refining and improving dialogue generation models, ensuring that they meet the high standards of human-like conversation."
        },
        {
            "heading": "7 Ethics Statement",
            "text": "Data Collection and Release. Dolphin is based on publicly available datasets that would not be possible without the hard work of a large number of researchers over the years. We are grateful for these efforts invested by pioneer colleagues. One downside of benchmarking could be that the original authors of the different datasets are not sufficiently acknowledged. In our work, we make sure that all publications of resources we use are properly cited, both by referencing these in this paper (Section 3) and highlighting them in our GitHub and\nleaderboard website.\n1. Data Privacy. Regarding data involved in Dolphin, we develop the benchmark using publicly available data. For this reason, we do not have significant privacy concerns. In addition, the new datasets we develop and release for code-switched machine translation have undergone manual inspection to ensure there is no unintended leak of privacy information in any of the samples.\n2. Intended Use. We believe our work will spur further research on studying language models on Arabic NLG benchmark. We create a publicly available leaderboard and benchmark several multilingual and Arabicdedicated SOTA models on Dolphin. The benchmark will facilitate a unified evaluation and pave the way for a healthy competition that could push SoTA on Arabic language generation.\n3. Potential Misuse and Bias. The datasets we collect to create Dolphin may contain potential harmful contents. Additionally, the models we evaluate might be exposed to bias and as a result may generate unintended contents. Therefore, we recommend that these datasets and models not be used in applications without careful prior consideration of potential misuse and bias."
        },
        {
            "heading": "Acknowledgements",
            "text": "We gratefully acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN-2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 435-2018-0576; 895-2020-1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada,14 UBC ARCSockeye.15 We thank the Google TFRC program for providing us with free TPU access.16"
        },
        {
            "heading": "A NLG Benchmarks",
            "text": "Existing NLG benchmarks can be classified into three distinct categories: Arabic-specific, X-specific (where X refers to languages other than Arabic, such as English, Chinese, and others), and multilingual benchmarks. In this section, we shall provide a brief overview of each category, highlighting their respective characteristics and scope. We will highlight aspects such as the target language, dataset size, and the breadth of tasks covered. This analysis is summarized in Table 1 and Figure 2. The current NLG benchmarks can be divided into three main groups: benchmarks that focus on Arabic, benchmarks that focus on languages other than Arabic (X-specific), and benchmarks that cover multiple languages. In this section, we will give a brief summary of each category, emphasizing their unique features and scope. We will discuss factors like the target language, dataset size, and the range of tasks included.\nA.1 Arabic Benchmarks AraBench. AraBench is an evaluation benchmark for dialectal Arabic to English machine translation (MT) introduced by (Sajjad et al., 2020). It consists of five publicly available datasets: ArabicDialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech\ncorpus (Elmahdy et al., 2014), and the English Bible translated into MSA, Tunisian, and Morocco.17 AraOPUS-20. This is an MT benchmark proposed by Nagoudi et al. (2022b). It consists of parallel bitext between Arabic and 20 languages extracted from the OPUS publicly available corpora (Tiedemann, 2012). The languages paired with Arabic include high-resource languages such as English, French, and Spanish and low-resource ones such as Cebuano,18 Tamashek,19 and Yoruba.20 ARGEN. The ARabic natural language GENeration (ARGEN) benchmark was introduced by Nagoudi et al. (2022a). It is composed of 19 datasets and covers the seven tasks: machine translation, code-switched text translation, summarization, news title generation, question generation, paraphrasing, and transliteration.\nA.2 X-Specific Benchmarks GLGE. The General Language Generation Evaluation(GLGE) by Liu et al. (2021) is a multi-task benchmark for evaluating the generalization capabilities of NLG in the English language. GLGE has eight English language generation datasets, covering four NLG tasks: data-totext, dialog, table-to-text, and summarization. BanglaNLG. BanglaNLG is a benchmark designed for Bangala Bhattacharjee et al. (2023) comprising seven datasets across six NLG tasks: machine translation, text summarization, question answering, dialogue generation, headline generation, and cross-lingual summarization. CUGE. The Chinese Language Understanding Generation Evaluation Benchmark Yao et al. (2021) covers both language understanding and generation. The language generation collection contains nine datasets across eight tasks. The tasks are open-domain question answering, document retrieval, summarization, data-to-text, knowledgedriven conversation, machine translation, crosslingual text summarization, and mathematical computation. The benchmark also covers the tasks of grammatical error correction and reverse dictionary generation, but treats these under the NLU component.\n17The United Bible Societies https://www.bible.com 18Language spoken in the southern Philippines 19Tamashek is a variety of Tuareg, a Berber macro-language widely spoken by nomadic tribes across North Africa countries.\n20Yoruba is a language spoken in West Africa, primarily in Southwestern Nigeria.\nBahasa Indonesia. The Bahasa Indonesia language has over 200M active speakers, yet it is still considered a low-resource language. To overcome this problem, (Guntara et al., 2020) introduced a machine translation benchmark with 14 datasets across four domains: news, religion, conversation, and general. PhoMT. Doan et al. (2021) introduces a new Vietnamese-English parallel dataset that is larger and of higher quality than the existing benchmark corpus. The authors conduct experiments to evaluate various translation models on the new dataset and find that the best performance is achieved by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. LOT. The LOng Text understanding and generation benchmark targets Chinese long text modeling in a story-centric manner Guan et al. (2022). LOT combines two comprehension tasks and twogeneration tasks. The two generation tasks are commonsense reasoning and discourse structure.\nA.3 Multi-Lingual NLG Benchmarks\nIndoNLG. IndoNLG covers three low resources languages widely spoken in Indonesia: Indonesian, Javanese, and Sundanese Cahyawijaya et al. (2021). It consists of ten distinct datasets, encompassing four tasks. These are summarization, question answering, chit-chat, and machine translation. CLSE. The Corpus of Linguistically Significant Entities Chuklin et al. (2022) is a multilingual named entities corpus that covers 34 languages, 74 semantic classes, and 222 distinguishable linguistic signatures. The authors also developed an expanded version of the Schema-Guided Dialog Dataset (SG-CLSE) to illustrate one of the potential uses of CLSE in three languages: French, Marathi, and Russian. GEMv1. The Generation Evaluation and Metrics benchmark (Gehrmann et al., 2021) is a multilingual benchmark environment for NLG. GEM features 18 languages across 13 datasets spanning five NLG tasks: data-to-text, dialog response generation, reasoning, summarization, and simplification.21 GEMv2. Gehrmann et al. (2022) propose a second version, GEMv2, styled after GEMv1 with a new set of datasets and more challenging tasks. This new version supports 40 documented datasets in 51 languages. It introduces a modular infrastructure\n21Two of the datasets do not include English at all.\nfor datasets and models, with an online evaluation process that collects model outputs and computes metrics for all datasets. GEMv2 is built around nine NLG tasks data-to-text, dialog response generation, paraphrasing, generative question answering, question generation, reasoning, slide generation, simplification, and summarization. IndicNLG. The first benchmark for Indic languages Kumar et al. (2022) covers 11 Indic languages belonging to two language families: IndoAryan and Dravidian. IndicNLG involves the five following tasks: biography generation, news headline generation, sentence summarization, paraphrase generation, and question generation. MTG. Chen et al. (2022) introduce the Multilingual Text Generation to promote knowledge transfer and cross-lingual generation between arbitrary language pairs. MTG contains 400K of humanly annotated data samples in five languages, covering four generation tasks. These are story generation, question generation, title generation, and text summarization."
        },
        {
            "heading": "B Dolphin Tasks",
            "text": ""
        },
        {
            "heading": "C Arabic and Multilingual S2S LLMs",
            "text": "In this section, we list the Arabic and multilingual sequence-to-sequence (S2S) pretrained LMs we finetune on Dolphin. AraT5. (Nagoudi et al., 2022a) is an adaptation of the T5 model specifically designed for the Arabic language. It is pre-trained on a large (248GB of Arabic text) diverse (MSA and Arabic dialects) dataset to effectively handle different Arabic tasks. In addition to Arabic, AraT5\u2019s vocabulary covers 11 other languages. In this work, we evaluate a new in-house version of AraT5 dubbed AraT5v2. AraT5v2. Our analysis shows that AraT5 requires a large number of epochs to converge, making it an expensive model. For this reason, we pretrain a new version of the model from scratch exploiting a larger (\u223c 400GB) and more diverse pretraining dataset than used by (Nagoudi et al., 2022a). As we show in our results, the new model converges faster than AraT5 and achieves better results under our cap of 20 epochs for finetuning across all models. AraBART. (Eddine et al., 2022) is a model based on the encoder-decoder BART base architecture (Lewis et al., 2020), featuring six encoder and 6 decoder layers. It is pretrained on the same corpus as AraBERT (Antoun et al., 2020), with reversed preprocessing for more natural text gener-\nation. AraBART is designed for various NLP tasks, demonstrating robust performance across different tasks in the Arabic language. mBART. A multilingual encoder-decoder model proposed by Liu et al. (2020). mBART is pretrained by denoising full texts in 50 languages, including Arabic. Then, it is finetuned on parallel MT data contains a total of 230M parallel sentences under three settings: individually toward English and vice versa (i.e., many-to-English, and Englishto-many), or between multiple languages simultaneously (many-to-many). mT5. (Xue et al., 2020) is a multilingual variant of the of Text-to-Text Transfer Transformer model (T5) (Raffel et al., 2019) that covers 101 languages. It is pretrained on a new Common Crawl-based dataset (\u223c 26.76TB), designed to achieve SOTA performance on a variety of multilingual NLP tasks such as question answering, document summarization, and MT. mT0. (Muennighoff et al., 2022) is a group of sequence-to-sequence models ranging in size between 300M to 13B parameters trained to investigate the cross-lingual generalization through multitask finetuning. The models are finetuned from preexisting mT5 (Xue et al., 2020) multilingual language models using a cross-lingual task dataset called xP3. mT0 models can execute human instructions in many languages without any prior training.\nD Leaderboard"
        }
    ],
    "title": ": A Challenging and Diverse Benchmark for Arabic NLG",
    "year": 2023
}