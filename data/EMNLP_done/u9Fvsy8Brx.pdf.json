{
    "abstractText": "Multilingual sequence-to-sequence models perform poorly with increased language coverage and fail to consistently generate text in the correct target language in few-shot settings. To address these challenges, we propose mmT5, a modular multilingual sequence-to-sequence model. mmT5 utilizes language-specific modules during pre-training, which disentangle language-specific information from language-agnostic information. We identify representation drift during fine-tuning as a key limitation of modular generative models and develop strategies that enable effective zero-shot transfer. Our model outperforms mT5 at the same parameter sizes by a large margin on representative natural language understanding and generation tasks in 40+ languages. Compared to mT5, mmT5 raises the rate of generating text in the correct language under zero-shot settings from 7% to 99%, thereby greatly alleviating the source language hallucination problem.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonas Pfeiffer"
        },
        {
            "affiliations": [],
            "name": "Francesco Piccinno"
        },
        {
            "affiliations": [],
            "name": "Massimo Nicosia"
        },
        {
            "affiliations": [],
            "name": "Xinyi Wang"
        },
        {
            "affiliations": [],
            "name": "Machel Reid"
        },
        {
            "affiliations": [],
            "name": "Sebastian Ruder"
        }
    ],
    "id": "SP:8ee9ebbf90ba74200c2f796f8cc850ea7a5c255b",
    "references": [
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Anna Korhonen",
                "Ivan Vulic."
            ],
            "title": "Composable sparse fine-tuning for crosslingual transfer",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27,",
            "year": 2022
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "MAD-G: Multilingual adapter generation for efficient cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "MAD-G: Multilingual adapter generation for efficient cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u2013 4637, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Ankur Bapna",
                "Orhan Firat."
            ],
            "title": "Simple, scalable adaptation for neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP",
            "year": 2019
        },
        {
            "authors": [
                "Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances",
            "year": 2020
        },
        {
            "authors": [
                "Isaac Caswell",
                "Theresa Breiner",
                "Daan van Esch",
                "Ankur Bapna."
            ],
            "title": "Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6588\u20136608, Barcelona,",
            "year": 2020
        },
        {
            "authors": [
                "Ethan C. Chau",
                "Lucy H. Lin",
                "Noah A. Smith."
            ],
            "title": "Parsing with multilingual bert, a small treebank, and a small corpus",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Vincent S. Chen",
                "Sen Wu",
                "Alexander J. Ratner",
                "Jen Weng",
                "Christopher R\u00e9."
            ],
            "title": "Slice-based learning: A programming model for residual learning in critical data slices",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Process-",
            "year": 2019
        },
        {
            "authors": [
                "Lewkowycz",
                "Erica Moreira",
                "Rewon Child",
                "Oleksandr Polozov",
                "Katherine Lee",
                "Zongwei Zhou",
                "Xuezhi Wang",
                "Brennan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "Palm: Scaling language",
            "year": 2023
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Dario Stojanovski",
                "Alexander Fraser."
            ],
            "title": "Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Eunsol Choi",
                "Michael Collins",
                "Dan Garrette",
                "Tom Kwiatkowski",
                "Vitaly Nikolaev",
                "Jennimaria Palomaki."
            ],
            "title": "TyDi QA: A benchmark for informationseeking question answering in typologically diverse languages",
            "venue": "Transactions of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Proceedings of the 58th Annual",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Proceedings of the 58th Con-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: Evaluating cross-lingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2019
        },
        {
            "authors": [
                "Dheeru Dua",
                "Shruti Bhosale",
                "Vedanuj Goswami",
                "James Cross",
                "Mike Lewis",
                "Angela Fan."
            ],
            "title": "Tricks for training sparse translation models",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Constantin Eichenberg",
                "Sidney Black",
                "Samuel Weinbach",
                "Letitia Parcalabescu",
                "Anette Frank."
            ],
            "title": "MAGMA \u2013 multimodal augmentation of generative models through adapter-based finetuning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2416\u2013",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer."
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "Journal of Machine Learning Research, 23:120:1\u2013120:39.",
            "year": 2022
        },
        {
            "authors": [
                "Xavier Garcia",
                "Noah Constant",
                "Ankur Parikh",
                "Orhan Firat."
            ],
            "title": "Towards continual learning for multilingual machine translation via vocabulary substitution",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Mike Lewis",
                "Ari Holtzman",
                "Noah A. Smith",
                "Luke Zettlemoyer."
            ],
            "title": "Demix layers: Disentangling domains for modular language modeling",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Mubasshir",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "XL-sum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzkebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameterefficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,",
            "year": 2022
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Sneha Kudugunta",
                "Yanping Huang",
                "Ankur Bapna",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "Minh-Thang Luong",
                "Orhan Firat."
            ],
            "title": "Beyond distillation: Task-level mixture-ofexperts for efficient inference",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual",
            "year": 2021
        },
        {
            "authors": [
                "Anne Lauscher",
                "Olga Majewska",
                "Leonardo F.R. Ribeiro",
                "Iryna Gurevych",
                "Nikolai Rozanov",
                "Goran Glava\u0161."
            ],
            "title": "Common sense or world knowledge? investigating adapterbased knowledge injection into pretrained transformers",
            "venue": "Proceedings of Deep Learning Inside Out (DeeLIO): The",
            "year": 2020
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Hang Le",
                "Juan Miguel Pino",
                "Changhan Wang",
                "Jiatao Gu",
                "Didier Schwab",
                "Laurent Besacier."
            ],
            "title": "Lightweight adapter tuning for multilingual speech translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder."
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, De-",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Sebastian Ruder",
                "Mostafa Dehghani",
                "James Henderson."
            ],
            "title": "Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Naman Goyal",
                "Xi Lin",
                "Xian Li",
                "James Cross",
                "Sebastian Riedel",
                "Mikel Artetxe."
            ],
            "title": "Lifting the curse of multilinguality by pre-training modular transformers",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterFusion: Non-destructive task composition for transfer learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti."
            ],
            "title": "Modular deep learning",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654\u20137673, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "UNKs Everywhere: Adapting Multilingual Language Models to New Scripts",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Online, November ,",
            "year": 2021
        },
        {
            "authors": [
                "Jerin Philip",
                "Alexandre Berard",
                "Matthias Gall\u00e9",
                "Laurent Besacier."
            ],
            "title": "Monolingual adapters for zero-shot neural machine translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4465\u20134470, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Telmo Pires",
                "Robin M. Schmidt",
                "Yi-Hsiu Liao",
                "Stephan Peitz."
            ],
            "title": "Learning language-specific layers for multilingual machine translation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Clifton Poth",
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Iryna Gurevych."
            ],
            "title": "What to pre-train on? efficient intermediate task selection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research, 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi."
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach,",
            "year": 2017
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi."
            ],
            "title": "Efficient parametrization of multi-domain deep neural networks",
            "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8119\u20138127.",
            "year": 2018
        },
        {
            "authors": [
                "Andreas R\u00fcckl\u00e9",
                "Jonas Pfeiffer",
                "Iryna Gurevych."
            ],
            "title": "Multicqa: Zero-shot transfer of self-supervised text matching models on a massive scale",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder."
            ],
            "title": "An overview of multi-task learning in deep neural networks",
            "venue": "arXiv preprint.",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Noah Constant",
                "Jan Botha",
                "Aditya Siddhant",
                "Orhan Firat",
                "Jinlan Fu",
                "Pengfei Liu",
                "Junjie Hu",
                "Dan Garrette",
                "Graham Neubig",
                "Melvin Johnson."
            ],
            "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation",
            "venue": "Proceedings of the 2021 Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc V. Le",
                "Geoffrey E. Hinton",
                "Jeff Dean."
            ],
            "title": "Outrageously large neural networks: The sparselygated mixture-of-experts layer",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon,",
            "year": 2017
        },
        {
            "authors": [
                "Asa Cooper Stickland",
                "Alexandre Berard",
                "Vassilina Nikoulina."
            ],
            "title": "Multilingual domain adaptation for NMT: decoupling language and domain information with adapters",
            "venue": "Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-",
            "year": 2021
        },
        {
            "authors": [
                "Asa Cooper Stickland",
                "Iain Murray."
            ],
            "title": "BERT and pals: Projected attention layers for efficient adaptation in multitask learning",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Ahmet \u00dcst\u00fcn",
                "Alexandre Berard",
                "Laurent Besacier",
                "Matthias Gall\u00e9."
            ],
            "title": "Multilingual unsupervised neural machine translation with denoising adapters",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Ahmet \u00dcst\u00fcn",
                "Arianna Bisazza",
                "Gosse Bouma",
                "Gertjan van Noord."
            ],
            "title": "UDapter: Language adaptation for truly Universal Dependency parsing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2302\u20132315, Online. Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Marko Vidoni",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "Orthogonal language and task adapters in zero-shot cross-lingual transfer",
            "venue": "arXiv preprint.",
            "year": 2020
        },
        {
            "authors": [
                "Tu Vu",
                "Aditya Barua",
                "Brian Lester",
                "Daniel Cer",
                "Mohit Iyyer",
                "Noah Constant."
            ],
            "title": "Overcoming catastrophic forgetting in zero-shot cross-lingual generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9279\u20139300, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Ruize Wang",
                "Duyu Tang",
                "Nan Duan",
                "Zhongyu Wei",
                "Xuanjing Huang",
                "Jianshu Ji",
                "Guihong Cao",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "K-adapter: Infusing knowledge into pre-trained models with adapters",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyi Wang",
                "Yulia Tsvetkov",
                "Sebastian Ruder",
                "Graham Neubig."
            ],
            "title": "Efficient test time adapter ensembling for low-resource language varieties",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730\u2013737, Punta Cana, Dominican Republic. Association",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Wu",
                "Mark Dredze."
            ],
            "title": "Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120\u2013 130, Online",
            "venue": "Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-totext transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Ankur Bapna",
                "Rico Sennrich",
                "Orhan Firat."
            ],
            "title": "Share or not? learning to schedule language-specific capacity for multilingual translation",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\nMultilingual pre-trained models (Conneau et al., 2020a; Xue et al., 2021) have demonstrated impressive performance on natural language understanding (NLU) tasks across different languages (Hu et al., 2020; Ruder et al., 2021). These models are typically trained on large amounts of unlabeled data in hundreds of languages. Recent large language models (Brown et al., 2020; Chowdhery et al., 2023) display surprising multilingual capabilities despite being pre-trained predominantly on English data. However, all of these models share a key limitation: representations of all languages compete for the model\u2019s limited capacity. As a result, models perform poorly with an increasing number of pre-training languages and on languages with less pre-training data. This is also known as the \u201ccurse of multilinguality\u201d (Conneau et al., 2020a).\nNatural language generation (NLG) tasks present another challenge for current multilingual models, which may overfit to the training languages and partially forget their generation ability in the target language (Vu et al., 2022), generating text with the correct meaning in the wrong language. We refer to this as the \u201csource language hallucination problem\u201d.\nTo address these two limitations, we propose the modular multilingual T5 (mmT5, Figure 1), the first modular multilingual generative model. During pre-training, mmT5 allocates a small amount of language-specific parameters to increase capacity for multilingual modeling. At fine-tuning time, we freeze the language-specific modules while tuning the shared parameters, allowing direct adaptation to a target language by swapping to the corresponding language-specific module.\nHowever, we observe an additional challenge for mmT5: the fine-tuned shared representations\nmay drift away from the frozen modular representations in the decoder. The modular model is thus susceptible to generating text in the incorrect language, similar to its non-modular counterparts. To ameliorate this, we propose to freeze a subset of shared decoder parameters, which shows large improvements in zero-shot cross-lingual generation for modular generative models.\nIn general, we find that mmT5 is an effective model that overcomes the two limitations of multilingual sequence-to-sequence models: 1) mmT5 alleviates the curse of multilinguality by adding additional model capacity to different languages during pre-training. It outperforms both standard baselines as well as mT5 (Xue et al., 2021) at the same parameter sizes on a representative set of multilingual NLU and NLG tasks; 2) mmT5 resolves the source language hallucination problem with impressive ability on zero-shot cross-lingual text generation. Our analysis (\u00a76.4) shows that mT5 only generates text in the target language 7% of the time for a zero-shot multilingual summarization task, while mmT5 generates text in the correct language for 99% of examples.\n2 Related work\nModular language models Much work has focused on post-hoc modularity of pre-trained multilingual models, i.e., modular representations are added to existing dense models. The most commonly used modules are known as adapters (Rebuffi et al., 2017, 2018; Houlsby et al., 2019). They enable specialization to new data settings (Chen et al., 2019; R\u00fcckl\u00e9 et al., 2020), combination of new and existing knowledge (Stickland and Murray, 2019; Wang et al., 2021a; Pfeiffer et al., 2021a; Lauscher et al., 2020a; Mahabadi et al., 2021b; Poth et al., 2021), and adaptation to new crosslingual (Pfeiffer et al., 2020, 2021b; \u00dcst\u00fcn et al., 2020; Vidoni et al., 2020; Ansell et al., 2021a, 2022; Wang et al., 2021b) and NMT scenarios (Bapna and Firat, 2019; Philip et al., 2020; Chronopoulou et al., 2020; Le et al., 2021; \u00dcst\u00fcn et al., 2021; Stickland et al., 2021; Garcia et al., 2021; Dua et al., 2022; Zhang et al., 2021; Pires et al., 2023).\nOur approach, in contrast, uses modularity a priori, i.e., modularity is integrated into the module architecture as an inductive bias. Such modularity is similar to parameter sharing strategies commonly defined in multi-task learning (Ruder, 2017) as well as to mixture-of-experts approaches (MoE; Shazeer\net al., 2017), which have been used to scale models to trillion parameters (Fedus et al., 2022) and for domain-specific pre-training of LMs (Gururangan et al., 2022). The most related work to ours is X-Mod (Pfeiffer et al., 2022), which pre-trains an encoder-only BERT-style model in a modular fashion. Their model, however, cannot be used for natural language generation and underperforms our model on NLU tasks (see Section 4).\nLimitations of multilingual language models State-of-the-art multilingual LMs are pre-trained on large amounts of multilingual data in around 100 languages. Prior work has demonstrated, however, that models\u2019 performance deteriorates with increasing language coverage given the same fixed capacity, known as the curse of multilinguality (Conneau et al., 2020b). Prior studies also found that models perform poorly on languages that are underrepresented in pre-training (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020b; Artetxe et al., 2020; Pfeiffer et al., 2020, 2021b; Chau et al., 2020; Ponti et al., 2020). For natural language generation, multilingual models have been observed to overfit to the source language and fail to generate text consistently in the correct target language (Vu et al., 2022).\n3 mmT5\nStandard multilingual models update the same model parameters for hundreds of languages during pre-training, resulting in the curse of multilinguality where different languages compete for the limited model capacity (Conneau et al., 2020a). We propose mmT5, the first modular sequenceto-sequence multilingual model that allocates language specific modules during pre-training. In this section, we discuss the architecture of mmT5, its training and fine-tuning methods, and our strategies to resolve the source language hallucination problem with mmT5.\n3.1 Modeling First, we describe the overall architecture of mmT5. We augment a standard Transformer encoderdecoder model with language-specific modules at every transformer layer (see Figure 1). The selection of modules (i.e., fixed routing; Pfeiffer et al., 2023) is performed via the language ID provided with each example1; all tokens of an example are\n1Our pre-training data contains such metadata; alternatively, automatic language ID methods can be used (see \u00a76.4).\npassed through the same language-specific module. We use bottleneck adapters as the language-\nspecific module because they perform better at smaller model sizes compared to other modular methods such as continuous prompts (Mahabadi et al., 2021a; He et al., 2022). We place a module after the feed-forward component in each layer. In contrast to Pfeiffer et al. (2022) that only experimented with encoder-only models, we focus on a more general sequence-to-sequence model following the T5 architecture (Raffel et al., 2020).\nWe add N \u00d7 L modular components to the T5 architecture where L is the number of layers of the model and N corresponds to the number of languages which the model is pre-trained on. The transformer weights are shared across languages while the modular component provides the model with language-specific capacity. During a forward pass, each input is first passed through the shared transformer weights and then routed through the corresponding language-specific module based on the language of the input. We follow this procedure for all transformer layers until the representations are passed to the shared prediction head.\n3.2 Modular Pre-training, Fine-tuning, and Inference\nWe pre-train both language-specific modules and shared parameters jointly. During fine-tuning, we freeze all language-specific modules and only update the shared parameters. This paradigm allows us to more effectively adapt the fine-tuned model to any of the languages included in the pre-training data by simply switching to the corresponding language-specific module. At inference, the module corresponding to the target language is used together with the fine-tuned shared parameters.\n3.3 Overcoming Modular Representation Drift\nWhen fine-tuning the modular model for transfer settings in \u00a75, we observe a scenario of modular representation drift: we find that the shared parameters that are updated during task-specific training drift away from the modular parameters and become thus less compatible with modules that are used for inference. In practice, this leads to a loss of compositional generalization where the modular model generates text in the incorrect language, similar to its non-modular counterparts (Vu et al., 2022); see \u00a76.4.\nIn order to ameliorate this drift, we propose to freeze parts of the model, with a focus on the decoder. We find that freezing the decoder feedforward parameters provides the biggest benefit (see \u00a76.1 for the detailed ablation) and almost completely eliminates the source language hallucination problem in modular models.2\n4 Experiments\nPre-training Details We pre-train mmT5 on data from 100 languages in mC4 (Xue et al., 2021) following the general pre-training setup of mT5 (Xue et al., 2021), if not specified otherwise. We pre-train mmT5 at two model sizes: small (300M parameters), and base (580M parameters). We train model variants with an input sequence length of 1024 and a target sequence length of 256 for 1M update steps with a batch size of 1024. The bottleneck size of each module is half of the hidden dimension of the transformer model. For instance, as the base variant has a hidden dimension of 768, we set the bottleneck size to 384.3 We additionally pretrain a non-modular variant of our modular model, mT5S , where all parameters are shared across all languages. The mT5S variant uses exactly the same hyper-parameters and pre-training setup as mmT5. To ensure that the models are directly comparable and have exactly the same number of parameters, we add shared bottleneck layers to mT5S in the same configuration as in mmT5.\nExperimental setting We conduct experiments across datasets in zero-shot cross-lingual transfer and multilingual training scenarios. For zero-shot cross-lingual transfer, we train the model on a subset of languages (e.g. only English) and evaluate the model on held-out data of the same task in other languages. In multilingual training, we finetune the model on multiple languages of the same task, and evaluate the model on the same set of languages. As the language-specific modular components are replaced at inference time, we do not update the parameters of the modular components. We do the same for our shared model variants, in order for the number of trainable parameters to be equal for comparable scenarios.4 For each dataset, we select the best model checkpoint based on performance on the validation set.\n2We observe little benefit to freezing decoder parameters in non-modular models, however.\n3We analyze the impact of bottleneck sizes in \u00a76.2. 4Here, we follow the procedure of Pfeiffer et al. (2022).\nEvaluation Tasks For zero-shot cross-lingual transfer, we evaluate on the XQuAD (Artetxe et al., 2020) and TyDi QA GoldP (Clark et al., 2020) question answering datasets; on the XNLI (Conneau et al., 2018) natural language inference dataset; on XL-Sum (Hasan et al., 2021) for summarization;5\nand MASSIVE (FitzGerald et al., 2023) for semantic parsing.6 We mainly fine-tune the model on English training data and evaluate on the target languages (Hu et al., 2020). For XL-Sum, we additionally evaluate in a multi-source zero-shot transfer setting where we train jointly on data in Arabic, English, Japanese and Chinese (XL-Sumar,en,ja,zh).\nFor multilingual training, we evaluate on semantic parsing (MASSIVE) and summarization (XLSum) datasets. For each dataset, we fine-tune and evaluate the model on all languages jointly.\nBaselines Our main comparison method is mT5S , a shared model that is pre-trained with the same hyper-parameters, setup, and number of parameters as our modular model. We also compare to the published results of the mT5 encoderdecoder model (Xue et al., 2021). In addition, we compare to several encoder-only models including mBERT (Devlin et al., 2019), X-Mod (Pfeiffer et al., 2022), and XLM-R (Conneau et al., 2020b). Encoder-only models are generally smaller as they lack a decoder but cannot easily be used for generation tasks. We provide an overview of the model sizes of the baselines and our method in Table 1.\n5We do not evaluate on Oromo, Kirundi, Pidgin, and Tigrinya as they were not seen during pre-training.\n6We do not evaluate on Hebrew and Tagalog as they were not seen during pre-training.\nDecoder Freezing Configurations To overcome the modular representation drift described in \u00a73.3, we experiment with different configurations of freezing parts of the model when fine-tuning the model on a downstream task. We experiment with freezing the LayerNorm (LN), self-attention (Att), cross-attention (CrossAtt) and feed-forward component (FFN) in the encoder (Enc) and decoder (Dec) parts of the transformer model. We ablate freezing configurations in \u00a76.1 and report test results of the freezing configuration that performs best on the dev set for each dataset for mmT5. For dense models, we observe no impact with freezing and report results using full fine-tuning.\n5 Results\n5.1 Pre-training\nWe first compare the language modeling perplexities of different model sizes for mmT5 and mT5S during pre-training in Figure 2. We find that mmT5 significantly outperforms its fully shared counterpart during the early stages of pre-training and maintains the gap throughout the pre-training process. From an efficiency perspective, mmT5 only requires 282k and 220k update steps respectively for the small and base versions to achieve the same final perplexity as the mT5S models at 1M update steps. This corresponds to a \u2248 4\u00d7 efficiency boost when training a modular multilingual model compared to a fully dense one.\n5.2 Fine-tuning\nWe present our main results on the test sets for zero-shot cross-lingual transfer and multilingual training scenarios in Tables 2 and 3, respectively.\nZero-Shot XQuAD TyDiQA(GoldP) XNLI XL-Sumen XL-Sumar,en,ja,zh MASSIVE F1 / EM F1 / EM Acc RG1 / RG2 / RGL RG1 / RG2 / RGL EM\nmmT5 outperforms both the original mT5 as well as mT5S across all model sizes. It achieves performance similar to XLM-R at the same parameter size\u2014despite its encoder-decoder configuration\u2014 and significantly outperforms X-Mod, the only other modular model.\nZero-shot For zero shot cross-lingual transfer scenarios, we see large gains for generative tasks in particular. For question answering (XQuAD and TyDiQA), we observe an average relative F1 improvement of 5.5 and 6.3 for the small and base models respectively. For summarization, we see larger zero-shot gains when jointly training on more than one language. We suspect that this is due to the increase in training data and due to positive transfer during multi-source training, which modular methods are better able to harness. This is in line with previous findings that multi-source training improves cross-lingual transfer in adapterbased setups (Ansell et al., 2021b). We also see a gain of 6.1 EM points on MASSIVE. The smallest gains are achieved for the classification task XNLI. Here, mmT5 improves over the baselines only by 1\u20132.4 accuracy points. We hypothesize that due to the constrained formulation of the task,\nwhich only requires predicting a single token, the full multilingual generative capabilities of mmT5 are under-utilized. Overall, we see a clear trend that our modular models significantly outperform their respective dense counterparts especially for generation tasks.\nMultilingual training For multilingual training in Table 3, we also find that the modular models outperform their dense counterparts across all tasks we experiment with. Here we find the largest gains for semantic parsing (MASSIVE). For summarization (XL-SUM), we see smaller, but still consistent gains. These results indicate that modular representations are not only useful in transfer settings but that mmT5 can also leverage labeled data in the target language to deliver superior performance compared to the standard non-modular models.\n6 Analysis and Ablations\n6.1 Impact of Freezing Configuration\nWe investigate the impact of the freezing configuration on the performance of the model. In Table 5, we compare the best-performing freezing configurations with a non-frozen baseline for mmT5 base (we show the results of all freezing configurations in Appendix A.1). We observe significant improvements when freezing the feed-forward layer of the decoder during fine-tuning, particularly in zeroshot scenarios. For multilingual training, freezing of the decoder has less effect on the performance. We also find that freezing parts of the decoder has no effect on the dense mT5S model across all tasks (see Appendix A.1).\n6.2 Impact of Bottleneck Size\nWe experiment with different bottleneck sizes of the modular components to understand the impact of providing each language with more capacity. We report results for XQuAD, and XNLI in Figure 3 using mmT5 base and bottleneck sizes of 96, 192, 384, and 768. We find that for all three tasks the bottleneck size has little effect on the downstream task performance, achieving only 0.5\u2013 2 absolute points difference between the larger and the smaller bottleneck sizes. This suggests that it is sufficient to provide the model with only a small amount of language-specific parameters in order to learn idiosyncratic information and mitigate catastrophic interference, and highlights the parameter-efficiency of modular models.\n6.3 Impact of Model Size\nIn Figure 4, we plot the performance difference of mmT5 and mT5S for the small and base variants. We find that the modular model outperforms the dense variant across model sizes with a similar gap, indicating that the positive effect of modularity may not diminish at scale.\n6.4 Source Language Hallucination\nWe perform an analysis of the generated text on the XL-Sum dev sets for mT5S and mmT5 models trained in a zero-shot setting on XL-Sumar,en,ja,zh\nusing full fine-tuning and a decoder freezing configuration. We automatically detect the language of the generated text using the Language Detection from the Google Cloud Translation API7 (Caswell et al., 2020). We show the results in Figure 6. We find that most models tend to generate text in one of the source languages (in this setting: Arabic, English, Japanese, and Chinese). This holds true also for mmT5 when we fine-tune the decoder. However, when freezing the decoder we observe a dramatic improvement in the target language generation rate from 1% to 99% of examples for mmT5, essentially solving the issue of source language hallucination in cross-lingual transfer scenarios. This improvement in language consistency also helps explain the significant improvement of the modular model over its dense counterparts on natural language generation tasks.\n7https://cloud.google.com/translate/ docs/basic/detecting-language\nIn addition, we manually analyze outputs of mT5S and mmT5 on XQuAD and find similar issues of source language hallucinations. We show examples in Figure 5. Although the task is extractive QA, i.e., the answer is a substring of the input, mT5S tends to translate subwords into English (the source language). This does not happen to mmT5 when freezing parts of the decoder, partially explaining the large improvements of mmT5 over mT5S on TyDi QA in Table 2.\n6.5 Module Re-Use for Unseen Languages In the previous sections we have evaluated the cross-lingual performance of mmT5 on languages\nseen during pre-training. However, with more than 7000 languages spoken in the world (Joshi et al., 2020), mmT5 covers less than 1% of them. While extending the model to unseen languages is out of scope for this work8, we evaluate the potential reusability of existing language modules for truly unseen languages with a case study on Tagalog. We utilize the base mmT5 model fine-tuned on the English MASSIVE training dataset (see Table 2). As a Tagalog language module does not exist within mmT5, we test all existing other language modules when evaluating on the Tagalog test set. In Figure 7, we report the Exact Match (EM) zero-shot accuracies for all languages. The module performing best corresponds to Javanese, which is the most closely related language to Tagalog as both belong to the Malayo-Polynesian subgroup of the Austronesian language family. This finding demonstrates the effectiveness of modular models; modular components specifically incorporate interpretable concepts, which can be re-used for unseen scenarios. Additionally, they can be further finetuned or adapted to the target domain if training data is available.\n7 Conclusion\nWe have proposed mmT5, a modular multilingual encoder-decoder model. During multilingual pretraining the majority of parameters of mmT5 are shared between tasks, but each language is provided with a small amount of parameters only accessible to the respective language. We demonstrated that integrating modularity as an architec-\n8Previous work has demonstrated that it is possible to extend multilingual models to unseen languages (Pfeiffer et al., 2020, 2021b, 2022).\ntural inductive bias significantly improves training efficiency, where the same perplexity as an equivalent fully dense model is achieved at a quarter of the update steps. mmT5 considerably outperforms comparable models on a large number of tasks including Question Answering, Semantic Parsing, Summarization and Classification in both zero-shot as well as multilingual scenarios. Finally, we show that by freezing parts of the decoder when fine-tuning mmT5 on a target task in a source language, the model consistently generates text in the target language. Consequently, modularity arguably solves source language hallucinations in cross-lingual transfer scenarios.\n8 Limitations and Future Work\nIn this paper, we explored the use of modularity for multilingual language models. We showed that modularity significantly improves cross-lingual performance on a number of generative tasks by mitigating hallucinations in the source language. However, there are still many avenues for future work.\nFirst, we did not consider placing the modules in different parts of the model. We only experimented with placing bottleneck layers after the feed-forward component of each transformer layer. Previous work has demonstrated that depending on the modality, different placements perform better (Pfeiffer et al., 2021a; Eichenberg et al., 2022).\nSecond, we only experimented with extending the vanilla transformer architecture with modular components. Future work might consider modularizing different parts of the transformer, such as the attention-components or entire feed-forward layers like in Kudugunta et al. (2021).\nThird, we performed fixed routing under the assumption that the language ID is easy to obtain. We chose this path, as learning-to-route has many difficulties such as training instabilities (Pfeiffer et al., 2023). However, this architecture design limits the sharing of information (e.g. domains) across languages. Consequently, a combination of fixed routing and learned routing would allow the model to learn how to share information across subsets of languages.\nFourth, we did not try using mmT5 for machine translation. Using a modular design for this type of task setup is quite natural, as modules from the encoder and decoder can be easily replaced with the source and target language components, respectively. The effectiveness of modular sequence-tosequence models for NMT has been investigated previously (Bapna and Firat, 2019; Philip et al., 2020; Chronopoulou et al., 2020; Le et al., 2021; \u00dcst\u00fcn et al., 2021; Stickland et al., 2021; Garcia et al., 2021; Dua et al., 2022).\nFinally, we did not consider extending the model to languages beyond those we pre-trained on. While our preliminary results (see \u00a7 6.5) suggest that there are benefits of reusing related language modules to learn unseen languages, this requires further experimentation. However, previous works have demonstrated that modular (Pfeiffer et al., 2022) as well as dense models can be adapted to new languages and scripts (Pfeiffer et al., 2020, 2021b). Alternatively, future work might consider using post-hoc adaptation techniques, such as LoRA (Hu et al., 2022), to adapt modules to new languages.\nAcknowledgements\nWe thank Andrea Gesmundo, Marc\u2019Aurelio Ranzato, Srini Narayanan, and Emanuele Bugliarello for helpful feedback on a draft of this paper.\nReferences Alan Ansell, Edoardo Maria Ponti, Anna Korhonen, and Ivan\nVulic. 2022. Composable sparse fine-tuning for crosslingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 1778\u20131796. Association for Computational Linguistics.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glava\u0161, Ivan Vulic\u0301, and Anna Korhonen. 2021a. MAD-G: Multilingual adapter generation for efficient cross-lingual transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762\u2013 4781, Punta Cana, Dominican Republic. Association for Computational Linguistics.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glava\u0161, Ivan Vulic\u0301, and Anna Korhonen. 2021b. MAD-G: Multilingual adapter generation for efficient cross-lingual transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762\u2013 4781, Punta Cana, Dominican Republic. Association for Computational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u2013 4637, Online. Association for Computational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 1538\u2013 1548. Association for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. 2020. Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6588\u20136608, Barcelona, Spain (Online). International Committee on Computational Linguistics.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020. Parsing with multilingual bert, a small treebank, and a small corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, pages 1324\u20131334.\nVincent S. Chen, Sen Wu, Alexander J. Ratner, Jen Weng, and Christopher R\u00e9. 2019. Slice-based learning: A programming model for residual learning in critical data slices. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9392\u20139402.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24:240:1\u2013240:113.\nAlexandra Chronopoulou, Dario Stojanovski, and Alexander Fraser. 2020. Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2703\u20132711, Online. Association for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for informationseeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020a. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020b. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Conference of the Association for Computational Linguistics, ACL 2020, Virtual Conference, July 6-8, 2020, pages 8440\u2013 8451.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485, Brussels, Belgium. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186.\nDheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan. 2022. Tricks for training sparse translation models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3340\u20133345, Seattle, United States. Association for Computational Linguistics.\nConstantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. 2022. MAGMA \u2013 multimodal augmentation of generative models through adapter-based finetuning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2416\u2013 2428, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23:120:1\u2013120:39.\nJack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, G\u00f6khan T\u00fcr, and Prem Natarajan. 2023. MASSIVE: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4277\u20134302. Association for Computational Linguistics.\nXavier Garcia, Noah Constant, Ankur Parikh, and Orhan Firat. 2021. Towards continual learning for multilingual machine translation via vocabulary substitution. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1184\u20131192, Online. Association for Computational Linguistics.\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2022. Demix layers: Disentangling domains for modular language modeling. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 5557\u20135576. Association for Computational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In\nFindings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693\u20134703, Online. Association for Computational Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzkebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameterefficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 2790\u20132799.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 4411\u20134421. PMLR.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6282\u20136293. Association for Computational Linguistics.\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. 2021. Beyond distillation: Task-level mixture-ofexperts for efficient inference. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 3577\u20133599. Association for Computational Linguistics.\nAnne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glava\u0161. 2020a. Common sense or world knowledge? investigating adapterbased knowledge injection into pretrained transformers. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 43\u201349, Online. Association for Computational Linguistics.\nAnne Lauscher, Vinit Ravishankar, Ivan Vulic\u0301, and Goran Glava\u0161. 2020b. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483\u20134499, Online.\nHang Le, Juan Miguel Pino, Changhan Wang, Jiatao Gu, Didier Schwab, and Laurent Besacier. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021, pages 817\u2013824. Association for Computational Linguistics.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021a. Compacter: Efficient low-rank hypercomplex adapter layers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1022\u20131035.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021b. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 565\u2013576. Association for Computational Linguistics.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3479\u2013 3495, Seattle, United States. Association for Computational Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. 2021a. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487\u2013503, Online. Association for Computational Linguistics.\nJonas Pfeiffer, Sebastian Ruder, Ivan Vulic\u0301, and Edoardo Maria Ponti. 2023. Modular deep learning. arXiv preprint.\nJonas Pfeiffer, Ivan Vulic\u0301, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654\u20137673, Online. Association for Computational Linguistics.\nJonas Pfeiffer, Ivan Vulic\u0301, Iryna Gurevych, and Sebastian Ruder. 2021b. UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Online, November , 2021.\nJerin Philip, Alexandre Berard, Matthias Gall\u00e9, and Laurent Besacier. 2020. Monolingual adapters for zero-shot neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4465\u20134470, Online. Association for Computational Linguistics.\nTelmo Pires, Robin M. Schmidt, Yi-Hsiu Liao, and Stephan Peitz. 2023. Learning language-specific layers for multilingual machine translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 14767\u201314783. Association for Computational Linguistics.\nEdoardo Maria Ponti, Goran Glava\u0161, Olga Majewska, Qianchu Liu, Ivan Vulic\u0301, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362\u2013 2376, Online. Association for Computational Linguistics.\nClifton Poth, Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, and Iryna Gurevych. 2021. What to pre-train on? efficient intermediate task selection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 10585\u201310605. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:140:1\u2013140:67.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 506\u2013516.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2018. Efficient parametrization of multi-domain deep neural networks. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8119\u20138127.\nAndreas R\u00fcckl\u00e9, Jonas Pfeiffer, and Iryna Gurevych. 2020. Multicqa: Zero-shot transfer of self-supervised text matching models on a massive scale. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 2471\u20132486. Association for Computational Linguistics.\nSebastian Ruder. 2017. An overview of multi-task learning in deep neural networks. arXiv preprint.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. 2021. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215\u201310245, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparselygated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\nAsa Cooper Stickland, Alexandre Berard, and Vassilina Nikoulina. 2021. Multilingual domain adaptation for NMT: decoupling language and domain information with adapters. In Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10- 11, 2021, pages 578\u2013598. Association for Computational Linguistics.\nAsa Cooper Stickland and Iain Murray. 2019. BERT and pals: Projected attention layers for efficient adaptation in multitask learning. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 5986\u20135995. PMLR.\nAhmet \u00dcst\u00fcn, Alexandre Berard, Laurent Besacier, and Matthias Gall\u00e9. 2021. Multilingual unsupervised neural machine translation with denoising adapters. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6650\u20136662. Association for Computational Linguistics.\nAhmet \u00dcst\u00fcn, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. 2020. UDapter: Language adaptation for truly Universal Dependency parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2302\u20132315, Online. Association for Computational Linguistics.\nMarko Vidoni, Ivan Vulic\u0301, and Goran Glava\u0161. 2020. Orthogonal language and task adapters in zero-shot cross-lingual transfer. In arXiv preprint.\nTu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, and Noah Constant. 2022. Overcoming catastrophic forgetting in zero-shot cross-lingual generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9279\u20139300, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. 2021a. K-adapter: Infusing knowledge into pre-trained models with adapters. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 1405\u20131418. Association for Computational Linguistics.\nXinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig. 2021b. Efficient test time adapter ensembling for low-resource language varieties. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 730\u2013737, Punta Cana, Dominican Republic. Association for Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120\u2013 130, Online. Association for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-totext transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\nBiao Zhang, Ankur Bapna, Rico Sennrich, and Orhan Firat. 2021. Share or not? learning to schedule language-specific capacity for multilingual translation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\nA Appendix\nA.1 Freezing combinations We show results with different freezing combinations in Table 5. We find that freezing the FFN component of the Decoder results in the biggest performance gains.\nA.2 Language-ID prediction on Cross-lingual Summarization\nWe report the languages predicted by the Language Detection model from the Google Cloud Translation API9 (Caswell et al., 2020) for the XL-Sumar,en,ja,zh task in Table 10. We find that mmT5 achieves near perfect performance for all target languages when freezing parts of the decoder (s7)\u201399% of the text is generated in the correct target language\u2013significantly outperforming all other model variants. Interestingly, mmT5 hallucinates in the source language when the decoder is finetuned (s1), resulting in a drop down to only 2% in the correct target language. mT5S also benefits slightly from freezing parts of the decoder, with an improvement from 7% to 18% target language generation, however, this is no where close to the performance of mmT5.\nA.3 Language-level Results XNLI. We report XNLI validation results in Table 11 and test results in Table 6.\nXQuAD. We report XQuAD validation results in Table 9 and test results in Table 7.\nMASSIVE. We report MASSIVE validation results in Table 17 and test results in Table 8.\nTyDiQA. We report TyDiQA validation results in Table 23.\nMultilingual XL-Sum We report XL-Sum validation results in Tables 18, 19,20, 21, and 22 and test results in Table 15.\nZeroshot XL-Sumen We report XL-Sum validation results in Table 14 and test results in Table 13.\nZeroshot XL-Sumar,en,ja,zh We report XL-Sum validation results in Table 16 and test results in Table 12.\nA.4 Language-level Pre-training Perplexities We report the language-level perplexities of the different model variants and sizes in Figures 8, 9, 10, 11, 12, 13, 14.\n9https://cloud.google.com/translate/ docs/basic/detecting-language\nmodel ar de el en es hi ru th tr vi zh avg F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM F1 / EM\nsm al l mmT5 60.6 / 44.6 71.0 / 53.6 64.9 / 47.1 82.5 / 70.3 74.1 / 56.1 59.2 / 43.9 69.5 / 50.8 58.9 / 47.0 62.4 / 43.4 64.3 / 45.4 64.2 / 52.4 66.5 / 50.4 mT5S 53.5 / 37.1 67.2 / 48.7 59.5 / 41.1 81.7 / 69.7 69.8 / 53.7 54.7 / 40.8 62.8 / 44.5 50.3 / 37.9 57.2 / 39.3 59.7 / 40.9 64.7 / 54.0 61.9 / 46.2\ntgt lang am ar az bn cy pred lang cfg am ar en ja zh ar ar en ja zh az ar en ja zh bn ar en ja zh cy ar en ja zh\nmmT5 s1 0.00 0.50 0.07 0.02 0.36 1.0 1.0 0.0 0.0 0.0 0.00 0.18 0.29 0.06 0.29 0.00 0.4 0.10 0.05 0.41 0.06 0.05 0.84 0.0 0.01 mmT5 s7 0.99 0.00 0.00 0.00 0.00 1.0 1.0 0.0 0.0 0.0 1.00 0.00 0.00 0.00 0.00 1.00 0.0 0.00 0.00 0.00 0.91 0.00 0.08 0.0 0.00 mT5S s1 0.03 0.96 0.00 0.00 0.01 1.0 1.0 0.0 0.0 0.0 0.08 0.12 0.31 0.27 0.09 0.02 0.3 0.02 0.43 0.19 0.13 0.10 0.76 0.0 0.00 mT5S s7 0.02 0.95 0.00 0.00 0.00 1.0 1.0 0.0 0.0 0.0 0.36 0.03 0.35 0.05 0.02 0.13 0.1 0.13 0.47 0.11 0.22 0.01 0.76 0.0 0.00\namharic arabic azerbaijani bengali burmese chinese_simplified chinese_traditional Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 12.5 / 1.8 / 11.9 17.2 / 3.3 / 15.7 14.0 / 4.0 / 12.1 8.7 / 2.2 / 7.9 17.8 / 3.6 / 16.2 3.9 / 1.2 / 3.6 5.4 / 2.0 / 4.9 mT5S s1 13.6 / 1.2 / 13.0 15.3 / 0.6 / 15.0 14.0 / 1.9 / 12.7 9.5 / 0.3 / 9.4 18.1 / 0.5 / 17.8 5.6 / 1.2 / 5.4 5.8 / 1.6 / 5.6\nB as\ne mmT5 s1 12.6 / 0.2 / 12.4 15.0 / 0.3 / 14.7 13.9 / 1.4 / 12.5 9.5 / 0.2 / 9.3 17.8 / 0.2 / 17.6 5.3 / 0.5 / 5.1 5.0 / 0.5 / 4.9 s7 16.3 / 5.0 / 14.2 19.8 / 4.7 / 17.4 17.7 / 5.4 / 14.7 15.0 / 5.0 / 12.5 22.0 / 4.8 / 19.5 9.1 / 2.7 / 8.1 8.1 / 2.5 / 7.3\nmT5S s1 13.8 / 1.6 / 13.0 16.0 / 1.3 / 15.3 15.4 / 2.9 / 13.6 9.9 / 0.6 / 9.5 17.8 / 1.7 / 16.9 6.6 / 1.8 / 6.3 6.8 / 2.2 / 6.5s7 13.7 / 1.4 / 13.1 16.7 / 1.3 / 16.0 16.7 / 3.6 / 14.7 9.9 / 0.5 / 9.6 18.2 / 1.4 / 17.4 6.4 / 1.6 / 6.2 6.0 / 1.6 / 5.8\nenglish french gujarati hausa hindi igbo indonesian Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 41.6 / 17.0 / 32.6 27.8 / 8.7 / 22.5 12.0 / 2.9 / 11.1 19.3 / 6.6 / 17.2 13.6 / 3.0 / 12.6 28.1 / 9.7 / 22.9 22.5 / 6.0 / 19.1 mT5S s1 42.3 / 17.9 / 33.2 22.1 / 3.6 / 18.9 10.6 / 0.4 / 10.4 18.3 / 4.3 / 16.3 13.1 / 0.9 / 12.8 25.4 / 5.0 / 21.8 18.1 / 2.8 / 16.2\nB as\ne mmT5 s1 45.0 / 20.8 / 35.6 21.1 / 3.6 / 17.8 10.6 / 0.2 / 10.4 20.1 / 5.5 / 17.7 12.4 / 0.2 / 12.1 24.5 / 4.0 / 21.4 18.4 / 2.9 / 16.2 s7 46.1 / 21.8 / 36.6 27.9 / 8.4 / 22.2 15.9 / 5.6 / 13.6 27.5 / 11.1 / 22.0 16.6 / 4.4 / 14.7 25.4 / 9.3 / 20.6 26.1 / 8.0 / 21.4\nmT5S s1 44.7 / 20.6 / 35.4 21.4 / 3.9 / 18.2 11.1 / 0.8 / 10.7 19.5 / 4.9 / 16.9 14.0 / 1.7 / 13.3 26.2 / 5.4 / 22.0 19.2 / 3.5 / 16.8s7 43.3 / 18.7 / 34.1 22.7 / 4.7 / 19.3 11.3 / 0.7 / 11.0 21.7 / 6.3 / 18.7 14.2 / 1.5 / 13.6 27.1 / 6.5 / 22.8 20.9 / 4.3 / 18.2\njapanese korean kyrgyz marathi nepali pashto persian Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 3.7 / 0.8 / 3.4 16.7 / 4.5 / 14.8 10.3 / 1.6 / 9.6 8.9 / 1.6 / 8.4 13.0 / 3.8 / 11.6 17.7 / 3.2 / 16.1 18.2 / 4.0 / 16.7 mT5S s1 4.0 / 1.0 / 3.9 13.6 / 0.8 / 13.3 12.5 / 0.5 / 12.1 11.2 / 1.0 / 10.8 10.7 / 0.5 / 10.5 14.3 / 0.3 / 14.1 16.4 / 0.7 / 16.1\nB as\ne mmT5 s1 3.3 / 0.4 / 3.2 13.2 / 0.4 / 12.9 12.2 / 0.3 / 11.8 10.9 / 0.3 / 10.6 10.5 / 0.1 / 10.4 14.5 / 0.1 / 14.4 15.9 / 0.2 / 15.7 s7 6.3 / 2.3 / 5.8 19.7 / 5.7 / 16.9 13.3 / 2.2 / 12.0 11.5 / 2.2 / 10.6 14.1 / 3.8 / 12.4 18.0 / 4.4 / 15.8 23.0 / 6.8 / 19.8\nmT5S s1 4.5 / 1.1 / 4.4 15.4 / 2.5 / 14.4 12.8 / 0.8 / 12.2 12.0 / 1.3 / 11.3 11.5 / 1.1 / 11.0 15.9 / 1.5 / 15.1 17.4 / 1.8 / 16.4s7 4.2 / 0.9 / 4.1 15.2 / 1.8 / 14.2 13.5 / 0.9 / 12.8 11.9 / 1.0 / 11.4 11.9 / 1.1 / 11.4 16.0 / 1.1 / 15.3 17.8 / 1.6 / 17.0\nportuguese punjabi russian scottish_gaelic serbian_cyrillic serbian_latin sinhala Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 30.7 / 10.1 / 24.9 12.1 / 3.1 / 10.6 14.3 / 2.5 / 13.0 27.6 / 10.7 / 23.3 8.3 / 1.4 / 7.7 10.9 / 2.2 / 9.8 13.9 / 3.3 / 12.8 mT5S s1 23.7 / 5.1 / 20.3 12.1 / 0.3 / 11.9 14.6 / 1.1 / 13.8 21.4 / 5.1 / 18.6 13.5 / 0.6 / 13.0 17.2 / 2.2 / 15.3 10.5 / 0.3 / 10.3\nB as\ne mmT5 s1 24.6 / 5.5 / 20.6 12.2 / 0.2 / 12.0 13.9 / 0.6 / 13.1 21.6 / 5.6 / 18.5 13.0 / 0.4 / 12.7 16.2 / 1.6 / 14.5 10.3 / 0.1 / 10.2 s7 31.3 / 10.5 / 24.5 17.8 / 5.8 / 14.6 18.9 / 3.9 / 16.3 24.8 / 10.5 / 20.2 15.6 / 2.8 / 13.8 15.8 / 3.5 / 13.6 17.5 / 6.7 / 14.8\nmT5S s1 23.7 / 5.6 / 19.8 12.5 / 0.7 / 12.0 14.8 / 1.3 / 13.7 22.3 / 6.3 / 18.8 13.9 / 0.9 / 13.2 17.3 / 2.3 / 15.2 10.6 / 0.5 / 10.3s7 24.9 / 6.5 / 20.8 12.1 / 0.4 / 11.8 15.6 / 1.6 / 14.4 24.6 / 8.3 / 20.3 14.2 / 0.9 / 13.5 18.7 / 2.8 / 16.4 11.0 / 0.6 / 10.8\nsomali spanish swahili tamil telugu thai turkish Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 20.1 / 4.9 / 16.9 21.5 / 5.8 / 17.6 22.0 / 5.9 / 18.2 13.9 / 4.3 / 12.3 12.8 / 3.6 / 11.7 13.0 / 4.1 / 11.5 15.2 / 4.5 / 13.0 mT5S s1 19.2 / 2.5 / 16.5 19.4 / 3.4 / 17.0 19.3 / 3.4 / 16.7 9.5 / 0.7 / 9.2 10.0 / 0.6 / 9.8 9.2 / 1.0 / 8.6 16.3 / 2.8 / 14.5\nB as\ne mmT5 s1 19.5 / 2.7 / 16.7 19.9 / 3.5 / 17.0 18.5 / 3.2 / 16.2 9.0 / 0.2 / 8.7 9.7 / 0.2 / 9.5 8.9 / 0.4 / 8.3 16.6 / 2.6 / 14.5 s7 22.0 / 6.5 / 17.3 25.6 / 7.0 / 20.2 25.6 / 8.3 / 20.2 17.6 / 6.6 / 14.5 15.9 / 5.5 / 13.6 14.1 / 4.7 / 12.4 21.6 / 7.0 / 17.5\nmT5S s1 19.6 / 2.6 / 16.8 20.0 / 3.8 / 17.1 20.3 / 4.3 / 17.2 9.9 / 1.0 / 9.4 10.3 / 0.9 / 9.8 10.0 / 1.2 / 9.2 17.6 / 3.5 / 15.1s7 21.1 / 3.3 / 17.9 20.8 / 4.3 / 17.7 21.8 / 4.9 / 18.5 10.3 / 1.1 / 9.8 10.6 / 0.9 / 10.2 9.8 / 1.0 / 9.0 19.6 / 4.6 / 16.7\nukrainian urdu uzbek vietnamese welsh yoruba avg Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 10.7 / 1.7 / 9.9 16.8 / 3.8 / 15.2 8.9 / 0.6 / 8.7 29.7 / 11.6 / 22.1 20.6 / 5.4 / 17.6 30.0 / 10.5 / 24.2 16.7 / 4.7 / 14.4 mT5S s1 13.4 / 0.9 / 12.8 13.1 / 0.3 / 13.0 12.8 / 0.6 / 12.3 21.2 / 3.7 / 18.2 22.3 / 3.3 / 19.9 23.8 / 4.0 / 20.9 15.5 / 2.2 / 14.2\nlang amharic arabic azerbaijani bengali burmese chinese_simplified chinese_traditional Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 29.1 / 13.0 / 22.2 37.7 / 17.3 / 29.6 19.0 / 7.2 / 15.9 27.6 / 12.8 / 22.3 38.3 / 17.7 / 29.7 38.3 / 19.1 / 33.2 39.8 / 21.4 / 34.0 mT5S s1 13.0 / 3.1 / 12.0 37.6 / 17.3 / 29.4 13.4 / 2.6 / 12.1 7.4 / 1.1 / 7.2 11.9 / 1.9 / 11.4 38.5 / 19.5 / 33.4 39.9 / 21.6 / 34.1\nB as\ne mmT5 s1 9.4 / 0.3 / 9.3 41.2 / 20.9 / 31.9 12.3 / 1.8 / 11.2 8.3 / 0.4 / 8.1 15.1 / 0.3 / 14.9 64.1 / 51.2 / 60.8 62.9 / 49.8 / 58.8 s7 34.0 / 16.6 / 25.3 42.4 / 22.3 / 33.5 26.3 / 11.0 / 21.1 34.5 / 18.0 / 26.7 40.1 / 19.0 / 30.9 48.4 / 29.6 / 43.2 49.4 / 31.3 / 43.6\nmT5S s1 14.9 / 4.2 / 13.6 40.4 / 20.1 / 31.5 15.9 / 4.4 / 14.1 11.4 / 3.1 / 10.6 16.5 / 4.1 / 15.1 44.3 / 26.2 / 39.3 45.4 / 27.8 / 39.7s7 10.3 / 3.4 / 9.4 40.5 / 20.1 / 31.8 12.3 / 4.2 / 10.9 6.3 / 2.0 / 5.9 22.3 / 6.3 / 19.9 41.5 / 21.9 / 36.0 42.7 / 24.2 / 36.8\nlang english french gujarati hausa hindi igbo indonesian Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 39.0 / 14.5 / 30.6 35.4 / 14.2 / 27.1 27.1 / 12.5 / 21.7 28.4 / 11.7 / 22.9 30.1 / 13.4 / 24.1 37.7 / 17.4 / 29.0 31.4 / 11.8 / 25.0 mT5S s1 39.1 / 14.7 / 30.4 22.1 / 4.4 / 19.4 9.1 / 1.6 / 8.8 16.4 / 5.0 / 14.9 12.7 / 3.2 / 12.0 21.8 / 6.2 / 19.1 17.1 / 3.4 / 15.7\nB as\ne mmT5 s1 44.4 / 20.2 / 35.1 23.2 / 5.3 / 19.5 8.6 / 0.4 / 8.4 15.3 / 4.7 / 14.0 10.6 / 0.4 / 10.5 22.9 / 5.1 / 20.1 17.9 / 3.8 / 16.1 s7 43.9 / 19.4 / 34.8 37.3 / 15.2 / 27.8 32.1 / 16.1 / 25.0 36.6 / 16.4 / 27.6 34.8 / 16.7 / 27.1 40.3 / 19.7 / 30.3 35.8 / 15.0 / 27.6\nmT5S s1 42.9 / 18.4 / 33.7 23.7 / 5.4 / 20.1 12.3 / 4.1 / 11.4 20.3 / 6.8 / 18.0 14.9 / 4.8 / 13.8 31.5 / 11.6 / 25.4 20.9 / 5.5 / 18.4s7 41.9 / 17.2 / 32.8 23.9 / 6.4 / 20.5 9.3 / 2.9 / 8.7 11.9 / 4.2 / 10.8 13.4 / 4.1 / 12.6 21.3 / 8.2 / 17.2 19.3 / 5.9 / 17.0\nlang japanese korean kyrgyz marathi nepali pashto persian Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 45.0 / 25.6 / 37.1 31.1 / 13.3 / 25.5 19.3 / 6.7 / 16.5 19.2 / 8.1 / 16.0 30.1 / 14.3 / 24.7 38.1 / 17.0 / 28.7 37.7 / 17.0 / 29.1 mT5S s1 41.5 / 22.3 / 33.7 8.6 / 2.4 / 7.8 9.9 / 1.4 / 9.4 11.8 / 3.4 / 10.8 8.2 / 1.4 / 7.9 18.1 / 3.6 / 16.1 21.5 / 5.6 / 18.7\nB as\ne mmT5 s1 46.7 / 27.0 / 38.1 10.3 / 1.0 / 10.0 10.9 / 0.6 / 10.5 9.0 / 0.5 / 8.8 9.1 / 0.3 / 9.0 18.7 / 3.6 / 16.2 18.8 / 4.3 / 16.6 s7 49.0 / 29.1 / 40.3 34.5 / 15.7 / 27.4 26.1 / 10.7 / 20.9 26.1 / 11.7 / 20.7 35.6 / 18.1 / 27.8 42.5 / 20.3 / 30.9 41.2 / 19.7 / 30.7\nmT5S s1 45.2 / 25.7 / 36.6 11.0 / 4.3 / 10.0 12.0 / 2.1 / 11.3 13.4 / 4.9 / 12.0 12.6 / 3.6 / 11.5 21.0 / 4.9 / 18.1 23.6 / 6.5 / 19.9s7 48.1 / 28.6 / 39.7 16.8 / 5.9 / 15.1 9.0 / 1.9 / 8.5 11.1 / 3.7 / 10.1 9.8 / 2.9 / 9.2 19.1 / 5.8 / 16.8 27.3 / 10.2 / 22.7\nlang portuguese punjabi russian scottish_gaelic serbian_cyrillic serbian_latin sinhala Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 33.8 / 12.5 / 26.4 31.5 / 13.6 / 23.5 25.4 / 8.3 / 20.3 29.6 / 12.1 / 24.5 21.7 / 6.3 / 18.0 11.0 / 3.2 / 9.5 29.0 / 14.4 / 22.2 mT5S s1 23.3 / 5.3 / 20.7 10.6 / 1.1 / 10.3 13.0 / 1.5 / 12.5 18.6 / 5.6 / 16.9 11.3 / 0.9 / 11.2 11.3 / 2.3 / 10.2 8.7 / 1.1 / 8.5\nB as\ne mmT5 s1 24.9 / 6.7 / 21.0 10.4 / 0.7 / 10.3 13.7 / 1.0 / 13.0 17.4 / 5.4 / 15.4 11.9 / 0.8 / 11.6 14.9 / 2.3 / 13.4 8.1 / 0.3 / 8.0 s7 38.8 / 15.8 / 28.4 37.0 / 17.9 / 26.7 30.7 / 11.1 / 23.5 31.2 / 14.5 / 24.5 28.6 / 9.4 / 22.1 19.0 / 5.2 / 16.0 34.4 / 19.0 / 25.5\nmT5S s1 25.6 / 6.6 / 21.6 13.2 / 2.7 / 12.4 16.1 / 2.9 / 14.8 22.8 / 7.6 / 19.6 13.8 / 1.7 / 13.1 18.4 / 3.8 / 16.2 10.2 / 2.3 / 9.7s7 26.2 / 7.7 / 22.3 8.5 / 1.9 / 8.0 12.9 / 2.8 / 11.7 20.0 / 7.7 / 17.5 10.6 / 1.4 / 10.2 7.3 / 1.8 / 6.5 5.5 / 1.3 / 5.2\nlang somali spanish swahili tamil telugu thai turkish Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 29.3 / 11.6 / 22.7 28.5 / 8.2 / 22.3 30.0 / 10.9 / 22.9 25.3 / 12.0 / 20.6 23.4 / 10.6 / 19.2 26.3 / 12.8 / 20.3 24.8 / 9.9 / 20.5 mT5S s1 16.8 / 2.7 / 15.1 18.8 / 3.7 / 16.8 14.6 / 2.9 / 13.5 8.0 / 1.8 / 7.6 8.5 / 1.3 / 8.3 7.1 / 2.2 / 6.6 16.4 / 3.8 / 14.7\nB as\ne mmT5 s1 15.9 / 3.0 / 14.0 19.7 / 4.3 / 17.0 16.4 / 3.5 / 14.5 7.5 / 0.6 / 7.3 8.3 / 0.4 / 8.1 7.4 / 0.6 / 7.0 16.0 / 3.4 / 14.0 s7 34.0 / 15.0 / 25.1 32.6 / 10.4 / 24.0 36.0 / 15.0 / 26.4 29.6 / 14.8 / 23.2 26.8 / 12.8 / 21.1 30.7 / 15.7 / 23.3 31.2 / 13.8 / 25.1\nmT5S s1 18.6 / 3.8 / 16.4 21.2 / 4.7 / 18.3 16.8 / 4.1 / 15.2 10.6 / 3.5 / 9.7 10.8 / 2.9 / 10.2 10.9 / 3.9 / 9.9 18.4 / 5.4 / 16.0 s7 12.6 / 2.5 / 11.5 19.8 / 4.9 / 17.3 13.8 / 3.5 / 12.7 10.0 / 3.9 / 9.1 8.7 / 2.7 / 8.0 11.1 / 4.7 / 9.8 17.8 / 6.7 / 15.4\nlang ukrainian urdu uzbek vietnamese welsh yoruba avg Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al l mmT5 s7 22.4 / 7.4 / 18.3 34.5 / 15.6 / 26.0 4.2 / 1.0 / 4.0 36.6 / 16.9 / 26.0 27.8 / 11.2 / 21.7 30.8 / 13.6 / 23.7 29.4 / 12.6 / 23.4 mT5S s1 11.4 / 1.2 / 11.1 15.6 / 3.0 / 12.6 1.0 / 1.6 / 10.0 16.9 / 3.2 / 15.2 11.2 / 3.0 / 16.2 13.6 / 4.1 / 17.3 17.0 / 4.8 / 15.1\nlang english french gujarati hausa hindi igbo indonesian Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al\nl m m\nT 5 s1 38.5 / 30.0 / 33.5 38.1 / 29.3 / 31.5 33.5 / 27.0 / 31.2 41.9 / 32.6 / 36.0 39.0 / 31.2 / 35.7 43.0 / 31.8 / 34.5 38.0 / 30.0 / 33.8 s6 36.1 / 28.4 / 32.7 38.0 / 29.4 / 32.4 32.0 / 26.0 / 31.9 39.4 / 30.9 / 36.1 36.8 / 29.5 / 35.3 38.2 / 28.8 / 34.6 35.6 / 28.1 / 33.2 s7 37.3 / 29.1 / 33.5 37.9 / 29.1 / 33.0 32.8 / 26.5 / 31.8 40.4 / 31.4 / 37.1 38.1 / 30.4 / 35.8 41.0 / 30.8 / 35.0 36.5 / 28.9 / 34.1 s10 36.4 / 28.5 / 32.4 37.9 / 29.5 / 32.0 32.5 / 26.5 / 31.2 39.4 / 30.7 / 35.6 36.9 / 29.6 / 34.9 38.5 / 29.3 / 33.8 35.9 / 28.3 / 32.9 s14 36.7 / 28.7 / 33.3 38.3 / 29.5 / 33.1 32.8 / 26.7 / 31.5 40.4 / 31.4 / 36.6 37.8 / 30.2 / 35.7 41.3 / 31.2 / 34.9 36.3 / 28.6 / 34.1\nm T\n5S s1 38.3 / 29.8 / 31.8 38.3 / 29.6 / 30.9 33.0 / 26.5 / 29.1 41.3 / 32.0 / 34.2 38.6 / 30.8 / 33.7 42.8 / 31.6 / 31.9 37.7 / 30.0 / 32.1 s6 36.2 / 28.3 / 30.0 37.2 / 28.8 / 30.3 31.9 / 26.0 / 29.2 38.0 / 29.6 / 32.3 36.9 / 29.4 / 32.5 37.4 / 28.5 / 31.7 35.0 / 27.6 / 30.7 s7 36.8 / 28.5 / 30.7 37.6 / 28.9 / 30.7 32.3 / 26.1 / 28.8 38.3 / 29.9 / 33.1 37.0 / 29.6 / 32.9 38.4 / 28.7 / 31.9 35.5 / 27.9 / 31.0 s10 35.4 / 27.7 / 30.6 36.8 / 28.6 / 30.5 31.8 / 25.9 / 29.3 37.2 / 28.9 / 32.6 36.1 / 28.8 / 32.9 35.6 / 27.4 / 32.4 34.3 / 27.0 / 30.9 s14 36.7 / 28.5 / 31.0 37.6 / 28.9 / 30.8 32.3 / 26.0 / 29.5 38.7 / 30.1 / 33.6 37.4 / 29.8 / 33.6 38.4 / 28.9 / 32.5 35.6 / 28.1 / 31.5\nB as\ne\nm m\nT 5 s1 42.6 / 17.9 / 33.5 41.2 / 20.2 / 31.5 38.8 / 23.2 / 31.2 45.6 / 25.3 / 36.0 44.0 / 26.2 / 35.7 46.3 / 25.0 / 34.5 42.3 / 21.5 / 33.8 s6 41.9 / 17.0 / 32.7 41.6 / 20.9 / 32.4 39.1 / 23.9 / 31.9 46.1 / 25.1 / 36.1 43.3 / 25.7 / 35.3 46.7 / 25.5 / 34.6 41.6 / 21.0 / 33.2 s7 42.7 / 17.9 / 33.5 42.6 / 21.8 / 33.0 39.2 / 24.0 / 31.8 46.9 / 26.2 / 37.1 44.0 / 26.4 / 35.8 47.3 / 26.0 / 35.0 42.6 / 22.0 / 34.1 s10 41.6 / 16.7 / 32.4 41.5 / 20.7 / 32.0 38.3 / 23.0 / 31.2 45.6 / 24.7 / 35.6 42.9 / 25.3 / 34.9 45.6 / 24.9 / 33.8 41.2 / 20.6 / 32.9 s14 42.5 / 17.6 / 33.3 42.7 / 21.9 / 33.1 38.9 / 23.4 / 31.5 46.9 / 26.1 / 36.6 44.0 / 26.2 / 35.7 47.2 / 25.8 / 34.9 42.6 / 21.8 / 34.1\nm T\n5S s1 41.0 / 16.4 / 31.8 40.0 / 19.1 / 30.9 36.6 / 21.0 / 29.1 44.1 / 23.0 / 34.2 42.0 / 24.0 / 33.7 43.5 / 22.9 / 31.9 40.5 / 20.0 / 32.1 s6 38.7 / 14.0 / 30.0 39.4 / 18.8 / 30.3 35.9 / 20.8 / 29.2 41.7 / 20.7 / 32.3 40.6 / 22.6 / 32.5 42.8 / 22.4 / 31.7 38.8 / 18.1 / 30.7 s7 39.7 / 14.9 / 30.7 40.1 / 19.1 / 30.7 35.9 / 20.7 / 28.8 42.8 / 21.7 / 33.1 41.0 / 23.1 / 32.9 43.3 / 22.9 / 31.9 39.2 / 18.7 / 31.0 s10 39.4 / 14.6 / 30.6 39.6 / 18.9 / 30.5 35.8 / 21.0 / 29.3 42.0 / 21.0 / 32.6 40.9 / 23.0 / 32.9 43.7 / 23.0 / 32.4 39.0 / 18.4 / 30.9 s14 40.0 / 15.3 / 31.0 39.9 / 19.2 / 30.8 36.5 / 21.6 / 29.5 43.2 / 22.2 / 33.6 41.7 / 23.9 / 33.6 44.3 / 23.4 / 32.5 39.5 / 19.1 / 31.5\nSm al\nl m m\nT 5 s1 42.2 / 34.3 / 39.5 38.6 / 33.2 / 36.7 26.7 / 21.1 / 23.8 32.6 / 26.5 / 30.2 39.8 / 32.6 / 36.5 44.4 / 34.1 / 38.6 43.1 / 33.8 / 37.3 s6 39.1 / 32.5 / 39.9 37.9 / 32.3 / 38.8 25.6 / 21.2 / 25.3 30.6 / 25.4 / 29.8 36.8 / 30.1 / 36.9 42.7 / 33.1 / 38.4 41.4 / 32.6 / 36.8 s7 41.5 / 34.3 / 41.1 39.3 / 34.0 / 39.2 26.4 / 21.6 / 25.4 31.9 / 26.3 / 30.6 38.3 / 31.3 / 36.7 43.7 / 33.9 / 38.9 42.0 / 32.7 / 37.6 s10 39.8 / 32.9 / 39.5 38.3 / 32.9 / 38.7 26.3 / 21.8 / 25.2 30.9 / 25.4 / 29.7 37.3 / 30.4 / 36.3 42.5 / 33.1 / 38.0 41.6 / 32.7 / 36.6 s14 41.4 / 34.0 / 41.1 39.4 / 34.0 / 39.2 26.1 / 21.7 / 25.4 32.4 / 26.6 / 30.5 38.3 / 31.1 / 37.3 43.4 / 33.6 / 38.5 41.8 / 32.7 / 37.4\nm T\n5S s1 42.9 / 34.8 / 37.9 38.6 / 32.9 / 35.1 26.4 / 21.4 / 22.1 32.9 / 26.9 / 28.7 39.0 / 31.6 / 34.5 44.2 / 34.0 / 36.2 42.8 / 33.5 / 35.8 s6 39.0 / 32.3 / 37.2 38.3 / 33.1 / 34.7 24.6 / 20.7 / 23.6 31.1 / 25.7 / 25.7 36.9 / 30.3 / 33.6 41.3 / 32.1 / 35.5 40.8 / 31.9 / 34.2 s7 40.1 / 33.1 / 37.5 38.2 / 33.0 / 36.1 25.4 / 21.1 / 23.7 31.9 / 26.2 / 28.3 37.3 / 30.4 / 34.1 42.4 / 33.0 / 35.8 41.7 / 32.5 / 34.6 s10 37.8 / 31.2 / 37.5 36.5 / 31.5 / 34.9 23.9 / 20.0 / 23.9 27.9 / 22.5 / 28.4 36.0 / 29.4 / 34.0 40.4 / 31.3 / 36.0 40.4 / 31.4 / 34.6 s14 40.4 / 33.1 / 38.6 38.3 / 33.0 / 37.1 25.2 / 21.0 / 23.7 32.0 / 26.2 / 28.8 37.3 / 30.2 / 34.8 42.5 / 33.1 / 36.3 41.5 / 32.4 / 35.0\nB as\ne\nm m\nT 5 s1 47.9 / 28.3 / 39.5 43.0 / 25.6 / 36.7 30.1 / 14.9 / 23.8 37.1 / 22.4 / 30.2 44.8 / 28.1 / 36.5 49.7 / 28.5 / 38.6 47.3 / 26.5 / 37.3 s6 48.5 / 28.3 / 39.9 45.0 / 27.8 / 38.8 31.3 / 16.3 / 25.3 36.2 / 21.5 / 29.8 44.7 / 28.4 / 36.9 49.0 / 28.1 / 38.4 46.5 / 25.9 / 36.8 s7 49.8 / 29.8 / 41.1 45.4 / 28.3 / 39.2 31.5 / 16.4 / 25.4 37.0 / 22.5 / 30.6 44.9 / 28.5 / 36.7 49.7 / 28.7 / 38.9 47.4 / 26.8 / 37.6 s10 48.0 / 27.9 / 39.5 44.9 / 27.6 / 38.7 30.8 / 16.1 / 25.2 36.0 / 21.6 / 29.7 44.1 / 27.7 / 36.3 48.6 / 27.5 / 38.0 46.3 / 25.6 / 36.6 s14 49.7 / 29.8 / 41.1 45.4 / 28.2 / 39.2 31.8 / 16.7 / 25.4 37.2 / 22.5 / 30.5 45.4 / 29.1 / 37.3 49.4 / 28.4 / 38.5 47.4 / 26.7 / 37.4\nm T\n5S s1 46.3 / 26.4 / 37.9 40.8 / 24.0 / 35.1 28.4 / 13.3 / 22.1 35.3 / 20.4 / 28.7 43.1 / 26.3 / 34.5 47.1 / 25.7 / 36.2 45.7 / 24.9 / 35.8 s6 45.0 / 25.3 / 37.2 40.5 / 23.3 / 34.7 28.8 / 14.2 / 23.6 32.0 / 17.3 / 25.7 41.1 / 24.8 / 33.6 45.8 / 24.6 / 35.5 44.0 / 23.1 / 34.2 s7 46.0 / 26.0 / 37.5 42.0 / 24.7 / 36.1 29.4 / 14.6 / 23.7 34.8 / 20.1 / 28.3 41.9 / 25.4 / 34.1 46.2 / 25.0 / 35.8 44.4 / 23.4 / 34.6 s10 45.3 / 25.6 / 37.5 40.4 / 23.3 / 34.9 28.9 / 14.6 / 23.9 34.6 / 20.2 / 28.4 41.7 / 25.3 / 34.0 46.2 / 25.0 / 36.0 44.4 / 23.6 / 34.6 s14 46.9 / 26.9 / 38.6 43.2 / 26.1 / 37.1 29.3 / 14.5 / 23.7 35.2 / 20.8 / 28.8 42.6 / 26.1 / 34.8 46.9 / 25.7 / 36.3 44.9 / 23.9 / 35.0\nSm al\nl m m\nT 5 s1 40.6 / 30.4 / 33.5 39.7 / 29.0 / 33.3 33.1 / 25.5 / 28.8 37.1 / 28.1 / 30.9 30.5 / 23.1 / 25.6 30.4 / 23.0 / 26.1 36.4 / 29.2 / 31.9 s6 38.6 / 29.0 / 32.9 37.7 / 28.2 / 33.7 31.0 / 23.9 / 27.9 34.8 / 27.4 / 32.2 29.0 / 22.5 / 27.3 23.6 / 19.1 / 26.4 35.2 / 28.3 / 34.7 s7 39.5 / 29.3 / 33.7 38.8 / 28.8 / 33.9 31.7 / 24.4 / 28.8 36.3 / 28.2 / 32.1 29.4 / 22.7 / 27.9 26.1 / 20.5 / 27.0 35.5 / 28.7 / 35.0 s10 38.8 / 29.0 / 32.7 37.7 / 28.3 / 33.2 31.1 / 23.9 / 27.0 35.3 / 27.9 / 32.4 29.2 / 22.5 / 26.7 23.5 / 18.8 / 25.5 35.5 / 28.4 / 34.2 s14 39.4 / 29.3 / 33.5 38.3 / 28.3 / 33.5 31.0 / 23.9 / 28.7 35.3 / 27.6 / 32.1 29.2 / 22.6 / 27.9 24.4 / 19.3 / 26.9 35.0 / 28.1 / 34.3\nm T\n5S s1 40.4 / 30.2 / 32.1 39.7 / 29.1 / 31.3 32.2 / 24.8 / 26.9 36.9 / 27.9 / 28.3 30.4 / 22.9 / 24.4 30.2 / 23.0 / 24.6 35.8 / 27.9 / 30.4 s6 38.4 / 28.6 / 30.6 37.9 / 28.1 / 30.9 31.1 / 23.9 / 25.2 35.0 / 27.6 / 29.2 28.7 / 22.3 / 24.6 24.3 / 19.3 / 23.1 34.5 / 27.7 / 31.2 s7 38.6 / 28.8 / 31.2 38.3 / 28.1 / 30.9 30.7 / 23.5 / 25.6 34.7 / 27.1 / 29.5 28.6 / 21.8 / 24.9 25.1 / 19.6 / 23.8 35.6 / 28.4 / 30.9 s10 37.3 / 28.0 / 30.9 36.9 / 27.4 / 31.2 30.4 / 23.3 / 25.4 33.1 / 26.4 / 29.5 28.0 / 21.6 / 24.9 23.4 / 18.8 / 23.5 33.9 / 27.1 / 31.9 s14 38.8 / 28.9 / 31.5 38.3 / 28.4 / 31.5 31.2 / 23.8 / 25.7 34.9 / 27.4 / 30.1 29.2 / 22.3 / 25.2 25.9 / 20.4 / 24.6 35.3 / 28.3 / 31.8\nB as\ne\nm m\nT 5 s1 44.5 / 22.4 / 33.5 44.2 / 25.9 / 33.3 37.3 / 17.4 / 28.8 41.0 / 22.1 / 30.9 34.3 / 13.5 / 25.6 34.8 / 14.4 / 26.1 40.1 / 25.5 / 31.9 s6 43.9 / 21.6 / 32.9 44.4 / 26.1 / 33.7 36.2 / 16.4 / 27.9 42.1 / 23.0 / 32.2 35.6 / 14.6 / 27.3 34.5 / 14.6 / 26.4 42.4 / 28.1 / 34.7 s7 44.8 / 22.5 / 33.7 44.8 / 26.4 / 33.9 37.1 / 17.4 / 28.8 42.1 / 23.2 / 32.1 36.4 / 15.5 / 27.9 35.9 / 15.2 / 27.0 42.5 / 28.2 / 35.0 s10 43.6 / 21.2 / 32.7 43.8 / 25.6 / 33.2 35.2 / 15.8 / 27.0 41.8 / 23.1 / 32.4 34.9 / 14.1 / 26.7 33.6 / 13.8 / 25.5 41.8 / 27.4 / 34.2 s14 44.7 / 22.4 / 33.5 44.7 / 26.2 / 33.5 37.2 / 17.3 / 28.7 42.3 / 23.4 / 32.1 36.2 / 15.3 / 27.9 35.5 / 15.4 / 26.9 42.4 / 27.6 / 34.3\nm T\n5S s1 42.9 / 20.7 / 32.1 42.5 / 23.2 / 31.3 35.0 / 15.6 / 26.9 38.2 / 19.2 / 28.3 32.5 / 12.0 / 24.4 33.0 / 12.8 / 24.6 38.6 / 24.0 / 30.4 s6 41.3 / 18.7 / 30.6 41.1 / 22.5 / 30.9 32.9 / 13.8 / 25.2 37.5 / 18.8 / 29.2 32.3 / 12.0 / 24.6 30.1 / 11.3 / 23.1 38.5 / 24.4 / 31.2 s7 41.9 / 19.5 / 31.2 41.7 / 22.6 / 30.9 33.5 / 14.1 / 25.6 38.9 / 19.7 / 29.5 32.7 / 12.3 / 24.9 31.4 / 12.1 / 23.8 38.7 / 24.2 / 30.9 s10 41.7 / 19.1 / 30.9 41.5 / 23.0 / 31.2 33.1 / 14.0 / 25.4 38.6 / 19.7 / 29.5 32.7 / 12.2 / 24.9 30.6 / 11.7 / 23.5 39.3 / 25.1 / 31.9 s14 42.3 / 19.8 / 31.5 42.5 / 23.6 / 31.5 33.6 / 14.4 / 25.7 39.5 / 20.5 / 30.1 33.2 / 12.5 / 25.2 32.7 / 12.8 / 24.6 39.4 / 25.2 / 31.8\nTable 20: Validation set results for XL-Sum in the multisource setup for languages Portuguese, Punjabi, Russian, Scottish Gaelic, Serbian, and Sinhala. We report results for the different freezing configurations.\nlang somali spanish swahili tamil telugu thai turkish Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL Rg1 / Rg2 / RgL\nSm al\nl m m\nT 5 s1 38.4 / 28.3 / 30.6 31.3 / 23.5 / 27.3 38.9 / 30.0 / 33.3 31.8 / 26.6 / 31.2 29.0 / 23.6 / 27.5 31.5 / 24.8 / 28.2 33.2 / 27.8 / 32.0 s6 37.0 / 27.4 / 30.8 32.2 / 24.2 / 27.1 37.7 / 28.8 / 33.7 28.9 / 24.2 / 31.3 26.8 / 22.1 / 27.9 31.0 / 24.8 / 29.3 30.3 / 25.3 / 31.5 s7 37.5 / 27.7 / 31.2 30.8 / 23.1 / 27.6 38.3 / 29.5 / 34.7 30.8 / 26.0 / 32.2 27.6 / 22.7 / 28.4 31.8 / 25.3 / 29.8 31.5 / 26.4 / 32.4 s10 36.8 / 27.2 / 30.2 31.9 / 23.9 / 26.7 37.9 / 28.9 / 33.6 28.8 / 24.0 / 31.0 26.9 / 22.2 / 27.3 31.5 / 25.0 / 29.3 30.5 / 25.6 / 31.0 s14 37.6 / 27.6 / 31.0 31.1 / 23.3 / 27.5 38.3 / 29.4 / 34.3 30.4 / 25.6 / 32.1 27.7 / 22.6 / 28.2 31.8 / 25.1 / 29.6 31.2 / 26.1 / 32.1\nm T\n5S s1 38.0 / 28.0 / 29.0 31.7 / 23.9 / 25.6 38.5 / 29.7 / 31.7 31.9 / 26.7 / 28.7 28.5 / 23.2 / 25.3 31.1 / 24.4 / 26.2 32.9 / 27.5 / 30.2 s6 35.4 / 26.3 / 28.4 32.1 / 23.9 / 25.1 36.1 / 27.9 / 31.3 29.7 / 25.0 / 26.2 26.9 / 22.3 / 25.0 30.6 / 24.7 / 26.7 29.8 / 25.1 / 28.3 s7 35.8 / 26.4 / 28.7 30.9 / 23.1 / 25.2 36.6 / 28.1 / 31.3 29.9 / 25.1 / 28.5 26.9 / 21.9 / 25.1 30.9 / 24.5 / 26.9 30.6 / 25.8 / 29.2 s10 34.6 / 25.8 / 28.7 31.6 / 23.6 / 25.6 35.3 / 27.0 / 31.5 27.5 / 22.7 / 28.3 26.0 / 21.6 / 25.3 30.4 / 24.2 / 27.0 29.2 / 24.5 / 28.8 s14 36.1 / 26.2 / 29.0 32.2 / 23.9 / 25.6 37.1 / 28.3 / 32.1 30.2 / 25.3 / 29.1 27.1 / 22.2 / 25.8 30.5 / 24.3 / 27.7 30.7 / 25.8 / 29.7\nB as\ne\nm m\nT 5 s1 41.7 / 21.4 / 30.6 36.2 / 14.8 / 27.3 42.7 / 22.6 / 33.3 37.0 / 22.4 / 31.2 33.8 / 20.0 / 27.5 35.5 / 20.3 / 28.2 38.0 / 21.3 / 32.0 s6 41.5 / 21.5 / 30.8 36.0 / 14.5 / 27.1 43.1 / 23.0 / 33.7 37.0 / 22.4 / 31.3 33.9 / 20.1 / 27.9 36.5 / 21.4 / 29.3 37.3 / 20.5 / 31.5 s7 41.7 / 21.7 / 31.2 36.5 / 15.1 / 27.6 44.2 / 24.0 / 34.7 37.9 / 23.4 / 32.2 34.4 / 20.7 / 28.4 37.4 / 22.0 / 29.8 38.3 / 21.6 / 32.4 s10 41.0 / 20.9 / 30.2 35.6 / 14.1 / 26.7 43.2 / 22.9 / 33.6 36.7 / 22.1 / 31.0 33.3 / 19.6 / 27.3 36.4 / 21.2 / 29.3 36.9 / 20.1 / 31.0 s14 41.7 / 21.7 / 31.0 36.3 / 14.9 / 27.5 43.8 / 23.6 / 34.3 37.9 / 23.1 / 32.1 34.4 / 20.6 / 28.2 37.2 / 21.7 / 29.6 38.1 / 21.3 / 32.1\nm T\n5S s1 39.6 / 19.4 / 29.0 34.3 / 13.3 / 25.6 41.3 / 21.1 / 31.7 34.7 / 20.3 / 28.7 31.3 / 17.6 / 25.3 33.6 / 18.7 / 26.2 36.2 / 19.5 / 30.2 s6 38.5 / 18.2 / 28.4 33.6 / 12.2 / 25.1 40.4 / 20.2 / 31.3 31.5 / 17.8 / 26.2 30.6 / 17.0 / 25.0 33.5 / 18.6 / 26.7 33.9 / 17.3 / 28.3 s7 39.1 / 19.0 / 28.7 33.7 / 12.6 / 25.2 40.7 / 20.5 / 31.3 34.1 / 19.9 / 28.5 30.8 / 17.3 / 25.1 33.9 / 19.1 / 26.9 34.9 / 18.3 / 29.2 s10 38.8 / 18.7 / 28.7 34.1 / 12.6 / 25.6 40.8 / 20.5 / 31.5 33.7 / 19.8 / 28.3 30.8 / 17.4 / 25.3 33.8 / 18.9 / 27.0 34.5 / 18.0 / 28.8 s14 39.3 / 19.1 / 29.0 34.2 / 12.8 / 25.6 41.2 / 20.9 / 32.1 34.7 / 20.6 / 29.1 31.5 / 17.9 / 25.8 34.7 / 19.6 / 27.7 35.4 / 18.8 / 29.7\nSm al\nl m m\nT 5 s1 51.6 / 34.9 30.4 / 16.8 62.7 / 51.4 46.9 / 32.1 49.9 / 33.6 26.6 / 19.6 46.3 / 28.6 34.4 / 23.8 31.5 / 22.1 42.3 / 29.2 s6 57.5 / 39.4 39.5 / 24.8 68.3 / 58.2 51.7 / 35.2 54.3 / 40.0 26.5 / 15.9 58.2 / 36.5 44.8 / 32.5 43.5 / 29.6 49.4 / 34.7 s7 63.2 / 46.0 40.6 / 26.5 70.7 / 60.5 59.3 / 41.9 57.9 / 43.4 30.7 / 18.5 60.3 / 39.9 36.4 / 25.7 38.1 / 23.9 50.8 / 36.3 s10 39.7 / 56.8 25.7 / 40.6 57.7 / 69.4 34.7 / 49.1 40.2 / 53.6 17.0 / 26.9 34.7 / 57.8 31.5 / 45.9 28.7 / 39.5 34.4 / 48.7 s14 63.1 / 47.6 40.5 / 25.7 71.0 / 60.2 58.1 / 40.3 56.1 / 41.1 30.4 / 18.1 59.3 / 37.9 40.9 / 29.9 33.9 / 22.9 50.4 / 36.0\nm T\n5S s1 45.6 / 30.2 25.5 / 14.2 61.7 / 50.0 45.1 / 29.9 49.7 / 33.8 27.1 / 20.3 50.8 / 35.0 33.1 / 21.8 18.5 / 13.6 39.7 / 27.6 s6 47.3 / 31.6 30.0 / 18.6 65.9 / 54.3 51.4 / 35.7 52.1 / 35.2 26.9 / 18.1 57.1 / 40.3 40.1 / 28.1 19.0 / 13.8 43.3 / 30.6 s7 51.4 / 31.4 31.3 / 18.6 68.4 / 57.0 52.4 / 33.5 54.4 / 36.1 27.7 / 19.2 54.7 / 35.5 33.4 / 23.6 20.1 / 14.6 43.8 / 30.0 s10 46.5 / 30.7 30.8 / 19.5 66.8 / 55.7 51.2 / 35.5 50.8 / 35.9 28.3 / 20.3 55.6 / 38.1 43.1 / 29.5 19.9 / 14.9 43.7 / 31.1 s14 50.4 / 33.7 26.6 / 15.9 69.3 / 58.6 55.5 / 37.2 56.6 / 39.5 28.3 / 18.8 59.9 / 40.8 36.2 / 23.0 17.3 / 12.4 44.5 / 31.1\nB as\ne\nm m\nT 5 s1 65.8 / 45.5 51.3 / 32.7 74.1 / 63.4 65.2 / 50.9 69.2 / 50.1 54.2 / 44.2 55.3 / 32.5 61.8 / 44.3 53.0 / 37.2 61.1 / 44.5 s6 75.2 / 59.2 57.9 / 38.1 76.5 / 65.7 71.6 / 57.4 76.8 / 62.3 54.6 / 42.0 67.4 / 45.7 69.8 / 52.1 54.7 / 39.8 67.2 / 51.4 s7 75.2 / 59.8 59.7 / 38.1 77.5 / 67.7 73.3 / 59.0 77.4 / 61.6 59.2 / 48.9 67.5 / 45.3 69.6 / 53.7 61.6 / 45.1 69.0 / 53.2 s10 74.3 / 57.0 56.9 / 37.2 75.7 / 64.1 72.1 / 58.2 77.6 / 62.3 56.0 / 43.8 66.1 / 43.7 69.3 / 51.9 52.1 / 38.0 66.7 / 50.7 s14 73.7 / 58.0 59.2 / 38.1 77.0 / 67.3 72.6 / 58.4 76.5 / 61.4 58.9 / 48.9 68.1 / 45.1 68.5 / 50.3 61.6 / 45.0 68.5 / 52.5\nm T\n5S . s1 64.1 / 40.7 41.4 / 25.7 72.5 / 61.1 64.1 / 48.0 70.3 / 56.5 42.1 / 30.1 58.4 / 36.5 58.2 / 41.3 48.7 / 39.2 57.8 / 42.1 s6 66.5 / 44.8 42.8 / 27.4 74.8 / 62.7 67.4 / 50.6 75.3 / 60.7 53.9 / 40.9 65.0 / 43.6 59.7 / 42.9 55.0 / 42.0 62.3 / 46.2 s7 70.2 / 47.7 52.7 / 32.7 74.8 / 64.3 68.5 / 52.9 74.3 / 58.9 49.8 / 39.9 63.6 / 39.7 58.4 / 40.1 56.8 / 46.2 63.2 / 46.9 s10 67.5 / 46.9 48.5 / 30.1 74.2 / 63.9 67.0 / 51.0 73.5 / 57.7 47.7 / 35.1 64.7 / 42.5 57.9 / 42.3 53.3 / 42.6 61.6 / 45.8 s14 68.3 / 45.8 55.4 / 38.1 75.7 / 64.1 67.8 / 52.2 75.3 / 60.4 52.8 / 40.9 63.7 / 40.6 61.4 / 43.1 55.8 / 44.7 64.0 / 47.8\nTable 23: Results for the validation set of TyDiQA. We report results for the different configurations of freezing."
        }
    ],
    "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
    "year": 2023
}