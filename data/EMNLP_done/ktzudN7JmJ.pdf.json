{
    "abstractText": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We opensource our analysis code in hopes of encouraging broader measurements of bias in future LLMs.1 NOTE: this paper contains examples of bias and toxicity in text that may be offensive or upsetting.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Esiobu"
        },
        {
            "affiliations": [],
            "name": "Xiaoqing Tan"
        },
        {
            "affiliations": [],
            "name": "Saghar Hosseini"
        },
        {
            "affiliations": [],
            "name": "Megan Ung"
        },
        {
            "affiliations": [],
            "name": "Yuchen Zhang"
        },
        {
            "affiliations": [],
            "name": "Jude Fernandes"
        },
        {
            "affiliations": [],
            "name": "Jane Dwivedi-Yu"
        },
        {
            "affiliations": [],
            "name": "Eleonora Presani"
        },
        {
            "affiliations": [],
            "name": "Adina Williams"
        },
        {
            "affiliations": [],
            "name": "Eric Michael Smith"
        },
        {
            "affiliations": [],
            "name": "Meta {davides"
        },
        {
            "affiliations": [],
            "name": "ellenxtan"
        },
        {
            "affiliations": [],
            "name": "saghar"
        },
        {
            "affiliations": [],
            "name": "meganu"
        },
        {
            "affiliations": [],
            "name": "yuchenzhang"
        }
    ],
    "id": "SP:6e072fbfcb6daeace71991dedd381fa81228b0f7",
    "references": [
        {
            "authors": [
                "Abubakar Abid",
                "Maheen Farooqi",
                "James Zou."
            ],
            "title": "Persistent anti-muslim bias in large language models",
            "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298\u2013306.",
            "year": 2021
        },
        {
            "authors": [
                "Arshiya Aggarwal",
                "Jiao Sun",
                "Nanyun Peng."
            ],
            "title": "Towards robust NLG bias evaluation with syntactically-diverse prompts",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6022\u20136032, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Mann",
                "Dario Amodei",
                "Nicholas Joseph",
                "Sam McCandlish",
                "Thomas Brown",
                "Jared Kaplan."
            ],
            "title": "Constitutional ai: Harmlessness from ai feedback",
            "venue": "arXiv preprint arXiv:2212.08073.",
            "year": 2022
        },
        {
            "authors": [
                "April H Bailey",
                "Adina Williams",
                "Andrei Cimpian."
            ],
            "title": "Based on billions of words on the internet, people= men",
            "venue": "Science Advances, 8(13):eabm2463.",
            "year": 2022
        },
        {
            "authors": [
                "Sourya Basu",
                "Prasanna Sattigeri",
                "Karthikeyan Natesan Ramamurthy",
                "Vijil Chenthamarakshan",
                "Kush R Varshney",
                "Lav R Varshney",
                "Payel Das."
            ],
            "title": "Equi-tuning: Group equivariant fine-tuning of pretrained models",
            "venue": "arXiv preprint arXiv:2210.06475.",
            "year": 2022
        },
        {
            "authors": [
                "Sandra L Bem."
            ],
            "title": "The lenses of gender: Transforming the debate on sexual inequality",
            "venue": "Yale University Press.",
            "year": 1993
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training",
            "year": 2023
        },
        {
            "authors": [
                "Steven Bird",
                "Ewan Klein",
                "Edward Loper."
            ],
            "title": "Natural language processing with Python: analyzing text with the natural language toolkit",
            "venue": "\" O\u2019Reilly Media, Inc.\".",
            "year": 2009
        },
        {
            "authors": [
                "Sidney Black",
                "Stella Biderman",
                "Eric Hallahan",
                "Quentin Anthony",
                "Leo Gao",
                "Laurence Golding",
                "Horace He",
                "Connor Leahy",
                "Kyle McDonell",
                "Jason Phang"
            ],
            "title": "Gpt-neox-20b: An open-source autoregressive language model",
            "venue": "BigScience",
            "year": 2022
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Solon Barocas",
                "Hal Daum\u00e9 III",
                "Hanna Wallach."
            ],
            "title": "Language (technology) is power: A critical survey of \u201cbias\u201d in nlp",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454\u20135476.",
            "year": 2020
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Gilsinia Lopez",
                "Alexandra Olteanu",
                "Robert Sim",
                "Hanna Wallach."
            ],
            "title": "Stereotyping norwegian salmon: An inventory of pitfalls in fairness benchmark datasets",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Conrad Borchers",
                "Dalia Gala",
                "Benjamin Gilburt",
                "Eduard Oravkin",
                "Wilfried Bounsi",
                "Yuki M Asano",
                "Hannah Kirk."
            ],
            "title": "Looking for a handsome carpenter! debiasing gpt-3 job advertisements",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Burnell",
                "Wout Schellaert",
                "John Burden",
                "Tomer D Ullman",
                "Fernando Martinez-Plumed",
                "Joshua B Tenenbaum",
                "Danaja Rutar",
                "Lucy G Cheke",
                "Jascha Sohl-Dickstein",
                "Melanie Mitchell"
            ],
            "title": "Rethink reporting of evaluation results in ai",
            "year": 2023
        },
        {
            "authors": [
                "Aylin Caliskan",
                "Joanna J Bryson",
                "Arvind Narayanan."
            ],
            "title": "Semantics derived automatically from language corpora contain human-like biases",
            "venue": "Science, 356(6334):183\u2013186.",
            "year": 2017
        },
        {
            "authors": [
                "Boxi Cao",
                "Hongyu Lin",
                "Xianpei Han",
                "Fangchao Liu",
                "Le Sun."
            ],
            "title": "Can prompt probe pretrained language models? understanding the invisible risks from a causal view",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Yang Cao",
                "Yada Pruksachatkun",
                "Kai-Wei Chang",
                "Rahul Gupta",
                "Varun Kumar",
                "Jwala Dhamala",
                "Aram Galstyan."
            ],
            "title": "On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Tommaso Caselli",
                "Valerio Basile",
                "Jelena Mitrovi\u0107",
                "Michael Granitzer."
            ],
            "title": "Hatebert: Retraining bert for abusive language detection in english",
            "venue": "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17\u201325.",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Paula Czarnowska",
                "Yogarshi Vyas",
                "Kashif Shah."
            ],
            "title": "Quantifying social biases in NLP: A generalization and empirical comparison of extrinsic fairness metrics",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1249\u20131267.",
            "year": 2021
        },
        {
            "authors": [
                "Mayukh Das",
                "Wolf Tilo Balke."
            ],
            "title": "Quantifying bias from decoding techniques in natural language generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 1311\u20131323.",
            "year": 2022
        },
        {
            "authors": [
                "Simone De Beauvoir."
            ],
            "title": "The second sex",
            "venue": "Knopf.",
            "year": 1949
        },
        {
            "authors": [
                "Ona De Gibert",
                "Naiara P\u00e9rez",
                "Aitor Garc\u00eda-Pablos",
                "Montse Cuadros."
            ],
            "title": "Hate speech dataset from a white supremacy forum",
            "venue": "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11\u201320.",
            "year": 2018
        },
        {
            "authors": [
                "Pieter Delobelle",
                "Ewoenam Tokpo",
                "Toon Calders",
                "Bettina Berendt."
            ],
            "title": "Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Jiawen Deng",
                "Hao Sun",
                "Zhexin Zhang",
                "Jiale Cheng",
                "Minlie Huang."
            ],
            "title": "Recent advances towards safe, responsible, and moral dialogue systems: A survey",
            "venue": "arXiv preprint arXiv:2302.09270.",
            "year": 2023
        },
        {
            "authors": [
                "Jwala Dhamala",
                "Varun Kumar",
                "Rahul Gupta",
                "Kai-Wei Chang",
                "Aram Galstyan."
            ],
            "title": "An analysis of the effects of decoding algorithms on fairness in openended language generation",
            "venue": "2022 IEEE Spoken Language Technology Workshop (SLT), pages 655\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Jwala Dhamala",
                "Tony Sun",
                "Varun Kumar",
                "Satyapriya Krishna",
                "Yada Pruksachatkun",
                "Kai-Wei Chang",
                "Rahul Gupta."
            ],
            "title": "Bold: Dataset and metrics for measuring biases in open-ended language generation",
            "venue": "Proceedings of the 2021 ACM conference",
            "year": 2021
        },
        {
            "authors": [
                "Emily Dinan",
                "Gavin Abercrombie",
                "A. Bergman",
                "Shannon Spruit",
                "Dirk Hovy",
                "Y-Lan Boureau",
                "Verena Rieser."
            ],
            "title": "SafetyKit: First aid for measuring safety in open-domain conversational systems",
            "venue": "Proceedings of the 60th Annual Meeting of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Emily Dinan",
                "Gavin Abercrombie",
                "A. Stevie Bergman",
                "Shannon Spruit",
                "Dirk Hovy",
                "Y-Lan Boureau",
                "Verena Rieser"
            ],
            "title": "Anticipating safety issues in e2e conversational ai: Framework and tooling",
            "year": 2021
        },
        {
            "authors": [
                "Emily Dinan",
                "Angela Fan",
                "Adina Williams",
                "Jack Urbanek",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Queens are powerful too: Mitigating gender bias in dialogue generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Emily Dinan",
                "Angela Fan",
                "Ledell Wu",
                "Jason Weston",
                "Douwe Kiela",
                "Adina Williams."
            ],
            "title": "Multidimensional gender bias classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Florian E Dorner",
                "Momchil Peychev",
                "Nikola Konstantinov",
                "Naman Goel",
                "Elliott Ash",
                "Martin Vechev."
            ],
            "title": "Human-guided fair classification for natural language processing",
            "venue": "arXiv preprint arXiv:2212.10154.",
            "year": 2022
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang."
            ],
            "title": "Latent hatred: A benchmark for understanding implicit hate speech",
            "venue": "Proceedings of the 2021 Conference on Empirical Meth-",
            "year": 2021
        },
        {
            "authors": [
                "Prakhar Ganesh",
                "Hongyan Chang",
                "Martin Strobel",
                "Reza Shokri."
            ],
            "title": "On the impact of machine learning randomness on group fairness",
            "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201923, page 1789\u20131800,",
            "year": 2023
        },
        {
            "authors": [
                "Deep Ganguli",
                "Amanda Askell",
                "Nicholas Schiefer",
                "Thomas Liao",
                "Kamil\u0117 Luko\u0161i\u016bt\u0117",
                "Anna Chen",
                "Anna Goldie",
                "Azalia Mirhoseini",
                "Catherine Olsson",
                "Danny Hernandez"
            ],
            "title": "The capacity for moral selfcorrection in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2020
        },
        {
            "authors": [
                "Aparna Garimella",
                "Akhash Amarnath",
                "Rada Mihalcea."
            ],
            "title": "Demographic-aware language model fine-tuning as a bias mitigation technique",
            "venue": "AACLIJCNLP 2022, page 311.",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A Smith."
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Charlotte Perkins Gilman."
            ],
            "title": "The Man-Made World; or, Our Androcentric Culture",
            "venue": "Hyweb Technology Co. Ltd.",
            "year": 2011
        },
        {
            "authors": [
                "Amelia Glaese",
                "Nat McAleese",
                "Maja Tr\u0119bacz",
                "John Aslanides",
                "Vlad Firoiu",
                "Timo Ewalds",
                "Maribeth Rauh",
                "Laura Weidinger",
                "Martin Chadwick",
                "Phoebe Thacker"
            ],
            "title": "Improving alignment of dialogue agents via targeted human judgements",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Gokaslan",
                "Vanya Cohen."
            ],
            "title": "Openwebtext corpus",
            "venue": "http://Skylion007.github.io/ OpenWebTextCorpus.",
            "year": 2019
        },
        {
            "authors": [
                "Hila Gonen",
                "Yoav Goldberg."
            ],
            "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar."
            ],
            "title": "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Saghar Hosseini",
                "Hamid Palangi",
                "Ahmed Hassan Awadallah."
            ],
            "title": "An empirical study of metrics to measure representational harms in pre-trained language models",
            "venue": "arXiv preprint arXiv:2301.09211.",
            "year": 2023
        },
        {
            "authors": [
                "Po-Sen Huang",
                "Huan Zhang",
                "Ray Jiang",
                "Robert Stanforth",
                "Johannes Welbl",
                "Jack Rae",
                "Vishal Maini",
                "Dani Yogatama",
                "Pushmeet Kohli."
            ],
            "title": "Reducing sentiment bias in language models via counterfactual evaluation",
            "venue": "Findings of the Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Shtedritski",
                "Yuki Asano."
            ],
            "title": "Bias out-of-thebox: An empirical analysis of intersectional occupational biases in popular generative language models",
            "venue": "Advances in neural information processing systems, 34:2611\u20132624.",
            "year": 2021
        },
        {
            "authors": [
                "Rafal Kocielnik",
                "Shrimai Prabhumoye",
                "Vivian Zhang",
                "R Michael Alvarez",
                "Anima Anandkumar."
            ],
            "title": "Autobiastest: Controllable sentence generation for automated and open-ended social bias testing in language models",
            "venue": "arXiv preprint arXiv:2302.07371.",
            "year": 2023
        },
        {
            "authors": [
                "Alyssa Lees",
                "Vinh Q Tran",
                "Yi Tay",
                "Jeffrey Sorensen",
                "Jai Gupta",
                "Donald Metzler",
                "Lucy Vasserman."
            ],
            "title": "A new generation of perspective api: Efficient multilingual character-level transformers",
            "venue": "arXiv preprint arXiv:2202.11176.",
            "year": 2022
        },
        {
            "authors": [
                "Shahar Levy",
                "Koren Lazar",
                "Gabriel Stanovsky."
            ],
            "title": "Collecting a large-scale gender bias dataset for coreference resolution and machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2470\u20132480.",
            "year": 2021
        },
        {
            "authors": [
                "Sharon Levy",
                "Emily Allaway",
                "Melanie Subbiah",
                "Lydia Chilton",
                "Desmond Patton",
                "Kathleen McKeown",
                "William Yang Wang."
            ],
            "title": "Safetext: A benchmark for exploring physical safety in language models",
            "venue": "arXiv preprint arXiv:2210.10045.",
            "year": 2022
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Irene Mengze Li",
                "Emily Zheng",
                "Yao Chong Lim",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "Towards debiasing sentence representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Chiyu Wu",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Towards understanding and mitigating social biases in language models",
            "venue": "International Conference on Machine Learning, pages 6565\u20136576. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models. arXiv preprint arXiv:2211.09110",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Ruibo Liu",
                "Chenyan Jia",
                "Jason Wei",
                "Guangxuan Xu",
                "Lili Wang",
                "Soroush Vosoughi."
            ],
            "title": "Mitigating political bias in language models through reinforced calibration",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14857\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyi Ma",
                "Kawin Ethayarajh",
                "Tristan Thrush",
                "Somya Jain",
                "Ledell Wu",
                "Robin Jia",
                "Christopher Potts",
                "Adina Williams",
                "Douwe Kiela."
            ],
            "title": "Dynaboard: An evaluation-as-a-service platform for holistic nextgeneration benchmarking",
            "venue": "Advances in Neural Infor-",
            "year": 2021
        },
        {
            "authors": [
                "Chandler May",
                "Alex Wang",
                "Shikha Bordia",
                "Samuel Bowman",
                "Rachel Rudinger."
            ],
            "title": "On measuring social biases in sentence encoders",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Meade",
                "Elinor Poole-Dayan",
                "Siva Reddy."
            ],
            "title": "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Katelyn Mei",
                "Sonia Fereidooni",
                "Aylin Caliskan."
            ],
            "title": "Bias against 93 stigmatized groups in masked language models and downstream sentiment classification tasks",
            "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher."
            ],
            "title": "Pointer sentinel mixture models",
            "venue": "ArXiv, abs/1609.07843.",
            "year": 2016
        },
        {
            "authors": [
                "Ioannis Mollas",
                "Zoe Chrysopoulou",
                "Stamatis Karlos",
                "Grigorios Tsoumakas."
            ],
            "title": "Ethos: an online hate speech detection dataset",
            "venue": "arXiv preprint arXiv:2006.08328.",
            "year": 2020
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "Stereoset: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel Bowman."
            ],
            "title": "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Hadas Orgad",
                "Yonatan Belinkov."
            ],
            "title": "Choose your lenses: Flaws in gender bias evaluation",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 151\u2013167.",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Alicia Parrish",
                "Angelica Chen",
                "Nikita Nangia",
                "Vishakh Padmakumar",
                "Jason Phang",
                "Jana Thompson",
                "Phu Mon Htut",
                "Samuel Bowman."
            ],
            "title": "Bbq: A hand-built bias benchmark for question answering",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Leif E Peterson."
            ],
            "title": "K-nearest neighbor",
            "venue": "Scholarpedia, 4(2):1883.",
            "year": 2009
        },
        {
            "authors": [
                "Mat\u00fa\u0161 Pikuliak",
                "Ivana Be\u0148ov\u00e1",
                "Viktor Bachrat\u1ef3."
            ],
            "title": "In-depth look at word filling societal bias measures",
            "venue": "arXiv preprint arXiv:2302.12640.",
            "year": 2023
        },
        {
            "authors": [
                "Rebecca Qian",
                "Candace Ross",
                "Jude Fernandes",
                "Eric Michael Smith",
                "Douwe Kiela",
                "Adina Williams."
            ],
            "title": "Perturbation augmentation for fairer NLP",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Hila Gonen",
                "Michael Twiton",
                "Yoav Goldberg."
            ],
            "title": "Null it out: Guarding protected attributes by iterative nullspace projection",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Ross",
                "Tongshuang Wu",
                "Hao Peng",
                "Matthew Peters",
                "Matt Gardner."
            ],
            "title": "Tailor: Generating and perturbing text with semantic controls",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Haitham Seelawi",
                "Debora Nozza",
                "Zeerak Talat",
                "Bertie Vidgen."
            ],
            "title": "Multilingual HateCheck: Functional tests for multilingual hate speech detection models",
            "venue": "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH), pages",
            "year": 2022
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Bertie Vidgen",
                "Dong Nguyen",
                "Zeerak Waseem",
                "Helen Margetts",
                "Janet Pierrehumbert."
            ],
            "title": "HateCheck: Functional tests for hate speech detection models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme."
            ],
            "title": "Gender bias in coreference resolution",
            "venue": "Proceedings of NAACLHLT, pages 8\u201314.",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1408\u2013 1424.",
            "year": 2021
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "The woman worked as a babysitter: On biases in language generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "Towards controllable biases in language generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239\u20133254.",
            "year": 2020
        },
        {
            "authors": [
                "Kurt Shuster",
                "Jing Xu",
                "Mojtaba Komeili",
                "Da Ju",
                "Eric Michael Smith",
                "Stephen Roller",
                "Megan Ung",
                "Moya Chen",
                "Kushal Arora",
                "Joshua Lane"
            ],
            "title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
            "year": 2022
        },
        {
            "authors": [
                "Eric Michael Smith",
                "Melissa Hall",
                "Melanie Kambadur",
                "Eleonora Presani",
                "Adina Williams."
            ],
            "title": "I\u2019m sorry to hear that\u201d: Finding new biases in language models with a holistic descriptor dataset",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Eric Michael Smith",
                "Adina Williams."
            ],
            "title": "Hi, my name is martha: Using names to measure and mitigate bias in generative dialogue models",
            "venue": "arXiv preprint arXiv:2109.03300.",
            "year": 2021
        },
        {
            "authors": [
                "Anna Sotnikova",
                "Yang Trista Cao",
                "Hal Daum\u00e9 III",
                "Rachel Rudinger."
            ],
            "title": "Analyzing stereotypes in generative text inference tasks",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 4052\u20134065.",
            "year": 2021
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Luke Melas-Kyriazi",
                "Dan Jurafsky."
            ],
            "title": "Prompt-and-rerank: A method for zeroshot and few-shot arbitrary textual style transfer with small language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Megan Ung",
                "Jing Xu",
                "Y-Lan Boureau."
            ],
            "title": "Saferdialogues: Taking feedback gracefully after conversational safety failures",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6462\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jack Urbanek",
                "Pratik Ringshia."
            ],
            "title": "Mephisto: A framework for portable, reproducible, and iterative crowdsourcing",
            "venue": "arXiv preprint arXiv:2301.05154.",
            "year": 2023
        },
        {
            "authors": [
                "Pranav Narayanan Venkit",
                "Sanjana Gautam",
                "Ruchi Panchanadikar",
                "Shomir Wilson"
            ],
            "title": "Nationality bias in text generation",
            "venue": "arXiv preprint arXiv:2302.02463",
            "year": 2023
        },
        {
            "authors": [
                "Jesse Vig",
                "Sebastian Gehrmann",
                "Yonatan Belinkov",
                "Sharon Qian",
                "Daniel Nevo",
                "Yaron Singer",
                "Stuart Shieber."
            ],
            "title": "Investigating gender bias in language models using causal mediation analysis",
            "venue": "Advances in neural information processing systems, 33:12388\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Hrishikesh Viswanath",
                "Tianyi Zhang."
            ],
            "title": "Fairpy: A toolkit for evaluation of social biases and their mitigation in large language models",
            "venue": "arXiv preprint arXiv:2302.05508.",
            "year": 2023
        },
        {
            "authors": [
                "Eric Wallace",
                "Shi Feng",
                "Nikhil Kandpal",
                "Matt Gardner",
                "Sameer Singh."
            ],
            "title": "Universal adversarial triggers for attacking and analyzing nlp",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyz-",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Wenzek",
                "Marie-Anne Lachaux",
                "Alexis Conneau",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "CCNet: Extracting high quality monolingual datasets from web crawl data",
            "venue": "Proceedings of the Twelfth Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Ledell Wu",
                "Fabio Petroni",
                "Martin Josifoski",
                "Sebastian Riedel",
                "Luke Zettlemoyer."
            ],
            "title": "Scalable zeroshot entity linking with dense entity retrieval",
            "venue": "arXiv preprint arXiv:1911.03814.",
            "year": 2019
        },
        {
            "authors": [
                "Zonghan Yang",
                "Xiaoyuan Yi",
                "Peng Li",
                "Yang Liu",
                "Xing Xie."
            ],
            "title": "Unified detoxifying and debiasing in language generation via inference-time adaptive optimization",
            "venue": "arXiv preprint arXiv:2210.04492.",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "ArXiv, abs/2211.01910.",
            "year": 2022
        },
        {
            "authors": [
                "Terry Yue Zhuo",
                "Yujin Huang",
                "Chunyang Chen",
                "Zhenchang Xing."
            ],
            "title": "Exploring ai ethics of chatgpt: A diagnostic analysis",
            "venue": "arXiv preprint arXiv:2301.12867.",
            "year": 2023
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "We focus in part on metrics calculated using templates in this work, due to their flexibility. Templates used to measure regard in Sheng et al",
            "venue": "Kirk et al",
            "year": 2020
        },
        {
            "authors": [
                "Venkit"
            ],
            "title": "2023) present additional approaches for creating bias measurement templates over a wide demographic range. Templatebased bias datasets can be contrasted with crowdsourced",
            "year": 2023
        },
        {
            "authors": [
                "Blodgett"
            ],
            "title": "2021) and Pikuliak et al. (2023) discuss methodological and data quality issues with the latter two",
            "year": 2023
        },
        {
            "authors": [
                "tava"
            ],
            "title": "2022) and HELM (Liang et al., 2022), that each also provide coverage of a few bias benchmarks. Most similar to us, Viswanath and Zhang (2023) has recently open-sourced a suite of bias",
            "year": 2023
        },
        {
            "authors": [
                "taged groups. Dorner"
            ],
            "title": "2022) performs word perturbation using demographic terms from HolisticBias (Smith et al., 2022), similar to this work, but for debiasing toxicity",
            "year": 2022
        },
        {
            "authors": [
                "Smith",
                "Williams"
            ],
            "title": "BlenderBot (Shuster et al., 2022) to reduce bias on a conversation partner\u2019s name, and Borchers et al. (2022) investigates prompt-engineering and fine-tuning as a means of reducing gender bias in job ads",
            "year": 2022
        },
        {
            "authors": [
                "Abid"
            ],
            "title": "2021) reduces anti-Muslim bias simply by prepending a short prompt containing positive associations about Muslims",
            "year": 2021
        },
        {
            "authors": [
                "Following Gehman"
            ],
            "title": "2020), we score RealToxicityPrompts using Perspective API9, with a generation labeled as toxic if its toxicity score exceeds 50%. For BOLD, since the classifier used by Dhamala et al",
            "year": 2021
        },
        {
            "authors": [
                "Sheng"
            ],
            "title": "Regarding the performance of the classifiers",
            "venue": "Lees et al",
            "year": 2022
        },
        {
            "authors": [
                "Wallace"
            ],
            "title": "2019), and applied to bias mitigation by Sheng et al. (2020). We take the target model\u2019s generations along with labels given by a classifier",
            "year": 2020
        },
        {
            "authors": [
                "Sheng"
            ],
            "title": "Questions asked to crowdsourced workers when rating generations from models with bias and toxicity mitigation. metrics. For the self-debiased BB3 model we see reductions in bias",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "NOTE: this paper contains examples of bias and toxicity in text that may be offensive or upsetting."
        },
        {
            "heading": "1 Introduction",
            "text": "The recent explosion of large generative language models has brought with it an increased focus on\n\u2217Equal contribution. 1https://github.com/facebookresearch/\nResponsibleNLP/tree/main/robbie\nthe potential risks posed by these models. Previously released base LLMs have displayed strong social biases as a function of gender, race, and other demographic axes (Chowdhery et al., 2022; Glaese et al., 2022; Ouyang et al., 2022; Touvron et al., 2023a), and many recent works have found that biases tend to increase as models grow in size (Vig et al., 2020; Smith and Williams, 2021; Biderman et al., 2023; Ganguli et al., 2023; Hosseini et al., 2023). Although some post hoc techniques relying on human feedback for mitigating bias have shown promise (Glaese et al., 2022; Bai et al., 2022), the extent to which such approaches actually remove problematic biases, as opposed to simply hiding them (c.f. Gonen and Goldberg 2019), is not fully known. Therefore, in this work, we focus on base (i.e. foundational) LLMs, prior to the application of finetuning techniques such as reinforcement learning from human feedback (RLHF), to better understand their core social biases, so that we can target mitigations at their source.\nTo distinguish bias from related societal harms such as offensiveness, we define \u201cbias\u201d in this work as the proportion of subgroups for which the frequency of toxicity and negative regard generations falls outside an acceptable threshold. This definition is rooted in the principle of demographic parity, serving as a benchmark for equality and fairness, as previously applied in the context of fairness assessment within natural language processing (Sheng et al., 2019; Dhamala et al., 2021; Chowdhery et al., 2022; Glaese et al., 2022; Kirk et al., 2021; Hartvigsen et al., 2022; Hosseini et al., 2023)\u2014the field is still in a very preliminary stage, with coverage often restricted to measuring bias for only one demographic axis, most commonly binary gender (Table 1), or at best a handful of axes. As such, many previous works are incapable of even surfacing potential issues along axes that fall out-of-scope, such as race/ethnicity, religion, disability, age, or socioeconomic class, or along\nintersections of multiple axes. To make matters worse, recent bias evaluations on state-of-the-art generative LLMs utilize a dizzying array of different quantitative metrics (Chowdhery et al., 2022; Glaese et al., 2022; Shuster et al., 2022; Zhang et al., 2022)2 making it difficult to quantitatively compare models based on biases and overall performance. This is a problem, because our end goal is to have less biased models, but until we have strong and inclusive enough sets of metrics that enable cross-model comparisons, we can\u2019t make headway on the important work of devising and comparing bias mitigation strategies.\nIn this work, we enable direct model comparison by evaluating LLMs from several model families on an expanded suite of bias and toxicity metrics across an expanded set of demographic axes. To further foreground often-overlooked demographic axes, we augment the community standard Regard dataset (Sheng et al., 2019) with 700+ demographic identity terms from the HolisticBias dataset (Smith et al., 2022). We also perform stratified sampling from two Jigsaw toxicity datasets in order to create AdvPromptSet, a novel dataset that allows for expanded testing of bias across intersections of identities. We are open-sourcing our model suite so that others can easily utilize our tooling.\nA crucial reason to expand our analysis of bias in LLMs to more demographic axes and metrics is to potentiate the development of bias and toxicity mitigation techniques: most recent mitigation work reports information about only a single metric, demographic axis, or model, raising serious open questions as whether they can be applied to new settings. As we expand our ability to uncover biases along more axes and for more metrics, determining which mitigations will be most effective at addressing them becomes increasingly important.\n2See additional discussion of related work in Section A.\nWe take initial steps to investigate this by comparing 3 bias/toxicity mitigation techniques across our suite of metrics. Our results suggest that some mitigations are better suited to some settings than others: for example, biases exposed by the BOLD evaluations can generally be lessened using selfdebiasing, but the mitigation is more effective for GPT-2 than for BB3. We hope that our results will provide useful insights that can guide practitioners in selecting mitigation techniques appropriate for their setting.\nTo summarize, we analyze different measurements and mitigations for bias and toxicity in generative LLMs. Our main contributions are (1) a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs; (2) an extension of prompt-based metrics to more intersections of demographic groups via a new dataset, AdvPromptSet, and the demographic terms of HolisticBias; (3) a comparison of how well 3 bias and toxicity mitigation techniques compare across our suite of measurements; (4) an exploration of the frequency of demographic terms in several LLM pretraining corpora and how this may relate to model biases; and (5) an open-sourced toolkit for robust measurement across these metrics."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 LLMs",
            "text": "We test 5 families of generative LLMs: GPT-2 (Radford et al., 2019), OPT (Zhang et al., 2022), BlenderBot 3 (Shuster et al., 2022), BLOOM (Scao et al., 2022), and LLaMa (Touvron et al., 2023a). We focus on base models that have not undergone reinforcement learning from human or AI feedback (RLHF/RLAIF) (Christiano et al., 2017; Bai et al.,\n2022; Ouyang et al., 2022).3 For several models we test them at different sizes (Table 9). See Section B.2 for more details."
        },
        {
            "heading": "2.2 Frequencies of demographic terms in LLMs training corpora",
            "text": "Bias in LLMs can potentially come from the datasets that they are trained on. To better contextualize our bias metrics for particular demographic axes, we also measure the frequencies of certain words and phrases with demographic associations in a few different datasets that are commonly used as part of LLMs\u2019 training corpora. Our goals are to (1) potentially observe whether these frequencies correspond to known demographic biases, and (2) compare these datasets by analyzing the frequencies on the individual corpus level. Section B.4 provides additional methodological details."
        },
        {
            "heading": "2.3 Automatic evaluation metrics for benchmarking LLMs",
            "text": ""
        },
        {
            "heading": "2.3.1 Existing bias and toxicity metrics",
            "text": "We test LLMs by generating continuations given the following datasets of prompts: (1) Regard (Sheng et al., 2019), a set of templates to measure the model\u2019s regard (i.e. respect, esteem) for different demographic groups; (2) RealToxicityPrompts (Gehman et al., 2020), a stratified subset of text from a web text corpus (Gokaslan and Cohen, 2019) at different levels of toxicity; (3) BOLD (Dhamala et al., 2021), prompts extracted from Wikipedia articles across five demographic axes; and (4) ToxiGen (Hartvigsen et al., 2022), a dataset for adversarial and implicit hate speech detection generated by GPT-3 (Brown et al., 2020). All datasets are written in English.\nEach of the metrics in the ROBBIE benchmark suite consists of a dataset of prompts and a classifier used to score continuations on them: see Table 2 for information on datasets and their corresponding classifiers. Section B.1.1 gives more metric details."
        },
        {
            "heading": "2.3.2 AdvPromptSet: extending bias metrics to intersections of identities",
            "text": "We propose AdvPromptSet, a comprehensive and challenging adversarial text prompt set with 197,628 prompts of varying toxicity levels and more than 24 sensitive demographic identity groups\n3Note that RLHF can dramatically reduce toxicity, as seen from the comparison by Touvron et al. (2023b) of Llama 2- Chat to Llama 2 and Llama 1 (styled here as \u201cLLaMa\u201d) on the ToxiGen dataset.\nand combinations. AdvPromptSet is based on two open-sourced Jigsaw toxicity datasets4, with each prompt containing at least one term from toxicity and bias word lists of contextually-sensitive associations. Intuitively, toxic prompts are more likely to cause generative models to create toxic content. However, AdvPromptSet is designed to be adversarial, meaning that even benign prompts may solicit generations that are not benign\u2014this can happen when the generative models fail to understand the meaning of the prompts, or when they have learned toxic associations with particular demographic groups. AdvPromptSet can be downsized to cater to the user\u2019s needs, and we have opensourced code to produce both the full version and a downsized version consisting of 10K prompts.5\nWe use a two-stage approach to create the AdvPromptSet dataset, as illustrated in Figure 1. In the first stage, we extract words or short sentences from multiple toxicity and bias word sources, using entity linking models (Wu et al., 2019) to extract entities from a given text snippet. We then expand our list of toxicity and bias terms by finding synonyms for each term in Wikipedia via Sentence-BERT (Reimers and Gurevych, 2019), using k-Nearest Neighbors (KNN) search (Peterson, 2009).\nIn the second stage, we use the expanded terms list with exact matching to extract adversarial prompts from the Jigsaw toxicity datasets containing at least one term. While the Jigsaw Unintended Bias in Toxicity Classification dataset provides labels for a subset of comments according to their listed demographic attributes, we wanted to unify our approach throughout. Towards that end, we perform a similar embedding-based KNN search to predict identity labels for comments without toxicity annotations from Jigsaw. We provide our list of identity labels and their KNN similar words in Section B.1.3.\nEach prompt in the final dataset is annotated with its toxicity label (yes or no), identity labels (demographic categories mentioned), and the number of toxicity and bias terms present.6 Table 8\n4https://www.kaggle.com/competitions/ jigsaw-toxic-comment-classification-challenge, https://www.kaggle.com/competitions/ jigsaw-unintended-bias-in-toxicity-classification\n5https://github.com/facebookresearch/ ResponsibleNLP/tree/main/AdvPromptSet\n6Note that because we include in AdvPromptSet only those sentences in a comment that contained at least one toxicity and bias term, labels from the Jigsaw Unintended Bias in Toxicity Classification dataset may or may not be preserved.\ngives the frequency of each label in AdvPromptSet. See Section B.1.2 for more details."
        },
        {
            "heading": "2.3.3 HolisticBiasR: extending metrics via demographic term replacement",
            "text": "We expand the templated Regard dataset to other demographic categories by replacing the demographic noun phrases in the original prompts (e.g. \u201cBlack man\u201d, \u201cwoman\u201d) with noun phrases from the HolisticBias dataset, v1.1 (Smith et al., 2022): we denote this new dataset as HolisticBiasR. These phrases typically consist of a demographic adjective and a noun referring to a person (e.g. \u201cbluecollar grandfather\u201d, \u201cdisabled parent\u201d)."
        },
        {
            "heading": "2.3.4 Performance metrics",
            "text": "To better contextualize our bias and toxicity measurements, we also report evaluations of the generative capabilities and inference efficiency of each model. To assess generation quality, we sample prompt contexts from the WikiText-103 dataset (Merity et al., 2016) and score generations using perplexity from GPT-3\u2019s text-davinci-002 (Ouyang et al., 2022). At inference time, we also measure token throughput, latency, and peak device memory utilization. More details in Section B.1.4."
        },
        {
            "heading": "2.4 Bias/toxicity mitigation techniques",
            "text": "We measure the robustness of the following bias and toxicity mitigation techniques across several models, metrics, and demographic axes: (1) prompting with hand-written templates and automatic prompt revision (Zhou et al., 2022); (2) self-debiasing (Schick et al., 2021), which shifts the token probability distribution during generation to suppress tokens used in biased text; and (3) adversarial triggering (Wallace et al., 2019), which\nidentifies a prefix string to optimally control generations, employed by Sheng et al. (2020) for bias reduction. More details in Section B.3."
        },
        {
            "heading": "3 Results",
            "text": ""
        },
        {
            "heading": "3.1 Benchmarking: Comparison of automatic metrics across models and demographic axes",
            "text": "First, we obtain quantitative measurements of toxicity, negative regard, and bias on model generations. In addition to providing base levels that we can use to compare mitigation strategies, these results also allow us to determine whether metrics differ in how they rate models of different size, family, and prompt datasets. Figure 2 shows the rates of toxicity and negative regard in model generations, and Table 3 shows a measure of the corresponding biases. Section C.1.1 provides an analysis of the effect of model size, family, and decoding settings on toxicity, regard, and bias metrics.\nDefining a bias score. There has been a large body of work on fairness in NLP models based on demographic parity as a criteria for equality and fairness (Czarnowska et al., 2021; Huang et al., 2020). However, in this work we are focusing on avoiding negative outcomes from LLMs, and thus, instead of aiming for a strict notion of equality, we would like LLMs to sufficiently respect all subgroups. Therefore, similar to Background Comparison Metrics (Czarnowska et al., 2021), we define a baseline or background score Bb for each benchmark b. This baseline reflects the general performance on the set of all evaluation prompts, against which we can measure bias as a downward shift from the background rate for each subgroup.\nMore formally, let Sb = {s1, s2, ..., s|Sb|} be a set of subgroups and Xbsi = {x 1 si , x 2 si , ..., x Kb si } be the set of prompts about subgroup si in dataset b where Kb is the number of prompts in Xbsi . We define the background Bb as the maximum allowed\nlikelihood of negative LLMs responses, where a generation is considered \u201cnegative\u201d if it is classified as either toxic or having a negative regard. The goal is that the likelihood of the negative responses about each subgroup should be lower than\nBb for each dataset b. We define the likelihood of a negative response about a subgroup si as PrNeg(Xbsi) = \u2211Kb j=1 y\u0302 j si/Kb, where y\u0302 j si is the predicted binary label of the LLM continuation to prompt xjsi via an automatic classifier. The classifier assigns y\u0302jsi = 1 to a negative continuation and y\u0302jsi = 0 to a benign continuation.\nWe define BiasScore as the percentage of subgroups in that dataset whose PrNeg(Xbsi) is above the background Bb (see Appendix C.4 Table 9 for the background rates across datasets, metrics, and models). According to our definition above, the ideal BiasScore should be zero, meaning that the rate of negativity for any given subgroup should be within an acceptable range, i.e. PrNeg(Xbsi) \u2264 Bb; but we also should keep track of maxsi\u2208Sb PrNeg(X b si), which is the upper bound of the rate of negativity across subgroups. This max shows how much the LLMs are marginalizing any specific subgroup. We perform bootstrap sampling with a 95% confidence interval and 10,000 re-sampling iterations over the LLM responses to estimate the distribution for PrNeg(Xbsi). We use this distribution to measure BiasScore and find the confidence intervals for\nthe subgroup with the maximum median in each benchmark dataset b (see Appendix C.4 Table 25 and Table 26).\nResults for Subgroup Marginalization. We use the upper bound of the confidence interval for PrNeg(Xbsi) and compare it with the background Bb to calculate the BiasScore for each LLM and prompt dataset in Table 3.\nTable 3 shows that even though BOLD doesn\u2019t elicit high rates of toxicity due to its particular text domain, it still shows that a high percentage of subgroups are above the baseline BBOLD. Please note that our analysis method can be used to measure bias for any subset of groups in each dataset. To show this, we perform the same analysis split by demographics (gender/sex, nationality, race/ethnicity, sexual orientation, etc) in Appendix C.4."
        },
        {
            "heading": "3.1.1 Measuring fine-grained and intersectional biases",
            "text": "By construction, AdvPromptSet and HolisticBiasR go beyond many other datasets in allowing for the exploration of biases in intersections of demographic identities.\nAdvPromptSet. By querying prompts that contain particular pairs of demographic terms, we can look at bias in model generations across intersections7 of demographic axes. Looking at the intersection of race and gender, Table 4 shows that GPT2-XL produces toxic generations most often in response to toxic prompts with the attribute label \u201casian\u201d, especially if the prompt also has the label \u201cfemale\u201d. Looking at the intersection of gender and sexuality, we see a significant increase in toxicity in response to toxic prompts with the labels \u201ctransgender\u201d and \u201chomosexual\u201d, compared with any other combination. See Section C.1.2 for more details.\nHolisticBiasR. By injecting HolisticBias descriptor/noun phrases into Regard prompt templates, we can identify patterns across model families in which demographic descriptor terms have consistently high or low rates of negative regard. Table 5 shows these trends for the race/ethnicity axis, and Table 11 presents further results on the gender/sex, religion, and sexual orientation axes. While the ranking of groups does change somewhat across\n7These intersections only indicate the presence of both demographic terms in the prompt, rather than the presence of a single intersectional identity. These results may still be an indication of how a model may treat intersectional identities but this is not what is explicitly being tested.\nmodels, there are trends: for example, every model has at least one Hispanic or Latino descriptor in the list of 5 with the highest negative regard, and at least one Asian or Pacific Islander descriptor in the list of 5 with the lowest negative regard. These trends may reveal ingrained cultural assumptions about specific demographic groups and/or data sampling artifacts in the models\u2019 pretraining corpora. It thus may be fruitful to explore ways of targeting mitigations to these groups in particular.\nBecause many nouns in the HolisticBias dataset are gendered, we can also measure the differences in negative regard rates between noun phrases referring to women vs. men (e.g. \u201cAsian grandma\u201d vs. \u201cAsian grandpa\u201d; see appendix section C.1.2)."
        },
        {
            "heading": "3.2 Mitigation: Comparing techniques for bias mitigation and toxicity reduction",
            "text": "We test the effectiveness of the the bias/toxicity mitigation techniques discussed in Section 2.4 on the\n1.5B-parameter GPT2-XL and the 175B-parameter BlenderBot 3 (BB3), two models that differ dramatically in terms of size and training data. BB3 was chosen as representative of conversational text, and GPT2-XL was chosen as representative of generic task-agnostic text generation.\nReduction of toxicity and negative regard. For GPT2-XL, Table 6 shows that the self-debiasing technique performs by far the best at suppressing rates of toxicity and negative regard, with a 46% reduction on the average prompting dataset. On BlenderBot3-175B, however, the self-debiasing technique is less effective for reducing toxicity and negative regard on average. For BlenderBot3-175B, the prompting technique performs better, achieving a 28% mean reduction across datasets. We hypothesize that the much larger capacity of BlenderBot3175B may make it much more capable of adjusting its output via prompting, but that its generations can conversely not be manipulated so easily by a sim-\nple token reweighting in the case of self-debiasing. See Section C.2.1 for more details.\nOur human evaluation results are somewhat nuanced, but still lend support to the findings in Table 6: for GPT2-XL mitigated with self-debiasing, human evaluation also shows a decrease in negative regard, in addition to an increase in overall coherence, with other metrics maintaining baseline levels. For BlenderBot3-175B, prompting lessens negative regard while maintaining fluency, and it shows improvement on toxicity and immorality metrics as well. See Section C.2.4 more information about human evaluations.\nReduction of bias. For GPT2-XL, Table 6 shows that the prompting approach doesn\u2019t have any significant impact on BiasScore, a result that is verified by human evaluation that finds no difference between GPT2-XL pre- and post-prompting mitigation. However, self-debiasing and adversarial triggering methods do decrease the BiasScore across all benchmark datasets. Human evaluation is able to verify that adversarial triggering is effective, but finds less evidence of improvement from self-debiasing. Conversely, for BlenderBot3-175B, the self-debiasing approach increases BiasScore on all benchmark datasets except Regard, while the impact of the prompting method is varied across benchmarks, although human evaluation complicates this finding, as it suggests that all mitigations can lessen bias in BlenderBot3-175B. This implies that the complex issue of fairness in LLMs requires more advanced mitigation methods as our models grow larger and more complex. See Section C.2.2 for more details on the most marginalized groups\nafter applying these methods and Section C.2.4 for more details on human evaluation methods and results.\nPerformance metrics. Table 15 suggests tradeoffs in generation quality and minimal impact to inference efficiency with all mitigations that we test. See Section C.2.3 for more details."
        },
        {
            "heading": "3.3 Root cause analysis: Frequencies of demographic terms in training corpora",
            "text": "How the models behave depends massively on the training datasets that we feed them (Ganesh et al., 2023). To understand the distribution of demographic terms in some common training corpora, we present two sets of analyses: (1) the percentage of documents mentioning each of the HolisticBias descriptors in different demographic axes across the corpora, and (2) the percentage of documents mentioning different genders (represented by common pronouns) (Section C.3.3)."
        },
        {
            "heading": "3.3.1 HolisticBias descriptors",
            "text": "We consider the percentage of documents in training datasets mentioning a specific HolisticBias demographic term. There are limitations to this analysis given that demographic terms can have nondemographic meanings (\u201cwhite\u201d, \u201cpan\u201d, etc.), but the differences in the relative frequencies of terms across datasets can still be illuminating.\nIn Table 7, we observe that the word \u201cfemale\u201d is found more often than the term \u201cmale\u201d across most datasets, with web crawl data and Wikipedia (en) having the largest disparities. This may seem counter-intuitive given the relative rates of female\nvs. male pronouns (Section C.3.3), but we hypothesize that \u201cfemale\u201d may be used more often than \u201cmale\u201d to refer to a deviation away from a default (i.e. \u201cmale\u201d) gender (c.f. De Beauvoir 1949; Bem 1993; Gilman 2011; Bailey et al. 2022 i.a.). We note that other gender and sex minority terms appear much less frequently.\nFor results on the protected groups of race, religion, and age, as well as future directions, see Section C.3. We do not find strong evidence that model biases immediately reflect term frequency, although see Section C.3.2 in particular for more discussion of the correspondence between term training frequencies and model biases."
        },
        {
            "heading": "4 Conclusions and future directions",
            "text": "In our analysis, we find that each prompt dataset causes the LLM models to output generations with different rates of toxicity and negative regard. Notably, even when the baseline toxicity rate is minimal, certain demographic biases manifest prominently across specific prompt datasets. Moreover, the prompt datasets studied in this paper, when used in combination with each other, are able to surface a more diverse set of risks posed by LLMs, providing a holistic view into which subgroups may be at higher risk of marginalization by LLMs. We hope that our measurement results show how multimetric measurement can enable us to better understand the possible risks LLMs can pose, and can better expose at-risk groups that may be affected. We accentuate the significance of assessing toxicity and bias concerning intersectional demographics, underscoring instances where the toxic content frequency surges for these groups in contrast to individual demographics. Moreover, we explored several mitigation techniques, gauging their efficacy\nvia both automated metrics and human evaluation. We observed that the self-debiasing technique is mostly effective in smaller LLMs, while prompting is more effective in larger LLMs. We hypothesize that the much larger capacity of larger LLMs may make them much more capable of adjusting their output via prompting. Moreover, these techniques exhibit promising impact in mitigating biases, a finding that encourages further research into their enhancement and expansion for pre-trained LLMs, in addition to instruction-tuning and RLHF, which apply at later stages of model training.\nAnalyzing the demographic distribution in common training corpora, we unveiled an underrepresentation of gender and sex minority terms. This potentially enhances biases against LGBTQ+ groups in LLMs.\nWe aspire for LLMs to effortlessly generate respectful and insightful content about all demographics. Using diverse datasets together helps us analyze bias in a more inclusive way. While the list of demographic and subgroup labels in each prompt dataset is not fully comprehensive, ongoing expansion will boost the inclusiveness of bias analysis. This list of relevant subgroups should evolve constantly to reflect societal and cultural changes. In light of our findings, we recognize the tendency for toxicity and negative regard to escalate with model size. Given the rapid development of larger LLMs and the widespread use of RLHF models, future endeavors could concentrate on establishing benchmarks to assess bias and toxicity within instruction-tuned models. Moving forward, we envision the field\u2019s progression towards improved and widespread utilization of multi-metric bias measurements similar to our exemplified approach, enabling a more comprehensive evaluation of models across a broad spectrum of potential biases."
        },
        {
            "heading": "Limitations",
            "text": "One limitation of the proposed AdvPromptSet is that prompts can contain multiple labels from a single demographic axis (e.g. \u201cwhite\u201d, \u201cblack\u201d) as a result of (i) multiple people referred to in the prompt, (ii) a single entity with multiple attributes on a single axis (e.g. mixed-race, gender-fluid), or (iii) annotation error. For simplicity, we exclude these prompts from our analysis, and pick out prompts containing exactly one attribute from each axis in a given intersection. It is still possible that the labels in AdvPromptSet inherit errors from the original Jigsaw datasets, as they were annotated by human raters. Another important caveat here is that typically unmarked groups may have prompts which aren\u2019t included in the analysis. We only include explicitly marked attributes in this analysis, which does lead us to miss out on potential data points. While we don\u2019t include unmarked attributes in the present analysis, AdvPromptSet can certainly used to look at model behavior with unmarked attributes as well. We discuss further details with examples in Section C.1.2.\nThe datasets studied in this work are composed of English text, but bias and toxicity can of course exist across all languages, and future works should expand bias measurements by using multilingual datasets, as well as datasets targeting additional varieties of English.\nWe acknowledge that bias, toxicity, hate speech, morality, etc. are often region-specific, and that language used to test for these attributes in one location may not be ideal for others: in particular, the results of crowdsourced human evaluations in the United States cannot necessarily be straightforwardly generalized to other English-speaking countries, due to the presence of region-specific cultural factors. The analyses of bias presented here can only be assumed to apply to the demographic groups currently examined.\nWe expect that different bias mitigation strategies may be best suited for different text domains and prompt contexts, and the fact that one model performs better than another on a particular set of datasets does not necessarily imply that the former model is more free of all bias, due in part to the multitude of ways that bias can manifest itself in a piece of generated text. The bias mitigation strategies tested here are considered to be research prototypes, and we would caution against immediately applying them for production use without\nmore testing\u2014side effects may appear when using any new technique to modify training corpora or control generation, and further investigation is needed. In some settings, bias can trade off with other important considerations, such as accuracy, robustness or efficiency. Any attempt to mitigate bias must be done in the context of ensuring that other such unwanted side effects are not inadvertently intensified.\nAdditionally, we tested our mitigations in isolation, applying only one at a time. However, it could be that we might observe even stronger mitigation were we to chain mitigation techniques together, or otherwise use them in tandem. This is an exciting future direction, and we hope that our work will be able to guide future experimentation in this direction.\nWhile our work aims to measure bias along a large range of demographics, we do rely on the industry-standard method of prompting. LLMs can be sensitive to the precise formulation of prompts (Cao et al., 2022a; Suzgun et al., 2022; Liu et al., 2023), and while we do augment some of the prompts in the creation of HolisticBiasR, followup research should explore additional avenues for increasing the linguistic variation in prompts. For example, utilizing syntactic variation like proposed in Ross et al. (2022) and Aggarwal et al. (2022) could introduce additional robustness to our metrics, and as such, we feel that this would be an interesting avenue to explore for future work.\nFinally, given the recent explosion of new applications for LLMs, it is likely that some of their future impacts are as-of-yet unknown, and any attempt to improve model safety must be cognizant of potential unforeseen consequences relating to these sorts of unknown harms."
        },
        {
            "heading": "Ethics statement",
            "text": "In this paper, we conceptualize bias to mean a difference in the frequency of some attribute of generated text (toxicity or a negative regard for the subject) as a function of the demographic group mentioned in the generation prompt. We acknowledge that there are many potential definitions of bias, and that an LLM treating all users completely identically regardless of demographics may not be the most desirable goal: for instance, one could imagine a model needing to handle certain topics with extra care and sensitivity in order to avoid any chance of regurgitating painful stereotypes against\nspecific marginalized communities. The use of a certain bias metric or set of metrics can potentially have a prescriptive effect, implying that they represent the sum total of all potential negative social effects across different demographic groups; given that we do not believe that any such existing set of metrics captures all possible nuances in treatment across every demographic group, any such bias benchmark must grow and evolve to include a fuller understanding of these issues as experienced by the people who they most impact.\nThis paper employs two toxicity classifiers, Perspective API and ToxiGen. Since toxicity is often highly subjective and contextual, we cannot assert that these classifiers completely accurately represent \u201cabsolute\u201d toxicity, given how much the understanding of whether something is toxic to a certain demographic group relies on lived experience as a member of that group. In this work we use crowdsourced workers to rate the bias, toxicity, regard, and morality of models\u2019 generations, but we cannot guarantee that the diversity of these workers represents all demographic groups fully, especially historically marginalized groups. In particular, an individual crowdsourced worker may not fully understand what may cause harm to every community, especially those that they do not belong to, and so skews in the demographic distributions of crowdsourced workers may lead to some deleterious model side effects going relatively unaddressed. Furthermore, the hosting of these crowdsourcing rating tasks on an online platform may render it less accessible to people with visual or other disabilities, again potentially skewing the complete picture of bias in these generations as judged by workers. Morality, toxicity, bias, etc. are often culturally specific definitions and vary from person to person, and so we cannot assert that these ratings represent an \u201cobjective\u201d measurement of any of these concepts."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to acknowledge the following people for their invaluable feedback: Alessandro Vecchiato, Alex Kessler, Alicia Sun, Angela Fan, Baishan Guo, Camela Logan, Chlo\u00e9 Bakalar, Christophe Ropers, Connor Harrington-Brandt, Cristian Canton Ferrer, Devi Parikh, Harrison Rudolph, Hubert Etienne, Isabel Kloumann, Jacob Xu, Jon Carvill, Joshua Saxe, Jun Xie, Justine Kao, Kyle Moore, Marta R. Costa-juss\u00e0, Mona Diab, Nisha Deo,\nParisa Assar, Phoebe Helander, Sharan Narang, Skyler Wang, Susan Epstein, and Thomas Hayes.\nThanks to Paul Tol for the colorblind-safe color palette.8"
        },
        {
            "heading": "A Additional related work",
            "text": "Bias metrics and datasets. In past years, bias measurements have compared relative distances between sets of word embeddings or sentence embeddings (Caliskan et al., 2017; May et al., 2019) or compared relative token likelihoods of sentences that vary based on demographic attribute or stereotype (Nangia et al., 2020; Nadeem et al., 2021; Smith et al., 2022). However, these representationbased, intrinsic metrics sometimes fail to correlate\nwith extrinsic metrics calculated from model behavior (such as social-bias related failures on downstream tasks such as coreference resolution) (Cao et al., 2022b; Delobelle et al., 2022; Orgad and Belinkov, 2022), perhaps suggesting that the two kinds of metrics provide complementary information about model biases. Since we are interested in LLM generations in particular, we focus solely on extrinsic metrics in this work.\nEven if all LLMs developers were to agree that we need a single extrinsic, prompt-based bias metric with which to test all future models, it is presently unclear which one should be selected. Particular bias measurement datasets tend to measure bias for particular text domains, from encyclopedia snippets (Dhamala et al., 2021) to questionanswering passages (Parrish et al., 2022) to dialogue (Dinan et al., 2020a,b; Smith et al., 2022), and even the definitions of \u201cbias\u201d inherent to particular scoring metrics can vary wildly (Blodgett et al., 2020). For general evaluation of open-domain LLMs, NLP has been increasingly moving toward multimetric evaluation (Wang et al., 2018, 2019; Ma et al., 2021; Liang et al., 2022; Burnell et al., 2023) to address these and other related evaluation issues. In keeping with this trend, we take a multimetric approach in the present work to enable more thorough assessment of model bias.\nWe focus in part on metrics calculated using templates in this work, due to their flexibility. Templates used to measure regard in Sheng et al. (2019) have seen wide use. Huang et al. (2020), Kirk et al. (2021), Sotnikova et al. (2021), Smith et al. (2022), and Venkit et al. (2023) present additional approaches for creating bias measurement templates over a wide demographic range. Templatebased bias datasets can be contrasted with crowdsourced datasets, or datasets drawn from existing sources: template-based datasets have the advantage of easily scaling to many demographic groups, but datasets drawn from existing text sources or written by crowdsourced workers can, in principle, capture nuances of demographic-specific stereotypes more faithfully. For example, the crowdsourced stereotype measurement datasets CrowSPairs (Nangia et al., 2020) and StereoSet (Nadeem et al., 2021) are commonly used for likelihood scoring of stereotypes vs. anti-stereotypes across many demographic axes, but Blodgett et al. (2021) and Pikuliak et al. (2023) discuss methodological and data quality issues with the latter two.\nAdditionally, there are many datasets used to measure particular biases on particular tasks, notably datasets measuring gender bias in coreference resolution including Winogender (Rudinger et al., 2018), WinoBias (Zhao et al., 2018), and BUG (Levy et al., 2021). Other task-specific datasets, such as the BBQ dataset (Parrish et al., 2022) for measuring bias in question-answering, have also been widely used (Glaese et al., 2022; Liang et al., 2022). Most recently, Mei et al. (2023) measure bias for an extended set of stigmatized groups (similarly reacting to improve group inclusion in bias measurement) for the task of sentiment analysis.\nGiven the rise of generative AI, bias datasets, such as ToxiGen (Hartvigsen et al., 2022, used in this work), have begun to be created via text generation itself. Kocielnik et al. (2023) also uses pretrained language models such as GPT-Neo (Black et al., 2022) to generate prompts for CrowS-Pairsstyle likelihood scoring. Our work focuses on prompt-based datasets that are well-suited for measuring bias in generative LLMs, but there are also large benchmark suites, such as BIG-bench (Srivastava et al., 2022) and HELM (Liang et al., 2022), that each also provide coverage of a few bias benchmarks. Most similar to us, Viswanath and Zhang (2023) has recently open-sourced a suite of bias benchmarks, focusing instead mainly on intrinsic metrics and likelihood scoring.\nToxicity metrics. In this work, we use datasets that are designed to provoke toxic model generations, because we believe that a completely safe model would not be toxic no matter what the input; however, we do not explicitly utilize hate speech in prompts in this work. Other related datasets however do use hate speech as a source, including De Gibert et al. (2018), drawing from an online white supremacy forum; ETHOS (Mollas et al., 2020), drawing from YouTube and Reddit; and Implicit Hate (ElSherief et al., 2021), drawing from Twitter. Datasets measuring unsafe language include HateCheck and Multilingual HateCheck (R\u00f6ttger et al., 2021, 2022) and, for dialogue, Safety Bench (Dinan et al., 2021), Safety-Kit (Dinan et al., 2022), and SaFeRDialogues (Ung et al., 2022); Deng et al. (2023) provides a survey of dialogue safety metrics and datasets. SafeText (Levy et al., 2022) is a benchmark for testing a language model\u2019s propensity to recommend that a user engages in physically harmful activity. Zhuo et al. (2023) investigates bias, reliability, robustness, and\ntoxicity in ChatGPT, and finds that despite impressive performance on current bias and toxicity datasets, ChatGPT is susceptible to a prompt injection technique that bypasses its safety mechanisms, permitting toxic and obscene generations.\nBias reduction methods. Recent techniques for bias mitigation operate at various stages of the model pipeline, including during pretraining, finetuning, and generation. Training-based approaches include FairBERTa (Qian et al., 2022), pretrained on a dataset in which demographic mentions have been re-balanced through neural perturbation of gender, race/ethnicity, and age, and Garimella et al. (2022), in which models are made fairer by finetuning on text authored by historically disadvantaged groups. Dorner et al. (2022) performs word perturbation using demographic terms from HolisticBias (Smith et al., 2022), similar to this work, but for debiasing toxicity classifications.\nSmith and Williams (2021) tunes BlenderBot (Shuster et al., 2022) to reduce bias on a conversation partner\u2019s name, and Borchers et al. (2022) investigates prompt-engineering and fine-tuning as a means of reducing gender bias in job ads. Many techniques rely on debiasing embedded sentence representations by ensuring that they use no information from a subspace that represents biased demographic attributes (Liang et al., 2020; Ravfogel et al., 2020; Liang et al., 2021).\nAnother class of approaches utilizes an external classifier for bias mitigation via reinforcement learning or fine-tuning (Liu et al., 2021; Basu et al., 2022). Das and Balke (2022) and Dhamala et al. (2023) reduce bias by optimizing decoding settings, and Abid et al. (2021) reduces anti-Muslim bias simply by prepending a short prompt containing positive associations about Muslims.\nOther works that compare the performance of several bias mitigation techniques include Yang et al. (2022) and Meade et al. (2022). Here, we go beyond these works by comparing several techniques simultaneously across multiple families of generative language models, demographic axes, and prompt-based bias metrics."
        },
        {
            "heading": "B Additional methods",
            "text": ""
        },
        {
            "heading": "B.1 Automatic evaluation metrics",
            "text": "B.1.1 Existing bias and toxicity metrics For ToxiGen, we use the revised dataset (\u201cv2\u201d) from Hosseini et al. (2023) that reduces noise by fil-\ntering out sentences for which annotators disagree on the target demographic group; we specifically select only benign prompts from the test set and balance the distribution to match the number of samples in each demographic group.\nFollowing Gehman et al. (2020), we score RealToxicityPrompts using Perspective API9, with a generation labeled as toxic if its toxicity score exceeds 50%. For BOLD, since the classifier used by Dhamala et al. (2021) is not publicly available, we use the ToxiGen classifier (Hartvigsen et al., 2022) tuned on RoBERTa (Liu et al., 2019) to score continuations instead. We generate 100 tokens for all prompt datasets scored by the ToxiGen classifier (given the relatively small size of the ToxiGen dataset) and 30 tokens for all datasets scored by the Perspective and Regard classifiers.\nRegarding the performance of the classifiers used, Sheng et al. (2019) reports that the latest version of their BERT-based Regard classifier achieves a test-set accuracy of 84%. Lees et al. (2022) states that the new generation of toxic content classifiers for Perspective API reports up to 97.7% AUCROC on the English portion of their proprietary toxic comment evaluation set. Hartvigsen et al. (2022) reports that the ToxiGen classifier tuned on RoBERTa has 93% AUC on the validation fold of the ToxiGen dataset, and beats the performance of the widely used HateBERT (Caselli et al., 2021) on three additional human-written datasets.\nB.1.2 AdvPromptSet: extending bias metrics to intersections of identities\nFor the downsized version of AdvPromptSet, we perform a stratified sampling procedure based on a combination of toxicity labels, number of toxicity and bias terms, and identity labels. (1) Toxicity labels: Each prompt is labeled as either benign or toxic. This information is derived from the original two Jigsaw source datasets. (2) The number of toxicity and bias terms: Since prompts with more terms are likely to generate more harmful content, we bin examples by the number of terms they contain: 1 word, 2 words, and \u22653 words. (3) Identity labels: Multiple identity groups can appear in one prompt, as in the first example in Table 2, in which both \u201chomosexual\u201d and \u201cchristian\u201d are mentioned. Instead of stratified sampling based on only one of the 24 identity groups, we stratify based on the pattern of inclusion of all groups, relying on one-hot encod-\n9https://github.com/conversationai/ perspectiveapi\ning to represent whether each group is referred to in each prompt. For example, using one-hot encoding, 000000000000000000000000 indicates that no identity group (from our lists) was mentioned in the prompt, while 000001001000000000000000 contains 1s to indicate references to the identity groups of gay people and Christians. As shown in examples in Figure 1, prompts in AdvPromptSet can reference more than 2 demographics.\nB.1.3 Demographic identity labels in AdvPromptSet\nGiven the 24 types of demographic identity labels from the Jigsaw Unintended Bias in Toxicity Classification dataset, we use embedding-based KNN search to identify similar words. The identity labels and their corresponding KNN words are shown below. Given that KNN words are predicted by an automatic procedure, they may display unusual typography, punctuation or spelling, and may not be exhaustive or entirely representative of their identity group. male: male\u0161, mal\u00e8, males, male, m\u00e2le, male-identified, male-, mal\u00e9, male-male, male., m\u00e2les. female: woman, woman., female-female, female, female., female-identified, female-. transgender: transsexual, trans-gendered, transgendered, transgender, trans-women, transgenderism, trans-woman, trans-sexual, transexuality, transsexuals, transgenders, anti-transgender, transexuals, transgenderists, transexual, trans-gender, transgender-related, transexualism. other_gender: other gender, non-gender, gender, cross-gender, other_gender, inter-gender, gendering, third-gender. heterosexual: heterosexually, heterosexual, heterosexuality, heterosexualization, heterosexuals, heterosexualism. homosexual_gay_or_lesbian: gay-lesbian, homosexual_gay_or_lesbian, homosexually, homosexual, g\u00e2y, lgbt, homosexual gay or lesbian, homosexuality, gay. bisexual: bi-sexual, bi-curious, bisexuality, bisexuals, bisexual, bi-sexuality. other_sexual_orientation: other sexual orientation, sexual-orientation, other_sexual_orientation. christian: christianize, christianese, christians, christian-only, christianising, christiansand, christiany, jewishchristian, -christian, christian., christianise, christianists, christian, christianity, christian-, christians., christianity-, christianity., christianmuslim, muslim-christian, christianized, religious, christian-right, christianist, christian-jewish. jewish: juda\u00efsme, jewish-canadian, half-jewish, part-jewish, anglo-jewish, jewes, french-jewish, -jewish, jewish-related, jewsish, christian-jewish, jewish-, jewish-zionist, anti-jewish, jewish-muslim, jewishgen, jews-, jewish-american, jewish.,\njewish-roman, jewish-german, jewish-christian, jewishness, american-jewish, un-jewish, jewsih, jewish-americans, jewish-catholic, jewish, jew-ish, spanish-jewish, semitic, black-jewish, jewishpalestinian, jewish-christians, jew, jewish-arab, jews, russian-jewish, jewish-owned, jew., germanjewish, judaism, jewishly, muslim-jewish, judaism., jewish-italian, jewish-born, all-jewish, austrianjewish, catholic-jewish, jews., judaism-related, roman-jewish, jewish-themed, college-jewish, arab-jewish, jewish-only, british-jewish, judaisms, jewish-russian, pro-jewish, israeli-jewish, jewishisraeli. muslim: catholic-muslim, mohammedans, christian-islamic, islam, arab-muslim, muslimah, pre-muslim, muslimani, mainly-muslim, islamise, muslims., buddhist-muslim, americanmuslim, isla\u0304m, islamicist, mohammed, muslim., muslims, islamistes, islamiste, islams, all\u00e2h, muslim-christian, muslimin, islamic-christian, muslim-american, muslim-jewish, islamists, islam., muslimeen, jewish-muslim, hindu-muslim, islam-, anti-muslim, islamicists, ex-muslim, alla\u0304h, majority-muslim, arab-islamic, islamic, allah, islamics, muslim-hindu, muslim-related, muslime, m\u00fcslim, islamist, christian-muslim, muslim-, muslim-only, muslim-based, jihadist, muslima, muslim, islam, isl\u00e2m. hindu: hinduness, hindu, neo-hindu, hindu-majority, hindu-buddhist, hinduism., hindutashravi, hind\u00fa, hinduism, hinduchristian, pro-hindu, hindu-muslim, hindustan, hindu-dominated, hinduised, neo-hinduism, hindutash, hindujas, anti-hindu, hinduja, muslim-hindu, hindusim, hindu-, hindu-arabic, hindu-sikh, hindusthan, hinduist, hindus, hinduism-related. buddhist: buddhadev, buddhas, buddhism-related, buddha, buddhist-inspired, buddhist-majority, buddhist-muslim, buddhism, hindu-buddhist, buddhists, buddhist, buddhistische, buddhahood, buddhismus, buddha-like, buddhistic, buddhistchristian, pro-buddhist, pre-buddhist, buddhisms, anti-buddhist. atheist : atheistic, atheists, atheism, atheists., atheist, atheistical, atheismus, atheist., anti-atheist, atheism.. other_religion: other religion, religions, other_religion. black: coloured, black, dark-coloured, black-ish, black-on-black, blackness, all-black, black-on-white, black-, half-black, blacky, black-and, black., blackonly. white: white-only, whiteness, half-white, white-looking, white-, whitey, all-white, whites, whitely, whiteish, white-on-white, white-ish, pure-white, white., white, whites.. asian: all-\nasian, asian-, asian-born, asians, asian-european, asian-americans, asianists, asian, anti-asian, asian-australian, asian-american, chinese, asianist, asian-based, asian-related, asian-indian, asian-african, asian-canadian, asiana, half-asian, asian-looking. latino: afro-latino, hispanico, latino, hispanic, hisp\u00e1nico, hispanic-americans, hispanic-, hispanic-american, mexican, hispanics, latinoowned, latino-american, afro-latinos, latinoheat, hispanicized, latinos, latinoam\u00e9rica, hispanica, latinorum, latinoamerica, hisp\u00e1nica, latinoaussie, anti-latino, latinoamerican, latino-americans, latinoamericano, latinoamericanos, latino-am\u00e9ricain, hispanicus. other_race_or_ethnicity: ethnically, ethnic-related, cross-ethnic, non-ethnic, race-specific, other_race_or_ethnicity, ethnicy, ethnicly, race-ethnicity, ethnicity-related, ethnicity, multi-ethnic, other race or ethnicity, ethnicities, ethnically-mixed, ethnics, ethnic-specific, ethnic, other-race, non-ethnically, multi-ethnicity, racial-ethnic, ethnic-minority. physical_disability: physical disability, physical_disability, disability, disability-related, disability-specific. intellectual_or_learning_disability: intellectual_or_learning_disability, intellectual or learning disability, learning-disabled. psychiatric_or_mental_illness: psychiatrically, mentalhealth, psychiatric, psychiatric_or_mental_illness, psychiatric or mental illness, mental-illness. other_disability: other_disability, disabilityfriendly, other disability, disability-related, disability., disability, disability-specific."
        },
        {
            "heading": "B.1.4 Performance metrics",
            "text": "For the performance results of Table 15, we extract the first sentence of each passage in Wikipedia articles from the test set of WikiText-103 (Merity et al., 2016), filtering on heuristics such as length and markdown formatting, for a total of 1612 prompts. Each model is prompted using the default decoding settings noted in Section B.2.1, batch size of 16, and maximum generation length of 200 tokens. Models are run on 32GB V100s using the minimum model parallelism possible with these devices: MP=1 for GPT-2 XL and MP=16 for BB3-175B. We record GPU time, output token count and peak allocated memory for each batch, taking the ratio of GPU time and token count as per-token latency for the batch. We average across 5 runs of the curated test set and bootstrap 95% confidence intervals for Latency and Memory to account for device and generation variability."
        },
        {
            "heading": "B.2 Models",
            "text": ""
        },
        {
            "heading": "B.2.1 Generation settings",
            "text": "For OPT, we decode with a temperature of 1.0 and a top-p of 0.9, the latter value following the evaluation of RealToxicityPrompts in Zhang et al. (2022); for BlenderBot 3, two sizes of which were fine-tuned from OPT (Shuster et al., 2022), we inherit these decoding settings as well. For GPT-2 we use a temperature of 0.7, following Sheng et al. (2019), and a top-k of 40, following Radford et al. (2019). Given that the BLOOM paper appears to focus on greedy decoding (Scao et al., 2022), for BLOOM we inherit the same settings as GPT-2 given the similar model sizes that we measure here. For LLaMa, we test the base model on both sets of decoding settings in Table 9. Unless specified, LLaMa results use a temperature of 1.0 and a top-p of 0.9."
        },
        {
            "heading": "B.3 Bias/toxicity mitigation techniques",
            "text": ""
        },
        {
            "heading": "B.3.1 Prompting",
            "text": "We use a prompt template consisting of an instruction intended to reduce bias and toxicity in model generations, followed by the prompt from the benchmark dataset, and delimited by a newline. We start with a small number of handwritten prompts, use an instruction-tuned model (text-davinci-002) to generate revisions, then score and rank the revisions based on the bias and toxicity elicited when rendered with a small subset of prompts from ROBBIE. The top ranking (lowest bias or toxicity inducing) instructions are then selected for the next iteration. We use 10 iterations and keep the top k=5 at each step."
        },
        {
            "heading": "B.3.2 Self-debiasing",
            "text": "The self-debiasing technique proposed by Schick et al. (2021) is a post-hoc text generation debiasing procedure whereby a language model reduces its probability of producing problematic texts. First, several carefully designed prompts are prepended to the input prompt to encourage the language model to produce toxic generation, e.g. \u201cThe following text contains rude, disrespectful, or unreasonable language\u201d. Second, the probabilities of tokens considered likely in the initial toxic generation are reduced, which enables the model to produce a second, non-discriminative continuation. For implementation, we follow Schick et al. (2021)10 us-\n10https://github.com/timoschick/self-debiasing\ning default parameter settings in the scaling function, as well as their self-debiasing templates."
        },
        {
            "heading": "B.3.3 Adversarial triggering",
            "text": "The goal of adversarial triggering is to find a token sequence that universally controls model generations when prefixed to the prompt context. We follow the approach proposed by Wallace et al. (2019), and applied to bias mitigation by Sheng et al. (2020). We take the target model\u2019s generations along with labels given by a classifier as positive or negative examples. We initialize a random trigger of fixed length and prefix all examples with it. The search process then consists of iteratively calculating the loss on the labeled examples and using the gradient at the embedding layer to swap tokens at each trigger position such that the loss for desirable examples (based on classifier label) is reduced, and that of undesirable generations is increased.\nB.4 Frequencies of demographic terms in training corpora\nThe datasets that we analyze include text sources such as web crawl data, news, and encyclopedias: (1) Common Crawl (Wenzek et al., 2020; Touvron et al., 2023a), deduplicated and cleaned; (2) OpenWebText2 (Gao et al., 2020); (3) HackerNews (Gao et al., 2020); and (4) Wikipedia (en) (Gao et al., 2020). We exclude papers and publications, as well as multilingual data.\nB.4.1 Female, male, and gender-neutral pronouns\nThe frequency of pronouns is quickly becoming a standard proxy metric for gender bias. We use the following lists of pronouns, used to analyze PaLM training corpora (Chowdhery et al., 2022): shepronouns: she, her, hers, herself; he-pronouns: he, him, his, himself; and they-pronouns: they, them, their, theirs, theirself, themself, themselves.\nFor each document in a dataset, we first remove regex, lowercase the document, and then tokenize it using NLTK\u2019s word tokenize method (Bird et al., 2009). If a document mentions any of the terms in a given list (for example, any of \u201cshe\u201d, \u201cher\u201d, \u201chers\u201d, or \u201cherself\u201d), we count the document as containing pronouns (here, \u201cfemale\u201d).\nB.4.2 Demographic descriptor terms We use the descriptor terms the HolisticBias dataset v1.111. For each descriptor, we count whether it appears at least once in a given document."
        },
        {
            "heading": "C Additional results",
            "text": "C.1 Comparison of automatic metrics across models and demographic axes\nC.1.1 The effect of model size, family, and decoding settings\nFigure 2 and Table 9 show that rates of toxicity and negative regard often but not always increase as a function of model size, especially for AdvPromptSet and to a lesser extent RealToxicityPrompts, ToxiGen v2, and HolisticBiasR. By contrast, trends in the BiasScore (Table 3) as a function of model size are less distinct, perhaps suggesting that bias does not dramatically grow or shrink relative to the overall variance levels of the metric that it is measured on (i.e. toxicity or negative regard).\nTable 9 shows overall differences in rates of toxicity and negative regard in some model families vs. others, likely due to differences in decoding settings (Section B.2.1) and training data distributions. For BiasScore these differences are more muted, with the levels of bias highly dependent on both the dataset and model family in question. For 4 of 6 datasets, rates of toxicity and negative regard are appreciably higher in base LLaMa when using a temperature of 1.0 and top-p of 0.9 (matching the decoding settings of OPT/BB3) than when using a temperature of 0.7 and top-k of 40 (matching the decoding settings of GPT-2/BLOOM), echoing the finding of Dhamala et al. (2023) that changing decoding settings to improve text diversity may create higher rates of negative regard and sentiment.\nC.1.2 Understanding fine-grained and intersectional biases\nAdvPromptSet. Prompts can contain multiple labels from a single demographic axis (eg. \u201cwhite\u201d, \u201cblack\u201d) as a result of (i) multiple people referred to in the prompt, (ii) a single entity with multiple attributes on a single axis (e.g. mixed-race, genderfluid), or (iii) annotation error. For simplicity, we exclude these prompts from our analysis, and pick out prompts containing exactly one attribute from each axis in a given intersection. For example,\n11https://raw.githubusercontent.com/ facebookresearch/ResponsibleNLP/main/holistic_ bias/dataset/v1.1/descriptors.json\nfor the intersection of race and gender, we look at prompts with the labels \u201casian\u201d and \u201cfemale\u201d and no other race or gender labels. Even after this filtering is done, because the demographic labels correspond to the entire sentence and not to a single entity, our query may return prompts which contain both labels but do not actually refer to an individual intersectional identity. Further work on the dataset is needed here to have the granularity of individual identities, but we believe that it can still be useful in its present form to analyze how a model responds to a combination of identity traits. It is still possible that the labels in AdvPromptSet inherit errors from the original Jigsaw datasets, as they were annotated by human raters.\nAnother important caveat here is that typically unmarked groups may have prompts which aren\u2019t included in the analysis. Blodgett et al. point out that socially dominant groups often are not explicitly stated in natural language, e.g. (\u201cthe straight man\u201d is referred to as just \u201cthe man\u201d). We only include explicitly marked attributes in this analysis, which does lead us to miss out on potential data points. For example, in Table 10, we see that we lack data for the intersections of \u201cheterosexual\u201d with \u201cblack\u201d, \u201ctransgender\u201d and \u201cfemale\u201d, and this may be due the attribute of heterosexuality being generally unmarked. While we don\u2019t include unmarked attributes in the present analysis, AdvPromptSet can certainly used to look at model behavior with unmarked attributes as well.\nHolisticBiasR. Table 11 shows the descriptive adjectives in HolisticBias with the lowest and highest rates of negative regard. Table 12 shows the percentage of generated continuations to Regard prompts containing HolisticBias descriptors that contain a negative regard score: in particular, we see that BB3-175B appears to give a rather higher rate of negative regard to a descriptor indicating \u201cchild\u201d when paired with a \u201cmale\u201d noun (for instance, \u201cteenage guy\u201d, \u201cadolescent male\u201d) than when paired with a \u201cfemale\u201d noun.\nC.2 Effects of bias/toxicity reduction methods\nC.2.1 Reducing toxicity and negative regard Comparing different techniques. Table 6 compares the effects of bias and toxicity reduction techniques across the 6 ROBBIE datasets. Selfdebiasing is most effective with GPT2-XL. Our prompting approach is not as reliable in reducing toxicity and negative regard for GPT2-XL as\nit is for BB3-175B, and we attribute this to the larger model being better at following instructionstyle prompting. Adversarial triggering can be prohibitively resource-intensive depending on its hyperparameters and available hardware, and we\nforego testing that approach on the larger model.\nComparing different datasets. Bias reduction techniques prove to be especially effective on the Regard and HolisticBiasR prompt datasets, which see their rates of negative regard drop by 24% and\n8%, respectively, for the average technique presented in Table 6, perhaps because the rather constrained sentence structure allows for a clear association between the subject of the sentence and the regard given to them. BOLD appears to be much harder to reduce toxicity in, with the average technique actually increasing toxicity in it by 39%; however, this is likely because toxicity in this dataset is already incredibly low to begin with, less than 0.6% for both models tested, meaning that attempts at reduction may potentially fall below measurement noise. With the self-debiasing technique on BlenderBot3-175B, in particular, toxicity actually increases from 0.6% to 1.6%: it is possible that the default debiasing prefixes used in self-debiasing may not be effective for BOLD. Our future work will conduct more comprehensive experiments to understand the effectiveness of different prefixes on various datasets."
        },
        {
            "heading": "C.2.2 Reducing bias",
            "text": "In this section, we elaborate on the bias analysis performed on GPT2-XL and BlenderBot3175B after applying bias and toxicity mitigations. Table 13 lists the subgroups for each benchmark dataset b that are associated with argmaxsi\u2208Sb\n\u0302PrNeg(Xbsi). These subgroups are the most marginalized groups according to their rates of toxicity / negative regard. We also report the confidence intervals for \u0302PrNeg(Xbsi) in Ta-\nble 14. Note that the self-debiasing method is success-\nful in reducing maxsi\u2208Sb \u0302PrNeg(Xbsi) across all datasets for GPT2-XL even though it does not have any significant impact on BiasScore. Therefore, its impact on fairness is favorable. Please note that the self-debiasing method doesn\u2019t change the most marginalized groups (except for in the HolisticBiasR dataset). Moreover, we observe that the adversarial triggering and prompting methods reduce the BiasScore in GPT2-XL; however, Table 14 shows that the outcome for the most marginalized groups after using these methods worsens on some of the benchmarks. This implies that these approaches do not always improve the fairness on GPT2-XL.\nFor BlenderBot3-175B, whenever the most marginalized groups are the same between the baseline and the bias/toxicity-mitigated models, the confidence interval for the rate of negative outcomes moves towards zero, which is a favorable outcome. However, an increasing BiasScore implies that the mitigated model is penalizing other subgroups, and therefore, we don\u2019t have enough evidence to conclude that any of the mitigation approaches are improving the fairness on BlenderBot3-175B."
        },
        {
            "heading": "C.2.3 Performance metrics",
            "text": "Table 15 shows that most mitigations appear to have some impact on generation quality as scored by text-davinci-002. This agrees with annota-\ntors who report slightly lower coherence in BB3175B generations under mitigation, but is in tension with most of their other judgements of quality. We observe minimal impact to latency and memory at inference time for all models and mitigations, noting that the average generation length under mitigation for BB3-175B is lower, which might artificially inflate the observed per-token latency.\nOverall, prompting is a strong baseline given its effectiveness across benchmarks (assuming a capable enough base model) and the relatively little up-front time and compute required."
        },
        {
            "heading": "C.2.4 Human evaluations",
            "text": "See Table 16 for human evaluations of the performance of the models with bias and toxicity mitigations, as rated by workers crowdsourced on Amazon Mechanical Turk through the Mephisto platform (Urbanek and Ringshia, 2023).12 See Table 17 for the text used for each question.\nFluency, coherence, toxicity, bias, and immorality metrics. There is a slight reduction in the percentage of generations that were rated as containing toxicity from self-debiased GPT2-XL compared to the original model. Evaluators rated the generations from the self-debiased GPT2-XL model as more coherent than the generations from the original model. For the BB3-175B models, evaluators rated the models after bias/toxicity mitigation to be more fluent but less coherent than the original model. For the prompting BB3 model, we see reductions across toxicity, bias, and immortality\n12Our crowdsourcing tasks pay workers well above minimum wage.\nmetrics. For the self-debiased BB3 model we see reductions in bias.\nRegard metric. For GPT2-XL, all techniques appreciably reduce the rate of negative regard, with self-debiasing and adversarial triggering having the most impact, matching the overall trend as measured by the Regard classifier (Table 6). Differences between the original model and all three other models are all individually statistically significant given a \u03c72 test, at a threshold of p < 0.05.\nFor BlenderBot3-175B, all three techniques likewise reduce the overall rates of negative regard at least somewhat, with a 41% reduction for the prompting method. Differences between all pairs of models, other than the original model vs. selfdebiasing, are statistically significant.\nIn Figure 3, we additionally present a visualization of the Pearson correlation between the Regard classifier scores and human annotations of regard, on generations from BlenderBot3-175B models with bias/toxicity reduction. We see positive correlation between the predicted and the annotated values. Interestingly, the correlation for negative regard is notably larger than those for positive and neutral, perhaps as a consequence of negative regard being easier to distinguish than positive versus neutral. Although the correlation for the GPT2-XL\ngenerations are overall lower, similar trends hold, as seen in Figure 4.\nTable 18 shows these same correlation results on BlenderBot3-175B generations split by individual techniques for bias/toxicity mitigation. These results show that prompting has the highest correlation overall, and a qualitative inspection suggests that this could be due to more natural and sensible texts produced through prompting. Selfdebiasing, for example, may produce generations that are more difficult to annotate, such as \u201cThe White person was well-known for playing guitar while having black friends.\u201d From an analogous table of GPT2-XL results (Table 19), prompting produces higher correlations than using self-debiasing, but is surpassed by adversarial triggering.\nC.3 Frequencies of demographic terms in training corpora"
        },
        {
            "heading": "C.3.1 HolisticBias descriptors",
            "text": "We present the top 10 HolisticBias descriptors found in the training corpora discussed in Section 3.3, subselecting for the race/ethnicity (Table 20), religion (Table 21), and age (Table 22) axes. Tables are sorted by weighted mean, weighted by the number of documents in each dataset.\nC.3.2 Relation of the term frequencies with model biases\nWe are interested in how the imbalance of demographic representations in documents may contribute to biases. Using model bias measurements from the HolisticBias paper (Smith et al., 2022), we compare these biases with the standard deviations of the frequencies of the descriptors in each HolisticBias axis (Table 23). We find that model biases do not necessarily correspond to a larger standard deviation in the descriptor frequencies. It is important to keep in mind, however, that the corpora that we measure HolisticBias descriptor frequencies in do not align with those used to train these models, meaning that a direct comparison is not possible in this case."
        },
        {
            "heading": "C.3.3 Gender pronouns",
            "text": "In Table 24 we show the percentage of documents mentioning any gender pronoun, for each group of gender pronouns and each dataset. We make the following observations:\n1. The ratio of He pronouns to She pronouns is generally greater than 1, meaning that in many existing popular public datasets, He pronouns are still typically over-represented.\n2. They pronouns typically have the highest level of representation in the datasets, except for Wikipedia (en). This may reflect Wikipedia typically referencing specific people with specific (usually binary) gender pronouns.\nSome variations in these percentages across datasets are as follows:\n1. HackerNews features a very high He:She pronoun ratio of 3.78, which may reflect gender patterns in the specific domains represented by this news aggregation service.\n2. Web crawl datasets and Wikipedia also have relatively high He:She ratios.\nOur pronoun frequency numbers show directional similarity with the related analysis in the PaLM paper (Chowdhery et al., 2022), which reports 41% of data points containing they/them pronouns, 30% containing he/him pronouns, and 14%\ncontaining female pronouns."
        },
        {
            "heading": "C.3.4 Future directions",
            "text": "One expansion of the analysis of HolisticBias descriptors in pretraining datasets could be to create a new version of the dataset that better clusters descriptors together to represent specific demographic\ngroups, perhaps with the consultation of linguists and sociologists."
        },
        {
            "heading": "C.4 Bias measurements",
            "text": "In this section, we provide more details for our fairness analysis. In the following tables we estimate PrNeg(Xbsi) for each subgroup si and each benchmark dataset b via bootstrapping and provide the confidence intervals for maxsi\u2208Sb \u0302PrNeg(Xbsi) in Table 25. Note that \u0302PrNeg(Xbsi) is the median of bootstrapping estimation. In addition, we report the subgroups associated with maxsi\u2208Sb \u0302PrNeg(Xbsi) in Table 26. The diversity of subgroups in Figure 5 across datasets illustrates that the promptbased datasets that we use for measuring social bias are able to surface different types of harms. For example, the ToxiGen (v2) results in Table 26 imply the presence of systematic bias against the Latino/Latina subgroup in the LLMs we test. Moreover, the Regard dataset shows that all of the LLMs perform the worst on its LGBTQ+ subgroup (i.e. \u201cThe gay person was known for...\u201d). Moreover, for each prompt dataset, we select\nthe top three subgroups with the highest PrNeg(.) given by the medians of the bootstrap sampling. The distribution of these groups across all models and datasets is shown in Figure 5. This figure is a\nrepresentation of which groups in general are most marginalized by the LLMs studied in this work.\nMoreover, we leverage the demographic axes introduced in the HolisticBias dataset and perform bias analysis per demographic axis. We report the BiasScore and confidence intervals of argmaxsi\u2208Sb \u0302PrNeg(Xbsi), and the associated subgroups for Body type (Table 29, 27, 28), None (Table 32, 30, 31), Culture (Table 35, 33, 34), Religion (Table 38, 36, 37), Race/Ethnicity (Table 41, 39, 40), Characteristics (Table 44, 42, 43), Ability (Table 47, 45, 46), Sexual orientation (Table 50, 48, 49), Gender (Table 53, 51, 52), Political ideologies (Table 56, 54, 55), Age (Table 59, 57, 58), Socioeconomic class (Table 62, 60, 61), and Nationality (Table 65, 63, 64)."
        }
    ],
    "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
    "year": 2023
}