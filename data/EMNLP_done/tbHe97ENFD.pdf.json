{
    "abstractText": "Over the past few years, various domainspecific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in generalpurpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jaeyoung Choe"
        },
        {
            "affiliations": [],
            "name": "Keonwoong Noh"
        },
        {
            "affiliations": [],
            "name": "Nayeon Kim"
        },
        {
            "affiliations": [],
            "name": "Seyun Ahn"
        },
        {
            "affiliations": [],
            "name": "Woohwan Jung"
        }
    ],
    "id": "SP:3816bc458d05058f66f6d7f357f061e83e4a465b",
    "references": [
        {
            "authors": [
                "Emily Alsentzer",
                "John R Murphy",
                "Willie Boag",
                "WeiHung Weng",
                "Di Jin",
                "Tristan Naumann",
                "WA Redmond",
                "Matthew BA McDermott."
            ],
            "title": "Publicly available clinical bert embeddings",
            "venue": "NAACL HLT 2019, page 72.",
            "year": 2019
        },
        {
            "authors": [
                "Julio Cesar Salinas Alvarado",
                "Karin Verspoor",
                "Timothy Baldwin."
            ],
            "title": "Domain adaption of named entity recognition to support credit risk assessment",
            "venue": "Proceedings of the Australasian Language Technology Association Workshop 2015, pages 84\u201390.",
            "year": 2015
        },
        {
            "authors": [
                "Dogu Araci."
            ],
            "title": "Finbert: Financial sentiment analysis with pre-trained language models",
            "venue": "arXiv preprint arXiv:1908.10063.",
            "year": 2019
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "Scibert: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Wenhu Chen",
                "Charese Smiley",
                "Sameena Shah",
                "Iana Borova",
                "Dylan Langdon",
                "Reema Moussa",
                "Matt Beane",
                "Ting-Hao Huang",
                "Bryan Routledge"
            ],
            "title": "Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Katherine Lee",
                "Daphne Ippolito",
                "Andrew Nystrom",
                "Chiyuan Zhang",
                "Douglas Eck",
                "Chris Callison-Burch",
                "Nicholas Carlini."
            ],
            "title": "Deduplicating training data makes language models better",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhuang Liu",
                "Degen Huang",
                "Kaiyu Huang",
                "Zhuang Li",
                "Jun Zhao."
            ],
            "title": "Finbert: A pre-trained financial language representation model for financial text mining",
            "venue": "Proceedings of the twenty-ninth international conference on international joint conferences",
            "year": 2021
        },
        {
            "authors": [
                "Lefteris Loukas",
                "Manos Fergadiotis",
                "Ion Androutsopoulos",
                "Prodromos Malakasiotis."
            ],
            "title": "EDGARCORPUS: Billions of tokens make the world go round",
            "venue": "Proceedings of the Third Workshop on Economics and Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Lefteris Loukas",
                "Manos Fergadiotis",
                "Ilias Chalkidis",
                "Eirini Spyropoulou",
                "Prodromos Malakasiotis",
                "Ion Androutsopoulos",
                "Georgios Paliouras."
            ],
            "title": "FiNER: Financial numeric entity recognition for xbrl tagging",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Pekka Malo",
                "Ankur Sinha",
                "Pekka Korhonen",
                "Jyrki Wallenius",
                "Pyry Takala."
            ],
            "title": "Good debt or bad debt: Detecting semantic orientations in economic texts",
            "venue": "Journal of the Association for Information Science and Technology, 65(4):782\u2013796.",
            "year": 2014
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "James Melville."
            ],
            "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
            "venue": "arXiv preprint arXiv:1802.03426.",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Agam Shah",
                "Suvan Paturi",
                "Sudheer Chava."
            ],
            "title": "Trillion dollar words: A new financial dataset, task & market analysis",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6664\u20136679,",
            "year": 2023
        },
        {
            "authors": [
                "Raj Sanjay Shah",
                "Kunal Chawla",
                "Dheeraj Eidnani",
                "Agam Shah",
                "Wendi Du",
                "Sudheer Chava",
                "Natraj Raman",
                "Charese Smiley",
                "Jiaao Chen",
                "Diyi Yang"
            ],
            "title": "When flue meets flang: Benchmarks and large pre-trained language model for financial do",
            "year": 2022
        },
        {
            "authors": [
                "Ankur Sinha",
                "Tanmay Khandait."
            ],
            "title": "Impact of news on the commodity market: Dataset and results",
            "venue": "Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2, pages",
            "year": 2021
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum."
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Shijie Wu",
                "Ozan Irsoy",
                "Steven Lu",
                "Vadim Dabravolski",
                "Mark Dredze",
                "Sebastian Gehrmann",
                "Prabhanjan Kambadur",
                "David Rosenberg",
                "Gideon Mann."
            ],
            "title": "Bloomberggpt: A large language model for finance",
            "venue": "arXiv preprint arXiv:2303.17564.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Yang",
                "Mark Christopher Siy Uy",
                "Allen Huang."
            ],
            "title": "Finbert: A pretrained language model for financial communications",
            "venue": "arXiv preprint arXiv:2006.08097.",
            "year": 2020
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia"
            ],
            "title": "Glm-130b: An open bilingual pre-trained model",
            "venue": "arXiv preprint arXiv:2210.02414",
            "year": 2022
        },
        {
            "authors": [
                "Shah"
            ],
            "title": "issue, the authors of this study employed a rulebased approach to split sentences that exhibit a neutral stance. All scores were obtained by strictly following the settings provided in the GitHub",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pretrained Language Models (PLMs) have been successfully employed in various natural language processing tasks. This trend has been extended to further pretraining of PLMs on domain-specific corpus in various fields such as biomedical (Lee et al., 2020), scientific (Beltagy et al., 2019), and clinical (Alsentzer et al., 2019) domains.\nIn financial domain, Araci (2019); Yang et al. (2020); Liu et al. (2021); Loukas et al. (2022) proposed domain-specific PLMs based on BERT (Devlin et al., 2019) model. However, each study tested a small number of tasks, resulting in inadequate validation. We conduct extensive experiments to show that the generalization performance of existing financial PLMs is unsatisfactory. Even general domain PLMs such as BERT and RoBERTa (Liu\n\u2217Major in Bio Artificial Intelligence\net al., 2019) outperformed the financial PLMs on financial tasks.\nTo investigate the reasons for the low generalization performance, we categorize the financial corpus into five groups and examine their use in existing financial PLMs. This reveals that most exexisting PLMs are pretrained on a corpus consisting of only two of these corpus groups. This indicates that the pretraining data for financial PLMs lacks diversity, which might cause their low generalization performance across various financial tasks.\nMotivated by this observation, we collect a broad range of corpus and analyze the impact of corpus diversity on the performance of language models. Our investigations demonstrate that as a corpus becomes more diverse, the model\u2019s performance improves. Incorporating a diverse corpus, even with fewer tokens, yields a better generalization performance than relying on numerous tokens from a nondiverse corpus. Furthermore, when using diverse corpora, the model exhibits robust generalization performance on unseen tasks.\nWe train our Financial Language Model (FiLM) on a corpus with diverse documents and evaluate it on six financial tasks, including recently introduced works (Chen et al., 2021; Loukas et al., 2022; Shah et al., 2023). Our experimental results show that FiLM outperforms not only existing financial PLMs but also general domain PLMs on most financial tasks. In addition, we achieve an improvement of performance while reducing the number of tokens trained and energy consumption. To the best of our knowledge, FiLM is the first model to surpass RoBERTa in financial domain. We make our model and code publicly available on GitHub1 and Huggingface hub2 for continuous advancement in financial domain.\n1https://github.com/deep-over/FiLM 2https://huggingface.co/HYdsl/FiLM"
        },
        {
            "heading": "2 Proposed method",
            "text": "First, the datasets used to train the model are introduced. Second, we describe the method for preprocessing the datasets. Finally, we describe FiLM training procedure."
        },
        {
            "heading": "2.1 Pretraining datasets",
            "text": "For further pretraining the language models, we collected a financial corpus from 10 sources. There are various financial documents with different characteristics. For instance, financial reports have a higher prevalence of numerical information than other texts such as the abstracts of research in finance (Loukas et al., 2022).\nAs shown in Table 1, we categorize the pretraining datasets into the five groups as follows:\n\u2022 News: The datasets are sourced from financial news articles.\n\u2022 SEC filings: This dataset comprises financial reports (10-K, 10-Q) submitted to the U.S. Securities and Exchange Commission (SEC).\n\u2022 Earnings call: This dataset comprises the information and transcripts of earnings confer-\nence calls obtained from Seeking Alpha. \u2022 Papers: This group contains the abstracts of\nresearch papers in the field of economics. \u2022 MISC: It includes other datasets in the finan-\ncial domain. For more detailed information on the corpus and groups, please refer to Appendix A.1. Figure 1a shows the embeddings of sentences in the pretraining corpus. The color of each point represents the group to which the corresponding sentence belongs. This indicates that sentences within the same group have similar embeddings, thereby forming clusters. For example, sentences in the financial news group are primarily distributed in the right and lower areas, whereas SEC filings are concentrated on the top side.\nNone of the corpus groups was distributed over the entire space. From this observation, we could conclude that it is essential to use data from all groups for pretraining to enable the language model to learn diverse features.\nHowever, existing studies do not use all these groups to further train language models. In addition, most of them (Yang et al., 2020; Araci, 2019; Loukas et al., 2022) use only two groups of datasets. Consequently, these models might not achieve high performance across the entire financial domain. To attain robust performance over the entire financial domain, we employ all groups of datasets for pretraining."
        },
        {
            "heading": "2.2 Preprocessing",
            "text": "The preprocessing for the pretraining involves two steps: cleaning and deduplication. In the cleaning step, we remove HTML tags using BeautifulSoup3 and unnecessary special characters such as\n3www.crummy.com/software/BeautifulSoup/\nnewline characters. Deduplication could improve the efficiency of pretraining and the accuracy of PLMs (Lee et al., 2022). Thus, we remove duplicate sentences from the datasets during preprocessing. For deduplication, we remove all but one sentence from each group of duplicate sentences in the corpus. The contents of the remaining documents were preserved if a sentence was removed during deduplication. After deduplication, the token count decreased from 3.3B to 2.4B, and the size of the training dataset reduced from 19.5GB to 14GB."
        },
        {
            "heading": "2.3 Training procedure for FiLM",
            "text": "To train FiLM, we further pretrain RoBERTa on financial corpus presented in Section \u00a72.1. Following RoBERTa, we use the masked language model for the pretraining task. In addition, we use the same tokenization method and model input format as in RoBERTa. We further pretrain our model for only one epoch because the performance saturates after one epoch. We use a batch size of 16 and a learning rate of 1e-5. We use the Adam optimizer and do not set any specific scheduler or warm-up steps. Please refer to Table 5 for the hyperparameter settings."
        },
        {
            "heading": "3 Financial tasks for evaluation",
            "text": "We evaluate the performance of financial PLMs on the following six financial NLP tasks:\n\u2022 FPB (Malo et al., 2014): This sentiment classification task involves three categories: positive, negative, and neutral. The dataset comprises 4,840 sentences from financial news articles.\n\u2022 NER (Alvarado et al., 2015): The goal of this financial named entity recognition task is to identify four types of named entities (PER, LOC, ORG, and MISC) within financial contracts reported to the SEC.\n\u2022 Headline (Sinha and Khandait, 2021): The objective of this task is to classify the impact of news articles pertaining to gold commodities based on the headline of the articles.\n\u2022 FiNER (Loukas et al., 2022) This is a numeric entity recognition task. The dataset comprises XBRL-tagged financial reports from publicly traded companies.\n\u2022 FinQA (Chen et al., 2021): This is a financial question-answering task for evaluating numerical reasoning and understanding. The dataset\nis based on profit and loss reports of S&P 500 companies.\n\u2022 FOMC (Shah et al., 2023): This aims to classify text generated by the Federal Open Market Committee (FOMC) in order to assess its impact on the financial markets. This dataset classifies the policy stance as either \"Hawkish\" or \"Dovish.\" Data collection for this task was conducted up until October 15, 2022.\nFigure 1b shows the embeddings of the sentences sampled from the financial tasks. The colors of each point distinguish the six tasks and the gray points represent the sentences from the pretraining data. FiNER and FinQA are located on the top side because both tasks use SEC filings to create the dataset. Meanwhile, the Headline task is located further away from the other tasks due to its focus on the gold commodity. In addition, the sentences in FPB, NER, and FOMC form their own clusters, separate from the other datasets.\nAs observed, each financial NLP task has unique aims and distinctive textual features. This implies that the performance of language model has to be evaluated across a broad range of tasks. For more detailed information about financial tasks, please refer to Appendix B."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment setup",
            "text": "We compared the FiLM model with existing financial domain PLMs: FinBERT-A (Araci, 2019), FinBERT-Y (Yang et al., 2020), FLANG-BERT & FLANG-RoBERTa (Shah et al., 2022), and SECBERT (Loukas et al., 2022). Furthermore, we finetune general domain PLMs, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), to establish the baselines. For detailed information on each model, please refer to Appendix C.\nFor all experiments, except for FiNER and FinQA, the results are computed based on the average score across three seed values. For the standard deviation obtained through all experiments, please refer to Table 8.\nAll the models are trained on an RTX3090 GPU. For the detailed settings for fine-tuning, please refer to Appendix B. The dataset and hyperparameters used to further pretrain FiLM are provided in Appendix A. Using this setup, the training of FiLM on the entire dataset can be completed within 24 hours, resulting in a high-performance language model while maintaining a low training cost."
        },
        {
            "heading": "4.2 Main results",
            "text": "Table 2 reports the results of evaluating the performance of PLMs on financial tasks. Despite further training on financial corpus, existing financial PLMs often perform worse than general domain PLMs on certain tasks. For example, RoBERTa outperforms all existing financial PLMs on FPB, FinQA, and FOMC. In addition, BERT outperforms FinBERT-A, FinBERT-Y, and FLANG-BERT on FinQA tasks. This implies that financial PLMs exhibit a deficiency in their generalization capabilities, limiting their effectiveness across a diverse spectrum of documents within the financial domain. However, our model, which incorporates a diverse range of financial corpus for pretraining, has been equipped with robust generalization capabilities, enabling it to perform well across various NLP tasks in the financial domain. Although our model is trained on fewer tokens (2.4B) than SEC-BERT (3.1B) and FinBERT-Y (4.9B), it outperforms existing PLMs on most financial tasks."
        },
        {
            "heading": "4.3 Impacts of the diversity in the pretraining dataset",
            "text": "We investigate the variation in the performance of the language model based on the number of corpus groups used. To achieve this, we generate all combinations of corpus groups and pretrain separate RoBERTa on each combination. Then, we evaluate each model by calculating the average F1 score across four downstream tasks: FPB, NER, Headline, and FiNER. Figure 2 presents the average F1 scores when the number of corpus groups varied. This indicates that as the number of groups increases, the model\u2019s performance improves. This\nfinding suggests that a more diverse corpus leads to enhanced model performance."
        },
        {
            "heading": "4.4 Performance extrapolation",
            "text": ""
        },
        {
            "heading": "4.4.1 SEC filings dataset",
            "text": "We present empirical evidence that performance improvements can be achieved even for unseen corpus groups. To validate this, we select downstream tasks derived from SEC filings: NER, FiNER, and FinQA. Table 3 shows the performance of our model trained on a different pretraining corpus. The model pretrained on the four corpus groups without SEC filings outperform the model further trained on SEC filings only. Furthermore, despite the use of a larger number (3.1B) of tokens derived\nfrom SEC filings for further pretraining, this model does not exceed the performance of the model trained using fewer (2.1B) tokens from four different corpus groups. This demonstrates that incorporating diverse corpora can improve performance for unseen tasks, which can be more important than data volume. Furthermore, we anticipate that our model, denoted FiLM, may exhibit robust performance across a wide range of financial downstream tasks derived from unseen financial corpora."
        },
        {
            "heading": "4.4.2 Macroeconomic perspective",
            "text": "The FOMC task is a macroeconomics-based task designed to predict changes in monetary policy stance (Shah et al., 2023). This dataset comprises sentences extracted from FOMC speeches, meeting minutes, and press conferences. None of these sentences are included in pretraining dataset used for FiLM. To substantiate that FiLM performs robustly on unseen tasks, we evaluate models on the FOMC task. Table 2 represents that existing financial PLMs underperform RoBERTa, as pointed out in Shah et al. (2023). Meanwhile, our FiLM model outperforms RoBERTa. This highlights that FiLM is the first model to surpass RoBERTa in financial domain. For the results of all experiments of the FOMC task and their detailed explanation, please refer to Table 9 and Appendix D, respectively."
        },
        {
            "heading": "4.5 Cost-efficiency",
            "text": "Recently, the environmental impacts of large language models have become a critical issue due to their significant energy consumption and carbon emissions during the training model process (Strubell et al., 2019; Scao et al., 2022; Zeng et al., 2022; Touvron et al., 2023). Since further training is required for developing domain-specific PLMs, additional resources are consumed. We provide empirical evidence that when ensuring corpus diversity, energy consumption is decreased while enhancing performance. Table 4 shows the total energy consumption for training a financial PLM. We compute the electric energy required for training a model, using the following formula (Touvron et al.,\n2023): Energy (Wh) = GPU power (W) \u00d7 GPU time (h). Consequently, compared to FinBERT-Y, FiLM exhibits an 82% reduction in total energy consumption while achieving an average performance of 10% gain.\nConclusion\nWe show that financial PLMs struggle with various financial tasks due to constrained pretraining data diversity. To address this problem, we train our FiLM model using a broad spectrum of financial data. We empirically show that FiLM outperforms other financial PLMs across six financial downstream tasks. Specifically, FiLM works robustly on unseen tasks and also attains superior performance on macroeconomics-based task. Furthermore, our experimental results indicate training on diverse corpora reduces energy consumption, resulting in environmental benefits. Our study underscores the significance of leveraging diverse data to train domain-specific language models.\nLimitations\nFinancial documents often attribute substantial significance to numerical data, more so than other types of documents. Therefore, we acknowledge the necessity of an efficient technique for processing numerical tokens, particularly for financial PLMs. However, when we tested several methods proposed in previous studies, we did not observe any significant improvement. Techniques for handling numeric tokens are excluded from our study; however, we highlight this as a valuable area for future investigations. Finally, our FiLM is an encoder-based model that is not suitable for generative tasks, such as financial news summarization."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. RS-2023-00261068,\nDevelopment of Lightweight Multimodal AntiPhishing Models and Split-Learning Techniques for Privacy-Preserving Anti-Phishing), (No.RS2022-00155885, Artificial Intelligence Convergence Innovation Human Resources Development (Hanyang University ERICA)), and (2018-0-00192, the National Program for Excellence in SW). This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. NRF2022R1G1A1013549). Finally, we thank the reviewers for their detailed feedback, which helped to improve the quality of this paper."
        },
        {
            "heading": "A Pretraining information",
            "text": "A.1 Datasets\n\u2022 TRC2: TThe Thomson Reuters Text Research Collection (TRC2) corpus comprises 1,800,370 news stories published from 2008 to 2009. For more information on the data and acquisition, please refer to https://trec. nist.gov/data/reuters/reuters.html\n\u2022 Investing.com4: Investing.com is a financial platform and news website that offers a wide range of information including stocks, futures, options, and commodities. The historical dataset is a compilation of news data sourced from Investing.com between 2009 and 2020.02. \u2022 NYtimes 5: The New York Times is a wellknown newspaper worldwide. The NYTimes Dataset comprises finance-related articles collected from the NYTimes website. The data collection period spanned from 2005 to 2021.\n\u2022 EIA: The U.S. Energy Information Administration (EIA) gathers, analyzes, and shares unbiased energy information to help create better policies, ensure efficient markets, and provide a public understanding of the impact of energy on the economy and environment. We collected news data only from the information provided by eia.gov. \u2022 SEC filings6: The SEC receives annual reports (10-K) and quarterly reports (10-Q) from public US companies. These reports contains information about a company\u2019s business and financial performance. We downloaded the reports from SEC Edgar between 2020 and 2021. For the experiment(Section 4.4), additional data from 1993 to 2019 were included (Loukas et al., 2021).\n\u2022 Earnings call: Earnings conference calls are important in delivering corporate information. Seeking Alpha7 provides access to earnings conference call transcripts, and we collected the data. \u2022 Arxiv8: This dataset is a collection of abstracts from economics research sourced from arxiv.org\n\u2022 AIHUB: This dataset comprises a collection of Korean academic papers obtained and translated by professional translators. Furthermore, professors specializing in translation studies reviewed the translated corpus. We used a subset of the corpus that focused on economicsrelated data. This research used datasets from \u2018The Open AI Dataset Project (AI-Hub, South\n4https://www.kaggle.com/datasets/gennadiyr/ us-equities-news-data\n5https://www.nytimes.com/section/business/ economy\n6https://www.sec.gov/ 7https://seekingalpha.com/ 8https://arxiv.org/search/econ\nKorea)\u2019. All data information can be accessed through \u2019AI-Hub\u2019(www.aihub.or.kr). \u2022 FinWEB9: The dataset is a website that provides economic knowledge and information on finance, loans, and products. We collected data by crawling websites. \u2022 Investopedia10: The dataset is a financial website that functions as an economic dictionary, similar to Wikipedia, and provides definitions of economic terms. We collected all the economic terms available on the website.\nA.2 Hyperparameters for pretraining\nTable 5 shows the hyperparameters used to further pretrain FiLM."
        },
        {
            "heading": "B Fine-tuning methodologies",
            "text": "We introduce the methods applied for finetuning PLMs on financial tasks as well as the experimental settings used. All tasks followed the methods and datasets proposed in each study. The headline task was performed using a sentiment classification method proposed by the authors11. Table 6 lists the descriptions, dataset sizes, and metrics for each task. We followed the finetuning method of BERT. Furthermore, classification tasks (FPB, Headline, FOMC) were finetuned using sequence classification, which involved passing the [CLS] token representation for processing. The entity recognition (NER, FiNER) task follows token classification finetuning using word representations. FinQA follows the question answering system introduced by Chen et al. (2021). Table 7 provides the parameters used to finetune each task are provided. FiNER12 and FinQA13 can be accessed and reviewed on the official GitHub repository provided.\n9https://www.finweb.com/ 10https://www.investopedia.com/ 11https://www.kaggle.com/datasets/ankurzing/\nsentiment-analysis-in-commodity-market-gold 12https://github.com/nlpaueb/finer 13https://github.com/czyssrs/FinQA"
        },
        {
            "heading": "C Existing financial PLMs",
            "text": "We provide a summary of the financial PLMs used in previous research and organize the huggingface hub model used for the experiments. In the list below, items marked with \"\u2713\" indicate models that have undergone further training, while those without the mark are models trained from scratch.\n\u2022 FinBERT-A \u2713 (Araci, 2019) is the initial financial PLM. It is trained using the TRC2 dataset. FinBERT-A focuses on the FPB task and demonstrates superior performance compared with the original BERT. https: //huggingface.co/ProsusAI/finbert\n\u2022 FinBERT-Y (Yang et al., 2020) used more than three datasets compared with FinBERTA and validated its performance across three additional tasks. In addition, instead of using the traditional BERT tokenizer and pretraining for the financial approach, FinBERT-Y generates and applies a financial vocabulary, resulting in improved performance. https://huggingface.co/ yiyanghkust/finbert-tone\n\u2022 FinBERT-L (Liu et al., 2021) collected a general domain corpus (3.3B tokens) and a financial corpus (12.7B tokens) to train the financial BERT model. Unlike the traditional further pretraining approach, FinBERT-L employs multitask self-supervised pretraining during the training process. However, because the proposed model is not publicly available, a comparative experiment could not be conducted.\n\u2022 SEC-BERT (Loukas et al., 2022) is the first to introduce the FiNER task. The BERT model was pretrained exclusively using the SEC filings. This study emphasized constructing vocabulary solely from the financial reports dataset, with a specific focus on numerical tokens found in financial reports. In addition, SEC-BERT proposes a method for substituting numeric tokens in the context of FiNER. https://huggingface.co/ nlpaueb/sec-bert-base\n\u2022 FLANG-BERT & FLANG-RoBERTa \u2713 (Shah et al., 2022) is the first to create a benchmark dataset for financial tasks by aggregating a diverse range of tasks. In addition,\nthis study investigated and applied pretraining methods optimized for economics during the finetuning process. https://huggingface. co/SALT-NLP/FLANG-BERT"
        },
        {
            "heading": "D Results of the FOMC task",
            "text": "In this section, we provide supplementary explanations for the results presented in Table 9. The FOMC task is composed of datasets that are categorized into three types:\n\u2022 Meeting Minutes (MM): These are reports derived from the FOMC\u2019s eight annually scheduled meetings. \u2022 Press Conference Transcripts (PC): These include prepared remarks as well as the Q&A sessions between the Federal Reserve Chair and the press. \u2022 Speeches (SP): This category comprises any speeches delivered by officials of the Federal Reserve. \u2022 Additionally, there is a \"Combined\" category that merges all three aforementioned types.\nTexts with a \"-S\" indicator signify that they have been split. This is because the FOMC often employs neutral sentences to maintain market stability and minimize excessive reactions. To address this issue, the authors of this study employed a rulebased approach to split sentences that exhibit a neutral stance. All scores were obtained by strictly following the settings provided in the GitHub link14 in the Shah et al. (2023). Numbers in parentheses indicate the standard deviation.\n14https://github.com/gtfintechlab/ fomc-hawkish-dovish\nFiLM demonstrates superior performance across all types with the exception of \"PC\". When compared to RoBERTa-base, there is an improvement of +1.6 in performance in the \"Combined\". Notably, there are substantial increases in performance metrics for \"MM-S\" and \"SP\", with \"MM-S\" showing a 3.98% improvement and \"SP\" a 4.69% improvement."
        },
        {
            "heading": "E Comparison with a financial LLM",
            "text": "Wu et al. (2023) introduced the BloombergGPT, a language model with 50 billion parameters trained on a large-scale financial corpus. Table 10 presents the results of the BloombergGPT and encoder-based PLMs on FPB and NER. For the BloombergGPT, we report the numbers provided in this study (Wu et al., 2023). Note that BloombergGPT was evaluated by conducting 5-shot learning on the FPB and 20-shot learning on the NER. Remarkably, FiLM, BERT, and RoBERTa outperformed BloombergGPT, which had many more parameters and required a higher cost. This demonstrates the limitations of using large language models for in-context learning in financial domain.\nF Visualization of financial domain datasets\nTo examine the distribution of sentences from financial domain datasets in the embedding space in Figure 1, we sampled 10,000 sentences from each pretraining dataset and 500 sentences from each downstream task. Then, we generate embedding representations for sampled sentences using the approach proposed in Reimers and Gurevych\n(2019). To visualize sentence embeddings, we reduce the dimensionality of embeddings using UMAP (McInnes et al., 2018)."
        },
        {
            "heading": "G Quantitative analysis for the corpus grouping",
            "text": "To confirm whether the corpus groups we divided have distinct characteristics, we visualized sentence embeddings for each group in Figure 1a. Furthermore, in this section, we aim to establish the unique features of each group through quantitative analysis. We calculate the mean distances between sentence embeddings within a corpus group (Table 11) and across corpus groups (Table 12). Inter-group distances are generally larger than intra-group distances. Thus, sentences within the same group have certain similarities. This result supports our claim that creating training data using diverse corpus groups is a more effective approach. This is because the distinct characteristics of each group contribute to a more comprehensive and varied learning experience."
        },
        {
            "heading": "H Effectiveness of the deduplication",
            "text": "We compared the results before and after applying the preprocessing step to the proposed method (Section 2.2). Specifically, we compare the model\u2019s performance trained on the entire corpus before pre-\nprocessing, referred to as the \u2018Before\u2019 model, with the performance of our FiLM model trained using the proposed method after preprocessing. Table 13 presents the results comparison. In addition, we compare the number of duplicate sentences before and after preprocessing for each dataset, as shown in Table 14."
        },
        {
            "heading": "I Similarity between corpus groups and downstream tasks",
            "text": "In this section, we conduct a qualitative analysis using corpus embeddings to measure the similarity between corpus groups and tasks. The Figure 1a embedding map illustrates that even within the same Financial Domain Dataset, each corpus has distinct characteristics, leading to a wide distribution in the embedding space. Furthermore, to confirm similar corpus groups for each downstream task in financial domain, we calculate both the vocabulary overlap ratio and distances in the embedding space. Figure 3 shows the vocabulary ratio between the corpus groups and downstream tasks. The top 1,000 most frequent unigrams are used to calculate the ratio. Figure 4 shows the two\nclosest corpus groups for each downstream task in the embedding space. \u25cf markers represent sentence embeddings in corpus groups, while \u2715 markers indicate sentence embeddings in downstream task. The mean distance of sentence embeddings between each corpus group and downstream task are calculated, and the two nearest groups are selected based on these distances. We discover that there are slight differences between corpus groups identified as similar in the embedding space and those identified as similar based on the vocabulary overlap ratio. For instance, we noted that the News corpus group demonstrated similarity to FPB in vocabulary overlap, but the SEC-filings corpus group showed similarity to FPB within the embedding space. This indicates that multiple factors should be considered comprehensively when assessing the similarity between the datasets."
        }
    ],
    "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models",
    "year": 2023
}