{
    "abstractText": "The training and inference efficiency of everlarger deep neural networks highly rely on the performance of tensor operators on specific hardware platforms. Therefore, a compilationbased optimization flow with automatic tensor generation and parameter tuning is necessary for efficient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet efficient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, ATFormer with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yang Bai"
        },
        {
            "affiliations": [],
            "name": "Wenqian Zhao"
        },
        {
            "affiliations": [],
            "name": "Shuo Yin"
        },
        {
            "affiliations": [],
            "name": "Zixiao Wang"
        },
        {
            "affiliations": [],
            "name": "Bei Yu"
        }
    ],
    "id": "SP:b17f9e376ba30b31a39918404d0bbc613ee7a303",
    "references": [
        {
            "authors": [
                "Steiner",
                "Steven Johnson",
                "Kayvon Fatahalian",
                "Fr\u00e9do Durand"
            ],
            "title": "Learning to optimize halide with tree search and random programs",
            "venue": "ACM Transactions on Graphics (TOG)",
            "year": 2019
        },
        {
            "authors": [
                "Byung Hoon Ahn",
                "Prannoy Pilligundla",
                "Amir Yazdanbakhsh",
                "Hadi Esmaeilzadeh."
            ],
            "title": "Chameleon: Adaptive code optimization for expedited deep neural network compilation",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Sercan O Ar\u0131k",
                "Tomas Pfister."
            ],
            "title": "Tabnet: Attentive interpretable tabular learning",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2020
        },
        {
            "authors": [
                "Sercan \u00d6. Arik",
                "Tomas Pfister."
            ],
            "title": "Tabnet: Attentive interpretable tabular learning",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, pages 6679\u20136687. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Riyadh Baghdadi",
                "Jessica Ray",
                "Malek Ben Romdhane",
                "Emanuele Del Sozzo",
                "Abdurrahman Akkas",
                "Yunming Zhang",
                "Patricia Suriana",
                "Shoaib Kamil",
                "Saman Amarasinghe."
            ],
            "title": "Tiramisu: A polyhedral compiler for expressing fast and portable code",
            "venue": "In",
            "year": 2019
        },
        {
            "authors": [
                "Yang Bai",
                "Xufeng Yao",
                "Qi Sun",
                "Bei Yu."
            ],
            "title": "Autogtco: Graph and tensor co-optimize for image recognition with transformers on gpu",
            "venue": "IEEE/ACM International Conference on ComputerAided Design (ICCAD).",
            "year": 2021
        },
        {
            "authors": [
                "Yang Bai",
                "Xufeng Yao",
                "Qi Sun",
                "Wenqian Zhao",
                "Shixin Chen",
                "Zixiao Wang",
                "Bei Yu."
            ],
            "title": "Gtco: Graph and tensor co-design for transformer-based image recognition on tensor cores",
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and",
            "year": 2023
        },
        {
            "authors": [
                "I. Bello",
                "B. Zoph",
                "Q. Le",
                "A. Vaswani",
                "J. Shlens."
            ],
            "title": "Attention augmented convolutional networks",
            "venue": "IEEE International Conference on Computer Vision (ICCV).",
            "year": 2019
        },
        {
            "authors": [
                "Ali Furkan Biten",
                "Ruben Tito",
                "Andres Mafla",
                "Lluis Gomez",
                "Mar\u00e7al Rusinol",
                "Ernest Valveny",
                "CV Jawahar",
                "Dimosthenis Karatzas."
            ],
            "title": "Scene text visual question answering",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Cao",
                "Tao Qin",
                "Tie-Yan Liu",
                "Ming-Feng Tsai",
                "Hang Li."
            ],
            "title": "Learning to rank: from pairwise approach to listwise approach",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2007
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin."
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "CoRR.",
            "year": 2016
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin."
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 785\u2013794.",
            "year": 2016
        },
        {
            "authors": [
                "Tianqi Chen",
                "Thierry Moreau",
                "Ziheng Jiang",
                "Lianmin Zheng",
                "Eddie Yan",
                "Haichen Shen",
                "Meghan Cowan",
                "Leyuan Wang",
                "Yuwei Hu",
                "Luis Ceze",
                "Carlos Guestrin",
                "Arvind Krishnamurthy"
            ],
            "title": "TVM: An automated end-to-end optimizing compiler",
            "year": 2018
        },
        {
            "authors": [
                "Tianqi Chen",
                "Thierry Moreau",
                "Ziheng Jiang",
                "Lianmin Zheng",
                "Eddie Yan",
                "Haichen Shen",
                "Meghan Cowan",
                "Leyuan Wang",
                "Yuwei Hu",
                "Luis Ceze"
            ],
            "title": "2018b. {TVM}: An automated end-to-end optimizing compiler for deep learning",
            "year": 2018
        },
        {
            "authors": [
                "Tianqi Chen",
                "Lianmin Zheng",
                "Eddie Yan",
                "Ziheng Jiang",
                "Thierry Moreau",
                "Luis Ceze",
                "Carlos Guestrin",
                "Arvind Krishnamurthy."
            ],
            "title": "Learning to optimize tensor programs",
            "venue": "Annual Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2018
        },
        {
            "authors": [
                "Jack Choquette",
                "M.-J. Edward Lee",
                "Ronny Krashinsky",
                "Vishnu Balan",
                "Brucek Khailany"
            ],
            "title": "2021. 3.2 the A100 datacenter GPU and ampere architecture",
            "venue": "In IEEE International Solid-State Circuits Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Annual Conference of the North American Chapter of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Pratik Fegade",
                "Tianqi Chen",
                "Phillip Gibbons",
                "Todd Mowry."
            ],
            "title": "Cortex: A compiler for recursive deep learning models",
            "venue": "Machine Learning and Systems (MLSys).",
            "year": 2021
        },
        {
            "authors": [
                "Xiaotian Gao",
                "Wei Cui",
                "Lintao Zhang",
                "Mao Yang."
            ],
            "title": "Opevo: An evolutionary method for tensor operator optimization",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2016
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Guyue Huang",
                "Yang Bai",
                "Liu Liu",
                "Yuke Wang",
                "Bei Yu",
                "Yufei Ding",
                "Yuan Xie."
            ],
            "title": "Alcop: Automatic load-compute pipelining in deep learning compiler for ai-gpus",
            "venue": "Proceedings of Machine Learning and Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Zhe Jia",
                "Marco Maggioni",
                "Jeffrey Smith",
                "Daniele Paolo Scarpazza."
            ],
            "title": "Dissecting the nvidia turing T4 GPU via microbenchmarking",
            "venue": "CoRR, abs/1903.07486.",
            "year": 2019
        },
        {
            "authors": [
                "Steinberg",
                "Andy Swing",
                "Mercedes Tan",
                "Gregory Thorson",
                "Bo Tian",
                "Horia Toma",
                "Erick Tuttle",
                "Vijay Vasudevan",
                "Richard Walter",
                "Walter Wang",
                "Eric Wilcox",
                "Doe Hyun Yoon."
            ],
            "title": "In-datacenter performance analysis of a tensor processing unit",
            "venue": "In",
            "year": 2017
        },
        {
            "authors": [
                "Samuel J Kaufman",
                "Phitchaya Mangpo Phothilimthana",
                "Yanqi Zhou",
                "Charith Mendis",
                "Sudip Roy",
                "Amit Sabne",
                "Mike Burrows."
            ],
            "title": "A learned performance model for tensor processing units",
            "venue": "Machine Learning and Systems (MLSys).",
            "year": 2020
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "Tie-Yan Liu."
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on",
            "year": 2017
        },
        {
            "authors": [
                "Fredrik Kjolstad",
                "Shoaib Kamil",
                "Stephen Chou",
                "David Lugato",
                "Saman Amarasinghe."
            ],
            "title": "The tensor algebra compiler",
            "venue": "Proceedings of the ACM on Programming Languages.",
            "year": 2017
        },
        {
            "authors": [
                "Mingzhen Li",
                "Yi Liu",
                "Xiaoyan Liu",
                "Qingxiao Sun",
                "Xin You",
                "Hailong Yang",
                "Zhongzhi Luan",
                "Lin Gan",
                "Guangwen Yang",
                "Depei Qian."
            ],
            "title": "The deep learning compiler: A comprehensive survey",
            "venue": "IEEE Transactions on Parallel and Distributed Systems",
            "year": 2020
        },
        {
            "authors": [
                "Jared Roesch",
                "Steven Lyubomirsky",
                "Logan Weber",
                "Josh Pollock",
                "Marisa Kirisame",
                "Tianqi Chen",
                "Zachary Tatlock."
            ],
            "title": "Relay: A New IR for Machine Learning Frameworks",
            "venue": "ACM SIGPLAN Symposium on Programming Language Design & Implementation",
            "year": 2018
        },
        {
            "authors": [
                "Amit Sabne"
            ],
            "title": "Xla : Compiling machine learning for peak performance",
            "year": 2020
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen."
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2018
        },
        {
            "authors": [
                "Lianlei Shan",
                "Minglong Li",
                "Xiaobin Li",
                "Yang Bai",
                "Ke Lv",
                "Bin Luo",
                "Si-Bao Chen",
                "Weiqiang Wang."
            ],
            "title": "Uhrsnet: A semantic segmentation network specifically for ultra-high-resolution images",
            "venue": "IEEE International Conference on Pattern Recogni-",
            "year": 2021
        },
        {
            "authors": [
                "Benoit Steiner",
                "Chris Cummins",
                "Horace He",
                "Hugh Leather."
            ],
            "title": "Value learning for throughput optimization of deep learning workloads",
            "venue": "Machine Learning and Systems (MLSys).",
            "year": 2021
        },
        {
            "authors": [
                "Qi Sun",
                "Xinyun Zhang",
                "Hao Geng",
                "Yuxuan Zhao",
                "Yang Bai",
                "Haisheng Zheng",
                "Bei Yu."
            ],
            "title": "Gtuner: tuning dnn computations on gpu via graph attention network",
            "venue": "ACM/IEEE Design Automation Conference (DAC).",
            "year": 2022
        },
        {
            "authors": [
                "Kai Sheng Tai",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Improved semantic representations from tree-structured long short-term memory networks",
            "venue": "arXiv preprint.",
            "year": 2015
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Bichen Wu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Yanghan Wang",
                "Fei Sun",
                "Yiming Wu",
                "Yuandong Tian",
                "Peter Vajda",
                "Yangqing Jia",
                "Kurt Keutzer."
            ],
            "title": "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search",
            "venue": "IEEE Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Yu Xiang",
                "Alexandre Alahi",
                "Silvio Savarese."
            ],
            "title": "Learning to track: Online multi-object tracking by decision making",
            "venue": "IEEE International Conference on Computer Vision (ICCV).",
            "year": 2015
        },
        {
            "authors": [
                "Wenqian Zhao",
                "Yang Bai",
                "Qi Sun",
                "Wenbo Li",
                "Haisheng Zheng",
                "Nianjuan Jiang",
                "Jiangbo Lu",
                "Bei Yu",
                "Martin DF Wong."
            ],
            "title": "A high-performance accelerator for super-resolution processing on embedded gpu",
            "venue": "IEEE Transactions on Computer-Aided Design",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Chengfan Jia",
                "Minmin Sun",
                "Zhao Wu",
                "Cody Hao Yu",
                "Ameer Haj-Ali",
                "Yida Wang",
                "Jun Yang",
                "Danyang Zhuo",
                "Koushik Sen"
            ],
            "title": "Ansor: Generating high-performance tensor programs for deep learning",
            "venue": "In USENIX Symposium on Operating",
            "year": 2020
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Ruochen Liu",
                "Junru Shao",
                "Tianqi Chen",
                "Joseph E Gonzalez",
                "Ion Stoica",
                "Ameer Haj Ali."
            ],
            "title": "TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers",
            "venue": "Thirty-fifth Conference on Neural Informa-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, there has been a significant improvement in model performance for deep neural networks (DNNs) (He et al., 2016; Sandler et al., 2018; Shan et al., 2021; Devlin et al., 2019; Wu et al., 2019; Biten et al., 2019; Bello et al., 2019). However, this progress has been accompanied by a significant increase in the number of operators and, consequently, the computational complexity of DNNs. As a result, it has become increasingly challenging to efficiently deploy DNNs with optimized tensor programs on certain hardware accelerators like CPUs, GPUs and TPUs (Jouppi et al., 2017).\nTo overcome the limitations, mainstream searchbased tensor compilers (Chen et al., 2018a; Zheng et al., 2020; Bai et al., 2021; Li et al., 2020; Fegade et al., 2021) are developed. These compilers\nautomatically search for the optimal deployment configuration of each operator on increasingly heterogeneous platforms. Conducting on-device measurements is extremely time-consuming, making it impossible to place all the generated tensor programs on the target platform for measurement during the compilation process. Therefore, the prediction via an optimal cost model is crucial in reducing the time-consuming measurements during the compilation which can significantly improve search efficiency and quality.\nNevertheless, the existing cost models are capable of selecting nearly optimal configurations but suffer from excessively long optimization time. These long optimization times not only impede the deployment period but also raise concerns about the practicality of search-based compilers. Furthermore, statistic cost models trained on one hardware platform exhibit significant performance degradation on different hardware, making them unusable across different platforms. It is noteworthy that the execution times of tensor programs can vary signif-\nicantly on different platforms due to domain gaps, making it challenging to deploy optimized models on multiple platforms. This is further compounded by the significant differences in the features extracted from various platforms. Even when extracted on GPUs, the feature\u2019s stability and performance cannot be guaranteed across different GPU architectures such as Volta, Turing, and Ampere. Therefore, additional engineering efforts are necessary to account for the differences in hardware architectures, resulting in a laborious and cumbersome feature extraction process.\nTo address these challenges, we propose a powerful yet simple approach that uses attention-inspired blocks to enhance the performance of cost models. These blocks can capture global and long-range dependencies among tensor program statements. Additionally, transferable features with pre-trained parameters are used to expedite search convergence across different hardware platforms. These techniques can be easily incorporated into existing search algorithms and improve efficiency in an endto-end fashion. Our design, ATFormer, consistently outperforms popular DNN benchmarks, including small and large-scale models. Furthermore, our techniques enable cross-platform transfer learning, resulting in more efficient deployment.\nThe main contributions of this paper are the following: (i) We highlight the limitations of current auto-tuning frameworks. Existing tree-based performance models are insufficient for evaluating inference in a large search space and transferable knowledge is difficult to acquire across different platforms. (ii) A simple yet efficient design that utilizes attention-based blocks to explore the correlation between all innermost non-loop statements in a full tensor program, resulting in accurate prediction. (iii) Our approach enables rapid adaptation of performance tuning across various GPU platforms using pre-trained parameters on static datasets, not only in cross-operator but also crossplatform scenarios. Comprehensive experiments on modern DNN benchmarks and the large-scale TenSet (Zheng et al., 2021) demonstrate the consistent and superior performance of our method."
        },
        {
            "heading": "2 Background and Related Work",
            "text": "Deep Learning Compiler. Recently, the development of compiler-based optimization frameworks, such as Halide (Adams et al., 2019), TVM (Chen et al., 2018b), XLA (Sabne, 2020),\nand TACO (Kjolstad et al., 2017), has progressed rapidly. These optimization schemes typically consist of two parts: DL framework frontends and code generation backends, as illustrated in Figure 1. The frontend converts an input model into a high-level graph-based intermediate representation (IR) and applies target-independent optimizations, such as operator fusion and data layout transformation. In the backend, target-dependent optimization passes, along with hardware features, further optimize the final performance. TVM (Chen et al., 2018a) is a state-of-the-art search-based tensor compiler that is widely used in academia and industry. Its autotuning aims to achieve performance comparable to hand-tailored libraries and has achieved promising results. TVM has two versions of auto-tuning: AutoTVM (Chen et al., 2018c) and Ansor (Zheng et al., 2020). While AutoTVM is a semi-automated framework that requires pre-defined manual templates, Ansor is more advanced and fully automated. However, both frameworks need to collect data on-the-fly during the search, resulting in an extremely long compilation time.\nTree-based Performance Model. Decision trees are frequently used in classification and regression problems. To enhance their performance, an ensemble learning approach is typically employed to reduce variance. XGBoost (Chen and Guestrin, 2016a) and LightGBM are powerful feature models in sequence modeling tasks. To achieve accurate prediction, a number of works, including (Chen et al., 2018c; Zheng et al., 2020; Ahn et al., 2020; Gao et al., 2021; Bai et al., 2021, 2023; Huang et al., 2023; Zhao et al., 2023), use XGBoost as the performance model during the tuning. AutoTVM extracts domain-specific features from a provided low-level abstract syntax tree (AST). During optimization, these features, which include loop structure information and generic annotations, are explored. Moreover, TreeGRU (Tai et al., 2015) recursively encodes a low-level AST into an embedding vector, which is mapped to a final predicted score within a fully-connected layer to enhance performance. Halide (Adams et al., 2019) builds regression models with hardware-specific features for auto-scheduling. TabNet (Ar\u0131k and Pfister, 2020) uses sequential attention to select the most salient features to reason at each decision via a deep tabular architecture.\nDNN-based Performance Model. In contrast, some recent approaches aim to reduce the impact of\nsearch algorithms on final performance by utilizing more robust and powerful cost models. (Kaufman et al., 2020) and (Sun et al., 2022) employ graph neural networks to predict the latency of DNNs on TPUs. (Steiner et al., 2021) formulates the tuning process as a deterministic Markov Decision Process (Xiang et al., 2015) and solves it by learning an approximation of the value function. Tiramisu (Baghdadi et al., 2019) manually extracts 2534 features from the structure of AST, and forwards the AST as a computation stream to propagate features during the training. These models are trained effectively on a dataset with only a few thousand schedules using the hardware-dependent features crafted by heavy feature engineering techniques. However, complex feature engineering can become problematic in such cases. As hardware-specific features are difficult to transfer to a new platform, a learned performance model trained on one hardware platform typically performs poorly on another. This leads to an issue we call cross-hardware unavailability. Additionally, this approach cannot keep pace with the rapid development of new hardware, which further exacerbates the problem."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "We describe a DNN model as a computation graph and then define some important terminologies.\nDefinition 1 (Subgraph). Computation Graph G is partitioned into a set of subgraphs S based on the graph-level optimizer (Roesch et al., 2018).\nEach search task is extracted from an independent subgraph Si on a specific hardware platform H. Thus, we define search task Q as follows:\nQH(S|G) = { Q1(S1|G), Q 2 (S2|G), . . . , Q n (Sn|G) } ,\n(1) where n is the number of subgraphs inG. Note that each subgraph Si contains a computation-intensive operator \u03c3 and \u03c3 \u2208 Si. Therefore, we use Qi(Si|G) to represent the i\u2212th search task in G. Each subgraph Si has its own search space, which is determined by the input and output shapes, data precisions, memory layout, and the hardware platform. The search space is usually large enough to cover almost all kinds of tensor candidates.\nDefinition 2 (Hierarchical Search Space). A tensor program, denoted by p, represents an implementation of the subgraph using low-level primitives that\nAlgorithm 1 Search-based Framework Input: Search space \u03c61, \u03c62 with operator \u03c3 and setting k. Output: Tensor program p\u2217 with best configuration c\u2217. 1: while nTrials < eachSubgraphTrials do 2: GS1 \u2190 GenerateHighSketch(\u03c61,\u03c3,k); 3: GS2 \u2190 Sampling(GS1,\u03c62,\u03c3,k); 4: P\u2190 EvolutionSearch(GS1, GS2); 5: for p \u2208 P do 6: c\u2190 f(\u00f0(\u03c61, \u03c62|\u03c3, k)); 7: end for 8: nTrials\u2190 nTrials + batchSize; 9: end while 10: c\u2217 \u2190 best tensor program configurations;\nare dependent on the hardware platform. Each tensor program can be considered as a candidate in the search space. We define the hierarchical search space \u03c61,2, which decouples high-level structures \u03c61 from low-level details \u03c62, allowing for the efficient exploration of potential tensor candidates during the tuning process.\nHere, we can transform a tuning problem into an optimization problem that explores the potential tensor programs in a hierarchical search space.\nProblem 1. Given code generation function \u00f0, high-level structure generation parameters \u03c61, lowlevel detail sampling parameters \u03c62, computationintensive operator \u03c3 and operator setting k (e.g., kernel size), our goal is to use \u03c61,2 to build a hierarchical search space and generate tensor program p to achieve the optimal prediction score y\u2217 on a specific hardware platform H.\n\u03c6\u22171,2 = argmax \u03c6 y,\ny = fH(\u00f0(\u03c61, \u03c62|\u03c3, k)). (2)\nThe cost model f predicts score y of the tensor program p. The accuracy of the cost model f is crucial in finding ideal optimization configuration."
        },
        {
            "heading": "3.2 Performance Model",
            "text": "The process of optimization using our design is outlined in Algorithm 1. The input is a set of tobe-optimized operators or subgraphs with different configurations. To implement our workflow, three functions are defined: GenerateHighSketch(), Sampling(), and EvolutionSearch(), as shown in Algorithm 1. GenerateHighSketch() takes \u03c61, \u03c3, and k as input and returns the high-level generation sketch GS1 as output. Sampling() takes GS1, \u03c62, \u03c3, and k as input and returns the low-level annotation samples GS2 as output.\nEvolutionSearch() takes the high-level generation sketch GS1 and the low-level annotation samples GS2 as input and returns a group of tensor candidates for the cost model training. Next, an evolutionary search strategy is used along with a learned cost model to fine-tune the performance of the generated tensor programs. By iteratively mutating high-quality tensor programs, it can generate new programs with potentially higher quality. After a number of measurement trials, the best tensor program configurations can be identified. Hierarchical Feature Generation. The input of ATFormer is a series of mix-grained feature vectors extracted from p\u03c3, where p\u03c3 is the full tensor program to implement operator \u03c3. Each vector represents a single computation statement within p\u03c3. These mix-grained feature vectors are composed of two important components: (i) CoarseGrained operator embedding features that capture the high-level structure of the operator \u03c3 and (ii) Fine-Grained statement features that capture the low-level details of each statement within program p\u03c3. Each operator in the subgraph S can be classified into a few categories, and we represent each operator with a one-hot embedding feature vector that covers all possible operator types. In practice, we use feature vectors of length 10 for the operator embedding and length 164 for the statement features, consistent with the approach used in Ansor (Zheng et al., 2020). The prediction score for a subgraph is computed as the sum of the prediction scores for each innermost non-loop statement within the loop nests of the full tensor program. More details can be found in Figure 2. Model Architecture Our proposed ATFormer model consists of three layers: (i) a kernel embedding layer, which extracts a compact feature representation; (ii) a computation processing layer, which captures essential information from the innermost non-loop computation statements in the\nneighborhood; and (iii) a simple regression layer for making the final prediction. ATFormer can be easily integrated into existing search algorithms and consistently improve the efficiency of autotuning. We believe that the simplicity of our method will attract more research attention to the field of tensor operator optimization, further enhancing training and inference efficiency. The feature processing of computation and regression in ATFormer is illustrated in Figure 3. The kernel embedding layer is composed of two fully connected layers with ReLU activation. The function of the kernel embedding layer is to project the features from low dimension space to a new embedding space for similarity measurement. Starting from the batched tensor programs I \u2208 RL\u00d7Din representing a specific type of operator \u03c3, where L is the accumulated number of the feature statements within I . A kernel embedding layer then generates a set of feature statements E \u2208 RL\u00d7Dout in embedding space. Typically, we use Dout = 512. The value L is determined by the parameters of high-level structures \u03c61 and the low-level details sampling \u03c62 for each subgraph S.\nAs for the computation layer, a set of feature statements E \u2208 RL\u00d7Dout should be split into M stacks of feature statements Z \u2208 RM\u00d7N\u00d7Dout firstly. Each stack contains N feature statements of innermost non-loop computation within a full tensor program p. We adopt the self-attention mechanism for feature statements aggregation. With the parameter tensors written as WQ,WK ,W V , a full tensor program with a set of innermost nonloop feature statements Z is first encoded into query Q, key K, and value V by three identical linear transformations: Q,K,V = Z>W . Then it will be further calculated by the self-attention\nlayer as: Attention(Q,K,V ) = Softmax ( Q>K\u221a dk ) V .\n(3) The final prediction of these M tensor programs is computed by a regression layer with a dimension from 512 to 1. The predicted score is y \u2208 RM\u00d71. Loss Function The model ranks the performance of potential candidates in a large search space. Therefore, the model can be trained with ranking losses or regression losses to predict relative or absolute scores. To explore the loss function to train ATFormer, a common choice is to use the squared error function as a regressor which can mostly care about identifying the well-performing tensor programs. The loss function of the model f on a full tensor program p with throughput h is MSELoss(f, p, h) = ( \u2211 s\u2208S(p) f\u0302(s)\u2212 y)2, where S(p) is the set of innermost non-loop computation statements in tensor program p. We train ATFormer as the performance model f . However, we only care about the relative order of tensor program runtime rather than their absolute values during the compilation. We instead use the following RankLoss (Cao et al., 2007) to rank the performance of candidates in the large design space. This can fully exploit the optimal candidates to reduce the impact of the search algorithm on final prediction results. The loss function is defined as follows:\nRankLoss = \u2211\ns(i),s(j)\u2208S(p)\nlog(1 + ef(i,j)); (4)\nf(i, j) =\u2212sign(yi \u2212 yj)(f\u0302(si)\u2212 f\u0302(sj)). (5)\nWe can use the prediction f\u0302(x) to select the topperforming implementations of a full tensor program p. The computation graph G is trained for tensor programs extracted from all subgraphs. The throughput of all tensor programs is normalized to be in the range of [0, 1]."
        },
        {
            "heading": "3.3 Transfer Learning",
            "text": "The trade-off between search time and performance improvement is interesting to explore and exploit, as long search times may not always be acceptable. Our current focus is on developing a cost model for optimizing tensor operators on a specific hardware platform. However, in practical settings, we require a cost model that can be used across various hardware platforms. This would allow us to reuse a single-cost model for multiple platforms by providing it with new online data during auto-tuning. To achieve this, we pre-train the cost model with an offline static dataset and exploit transferable features that are invariant to both source and target domains to speed up the optimization process, as depicted in Figure 4. The use of transferable features greatly contributes to the success of transfer learning, as different designs may have varying degrees of invariance. By training the cost model offline using a dataset, we can significantly reduce the frequency of on-device measurements and use the pre-trained parameters as a starting point for new search tasks via transfer learning."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 End-to-End Execution Evaluations",
            "text": "Workloads. We evaluate the performance of ATFormer on various DNNs, including small and large-scale models. For small-scale models, we use AlexNet, VGG-16, MobileNet-V2, ResNet-18/50 and Bert-Tiny to evaluate the design. As for the large-scale models, we use BERT and GPT-3 models, specifically BERTbase, BERTlarge, GPT-2large and GPT-3350M . We report the the end-to-end inference latency with batch size 1 on RTX 2080Ti. Baselines and Settings. For statistic model, we use XGBoost as a baseline which has proven to be a state-of-the-art feature-based model in auto-tuning framework (Zheng et al., 2020). For DNN-based\nlearning, we use LSTM with eight heads and 1024 hidden dimensions, and TabNet is implemented in TenSet as another baseline. Note that the search algorithm uses the default configurations, and the search terminates when it runs out of allowed measurement trials. We keep the rest of the factors the same for a fair comparison. Main Results. Figure 6 shows the final optimized total latency results on the RTX2080Ti GPU. Overall, the ATFormer-series model performs the best in all cases. Compared with the tree-based model XGBoost, ATFormer outperforms them in all cases with 1.15 \u2212 1.61\u00d7 speedup. Compared with the DNN-based model TabNet, ATFormer outperforms them in all cases with 1.14\u22122.14\u00d7 speedup. Compared with LSTM, ATFormer performs equally the best and achieves 0.96\u22121.48\u00d7 speedup. Although LSTM surpasses ATFormer a little in finding the best configuration on Bert-Tiny and VGG-16, the amount of computation that can be parallelized in ATFormer leads to a shorter used time. Overall, the experiment results from the GeoMean verify the effectiveness of the attention-based modules over the tree- and DNN-based performance models."
        },
        {
            "heading": "4.2 Transfer Learning Evaluations",
            "text": "As mentioned in Section 3.3, we use RTX 2080Ti and 3090 GPUs as different platforms to verify our design by two typical metrics: i) Fix the measurement trails and compare the total latency and ii) Fix a converged latency, and then compare the search time to reach it. To explore transferable features and fast adaptation of auto-tuning between different hardware platforms, ATFormer is pre-trained with a number of samples from TenSet and then finetuned using online datasets on different platforms. Therefore, we divide our experiment settings into \u201ctraditional learning\u201d and \u201ctransfer learning\u201d parts.\nTraditional Learning. In Table 1, ATFormer achieves the best total latency on RTX 2080Ti, and it performs almost equally best with ATFormer-1L about total latency with a fixed measurement trail\non 3090 GPU. The results show that self-attention based models perform best in the final performance compared with the tree-based and DNN-based cost models on different types of GPUs. Transfer Learning. In Table 1, experiment results on RTX 2080Ti and 3090 show that the pretrained parameters make the search convergence much faster. With the increasing number of training tasks in the offline dataset from 50 to 500, the learning ability of cost models with self-attention blocks, including MHA, ATFormer-1L, and ATFormerMask, become more stable, and they can adapt to the new tasks via transfer learning. ATFormerseries model performs better than the statistic and DNN-based model XGBoost, LSTM in optimized total latency with the parameters trained from TenSet-100 to TenSet-500. All large-scale models are exported from Hugging Face, with a batch size of 1 and a maximum input sequence length of 512. As shown in Table 2, ATFormer achieves latency speedups of 1.39\u00d7, 1.11\u00d7, 1.10\u00d7, and 1.16\u00d7 on the 3090 GPU compared to PyTorch runtime. In terms of end-to-end tuning time, ATFormer achieves speedups of 4.97\u00d7, 5.10\u00d7, 5.69\u00d7, and 6.08\u00d7 compared to traditional learning.\nThe performance of our efficient transfer learning on NVIDIA RTX 3090 GPU can be found in Figure 7. As for the TenSet-50 datasets, curves start from different points at the beginning, and we can find that XGBoost performs best. It means that the transferable features in the ATFormerseries models are not fully exploited on the limited dataset (task#50) during the training. Obviously, the adaptation skills amplify rapidly with the increasing number of tasks on the offline dataset. From TenSet-100 to TenSet-500, we can find that ATFormer-series models show fast adaptation and generalization ability across hardware platforms and operators compared with XGBoost and LSTM models.\nIn Table 3, we make the traditional learning and\ntransfer learning on different hardware platforms for ResNet-18 have an approximate converged latency. ATFormer reduces the search time by up to 5.1\u00d7 while maintaining the same search quality on RTX 2080Ti. This is the best speedup compared with 3.6\u00d7 by XGBoost, 4.2\u00d7 by LSTM, 4.8\u00d7 by ATFormer-1L, and 2.2\u00d7MHA, respectively. Under the same conditions, ATFormer also performs the best with reducing the search time by up to 7.7\u00d7 on RTX 3090 compared with 3.4\u00d7 by XGBoost, 4.5\u00d7 by LSTM, 4.2\u00d7 by ATFormer-1L, respectively. Traditional learning with a mask-guided training scheme degrades the performance on total latency and search time. However, transfer learning with a mask-guided training scheme for ATFormerMask performs best in most cases. Comprehensive experiments show that it is not easy to make ATFormer-Mask have the approximate converged latency on RTX 2080Ti and 3090 compared with traditional learning and transfer learning. It means that ATFormer-Mask with pre-trained parameters has better task generation for tensor programs and achieves better performance during tuning. Transfer learning across different types of CPUs can be found in Appendix A.6. Overall, ATFormer takes full advantage of transferable features learned from the source domain Tesla T4 GPU and transfers the knowledge to the\ndifferent target domains RTX 2080Ti and RTX 3090 to accelerate the convergence speed with a fixed number of measurement trails. Fast convergence is desirable for many users of auto-tuning to have better control of the optimization cost and good performance. For instance, deployment engineers may want to obtain an optimized model as soon as possible or quickly get an upper-bound estimation of total inference latency in real-world production. They can use the cost model like ATFormer with strong generalization as decent pretrained parameters to accelerate not only the convergence speed but also the total execution inference time. Finally, comprehensive experiments with pretrained parameters on different sizes of the TenSet dataset show that ATFormer-series models enable fast adaptation in not only cross-operator but also cross-platform scenarios."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "Various designs are evaluated in this section. We report the performance about total latency, search time on ResNet-18 and MobileNet-V2 and accuracy on the static datasets. Loss Functions. Table 4 shows two different loss functions in our experiments. Method (a) is ATFormer with Root Mean Square Error (RMSE) loss function while method (b) is with lambdaRank loss function. Compared with method (a) and method (b), we find that lambdaRank loss always outperforms RMSE in our design for different\nworkloads of DNNs. It shows that the goal of a decent cost model is to rank the performance of different tensor programs by relative scores in a given search space.\nConvergence Speed. In Table 4, method (d) is the proposed ATFormer, which adapts the pre-trained parameters to the new task via transfer learning into method (c). Note that ATFormer with the pretrained parameters minimizes the total latency of all subgraphs in three DNNs as much as possible and the search time as quickly as possible. The proposed ATFormer improves the total latency by 4.66\u00d7 speedup and convergence speed by 1.55\u00d7 speedup. Method (f) is the AutoTVM with lambdaRank loss function. The performance is inferior to the baseline configuration.\nTraining Schemes. In Table 4, method (c) incorporates the mask module into method (b) during traditional learning. Method (d) imports the mask module into method (e) during transfer learning, resulting in a notable increase in convergence speed. It\u2019s worth noting that adding a mask scheme during traditional learning is not very helpful and can even cause a decrease in the total latency. However, for transfer learning with pre-trained parameters, incorporating the mask module is crucial for achieving faster convergence speed. The introduced techniques do not require expensive training resources in terms of both time and computation power.\nModel Architectures. Table 5 lists ATFormer with various architectures. To achieve high accuracy while minimizing the model parameters, we find that the self-attention block, which contains four heads with 512 hidden dimensions, performs the best on the total latency and search time. Note that ATFormer does not benefit from deeper encoder layers in the Transformer model. Thanks to its simple and efficient architecture, the inference latency of ATFormer is consistently lower than that of the DNNs it optimizes. Thus, we set the two encoder layers as the final decision. Table 6 shows the relationship between the hierarchical-level features and different architectures to affect total latency and search time on ResNet-18.\nAccuracy. Table 7 presents the pairwise comparison accuracy of ATFormer and XGBoost on various scales of static datasets. The findings indicate that ATFormer outperforms XGBoost, demonstrating the highest measurement accuracy and providing optimal search quality during the tuning. We successfully conduct the training process on a server\nequipped with an Intel Core i9-12900K CPU, a NVIDIA GeForce RTX 3090 GPU, and a 2TB hard disk. Table 8 presents the specific training times (in seconds) of the ATFormer series models on static datasets. Note that our approach is also suitable for scenarios involving large batch sizes. Table 9 lists experimental results using batch size 8 on the NVIDIA 3090 GPU via traditional learning."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper introduces ATFormer, a novel and effective design for optimizing tensor programs. ATFormer employs hierarchical features with varying levels of granularity to model the end-to-end compilation. Moreover, self-attention blocks are utilized to explore global dependencies of a complete tensor program for high-quality evaluation. Through transfer learning, ATFormer achieves faster-converged latency and superior transferability across different hardware platforms, outperforming previous state-of-the-art benchmarks. Limitations. We plan to do the transfer learning from GPUs to CPUs and explore the potential of combining with post-training quantization or pruning to efficiently deploy models. Additionally, we will explore more universal and efficient methods for optimizing tensor programs with ATFormer. This includes leveraging hardware features to optimize performance on domain-specific accelerators, such as NVIDIA\u2019s Tensor Cores."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Feature Extraction Details",
            "text": "The feature before ATFormer training can be represented as two different granularities: coarsegrained and fine-grained levels. The coarse-grained level feature can describe each search task in the computation graph. It has 10 elements with the one-hot encoding pattern. In our specific code implementation, the coarse-grained vector contains these operators:\u201cmax\u201d, \u201cmin\u201d, \u201cadd\u201d, \u201cConv2dOutput\u201d, \u201cConv2d_winograd\u201d, \u201cDepthwiseConv2d\u201d, \u201cdense\u201d, \u201csoftmax\u201d, \u201ccompute(b, i, j)\u201d. The \u201cmax\u201d and \u201cmin\u201d can represent some activation functions in deep learning. \u201cdense\u201d means the fully connected layer in computation graph and \u201ccompute(b, i, j)\u201d is a very important function to implement each tensor operation in deep learning. If the intermediate representation about some operators are fused into the same \u201ccompute(b, i, j)\u201d primitive, it means these operators are fused together and can run very efficiently on the specific hardware platforms. As for the fine-grained vector, the length of it including all the listed features for one statement is 164. We use the same set of features for both Turing 2080Ti and Ampere 3090 GPUs. It can be summarized as follows:\n\u2022 Number of float operations: The number of addition, subtraction, division, modulo operation, less-than, greater-than, intrinsic math function such as exp, sqrt.\n\u2022 Number of integer operations: Similar to the number of float operations, but for the operations with integer operations.\n\u2022 Vectorization related features: The number of the innermost vectorized loop statements in a full tensor program.\n\u2022 Unrolling related features: The number of the innermost unrolling loop statements in a full tensor program.\n\u2022 Parallelization related feature: The number of the innermost parallelization loop statements in a full tensor program.\n\u2022 GPU thread binding related features: The lengths of blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y, threadIdx.z and virtual threads which can avoid bank conflict problem in shared memory.\n\u2022 Arithmetic intensity curve: We only sample 10 points from a curve which is defined as FLOPsBytes which is similar to the roof-line model used in computer architecture. It can help us to recognize the type of the search task or operator in computation graph such as compute-intensive or memory-intensive operator on a specific hardware platform.\n\u2022 Buffer Access Feature: We perform feature extraction for at most five buffers. It includes \u201cAccess type\u201d, \u201cBytes\u201d, \u201cUnique bytes\u201d, \u201cLines\u201d, \u201cUnique Lines\u201d, \u201cReuse type\u201d, \u201cReuse distance\u201d, \u201cReuse counter\u201d, \u201cStride\u201d, \u201cAccessed bytes divided by reuse\u201d.\n\u2022 Allocation related features: The size of the allocated buffer for the output results of each statement in a full tensor program.\nWith the combination of coarse-grained and finegrained feature vectors, we can construct them into a hierarchical feature vector to take full advantage of each statements in a full tensor program.\nA.2 Implementation Details ATFormer is implemented on the top of Ansor and evaluated from two aspects: end-to-end search efficiency and quality, as well as performance portability. We compare ATFormer against the state-ofthe-art methods, including both the statistic and DNN-based cost models. The items labeled with XGBoost represent the Ansor default configuration. We also provide a detailed ablation study of the model architecture, accuracy, loss function, convergence speed, and training scheme, with insights and qualitative results. The generated tensor programs are evaluated on two different GPU architectures: Turing RTX 2080Ti and Ampere RTX 3090, with float32 data types used for all evaluations. We train the cost model using the Adam optimizer for 50 epochs, with a starting learning rate of 7e\u22124 that decays to 1e\u22126, and a training batch size set to 512. We use TVM v0.8dev in TenSet (Zheng et al., 2021), LLVM 11.0, and CUDA 11.0 for compilation, while XGBoost 1.5.0 and PyTorch 1.7.1 are used for training models. The use of a \u201cmask\u201d is a widely adopted technique for training transformers. In Figure 5, each tensor program is transformed into a sequence of vectors, with each vector representing a tensor computation statement. During training, all sequences are of the same length, and any shorter sequences are\npadded with zeros at the end. The padded items are masked out and excluded from the loss computation. Our ablative models, including MHA, ATFormer-1L, ATFormer, and ATFormer-M, were also experimented with. MHA is the basic MultiHead Attention layer, ATFormer-1L only has one encoder layer, ATFormer has two encoder layers, and ATFormer-M uses the \"mask\" scheme during training."
        },
        {
            "heading": "A.3 Dataset Details",
            "text": "We evaluated our design using TenSet, a large-scale and challenging dataset for search-based tensor compilers. TenSet comprises 52 million performance records of tensor programs obtained from real measurements on different hardware platforms. Various randomly generated tensor programs for popular workloads are compiled via the TVM compiler and executed on the target hardware platforms. To ensure the inclusion of diverse workloads essential for generalization ability, we collected tensor programs from 120 networks with 13,848 tasks on the NVIDIA Tesla T4 GPU. This dataset serves as a series of static offline datasets."
        },
        {
            "heading": "A.4 Benchmark Details",
            "text": "We evaluate the performance of generated programs by ATFormer on two levels: end-to-end network evaluations and performance portability via transfer learning. For each level of evaluation, we compare ATFormer against the state-of-the-art methods, including the statistic models:\n\u2022 XGBoost (Chen and Guestrin, 2016b)\n\u2022 LightGBM (Ke et al., 2017)\nand DNN-based models:\n\u2022 LSTM (Hochreiter and Schmidhuber, 1997)\n\u2022 Multi-Head Attention (Vaswani et al., 2017)\n\u2022 TabNet (Arik and Pfister, 2021)\nThe generated tensor programs are benchmarked on two different architecture GPU platforms:\n\u2022 NVIDIA 2080Ti GPU with Turing architecture (Jia et al., 2019)\n\u2022 NVIDIA 3090 GPU with Ampere architecture (Choquette et al., 2021)\nWe use float32 as the data type for all evaluations. We train our model with the Adam optimizer for 50 epochs with a starting learning rate of 7e\u22124, the learning rate decays to 1e\u22126, and the training batch size is set to 512. We use TVM v0.8dev in TenSet, LLVM 11.0 and CUDA 11.0 for compilation. Meanwhile, we use XGBoost 1.5.0 and PyTorch 1.7.1 for training models.\nTo explore transferable features and fast adaptation of ATFormer between different hardware platforms, ATFormer is pre-trained using offline learning with a number of samples from TenSet, and then fine-tuned using online learning on different platforms. For the offline learning, we randomly sample 50, 100, 200, 300, 500 search tasks from TenSet NVIDIA Tesla T4 GPU.\nWe train 40 models including XGBoost, LightGBM, LSTM, TabNet, Multi-head attention, ATFormer-1L, ATFormer, ATFormer-Mask for all of experiment evaluation in this paper. Due to the limitation of maximum file size (100MB) in supplementary material, we release the pre-trained model offline learning by Tenset-500 for AFTormer-1L, ATFormer, ATFormer-Mask, Multi-head attention and TabNet. All of the pre-trained models for XGBoost. And we release running scripts in the supplementary material to reproduce the results in Section 5 Table 1. More details about the hyperparameters of each cost model in our experiments can be found in Table 12, Table 13, Table 14, Table 15, Table 16, Table 17, and Table 18."
        },
        {
            "heading": "A.5 Convergence Analysis.",
            "text": "In Figure 8, we present the tuning trials-latency curves that illustrate various stages of auto-tuning with different configurations on ResNet-18. We performed four types of experiments on ResNet using two settings: with transfer learning and without transfer learning. The blue line indicates ATFormer with transfer learning to expedite the tuning pro-\ncess. We observe that the converged latency is the best among the four configurations. The orange line represents the same tuning process with the XGBoost cost model, and we note that the converged latency is inferior to the one with ATFormer. The green line shows ATFormer without transfer learning, and we can observe that the convergence speed is exceptionally fast. The red line represents the Ansor optimization, and we observe that the convergence speed and the final converged latency are both less than the ones achieved by the green line with ATFormer. Therefore, we can infer that ATFormer can expedite the tuning process compared to traditional learning methods through transfer learning and outperforms the state-of-the-art tensor compiler Ansor.\nThe main components in ATFormer model architecture can be categorized into three layers:\n\u2022 Kernel embedding layer: The function of kernel embedding layer is to change the 164+ 10 dimensions into 512 dimensions.\n\u2022 Computation layer: The function of computation layer is to obtain the relationship between each innermost non-loop statement in loop nests of a full tensor program.\n\u2022 Regression layer: The function of regression layer is to project the final prediction about each innermost non-loop statement in an one\ndimension scalar."
        },
        {
            "heading": "A.6 Other Platforms: Intel CPUs",
            "text": "We use the dataset from Intel Platinum-8272 to verify transferability on Intel E5-2698 CPU with a fixed converged latency (6.13ms) by the same measurement trials for ResNet-18. More details can be found in Table 10. Therefore, ATFormer also works well for CPU with lots of different DNN\nbenchmarks including ResNet-50, VGG-16, BERTTiny with batch size 1. As for the ResNet-18, we fix the converged latency to 19.59ms, the traditional learning will cost 658s to search the optimal configuration with XGBoost performance model. But the ATFormer can search the optimal implementation of ResNet-50 with 643s by the same measurement trials under the 16.90ms converged latency. We can get the same conclusions from the VGG-16 and BERT-Tiny neural networks."
        },
        {
            "heading": "A.7 Performance on Tensor Cores",
            "text": "The recent advancements of GPU hardware technology have resulted in a significant increase in computing power, particularly with the introduction of the Tensor Cores on NVIDIA GPUs. Unlike the scalar-to-scalar primitives found in CPUs or the general CUDA Cores in GPUs, Tensor Cores provide specialized tensor computation capacities, which can deliver over 10\u00d7 higher throughput. Notably, the initial version of Tensor Core is designed for handling the GEMM with half-precision input and full-precision output. Recently, new features supporting different datatypes such as int8, int4 and int1 input variables have been introduced in the latest architecture (Truing and Ampere). The collection process takes 5 days with a server equipped with an Intel Core i9-12900K CPU and NVIDIA GeForce RTX 3090 GPU. The sampling selection process for the operator is conducted in a manner similar to that on the GPU\u2019s CUDA cores. We use the floating point 16 (fp16) as the experiemnt datatype and additional experimental results on transfer learning are presented in Table 11."
        }
    ],
    "title": "ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs",
    "year": 2023
}