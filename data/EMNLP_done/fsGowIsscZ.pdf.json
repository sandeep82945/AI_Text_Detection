{
    "abstractText": "In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. We also show that this degradation in trust reduces the success of human-AI collaboration and that different types of miscalibration\u2014unconfidently correct and confidently incorrect\u2014have different negative effects on user trust. Our findings highlight the importance of calibration in user-facing AI applications and shed light on what aspects help users decide whether to trust the AI system.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shehzaad Dhuliawala"
        },
        {
            "affiliations": [],
            "name": "Vil\u00e9m Zouhar"
        },
        {
            "affiliations": [],
            "name": "Mennatallah El-Assady"
        },
        {
            "affiliations": [],
            "name": "Mrinmaya Sachan"
        }
    ],
    "id": "SP:5a21976e5584e18b759bba7079c597d5397631a9",
    "references": [
        {
            "authors": [
                "Ighoyota Ben Ajenaghughrure",
                "Sonia C Sousa",
                "Ilkka Johannes Kosunen",
                "David Lamas."
            ],
            "title": "Predictive model to assess user trust: A psycho-physiological approach",
            "venue": "10th Indian conference on humancomputer interaction, pages 1\u201310.",
            "year": 2019
        },
        {
            "authors": [
                "Ighoyota Ben Ajenaghughrure",
                "Sonia Cl\u00e1udia Da Costa Sousa",
                "David Lamas."
            ],
            "title": "Psychophysiological modeling of trust in technology: Influence of feature selection methods",
            "venue": "ACM on Human-Computer Interaction, 5(EICS):1\u201325.",
            "year": 2021
        },
        {
            "authors": [
                "Gagan Bansal",
                "Besmira Nushi",
                "Ece Kamar",
                "Eric Horvitz",
                "Daniel S Weld."
            ],
            "title": "Is the most accurate AI the best teammate? Optimizing AI for teamwork",
            "venue": "AAAI Conference on Artificial Intelligence, volume 35/13, pages 11405\u201311414.",
            "year": 2021
        },
        {
            "authors": [
                "Gagan Bansal",
                "Besmira Nushi",
                "Ece Kamar",
                "Daniel S Weld",
                "Walter S Lasecki",
                "Eric Horvitz."
            ],
            "title": "Updates in human-AI teams: Understanding and addressing the performance/compatibility tradeoff",
            "venue": "AAAI Conference on Artificial Intelligence, volume",
            "year": 2019
        },
        {
            "authors": [
                "Gagan Bansal",
                "Tongshuang Wu",
                "Joyce Zhou",
                "Raymond Fok",
                "Besmira Nushi",
                "Ece Kamar",
                "Marco Tulio Ribeiro",
                "Daniel Weld."
            ],
            "title": "Does the whole exceed its parts? The effect of AI explanations on complementary team performance",
            "venue": "CHI Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Babak Ehteshami Bejnordi",
                "Mitko Veta",
                "Paul Johannes Van Diest",
                "Bram Van Ginneken",
                "Nico Karssemeijer",
                "Geert Litjens",
                "Jeroen AWM Van Der Laak",
                "Meyke Hermsen",
                "Quirine F Manson",
                "Maschenka Balkenhol"
            ],
            "title": "Diagnostic assessment of deep learning",
            "year": 2017
        },
        {
            "authors": [
                "Nina L Corvelo Benz",
                "Manuel Gomez Rodriguez."
            ],
            "title": "Human-aligned calibration for AI-Assisted decision making",
            "venue": "arXiv preprint arXiv:2306.00074.",
            "year": 2023
        },
        {
            "authors": [
                "Miguel A. Cardona",
                "Roberto J. Rodr\u00edguez",
                "Kristina Ishmael."
            ],
            "title": "Artificial intelligence and the future of teaching and learning",
            "venue": "Technical Report 1, Department of Education, United States of America.",
            "year": 2023
        },
        {
            "authors": [
                "Sabrina Chiesurin",
                "Dimitris Dimakopoulos",
                "Arash Eshghi",
                "Ioannis Papaioannou",
                "Verena Rieser",
                "Ioannis Konstas"
            ],
            "title": "The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational ques",
            "year": 2023
        },
        {
            "authors": [
                "Chi-Keung Chow."
            ],
            "title": "An optimum character recognition system using decision functions",
            "venue": "IRE Transactions on Electronic Computers, 4:247\u2013254.",
            "year": 1957
        },
        {
            "authors": [
                "Edward L Deci",
                "Richard Koestner",
                "Richard M Ryan."
            ],
            "title": "A meta-analytic review of experiments examining the effects of extrinsic rewards on intrinsic motivation",
            "venue": "Psychological bulletin, 125(6):627.",
            "year": 1999
        },
        {
            "authors": [
                "Shehzaad Dhuliawala",
                "Leonard Adolphs",
                "Rajarshi Das",
                "Mrinmaya Sachan."
            ],
            "title": "Calibration of machine reading systems at scale",
            "venue": "arXiv preprint arXiv:2203.10623.",
            "year": 2022
        },
        {
            "authors": [
                "Ran El-Yaniv"
            ],
            "title": "On the foundations of noisefree selective classification",
            "venue": "Journal of Machine Learning Research, 11(5).",
            "year": 2010
        },
        {
            "authors": [
                "John Ferron",
                "Gianna Rendina-Gobioff."
            ],
            "title": "Interrupted time series design",
            "venue": "Wiley StatsRef: Statistics Reference Online.",
            "year": 2014
        },
        {
            "authors": [
                "Biniam Gebru",
                "Lydia Zeleke",
                "Daniel Blankson",
                "Mahmoud Nabil",
                "Shamila Nateghi",
                "Abdollah Homaifar",
                "Edward Tunstel."
            ],
            "title": "A review on human\u2013machine trust evaluation: Human-centric and machine-centric perspectives",
            "venue": "IEEE Transactions on",
            "year": 2022
        },
        {
            "authors": [
                "Corrado Gini"
            ],
            "title": "Variabilit\u00e0 e mutabilit\u00e0: Contributo allo studio delle distribuzioni e delle relazioni statistiche.[Fasc. I.",
            "venue": "Tipogr. di P. Cuppini",
            "year": 1912
        },
        {
            "authors": [
                "Ana Valeria Gonzalez",
                "Gagan Bansal",
                "Angela Fan",
                "Robin Jia",
                "Yashar Mehdad",
                "Srinivasan Iyer."
            ],
            "title": "Human evaluation of spoken vs",
            "venue": "visual explanations for open-domain qa. arXiv preprint arXiv:2012.15075.",
            "year": 2020
        },
        {
            "authors": [
                "G Mark Grimes",
                "Ryan M Schuetzler",
                "Justin Scott Giboney."
            ],
            "title": "Mental models and expectation violations in conversational AI interactions",
            "venue": "Decision Support Systems, 144:113515.",
            "year": 2021
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International conference on machine learning, pages 1321\u20131330. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Yunsan Guo",
                "Jian Wang",
                "Runfan Wu",
                "Zeyu Li",
                "Lingyun Sun."
            ],
            "title": "Designing for trust: A set of design principles to increase trust in chatbot",
            "venue": "CCF Transactions on Pervasive Computing and Interaction, 4(4):474\u2013481.",
            "year": 2022
        },
        {
            "authors": [
                "Rex Hartson",
                "Pardha S Pyla."
            ],
            "title": "The UX Book: Process and guidelines for ensuring a quality user experience",
            "venue": "Elsevier.",
            "year": 2012
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S Weld",
                "Luke Zettlemoyer."
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "arXiv preprint arXiv:1705.03551.",
            "year": 2017
        },
        {
            "authors": [
                "Lukasz Kaiser",
                "Aidan N Gomez",
                "Noam Shazeer",
                "Ashish Vaswani",
                "Niki Parmar",
                "Llion Jones",
                "Jakob Uszkoreit."
            ],
            "title": "One model to learn them all",
            "venue": "arXiv preprint arXiv:1706.05137.",
            "year": 2017
        },
        {
            "authors": [
                "Amita Kamath",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Selective question answering under domain shift",
            "venue": "arXiv preprint arXiv:2006.09462.",
            "year": 2020
        },
        {
            "authors": [
                "Zahra Rezaei Khavas",
                "S Reza Ahmadzadeh",
                "Paul Robinette."
            ],
            "title": "Modeling trust in human-robot interaction: A survey",
            "venue": "Social Robotics: 12th International Conference, ICSR 2020, Golden, CO, USA, November 14\u201318, 2020, Proceedings 12, pages 529\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association of Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Po-Ming Law",
                "Leo Yu-Ho Lo",
                "Alex Endert",
                "John Stasko",
                "Huamin Qu."
            ],
            "title": "Causal perception in questionanswering systems",
            "venue": "CHI Conference on Human Factors in Computing Systems, pages 1\u201315.",
            "year": 2021
        },
        {
            "authors": [
                "John D Lee",
                "Neville Moray."
            ],
            "title": "Trust, selfconfidence, and operators\u2019 adaptation to automation",
            "venue": "International journal of human-computer studies, 40(1):153\u2013184.",
            "year": 1994
        },
        {
            "authors": [
                "Zhuoyan Li",
                "Zhuoran Lu",
                "Ming Yin"
            ],
            "title": "Modeling human trust and reliance in AI-assisted decision making: A markovian approach",
            "year": 2023
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Tianyi Zhang",
                "Percy Liang."
            ],
            "title": "Evaluating verifiability in generative search engines",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Shuai Ma",
                "Ying Lei",
                "Xinru Wang",
                "Chengbo Zheng",
                "Chuhan Shi",
                "Ming Yin",
                "Xiaojuan Ma."
            ],
            "title": "Who should i trust: AI or myself? Leveraging human and AI correctness likelihood to promote appropriate trust in AI-assisted decision-making",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Ruth Mayo."
            ],
            "title": "Cognition is a matter of trust: Distrust tunes cognitive processes",
            "venue": "European Review of Social Psychology, 26(1):283\u2013327.",
            "year": 2015
        },
        {
            "authors": [
                "Joseph E McGrath."
            ],
            "title": "Methodology matters: Doing research in the behavioral and social sciences",
            "venue": "Readings in Human\u2013Computer Interaction, pages 152\u2013169. University of Illinois, Urbana.",
            "year": 1995
        },
        {
            "authors": [
                "Bhaskar Mitra",
                "Nick Craswell"
            ],
            "title": "An introduction to neural information retrieval. Foundations and Trends\u00ae in Information Retrieval, 13(1):1\u2013126",
            "year": 2018
        },
        {
            "authors": [
                "Hussein Mozannar",
                "Arvind Satyanarayan",
                "David Sontag."
            ],
            "title": "Teaching humans when to defer to a classifier via exemplars",
            "venue": "AAAI Conference on Artificial Intelligence, volume 36/5, pages 5323\u20135331.",
            "year": 2022
        },
        {
            "authors": [
                "Cecilie Bertinussen Nordheim",
                "Asbj\u00f8rn F\u00f8lstad",
                "Cato Alexander Bj\u00f8rkli."
            ],
            "title": "An initial model of trust in chatbots for customer service\u2014findings from a questionnaire study",
            "venue": "Interacting with Computers, 31(3):317\u2013335.",
            "year": 2019
        },
        {
            "authors": [
                "Mahsan Nourani",
                "Chiradeep Roy",
                "Jeremy E Block",
                "Donald R Honeycutt",
                "Tahrima Rahman",
                "Eric Ragan",
                "Vibhav Gogate."
            ],
            "title": "Anchoring bias affects mental model formation and user reliance in explainable ai systems",
            "venue": "26th International Conference on Intelli-",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Papenmeier",
                "Gwenn Englebienne",
                "Christin Seifert."
            ],
            "title": "How model accuracy and explanation fidelity influence user trust",
            "venue": "arXiv preprint arXiv:1907.12652.",
            "year": 2019
        },
        {
            "authors": [
                "Andrea Papenmeier",
                "Dagmar Kern",
                "Gwenn Englebienne",
                "Christin Seifert."
            ],
            "title": "It\u2019s complicated: The relationship between user trust, model accuracy and explanations in AI",
            "venue": "ACM Transactions on ComputerHuman Interaction (TOCHI), 29/4:1\u201333.",
            "year": 2022
        },
        {
            "authors": [
                "Amy Rechkemmer",
                "Ming Yin."
            ],
            "title": "When confidence meets accuracy: Exploring the effects of multiple performance indicators on trust in machine learning models",
            "venue": "Chi conference on human factors in computing systems, pages 1\u201314.",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144.",
            "year": 2016
        },
        {
            "authors": [
                "Pedro Rodriguez",
                "Shi Feng",
                "Mohit Iyyer",
                "He He",
                "Jordan Boyd-Graber."
            ],
            "title": "Quizbowl: The case for incremental question answering",
            "venue": "arXiv preprint arXiv:1904.04792.",
            "year": 2019
        },
        {
            "authors": [
                "Paul Slovic."
            ],
            "title": "The psychology of risk",
            "venue": "Sa\u00fade e Sociedade, 19(4):731\u2013747.",
            "year": 2010
        },
        {
            "authors": [
                "Lucia Specia",
                "Dhwaj Raj",
                "Marco Turchi."
            ],
            "title": "Machine translation evaluation versus quality estimation",
            "venue": "Machine translation, 24:39\u201350.",
            "year": 2010
        },
        {
            "authors": [
                "Brian Stanton",
                "Theodore Jensen"
            ],
            "title": "Trust and artificial intelligence",
            "year": 2021
        },
        {
            "authors": [
                "Carmen Thoma."
            ],
            "title": "Under-versus overconfidence: An experiment on how others perceive a biased selfassessment",
            "venue": "Experimental Economics, 19:218\u2013239.",
            "year": 2016
        },
        {
            "authors": [
                "Amy Turner",
                "Meena Kaushik",
                "Mu-Ti Huang",
                "Srikar Varanasi"
            ],
            "title": "Calibrating trust in AI-assisted decision making",
            "year": 2022
        },
        {
            "authors": [
                "Amos Tversky",
                "Daniel Kahneman."
            ],
            "title": "Advances in prospect theory: Cumulative representation of uncertainty",
            "venue": "Journal of Risk and uncertainty, 5:297\u2013 323.",
            "year": 1992
        },
        {
            "authors": [
                "Alicia Vikander"
            ],
            "title": "Background explanations reduce users\u2019 over-reliance on AI: A case study on multi-hop question answering",
            "year": 2023
        },
        {
            "authors": [
                "Kailas Vodrahalli",
                "Tobias Gerstenberg",
                "James Zou."
            ],
            "title": "Uncalibrated Models Can Improve Human-AI Collaboration",
            "venue": "ArXiv:2202.05983 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Xinru Wang",
                "Ming Yin."
            ],
            "title": "Effects of explanations in AI-assisted decision making: Principles and comparisons",
            "venue": "ACM Transactions on Interactive Intelligent Systems, 12(4):1\u201336.",
            "year": 2022
        },
        {
            "authors": [
                "Ming Yin",
                "Jennifer Wortman Vaughan",
                "Hanna Wallach."
            ],
            "title": "Understanding the effect of accuracy on trust in machine learning models",
            "venue": "CHI conference on human factors in computing systems, pages 1\u201312.",
            "year": 2019
        },
        {
            "authors": [
                "Yunfeng Zhang",
                "Q Vera Liao",
                "Rachel KE Bellamy."
            ],
            "title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making",
            "venue": "Conference on fairness, accountability, and transparency, pages 295\u2013305.",
            "year": 2020
        },
        {
            "authors": [
                "Jianlong Zhou",
                "Huaiwen Hu",
                "Zhidong Li",
                "Kun Yu",
                "Fang Chen."
            ],
            "title": "Physiological indicators for user trust in machine learning with influence enhanced fact-checking",
            "venue": "Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9,",
            "year": 2019
        },
        {
            "authors": [
                "Vil\u00e9m Zouhar",
                "Shehzaad Dhuliawala",
                "Wangchunshu Zhou",
                "Nico Daheim",
                "Tom Kocmi",
                "Yuchen Eleanor Jiang",
                "Mrinmaya Sachan."
            ],
            "title": "Poor man\u2019s quality estimation: Predicting reference-based MT metrics without the reference",
            "venue": "17th Conference of",
            "year": 2023
        },
        {
            "authors": [
                "Vil\u00e9m Zouhar",
                "Michal Nov\u00e1k",
                "Mat\u00fa\u0161 \u017dilinec",
                "Ond\u0159ej Bojar",
                "Mateo Obreg\u00f3n",
                "Robin L Hill",
                "Fr\u00e9d\u00e9ric Blain",
                "Marina Fomicheva",
                "Lucia Specia",
                "Lisa Yankovskaya."
            ],
            "title": "Backtranslation feedback improves user confidence in MT, not quality",
            "venue": "Con-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "AI systems are increasingly being touted for use in high-stakes decision-making. For example, a doctor might use an AI system for cancer detection from lymph node images (Bejnordi et al., 2017), a teacher may be assisted by an AI system when teaching students (Cardona et al., 2023), or individuals may rely on AI systems to fulfill their information requirements (Mitra et al., 2018). AI systems are integrated across diverse domains, with an expanding presence in user-centric applications. Despite their growing performance, today\u2019s AI systems are still sometimes inaccurate, reinforcing the need for human involvement and oversight.\n*Shared first authorship 0Data & code: github.com/zouharvi/trust-intervention\nAn effective approach for facilitating decisionmaking in collaborative settings is for the AI system to offer its confidence alongside its predictions. This is shown in Figure 1, where the AI system provides an additional message that enables the user to either accept or reject the system\u2019s answer based on the additional message, such as the confidence score. This makes a strong case for the AI\u2019s confidence being calibrated (Guo et al., 2017) \u2013 when the confidence score aligns with the probability of the prediction being correct.\nWhen a user interacts with an AI system, they develop a mental model (Hartson and Pyla, 2012) of how the system\u2019s confidence relates to the integrity of its prediction. The issue of trust has been extensively studied in psychology and cognitive science with Mayo (2015); Stanton et al. (2021) finding that incongruence (mismatch between mental model and user experience) creates distrust. Given the ever-increasing reliance on AI systems, it is crucial that users possess a well-defined mental model that guides their trust in these systems. Nevertheless, our current understanding regarding the evolution of user trust over time, its vulnerability to trust-depleting incidents, and the methods to re-\nstore trust following such events remain unclear. Addressing these inquiries holds great significance in the advancement of reliable AI systems.\nIn this paper, our objective is to investigate user interactions with an AI system, with a specific focus on how the system\u2019s confidence impacts these interactions. Through a series of carefully designed user studies, we explore the implications of miscalibrated confidences on user\u2019s perception of the system and how this, in turn, influences their trust in the system. Our experiments shed light on how users respond to various types of miscalibrations. We find that users are especially sensitive to confidently incorrect miscalibration (Section 4.1) and that the trust does not recover even after a long sequence of calibrated examples. Subsequently, we delve into an analysis of how trust degradation corresponds to the extent of miscalibration in the examples provided (Section 4.2). Then, we assess whether diminished trust in an AI system for a specific task can extend to affect a user\u2019s trust in other tasks (Section 4.3). We also explore different methodologies for modeling a user\u2019s trust in an AI system (Section 5). Our results show how reduced trust can lower the performance of the human-AI team thus highlighting the importance of holistic and user-centric calibration of AI systems when they are deployed in high-stakes settings."
        },
        {
            "heading": "2 Related Work",
            "text": "Human-AI Collaboration. Optimizing for cooperation with humans is more productive than focusing solely on model performance (Bansal et al., 2021a). Human-AI collaboration research has focused on AI systems explaining their predictions (Ribeiro et al., 2016) or examining the relationship between trust and AI system\u2019s accuracy (Rechkemmer and Yin, 2022; Ma et al., 2023). Related to our work, Papenmeier et al. (2019); Bansal et al. (2021b); Wang and Yin (2022); Papenmeier et al. (2022) examined the influence of explanations and found that inaccurate ones act as deceptive experiences which erode trust.\nNourani et al. (2021); Mozannar et al. (2022) study the development of mental models which create further collaboration expectations. This mental model, or the associated expectations, can be violated, which results in degraded trust in the system and hindered collaboration (Grimes et al., 2021). The field of NLP offers several applications where trust plays a vital role, such as chatbots for various\ntasks or multi-domain question answering (Law et al., 2021; Vikander, 2023; Chiesurin et al., 2023) and transparency and controllability are one of the key components that increase users\u2019 trust Bansal et al. (2019); Guo et al. (2022).\nTrust and Confidence Calibration. A common method AI systems use to convey their uncertainty to the user is by its confidence (Benz and Rodriguez, 2023; Liu et al., 2023). For the system\u2019s confidence to reflect the probability of the system being correct, the confidence needs to be calibrated, which is a long-standing task (Guo et al., 2017; Dhuliawala et al., 2022). This can be any metric, such as quality estimation (Specia et al., 2010; Zouhar et al., 2021) that makes it easier for the user to decide on the AI system\u2019s correctness. Related to calibration is selective prediction where the model can abstain from predicting. The latter has been studied in the context of machine learning (Chow, 1957; El-Yaniv et al., 2010) and its various applications (Rodriguez et al., 2019; Kamath et al., 2020; Zouhar et al., 2023).\nTrust calibration is the relation between the user\u2019s trust in the system and the system\u2019s abilities (Lee and Moray, 1994; Turner et al., 2022; Zhang et al., 2020; Yin et al., 2019; Rechkemmer and Yin, 2022; Gonzalez et al., 2020; Vodrahalli et al., 2022). Specifically, Vodrahalli et al. (2022) explore jointly optimization of calibration (transformation of the AI system reported confidence) with human feedback. They conclude that uncalibrated models improve human-AI collaboration. However, apart from their experimental design being different from ours, they also admit to not studying the temporal effect of miscalibrations. Because of this, our results are not in contradiction.\nModeling User Trust. Ajenaghughrure et al. (2019); Zhou et al. (2019) predictively model the user trust in the AI system. While successful, they use psychological signals, such as EEG or GSR, for their predictions, which is usually inaccessible in the traditional desktop interface setting. Li et al. (2023) use combination of demographic information together with interaction history to predict whether the user is going to accept or reject AI system\u2019s suggestion. The field has otherwise focused on theoretical frameworks to explain factors that affect trust in mostly human-robot interaction scenarios (Nordheim et al., 2019; Khavas et al., 2020; Ajenaghughrure et al., 2021; Gebru et al., 2022)."
        },
        {
            "heading": "3 Human AI Interaction over Time",
            "text": "We begin by providing a preliminary formalism for a human-AI interaction over time. It comprises of two interlocutors, an AI system and a user. At time t, the user provides the AI system with an input or a question qt and the AI system responds with an answer yt along with a message comprising of its confidence in the answer mt. The user has two options, either they accept the AI\u2019s answer or reject it and try to find an answer themselves. The AI is either correct (at = 1) or incorrect (at = 0). The combination of correctness at and confidence mt results in four different possibilities each with a different reward, or risk, shown in Figure 2. For example, confidently incorrect may lead to the user disastrously accepting a false answer while unconfidently correct will make the user spend more time finding the answer themselves.\nDuring the interaction, the user learns a mental model (\u03a8t) of the AI system that they can use to reject and accept the AI\u2019s prediction. This mental model encapsulates something commonly referred to as user trust, which is, however, abstract and can not be measured directly. Instead, in our study, we rely on a proxy that describes a manifestation of this trust. We ask the user to make an estimate of their trust by tying it to a monetary reward. We assume that both depend on the given question qt, message mt, and history. The users place a bet between 0\u00a2 and 10\u00a2, i.e. uBt = U\nB(qt,mt,\u03a8t) \u2208 [0\u00a2, 10\u00a2]. We formally define the user\u2019s decision to accept or reject the AI\u2019s answer as uDt = U\nD(qt,mt,\u03a8t)\u2208{1, 0}, given question qt, message mt, and history. In this work, by the user\u2019s mental model, we refer to it in the context of the features the user might use to decide how much they are willing to bet on the AI\u2019s prediction and how likely they are to agree with the AI and how \u03a8t changes over time."
        },
        {
            "heading": "3.1 Study Setup",
            "text": "To study how user trust changes temporally we design a set of experiments with a sequence of\ninteractions between a user and a simulated AI question-answering (QA) system. We recruit participants who are told that they will evaluate a QA system\u2019s performance on a sequence of questionanswer pairs. The participants are shown the AI\u2019s produced confidence in its answer and then are instructed to use this confidence to assess its veracity. We term an instance of the AI\u2019s question, prediction, and confidence as a stimulus to the user. This method of using user interactions with a system to study user trust is similar to the study performed by Gonzalez et al. (2020). After the participant decides if the system is correct or incorrect, they bet from 0\u00a2 to 10\u00a2 on their decision about the system\u2019s correctness. We then reveal if the AI was correct or incorrect and show the user the gains or losses. The monetary risk is chosen intentionally in order for the participants to think deeply about the task. An alternative, used by Vodrahalli et al. (2022), is to simply ask for participants\u2019 confidence in the answer. While straightforward, we consider this to be inadequate in the crowdfunding setting. This decision is further supported by the fact that there is a difference between what participants report and what they do (Papenmeier et al., 2019). The average duration of the experiment was 6.7 minutes (Figure 9) and we collected 18k stimuli interactions (Table 3). See Figure 3 for an overview of the experiment design and Figure 13 for the annotation interface.1"
        },
        {
            "heading": "3.2 Simulating AI",
            "text": "To investigate users\u2019 interactions, we simulate an AI system that outputs predictions and confidences. The prediction and confidence are produced using a pre-defined generative process.\nOur simulated AI encompasses four modes for the generation of AI \u2018correctness\u2019 and confidence values. For miscalibrated questions, we have two modes: confidently incorrect (CI) and unconfidently correct (UC) modes, while for calibrated questions we use the accurate mode (control) to generate questions.\nWe define a conditional variable ct which denotes the aforementioned conditions. Then, based on the condition ct, we have the following data generation process at timestep t. In our data generation process, we first decide the AI correctness at \u2208 [0, 1] and then decide the confidence mt \u2208 [0%, 100%] as below:\n1Online demo: zouharvi.github.io/trust-intervention\nat \u223c Bernoulli(0.7) if ct = calibratedBernoulli(0.0) if ct = CI Bernoulli(1.0) if ct = UC\nmt \u223c  Uniform(0.45, 0.85) if ct = cal. \u2227 at = 1 Uniform(0.2, 0.55) if ct = cal. \u2227 at = 0 Uniform(0.7, 1.0) if ct = CI \u2227 at = 0 Uniform(0.1, 0.4) if ct = UC \u2227 at = 1\nTo control for participants prior knowledge of the answers to the provided questions, we use randomly generated questions with fictional premises. We also experimented with questions sourced from a combination of Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Unfortunately, this approach resulted in a lot of noise and instances of misconduct as participants would look up the answers to increase their monetary reward. See Appendix A for a description of stimuli generation. We note that the set of questions that the participants see have similar ECE (Expected Calibration Error) scores and we compare this to a real NLP model in Appendix B.\nmaximum payout for a bet is 10\u00a2. UI Elements show possible user actions. See Figure 13 for screenshots."
        },
        {
            "heading": "4 Experiments",
            "text": "We perform three types of experiments. In Section 4.1, we establish the different effects of confidently incorrect and unconfidently correct stimuli. Then, in Section 4.2 we see how the size of confidently incorrect intervention affects the users interaction with the AI system and in Section 4.3 explore if miscalibration is transferable between question types. Lastly, we predict the user interaction in Section 5."
        },
        {
            "heading": "4.1 Effect of Miscalibration",
            "text": "We categorize AI behavior into four categories (Figure 2) and design an experiment to answer:\nRQ1: Do miscalibrated examples affect user trust and alter how they interact with the AI system?\nWe posit that miscalibrated stimuli decrease user trust and subsequently verify the hypotheses:\nWe assign each user to a particular condition. For the control group, we show 60 calibrated stimuli. For confidently incorrect and unconfidently correct groups, we show 10 calibrated, then 5 miscalibrated (according to the particular mode), and then 45 calibrated stimuli. We then observe, in particular, the user bet value and accuracy (Figure 4).\nConfidently incorrect intervention. The control group, which was shown only calibrated stimuli, quickly learns to bet higher than at the beginning and becomes progressively better at it. The confidently incorrect intervention group has the same start but then is faced with the intervention, where they bet incorrectly because of the inaccurate confidence estimation. Even after the intervention, their bet values remain significantly lower and they are worse at judging when the AI is correct. The difference in bet values before and after intervention across confidence levels is also observable in Figure 11. We use the user bet value as a proxy for trust (u\u0304Bcontrol = 7\u00a2, u\u0304 B CI = 5\u00a2) and the user correctness of the bet (u\u0304Bcontrol = 89%, u\u0304 B CI = 78%). The significances are p<10\u22124 and p=0.03, respec-\ntively, with two-sided t-test. Owing to possible errors due to user randomization, we also performed a quasi-experimental analysis of our data to better quantify the effect of our intervention. Interrupted Time Series (Ferron and Rendina-Gobioff, 2014, ITS) analysis is a quasi-experimental method that allows us to assess and quantify the causal effect of our intervention on a per-user basis. ITS models the user\u2019s behavior before and after the intervention and quantifies the effect of the intervention. As the comparison is intra-user, it helps mitigate randomness arising from the inter-user comparison between treatment and control. We use ITS with ARIMA modeling, which is expressed as\nuBt = \u03b20 + \u03b21t+ \u03b221t>15 + \u03f5t + . . .\nwhere 1t>15 is the indicator variable indicating whether t is after the intervention.2 We are interested in the \u03b22 values that indicate the coefficient of deviation from the user bet values before the intervention. Using ITS we find a \u03b22 = \u22121.4 (p<0.05 with two-sided t-test), showing a significant drop in user bet value after the confidently incorrect intervention. We thus reject the null hypothesis and empirically verify H1.\nUnconfidently correct intervention. We now turn to the unconfidently correct intervention. From Figure 2, this type of intervention is symmetric to confidently incorrect apart from the fact that the baseline model accuracy is 70%. Figure 5 shows that users are much less affected by this type of miscalibration. A one-sided t-test shows a statistically significant difference between the average bet values across control and unconfidently correct\n2We ignore the moving average and error terms for brevity. See Appendix C for the full formula.\ngroups (p<10\u22123 with two-sided t-test), which provides evidence for H2. Prior work in understanding psychology has found similar results where humans tend to be more sympathetic to underconfident subjects (Thoma, 2016). While applying findings from human-human interaction to human-AI interactions, we exercise caution and acknowledge the need for further research.\nConsequences of lower trust. We now examine how user\u2019s fallen trust in the system affects their task performance. We assert that when the human\u2019s trust is calibrated, i.e., the human can effectively decide when the AI is likely to be right or wrong, signifies a strong collaboration. The overall monetary gain, which the user accumulates, acts as a good proxy for the collaboration. To analyze this difference we fit a linear model after the intervention to predict the rate of score increase. We model the cumulative gain at timestep t as t \u00b7 \u03b1+ c where \u03b1 is perceived as the expected gain in \u00a2 per one interaction. We report \u03b1 for all three interventions. The results in Figure 6 show that without intervention, \u03b1 = 5.2, which is much higher than with unconfidently correct intervention (\u03b1 = 4.2) and confidently incorrect intervention (\u03b1 = 4.0). Notably, the confidently incorrect intervention has a more negative effect than the unconfidently correct intervention. We thus empirically validate H3, miscalibrated examples significantly reduce the performance of the human-AI team in the long run.\nRQ1 Takeaways: \u2022 User trust in the AI system is affected by mis-\ncalibrated examples. \u2022 Confidently incorrect stimuli reduce trust more\nthan unconfidently correct stimuli."
        },
        {
            "heading": "4.2 Intervention Size",
            "text": "Seeing a noticeable drop in user trust when faced with model confidence errors, we ask:\nRQ2: How many miscalibrated examples does it take to break the user\u2019s trust in the system?\nWe do so by changing the number of confidently incorrect stimuli from original 5 to 1, 3, 7, and 9 and measure how much are users able to earn after the intervention, how much they are betting immediately after the intervention and later one. We now discuss the average results in Table 1.\nUpon observing an increase in intervention size, we note an initial decreasing trend followed by a plateau in \u03b22 (4th column), implying a decrease in trust and user bet values, albeit only up to a certain level. Shifting our focus to accuracy, which measures the users\u2019 ability to determine the AI\u2019s correctness, we observe an initial decline as well (6th column). This decline suggests that users adapt to the presence of miscalibrated examples. However, after 40 examples (25 after intervention), the accuracy begins to rise (8th column) once again, indicating that users adapt once more. Next, we analyze \u00a2 and \u03b1, which represent the total reward and the rate of reward increase. As the intervention size increases, both \u00a2 and \u03b1 (2nd and 3rd columns) continue to decline. This means that the performance is what is primarily negatively affected. Based on these findings, we conclude that users possess the ability to adapt their mental models as they encounter more calibrated stimuli. However, the decreased trust still leads them to place fewer bets on the system\u2019s predictions, resulting in a diminished performance of the human-AI team.\nRQ2 Takeaways: \u2022 even 5 inaccurate confidence estimation exam-\nples are enough to long-term affect users\u2019 trust \u2022 with more inaccurate confidence estimation ex-\namples, users are more cautious"
        },
        {
            "heading": "4.3 Mistrust Transferability",
            "text": "Increasingly, single machine learning models are used on a bevy of different topics and tasks (Kaiser et al., 2017; OpenAI, 2023). Owing to the distribution of the training data, the AI\u2019s performance will vary over input types. Although users are generally not privy to training data input types, Mozannar et al. (2022) show that users use this variance in model behavior to learn when the model is likely to be wrong. Inspired by this we ask:\nRQ3: Do miscalibrated questions of one type of question affect user trust in the model\u2019s output for a different type of question?\nIn the next experiment, we simulate this by having two types of questions \u2013 either related to trivia or math. Then, we introduce a confidently incorrect intervention only for one of the types and observe the change in trust on the other one. For example, we introduce a confidently incorrect math questions and then observe how it affects trust on trivia stimuli. We refer to the type of questions we provide intervention for as \u201caffected\u201d questions while the other as \u201cunaffected\u201d questions. We run two sets of experiments where we mix trivia and math as affected questions.\nThe results in Figure 7 show that there is a gap between trust in the unaffected and affected stimuli type. The gap (u\u0304Bunaffected = 5.4\u00a2, u\u0304 B affected = 5.0\u00a2) is smaller than in the control settings (Figure 4) but still statistically significant (p<10\u22123 with twosided t-test). This is supported by the analysis using ITS where we look for the relative change to compare user bet values before and after the intervention. We find a significant decrease in bet values for both affected and unaffected questions (\u03b2affected = \u22120.94, \u03b2unaffected = \u22120.53, p<0.05 with two-sided t-test).\nRQ3 Takeaways: \u2022 Miscalibrated responses of one type affect the\nuser\u2019s overall trust in the system \u2022 Miscalibrated responses of one type further re-\nduce user trust in examples of the same type \u2022 Thus users also take into consideration question\ntypes as they create mental models of the AI system correctness"
        },
        {
            "heading": "5 Modeling User Trust",
            "text": "In human-AI collaboration systems, it is the collaboration performance that is more important than the accuracy of the AI system itself (Bansal et al., 2021a). In such cases, an AI system that can understand and adapt to how its output is used is more better. An important challenge in understanding the user\u2019s behavior is estimating how likely the user is to trust the system. This would also allow the system to adapt when user trust in the system is low by perhaps performing a positive intervention that increases user trust. We apply our learnings from the previous section and show that systems that explicitly model the user\u2019s past interactions with the system are able to better predict and estimate the user\u2019s trust in the system. We now develop increasingly complex predictive statistical models of user behavior, which will reveal what contributes to the user process and affects trust. For evaluation, we use F1 and accuracy (agreement) and mean absolute error (bet value) for interpretability.\n\u2022 uDt \u2208{T, F} Will the user agree? (F1) \u2022 uBt \u2208 [0, 10] How much will the user bet? (MAE)"
        },
        {
            "heading": "5.1 Local Decision Modeling",
            "text": "We start by modeling the user decision at a particular timestep without explicit access to the history and based only on the pre-selected features that represent the current stimuli and the aggregated user history. These are:\n\u2022 Average previous bet value \u2022 Average previous TP/FP/TN/FN decision. For\nexample, FP means that the user decided the AI system was correct which was not the case.\n\u2022 AI system confidence \u2022 Stimulus number in user queue\nEach sample (input) is turned into a vector,3 and we treat this as a supervised machine learning task for which we employ linear/logistic regression, decision trees, and multilayer perceptron (see code for details). We evaluate the model on a dev set which is composed of 20% of users4 which do not appear in the training data and present the results in Table 2. It is important to consider the uninformed baseline because of the class imbalance. The results show, that non-linear and autoregressive models predict the user decisions better although not flawlessly.\nDecision trees provide both the importance of each feature and also an explainable decision procedure for predicting the user bet (see Figure 15). They also offer insights into feature importance via Gini index (Gini, 1912). For our task of predicting bet value, it is: previous average user bet (63%), AI system confidence (31%), stimulus number (1%), and then the rest. The R2 feature values of linear regression reveal similar importance: previous average user bet (0.84), AI system confidence (0.78), previous average TP (0.70) and then the rest. The mean absolute error for bet value prediction of random forest models based only on the current confidence (stateless, i.e. no history information) is 2.9\u00a2. This is in contrast to a mean absolute error of 2.0\u00a2 for a full random forest model. This shows that the interaction history is key in predicting user trust."
        },
        {
            "heading": "5.2 Diachronic Modeling",
            "text": "Recurrent networks can selectively choose to remember instances of the context that are crucial to making a prediction. Unlike alternate approaches\n3For example, \u27e8avg. bet: 6.7, TP: 50%, FP: 10%, TN: 30%, FN: 10%, conf: 81%, i: 13\u27e9\n4(30 + 30 + 30) \u00b7 20% \u00b7 60 = 1080 samples\nthat use an average of mean interactions a user had with a system, a GRU can effectively track where user trust in the system underwent a large change. To test this, we look at the information in the hidden state of the GRU we train on the user interactions (see Figures 8 and 12). The GRU\u2019s internal state is able to identify areas that caused shifts in the user\u2019s trust and changed their future interactions. This peak is much higher for the confidently incorrect than for the unconfidently correct interventions which is in line with our conclusion that confidently incorrect examples deteriorate trust more than unconfidently correct examples."
        },
        {
            "heading": "6 Discussion",
            "text": "We now contextualize our findings to real-world applications and discuss the differences and their implications.\nMiscalibration impacts user trust. Even a small (5) number of miscalibrated examples affects how users trust the system in the future. In our controlled setting we consider a symmetric risk-reward setup. However, past work has shown that trust is linked to risk. In real applications, the reward and cost of trusting the system might not be the same. For example in an AI system detecting cancer, having a doctor manualy do the screening has lower cost than a misdiagnosis.\nConfidently incorrect examples lower trust more than confidently correct examples. Standard methods of evaluating model calibration, such as the Expected Calibration Error (ECE), do not take this into account. A holistic calibration metric should take these user-centric aspects into account, particularly, how users interpret these confidence scores and how it affects their trust in the system.\nMiscalibration effects persist and affect user behavior over long time spans. In our setup, users interact with the system continuously over a ses-\nsion. After the intervention, their trust decreases over several interactions. Real-life user interactions with AI systems might not always follow this pattern. For example, a user might use a search engine in bursts when they have an information need. The larger time intervals between interactions might dampen the strong feelings of trust or mistrust.\nMistrust transfers between input types. Our experiments reveal that the model\u2019s miscalibration on a certain type of input also reduces the user\u2019s trust in the model on other types of inputs. In real-world applications, AI systems are generally presented to users as an abstraction and the user may or may not be aware of the underlying workings of the system. For example, recent user-facing LLMs often employ techniques such as a mixture-of-experts or smaller specialized models that perform different tasks. In such cases, the transfer of miscalibration can be erroneous.\nRNN outperforms linear models in modeling user trust. This is indicative that modeling user trust is complex and requires more sophisticated non-linear models. Like most deep learning models, a recurrent network requires more data for accurate prediction. However, user-facing applications can collect several features and with more data deep learning models might generalize better and help us dynamically track and predict user trust."
        },
        {
            "heading": "7 Conclusion",
            "text": "When interacting with AI systems, users create mental models of the AI\u2019s prediction and identify regions of the system\u2019s output they can trust. Our research highlights the impact of miscalibrations, especially in confidently incorrect predictions, which leads to a notable decline in user trust in the AI system. This loss of trust persists over multiple interactions, even with just a small number of miscalibrations (as few as five), affecting how users trust the system in the future. The lower trust in the system then hinders the effectiveness of human-AI collaboration. Our experiments also show that user mental models adapt to consider different input types. When the system is miscalibrated for a specific input type, user trust is reduced for that type of input. Finally, our examination of various trust modeling approaches reveals that models capable of effectively capturing past interactions, like recurrent networks, provide better predictions of user trust over multiple interactions."
        },
        {
            "heading": "8 Future work",
            "text": "Regaining trust. We examined how miscalibrated examples shatter user trust and we show that this effect persists. We also show that this lack of trust adversely affects human-AI collaboration. Understanding how to build user trust in systems could greatly aid system designers.\nComplex reward structures. In our experiments, the user is rewarded and penalized equally when they are correct and incorrect. This reward/penalty is also instantly provided to the user. This might not hold for other tasks, for example, in a radiology setting, a false negative (i.e. missing a tumor) has a very large penalty. Past work in psychology has shown that humans suffer from loss-aversion (Tversky and Kahneman, 1992) and are prone to making irrational decisions under risk. (Slovic, 2010). We leave experimentation involving task-specific reward frameworks to future work.\nEthics Statement The participants were informed that their data (anonymized apart from interactions) would be published for research purposes and had an option to raise concerns after the experiment via online chat. The participants were paid together with bonuses, on average, \u2243$24 per hour, which is above the Prolific\u2019s minimum of $12 per hour. The total cost of the experiment was \u2243$1500.\nBroader impact. As AI systems get more ubiquitous, user trust calibration is increasingly crucial. In human-AI collaboration, it is important that the user\u2019s trust in the system remains faithful to the system\u2019s capabilities. Over-reliance on faulty AI can be harmful and caution should be exercised during deployment of critical systems.\nLimitations\nSimulated setup. Our experiments were conducted on users who were aware that their actions were being observed, which in turn affects their behavior (McGrath, 1995). We hope our work inspires large-scale experiments that study how users interact directly with a live system.\nDomain separation. In the Type-Sensitivity Experiment (Section 4.3) we consider only two question types, trivia and math, and provide the participant with an indicator for the question type. In real-world usage, the user might provide inputs that may not be clearly distinct from each other.\nMonetary reward. A user interacting with an information system to seek information. In our experiments, we replace this goals with a monetary reward. This misalignment in the motivation also affects the participant behavior (Deci et al., 1999)."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Hussein Mozannar and Danish Pruthi for their feedback at various stages of the project. We also thank Shreya Sharma, Abhinav Lalwani, and Niharika Singh for being our initial test subjects for data collection. MS acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1)."
        },
        {
            "heading": "A Question Generation",
            "text": "We generate 60 trivia and math questions using ChatGPT using the following two prompts. We manually filter questions which are possibly answerable given expert knowledge. See Figure 13 for examples of generated questions. All the generated questions are part of the released data.\nGenerate a fake mathematical question that seems like they are answerable but a key information is missing. Generate two plausible definitive answers, the first of which is \u201ccorrect\u201d.\nGenerate fake trivia question that seems like they are answerable but a key information is missing and they are not related to the real world. Generate two plausible definitive answers, the first of which is \u201ccorrect\u201d."
        },
        {
            "heading": "B Real AI System Confidence",
            "text": "In all our experiments (control and with different interventions) we find a similar ECE scores (control: 0.29% , UC-5: 0.30%, CI-1: 0.28%, CI-3: 0.29%, CI-5: 0.28%, CI-7: 0.29%, CI-9: 0.29%) This is due to the intervention sizes being very small to have a major effect on the ECE score. We compare to other models: DPR on Natural Questions: 37.1%, ResNet-152 on Imagenet: 5.48%.\nC Interrupted Time Series\nuBt = \u03b20 + \u03b21 \u00b7 t+ \u03b22 \u00b7 1(t > 15)+ W\u2211 i=1 \u03d5iu B t\u2212i\ufe38 \ufe37\ufe37 \ufe38\nMoving average terms\n+ W\u2211 j=1\n\u03b8j\u03f5t\u2212j + \u03f5t\ufe38 \ufe37\ufe37 \ufe38 Error terms\n10 calibrated\nstimuli\n45 calibrated\nstimuli\n5 confidently incorrect stimuli\naverage accuracy of 11th stimulus is 20%\naverage bet value is 8\u00a2\naverage accuracy of 35th stimulus is 85%\naverage bet value is 6\u00a2\nspline from control group (without interventions)\nspline fitted to bet values\nhistograms of accuracy (projected from points) upper (semi-transparent) is control group\nbottom is confidently incorrect queue\nFigure 14: Annotated version of Figure 4. Average user bet values (y-axis) and bet correctness (point & histogram color) with control set of stimuli (top) and confidently incorrect stimuli (bottom). The spline shows 3rd degree polynomial. Transparent features are overlayed from the other graph.\n4 10\n4 8 8 10\n8 MAE: 2.85, #sample: 2829 prior_bet_avg <> 0.07\nMAE: 1.53, #sample: 971 prior_bet_avg <> 0.08 MAE: 2.63, #sample: 1858 ai_conf <> 68%\nMAE: 2.26, #sample: 351 ai_conf <> 72% MAE: 0.88, #sample: 620 TP_avg <> 85% MAE: 2.00, #sample: 519 prior_bet_avg <> 0.03 MAE: 2.29, #sample: 1339 prior_bet_avg <> 0.04\nFigure 15: First three layers of a decision tree that predicts bet value (in gray for each node)."
        }
    ],
    "title": "A Diachronic Perspective on User Trust in AI under Uncertainty",
    "year": 2023
}