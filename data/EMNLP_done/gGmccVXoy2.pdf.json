{
    "abstractText": "When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs\u2014complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of book covers), or personal context (e.g., when they read a book). Standard retrieval models that rely on lexical or semantic overlap between query and document text are challenged in such retrieval settings, known as tip of the tongue (TOT) retrieval. We introduce a simple but effective framework for handling such complex queries by decomposing the query with an LLM into individual clues, routing those as subqueries to specialized retrievers, and ensembling the results. Our approach takes advantage of off-the-shelf retrievers (e.g., CLIP for retrieving images of book covers) or incorporate retriever-specific logic (e.g., date constraints). We show that our framework incorporating query decomposition into retrievers can improve gold book recall up to 6% absolute gain for Recall@5 on WhatsThatBook, a new collection of 14,441 real-world query-book pairs from an online community for resolving TOT inquiries. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Kevin Lin"
        },
        {
            "affiliations": [],
            "name": "Kyle Lo"
        },
        {
            "affiliations": [],
            "name": "Joseph E. Gonzalez"
        },
        {
            "affiliations": [],
            "name": "Dan Klein"
        }
    ],
    "id": "SP:6a2f266e84f654bd4f67fab437a72b0a5d6674e0",
    "references": [
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein."
            ],
            "title": "Neural module networks",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 39\u201348. IEEE Computer Society.",
            "year": 2016
        },
        {
            "authors": [
                "Jaime Arguello",
                "Adam Ferguson",
                "Emery Fine",
                "Bhaskar Mitra",
                "Hamed Zamani",
                "Fernando Diaz."
            ],
            "title": "Tip of the tongue known-item retrieval: A case study in movie identification",
            "venue": "Proceedings of the 2021 Conference on Human Information Interaction and",
            "year": 2021
        },
        {
            "authors": [
                "Leif Azzopardi",
                "Maarten de Rijke",
                "Krisztian Balog."
            ],
            "title": "Building simulated queries for known-item topics: An analysis using six european languages",
            "venue": "Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development",
            "year": 2007
        },
        {
            "authors": [
                "Samarth Bhargav",
                "Georgios Sidiropoulos",
                "Evangelos Kanoulas."
            ],
            "title": "it\u2019s on the tip of my tongue\u2019: A new dataset for known-item retrieval",
            "venue": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, WSDM \u201922, page",
            "year": 2022
        },
        {
            "authors": [
                "Michael K. Buckland."
            ],
            "title": "On types of search and the allocation of library resources",
            "venue": "Journal of the American Society for Information Science, 30(3):143\u2013 147.",
            "year": 1979
        },
        {
            "authors": [
                "Daniel Fernando Campos",
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng",
                "Bhaskar Mitra."
            ],
            "title": "Ms marco: A human generated machine reading comprehension dataset",
            "venue": "ArXiv, abs/1611.09268.",
            "year": 2016
        },
        {
            "authors": [
                "David Elsweiler",
                "David E. Losada",
                "Jos\u00e9 C. Toucedo",
                "Ronald T. Fernandez."
            ],
            "title": "Seeding simulated queries with user-study data for personal search evaluation",
            "venue": "Proceedings of the 34th International ACM SIGIR Conference on Research and Development in",
            "year": 2011
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Unsupervised corpus aware language model pre-training for dense passage retrieval",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2843\u20132853,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Hagen",
                "Daniel W\u00e4gner",
                "Benno Stein."
            ],
            "title": "A corpus of realistic known-item topics with associated web pages in the clueweb09",
            "venue": "Advances in Information Retrieval, pages 513\u2013525, Cham. Springer International Publishing.",
            "year": 2015
        },
        {
            "authors": [
                "Claudia Hauff",
                "Matthias Hagen",
                "Anna Beyer",
                "Benno Stein."
            ],
            "title": "Towards realistic known-item topics for the clueweb",
            "venue": "Proceedings of the 4th Information Interaction in Context Symposium, IIIX \u201912, page 274\u2013277, New York, NY, USA. Association for Com-",
            "year": 2012
        },
        {
            "authors": [
                "Claudia Hauff",
                "Geert-Jan Houben."
            ],
            "title": "Cognitive processes in query generation",
            "venue": "Advances in Information Retrieval Theory, pages 176\u2013187, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2011
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Towards unsupervised dense information retrieval with contrastive learning",
            "venue": "arXiv preprint arXiv:2112.09118.",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Jhamtani",
                "Hao Fang",
                "Patrick Xia",
                "Eran Levy",
                "Jacob Andreas",
                "Ben Van Durme."
            ],
            "title": "Natural language decomposition and interpretation of complex utterances",
            "venue": "arXiv preprint arXiv:2305.08677.",
            "year": 2023
        },
        {
            "authors": [
                "Christian Johnson."
            ],
            "title": "Binary encoded word mover\u2019s distance",
            "venue": "Proceedings of the 7th Workshop on Representation Learning for NLP, pages 167\u2013172, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas O\u011fuz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "arXiv preprint arXiv:2004.04906.",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
            "venue": "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal"
            ],
            "title": "Decomposed prompting: A modular approach for solving complex",
            "year": 2022
        },
        {
            "authors": [
                "Jinyoung Kim",
                "W. Bruce Croft."
            ],
            "title": "Retrieval experiments using pseudo-desktop collections",
            "venue": "Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM \u201909, page 1297\u20131306, New York, NY, USA. Association for",
            "year": 2009
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "year": 2019
        },
        {
            "authors": [
                "Jin Ha Lee",
                "Allen Renear",
                "Linda C. Smith."
            ],
            "title": "Known-item search: Variations on a concept",
            "venue": "Proceedings of the American Society for Information Science and Technology, 43(1):1\u201317.",
            "year": 2006
        },
        {
            "authors": [
                "Patrick Lewis",
                "Yuxiang Wu",
                "Linqing Liu",
                "Pasquale Minervini",
                "Heinrich K\u00fcttler",
                "Aleksandra Piktus",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "PAQ: 65 million probably-asked questions and what you can do with them",
            "venue": "Transactions of the Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Sheshera Mysore",
                "Arman Cohan",
                "Tom Hope."
            ],
            "title": "Multi-vector models with textual guidance for finegrained scientific document similarity",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Kirk Roberts",
                "Tasmeer Alam",
                "Steven Bedrick",
                "Dina Demner-Fushman",
                "Kyle Lo",
                "Ian Soboroff",
                "Ellen Voorhees",
                "Lucy Lu Wang",
                "William R Hersh."
            ],
            "title": "TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19",
            "venue": "Jour-",
            "year": 2020
        },
        {
            "authors": [
                "S.E. Robertson",
                "S. Walker."
            ],
            "title": "On relevance weights with little relevance information",
            "venue": "Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201997, page 16\u201324, New York,",
            "year": 1997
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "venue": "Now Publishers Inc.",
            "year": 2009
        },
        {
            "authors": [
                "Christopher Sciavolino",
                "Zexuan Zhong",
                "Jinhyuk Lee",
                "Danqi Chen."
            ],
            "title": "Simple entity-centric questions challenge dense retrievers",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6138\u20136148, Online",
            "year": 2021
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725,",
            "year": 2016
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of",
            "year": 2018
        },
        {
            "authors": [
                "las Baskiotis",
                "Patrick Gallinari",
                "Thierry Arti\u00e8res",
                "Axel-Cyrille Ngonga Ngomo",
                "Norman Heino",
                "\u00c9ric Gaussier",
                "Liliana Barrio-Alvers",
                "Michael Schroeder",
                "Ion Androutsopoulos",
                "Georgios Paliouras"
            ],
            "title": "An overview of the bioasq large-scale biomedical",
            "year": 2015
        },
        {
            "authors": [
                "David Wadden",
                "Kyle Lo",
                "Bailey Kuehl",
                "Arman Cohan",
                "Iz Beltagy",
                "Lucy Lu Wang",
                "Hannaneh Hajishirzi."
            ],
            "title": "SciFact-open: Towards open-domain scientific claim verification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Jianyou Wang",
                "Kaicheng Wang",
                "Xiaoyue Wang",
                "Prudhviraj Naidu",
                "Leon Bergen",
                "Ramamohan Paturi"
            ],
            "title": "Doris-mae: Scientific document retrieval using multi-level aspect-based queries",
            "year": 2023
        },
        {
            "authors": [
                "Eugene Yang",
                "David D. Lewis",
                "Ophir Frieder",
                "David A. Grossman",
                "Roman Yurchak."
            ],
            "title": "Retrieval and richness when querying by document",
            "venue": "Biennial Conference on Design of Experimental Search & Information Retrieval Systems.",
            "year": 2018
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Jing Liu",
                "Ruiyang Ren",
                "Ji rong Wen."
            ],
            "title": "Dense text retrieval based on pretrained language models: A survey",
            "venue": "ArXiv, abs/2211.14876.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Tip of the tongue (TOT) refers to the retrieval setting in which a user is unable to formulate a precise query that identifies a sought item, even if the user knows they\u2019ve encountered this item before. For example, users searching for movies they watched or books they read long ago often resort to complex and creative queries that employ a diverse set of strategies to express information relevant to the sought item\u2014high-level categories (e.g., topic, genre), content details from the movie or book (e.g., events, characters), references to personal context\n1We release code and data at https://github.com/kl2806/whatsthatbook\n(e.g., when they last read the book), descriptions of extratextual elements (e.g., movie promotional posters, book covers), and more. In fact, in an annotation study of TOT queries for movies, Arguello et al. (2021) found over 30 types of informational facets that users may include when crafting queries. Figure 1 shows a TOT query and its corresponding gold book.\nA key challenge in TOT retrieval is that queries are not just longer and more complex than those in popular retrieval datasets, but resolving them requires an enriched document collection since\nquery-document relevance can\u2019t necessarily be established from document content alone (see Table 1). For example, in Figure 1, the query\u2019s description of the book cover\u2014The cover of this book was yellow with a silhouette of a cat\u2014can be highly useful for identifying the book, but necessitates the book\u2019s representation to contain that information.\nIn this work, we present a simple yet effective technique for improving TOT retrieval: First, we decompose queries into individual subqueries or clues that each capture a single aspect of the target document. Then, we route these subqueries to expert retrievers that can be trained individually. Finally, we combine their results with those from a base retriever that receives the original query. Experiments show improvement in gold book recall over description-only retrieval baselines on a set of 14,441 real-world query-book pairs collected from an online forum for resolving TOT inquiries, complete with cover images and metadata."
        },
        {
            "heading": "2 Decomposing Queries",
            "text": "In this section, we describe our a simple but effective method for tackling long, complex TOMT queries. Given a collection of documents d1, . . . , dn and a textual query q, the TOT retrieval task aims to identify the sought document d\u2217. The input (raw) documents are semi-structured; each document d contains metadata fields d(1), . . . , d(k). In our the WhatsThatBook dataset, the documents are books that have the fields that correspond to plot, its publication year, an image of its book cover, title, genre, and author etc. Missing elements take on a default value (e.g., blank image, earliest publish date in overall book collection)."
        },
        {
            "heading": "2.1 Method",
            "text": "First, the query decomposer takes a query q and outputs a set of subqueries q(1), \u00b7 \u00b7 \u00b7 , q(k), for k metadata fields. To do this, we use in-context learning with a large language model (LLM) to generate a subquery q that is relevant to that field or optionally output the string \"N/A\" if the q does not contain any relevant information to the field; this is repeated for each field and can be run in independently in parallel for each field.\nWe experiment with two prompting strategies, extractive and predictive. Extractive prompting aims to generate subqueries that are purely extractions from the original query. The LLM is instructed to output \"N/A\" if there is not information\nrelevant to the metadata field. Predictive prompting aims to generate subqueries that are similar to the content of the metadata field. Note that the subqueries need not be extractions from the original query and can be inferred metadata or even hallucinations from the query decomposer. Using LLMs to generate subqueries affords us the ability to set the few-shot prompt generation targets to be predictions. This is important as the information in queries are rarely presented in a form amenable for matching with the corresponding document field. For example, books have publish dates, but queries will rarely mention these dates; instead, users may articulate personal context (e.g., \u201cI read this book in highschool around 2002-2005\u201d). Then to simplify the learning task for a date-focused retrieval expert, we might ask the LLM to predict a \u201clatest possible publish date\u201d (e.g., 2005).\nSee Table 2 for examples of extractive and predictive subqueries and Appendix A for examples of their prompts. In practice, we use GPT 3.5 (gpt-3.5-turbo) few-shot prompting with up to 8 in-context examples. Each field has its own prompt template and set of examples. Subqueries for different fields can be generated in parallel, as the they are independent of each other."
        },
        {
            "heading": "2.2 Retrieval Experts",
            "text": "We have retriever models, or experts, that specialize to specific field types. Let R1, . . . , Rk represent these retrievers. Retrievers can be implemented as dense, sparse, or symbolic logic.\nIf a retriever requires training, we run the query decomposer over all query-document pairs (q, d) in the training set. This produces effectively k training datasets, where each dataset is comprised of a subquery and document-field pair. For example, field j would have training dataset of examples (q(j), d(j)).\nAt indexing time, each document\u2019s field is indexed according to the specifications of its retriever expert. For example, if the retriever is implemented as an embedding model, then that specific field is converted into an embedding. On the other hand, if the retriever is a sparse model, then a sparse index would be built using just that specific field\u2019s text.\nAt inference time, each retriever takes a subquery q(j) and retrieves documents from its associated index of fields."
        },
        {
            "heading": "2.3 Implementation details",
            "text": "In practice, for titles, authors, plot, and genre, we use Contriever (Izacard et al., 2021), a state-of-theart dense retriever.2 For both models, we train for a total of 10,000 steps with a batch size of 16, learning rate of 1e-4. For titles, we finetune with 3,327 extracted subqueries. For our base retriever, we use the full training set of original book descriptions. For embedding search during inference, we use the faiss library and project all embeddings to 768 Johnson (2022).\nFor cover images, we use CLIP (Radford et al., 2021), a state-of-the-art image-text model that can be used for image retrieval by scoring matches between embedded images and their textual descriptions. Specifically, we finetune ViT-B/323 on 2,220 extracted subqueries using cross-entropy loss with batch size of 4, learning rate of 5e-5 and weight decay of 0.2 for 10 epochs with the Adam optimizer (Kingma and Ba, 2014). We select the model with the best top 1 retrieval accuracy on a validation set.\nFor publish dates useing the predictive prompting, we use a symbolic function that heuristically scores 0 if a book was published after the subquery date (i.e. predicted latest publish date) and 1 otherwise. If necessary, we heuristically resolve the subquery to a year."
        },
        {
            "heading": "2.4 Combining retrieved results",
            "text": "In this work, we restrict to a simple strategy of using a weighted sum of all k retrieval scores across the (q(j), d(j)). That is, the final score is:\n2https://huggingface.co/facebook/contriever 3https://huggingface.co/sentence-transformers/clip-ViT-\nB-32\ns(q, d) = n\u2211\nj=1\nw(j)Rj(q (j), d(j))\nAll documents are scored in this manner, which induces a document ranking for a given query q. We tune the weights wj on the validation set and select the weights that have the best Recall@5."
        },
        {
            "heading": "3 Datasets",
            "text": "WhatsThatBook We introduce the WhatsThatBook dataset consisting of query-book pairs collected from a public online forum on GoodReads for resolving TOT inquiries about books.4 On this forum, users post inquiries describing their sought book and community members reply with links to books on GoodReads as proposed answers.5 If the searcher accepts a book as the correct answer, the post is manually tagged as SOLVED and a link to the found book is pinned to the thread. For these solved threads, we take the original inquiry as our query q and the linked book as gold d\u2217. At the end, WhatsThatBook contains 14,441 query-book pairs. Each query corresponds to a unique book. Finally, these books are associated with pages on GoodReads, which we used to obtain publication year metadata and images of book covers. We further collect 172,422 negative books (books that do not correspond to a gold query in our dataset) to make the setting more realistic, for a total of 186,863 books. To collect negative books, crawl books authored by crawling books by the authors of the positive books.\n4https://www.goodreads.com/group/show/185-what-sthe-name-of-that-book. We scraped data from February 2022.\n5This is a simplification of community interactions. Threads also may include dialogue between original poster and members but this is beyond the scope of our work.\nQuery-Document Clue-Field Clue-Field Clue-Field\nQuery: I think I saw this in a used store once and I remember saying to my new husband \u201cmy daughter use to read that book to her little brother\u201d and it\u2019s funny because on the outside cover is a little girl reading a book to her little brother. It\u2019s called.....my book, or my story, or something simple like that. It would be about 15 or more years old. The girl was blond and the boy brunet....I think!!!! Inside was the cutest little sentences and my kids use to do what each page said...thank you!! Extracted Title: It\u2019s called.....my book, or my story, or something simple like that. Extracted Date Clue: 15 years old (2006 or earlier) Extracted Cover: The outside cover is a little girl reading a book to her little brother.\nDescription: Glossy pictorial hardcover no dust jacket. 2001 7.75x9.13x25. GUIDE FOR PARENTS WITH PICTURES, HOW TO TEACHING CHILDREN READING. Actual Title: My First Book Actual Date First published September 1, 1984\nActual Cover:\nQuery: I read a book in 2008 or 2009 that was part of a series for young adults. It was fantasy, and about several families of witches and warlockes. The main character was named Holly. Extracted Genre: for young adults, fantasy Predicted Plot: Holly is a young witch who comes from a long line of magical families... Predicted Cover: Young woman with long, curly hair holding a wand and surrounded by swirling colors and symbols\nUnderstanding WhatsThatBook queries. To examine the information contained within queries, we analyze the results of query decomposition using extractive prompting (see Table 3). First, we find only 645 (4.5%) posts have no clue extracted (and thus aren\u2019t captured in the table total). Second, most posts have between one and three clues (\u00b5=2.33). Third, nearly every query contains some description of the book\u2019s plot elements. Beyond that, over half of the queries provide some description of temporal context surrounding the book. Queries containing descriptions of the author are rare, which is expected since knowing the author name would likely have allowed the user to find the book without seeking assistance. Given that, it\u2019s somewhat surprising that descriptions of titles occur almost a third of the time. Manually examining these, we find title clues are often uncertain guesses (\u201cI think the title might start with Nurse but I\u2019m not sure\u201d) or details that models are likely to struggle with (\u201cThe title is made up of either two or four short words (maybe one syllable)\u201d).\nReddit-TOMT (Books) We additionally use the books subset6 of the Reddit-TOMT (Bhargav et al., 2022) dataset, which includes 2,272 query-book pairs.7 Queries can refer to the same book, leaving 1,877 unique books in this dataset.\nExperimental setup. For the experiments in the rest of this paper, we split WhatsThatBook into\n6We note that the full Reddit-TOMT dataset also contains 13K queries matched with movies. To fully explore the capabilities of our approach, we restricted to the books subset specifically due to feasibility of obtaining images of book covers, while doing the same with movie posters was more difficult. We leave this to future investigations.\n7We removed 47 query-book pairs for which the gold book did not have a GoodReads link, which was necessary for obtaining cover images.\ntrain (n=11,552), validation (n=1,444) and test (n=1,445) sets. By the nature of our dataset construction, the number of queries and books is equal. We use all 14,441 books, which are gold targets with respect to some query, as our full document collection for indexing. As for Reddit-TOMT (Books), given its size we use the entire dataset as an additional test set."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Baseline models",
            "text": "Models that use original queries. We evaluate our approach against several popular retrieval models that have been used as baselines for a range of other retrieval datasets (see Table 1). For textonly models\u2014BM25 (Robertson and Walker, 1997; Robertson and Zaragoza, 2009), Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020), Contriever (Izacard et al., 2021), and ColBERT (Khattab and Zaharia, 2020)\u2014the document representation is simply the concatenation of all available document fields into a single text field. For our imageonly baseline\u2014CLIP (Radford et al., 2021)\u2014the document representation is only the embedded book cover. All baselines receive the same input (full) query, and are finetuned with the same hyperparameters (\u00a72.2) except using the full training set (as opposed to just examples with a successful subquery extraction).\nModels that use generated queries. To evaluate the effect of generating subqueries independently without training specialized retrievers, we also train the top performing text-only model from the set of models with queries enriched with LLMs. We experiment with using the concatentation of all subqueries generated by the query decomposer as the query representation for both the extractive and predicted subqueries. To isolate the effect of generating separate subqueries, we also use LLM to generate a single rewritten query that is used as input to Contriever, by simply prompting LLMs to generate a single new query that better retrieves the desired item (prompt in Appendix A. In addition, we generate hypothetical documents and use these as the query, similar to Gao and Callan (2022) though we differ in that we train on these generations as queries while Gao and Callan (2022) restricted their use to the zero-shot setting."
        },
        {
            "heading": "4.2 Results",
            "text": "Table 4 shows the test results on WhatsThatBook. We use Recall@K metric as our primary metric since each query has exactly one correct item.\nContriever is the best-performing baseline model. In this setting with low lexical overlap, we see that dense retrievers like DPR and Contriever outperform sparse retrievers like BM25.\nWithout extracting clues about the book cover, using CLIP on its own is not effective, likely due to its limited context window.8\nQuery decomposition improves performance. When using the rewritten queries without training individual expert retrievers, we observe that concatenating the predicted clues improve the model. However, both generating rewritten queries and entire hypothetical documents perform worse than just using the original document as input. Our approach to decompose queries and route clues to specialized retrievers improves performance (ranging from +6 to +7 Recall@K across all cutoffs) over the next best baseline retriever. Table 6 shows the results for each individual expert retriever on WhatsThatBook and the books subset of TOMT.\nPerformance degrades predictably with corpus size To see how well the performance scales as more documents are added, we also report the performance of the base performing baseline and our model as we add more documents to the corpus. We evaluate the performance for different corpus sizes by varying the number of negative books added to the corpus. Figure 2 shows that for both our decomposition and the baseline, the performance drops significantly as more negative books are added. There is a sharp decline of performance when negative books are first introduced (between corpus size 1 and 2). Since the negative books are collected from the authors of the positive books, they may be more difficult to distinguish than a random book. As more documents are added, there is a marginal decrease in performance smoothly. Furthermore, our decomposition method performance better than the baseline for different corpus sizes and has slightly less performance degradation as more documents are added to the corpus.\nMetadata fields exhibit long-tailed behavior. The query decomposer generates a plot subquery in at least 90% of the queries for both WhatsThatBook and TOMT. Dates occur in a large proportion of the queries, but are not specific enough to be effective identifying the book. While subqueries for the author appear very infrequently, when they do appear, they are much more effective than more generic subqueries such as dates or genres. Images\n8We pass the full query into CLIP and allow for truncation to happen naturally. This is a big issue with CLIP, which supports a narrow query length; hence, motivating our approach to extract clues about book covers from the full query.\nWhatsThatBook Reddit-TOMT (Books) Subquery Top 5 Top 10 Top 20 Top 100 % not N/A Top 5 Top 10 Top 20 Top 100 % not N/A\nare much more effective when using decomposition compared to the image only CLIP baseline, as the image retriever model is able to access the visual part of the text\nPredictive prompting performs better than extractive prompting. Overall, we find that predicting subqueries is more effective than extracting. Subqueries are generated for every query, thus, there is more data for the specialized retriever experts to train on, compared to extractive clues where some retrievers only have a small fraction of the dataset for finetuning. Moreover, the predictive clues allows the LLM to make inferences based\non information that is not explicitly present in the query. For example, the LLM is able to make inferences about the genre effectively based on the full query even when the user does not explicitly discuss the genre. Another benefit of the extractive clue is that the subqueries are more grounded in the original query.\nTrade-off exists between generating subquery and individual retriever expert performance. Between the prompting strategies, we find that there is often a trade-off between how selective the query decomposer is with generating a subquery for a metadata field, and the effectiveness when generated. For most of the metadata retrieval queries, the extractive prompting approach is slightly more effective than predictive prompting on the examples that it does not predict \"N/A\" on."
        },
        {
            "heading": "5 Error Analysis",
            "text": "We sample 50 predictions where the top performing model where the model fails to get the book correct in the top 5 and categorize the types of errors made by the model in Table 7. The most common kind of error is failure to model the relationship between the document and query, which happens in instances where there may be events that dense models fail to capture indicating that there is still headroom on the task. Moreover, documents are sometimes brief or written in a style that is not similar to other descriptions. Lastly, because users are recalling memories, some of these can be false memories that lead to errors in retrieval."
        },
        {
            "heading": "6 Related Work",
            "text": "Dense methods for document retrieval. Document retrieval has a long history of study in fields like machine learning, information retrieval, natural language processing, library and information sciences, and others. Recent years has seen the rise in adoption of dense, neural network-based methods, such as DPR (Karpukhin et al., 2020) and Contriever (Izacard et al., 2022), which have been shown can outperform sparse methods like BM25 (Robertson and Walker, 1997; Robertson and Zaragoza, 2009) in retrieval settings in which query-document relevance cannot solely be determined by lexical overlap. Researchers have studied these models using large datasets of querydocument pairs in web, Wikipedia, and scientific literature-scale retrieval settings (Campos et al., 2016; Sciavolino et al., 2021; Roberts et al., 2020). Many retrieval datasets have adopted particular task formats such as question answering (Kwiatkowski et al., 2019; Yang et al., 2018b; Tsatsaronis et al., 2015) or claim verification (Thorne et al., 2018;\nWadden et al., 2022). We direct readers to Zhao et al. (2022) for a comprehensive, up-to-date survey of methods, tasks, and datasets.\nKnown-item and TOT retrieval. Tip of the tongue (TOT) is a form of known-item retrieval (Buckland, 1979; Lee et al., 2006), a long-studied area in the library and information sciences. Yet, lack of large-scale public datasets has made development of retrieval methods for this task difficult. Prior work on known-item retrieval focused on constructing synthetic datasets (Azzopardi et al., 2007; Kim and Croft, 2009; Elsweiler et al., 2011). For example, Hagen et al. (2015) released a dataset of 2,755 query-item pairs from Yahoo! answers and injected query inaccuracies via hired annotators to simulate the phenomenon of false memories Hauff and Houben (2011); Hauff et al. (2012), a common property of TOT settings.\nThe emergence of large, online communities for resolving TOT queries has enabled the curation of realistic datasets. Arguello et al. (2021) categorized the types of information referred to in TOT queries\nfrom the website I Remember This Movie.9 Most recently, Bhargav et al. (2022) collected queries from the Tip Of The Tongue community on Reddit10 and evaluated BM25 and DPR baselines. Our work expands on their work in a key way: We introduce a new method for retrieval inspired by long, complex TOT queries. In order to test our method on a large dataset of TOT queries, we collected a new dataset of resolved TOT queries such that we also had access to metadata and book cover images, which were not part of Bhargav et al. (2022)\u2019s dataset.\nQuery Understanding and Decomposition. Our work on understanding complex informationseeking queries by decomposition is related to a line of work breaking down language tasks into modular subtasks (Andreas et al., 2016). More recently, LLMs have been used for decomposing complex tasks such as multi-hop questions into a sequence of simpler subtasks (Khot et al., 2022) or smaller language steps handled by simpler models (Jhamtani et al., 2023).\nRelated to decomposition of long, complex queries for retrieval is literature on document similarity (Mysore et al., 2022) or query-by-document (QBD) (Yang et al., 2018a). In these works, a common approach is decomposing documents into sub-passages (e.g. sentences) and performing retrieval on those textual units. The key differentiator between these works and ours is that document similarity or QBD are inherently symmetric retrieval operations, whereas our setting requires designing approaches to handle asymmetry in available information (and thus choice of modeling approach or representation) between queries and documents. In this vein, one can also draw parallels to Lewis et al. (2021), which demonstrates that retrieving over model-generated question-answering pairs instead of their originating documents can improve retrieval, likely due to improved query-document form alignment. In a way, this is similar to our use of LLMs to generate clues that better align with extratextual document fields, though our work is focused on query-side decomposition rather than document-side enrichment. More recently, Wang et al. (2023) propose using LLMs for decomposing different facets of complex queries for scientific documents.\n9https://irememberthismovie.com/ 10https://www.reddit.com/r/tipofmytongue/"
        },
        {
            "heading": "7 Conclusion",
            "text": "We study tip of the tongue retrieval, a real-world information-seeking setting in which users issue long, complex queries for re-finding items despite being unable to articulate identifying details about those items. We introduce WhatsThatBook, a large challenging dataset of real-world TOT queries for books. We also introduce a simple but effective approach to handling these complex queries that decomposes them into subqueries that are routed to expert retrievers for specialized scoring. Our simple framework allows for modular composition of different retrievers and leveraging of pretrained models for specific modalities such as CLIP for document images. We experiment with different subquery generation strategies and find that generating predictions of document fields is more effective. We observe improvements up to 6% absolute gain over state-of-the-art dense retrievers for Recall@5 when incorporating query decomposition into existing retrievers.\nLimitations\nGiven our proposed method is designed to handle TOT queries, there is an implicit assumption that the document collection covers the sought item with high probability. That is, a system tailored for TOT retrieval is unlikely to perform well if its index is missing too many books. While our dataset is large-scale, one limitation is that even a corpus of 187k documents does not cover the sought document sufficiently. Consider, for instance, the total number of books available on GoodReads (over 3.5B as of time of writing). Another limitation of our method is that the overhead for tackling another domain with this technique is non-trivial as the prompts and few-shot examples may not be directly transferable. We believe a promising avenue for future work is reducing the effort needed to bootstrap the design and training of retrieval experts and incorporating them into a query decomposer."
        },
        {
            "heading": "8 Acknowledgements",
            "text": "We thank the members of the Berkeley NLP group and the anonymous reviewers for their insightful feedback. This work was supported by a grant from DARPA SemaFor, and gifts from sponsors of Sky Computing and BAIR. The content does not necessarily reflect the position of the sponsors, and no official endorsement should be inferred."
        },
        {
            "heading": "A Prompts",
            "text": "Extractive Prompt\nYou are a utility built to take in forum queries from users looking for books and output the aspect that is about the cover. If there is not enough information , then output N/A. Do not guess and only output text that was in the\nquery. Here are some examples: Question: Hi there , I read this book in highschool around 2002 -2005. From what I remember , the main\ncharacter is nicknamed \"Mouse\" and she rides a big chestnut horse in jumper shows. I think this book may have been Australian. The cover just showed a chestnut horse and rider in mid jump. I think the title was one word --it may have been the name of the horse.I cannot remember the name or the author of this book. I have googled everything I can think of but I cannot to find this book and its driving me crazy! I'd be grateful for any help on this! Thank you! Cover Clue: The cover just showed a chestnut horse and rider in mid jump.\nQuestion: So I read this book somewhere between 2008 -2010 when I was in elementary school or middle school. It was about a girl who lived by the sea , and started out with her having dinner at this boarding house she lived in. And something happened (she received a fruit or something else that was banned in her\ntown) and she had to hide it. Then , the landlady of her house got mad when she found out. The MC went down to the beach and saw this giant walrus thing (I can 't remember clearly but I think there was also a guy who rode the walrus but maybe not) and the MC got on the walrus and they rode away to this magical land. I don 't remember much else , except the story had something to do with fruits for some reason , and the characters mother was likely from this magical land. Also , in the end , the main character brings Color and happiness back to her seaside village and I think there 's something else to do with strawberries. Please help! I've been searching for this book for years! Thanks so much <3 Cover Clue: N/A Question: I remember reading this around 2008/2009. It was about a girl being prettier than her mother , and when the mother gets jealous she sends her daughter to a type of boarding school for people who are well -known (Like nobles and princesses. She may be royalty). The cover was purple and captivating ,( which was why I picked the book to read ^^) with a girl 's face on it. However , I'm not sure if the cover really had a face on it, or if it was completely purple .(view spoiler)[I believe that at the end , a minion of the girl 's mother helped the girl get rid of her mother. Maybe it was because the mother\ntried to kill her daughter for being beautiful , especially since she was getting older. (hide spoiler)] Please help me find it. It was the first book that got me to love reading at that time. :) Cover Clue: The cover was purple and captivating ,(which was why I picked the book to read ^^) with a girl 's face on it. However , I'm not sure if the cover really had a face on it, or if it was completely purple.\nQuestion: I probably read this book in the late 90s, early 2000s, and cannot find the title anywhere. The main character is an outcast in her high school who lives in a trailer park near a cemetery. From what I remember she was walking through the cemetery trying to clear her mind when she wondered upon a funeral that had very few people at the funeral. After that she starts attending funerals on the weekend. She even has an outfit she wears just to go to these funerals. Cover Clue: N/A Question: hi im looking for a book there was two girls. the older one liked dumpster diving and had a duck shaped jar of memories that she could go into. the younger one was blonde and liked buying shirts from a thrift store and writing on them , and she c o u l d n t remember a n y o n e s faces. the younger one tried to mail her mom a coconut and the mailman was the only face she could remember. there was also a woman in a hospital who only said red s h o e s and the cover was green or blue. was read a couple years\nago so probably 2017 -2019 ish? Cover Clue: the cover was green or blue. Question: This book was blue (if that helps). And I think it had a mailbox on the front of it. There were\nthree girls and they hated their rival school. So to get the two schools to be friends they made everyone send letters to another person in the school. One girl got a guy who wanted to impress a girl at his school so him and the girl he was sending letters went on on \"Practice Dates\" And they ended up liking each other :) The other girl likes playing \"Games\" with her letter guy... i dont really remember what happened with them. And the third girl had a jerk as her letter guy.. I dont really remeber what happened with him either.If anyone knows the name of the book I'm looking for would you please let me know? :) Thanks so much :) :) :) Cover Clue: This book was blue (if that helps). And I think it had a mailbox on the front of it. Question: So I do not remember but the title , author , or cover , but I remember a bit of the plot. I believe it starts out with the main characters set up on a date. They d o n t know each o t h e r s names , but their date consists of a sexy get to know y o u /photo shoot where t h e y re not allowed to have sex but they do anyway. She gets pregnant but c a n t find him after. Flash forward a bit , she works for\nher f a t h e r s large New York office and they make a deal to with with her baby d a d d y s office. He finds out s h e s pregnant and so the story goes. I hope this is enough to go on and someone recognizes it. Thank you! Cover Clue: N/A\nNow here is the example , remember not to add in additional information that 's not in the question.Question: I read this book back around 2008 (I think) can 't remember author but read two of her works 1) is about this girl who falls in love with a guy who is actually a dragon , he takes her back to his homeland and I think he is injured , there are other dragons there and his relatives are also dragons author gives\nvery vivid imagery I believe there were types of dragons like fire and ice ones ... 2) the second book ( warning:spoilers ahead) is about this girl who has a sister named rose (not too sure about name) who is already engaged to some local village boy , but she falls in love with this cold man who is like \"ivy\" and they escape together for a while but she returns and the girl also loves him but he eludes her grasp , I remember the end she talks about the imagery on the wall of roses and ivy intertwining together ... Any help much appreciated !!! :) Cover Clue:\nPredictive Prompts\nYou are a utility for guessing titles of books. Given the book described below , what is a possible title for the book? Only return one predicted title without any extra text. Even if you 're unsure , try to come\nup with something .\\n\\nDescription: {}\nYou are a utility for guessing author names. Guess a possible author for the book described below. Don 't worry if you 're unsure. Only return the name and a short explanation of why.\\n\\nDescription: {}\nYou are a utility for categorizing books. Given the book description below , generate several possible genre tags for the book. Try to have a diversity of coarse and fine -grained genres. Don 't generate more than five genres .\\n\\nDescription: {}\nYou are a useful tool for generating ideas for cover art. Write 1 or 2 sentences depicting what a the cover of a book might look like based on the description below. Stick to only one idea.\\n\\nDescription: {}\nYou are a writing tool for generating ideas. Write a possible plot synopsis for a book based on the description below. Don 't include the title or author .\\n\\nDescription: {}\nQuery Rewrite Prompt\n\"You are a utility for helping users find books. Given the user 's book description below , generate a query that a user can copy -paste into a book database to find the book. The query should focus on the important aspects of the book that will help the database locate it. These can be keywords about the book 's title , author , genre , year , or distinguishing character or plot elements .\\n\\nUser 's Query: {}\\ nDatabase Query: \""
        }
    ],
    "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
    "year": 2023
}