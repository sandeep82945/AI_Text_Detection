{
    "abstractText": "Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens\u2014for instance, learning to \u201cskip\u201d over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shikhar Murty"
        },
        {
            "affiliations": [],
            "name": "Pratyusha Sharma"
        },
        {
            "affiliations": [],
            "name": "Jacob Andreas"
        },
        {
            "affiliations": [],
            "name": "Christopher D. Manning"
        }
    ],
    "id": "SP:8a76aab867a2f6a86f4dadc3953ff4cfbac179b4",
    "references": [
        {
            "authors": [
                "Christopher D. Manning"
            ],
            "title": "What does BERT",
            "year": 2019
        },
        {
            "authors": [
                "Cundy",
                "Marcus Hutter",
                "Shane Legg",
                "Joel Veness",
                "Pedro A Ortega."
            ],
            "title": "Neural networks and the chomsky hierarchy",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Ameet Deshpande",
                "Karthik Narasimhan."
            ],
            "title": "Guiding attention for self-supervised learning with transformers",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4676\u2013 4686.",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Patrick Verga",
                "Mohit Yadav",
                "Mohit Iyyer",
                "Andrew McCallum."
            ],
            "title": "Unsupervised latent tree induction with deep inside-outside recursive auto-encoders",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Brian DuSell",
                "David Chiang."
            ],
            "title": "Learning hierarchical structures with differentiable nondeterministic stacks",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Chris Dyer",
                "Adhiguna Kuncoro",
                "Miguel Ballesteros",
                "Noah A. Smith."
            ],
            "title": "Recurrent neural network grammars",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2016
        },
        {
            "authors": [
                "Edward Grefenstette",
                "Karl Moritz Hermann",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Learning to transduce with unbounded memory",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Michael Hahn."
            ],
            "title": "Theoretical Limitations of SelfAttention in Neural Sequence Models",
            "venue": "Transactions of the Association for Computational Linguistics, 8:156\u2013171.",
            "year": 2020
        },
        {
            "authors": [
                "Marc D. Hauser",
                "Noam Chomsky",
                "W. Tecumseh Fitch"
            ],
            "title": "The faculty of language: What is it, who has it, and how did it evolve? Science, 298(5598):1569\u20131579",
            "year": 2002
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "A structural probe for finding syntax in word representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Jennifer Hu",
                "Jon Gauthier",
                "Peng Qian",
                "Ethan Wilcox",
                "Roger Levy."
            ],
            "title": "A systematic assessment of syntactic generalization in neural language models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Stanislas Dehaene"
            ],
            "title": "Can transformers process",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher"
            ],
            "title": "Pointer sentinel mixture",
            "year": 2017
        },
        {
            "authors": [
                "Hao Peng",
                "Roy Schwartz",
                "Noah A. Smith."
            ],
            "title": "PaLM: A hybrid parser and language model",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Jorge Perez",
                "Pablo Barcelo",
                "Javier Marinkovic."
            ],
            "title": "Attention is turing-complete",
            "venue": "Journal of Machine Learning Research, 22(75):1\u201335.",
            "year": 2021
        },
        {
            "authors": [
                "Peng Qian",
                "Tahira Naseem",
                "Roger Levy",
                "Ram\u00f3n Fernandez Astudillo."
            ],
            "title": "Structural guidance for transformer language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Laurent Sartran",
                "Samuel Barrett",
                "Adhiguna Kuncoro",
                "Milo\u0161 Stanojevi\u0107",
                "Phil Blunsom",
                "Chris Dyer."
            ],
            "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale",
            "venue": "Transactions of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Yikang Shen",
                "Shawn Tan",
                "Alessandro Sordoni",
                "Aaron Courville."
            ],
            "title": "Ordered neurons: Integrating tree structures into recurrent neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Mitchell Stern",
                "Daniel Fried",
                "Dan Klein."
            ],
            "title": "Effective inference for generative neural parsing",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1695\u20131700, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Emma Strubell",
                "Patrick Verga",
                "Daniel Andor",
                "David Weiss",
                "Andrew McCallum."
            ],
            "title": "Linguisticallyinformed self-attention for semantic role labeling",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Sebastian Gehrmann",
                "Yonatan Belinkov",
                "Stuart M Shieber."
            ],
            "title": "Memory-augmented recurrent neural networks can learn generalized dyck languages",
            "venue": "arXiv preprint arXiv:1911.03329.",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "\u0141ukasz Kaiser",
                "Terry Koo",
                "Slav Petrov",
                "Ilya Sutskever",
                "Geoffrey Hinton."
            ],
            "title": "Grammar as a foreign language",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Yaushian Wang",
                "Hung-Yi Lee",
                "Yun-Nung Chen."
            ],
            "title": "Tree transformer: Integrating tree structures into self-attention",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Alex Warstadt",
                "Alicia Parrish",
                "Haokun Liu",
                "Anhad Mohananey",
                "Wei Peng",
                "Sheng-Fu Wang",
                "Samuel R. Bowman."
            ],
            "title": "BLiMP: The benchmark of linguistic minimal pairs for English",
            "venue": "Transactions of the Association for Computational Linguistics, 8:377\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Shunyu Yao",
                "Binghui Peng",
                "Christos Papadimitriou",
                "Karthik Narasimhan."
            ],
            "title": "Self-attention networks can process bounded hierarchical languages",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Shizhuo Dylan Zhang",
                "Curt Tigges",
                "Stella Biderman",
                "Maxim Raginsky",
                "Talia Ringer"
            ],
            "title": "Can transformers learn to solve problems recursively? arXiv preprint arXiv:2305.14699",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens\u2014for instance, learning to \u201cskip\u201d over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks."
        },
        {
            "heading": "1 Introduction",
            "text": "An important property of human language and thought is recursion, which allows us to compose and reason about complex objects in terms of simpler constituents (Hauser et al., 2002). While extensively studied in natural language syntax and semantics, recursion is also a key component of several other aspects of intelligent behaviors including mathematical reasoning, programming, and goaldirected planning. Most recursion-capable systems model recursive processes via a stack memory, which is updated as new computation is performed.\nFor instance, a programming language may implement recursion by maintaining a run-time stack of caller-callee frames, storing intermediate outputs in the stack, and updating the stack as new function calls are made. Similarly, a shift-reduce parser implements recursion through a stack of intermedi-\nate constituents, shifting tokens onto the stack as they are observed, and occasionally reducing stack elements into constituents as they are completed.\nIn contrast, the self-attention mechanism underlying modern neural sequence models has no explicit mechanism to maintain a stack memory as it generates strings, and instead relies on hidden representations to implicitly but imperfectly encode such information (Manning et al., 2020). While this encoding can model bounded recursive structure in formal languages (Yao et al., 2021), it is unclear if it is sufficient for robust syntactic generalization, especially under data-constrained settings.\nIn this work, we show that an explicit stack memory mechanism can improve syntactic generalization in Transformer language models (LMs). We introduce Pushdown Layers1, a drop-in replacement for standard self-attention that augments Transformer LMs with stack memory. This memory is modeled using a stack tape that stores estimated depths of every token in an incremental parse of the observed prefix. The stack tape is updated autoregressively: as new tokens are predicted, Transformers with Pushdown Layers (Pushdown Transformers) synchronously make probabilistic attachment decisions to either \u201cshift\u201d, thus assigning the newly predicted token a depth of 0, or \u201creduce\u201d with one of the constituents in the prefix so far, updating token depths accordingly (see Fig. 1). This stack tape is used to additively and softly modulate the attention of the Transformer over tokens\u2014for instance, Pushdown Layers may guide the LM to only attend to head words of constituents, or skip over reduced constituents by decreasing attention.\nPushdown Transformer LMs are syntactic language models that learn joint probabilities of sequences and parses in terms of individual word predictions and structure-building operations, and can be trained on any text corpus annotated with constituency parses. But unlike other syntactic language models with structural supervision (Vinyals et al., 2015; Choe and Charniak, 2016; Qian et al., 2021; Sartran et al., 2022), Pushdown Layers do not change the output space of the underlying sequence model, and impose no constraints on attention mechanisms\u2014the manner in which Pushdown Layers use syntactic structure for representation building is learnt purely via gradient descent.\nPushdown Transformers obtain strong general-\n1We borrow this term from pushdown automata, which are finite state machines augmented with stacks.\nization improvements over standard Transformer LMs. When trained on depth-bounded Dyck strings and evaluated on deeper Dyck strings, Pushdown Transformers improve performance over baseline LMs by over 25% (Section 4.1). When trained on sentence-level language modeling on the BLLIPLG datasets of Hu et al. (2020), Pushdown Transformers improve syntactic generalization over standard Transformer LMs by 5\u201313 points as well as other joint models of strings and parses such as Qian et al. (2021); Sartran et al. (2022) by 0.3\u2013 4 points (Section 4.2). When trained on a new, 100-million-token dataset of parsed Wikipedia articles we call WIKITREES, Pushdown Transformers match the syntactic generalization of ordinary Transformers with 3\u20135\u00d7 less data. Finally, when Pushdown Layers are inserted into a pre-trained GPT-2 (medium) model and fine-tuned on WIKITREES they yield improvements of 0.3\u20131 points on several GLUE text classification tasks."
        },
        {
            "heading": "2 Background",
            "text": "Multi-Head Self-Attention. Transformer language models (Vaswani et al., 2017) are a class of neural sequence models that use multi-head self-attention to obtain contextualized representations of tokens in a sequence, which are then used to predict the next token. In particular, let x = {x1, x2, . . . , xn} be an input sequence. Let hli \u2208 Rd be the hidden representation of the ith token at the lth attention block. Then, the hidden representation of the ith token is updated as\nhl+1i = FF(O \u00b7 [A1(h l \u2264i), , \u00b7 \u00b7 \u00b7 ,AK(hl\u2264i)]),\n(1)\nwhere O \u2208 Rd\u00d7d is a learnt matrix, FF denotes a feed-forward + residual + layer-norm block, and Ap is the pth self-attention head. Each attention head performs a weighted average over its inputs,\nAp(h l \u2264i) = i\u2211 j=1 \u03b1ijW p valueh l j , (2)\nwhere \u03b1ij is the attention weight assigned to the j th token by the ith token. These attention weights are computed as\n\u03b1ij = softmax[(W p keyh l j) \u22a4W pqueryh l i]. (3)\nEach self-attention head introduces learnt parameters W pkey,W p query,W p value \u2208 Rd/K\u00d7d.\nLimitations of Self-Attention. When trained on text corpora, transformers implicitly encode several aspects of linguistic structure unsupervisedly (Clark et al., 2019; Hewitt and Manning, 2019; Murty et al., 2023). However, there is mounting evidence that recursion, a key feature of human language, remains a challenge. Hahn (2020) shows theoretically that hard-attention cannot model simple recursive structures like 2DYCK (see Section 6 for an extended discussion). Empirically, Lakretz et al. (2022) show that self-attention struggles on center embedding phenomenon, and Zhang et al. (2023) show poor performance on simple recursive tree-traversal problems. We hypothesize that a key reason for poor modeling of recursive structure in self-attention is a lack of an explicit structural inductive bias. One common way to add such an inductive bias is via joint modeling of strings and syntactic structure, which we introduce next.\nSyntactic Language Models. Let y be the ground-truth syntactic parse of x. A long line of work (Vinyals et al., 2015; Dyer et al., 2016; Choe and Charniak, 2016; Qian et al., 2021; Sartran et al., 2022) considers learning joint distributions p(x, y) to incorporate explicit syntactic structure into neural language models, by learning to output a sequence of transition actions,\np(x, y) = p(axy) = \u220f i p(ai | a<i) (4)\nwhere actions ai correspond to both word-level predictions as well as structural actions corresponding to opening and closing of constituents, building up the parse tree in a top-down, left-to-right manner. Recent work explores using Transformers to parameterize these joint distributions. For instance, Qian et al. (2021); Sartran et al. (2022) train Transformer LMs over transition actions (Parsing as Language Modeling or PLM), sometimes with constrained attention heads (PLM-mask), and Transformer Grammars (TG; Sartran et al., 2022) model transition actions with Transformers, also with hard constraints on attention to model shift/reduce actions.\nThese models have several limitations that motivate our proposed approach. First, their outputs are sequences of transition actions that include both text and tree-building operations; as each constituent in a parse tree has an opening and closing transition action, and there are\u2248 n constituents for x, this increases input length by a factor of 3, leading to significant computation and memory over-\nhead. Second, inference in neural models operating on transitions require bespoke decoding procedures that carefully balance tradeoffs between highentropy word-level predictions and low-entropy structural predictions (Stern et al., 2017). Finally, to explicitly bias Transformer computations to mirror the recursive structure of parse trees, some approaches like PLM-mask (Qian et al., 2021) and TGs (Sartran et al., 2022) impose hard constraints on attention patterns. Pushdown Layers provide a softer syntactic bias that is amenable to gradientbased learning, while having broader applicability to phenomena beyond local tree-structuredness, such as topical dependencies, coreference, etc."
        },
        {
            "heading": "3 Pushdown Layers",
            "text": "Transformer LMs with Pushdown Layers are syntactic language models that generate strings while simultaneously building a parse tree over these strings from left to right. This parse tree is built incrementally by tracking the recursive state of every token, which is synchronously updated along with word-level predictions. This recursive state is represented via our stack tape as tree-depths of every prefix token, and updates are realized with a stack. The contents of the stack tape are used to softly modulate attention over prefix tokens via additive offsets to attention logits (Fig. 2)."
        },
        {
            "heading": "3.1 Stack Tape",
            "text": "Like ordinary self-attention, Pushdown Layers take a sequence of hidden states {hlk} as input, and output a sequence {hl+1k }. Additionally, Pushdown Layers use a stack tapeWk \u2208 {0, k}k to simulate a pushdown automaton that performs shift/reduce operations over tokens as they are predicted (Fig. 2). The contents of the stack tape encode recursive state by tracking the depth of each token within reduced constituents in the stack. Concretely, after observing the prefix x\u2264k = {x1, x2, . . . , xk}, Wk[j] = 0 if token xj has not been reduced with any other token, whileWk[j] = p means that xj has appeared in p reduce operations such that the resulting constituent has token xj at depth p\u2014in Fig. 2, the stack tape encodes [1, 1, 0] for the incremental parse [The dog] is.\nUpdating the Stack Tape. As shown in Fig. 2, along with predicting the next word happy, Transformers with Pushdown Layers (Pushdown Transformers) make an attachment decision to update their stack tape. In our running example, this is\ndone by selecting a constituent from the incremental parse [The dog] is happy.\nConcretely, given prefix x<k, Pushdown Transformers predict the next token xk as well as an update to the stack tapeWk\u22121. This is done by selecting a token rk to reduce with, out of candidate tokens {x1, x2, . . . , xk}, via attention over hidden states {hL1 ,hL2 , . . . ,hLk\u22121, h\u0303Lk }, where L is the final layer of the Transformer, and h\u0303Lk is a vector representation for the newly predicted token xk, obtained as h\u0303Lk = MLP(xk,h L k\u22121). This vector attends to all tokens to make a probabilistic attachment decision,\np(rk = j | x<k;Wk\u22121) \u221d{ (hL\u22a4j W \u22a4h\u0303 L k ) if j \u0338= k, shift + reduce\n(h\u0303L\u22a4k W \u22a4h\u0303 L k ) shift only\n(5)\nwhere W \u2208 Rd\u00d7d is a learnt parameter matrix. We use these probabilities to select token rk = argmax p(j | x<k;Wk\u22121) to reduce xk with, and the stack tape is updated accordingly via Algorithm 1. Note that attachment decisions to constituents are made by computing the attachment score for the rightmost token in the constituent. In our running example, the model selects the constituent [The dog] by selecting the word dog, forming the parse [[The dog] [is happy]] and updating the stack tape from [1, 1, 0]\u2192 [2, 2, 2, 2].\nAlgorithm 1: Stack Tape Update Input: Wk\u22121, k, rk, stack Output: Wk, stack\nUpdateStackTape(Wk\u22121, k, rk, stack)\nWk \u2190Wk\u22121 constituent\u2190 [k] if rk == k then\nstack.push(constituent) return\nend while True do\ntop\u2190 stack.pop() // Perform a reduce constituent\u2190 top+ constituent // Update depths in stack tape forall d \u2208 constituent do Wk[d] += 1 end if top == rk then\nbreak end\nend stack.push(constituent)"
        },
        {
            "heading": "3.2 Computing Attention Scores",
            "text": "We map contents of Wk onto a per-layer depth embedding dlkj for every token j \u2208 {0, 1, . . . , k}. These depth embeddings are added to attention keys, resulting in a locally additive modulation to attention scores,\n\u03b1\u0303lkj = softmax ( [hlj + d l kj ] \u22a4W\u22a4keyWqueryh l k ) .\n(6)\nOf course, since these logits are themselves part of a softmax and non-linearities, the overall effect can be arbitrarily non-linear. These modified attention weights are used to compute contextualized vectors using Eq 2 and Eq 1."
        },
        {
            "heading": "3.3 Training and Inference",
            "text": "Training. Given a corpus of strings annotated with parses, we first extract ground-truth values of Wk for every prefix x\u2264k. We also extract groundtruth attachment decisions for xk, given prefix x<k. With these quantities precomputed, we can train Pushdown Transformers in parallel, like standard Transformers. Attachment probabilities (Eq 5) are supervised with ground-truth attachments, along with the standard LM objective, all using hidden\nstates that are contextualized using the Pushdown Layer attention mechanism that uses the precomputed stack tape.\nInference. For any string x and parse y, joint probability p(x, y) factorizes as a product of wordlevel and attachment scores as\np(x, y) = n\u220f k=1 ( p(xk | x<k;Wk\u22121)\u00d7\np(rk | x<k;Wk\u22121) ) . (7)\nWhile computing the full marginal p(x) =\u2211 y p(x, y) is computationally infeasible due to the large space of possible parses, we approximate this by marginalizing over a smaller subset with beam search. Crucially, since our model predicts words and structural actions in parallel rather than sequentially, we do not need to use complex wordsynchronous decoding procedures (Stern et al., 2017) that introduce additional hyperparameters."
        },
        {
            "heading": "3.4 Implementation Details",
            "text": "FLOPs and memory overhead. Consider query and key matrices Q \u2208 Rnd\u00d7d,K \u2208 Rns\u00d7d where nd and ns refer to destination (hidden states attending) and source (hidden states being attended to). Let S \u2208 Rnd\u00d7ns be the (lower-triangular) matrix denoting pre-computed stack tape values for every prefix. For each Pushdown Layer, we use S to index into depth embeddings to obtain D \u2208 Rnd\u00d7ns\u00d7d, which is added to K to obtain KD \u2208 Rnd\u00d7ns\u00d7d. Unlike standard self-attention which multiplies Q and K directly, Pushdown Layers multiply Q (a 2D tensor) with KD (a 3D tensor). This is done by casting Q into a 3D tensor \u2208 Rnd\u00d71\u00d7d and performing a batched matrix multiplication with KD, leading to the same number of operations as standard self-attention 2. However, since Pushdown Layers require storing 3D tensors for keys, this increases memory requirements from O(nd \u00b7 ns + ns \u00b7 d + nd \u00b7 d) to O(nd \u00b7 ns + ns \u00b7 nd \u00b7 d+ nd \u00b7 d). We provide standalone code for implementing a Pushdown Layer block in Appendix D.\nAttending to hidden states with old memory. Pushdown Transformers build parse trees incrementally from left-to-right, and so, depth values of prefix tokens change as new tokens are predicted.\n2We note that standard self-attention is faster in practice due to better GPU memory bandwidth management,\nThus, a token at position i builds its representation based on attending to x\u2264i with a stack tape that may soon become \u201cstale\" due to future transition operations that reduce tokens in x\u2264i with new tokens. As an example, suppose we have the incremental parse [[The dog] [in [the park]]]. Here, the representation for in attends to representations of The, dog and in with depths [1, 1, 0] while the representation for park attends to these representations with updated depths [2, 2, 2]."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Warm-up: Dyck Languages",
            "text": "We train 6 layer LMs with Pushdown Layers (Pushdown-LM) as well as standard LMs on 100k strings sampled from DYCK20,10, the language of well-nested brackets with 20 bracket types and max-nesting depth of 10. To ensure that improvements are not merely due to multi-task learning with an attachment head, base-LM is also trained with an attachment loss in a standard multi-task learning setup. To test generalization, models are provided an input prefix from a separate DYCK language, and evaluated on choosing the correct closing bracket. Specifically, we test generalization to DYCK strings with deeper nesting of 15\u201350, and DYCK strings with longer-range dependencies than seen at training time (measured as the distance to the matching bracket that needs to be closed). From Table 1, we find that Pushdown-LM obtains over 25% accuracy point improvement over standard language models at generalizing to deeper structure, as well as large improvements at generalizing to longer-range dependencies."
        },
        {
            "heading": "4.2 Sentence-Level Language Modeling",
            "text": "Setup. Next, we train 16-layer Pushdown Transformer LMs on the BLLIP-LG dataset of Charniak et al. (2000), with training splits from Hu et al. (2020), and the same pre-processing as Qian et al. (2021). We use the same hyperparameters (model size, dropout, learning rate schedulers) as Sartran et al. (2022). To measure syntactic generalization, we evaluate on BLIMP (Warstadt et al., 2020) and the SG test suites (Hu et al., 2020). In BLIMP, models are provided with a grammatical and ungrammatical sentence, and evaluated on assigning a higher probability to the grammatical sentence. SG test suites consist of an extensive set of handcrafted test cases, covering 6 fine-grained syntactic phenomena. Each test case involves satisfying a specific inequality constraint among surprisal values of various continuations given prefixes, where these inequalities are grounded in theories of incremental language processing\u2014for instance, assigning a higher surprisal to the last verb in The painting that the artist deteriorated painted vs. The painting that the artist painted deteriorated. For BLIMP, we obtain p(x) by approximate marginalization via beam search. Since surprisal values \u2212 log p(xt | x<t) in SG test suites are meant to reflect incremental sentence processing, we perform marginalization based on the beam state at time\nstep t. We fix the beam size at 300.\nResults. We present results on SG test suites in Figure 3. As baselines, we compare against a standard 16 layer Transformer LM and prior structured models (TG, PLM) from Sartran et al. (2022). As expected, all models with an explicit notion of structure have much better syntactic generalization across all test suites. Next, we note that Pushdown-LM, a 16 layer Transformer LM with all self-attention blocks replaced with Pushdown Layers, outperforms prior approaches\u2014PushdownLM beats TG on 4/6 tests and PLM on 3/6 tests with similar performance on licensing. Next, we present results (averaged across 3 seeds) on BLIMP as well as aggregate SG test suite results and perplexity on the BLLIP test set in Table 2. Here, we note that Pushdown-LM achieves better syntactic generalization than prior structured models (including the PLM-mask model from (Qian et al., 2021)) on BLIMP. Finally, we find that PushdownLM achieves slight gains in perplexity compared to Base-LM."
        },
        {
            "heading": "4.3 Language Modeling with WIKITREES",
            "text": "Can Pushdown Layers continue to offer improvements on larger-scale language modeling? We construct WIKITREES, a dataset of over 100 million tokens extracted from Wikipedia Articles (WikiText-\n103; Merity et al. (2017)), parsed automatically using a state-of-the-art neural constituency parser (Kitaev et al., 2019). Typically, LMs trained on web-scale data are given multi-sentence contexts with large window sizes as inputs, and to adapt this to Pushdown-LMs we make a small number of modifications (see Appendix B for details).\nSample-Efficient Generalization. To measure sample efficiency in Pushdown Transformers, we train LMs on [10M, 50M, 100M] tokens from WIKITREES. To ensure stable training under low data regimes, we train a 12 layer GPT2 using the exact configuration and tokenization scheme as GPT2small (Radford et al., 2019), and additionally use dropout to prevent overfitting. For these experiments, we compare Base-LM with an LM where the final 6 self-attention blocks are Pushdown Layers (Pushdown-LM). To measure syntactic generalization, we compute aggregate performance on the SG test suites. From results in Fig. 4, we find that Pushdown-LMs exhibit drastically more sample-efficient syntactic generalization\u2014for instance, syntactic generalization of Pushdown-LM trained on 10M tokens requires over 40M tokens for the Base-LM to surpass.\nFinetuning for text classification. Can Pushdown Layers offer improvements on language understanding tasks, beyond syntactic generalization? To answer this, we perform staged finetuning of GPT2-medium with Pushdown Layers. Specifically, we finetune GPT-2 medium with the final 12 self-attention blocks replaced with Pushdown Layers (Pushdown-GPT2), as a language model on\nWIKITREES. We use this model to obtain parses on 4 text classification tasks: RTE, SST5, MRPC and STS-B from GLUE (Wang et al., 2019a), and use these parses to pre-compute the stack tape for every token. Then, in a second finetuning step, Pushdown-GPT2 is trained to perform text classification over these datasets by reducing each task into language modeling via prompting (See Appendix A for prompt details). As a comparison, we also perform the same staged finetuning for the standard GPT2-medium architecture. We report averaged results across 3 seeds in Table 3. We find that Pushdown Layers offer improvements on 3 out of 4 text classification tasks."
        },
        {
            "heading": "5 Analysis",
            "text": "For all analyses, we use the 16 layer Pushdown-LM trained on BLLIP-LG from Section 4.2.\nParsing. Since Pushdown-LM is a syntactic language model, we obtain parses via beam search (beam size = 300) to approximately recover the most likely parse y\u2217 = argmaxy p(x, y) under our\nmodel. However, since this parse is (a) unlabeled and (b) binarized, we perform an unlabeled F1 evaluation (using EVALB; Collins, 1997) over binarized ground-truth parses from the PTB test set. We also remove instances consisting of unknown words for our model, since our model is trained without any UNK tokens, giving us 2335 out of 2416 sentences. We compare our model against Kitaev et al. (2019), the parser that was used to annotate training data for Pushdown-LM. We also present unlabeled F1 on the auto-parsed BLLIP-LG test set. From results in Table 4, we note that our model achieves a very competitive unlabeled F1 score of 95.3, outperforming the official implementation of Kitaev et al. (2019)3. We also find that our model obtains a high F1 score of 97.3 on the BLLIP-LG test set.\nCase Study: Analyzing attention patterns on subject-verb agreement tasks. We consider the 3 Subject-Verb agreement tasks (Marvin and Linzen, 2018) from the SG test suites. On these\n3We use the benepar_en_large model from https:// github.com/nikitakit/self-attentive-parser which reports a score of 96.29 on the full PTB test set, while we obtain 95.66 (labeled F1, using the standard EVALB script).\ntasks, models are presented with a prefix consisting of a main subject and a distractor embedded subject, where these items conflict in number. The objective is to assign a higher logprob to the verb that agrees with the main subject rather than the distractor subject. For instance, for prefix The author that hurt the senators, the model must assign a higher probability to is than are.\nFrom Fig. 3, we find that Pushdown-LM significantly outperforms other models with close to 80% accuracy while Base-LM achieves less than 60% accuracy. To understand how Pushdown Layers modulate attention on these examples, we obtain attention scores over all prefix tokens (averaged across all layers). We present the average attention assigned to the distractor token for both Pushdown-LM and Base-LM in Fig. 5 where we observe that Pushdown-LM pulls attention away from the distractor noun, allowing it to predict the correct verb. Finally, we plot some (averaged) attention heatmaps in Fig. 6."
        },
        {
            "heading": "6 Other Related Work",
            "text": "While recursive structure is fundamental to natural language, modeling such structure is difficult for self-attention. Hahn (2020) considers DYCK, the simplest formal language with recursive structure, proving that hard attention cannot recognize DYCK and soft attention cannot recognize DYCK with low cross-entropy. In practice, we find that even simpler languages like PARITY are challenging for encoder-only Transformers (Chiang and Cholak, 2022; Bhattamishra et al., 2020). On the other hand, Transformers with decoders have been shown to be Turing-complete (Perez et al., 2021), but these constructions rely on the impractical assumption of running the decoder for an unbounded number of steps. In practice, we find that Transformer LMs struggle with generalization beyond regular languages and tend to learn shortcuts instead (Deletang et al., 2023; Liu et al., 2023).\nGiven these limitations, there is significant interest in inductive biases that encourage recursive structure in Transformers. One line of work considers constraining self-attention patterns according to syntactic parses (Strubell et al., 2018; Wang et al., 2019b; Peng et al., 2019; Deshpande and Narasimhan, 2020, among others). A second line of work adds structure to language modeling by learning joint probabilistic modeling of structure and strings (Chelba, 1997; Mirowski and Vlachos,\nPushdown-LM Base-LM\nPushdown-LM Base-LM\nFigure 6: Given a prefix containing a main noun and a distractor noun, Pushdown-LM pulls attention away from the distractor (here senator), helping the model predict the verb with the correct number. These attention maps average across all the instances in the number_src test of SG test suites, and we show the attention over all prefix tokens when the main verb is predicted\n2015; Choe and Charniak, 2016; Dyer et al., 2016, among others). Both of these ideas are combined in recent work of Qian et al. (2021); Sartran et al. (2022), that proposes joint string, parse Transformer language models with constrained attention patterns. While Pushdown Layers are also in this modeling tradition, we do so without operating on long transition actions, and enforce structural constraints via gradient based learning.\nA separate line of work proposes neural networks augmented with structured memory like stacks (Das et al., 1992; Grefenstette et al., 2015; Joulin and Mikolov, 2015; DuSell and Chiang, 2022) or random access memories (Kurach et al., 2015). Such augmented neural networks are vastly better at algorithmic generalization and learning recursive structure (Suzgun et al., 2019; Deletang et al., 2023). Our work is the first that designs a structured memory (the stack-tape) for Transformers, that is updated just like stacks in a shift/reduce\nmanner, but unlike prior work, the specific design of Pushdown Layers makes training parallelizable.\nFinally, there have been several efforts to add syntactic inductive biases into sequence models (typically RNNs) that can acquire and use parse structures in an unsupervised manner (Bowman et al., 2016; Shen et al., 2019; Drozdov et al., 2019; Kim et al., 2019, among others). We leave unsupervised training of Pushdown Transformers for future work."
        },
        {
            "heading": "7 Conclusion",
            "text": "We propose Pushdown Layers, a new kind of selfattention that augments Transformer language models with a stack based memory. Pushdown Layers enable auto-regressive Transformers to softly bias attention towards a recursive syntactic computation, through an updatable stack-tape that stores token depths in an incremental parse. When trained on synthetic and natural languages, we find that Transformer LMs with Pushdown Layers achieve better generalization to deep recursive structure, as well as better and more sample-efficient syntactic generalization. When pre-trained LMs are finetuned with Pushdown Layers, we obtain improvements on some GLUE tasks."
        },
        {
            "heading": "8 Reproducibility",
            "text": "Code and data for these experiments is available at https://github.com/MurtyShikhar/Pushdown-Layers.\nLimitations\nPushdown Layers require constituency-parse annotated datasets, which may not be available for many languages due to a lack of high performing off-theshelf constituency parsers. This also limits applicability to domains beyond natural and synthetic languages, such as algorithmic reasoning. Finally, Pushdown Layers can only be applied to languages with constituency structure, and our experiments are limited to English."
        },
        {
            "heading": "Acknowledgements",
            "text": "SM was funded by a gift from Apple Inc. CM is a fellow in the CIFAR Learning in Machines and Brains program. PS and JA are funded by Project CETI via grants from The Audacious Project: a collaborative funding initiative housed at TED. We thank John Hewitt, Sidd Karamcheti and R\u00f3bert Csord\u00e1s for feedback and discussions."
        },
        {
            "heading": "A Model Hyperparameters",
            "text": "BLLIP. All our hyperparameters for BLLIP are borrowed from the 16-layer Transformer LM used in Sartran et al. (2022). This includes model hyperparameters (hidden state dimension, number of attention heads, number of layers), dropout (input dropout, output dropout, attention dropout), and learning rate schedulers. We train for 300k steps, evaluating every 3k steps and early stop based on validation set perplexity.\nWikiTrees. For experiments on WikiTrees, we use the same model hyperparameters as GPT2small, and a context window of 512. We train with a batch size of 480, and train till validation loss stops decreasing, with a learning rate that linearly warms up from 0 to 6e-4 over 200 iterations, followed by a cosine learning rate scheduler. For sample efficiency experiments, we add dropout of 0.2 to prevent overfitting.\nGPT2-medium finetuning. We use a batch size of 256, and a constant learning rate of 3e-5. We early stop based on validation set performance, and report average of 3 runs. To convert text classification tasks into language modeling, we use the following prompts:\n\u2022 RTE: Premise: {p}. Hypothesis: {h}. Label: {l}, given a premise, hypothesis pair (p,h) with label l mapped into {Yes, No}.\n\u2022 MRPC: Given the sentence pair (s1, s2), we create a prompt Sentence1: {s1}. Sentence2: {s2}. Label: {l}. where l \u2208 {0, 1}.\n\u2022 SST5: Sentence: {s}. Sentiment: {l} for an input sentence s with label l.\n\u2022 STS-B: Given the sentence pair (s1, s2), we create a prompt Sentence1: {s1}. Sentence2: {s2}, and use the final hidden state to featurize a linear regressor, trained jointly with the LM."
        },
        {
            "heading": "B Training Pushdown-LM with context-windows",
            "text": "In standard LMs, context windows for training are arbitrary offsets into the entire corpora\u2014a window might start in the middle of some sentence. Because Pushdown-LMs always start with the stack state initialized as all 0s and make attachments only to stack tape contents, a Pushdown-LM cannot start in the middle of a sentence without the stack tape\nappropriately initialized. We get around this by simply sampling these context windows to always start at sentence boundaries. We also prepend a special token ROOT before the start of every sentence such that the attachment decision of the final word is made to this ROOT token."
        },
        {
            "heading": "C Parsing with Pushdown-LM.",
            "text": "Since the BLLIP-LG trained Pushdown-LM operates over sub-word tokens, parses produced by this model have subwords as leaf nodes. We process this by recursively merging leaf siblings that are part of the same word. For instance, given the bracketing (ab, (ra, (ca, dabra))), we recursively merge these to get a single node abracadabra. This procedure deterministically converts the parse over subwords into a parse tree over words.\nD Implementation details: Pseudocode for implementing Pushdown Layers\nSee Fig. 7 and Fig. 8 for reference implementations of Pushdown Layers and the attachment head."
        }
    ],
    "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
    "year": 2023
}