{
    "abstractText": "Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs)\u2014languages for which NLP research is particularly far behind in meeting user needs\u2014 it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarcedata scenario rather than zero-shot; its focus on user-centric tasks\u2014tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario is most realistic. XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility. We create new datasets for OCR, autocomplete, question answering, semantic parsing, and transliteration, and build on and refine existing datasets for other tasks. XTREME-UP provides a methodology for evaluating many modeling scenarios including text-only, multimodal (vision, audio, and text), supervised parameter tuning, and in-context learning.1 We evaluate commonly used models on the benchmark.2",
    "authors": [
        {
            "affiliations": [],
            "name": "Sebastian Ruder"
        },
        {
            "affiliations": [],
            "name": "Jonathan H. Clark"
        },
        {
            "affiliations": [],
            "name": "Alexander Gutkin"
        },
        {
            "affiliations": [],
            "name": "Mihir Kale"
        },
        {
            "affiliations": [],
            "name": "Min Ma"
        },
        {
            "affiliations": [],
            "name": "Massimo Nicosia"
        },
        {
            "affiliations": [],
            "name": "Shruti Rijhwani"
        },
        {
            "affiliations": [],
            "name": "Parker Riley"
        },
        {
            "affiliations": [],
            "name": "Jean-Michel A. Sarr"
        },
        {
            "affiliations": [],
            "name": "Xinyi Wang"
        },
        {
            "affiliations": [],
            "name": "John Wieting"
        },
        {
            "affiliations": [],
            "name": "Nitish Gupta"
        },
        {
            "affiliations": [],
            "name": "Anna Katanova"
        },
        {
            "affiliations": [],
            "name": "Christo Kirov"
        },
        {
            "affiliations": [],
            "name": "Dana L. Dickinson"
        },
        {
            "affiliations": [],
            "name": "Brian Roark"
        },
        {
            "affiliations": [],
            "name": "Bidisha Samanta"
        },
        {
            "affiliations": [],
            "name": "Connie Tao"
        },
        {
            "affiliations": [],
            "name": "David I. Adelani"
        },
        {
            "affiliations": [],
            "name": "Vera Axelrod"
        },
        {
            "affiliations": [],
            "name": "Isaac Caswell"
        },
        {
            "affiliations": [],
            "name": "Colin Cherry"
        },
        {
            "affiliations": [],
            "name": "Dan Garrette"
        },
        {
            "affiliations": [],
            "name": "Reeve Ingle"
        },
        {
            "affiliations": [],
            "name": "Melvin Johnson"
        },
        {
            "affiliations": [],
            "name": "Dmitry Panteleev"
        },
        {
            "affiliations": [],
            "name": "Partha Talukdar"
        }
    ],
    "id": "SP:7e2d10babf56f83a904335258bdaaac6e024e283",
    "references": [
        {
            "authors": [
                "Mokono",
                "Ignatius Ezeani",
                "Chiamaka Chukwuneke",
                "Mofetoluwa Oluwaseun Adeyemi",
                "Gilles Quentin Hacheme",
                "Idris Abdulmumin",
                "Odunayo Ogundepo",
                "Oreen Yousuf",
                "Tatiana Moteu",
                "Dietrich Klakow"
            ],
            "title": "MasakhaNER 2.0: Africa-centric transfer",
            "year": 2022
        },
        {
            "authors": [
                "Wolde",
                "Abdoulaye Faye",
                "Blessing Sibanda",
                "Orevaoghene Ahia",
                "Bonaventure F.P. Dossou",
                "Kelechi Ogueji",
                "Thierno Ibrahima DIOP",
                "Abdoulaye Diallo",
                "Adewale Akinfaderin",
                "Tendai Marengereke",
                "Salomey Osei"
            ],
            "title": "MasakhaNER: Named entity",
            "year": 2021
        },
        {
            "authors": [
                "Denis Anson",
                "Penni Moist",
                "Mary Przywara",
                "Heather Wells",
                "Heather Saylor",
                "Hantz Maxime."
            ],
            "title": "The effects of word completion and word prediction on typing rates using on-screen keyboards",
            "venue": "Assistive Technology, 18(2):146\u2013154.",
            "year": 2006
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama",
                "Gorka Labaka",
                "Eneko Agirre"
            ],
            "title": "A call",
            "year": 2020
        },
        {
            "authors": [
                "Akari Asai",
                "Jungo Kasai",
                "Jonathan Clark",
                "Kenton Lee",
                "Eunsol Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "XOR QA: Cross-lingual open-retrieval question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Akari Asai",
                "Sneha Kudugunta",
                "Xinyan Velocity Yu",
                "Terra Blevins",
                "Hila Gonen",
                "Machel Reid",
                "Yulia Tsvetkov",
                "Sebastian Ruder",
                "Hannaneh Hajishirzi"
            ],
            "title": "BUFFET: Benchmarking large language models for few-shot cross-lingual transfer",
            "year": 2023
        },
        {
            "authors": [
                "Maxim Krikun",
                "Pidong Wang",
                "Alexander Gutkin",
                "Apurva Shah",
                "Yanping Huang",
                "Zhifeng Chen",
                "Yonghui Wu",
                "Macduff Hughes."
            ],
            "title": "Building machine translation systems for the next thousand languages",
            "venue": "arXiv preprint arXiv:2205.03983.",
            "year": 2022
        },
        {
            "authors": [
                "Youssef Bassil",
                "Mohammad Alwani."
            ],
            "title": "PostEditing Error Correction Algorithm For Speech Recognition using Bing Spelling Suggestion",
            "venue": "(IJACSA) International Journal of Advanced Computer Science and Applications, 3(2):95\u2013101.",
            "year": 2012
        },
        {
            "authors": [
                "Steven Bird."
            ],
            "title": "Local languages, third spaces, and other high-resource scenarios",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7817\u20137829, Dublin, Ireland. Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Damian Blasi",
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "Systematic inequalities in language technology performance across the world\u2019s languages",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Isaac Caswell",
                "Theresa Breiner",
                "Daan van Esch",
                "Ankur Bapna."
            ],
            "title": "Language ID in the wild: Unexpected challenges on the path to a thousandlanguage web text corpus",
            "venue": "Proceedings of the 28th International Conference on Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Nanxin Chen"
            ],
            "title": "Maestro-U: Leveraging joint speech-text representation learning for zero supervised speech ASR",
            "venue": "In Proceedings of 2022 IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "PaLM: Scaling language modeling with Pathways",
            "venue": "arXiv preprint arXiv:2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Zhou",
                "Quoc V. Le",
                "Jason Wei."
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416.",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Eunsol Choi",
                "Michael Collins",
                "Dan Garrette",
                "Tom Kwiatkowski",
                "Vitaly Nikolaev",
                "Jennimaria Palomaki."
            ],
            "title": "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
            "venue": "Transactions of the",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Min Ma",
                "Simran Khanuja",
                "Yu Zhang",
                "Vera Axelrod",
                "Siddharth Dalmia",
                "Jason Riesa",
                "Clara Rivera",
                "Ankur Bapna."
            ],
            "title": "FLEURS: Fewshot learning evaluation of universal representations of speech",
            "venue": "2022 IEEE Spoken Language Technol-",
            "year": 2023
        },
        {
            "authors": [
                "Marie-Catherine de Marneffe",
                "Christopher D. Manning",
                "Joakim Nivre",
                "Daniel Zeman."
            ],
            "title": "Universal Dependencies",
            "venue": "Computational Linguistics, 47(2):255\u2013308.",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Dhakal",
                "Anna Maria Feit",
                "Per Ola Kristensson",
                "Antti Oulasvirta."
            ],
            "title": "Observations on typing from 136 million keystrokes",
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 1\u201312, Montr\u00e9al, Canada. Associ-",
            "year": 2018
        },
        {
            "authors": [
                "Bosheng Ding",
                "Junjie Hu",
                "Lidong Bing",
                "Mahani Aljunied",
                "Shafiq Joty",
                "Luo Si",
                "Chunyan Miao."
            ],
            "title": "GlobalWoZ: Globalizing MultiWoZ to develop multilingual task-oriented dialogue systems",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Rui Dong",
                "David Smith."
            ],
            "title": "Multi-input attention for unsupervised OCR correction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2363\u20132372, Melbourne, Australia. As-",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Nathanael Sch\u00e4rli",
                "Ekin Aky\u00fcrek",
                "Nathan Scales",
                "Xinying Song",
                "Xinyun Chen",
                "Olivier Bousquet",
                "Denny Zhou."
            ],
            "title": "Compositional semantic parsing with large language models",
            "venue": "Proceedings of the Eleventh International Confer-",
            "year": 2023
        },
        {
            "authors": [
                "Jack FitzGerald",
                "Christopher Hench",
                "Charith Peris",
                "Kay Rottmann."
            ],
            "title": "Massively multilingual natural language understanding 2022 (MMNLU-22) workshop and competition",
            "venue": "Proceedings of the Massively Multilingual Natural Language Un-",
            "year": 2022
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Ivan Vuli\u0107."
            ],
            "title": "Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
            "year": 2021
        },
        {
            "authors": [
                "Rahul Goel",
                "Waleed Ammar",
                "Aditya Gupta",
                "Siddharth Vashishtha",
                "Motoki Sano",
                "Faiz Surani",
                "Max Chang",
                "HyunJeong Choe",
                "David Greene",
                "Kyle He",
                "Rattima Nitisaroj",
                "Anna Trukhina",
                "Shachi Paul",
                "Pararth Shah",
                "Rushin Shah",
                "Zhou Yu"
            ],
            "title": "PRESTO: A mul",
            "year": 2023
        },
        {
            "authors": [
                "Naman Goyal",
                "Cynthia Gao",
                "Vishrav Chaudhary",
                "PengJen Chen",
                "Guillaume Wenzek",
                "Da Ju",
                "Sanjana Krishnan",
                "Marc\u2019Aurelio Ranzato",
                "Francisco Guzm\u00e1n",
                "Angela Fan"
            ],
            "title": "The Flores-101 evaluation benchmark for low-resource and multilingual",
            "year": 2022
        },
        {
            "authors": [
                "Barry Haddow",
                "Rachel Bawden",
                "Antonio Valerio Miceli Barone",
                "Jind\u0159ich Helcl",
                "Alexandra Birch."
            ],
            "title": "Survey of low-resource machine translation",
            "venue": "Computational Linguistics, 48(3):673\u2013732.",
            "year": 2022
        },
        {
            "authors": [
                "Tahmid Hasan",
                "Abhik Bhattacharjee",
                "Md. Saiful Islam",
                "Kazi Mubasshir",
                "Yuan-Fang Li",
                "Yong-Bin Kang",
                "M. Sohel Rahman",
                "Rifat Shahriyar."
            ],
            "title": "XLsum: Large-scale multilingual abstractive summarization for 44 languages",
            "venue": "Findings of the Associ-",
            "year": 2021
        },
        {
            "authors": [
                "Michael A. Hedderich",
                "David Adelani",
                "Dawei Zhu",
                "Jesujoba Alabi",
                "Udia Markus",
                "Dietrich Klakow."
            ],
            "title": "Transfer learning and distant supervision for multilingual transformer models: A study on African languages",
            "venue": "Proceedings of the 2020 Con-",
            "year": 2020
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Oana Ignat",
                "Jean Maillard",
                "Vishrav Chaudhary",
                "Francisco Guzm\u00e1n."
            ],
            "title": "OCR improves machine translation for low-resource languages",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1164\u20131174, Dublin, Ireland. As-",
            "year": 2022
        },
        {
            "authors": [
                "ISO."
            ],
            "title": "ISO 15924: Codes for the representation of names of scripts",
            "venue": "International Organization for Standardization, Geneva, Switzerland.",
            "year": 2004
        },
        {
            "authors": [
                "Paul Jaccard."
            ],
            "title": "Nouvelles recherches sur la distribution florale",
            "venue": "Bulletin de la Societe Vaudoise des Sciences Naturelles, 44(163):223\u2013270. In French.",
            "year": 1908
        },
        {
            "authors": [
                "Cibu Johny",
                "Lawrence Wolf-Sonkin",
                "Alexander Gutkin",
                "Brian Roark."
            ],
            "title": "Finite-state script normalization and processing utilities: The Nisaba Brahmic library",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury"
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP",
            "year": 2020
        },
        {
            "authors": [
                "Ahuja Kabir",
                "Diddee Harshita",
                "Hada Rishav",
                "Ochieng Millicent",
                "Ramesh Krithika",
                "Jain Prachi",
                "Nambi Akshay",
                "Ganu Tanuja",
                "Segal Sameer",
                "Axmed Maxamed",
                "Bali Kalika",
                "Sitaram Sunayana."
            ],
            "title": "MEGA: Multilingual evaluation of generative AI",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Simran Khanuja",
                "Sebastian Ruder",
                "Partha Talukdar."
            ],
            "title": "Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A Case Study for Indian Languages",
            "venue": "EACL 2023 Findings, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Shreya Khare",
                "Ashish R Mittal",
                "Anuj Diwan",
                "Sunita Sarawagi",
                "Preethi Jyothi",
                "Samarth Bharadwaj."
            ],
            "title": "Low resource ASR: The surprising effectiveness of high resource transliteration",
            "venue": "Proceedings of Interspeech, pages 1529\u20131533, Brno, Czechia. In-",
            "year": 2021
        },
        {
            "authors": [
                "Stella Biderman",
                "Alessia Battisti",
                "Ahmed Baruwa",
                "Ankur Bapna",
                "Pallavi Baljekar",
                "Israel Abebe Azime",
                "Ayodele Awokoya",
                "Duygu Ataman",
                "Orevaoghene Ahia",
                "Oghenefego Ahia",
                "Sweta Agrawal",
                "Mofetoluwa Adeyemi"
            ],
            "title": "Quality at a glance",
            "year": 2022
        },
        {
            "authors": [
                "Sameer Kumar",
                "Victor Bittorf",
                "Dehao Chen",
                "Chiachen Chou",
                "Blake Hechtman",
                "HyoukJoong Lee",
                "Naveen Kumar",
                "Peter Mattson",
                "Shibo Wang",
                "Tao Wang",
                "Yuanzhong Xu",
                "Zongwei Zhou"
            ],
            "title": "Scale MLPerf-0.6 models on Google TPU-v3 pods",
            "year": 2019
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Haoran Li",
                "Abhinav Arora",
                "Shuohui Chen",
                "Anchit Gupta",
                "Sonal Gupta",
                "Yashar Mehdad."
            ],
            "title": "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Jing Li",
                "Aixin Sun",
                "Jianglei Han",
                "Chenliang Li."
            ],
            "title": "A survey on deep learning for named entity recognition",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 34(1):50\u201370.",
            "year": 2020
        },
        {
            "authors": [
                "nie Wu",
                "Shuguang Liu",
                "Fan Yang",
                "Daniel Campos",
                "Rangan Majumder",
                "Ming Zhou"
            ],
            "title": "XGLUE: A new benchmark dataset for cross-lingual pretraining, understanding and generation",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Genta Indra Winata",
                "Peng Xu",
                "Feijun Jiang",
                "Yuxiang Hu",
                "Chen Shi",
                "Pascale Fung."
            ],
            "title": "BiToD: A bilingual multidomain dataset for task-oriented dialogue modeling",
            "venue": "Proceedings of the 35th Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Olga Majewska",
                "Evgeniia Razumovskaia",
                "Edoardo Maria Ponti",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "Cross-lingual dialogue dataset creation via outline-based generation",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Sabrina J. Mielke"
            ],
            "title": "Can you compare perplexity across different segmentations? Available in: http: //sjmielke.com/comparing-perplexities.htm",
            "venue": "Blog post",
            "year": 2019
        },
        {
            "authors": [
                "Verginica Barbu Mititelu",
                "Maria Mitrofan."
            ],
            "title": "The Romanian medical treebank \u2013 SiMoNERo",
            "venue": "Proceedings of the The 15th Edition of the International Conference on Linguistic Resources and Tools",
            "year": 2020
        },
        {
            "authors": [
                "Nikita Moghe",
                "Evgeniia Razumovskaia",
                "Liane Guillou",
                "Ivan Vuli\u0107",
                "Anna Korhonen",
                "Alexandra Birch."
            ],
            "title": "MULTI3NLU++: A multilingual, multiintent, multi-domain dataset for natural language understanding in task-oriented dialogue",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Peya Mowar",
                "Tanuja Ganu",
                "Saikat Guha."
            ],
            "title": "Towards optimizing OCR for accessibility",
            "venue": "arXiv preprint arXiv:2206.10254. Extended abstract for poster session at Accessibility, Vision, and Autonomy Meet (CVPR 2022 Workshop).",
            "year": 2022
        },
        {
            "authors": [
                "Bernard Opoku",
                "Steven Arthur."
            ],
            "title": "AfriSenti: A Twitter sentiment analysis benchmark for African languages",
            "venue": "arXiv preprint arXiv:2302.08956.",
            "year": 2023
        },
        {
            "authors": [
                "Nibal Nayef",
                "Fei Yin",
                "Imen Bizid",
                "Hyunsoo Choi",
                "Yuan Feng",
                "Dimosthenis Karatzas",
                "Zhenbo Luo",
                "Umapada Pal",
                "Christophe Rigaud",
                "Joseph Chazalon"
            ],
            "title": "ICDAR2017 robust reading challenge on multi-lingual scene text detection and script",
            "year": 2017
        },
        {
            "authors": [
                "Onyefuluchi",
                "Chris Chinenye Emezue",
                "Bonaventure F.P. Dossou",
                "Blessing Sibanda",
                "Blessing Bassey",
                "Ayodele Olabiyi",
                "Arshath Ramkilowan",
                "Alp \u00d6ktem",
                "Adewale Akinfaderin",
                "Abdallah Bashir"
            ],
            "title": "Participatory research for low-resourced machine",
            "year": 2020
        },
        {
            "authors": [
                "Massimo Nicosia",
                "Francesco Piccinno."
            ],
            "title": "Bytelevel massively multilingual semantic parsing",
            "venue": "Proceedings of the Massively Multilingual Natural Language Understanding Workshop (MMNLU-22), pages 25\u201334, Abu Dhabi, United Arab Emirates (Hy-",
            "year": 2022
        },
        {
            "authors": [
                "Joakim Nivre",
                "Marie-Catherine de Marneffe",
                "Filip Ginter",
                "Jan Haji\u010d",
                "Christopher D. Manning",
                "Sampo Pyysalo",
                "Sebastian Schuster",
                "Francis Tyers",
                "Daniel Zeman"
            ],
            "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
            "year": 2020
        },
        {
            "authors": [
                "Rubungo Andre Niyongabo",
                "Qu Hong",
                "Julia Kreutzer",
                "Li Huang."
            ],
            "title": "KINNEWS and KIRNEWS: Benchmarking cross-lingual text classification for Kinyarwanda and Kirundi",
            "venue": "Proceedings of the 28th International Conference on Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Nordhoff",
                "Harald Hammarstr\u00f6m."
            ],
            "title": "Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources",
            "venue": "Proceedings of the First International Workshop on Linked Science 2011. In conjunction with the In-",
            "year": 2011
        },
        {
            "authors": [
                "Addison Phillips",
                "Mark Davis."
            ],
            "title": "BCP 47 \u2013 Tags for Identifying Languages",
            "venue": "IETF Trust. RFC 5646.",
            "year": 2009
        },
        {
            "authors": [
                "Reiner Pope",
                "Sholto Douglas",
                "Aakanksha Chowdhery",
                "Jacob Devlin",
                "James Bradbury",
                "Anselm Levskaya",
                "Jonathan Heek",
                "Kefan Xiao",
                "Shivani Agrawal",
                "Jeff Dean."
            ],
            "title": "Efficiently scaling transformer inference",
            "venue": "arXiv preprint arXiv:2211.05102.",
            "year": 2022
        },
        {
            "authors": [
                "Maja Popovi\u0107."
            ],
            "title": "chrF: character n-gram F-score for automatic MT evaluation",
            "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for SQuAD",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Christophe Rigaud",
                "Antoine Doucet",
                "Micka\u00ebl Coustaty",
                "Jean-Philippe Moreux."
            ],
            "title": "ICDAR 2019 competition on post-OCR text correction",
            "venue": "2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1588\u20131593, Sydney,",
            "year": 2019
        },
        {
            "authors": [
                "Shruti Rijhwani",
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "OCR Post Correction for Endangered Language Texts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5931\u20135942, On-",
            "year": 2020
        },
        {
            "authors": [
                "Shruti Rijhwani",
                "Daisy Rosenblum",
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "Lexically aware semi-supervised learning for OCR postcorrection",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1285\u20131302.",
            "year": 2021
        },
        {
            "authors": [
                "Brian Roark",
                "Lawrence Wolf-Sonkin",
                "Christo Kirov",
                "Sabrina J. Mielke",
                "Cibu Johny",
                "Isin Demirsahin",
                "Keith Hall."
            ],
            "title": "Processing South Asian languages written in the Latin script: the Dakshina dataset",
            "venue": "Proceedings of the Twelfth Language Resources",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Noah Constant",
                "Jan Botha",
                "Aditya Siddhant",
                "Orhan Firat",
                "Jinlan Fu",
                "Pengfei Liu",
                "Junjie Hu",
                "Dan Garrette",
                "Graham Neubig",
                "Melvin Johnson."
            ],
            "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Amarjot Singh",
                "Ketan Bacchuwar",
                "Akshay Bhasin."
            ],
            "title": "A survey of OCR applications",
            "venue": "International Journal of Machine Learning and Computing, 2(3):314\u2013318.",
            "year": 2012
        },
        {
            "authors": [
                "Martin Sundermeyer",
                "Ralf Schl\u00fcter",
                "Hermann Ney."
            ],
            "title": "LSTM neural networks for language modeling",
            "venue": "Proceedings of Interspeech, pages 194\u2013197, Portland, OR, USA. International Speech Communication Association.",
            "year": 2012
        },
        {
            "authors": [
                "Cynthia Tam",
                "David Wells."
            ],
            "title": "Evaluating the benefits of displaying word prediction lists on a personal digital assistant at the keyboard level",
            "venue": "Assistive Technology, 21(3):105\u2013114.",
            "year": 2009
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
            "year": 2003
        },
        {
            "authors": [
                "Hans H. Wellisch."
            ],
            "title": "The Conversion of Scripts: Its Nature, History, and Utilization",
            "venue": "Information Sciences Series. John Wiley & Sons, New York.",
            "year": 1978
        },
        {
            "authors": [
                "Ken Whistler."
            ],
            "title": "Unicode normalization forms",
            "venue": "Technical Report TR15-51, Unicode Consortium. Version 14.0.0.",
            "year": 2021
        },
        {
            "authors": [
                "Ruder."
            ],
            "title": "NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2023), Dubrovnik, Croatia. Associa-",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Aditya Barua",
                "Noah Constant",
                "Rami AlRfou",
                "Sharan Narang",
                "Mihir Kale",
                "Adam Roberts",
                "Colin Raffel."
            ],
            "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
            "venue": "Transactions of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Mengjie Zhao",
                "Yi Zhu",
                "Ehsan Shareghi",
                "Ivan Vuli\u0107",
                "Roi Reichart",
                "Anna Korhonen",
                "Hinrich Sch\u00fctze."
            ],
            "title": "A closer look at few-shot crosslingual transfer: The choice of shots matters",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning (ICML), volume 139, pages",
            "year": 2021
        },
        {
            "authors": [
                "Roark"
            ],
            "title": "Wikipedia sentences written in the native scripts of the 12 languages were human-romanized by native speakers, resulting in parallel sentences in the native and Latin scripts.19 Two 10,000 sentence additions",
            "year": 2020
        },
        {
            "authors": [
                "Wikivoyage"
            ],
            "title": "Dataset Creation Details of the creation of the original dataset are available in the original publication",
            "venue": "(Goyal et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Xue"
            ],
            "title": "2022) in their base and large configurations on the multilingual training data we collected. Table 12 contains the Exact Match accuracies of a multilingual model trained on data from all languages but the code-switched sets",
            "year": 2022
        },
        {
            "authors": [
                "Piccinno"
            ],
            "title": "mT5 to catch up with ByT5 at larger sizes. E In-context learning examples We show in-context learning examples for a selection of tasks",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The development of natural language processing (NLP) technology that serves most of world\u2019s languages is hindered by the stark lack of data for most languages (Joshi et al., 2020). While there is increasing interest in developing datasets and models for under-represented languages (ULs), existing\n\u2217Equal contribution. We list detailed contributions in \u00a7A. 1Our results in \u00a74 indicate that few-shot in-context learning is less effective than fine-tuning on 100s of examples for ULs. We advocate for comparing such approaches directly as the community explores XTREME-UP.\n2https://github.com/google-research/xtreme-up\ndatasets are often informed by established research directions in the NLP community (de Marneffe et al., 2021). While linguistic tasks such as syntactic parsing have become less practically relevant (Glava\u0161 and Vulic\u0301, 2021), other impactful capabilities such as question answering or virtual assistants (Asai et al., 2021), often depend on ancillary technologies such as language ID, data filtering, automatic speech recognition (ASR), or optical character recognition (OCR) that are typically underperforming or unavailable for ULs (Caswell et al., 2020; Bapna et al., 2022; Kreutzer et al., 2022; Rijhwani et al., 2021; Khare et al., 2021). As a result, speakers of ULs are unable to reap the benefits, even if the development of models is successful.\nIn order to make progress on NLP for ULs, we should thus focus on evaluating models on tasks that are most likely to benefit speakers of those languages.3 To this end, we propose XTREME-UP (Under-Represented and User-Centric with Paucal4 Data), a benchmark focusing on evaluation of multilingual models on user-centric tasks in a scarce-\n3Speakers of ULs have needs ranging from standard NLP technology to language documentation and revitalization (Bird, 2022). Our focus is on standardized, institutional, and contact languages including dialects and non-standard language varieties spoken by large speaker populations.\n4We borrow the term paucal\u2014meaning few\u2014from linguistics, to emphasize the scarce-data nature of XTREME-UP.\ndata setting. We focus on tasks that technology users encounter in their daily lives: i) information access tasks reflecting generally useful NLP capabilities; and ii) input/output tasks that enable other technologies. We show the corresponding tasks and their role in interactions with language technology in Figure 1. Moving away from the cross-lingual zeroshot setting (Hu et al., 2020; Ruder et al., 2021), we standardize multilingual in-language fine-tuning based on the amount of data that can realistically be annotated within 8h for a language. Our results highlight the limitations of current models on ULs, demonstrate the potential of language models (LMs) to improve user-centric applications, and show the benefit of byte-based approaches.\nIn this work, we contribute the first massivelymultilingual few-example benchmark including: a) newly created data for QA, OCR, autocomplete, semantic parsing, and sentence-level transliteration; b) new task setups for named entity recognition (NER) enabling evaluation on natural\u2014rather than tokenized\u2014text; and for QA and retrieval providing a more interesting setting than the gold passage (GoldP) setup while offering a lower barrier-toentry than the full TyDi QA (Clark et al., 2020) or XOR (Asai et al., 2021) tasks; c) carefullydesigned experimental setups, standardizing inlanguage fine-tuning and in-context learning and focusing on the information access scenario for ULs for ASR and MT; d) baseline results with commonly used subword and byte-based models."
        },
        {
            "heading": "2 Related Work",
            "text": "Multilingual benchmarks Some studies employ highly multilingual individual datasets for the evaluation of multilingual models, including Universal Dependencies (de Marneffe et al., 2021) or XL-Sum (Hasan et al., 2021). At the same time, there is increasing work on datasets in ULs for a variety of applications (Niyongabo et al., 2020; Winata et al., 2023; Muhammad et al., 2023). Due to their rapidly growing capabilities, NLP models are increasingly evaluated on a suite of datasets. Existing multi-task multilingual benchmarks such as XTREME (Hu et al., 2020), XGLUE (Liang et al., 2020), and XTREME-R (Ruder et al., 2021) cover 20\u201350 mainly high-resource languages and prioritize tasks with available data, regardless of their utility to speakers. Recently, MEGA (Kabir\net al., 2023) and BUFFET (Asai et al., 2023) evaluate in-context learning on existing multilingual tasks. In contrast, XTREME-UP focuses on underrepresented languages, user-centric tasks, a more realistic scarce-data setting, and introduces new tasks and datasets.\nMultilingual evaluation The choice of the experimental setting and aggregation metric are important considerations in multilingual evaluation. Prior work focused on zero-shot cross-lingual transfer (Hu et al., 2020), which\u2014despite being compelling from a scientific perspective (Artetxe et al., 2020)\u2014is less practically useful. While in-language fine-tuning has been explored before (Lauscher et al., 2020; Hedderich et al., 2020), XTREME-UP is the first to standardize the setting across tasks based on realistic annotation costs. Different frameworks aggregate performance in different ways across languages. Blasi et al. (2022) assess the utility of a task by weighting model performance based on the size of the speaker population while Khanuja et al. (2023) introduce the Gini coefficient to quantify performance disparity across languages. XTREME-UP opts for a simple average over ULs, emphasizing intuitiveness and accessibility of the results."
        },
        {
            "heading": "3 XTREME-UP",
            "text": ""
        },
        {
            "heading": "3.1 Design Principles",
            "text": "XTREME-UP is motivated by the following design principles:\nUnder-represented languages Following Joshi et al. (2020) we select languages in categories 1\u20133 (e.g., Amharic, Estonian, Kinyarwanda) as under-represented, leaving categories 4\u20135 as highresource languages (e.g., English, German, Hindi). We focus on tasks with existing data in ULs and tasks where we can efficiently collect data at scale (see Appendix B for an overview of ULs in XTREME-UP).\nUser-centric tasks We focus on widely adopted user-facing tasks benefiting speakers of highresource languages. We further break these down into two major groups: 1) input/output tasks; and 2) information access tasks (see Figure 1).\nScarce data We focus on a realistic scenario where a small amount of data is available in each UL. Mirroring reality, we do not restrict the amount\nof training data available in high-resource languages, but rather provide only as many labeled training examples as can be annotated in a realistic amount of time for ULs (see Section 3.2).\nEfficiency We focus on massively multilingual evaluation settings that can still be run efficiently with a modest amount of compute.\nText-centric, yet multi-modal We focus on tasks that can be tackled using textual data alone and provide baseline systems that do so. We frame multi-modal tasks (OCR and ASR) so that natively multi-modal models can be evaluated fairly alongside text-only models. We accomplish this by releasing original audio, image, and text model inputs while also providing baseline system output that can be fed to second-stage text-only systems. We hope to see fully multi-modal models take up this challenge over the coming years.\nWe provide an overview of the tasks in XTREMEUP in Table 1. We discuss motivation and highlevel information in the next section and provide more details for each task in Appendix D."
        },
        {
            "heading": "3.2 How much data?",
            "text": "To ensure a realistic amount of training data, we limit the training data in each task per language to the number of examples that can be annotated in 8 hours. We believe this reflects the real difficulty of annotating training and evaluation data for a very large number of languages. In this way, we design for the task first. For each task, we estimate how\nlong it takes to annotate a single example for a trained annotator.5 We base our estimates on prior work and our own annotation efforts.6 We show the data annotation time estimates in Table 1. For tasks with larger training datasets, we sub-sample the available data accordingly. Table 1 shows the sub-sampled data sizes. We show the input and output format of each task in Table 2. We provide an example instance of each task in Appendix C."
        },
        {
            "heading": "3.3 Input / Output Tasks",
            "text": "Automatic speech recognition (ASR; D.1) The goal of ASR is to transcribe speech into humanreadable text. It thus serves as a fundamental step for enabling natural language understanding applications on speech input. In many contexts, users may strongly prefer to speak rather than type and so high-quality ASR is an enabling factor for such interactions. We employ the FLEURS dataset (Conneau et al., 2023) consisting of recordings in 102 languages for sentences from FLORES-101 (Goyal et al., 2022), which were translated from English Wikipedia to 101 languages. We evaluate on 77 under-represented languages.\nOptical character recognition (OCR; D.2) OCR, the process of converting text from images\n5For simplicity, we estimate the annotation time for labeling only, ignoring factors such as training annotators, data processing, data validation, interface design, etc. We note that unlabeled data may not be available for certain ULs (Nekoto et al., 2020) and its creation may require tools such as keyboards, which may not be available in all languages.\n6For autocomplete, we calculate average writing time.\ninto machine-readable formats, is used in a wide range of applications, from extracting data only available in paper books (Rijhwani et al., 2020) and imaging legal documents (Singh et al., 2012), to improving accessibility for people with low vision (Mowar et al., 2022). It is especially important for under-represented languages, where both training data and content that users may wish to access may not be available as digital text on the web.\nWe create a dataset that aims to fill the gaps in previous work in OCR for ULs (see Appendix D.2) by focusing on larger-scale, typologically diverse, and user-centric data. Our dataset contains transcriptions for books in seven languages: Amharic (am), Bengali (bn), Kannada (kn), Myanmar (Burmese; my), Sanskrit (sa), Sinhala (si), and Swahili (sw). The books domain is the primary use-case for a large number of downstream users, but is one of the most challenging for OCR models (Rigaud et al., 2019). The dataset consists of transcriptions of entire pages and thus enables leveraging the full context understanding capabilities of large language models.\nAutocomplete (D.3) Autocomplete (or predictive text), i.e., predicting the rest of a word a user is typing, is a useful technology that speeds up human-computer interaction (Anson et al., 2006). As such, autocomplete has become a technology\nthat users have come to expect and rely on for input in high-resource languages. The standard next word prediction task (Sundermeyer et al., 2012) does not accurately reflect this practical setting as it relies on predicting entire units (words, subwords, or characters); similarly, perplexity-based evaluation makes comparisons across segmentations and languages difficult (Mielke, 2019) and ignores threshold effects associated with top-k predictions in a user interface (Tam and Wells, 2009).\nTo fill this gap, we introduce a new autocomplete task that unifies character, subword, and tokenlevel LM settings by focusing on a \u201cword\u201d as the predictive unit. Models are required to complete the next word based on a left context of N words and an optional character n-gram prefix. We use accuracy@3 for evaluation to reflect the requirement of displaying a limited number of candidates to the user. We process high-quality natural language data from Universal Dependencies (de Marneffe et al., 2021), which we deduplicate against mC4 (Xue et al., 2021), the most common multilingual pretraining corpus in order to test models\u2019 predictive rather than memorization capabilities.\nTransliteration (D.4) Transliteration is the conversion of text between writing systems (Wellisch, 1978). Unlike translation, it does not change content but only script. Transliteration is important because it allows users to type in their preferred script (e.g., Latin script) even if it is different than their preferred display script (e.g. Devanagari) and is used internally by many machine translation systems to rewrite names from different scripts.\nWe extend the Dakshina dataset (Roark et al., 2020), which provides romanizations of Wikipedia sentences written in the native scripts of 12 South Asian languages, with: a) romanizations of native script Wikipedia for one new language (Amharic); and b) transliteration to a third script (Shahmukhi) for one already covered language (Punjabi). The resulting task covers 13 languages for which transliteration occurs from the Latin script to the native script, and vice versa, and between Shahmukhi, Gurmukhi, and Latin for Punjabi."
        },
        {
            "heading": "Machine translation (MT; App. D.5) MT is an",
            "text": "important technology for users of ULs wishing to read text written in a different language. However, most current approaches require large amounts of parallel training data to achieve good performance, which are often not available for ULs (Had-\ndow et al., 2022). We focus on the information dissemination scenario where content from highresource languages (including from tasks such as cross-lingual QA) is translated to enable information access by common users; as such, XTREMEUP includes translations from English into 93 languages, covering a wide range of high-resource and UL languages. Only 39 ULs are used for evaluation; the high-resource languages are included to allow for transfer learning.7 The dataset is adapted from FLORES-101 (Goyal et al., 2022), repurposing half of the dataset\u2019s original development set as a training set. See \u00a76 for a detailed discussion of how we distinguish freely-available unsupervised data versus purpose-annotated supervised data in XTREME-UP."
        },
        {
            "heading": "3.4 Information Access Tasks",
            "text": "Question Answering (D.6) Question answering enables responding to natural language questions with answers found in text. We focus on the information-seeking scenario (Kwiatkowski et al., 2019) where questions are asked without knowing the answer. Information-seeking question-answer pairs tend to exhibit less lexical and morphosyntactic overlap between the question and answer since they are written separately.\nWe include two variants of the task: In inlanguage QA, both question and passage are in the same language. We obtain original questions and passages from TyDi QA (Clark et al., 2020). For cross-language QA, the question is in the user\u2019s native language while passage and answer are in a language with a large amount of answer content available (English). We use examples from TyDi XOR (Asai et al., 2021) in 7 languages. We additionally collect new data in 23 new Indic languages for cross-lingual QA by professionally translating questions and answers from existing Indic languages in XOR QA. This methodology mitigates the issue of translating Western-centric English data to locales with different topical interests. Cross-lingual QA is especially important for ULs since they lack plentiful in-language answer content on the web.\nIn XTREME-UP\u2019s QA task, a system is given a question, title, and a passage and must provide the answer\u2014if any\u2014or otherwise return that the question has \u201cno answer\u201d in the passage.8 To this\n7Our baseline results were trained only on the 39 UL pairs for efficiency.\n8This follows SQuAD v2 (Rajpurkar et al., 2018).\nend, we generalize the gold passage (Clark et al., 2020) setting, augmenting it with negative examples. These negatives are obtained from (a) passages within the same article as a passage containing the answer and (b) question-answer pairs from the full TyDi QA dataset where no answer was found in the candidate Wikipedia article. The data is split into training, validation, and test splits in such a way to avoid deduplication and overlap of splits, even across our various QA tasks. 9\nRetrieval for QA (D.6) Within the informationseeking QA scenario, the above core QA task assumes answer candidate passages as an input. In practice, a passage retrieval system for questionanswering allows for the extraction of relevant text from a vast text corpus. The retrieved passages can then be used by a question-answering system to extract or generate an answer to the user\u2019s question. In XTREME-UP, we separate retrieval into two distinct tasks, in-language retrieval and crosslanguage retrieval. For in-language retrieval, both the questions and passages are in the same language. The preparation of negatives, deduplication, and splits are identical to the QA task above. For validation and test, we create an index of 271k inlanguage passages (447k English passages for the cross-language task) making for a small enough index for efficient experimentation, while containing distractors that make for a challenging task, since these distractors are drawn from the same articles containing the target passages.\nNamed entity recognition (NER; D.7) NER is an important capability for information access systems that users depend on with applications ranging from recognizing requests for entity lookups to performing information extraction to populate the knowledge graphs that handle those requests. NER is also a capability needed in spell-checking and localization systems (Li et al., 2020).10 Identifying entities in ULs poses challenges due to the use of different scripts, lack of capitalization, different numerical representations, etc. We build on MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al., 2022), two large\n9This turns out to be non-trivial given the different splits strategies across the various datasets and our decision to create a train, validation, and test set even where only a train and validation set were previously available for public download.\n10We emphasize the word capability here since we recognize that stand-alone NER systems may not be strictly necessary in the long run; however, the capability of recognizing and properly handling entities will remain.\nNER datasets in African languages, which provide data in the standard CoNLL tokenized format (Tjong Kim Sang and De Meulder, 2003). In order to enable evaluation in a setting that is closer to the real world, we automatically map the annotated spans to the original raw text. The combined data with byte-level span annotations\u2014 termed MasakhaNER-X\u2014covers 20 languages.11\nSemantic parsing (D.8) Semantic parsing is the task of mapping a natural language utterance to a logical form or a structured interpretation that can be executed by a system such as a virtual assistant. This task is especially timely as users will increasingly want to turn their interactions with assistants and chat-like dialog systems into actions on external systems, which require API calls; this capability is what the semantic parsing task evaluates.\nWe adapt the test split of MTOP12 (Li et al., 2021) with professional translators/annotators to 15 languages: Amharic, Belarusian, Bengali, Brazilian Portuguese, Finnish, German, Hausa, Hungarian, Japanese, Russian, Swahili, Tamil, Turkish, Yoruba, and Zulu. Together with the original MTOP languages, the new MTOP++ dataset covers a total of 20 languages. Differently from MTOP, we collect localized data (i.e., Western-centric entities are replaced with more culturally relevant entities for the target language), following recent trends in multilingual benchmarking (Lin et al., 2021; Ding et al., 2022; Majewska et al., 2023).\nWe also extend MTOP to three widely spoken but under-represented Indic languages in a codeswitching setting: Hindi-English, Bengali-English and Tamil-English. We automatically convert the test-split of MTOP to code-mixed utterances using PaLM (Chowdhery et al., 2022) and run human verification on such utterances."
        },
        {
            "heading": "3.5 Overall Evaluation",
            "text": "For each task, we evaluate model performance by computing a task-specific score. We employ character-level metrics such as character error rate (CER) and character n-gram F-score (chrF; Popovic\u0301, 2015) rather than their word-level counterparts as they enable more fine-grained evaluation and are better suited to morphologically rich languages. We obtain a final score by averaging the\n11We remove the Fon and Hausa subsets of MasakhaNER 2.0 due to quality issues in the annotated data.\n12Other datasets (see App. D.8) were not yet available at the start of the project and do not focus on ULs.\nscores of all tasks. For each task, we only average performance over ULs (discussed in \u00a73.1). For metrics such as character error rate (CER) where lower is better, we invert the scores before averaging scores across tasks. For mean reciprocal rank (MRR), which is in the 0.0\u20131.0 range, we renormalize it to the 0\u2013100 range before averaging. While this scalar provides a quick overall impression of a system\u2019s quality across a broad range of tasks, it is not a substitute for analyzing performance on individual tasks, languages, or types of examples."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental setting",
            "text": "Multilingual fine-tuning In contrast to prior benchmarks that focus on zero-shot cross-lingual transfer from English, XTREME-UP focuses on the more realistic scenario of fine-tuning on a small amount of data in the target language. To make this scenario scalable in a massively multilingual setting, XTREME-UP fine-tunes a single model on the combined training data across the available languages for each task. The data for each language is sub-sampled to emulate data sizes that can be realistically annotated within a reasonable time frame (see \u00a73.2).\nIn-language in-context learning We also provide a 5-shot in-context learning setting where a model is provided with an English instruction and 5 exemplars in the target language in order to evaluate the progress on few-shot learning with large models for ULs. We provide the instruction for each task in Appendix E.13"
        },
        {
            "heading": "4.2 Baselines",
            "text": "We provide results on a handful of baseline systems that have already been developed by the research community. Given that our focus in this paper is on the dataset and task setup rather than system building, we do not focus on offering novel modeling types nor do we exhaustively evaluate all possible models; rather we view these results as estimating a starting point from some well-known modeling approaches and seeding contributions from the broader research community.14\nMultilingual fine-tuning baselines For the main experimental setting of multilingual finetuning, we provide the following baselines: mT5base (Xue et al., 2021) and a subword-based multilingual encoder-decoder model; ByT5-base (Xue et al., 2022), a byte-based multilingual encoderdecoder model.\nIn-context learning baseline For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022). We provide additional information on the baseline systems in Table 3.\nTo offer baseline systems that allow experimentation with text-only models, we use upstream models to provide initial output for ASR and OCR, and present text-based baselines that use these as inputs. We expect these baselines to give way to fully multi-modal models as research progresses. These initial ASR and OCR outputs should be seen as part of a baseline system, not part of the XTREME-UP benchmark iteself. For ASR, we augment the data with predictions of the state-of-the-art Maestro-U (Chen et al., 2023) and then use a downstream text model to improve the outputs (Bassil and Alwani, 2012). Similarly, for OCR, we use the off-the-shelf Google Vision OCR15 to get first-pass outputs, and train language models to improve them (Dong and Smith, 2018; Rijhwani et al., 2020).\n13The choice of prompt and exemplars can have a significant impact on performance (Zhao et al., 2021a,b). We provide a single instruction and set of exemplars per task and language for replicability and leave the search for better instructions and exemplars to future work.\n14XTREME-UP offers a public results tracker for use in tracking the community\u2019s progress on XTREME-UP. We conceptualize these results not as a competition, but as offering insights about different models and their trade-offs, each justifying and explaining how it should be compared to the others and how it informs the research landscape. Submissions can be made via self-service git pull requests.\n15https://cloud.google.com/vision/docs/ocr\nInfrastructure Models were trained using seqio and t5x (Roberts et al., 2022) on TPUs (Kumar et al., 2019; Pope et al., 2022)."
        },
        {
            "heading": "4.3 Results",
            "text": "We show the baseline results in Table 4.\nByte-based models outperform subword-based on ULs. The byte-based ByT5 outperforms the subword-based mT5 across most tasks. Gains are particularly pronounced for tasks that require dealing with information on the character level such as autocomplete and transliteration and for predicting information on the word level such as for NER and semantic parsing. These results demonstrate that as we train and evaluate our models on underrepresented languages, standard modeling choices such as subword representations fall short.\nIn-context learning underperforms fine-tuning on limited data. The Flan-PaLM model generally performs worse than fine-tuned models, despite being much larger. Nevertheless, it achieves reasonable performance on machine translation, which is likely reflected in the pre-training data. On other tasks, however, it fails to reliably apply its English-centric knowledge to ULs. Despite finetuned models performing relatively well on NER, the in-context learning model is unable to consistently generalize to the task in a few-shot setting in under-represented languages. On semantic parsing, the model fails to generalize to the large number of domain-specific intents and slots using standard prompting in ULs.16 The autocomplete tasks in particular demonstrate the lack of robust cross-lingual information in the English-centric PaLM model: it struggles to complete a sentence given a character prefix and fails to reliably convert between different scripts in the same language. XTREME-UP thus provides a strong challenge to test the generalization abilities of in-context learning methods to ULs.\nThere is a lot of headroom left to improve performance on ULs. Overall, across all tasks there is still a considerable amount of headroom left. For ASR, OCR and transliteration, around 10% of characters are still incorrectly predicted. On autocomplete, models only make the correct prediction in about one fourth of all cases. For MT, on average\n16We leave the exploration of multilingual adaptive prompting and dynamic exemplar selection (Drozdov et al., 2023) methods to future work.\nonly about a third of n-grams in the hypothesis are also present in the reference, and vice versa. For QA and retrieval, there are large performance differences between in-language and cross-language settings and much headroom still left. On NER, models perform relatively well but are still far from perfect performance on the task. Finally, on semantic parsing models are only able to produce the correct output in around a third of all cases."
        },
        {
            "heading": "5 Analyses",
            "text": "Lowest-performing languages Models generally perform poorly on African languages. On transliteration, models perform relatively worst on the newly added Amharic language. On NER, which covers only African languages, performance is lowest for Amharic\u2014likely due to its different script\u2014and the extremely under-represented Ghom\u00e1l\u00e1\u2019. Similarly, translation models underperform in Amharic and Yoruba. On ASR, the lowestperforming languages are Yoruba but models also struggle with other languages such as Gaelic, and many South Asian languages such as Lao, Khmer, and Burmese.\nTask-specific observations ByT5 provides the best performance while the size of the model does not seem to impact performance much. Several aspects of the data lead to higher error rates in transliteration: the model struggles with input in the Perso-Arabic script and to produce output in Latin based on a different script. For autocomplete (see Appendix D.3), our analyses indicate that models perform better on text that uses the Latin script."
        },
        {
            "heading": "6 Recommendations",
            "text": "Use of splits XTREME-UP offers a train, validation, and test split for each task. We recommend using the training split for learning the parameters of your model or as exemplars for in-context learning while iteratively checking your progress on the validation (i.e. development) split. The test split should not be used for iterative evaluation of your models or other sorts of hill-climbing; instead, it should be reserved for reporting your results and comparing after you have finished development on your models. Experiments that follow this customary scientific rigor should expect to show better generalization and less overfitting to the test split.\nUse of additional pre-training data One potential confounder for results along different pretrained models is the variation in pre-training data; where this data overlaps with the targets (outputs) in XTREME-UP validation and test splits, results can be artificially inflated, providing a sense that results are better than they are in reality\u2014if the validation or test data leaked into the pre-training data via contamination during large-scale data scraping, then it\u2019s unlikely that the system would truly perform as well for new unseen inputs. Therefore, we recommend that when researchers modify the pre-training data for a model, they explicitly report overlap (contamination) between the targets of the XTREME-UP validation/test splits and their pre-training corpus.17\n17We recognize that this is a very large-scale undertaking, requiring a fairly large amount of compute. As such, we suggest that it\u2019s may only be needed when making claims that compare systems (e.g. that the system with possibly-\nUse of additional supervised data It is entirely possible that the community will find creative ways to improve models based on supervised data not included with XTREME-UP. However, researchers should bear in mind how this might affect the comparability of their results with other models. The following axes should be considered:\n1. Any additional data from high resource languages is always allowed in the XTREME-UP setting. 2. Supervised data (e.g. parallel data for MT) harvested from the web, religious, books, and other opportunistic sources will typically be out-of-domain and is therefore admissible; conversely, supervised data from ULs from highly similar tasks or domains should generally be considered against the spirit of the XTREME-UP benchmark. 3. Monolingual data from UL is admissible with the caveat that one should measure overlap with targets, as discussed above.\nAvoid off-the-shelf MT systems Data augmentation via automatically translating high-resource supervised data to languages with less supervised data has proven a very effective means of improving system quality. However, it is not necessarily realistic to use a pre-existing MT system (e.g. an API or an open-source model) since those systems have typically been trained on a large amount of parallel data\u2014or at least unknown data. This means that additional supervised data would then be leaking into the experimental setup, which is otherwise intended to reflect the reality that most under-represented languages have very little supervised data. If data augmentation via translation is used, we encourage researchers to report the parallel data sources used and argue why this experimental setup is realistic\u2014or to clearly point out such usage in their experiments as an unavoidable confound and discuss the limitations this sets on what conclusions can be drawn about how results will extrapolate to the breadth of under-represented languages.\nIn all cases, researchers should rigorously report what additional data was used and how; each use case comes with its own considerations and,\ncontaminated pre-training data is equivalent, better, or almost as good as some other system). Note, this analysis only needs to be done once for each pre-training corpus (e.g., once for mC4) and it is very likely that organizations with enough compute to pre-train a new model on a new corpus would also have sufficient compute to calculate overlap.\nabove all, researches should make a well-reasoned argument that their use of data (i) does not artificially inflate evaluation scores and (ii) reflects a real-world scenario of finding and applying data."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have presented XTREME-UP, a multilingual benchmark distinguished by its being (i) scarcedata, (ii) user-centric, and (iii) focused on underrepresented languages. The benchmark contains input modalities of text, images, and audio while still allowing experimentation with text-only models. We hope this benchmark will be useful in accelerating research that is useful to speakers of under-represented languages and in highlighting both the progress and limitations of current models of language."
        },
        {
            "heading": "Limitations",
            "text": "The dataset presented in this work does not represent all of the world\u2019s languages nor all of the under-represented languages. While we have made efforts to include languages and dialects across a broad variety of geographic regions and language families, it was not feasible to locate or create data in the same set of languages across all tasks. Since this is a data-focused paper, we present modeling results on a few strong modern models; this is not an exhaustive exploration of how all current models may perform on this dataset. We look forward to exploring more under-represented languages as more data becomes available."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Slav Petrov, Jason Riesa, Raphael Hoffmann, Dipanjan Das, Clara Rivera, Chris Alberti, Machel Reid, and Timothy Dozat for helpful discussions and feedback. We are grateful to Noah Constant for a review of a draft of the paper. We also gratefully acknowledge the contributions of the researchers who built the datasets that have gone into XTREME-UP; we recommend that all component datasets be cited individually when using XTREMEUP in a paper such that dataset authors (many of whom are not authors of this article) receive credit for their work and so that those original sources remain easily discoverable in the literature."
        },
        {
            "heading": "A Contributions",
            "text": "In this section, we provide more detail about the contributions of each author."
        },
        {
            "heading": "General overview",
            "text": "Project leads Sebastian Ruder, Jonathan Clark\nPrimary contributors and task owners Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel Sarr, Xinyi Wang, John Wieting\nMajor contributors Nitish Gupta, Anna Katanova, Christo Kirov, Dana Dickinson, Brian Roark, Bidisha Samanta, Connie Tao\nSupporting contributors David Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle, Melvin Johnson, Dmitry Panteleev, Partha Talukdar"
        },
        {
            "heading": "By Task",
            "text": "ASR Min Ma\nAutocomplete Jean-Michel Sarr, Vera Axelrod, Colin Cherry, Sebastian Ruder, Jonathan Clark\nMT Parker Riley, Isaac Caswell, Colin Cherry, Jonathan Clark\nNER Sebastian Ruder, David Adelani, Dan Garrette\nOCR Shruti Rijhwani, Dana Dickinson, Reeve Ingle, Dmitry Panteleev, Sebastian Ruder\nQA Mihir Kale, John Wieting, Nitish Gupta, Partha Talukdar, Jonathan Clark\nRetrieval John Wieting\nSemantic parsing Massimo Nicosia, Bidisha Samanta, Partha Talukdar\nTransliteration Alexander Gutkin, Anna Katanova, Christo Kirov, Brian Roark\nEvaluation framework and public results tracker Xinyi Wang\nProgram management Connie Tao\nFine tuning and modeling Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel Sarr, John Wieting, Sebastian Ruder, Jonathan Clark\nData processing Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel Sarr, Xinyi Wang, John Wieting, David Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle, Dmitry Panteleev, Sebastian Ruder, Jonathan Clark\nData collection Massimo Nicosia, Bidisha Samanta, Nitish Gupta, Anna Katanova, Christo Kirov, Dana Dickinson, Brian Roark\nIn-context learning Sebastian Ruder\nBenchmark design Jonathan Clark, Sebastian Ruder, Melvin Johnson"
        },
        {
            "heading": "B Language Coverage",
            "text": "We provide an overview of the under-represented languages in XTREME-UP in Table 5. For each language, we indicate a) the ISO 639-1 code (or ISO 639-3 code if the former is unavailable); b) its language family according to Glottolog (Nordhoff and Hammarstr\u00f6m, 2011); c) the number of datasets in XTREME-UP including the language; d) its resource level based on the taxonomy of Joshi et al. (2020) (0 is least and 5 is highest-resourced); and e) which tasks include the language."
        },
        {
            "heading": "C Task Examples",
            "text": "We provide an example instance of each task in Table 6."
        },
        {
            "heading": "D Data cards",
            "text": ""
        },
        {
            "heading": "D.1 ASR",
            "text": ""
        },
        {
            "heading": "D.1.1 Task description",
            "text": "Automatic speech recognition (ASR) transcribes speech inputs into human-readable text, serving as a fundamental step for various speech language understanding applications. The transcripts are often calibrated with some pre-trained language models to produce the final outputs. In this paper, we build the ASR benchmark in this way: first, transcribe input audio into text with a pre-trained speech recognition model; then calibrate the transcripts by fine-tuning pre-trained language models on paired transcripts and ground truths."
        },
        {
            "heading": "D.1.2 Data creation",
            "text": "Experimented on the FLEURS corpus (Conneau et al., 2023), we use Maestro-U (Chen et al., 2023) to generate the ASR transcripts. For the pre-trained language models, we choose mT5-base (Xue et al., 2021) and ByT5-base (Xue et al., 2022) models. We paired the ASR transcripts with the ground truths to fine-tune the mT5 or ByT5 models. The average character error rate (CER) of Maestro-U is 8.28% across 102 languages, providing a strong baseline. Therefore, we build the ASR benchmark in a selective way: first, we compare the MaestroU baseline CER on the dev set with the CER obtained by fine-tuned mT5 or fine-tuned ByT5. If the fine-tuned result is better, we choose the finetuned model for the language to rescore its test set; otherwise, we keep the baseline Maestro-U results for the test."
        },
        {
            "heading": "D.1.3 Data structure",
            "text": "We followed the data split of train, dev, and test sets in FLEURS, and filtered out the examples where Maestro-U prediction is empty (i.e., all the deletion errors). The pairs of transcript and ground truth are saved in jsonl and tsv format.\nThe individual language datasets are mostly distinguished by the language and region BCP-47 codes, e.g., the kam_ke code represents Kamba language spoken in Kenya. In some cases, when multiple writing systems are available for a language, the ISO 15924 script code is used as well, as is the case with the code sd_arab_in that denotes Sindhi as spoken in India and recorded using Arabic script, as opposed to its Pakistani counterpart.18"
        },
        {
            "heading": "D.1.4 Data statistics",
            "text": "The FLEURS dataset contains about 1.4k hours of audio in total for 102 languages. The training data contains 271,488 examples across 102 languages, average length per utterance is about 20 tokens. There are 34,661 examples in the validation (dev) set, and 77,943 examples in the test set."
        },
        {
            "heading": "D.1.5 Experiments and Discussion",
            "text": "We compared fine-tuned mT5-base and ByT5-base baselines, which were built on TPU. In addition, we explored the compute efficient fine-tuning on GPU, using a mT5-small model as pre-trained model. The three models took 4500, 6500 and 4000 steps to converge, respectively. We report the character error rate for the predicted transcripts by the finetuned models against the one for the Maestro-U baseline, which is 8.28% on average for 102 languages \u2013 a quite strict baseline. We observed small gains through fine-tuning with different pre-trained models, as shown in Table 7.\nIt is observed that ByT5 yields better fine-tuned results than mT5, indicating that byte is a better modeling unit when it comes to textual data of various writing systems. By calculating the average CER for 24 high-resourced language group and 78 low-resourced language group respectively, we find that both mT5 and ByT5 fine-tuned models can reduce CER from 6.40% baseline to 6.36% for high-resourced languages, while ByT5 on its own can further improve CERs for low-resourced languages from 8.86% baseline to 8.80%.\nFine-tuned ByT5 also generalized well on languages which were not seen in the pre-training\n18The es_419 code represents Latin American Spanish.\nphase. With a limited amount of fine-tuning data, ByT5 can improve baseline on the group of unseen languages, especially on Umbundu (umb_ao, -14% CER Relative). Even though only Romanized Chinese is used to pre-train ByT5, the fine-tuned ByT5 outperformed baselines for both Mandarin (in simplified Chinese, cmn_hans_cn), and Cantonese (in traditional Chinese, cmn_hant_hk)."
        },
        {
            "heading": "D.2 Optical character recognition (OCR)",
            "text": ""
        },
        {
            "heading": "D.2.1 General information",
            "text": "Dataset title UL-OCR"
        },
        {
            "heading": "D.2.2 Related work",
            "text": "While most existing datasets focus on higherresourced languages (Nayef et al., 2017; Rigaud et al., 2019), there has been recent interest in developing OCR for ULs. This includes the creation of a dataset for OCR on endangered languages (Rijhwani et al., 2020) and a synthetic dataset for 60 languages (Ignat et al., 2022)."
        },
        {
            "heading": "D.2.3 Data creation",
            "text": "We retrieve books that are in the public domain on Google Books. These are historic books, where the copyright has expired, as well as more recent and public-domain books, used in this dataset with approval from their publishers. We focus on languages with diverse scripts, for which no existing OCR dataset is currently available. We observe that many public-domain books in such languages are religious or linguistic in nature and were created for missionary purposes. In order to identify a diverse set of high-quality books, we first conduct an annotation task where we ask annotators to look at pages of a book and assign whether it is a) not in the target language, b) religious, c) consisting mainly of tables/other structured formatting, d) linguistic (e.g., a dictionary or grammar book), e) not intelligible, or f) low quality. Based on this annotation, we filtered out some languages that did not have a sufficient amount of high-quality public-domain books available. After filtering, the dataset contains annotated documents in seven under-represented languages \u2013 described in detail in Section 3.3."
        },
        {
            "heading": "D.3 Autocomplete",
            "text": ""
        },
        {
            "heading": "D.3.1 Task description",
            "text": "Autocomplete (or predictive text), i.e., predicting the rest of a word a user is typing, is a useful technology that speeds up human-computer interaction. However, while language modeling (LM) is a core\nnatural language processing (NLP) task, current LM evaluation does not address the practical constraints of human-computer interaction and current LMs are not directly useful for autocomplete in under-represented languages.\nIn order to evaluate multilingual models on an evaluation setting as close as possible to the realworld usage of autocomplete, we curated the Universal Dependencies (UD) dataset (Nivre et al., 2020; de Marneffe et al., 2021) according to a set of high level principles that we describe in the section below."
        },
        {
            "heading": "D.3.2 Data creation",
            "text": "The original UD dataset was filtered to better fit the user centric paradigm proposed. We removed a) treebanks using only ancient data, for example liturgical text written in Latin, Ancient Greek or Sanskrit; b) languages with fewer than 100 speakers like Akunts\u00fa; c) signed languages like the Swedish Sign Language; d) highly domain-specific content like for instance SiMoNERo (Mititelu and Mitrofan, 2020) which contains texts from three medical subdomains: cardiology, diabetes, endocrinology; e) languages that are \"high resource\" by XTREMEUP standards with the exception of English which we kept for prototyping; f) languages that do not have all three of: training, validation and test sets: g) languages with fewer than 1000 examples when combining training and validation set.\nThe resulting corpus features 23 languages: Basque, Belarusian, Bulgarian, Danish, Eastern Armenian, English, Estonian, Galician, Scottish Gaelic ,Greek, Hebrew, Icelandic, Indonesian, Irish, Latvian, Lithuanian, Nigerian Pidgin, Romanian, Slovak, Slovenian, Ukrainian, Urdu, and Uyghur."
        },
        {
            "heading": "D.3.3 Data structure",
            "text": "A data instance has two fields, input and target, for instance {input: \"en_-We look f$\", target: \"forward\"}. The input field is composed of a prefix \"en_-\" to indicates the language to the model and a context sentence: \"We look f$\". The target field is the word to predict. We normalize all text with Unicode NFKC normalization (Whistler, 2021).\nAnnotation process In the following, we describe how the example described above is generated from the source data. The original sentence is \u201cWe look forward to your active participation to make this forum an exciting meeting place for like minded individuals.\u201d The steps are: a) The context sentence including the target can have at most 10\nwords. A random word of more than 5 characters is chosen to be the target. b) A target context is sampled from the target and added to the context. In this example it is the character \"f\". The sample rule is to select a number of characters that can vary between 0 to the number of characters in the target minus three. In our example, the target \"forward\" could be sampled from \"\" to \"forw\". c) A specific token \"$\" is added just after the target context."
        },
        {
            "heading": "D.3.4 Data statistics",
            "text": "We sampled up to 2,000 examples from each language\u2019s training set, 1,000 examples from validation, and 1,000 examples from test. This prevents the languages from having disproportionately more data; where the original sets were smaller than these targets, we used all available data. We display the language statistics in Table 8. Note that these experiments are done on a preliminary dataset and not the final release version of XTREME-UP."
        },
        {
            "heading": "D.3.5 Experiment",
            "text": "We compared mT5 (Xue et al., 2021) and ByT5 (Xue et al., 2022), two state-of-the-art multilingual pre-trained LMs that are based on subwords and bytes respectively. The models were fine-tuned for 10 epochs on autocomplete training set, moreover. We used two metrics: top-3 word accuracy (Acc@3) and chrF: character n-gram F-score (Popovic\u0301, 2015)."
        },
        {
            "heading": "D.3.6 Results",
            "text": "We observe that ByT5 achieve better performance than mT5 for both Acc@3 and chrF on the autocomplete task as it is displayed in Table 9. Also ByT5 require less than half the time to fine-tune on the training set (45 minutes) compared to mT5 (1 hours and 30 minutes)."
        },
        {
            "heading": "D.3.7 Analyses",
            "text": "Based on Acc@3 and chrf, the most challenging languages for mT5 are Eastern Armenian ((hy)) and Uyghur (ug) respectively. Whereas Nigerian Pidgin is the (pcm) and Scottish Gaelic are the easiest languages. For ByT5, whether we consider Acc@3 or chrF, the most challenging language is Uyghur, and the easiest language is Galician (gl). Yet, these extremes only offer a qualitative comparison of mT5 and ByT5. Next, we investigate four questions around model performance: a) Do mT5 and ByT5 have the same cross-lingual generalization pattern? b) Do some languages yield higher scores because autocompletion guesses the\nsame words? c) Do some languages yield higher scores because they have a smaller vocabulary in their corpora? d) Does similarity to the Latin alphabet impact models\u2019 performance? We test several hypotheses below, considering a relationship to be significant when the p-value is under 0.05.\nDo mT5 and ByT5 have the same cross-lingual generalization pattern? mT5 and ByT5 have the same cross-lingual generalization pattern if the difficulty to generalize to a new language is the same for both models relatively to other languages. In other words, if models\u2019 performance are ranked similarly, they share the same cross-lingual generalization pattern. To evaluate this hypothesis we computed the Spearman\u2019s rank correlation between mT5 and ByT5 Acc@3. We got a Spearman\u2019s rank correlation of 0.69 with p-value < 0.001. This means that the two models have a high degree of relative agreement, in other words, if a new language is added, there is a high chance that the language is going to be challenging or not for both mT5 and ByT5."
        },
        {
            "heading": "Do some languages yield higher scores because autocompletion guesses the same words? If",
            "text": "our dataset in given language over-represents a word to predict, then the model might have misleadingly good performance by always predicting the same word. This would mean that the dataset is not balanced with regards to the diversity of target words. A common way to model the diversity of a distribution of words is to compute its entropy, so we computed the the Pearson correlation between the entropy of the test set\u2019s target word distribution in each language and mT5 and ByT5 Acc@3. The entropy of a distribution of word is maximal if every word is different, and it is minimal if it consist on a single word. mT5 and ByT5 displayed correlation coefficients of \u22120.16 and 0.13 respectively with p-value of 0.45 and 0.53 respectively. These results show that there is insufficient evidence to conclude that there is a significant linear relationship between target words diversity and model performance because the p-value is far above the 0.05 significance threshold. Hence, target word diversity is not a good predictor of model performance variability across languages.\nDo some languages yield higher scores because they have a smaller vocabulary in their corpora? We expect that languages with smaller corpora will be easier to fine-tune on because of a\nsmaller prediction space. To test that hypothesis, we computed the Pearson correlation between test set\u2019s vocabulary size and mT5 and ByT5\u2019s Acc@3 for each language. mT5 and ByT5 displayed correlation coefficients of \u22120.29 and 0.13 respectively with p-value of 0.17 and 0.54 respectively. Thus there is insufficient evidence to conclude that there is a significant linear relationship between vocabulary size and model performance because the pvalue is above the 0.05 significance threshold.\nDoes similarity to the Latin alphabet impact models\u2019 performance? We verify this hypothesis quantitatively by computing the similarity between a) a Latin alphabet composed of the 26 letters of the alphabet in lower and upper case and b) the alphabet of each language corresponding to all the characters in the test set except punctuation and special characters. The similarity was computed with the Jaccard similarity coefficient (Jaccard, 1908), i.e. the ratio of number of unique items in the intersection of both alphabets and the number of unique items in the union of both alphabets. Moreover we used the same methodology as before and computed the Pearson correlation between the Jaccard similarity index and chrF as this metric is more granular in models\u2019 character level performance. We observed a correlation of 0.56 and 0.75 for mT5 and ByT5 respectively with p-values < 0.01 respectively. It indicates that the similarity between the Latin alphabet and each language alphabet is significantly correlated to mT5 and ByT5 chrF."
        },
        {
            "heading": "D.3.8 Evaluation and Discussion",
            "text": "Whether we used a word level metric like Acc@3 or a character level metric like chrF, ByT5 is more accurate at autocomplete than mT5. We also observe that these models generalize more easily to languages written in an alphabet closer to the Latin alphabet, ByT5 being more sensitive to the alphabet of the input language."
        },
        {
            "heading": "D.4 Transliteration",
            "text": ""
        },
        {
            "heading": "D.4.1 Task description",
            "text": "Transliteration is the conversion of text in one writing system to another writing system, e.g., text written in the Devanagari script to the Latin script. It differs from translation in that it does not change the language content of the text, just the script. Many languages are written in multiple scripts, and the current task involves transliterating whole sen-\ntences, not just isolated terms, from one script to another."
        },
        {
            "heading": "D.4.2 Data Creation and Annotation process",
            "text": "Most of the data for the task comes from the romanized full-string subset of the Dakshina dataset (Roark et al., 2020), in which 10,000 Wikipedia sentences written in the native scripts of the 12 languages were human-romanized by native speakers, resulting in parallel sentences in the native and Latin scripts.19 Two 10,000 sentence additions were made to this data for the current transliteration task: Amharic Wikipedia sentences were similarly manually romanized by native speakers; and the Punjabi sentences from the Dakshina dataset, originally written in the Gurmukhi (Brahmic) script, were manually transliterated by native speakers to the Shahmukhi (Perso-Arabic) script."
        },
        {
            "heading": "D.4.3 Data Preparation",
            "text": "The resulting collection allows for overall 30 tasks converting between various scripts. These are summarised in Table 10 where, for each language indicated by the BCP-47 code (Phillips and Davis, 2009), the corresponding transliteration tasks are shown for scripts indicated by their ISO-15924 codes (ISO, 2004).\nAll the native script data was normalized using Unicode NFC (Whistler, 2021). The data was then further transformed using language-specific visual normalization for Brahmic and Perso-Arabic writing systems using the Nisaba script normalization library (Johny et al., 2021; Gutkin et al., 2022). Both NFC and visual normalization operations preserve visual invariance of the input text, with visual normalization handling many ambiguous cases that fall outside the scope of standard NFC."
        },
        {
            "heading": "D.4.4 Data Statistics",
            "text": "For each task, we establish 2,000 training sentences, 2,000 development set sentences, and close to 6,000 test sentences. Training data for any pretrained models used in the task cannot include the Dakshina dataset. Since this is a contextual fewshot transliteration benchmark, we do not provide the romanization lexicons that were released in the Dakshina dataset along with the full sentence romanizations.\nOur few-shot contextual transliteration task covers 13 languages from 3 language families\n19In the Dakshina distribution, the parallel sentences can be found in files named LANG.romanized.rejoined.tsv, where LANG is a BCP-47 language code.\n(Indo-Aryan, Dravidian and Semitic), all but one (Amharic) from South Asia."
        },
        {
            "heading": "D.4.5 Directionality and Evaluation Ambiguity",
            "text": "One difference between romanization in these languages and transliteration in the opposite direction (from the Latin script to the native script) is that none of the languages in the benchmark have an orthography in the Latin script, i.e., there is no single correct spelling in the Latin script for these languages. Rather, individuals tend to provide a rough phonetic transcription of the sentences using the Latin script. As a result, word identity may be difficult to achieve (hence high word-error rate), but string similarity should be relatively high between quality romanizations hence we use character-error rate to evaluate the transliterations. The ability to produce romanizations automatically has several key use cases, including simulation of parallel data from mono-script language samples, and for multilingual modeling of languages that use different scripts. For that reason, we include both directions in the benchmark."
        },
        {
            "heading": "D.4.6 Experimental Setup",
            "text": "Previously Xue et al. (2022) performed ByT5 finetuning and evaluation of transliteration and romanization directions separately on single-word, rather than full-sentence, data from vanilla Dakshina dataset. In this benchmark we remove the separation into transliteration and romanization by requiring all tasks to be fine-tuned jointly. In order to achieve this, during all stages of training, development and testing a special code is prepended to the input feature strings for each task. This task code indicates that the input features correspond to the conversion from writing system Source to writing system Target for a language lang. It is encoded as a string \u201clang_Source_Target\u201d. For example, for Punjabi (pa) conversion from Shahmukhi (Arab) to Gurmukhi (Guru) writing systems, the task code is \u201cpa_Arab_Guru\u201d.\nIn the full fine-tuning setup20 we jointly fine-tune the 30 transliteration tasks using mT5 and ByT5 models in Small, Base and Large configurations that correspond to around 300M, 582M and 1.2B parameters, respectively (Xue et al., 2021, 2022). Fine-tuning uses 10K training steps with a batch\n20This full fine-tuning setup contrasts with an efficient finetuning setup relying on fewer fine-tuning steps and smaller batch sizes, the results of which are reported in Table 4.\nsize of 128. We used Google TPU-v3 accelerators (Kumar et al., 2019) for fine-tuning all the configurations apart from ByT5 Large for which a more powerful TPU-v4 (Pope et al., 2022) was necessary."
        },
        {
            "heading": "D.4.7 Evaluation and Discussion",
            "text": "The evaluation results of the full fine-tuning setup described above are provided in Table 11, which shows the character error rate (CER) for each of the 30 transliteration tasks in six configurations, along with the corresponding averages over all the tasks.\nSome general trends are observable in these baseline results. The ByT5 error rates are generally substantially better than mT5, and, while the size of the configuration matters for mT5, it does not seem to matter much for ByT5. Overall, romanization is harder, i.e., transliterating into the Latin script yields higher error rates than transliterating out of it, perhaps due to the fact that there is no set orthography in the Latin script in those languages. For the best performing configuration (ByT5-Base), 9 out of 10 of the tasks with the lowest CER are from Latin script to native script. All of the tasks with the highest CER are into either the Latin or Perso-Arabic scripts, and all of the tasks transliterating Perso-Arabic input have worse-than-median CER. In other words: Perso-Arabic input is hard; Latin output is hard; and Perso-Arabic to Latin is particularly hard."
        },
        {
            "heading": "D.5 Machine Translation",
            "text": ""
        },
        {
            "heading": "D.5.1 Data Card",
            "text": "Basic Info\n1. Original datset name: FLORES-101\n2. Repository: https://github.com/ facebookresearch/flores/tree/main/ flores200\n3. Paper: Goyal et al. (2022)\n4. Point of Contact (original version): NLLB Team (flores@fb.com)\nWhy is this dataset part of XTREME-UP? Machine translation is an important tool for expanding language coverage for natural language processing tools. FLORES-101 is a high-quality, highlymultilingual dataset."
        },
        {
            "heading": "Data Fields",
            "text": "1. input: the source sentence, which is always English (string)\n2. target: the target-language translation of the source sentence (string)\nData Example {\"input\": \"<2xh> Local media reports an airport fire vehicle rolled over while responding.\", \"target\": \"Oonondaba basekuhlaleni bxele ukuba isithuthi somlilo sesitishi senqwelomoya siye saphethuka sisazama ukunceda.\"}\nLanguages Included in XTREME-UP release (93): Afrikaans (af), Amharic (am), Arabic (ar), (Eastern) Armenian (hy), Assamese (as), (North) Azerbaijani (az), Belarusian (be), Bengali (bn), Bosnian (bs), Bulgarian (bg), Burmese (my), Catalan (ca), Cebuano (ceb), Central Kurdish (ckb), Chinese (zh), Croatian (hr), Czech (cs), Danish (da), Dutch (nl), Estonian (et), Finnish (fi), French (fr), Fula (ff), Galician (gl), Georgian (ka), German (de), Greek (el), Gujarati (gu), Hausa (ha), Hebrew (he), Hindi (hi), Hungarian (hu), Icelandic (is), Igbo (ig), Indonesian (id), Irish (ga), Italian (it), Japanese (ja), Javanese (jv), Kannada (kn), Kazakh (kk), Khmer (km), Korean (ko), Kyrgyz (ky), Lao (lo), Latvian (lv), Lingala (ln), Lithuanian (lt), (Lu)Ganda (lg), Luxembourgish (lb), Macedonian (mk), Malay (ms), Malayalam (ml), Maltese (ml), Maori (mi), Marathi (mr), Mongolian (mn), Nepali (ne), Pedi (Sepedi) (Northern Sotho) (nso), Norwegian (no), Nyanja (Chichewa) (ny), Oriya (or), Oromo (om), Pashto (ps), Persian (fa), Polish (pl), Portuguese (pt), Punjabi (pa), Romanian (ro), Russian (ru), Serbian (sr), Shona (sn), Sindhi (sd), Slovak (sk), Slovenian (sl), Somali (so), Spanish (es), Swahili (sw), Swedish (sv), Tagalog (tl), Tajik (tg), Tamil (ta), Telugu (te), Thai (th), Turkish (tr), Ukrainian (uk), Urdu (ur), Uzbek (uz), Vietnamese (vi), Welsh (cy), Xhosa (xh), Yoruba (yo), Zulu (zu).\nEvaluated in benchmark (39): Amharic (am), (Eastern) Armenian (hy), Assamese (as), (North) Azerbaijani (az), Burmese (my), Central Kurdish (ckb), Gujarati (gu), Hausa (ha), Icelandic (is), Igbo (ig), Irish (ga), Javanese (jv), Kannada (kn), Khmer (km), Kyrgyz (ky), Lao (lo), Lingala (ln), (Lu)Ganda (lg), Luxembourgish (lb), Macedonian (mk), Malayalam (ml), Mongolian (mn), Nepali (ne), Pedi (Sepedi) (Northern Sotho) (nso), Nyanja\n(Chichewa) (ny), Oromo (om), Pashto (ps), Punjabi (pa), Shona (sn), Sindhi (sd), Somali (so), Swahili (sw), Tajik (tg), Telugu (te), Welsh (cy), Xhosa (xh), Yoruba (yo), Zulu (zu).\nData Statistics 50% of the FLORES-101 dev split was reserved for training and the remainder for validation. The original devtest split was unchanged and reserved for testing. This results in 499/498/1012 sentence pairs for train/validation/test, respectively.\nDataset Curators The original dataset was curated by the NLLB (No Language Left Behind) Team (flores@fb.com). The version included in XTREME-UP was curated by Parker Riley (prkriley@google.com) and Isaac Caswell (icaswell@google.com).\nCuration Rationale The original FLORES-101 dataset was created to be able to evaluate machine translation models in many languages. The version released in XTREME-UP was created to focus on low-resource languages and provide an in-domain train split along with validation and test splits, all of sizes in line with other tasks in XTREME-UP.\nData Sources The source data (selected by the NLLB Team) comes from Wikinews, Wikijunior, and Wikivoyage.\nDataset Creation Details of the creation of the original dataset are available in the original publication (Goyal et al., 2022).\nChanges to the Original Dataset for XTREMEUP The version of the dataset in XTREME-UP only has the source and target strings, removing additional metadata. We also include 93 of the original 100 non-English languages (the subset supported by Google Translate). Of these, only 39 are used for official evaluation."
        },
        {
            "heading": "D.6 Question Answering and Retrieval",
            "text": ""
        },
        {
            "heading": "D.6.1 Data Card",
            "text": "Basic Info\n1. Original datset names: TyDi QA, XOR-TyDi QA\n2. Additional cross-lingual data was collected as part of XTREME-UP, following similar methodology\nWhy is this dataset part of XTREME-UP? Question answering enables information access."
        },
        {
            "heading": "Data Fields",
            "text": "1. question: a question in the target language (string)\n2. title: the title of the evidence passage \u2014 target language for in-language setting, English for cross-language setting (string)\n3. passage: the evidence passage, which might contain an answer to the question \u2014 target language for in-language setting, English for cross-language setting (string)\n4. answer: the answer (if any) to the question (string)\nData Example See Table 6.\nLanguages See Table 5.\nData Statistics See Table 1.\nData Sources Evidence text was sourced from Wikipedia.\nDataset Creation Details of the creation of the original dataset are available in the original TyDi QA and XOR QA publications."
        },
        {
            "heading": "D.7 Named Entity Recognition (NER)",
            "text": "Dataset and task description The dataset contains processed data from MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al., 2022). Both datasets were created by Masakhane21.\nWhy is this dataset part of XTREME-UP? Named entity recognition is a fundamental task in natural language processing. The MasakhaNER datasets are high-quality multilingual datasets that provide data in 20 African languages. The data is human-annotated and thus higher quality than automatically collected NER datasets.\nLanguages and ISO 639-3 codes Bambara (bam), Ghom\u00e1l\u00e1\u2019 (bbj), \u00c9w\u00e9 (ewe), Igbo (ibo), Kinyarwanda (kin), Luganda (lug), Luo (luo), Mossi (mos), Naija (pcm), Chichewa (nya), chiShona (sna), Kiswahili (swa), Setswana (tsn), Akan/Twi (twi), Wolof (wol), isiXhosa (xho), Yor\u00f9b\u00e1 (yor), isiZulu (zul)\n21https://www.masakhane.io/\nChanges to the original datasets for XTREMEUP The original MasakhaNER datasets are provided in CoNLL format where each input sentence is already tokenized. This makes it difficult to evaluate NER models on natural text where tokenization may often be messy and introduces a bias towards word and subword-based models. To provide a level playing field and to enable evaluation of NER models on natural data data, we process the data in order to align the token-level annotations with byte-level spans in the original pre-tokenized text. For the NER task, we provide the original pretokenized text as input to the model. Hausa and Fon subsets of the original data were excluded as matching with the unlabeled source data revealed annotation artefacts in both language subsets."
        },
        {
            "heading": "D.8 Semantic parsing",
            "text": ""
        },
        {
            "heading": "D.8.1 Task description",
            "text": "Semantic parsing is the task of mapping a natural language utterance to a logical form or a structured interpretation that can be executed by a system such as a virtual assistant. For XTREME-UP, we adapted the MTOP (Li et al., 2021) test dataset to 15 languages, and to 3 code-switched Indic languages. The original MTOP data was published by Facebook and covers 6 languages across 11 domains, 117 intents and 78 slots."
        },
        {
            "heading": "D.8.2 Data creation",
            "text": "In this section, we describe the two processes used to extend the MTOP instances: the first involves translation and localization with professional translators and the second code-switching using a language model and verification by human annotators.\nIn both processes, we perform a linearization step of the query and parse. Given an English utterance from the MTOP English test set and the\ncorresponding slot information (slot names each with start and end bytes), we add slot tags around corresponding tokens in the query (Figure 2).\nMotivation Recently, researchers published more multilingual semantic parsing datasets that focus on virtual assistant domains (Li et al., 2021; FitzGerald et al., 2022; Moghe et al., 2022; Goel et al., 2023). We extend a portion of an existing semantic parsing dataset to new languages targeting the following features: a) high-quality utterances produced by professional translators; b) a wide range of domains and intents; c) inclusion of different language families and some underrepresented languages; d) sentences with culturally relevant entities; and e) code-mixed sentences, i.e., multiple language within the same sentence\u2014a common phenomenon in multilingual societies.\nTranslating MTOP to 15 languages: We take the bracketed versions of the slot-tagged English sentences from MTOP and we create translations and localization tasks to be carried out by professional translators. We ran two pilots on a small sample of the data to gather feedback and improve the annotation guidelines. The translators had to translate the original utterances to a given target language, while keeping the brackets around slot value translations and localizing those where possible. Once the pilots were completed without issues, we scaled the tasks to the full test set.\nWe carried out manual inspections on samples of the data to check if translation and localization was happening correctly, and a set of automatic checks on the full data to ensure that slots were matching between original and translated utterances. Data was sent back to annotators until all the issues were fixed."
        },
        {
            "heading": "Code-switching MTOP to 3 Indic languages:",
            "text": "We use PaLM to convert the linearized query into a code-mixed query using few-shot prompting. We experimented with different discrete prompt design strategies and selected the best prompts after a qualitative evaluation on a small held-out set (11 examples) covering all 11 domains. Specifically we experimented with three designs.\n\u2022 Naive prompting. The prompt contains (a) the task description followed by a set of examples consisting of (b) the original English linearized query and (c) the corresponding code-mixed version.\n\u2022 Parallel sentence prompting. In this case, the prompt contains (a) the task description, (b) the original English linearized query, and also (c) the target translated query (obtained with Google translate) and (d) the corresponding code-mixed query.\n\u2022 Parallel reordered sentence prompting. Similar to the previous, however, target translated queries are human written.\nWe observed that the Parallel sentence prompting was producing higher quality utterances, with 7/11 correct conversions for Hindi-English. 6/11 for Bengali-English, and 8/11 for Tamil-English. We used this strategy to design prompts with the help of native speakers of those languages. We selected 21 sentences from the training split for creating corresponding exemplars for the prompts. With the latter, we performed few-shot prompting with the 64b PaLM model and converted the test split of MTOP to a code-switched corpora.\nHuman annotators then had to check the PaLM generated data for the presence of code-mixing and for the labeling to be consistent between the original query and the code-mixed version. The annotators were instructed to fix the automatically generated data whenever they found such issues.\nD.8.3 Data structure and statistics To create the training, validation and testing splits for MTOP, we start from the English test set and remove intents with less than 10 examples. This leaves us with 53 intents and a maximum of 4,223 examples for each language (some original MTOP languages may have less examples, while our codeswitched data may have more due to multiple paraphrases).\nFor each intent, we randomly select training examples such that each slot is covered by at least one example, for a minimum of 5 examples. We end up with training, development and test sets containing respectively a maximum of 285, 239, and 3,669 instances for each language."
        },
        {
            "heading": "D.8.4 Experiments",
            "text": "We fine-tune mT5 (Xue et al., 2021) and ByT5 (Xue et al., 2022) in their base and large configurations on the multilingual training data we collected. Table 12 contains the Exact Match accuracies of a multilingual model trained on data from all languages but the code-switched sets. Table 13 contains the results of a model that includes the code-\nswitched sets. From both tables, we can see that ByT5-base is more accurate then the other models, even compared with the larger ones. This surprising result confirms similar findings on word-level tasks reported by Xue et al. (2022) and Nicosia and Piccinno (2022). We expect mT5 to catch up with ByT5 at larger sizes.\nE In-context learning examples\nWe show in-context learning examples for a selection of tasks in Table 14. Each example consists of a general instruction and prefixes for the input and target, which are repeated for each exemplar."
        }
    ],
    "title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
    "year": 2023
}