{
    "abstractText": "Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based finetuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model\u2019s decision-making process.1",
    "authors": [
        {
            "affiliations": [],
            "name": "John X. Morris"
        },
        {
            "affiliations": [],
            "name": "Chandan Singh"
        },
        {
            "affiliations": [],
            "name": "Alexander M. Rush"
        },
        {
            "affiliations": [],
            "name": "Jianfeng Gao"
        },
        {
            "affiliations": [],
            "name": "Yuntian Deng"
        }
    ],
    "id": "SP:e4f4fb87d313d22657b98e0437a38dd9d5a51cbe",
    "references": [
        {
            "authors": [
                "Abhineet Agarwal",
                "Yan Shuo Tan",
                "Omer Ronen",
                "Chandan Singh",
                "Bin Yu."
            ],
            "title": "Hierarchical shrinkage: improving the accuracy and interpretability of treebased methods",
            "venue": "arXiv:2202.00858 [cs, stat]. ArXiv: 2202.00858.",
            "year": 2022
        },
        {
            "authors": [
                "jan Chhablani",
                "Han Wang",
                "Jason Fries",
                "Maged Alshaibani",
                "Shanya Sharma",
                "Urmish Thakker",
                "Khalid Almubarak",
                "Xiangru Tang",
                "Dragomir Radev",
                "Mike Tian-jian Jiang",
                "Alexander Rush"
            ],
            "title": "PromptSource: An integrated development environment",
            "year": 2022
        },
        {
            "authors": [
                "Maciej Besta",
                "Nils Blach",
                "Ales Kubicek",
                "Robert Gerstenberger",
                "Lukas Gianinazzi",
                "Joanna Gajda",
                "Tomasz Lehmann",
                "Michal Podstawski",
                "Hubert Niewiadomski",
                "Piotr Nyczyk"
            ],
            "title": "Graph of thoughts: Solving elaborate problems with large language models",
            "year": 2023
        },
        {
            "authors": [
                "L. Breiman",
                "J.H. Friedman",
                "R.A. Olshen",
                "C.J. Stone."
            ],
            "title": "Classification and Regression Trees",
            "venue": "Wadsworth and Brooks, Monterey, CA.",
            "year": 1984
        },
        {
            "authors": [
                "Leo Breiman."
            ],
            "title": "Bagging predictors",
            "venue": "Machine learning, 24:123\u2013140.",
            "year": 1996
        },
        {
            "authors": [
                "Leo Breiman."
            ],
            "title": "Random forests",
            "venue": "Machine Learning, 45(1):5\u201332.",
            "year": 2001
        },
        {
            "authors": [
                "Harrison Chase."
            ],
            "title": "Langchain: Building applications with llms through composability",
            "venue": "https: //github.com/hwchase17/langchain.",
            "year": 2023
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou."
            ],
            "title": "Frugalgpt: How to use large language models while reducing cost and improving performance",
            "venue": "arXiv preprint arXiv:2305.05176.",
            "year": 2023
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin."
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013 794.",
            "year": 2016
        },
        {
            "authors": [
                "Hugh A Chipman",
                "Edward I George",
                "Robert E McCulloch."
            ],
            "title": "Bart: Bayesian additive regression trees",
            "venue": "The Annals of Applied Statistics, 4(1):266\u2013298.",
            "year": 2010
        },
        {
            "authors": [
                "Michael Collins."
            ],
            "title": "Three generative, lexicalised models for statistical parsing",
            "venue": "35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 16\u2013",
            "year": 1997
        },
        {
            "authors": [
                "Vin\u00edcius G Costa",
                "Carlos E Pedreira."
            ],
            "title": "Recent advances in decision trees: An updated survey",
            "venue": "Artificial Intelligence Review, pages 1\u201336.",
            "year": 2022
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini."
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First",
            "year": 2006
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "proceedings of Sinn und Bedeutung, volume 23, pages 107\u2013124.",
            "year": 2019
        },
        {
            "authors": [
                "Lingjia Deng",
                "Janyce Wiebe"
            ],
            "title": "MPQA 3.0: An entity/event-level sentiment corpus",
            "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2015
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire."
            ],
            "title": "A decisiontheoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences, 55(1):119\u2013139.",
            "year": 1997
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "Experiments with a new boosting algorithm",
            "venue": "In icml,",
            "year": 1996
        },
        {
            "authors": [
                "Bairu Hou",
                "Joe O\u2019Connor",
                "Jacob Andreas",
                "Shiyu Chang",
                "Yang Zhang"
            ],
            "title": "Promptboosting: Black-box text classification with ten forward passes",
            "year": 2022
        },
        {
            "authors": [
                "Eduard Hovy",
                "Laurie Gerber",
                "Ulf Hermjakob",
                "ChinYew Lin",
                "Deepak Ravichandran."
            ],
            "title": "Toward semantics-based answer pinpointing",
            "venue": "Proceedings of the First International Conference on Human Language Technology Research.",
            "year": 2001
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.",
            "year": 2004
        },
        {
            "authors": [
                "Shengding Hu",
                "Ning Ding",
                "Huadong Wang",
                "Zhiyuan Liu",
                "Jingang Wang",
                "Juanzi Li",
                "Wei Wu",
                "Maosong Sun."
            ],
            "title": "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Xiyang Hu",
                "Cynthia Rudin",
                "Margo Seltzer."
            ],
            "title": "Optimal sparse decision trees",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2019
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438",
            "year": 2020
        },
        {
            "authors": [
                "Nathanael Jo",
                "Sina Aghaei",
                "Jack Benson",
                "Andr\u00e9s G\u00f3mez",
                "Phebe Vayanos."
            ],
            "title": "Learning optimal fair classification trees",
            "venue": "arXiv preprint arXiv:2201.09932.",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Alexander Rush"
            ],
            "title": "How many data points is a prompt worth",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Jens Lehmann",
                "Robert Isele",
                "Max Jakob",
                "Anja Jentzsch",
                "Dimitris Kontokostas",
                "Pablo N Mendes",
                "Sebastian Hellmann",
                "Mohamed Morsey",
                "Patrick Van Kleef",
                "S\u00f6ren Auer"
            ],
            "title": "Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia",
            "year": 2015
        },
        {
            "authors": [
                "Xin Li",
                "Dan Roth."
            ],
            "title": "Learning question classifiers",
            "venue": "COLING 2002: The 19th International Conference on Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "Robert Logan IV",
                "Ivana Balazevic",
                "Eric Wallace",
                "Fabio Petroni",
                "Sameer Singh",
                "Sebastian Riedel."
            ],
            "title": "Cutting down on prompts and parameters: Simple few-shot learning with language models",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Jieyi Long"
            ],
            "title": "Large language model guided tree-ofthought",
            "year": 2023
        },
        {
            "authors": [
                "P. Takala"
            ],
            "title": "Good debt or bad debt: Detecting",
            "year": 2014
        },
        {
            "authors": [
                "Barcelona",
                "Spain. Bo Pang",
                "Lillian Lee"
            ],
            "title": "Seeing stars: Exploit",
            "year": 2005
        },
        {
            "authors": [
                "Silviu Pitis",
                "Michael R Zhang",
                "Andrew Wang",
                "Jimmy Ba."
            ],
            "title": "Boosted prompt ensembles for large language models",
            "venue": "arXiv preprint arXiv:2304.05970.",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander M. Rush."
            ],
            "title": "Mini chain: A tiny library for coding with large language models",
            "venue": "https:// github.com/srush/MiniChain.",
            "year": 2023
        },
        {
            "authors": [
                "Elvis Saravia",
                "Hsien-Chi Toby Liu",
                "Yen-Hao Huang",
                "Junlin Wu",
                "Yi-Shin Chen."
            ],
            "title": "CARER: Contextualized affect representations for emotion recognition",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Fabrizio Sebastiani."
            ],
            "title": "Machine learning in automated text categorization",
            "venue": "ACM computing surveys (CSUR), 34(1):1\u201347.",
            "year": 2002
        },
        {
            "authors": [
                "Chandan Singh",
                "Armin Askari",
                "Rich Caruana",
                "Jianfeng Gao"
            ],
            "title": "Augmenting interpretable models with llms during training",
            "year": 2023
        },
        {
            "authors": [
                "Chandan Singh",
                "John X. Morris",
                "Jyoti Aneja",
                "Alexander M. Rush",
                "Jianfeng Gao"
            ],
            "title": "Explaining patterns in data with language models via interpretable autoprompting",
            "year": 2023
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Hendrik Strobelt",
                "Albert Webson",
                "Victor Sanh",
                "Benjamin Hoover",
                "Johanna Beyer",
                "Hanspeter Pfister",
                "Alexander M. Rush"
            ],
            "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models",
            "year": 2022
        },
        {
            "authors": [
                "Yan Shuo Tan",
                "Chandan Singh",
                "Keyan Nasseri",
                "Abhineet Agarwal",
                "Bin Yu."
            ],
            "title": "Fast interpretable greedy-tree sums (figs)",
            "venue": "arXiv:2201.11931 [cs, stat]. ArXiv: 2201.11931.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Alvin Wan",
                "Lisa Dunlap",
                "Daniel Ho",
                "Jihan Yin",
                "Scott Lee",
                "Suzanne Petryk",
                "Sarah Adel Bargal",
                "Joseph E. Gonzalez."
            ],
            "title": "NBDT}: Neural-backed decision tree",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Boshi Wang",
                "Xiang Deng",
                "Huan Sun."
            ],
            "title": "Iteratively prompt pre-trained language models for chain of thought",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2714\u20132730, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Janyce Wiebe",
                "Theresa Wilson",
                "Claire Cardie."
            ],
            "title": "Annotating expressions of opinions and emotions in language",
            "venue": "Language resources and evaluation, 39:165\u2013210.",
            "year": 2005
        },
        {
            "authors": [
                "Benfeng Xu",
                "Quan Wang",
                "Zhendong Mao",
                "Yajuan Lyu",
                "Qiaoqiao She",
                "Yongdong Zhang."
            ],
            "title": "$k$NN prompting: Beyond-context learning with calibrationfree nearest neighbor inference",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L. Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan"
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Chenrui Zhang",
                "Lin Liu",
                "Jinpeng Wang",
                "Chuyuan Wang",
                "Xiao Sun",
                "Hongyu Wang",
                "Mingchen Cai."
            ],
            "title": "Prefer: Prompt ensemble learning via feedbackreflect-refine",
            "venue": "arXiv preprint arXiv:2308.12033.",
            "year": 2023
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang"
            ],
            "title": "2023b. Summit: Iterative text summarization via chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Tianyuan Zhang",
                "Zhanxing Zhu."
            ],
            "title": "Interpreting adversarially trained convolutional neural networks",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 7502\u20137511.",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Charlie Snell",
                "Dan Klein",
                "Jacob Steinhardt."
            ],
            "title": "Describing differences between text distributions with natural language",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pretrained language models (LMs) have made remarkable progress in recent years (Vaswani et al., 2017; Brown et al., 2020; OpenAI, 2023), but their large size makes them difficult to fine-tune with gradients for specific downstream tasks. As such, prompting has become the main interface for applying pretrained language models (LMs), where task-specific instructions are provided to guide an LM\u2019s behavior. The most common way to adapt LMs is to use few-shot in-context examples, where input-output pairs are shown to the model.\nYet, few-shot prompting has a clear downside. Prompt expressiveness is limited by the context length of the language model. This constraint prevents using more than a handful of examples for few-shot in-context learning, particularly in memory-constrained environments. If there is additional supervised data available for a task, users need to either ensemble together many prompts or back off to alternative LM fine-tuning approaches.\n1*Equal contribution. Scikit-learn-compatible API for using Tree-Prompt is available at github.com/csinva/treeprompt.\nIn this work, we propose Tree Prompting as an alternative method for incorporating task supervision. The key idea is to use training data to form a decision tree based on simple prompt-LM calls, with each prompt determined by the outcomes of previous calls. The method does not change the parameters of the language model, but instead uses its outputs to determine an effective tree structure. To determine the prompts used at each node of the decision tree, we propose a simple bagginginspired approach that samples few-shot examples to find the most informative prompt (see Fig. 1). To convert LM outputs into split features for decision path determination, we consider both using a pre-defined verbalizer (Hu et al., 2022) and a more expressive kNN Prompting approach (Xu et al., 2023). To learn the tree structure, we employ a classic decision tree learning algorithm (Breiman et al., 1984). The constructed tree is a sparse representation of the fine-tuning data, incorporating a large number of few-shot examples, but only requiring a constant number of LM calls for inference.\nTree Prompting offers several advantages over existing prompting approaches. It allows users to\neasily incorporate large supervised training datasets without requiring larger contexts. It also allows experts to examine the decision-making process underlying a prediction in detail, which can be improved by combining with prompt generation methods. Finally, Tree Prompting can be adapted to be compatible with many existing LMs that are only accessible to the public via API calls. We demonstrate these advantages in experiments on multi-class classification benchmarks."
        },
        {
            "heading": "2 Background: Decision Trees",
            "text": "Decision trees are a classic model for classification and regression. They provide a graphical, intuitive model of decision-making, based on a cascading series of binary decisions2. At each node in the tree, a decision is made based on a single feature of the input, which leads to the next node, and ultimately to a leaf node representing a prediction.\nLearning Decision trees are constructed greedily in a top-down manner, starting from the root node, where all training data (x, y) and features \u03d5(x) \u2208 {0, 1}d are available. At each node, a feature that best splits the dataset into two subsets is chosen. The \u201cbest split\u201d is determined by a criterion that measures the quality of a split. A commonly used criterion is the Gini impurity from the CART algorithm (Breiman et al., 1984). The selected feature creates two child nodes, each containing the subset of data that satisfies the respective split condition. This process is repeated recursively for each child node with the corresponding subset of the data until a stopping condition is met3. Each leaf node in the final decision tree represents a decision (such as a class label prediction), determined by the majority class of the instances in the leaf.\nInference A decision tree makes predictions on unseen data by traversing the tree from the root node to a leaf node. Starting from the root, the feature value of the example corresponding to the split feature at the current node is used to determine whether the left child or the right child node is visited next. This process is repeated until a leaf node is reached. The class label associated with this leaf node is then used as the prediction.\n2We exclusively focus on binary trees in this paper. 3Stopping conditions include reaching a maximum number\nof leaf nodes or when no feature improves the split quality."
        },
        {
            "heading": "3 Tree Prompting",
            "text": "Tree Prompting utilizes decision trees as a method of adapting LMs to specific tasks without finetuning the model. Assuming access to a set of textlabel pairs (x, y), the goal is to determine a tree to best classify this data. The algorithm then proceeds in a top-down manner, where at each node, it selects the best prompt based on the chosen method for finding prompt candidates and constructing split features.\nHowever, unlike standard decision trees, we do not have access to a predetermined set of features \u03d5(x). Tree Prompting instead constructs this feature function dynamically by constructing prompts. The value of a feature \u03d5i(x) is determined by running a prompt through the LM and mapping its response to a binary value.\nA major benefit of utilizing decision trees in this setting is their efficiency at inference time. Constructing the tree lets us compactly represent a large amount of task-specific training examples. If each \u03d5i requires running one prompt, there are 2D features. At inference time, we only need D prompt calls to classify a single datapoint.\nOur primary approach to find prompts for features \u03d5i(x) is to select random few-shot examples drawn from the training data, as shown in Figure 1. We take inspiration from bagging approaches (Breiman, 1996) that combine random training samples to produce complementary parallel models. By sampling random x, y pairs from the task training data and passing them to an LM, we are effectively bagging small training splits. Each prompt is constructed by alternating classes with their corresponding labels in a templated form.\nOnce a prompt is constructed, the feature value is set using a pre-defined verbalizer to transform the LM\u2019s output (Hu et al., 2022). A verbalizer is a function that maps the LM\u2019s output probabilities into a discrete decision. A simple implementation of a verbalizer is to determine whether the predicted probability for the token Yes/No is higher. In this work, we experiment with two different verbalizers: the first maps the logits to class labels (such as Positive/Negative for binary sentiment classification), and the second more generic verbalizer maps the logits to Yes/No.\nWhen the output logits of the LM are inaccessible4, we can discretize the LM\u2019s outputs into\n4Some recent LMs such as OpenAI\u2019s GPT-3.5 and GPT-4 do not provide output logits.\ncategories defined by the verbalizer using word matching. With few-shot prompts, large LMs have empirically been found to respect the template format and output only labels that they have seen in the demonstrations most of the time (Brown et al., 2020)."
        },
        {
            "heading": "3.1 Extensions",
            "text": "Instruction Prompts To leverage expert insights on specific tasks, we can instead use human-curated prompt candidates (Bach et al., 2022) as shown in Fig. 2. To diversify and enrich the pool of prompt candidates, we leverage the capabilities of GPT3.5 to generate paraphrases of the original prompts. This method provides the ability to incorporate domain-specific knowledge, and the prompts are more interpretable compared to random few-shot examples. However, it might be less adaptable to novel or unique task specifications compared to the other automatic prompt candidate generation methods.\nDynamic Prompts Instead of pre-constructing random prompt-based features, we can generate dynamic prompts while building the decision tree. At each node, we conduct a discrete prompt search\nto identify a list of prompt candidates that best explain the subset of data at this node. The prompt that best splits this subset into two further subsets is then selected. The prompt search algorithm used in this paper is iPrompt (Singh et al., 2023b), which employs an LM to generate potential prompt candidates, ranking them based on how well they explain the data. Dynamic prompts offer enhanced flexibility and adaptability, at the cost of additional computation.\nkNN Prompting Features As a more expressive alternative to predefined verbalizers, we consider the kNN Prompting approach (Xu et al., 2023). kNN Prompting employs the label of the nearest neighbor in an anchor set as the split feature, with the distance measured in terms of the KL divergence between output probabilities. This approach allows for the use of a large number of examples at each node, extending beyond the limitations imposed by the restricted context window size, making it more expressive. A downside of this approach is its dependence on access to the LM\u2019s output logits. Moreover, as multiple prompts are utilized at each node of the decision tree, this can compromise the interpretability of the model."
        },
        {
            "heading": "Finetuning",
            "text": ""
        },
        {
            "heading": "ICL GPT-2 Small",
            "text": ""
        },
        {
            "heading": "ICL GPT-2 Medium",
            "text": ""
        },
        {
            "heading": "ICL GPT-2 Large",
            "text": ""
        },
        {
            "heading": "ICL GPT-2 XL",
            "text": ""
        },
        {
            "heading": "ICL GPT-J",
            "text": ""
        },
        {
            "heading": "ICL LLAMA-2 7B",
            "text": "Tree Ensembles Trees generated via Tree Prompting can be used to construct typical tree ensembles such as random forests (Breiman et al., 1984) or gradient-boosted trees (Freund et al., 1996) by using a Tree Prompting tree as the base estimator. This incurs very little overhead when using a fixed set of prompts, as the split features can be shared across all trees after being computed once."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Datasets We evaluate Tree Prompting on 13 text classification datasets. Among them are binary classification datasets SST2 (Socher et al., 2013), SUBJ (Pang and Lee, 2004; Wiebe et al., 2005), MPQA (Deng and Wiebe, 2015), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), RTE (Dagan et al., 2006), IMDB (Maas et al., 2011), and multi-class classification datasets AGNews (Zhang\net al., 2015), CB (De Marneffe et al., 2019), DBPedia (Zhang et al., 2015; Lehmann et al., 2015), TREC (Li and Roth, 2002; Hovy et al., 2001), FPB (Malo et al., 2014), and Emotion (Saravia et al., 2018). Appendix A.1 provides dataset statistics in Table 6, and examples in Table 7.\nModel Settings For the LM, we run experiments using five pretrained models: GPT-2 Small (117M parameters), GPT-2 Medium (355M parameters), GPT-2 Large (774M parameters), GPT-2 XL (1.5B parameters) (Radford et al., 2019), and GPTJ (Wang and Komatsuzaki, 2021) (6B parameters).\nBaselines We compare our approach Tree Prompting, TreePrompt, to standard fine-tuning. In addition, we also compare against a conventional prompting baseline FSPrompting, which directly uses few-shot example demonstrations as the prompt. We also compare the performance of\n0 10 # LM calls\n0.6\n0.7 0.8 RO C AU\nC Emotion\n0 10 # LM calls\n0.7\n0.8\nFPB\n0 10 # LM calls\n0.94\n0.96\nIMDB\n0 10 # LM calls\n0.85\n0.90\nMR\n0 10 # LM calls\n0.85\n0.90\n0.95\nSST2\nTreePrompt TreePrompt (GBDT) Ensemble (Greedy) Ensemble (Boosting)\nFigure 3: Performance as a function of the number of LM evaluations per example (#LM calls). We use GPT-J as the base LM and class names as verbalizers. GBDT, gradient-boosting tree using Tree Prompting as the base classifier, fitted to a maximum of 40 LM calls provides an upper bound of accuracy we can get on individual datasets.\nour ensembling approach, TreePrompt Ens5, with two baseline ensembling strategies: Greedy, which adds prompts to an ensemble in order of their crossvalidation accuracy), and Boosting, which adds prompts to an ensemble using AdaBoost (Freund and Schapire, 1997; Hou et al., 2022; Pitis et al., 2023)."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Classification Accuracy",
            "text": "Our main results are summarized in Table 1. The table compares Tree Prompting across multiple language model sizes to other few-shot prompting and ensembling approaches, as well as to gradientbased fine-tuning. Approaches are allotted a maximum of 40 LM inference calls per example.\nResults show that Tree Prompting outperforms basic few-shot prompting and also ensemblingbased approaches across model sizes and almost all datasets. The performance difference is particularly large across smaller model classes. For instance, while FSPrompting averages an accuracy of 44.3% with GPT-2 Small, Tree Prompting elevates this to 60.5%. Tree Prompting can also be ensembled, which produces accuracy improvements at the cost of more LM calls.\nWe also compare Tree Prompting to gradient based fine-tuning, particularly on GPT-2 Large. Results show that Tree Prompting is less stable than fine-tuning, performing poorly on some tasks, but outperforms it on 5 of 10 tasks. This result shows that Tree Prompting can learn well from task supervision at the cost of additional runtime queries. (Tree Prompting could likely perform better compared to fine-tuning if we increased the maximum number of prompts beyond 40.)\n5This ensemble uses scikit-learn\u2019s GradientBoostingClassifier with its default parameters: 100 trees, each with max depth 3, with a learning rate of 0.1.\nRelative to the baselines, Tree Prompting is typically able to outperform them all while making fewer queries than Greedy and Boosting. Tree Prompting makes large improvements over fewshow prompting in most cases, even when the model size is large. We observe a failure of the boosting strategy when the number of classes is large (specifically for 14-class DBPedia). Tree Prompting with gradient boosting generally gives an increase at performance at the cost of a 5.7-times increase in queries."
        },
        {
            "heading": "5.2 Inference Efficiency",
            "text": "As computing outputs from language models can be costly, particularly with large LMs, the efficiency of Tree Prompting at inference time is crucial. In Fig. 3, we plot test accuracy against the number of language model evaluations per example (#LM Calls) to gauge this efficiency6. Tree Prompting frequently surpasses competing ensemble strategies in performance under the same number of LM calls, indicating an improvement in efficiency. This gain is more significant for multiclass datasets, such as Emotion and Financial phrasebank (FPB).\nTo establish an upper bound of accuracy, we consider Tree Prompting ensembling. This approach generally achieves the best test performance across all methods, although it also demands more LM calls than a single tree (up to 40 calls)."
        },
        {
            "heading": "5.3 Interpretability and Dynamic Prompts",
            "text": "A practical benefit of decision trees is increased interpretability. Each node of the decision tree can be inspected, offering insights into the decisionmaking process when predicting the label of a given input. Our few-shot approach for Tree Prompting\n6The mean number of LM calls may differ from the max depth of a tree, as reaching a leaf node can require fewer calls in an unbalanced tree.\nis challenging to interpret, but we can instead use interpretable prompts that are human-curated or dynamically constructed.\nFig. 2 demonstrates an instance of a decision tree learned from human-curated prompts on the Emotion dataset, where different colors represent the true labels of the data. At the root node, a high-level query is posed regarding whether the tweet\u2019s underlying emotion is love. Deeper in the tree, more granular questions are presented, e.g. whether the sentiment of the sentence is anger.\nDynamic prompts offer an additional advantage over human-curated prompts; they are capable of better reflecting the specific subset of data at each node, making the decision process more aligned with the data distribution at each tree node. Fig. 4 shows a tree learned using iPrompt to create dynamic prompts at each node of the tree. Prompts are suggested by GPT-4 and reranked according to the iPrompt algorithm with the verbalizer Yes/No corresponding to the positive/negative classes of\nthe MR dataset.\n5.4 Comparison with kNN Prompting\nNonparametric methods like kNN Prompting (Xu et al., 2023) can be employed to improve model expressivity, which allows using multiple prompts per node and avoids the reliance on pre-defined verbalizers. Table 2 provides a comparison between Tree Prompting and kNN Prompting. In this comparison, Tree Prompting uses kNN Prompting predictions as split features7. The results show that Tree Prompting outperforms vanilla kNN Prompting on most datasets, potentially due to its added flexibility of partitioning the input space using the decision tree, although it underperforms on three of the smaller datasets CB (250 training examples), CR (1.77k training examples), and TREC (5.44k training examples)."
        },
        {
            "heading": "5.5 Comparison to Larger LMs",
            "text": "Tree Prompting allows enhancing the performance of small LMs to match the performance of large LMs, as shown in Table 3. For these experiments instead of few-shot prompting we use instruction prompts curated from PromptSource (Bach et al., 2022)8. In this setting, even GPT-2 Small paired with Tree Prompting Ensemble is competitive against GPT-3 (text-davinci-003), outperforming it on two datasets (FPB and MR), albeit being slightly worse on the other two datasets (IMDB and SST2). With the larger LM GPT-J, Tree Prompting outperforms GPT-3 with conventional prompting across all datasets, demonstrating the potential of using a smaller model in a decision tree repeatedly to outperform a larger model, which might be useful in resource-constrained scenarios.\n7We binarize kNN Prompting predictions, which are multiclass labels, into multiple split features (each evaluating whether the output matches a certain class).\n8Initial experiments showed that instruction prompts outperform few-shot prompts for GPT-3."
        },
        {
            "heading": "6 Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Verbalizer Sensitivity",
            "text": "Table 4 shows the robustness of different approaches when employing a generic Yes/No verbalizer versus a class-name verbalizer. The results show that Tree Prompting consistently outperforms the baseline regardless of the verbalizer used, delivering decent performance even when using the generic Yes/No verbalizer. This feature could be useful in applications where class names are not meaningful words, such as in distinguishing between texts generated by different decoding settings (Naseh et al., 2023). Table 8 in Appendix A.3 shows full performance sensitivity results across different settings for the underlying LM, verbalizer,\nand source of prompts."
        },
        {
            "heading": "6.2 Prompt Source Sensitivity",
            "text": "Table 5 examines the sensitivity of various approaches to the source of prompt candidates. The comparison between using instruction prompts and few-shot prompts demonstrates that Tree Prompting consistently outperforms baselines regardless of the source of prompt candidates. It\u2019s worth noting that instruction prompts generally result in better performance than few-shot prompts, corroborating previous findings that in-context learning with a single prompt can work as well as multiple data demonstrations (Le Scao and Rush, 2021). However, curating instruction prompts requires extra human effort, since new prompts must be written for each new dataset."
        },
        {
            "heading": "6.3 Sample Complexity",
            "text": "Fig. 5 visualizes the performance of Tree Prompting in relation to the fraction of training samples used for training. When compared to baseline ensembling techniques, Tree Prompting sometimes underperforms in low-data regimes (on FPB, IMDB, and MR), but it eventually outperforms baselines as more training data is available."
        },
        {
            "heading": "7 Related Work",
            "text": "Prompting Language Models The rise of large language models (LMs) has led to a surge in the development of effective prompting methods (Strobelt et al., 2022; Lu et al., 2022; Bach et al., 2022; Logan IV et al., 2022; Zhong et al., 2022; Singh et al., 2023b). Building on top of these methods, emsembling techniques for averaging multiple LM calls have shown that they often improve performance (Jiang et al., 2020; Zhang et al., 2023a), e.g. boosting (Hou et al., 2022; Pitis et al., 2023). Chain prompting (Wang et al., 2022; Press et al., 2022; Chase, 2023; Rush, 2023) is a widely used method that divides complex tasks into manageable subtasks, linking these via prompt-LM calls.\nThis approach has proven effective across various applications, aligning with our intuition underlying this work: an LM can handle individual steps of a task more accurately than executing the task in full (Ma et al., 2023; Madaan et al., 2023; Zhang et al., 2023b). However, while chain prompting links prompt-LM calls, our approach organizes them within a decision tree, learning the tree structure and selecting appropriate prompts for each node.\nFrugal GPT (Chen et al., 2023) also bears relevance to our work, proposing a cascade of LMs that stops when an intermediate output is considered reliable, resulting in better computational efficiency. Viewed from the perspective of decision trees, this approach resembles a right-branching decision tree.\nConcurrent to our work, Tree of Thoughts (Yao et al., 2023; Long, 2023) organizes LM-generated \u201cthoughts\u201d within a tree structure for solution search. While we also use a tree structure, our aim is to partition the input space to simplify the LM\u2019s tasks at lower tree levels. We search the tree\u2019s structure and the prompt at each node during training, while keeping these elements static during inference. In contrast, Tree of Thoughts adjusts node prompts dynamically based on upper-level results. This sets it apart from our approach, where prompts remain constant post-training. Collectively, these works demonstrate the growing interest in merging tree structures with LMs for task decomposition, albeit with varied focuses and methodologies.\nDecision Tree Applications Dating back decades, decision trees have been a prevalent choice in the realms of classification and regression problems (Costa and Pedreira, 2022). In the field of natural language processing, decision trees and their ensemble variants such as Random Forest (Breiman, 2001), Gradient-boosted Trees (Freund et al., 1996), XGBoost (Chen and\nGuestrin, 2016), and BART (Chipman et al., 2010) have found use in areas like part-of-speech tagging (Magerman, 1995), syntactic parsing (Collins, 1997), and text classification (Sebastiani, 2002; Singh et al., 2023a). However, these studies predominantly utilize pre-defined textual features within their decision tree frameworks, contrasting our approach where the decision tree is used to direct the language model\u2019s behavior.\nDecision Trees for Interpretability Decision trees have also been applied to increase the interpretability of neural models. For example, Wan et al. (2021) used a decision tree structure where each node is a neural classifier for image classification. Zhang and Zhu (2019) learned a decision tree to explain the decisions made by an image classifier post hoc. While these works primarily target visionbased applications, we adopt a similar strategy for natural language processing, where each node in our decision tree embodies a distinct prompt-LM call. Furthermore, our dynamic prompt setting enables the concurrent learning of prompts and the decision tree structure, distinguishing our method from conventional decision tree applications that function within a pre-defined feature space."
        },
        {
            "heading": "8 Conclusions and Future Work",
            "text": "We introduce the Tree Prompting approach, a use of decision trees for task adaptation. Experiments demonstrate that Tree Prompting can offer improved performance across various text classification tasks while still remaining efficient during inference. On many tasks, the model is competitive with gradient fine-tuning. Additionally, the approach can be used with dynamic prompt creation to yield interpretable models.\nOur results suggest a future direction of exploring a flexible and modularized assembly of models. One exciting direction is to extend Tree Prompting\nto generalize to tasks beyond text classification, using previous outputs to guide subsequent prompts and LMs. Further exploration could involve extending Tree Prompting to jump across nodes in the tree (similar to Long (2023)) or introduce cycles in the tree (similar to Besta et al. (2023)), and ultimately developing a program of prompts by navigating various nodes in a decision tree as though calling different functions. Another direction could explore incorporating different criteria into the tree-building algorithm, e.g. fairness (Jo et al., 2022), sparsity (Hu et al., 2019; Tan et al., 2022), or smoothness (Agarwal et al., 2022)."
        },
        {
            "heading": "9 Limitations",
            "text": "Sample Complexity While Tree Prompting\u2019s adaptability and flexibility are its strengths, they also contribute to its higher sample complexity. As shown in Sec. 6.3, Tree Prompting lags behind fewshot prompting in low-data environments. Decision trees inherently risk overfitting, particularly when dealing with numerous features. This shortcoming can be partially offset through the use of larger training sets, and by restricting the tree\u2019s size in relation to the training set size.\nTraining Cost Although Tree Prompting demands fewer LM calls during inference compared to analogous techniques, its training process, which involves learning the decision tree, requires computing prompt features for every example in the associated data subset at each node. This can be resource-intensive for large LMs. Additionally, when paired with dynamic prompts that leverage automatic prompting methods (which are typically computation-heavy), the training process can be substantially expensive as each node necessitates running the autoprompting method once.\nInterpretability While decision trees are typically celebrated for their interpretability, the interpretability of Tree Prompting is bounded by the nature of the prompts and the verbalizer. Specifically, when employing a pre-defined prompt, its interpretability may not be as intuitive as that of dynamic prompts. If the prompt itself (such as when using few-shot demonstrations) lacks interpretability, the entire decision tree\u2019s interpretability is likely to be compromised."
        },
        {
            "heading": "10 Acknowledgements",
            "text": "JXM is supported by an NSF GRFP. YD is supported by an Nvidia Fellowship and NSF 2242302. AMR is supported by NSF 2242302, NSF CAREER 2037519, and a Sloan Fellowship. We would also like to thank Harvard University FAS Research Computing for providing computational resources."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Data",
            "text": "Table 6 presents dataset statistics, and Table 7 shows example input, output pairs from each dataset."
        },
        {
            "heading": "A.2 Tree Visualizations",
            "text": "Appendix A.2 shows another example tree learned on the MR dataset."
        },
        {
            "heading": "A.3 Full Ablation Results",
            "text": "Full ablation results for different choices of prompts and verbalizers can be found in Table 8."
        }
    ],
    "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
    "year": 2023
}