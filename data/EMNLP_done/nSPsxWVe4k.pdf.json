{
    "abstractText": "The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training. Structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6%, while a structure-aware parser only achieves 70.8%. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models\u2019 lexical and structural generalization capacities.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bingzhi Li"
        },
        {
            "affiliations": [],
            "name": "Lucia Donatelli"
        },
        {
            "affiliations": [],
            "name": "Alexander Koller"
        },
        {
            "affiliations": [],
            "name": "Tal Linzen"
        },
        {
            "affiliations": [],
            "name": "Yuekun Yao"
        },
        {
            "affiliations": [],
            "name": "Najoung Kim"
        }
    ],
    "id": "SP:c683949760f00df6426930c7682c0782c1a6f66f",
    "references": [
        {
            "authors": [
                "Ekin Akyurek",
                "Jacob Andreas."
            ],
            "title": "Lexicon learning for few shot sequence modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Cem Anil",
                "Yuhuai Wu",
                "Anders Johan Andreassen",
                "Aitor Lewkowycz",
                "Vedant Misra",
                "Vinay Venkatesh Ramasesh",
                "Ambrose Slone",
                "Guy Gur-Ari",
                "Ethan Dyer",
                "Behnam Neyshabur."
            ],
            "title": "Exploring length generalization in large language models",
            "venue": "Advances in",
            "year": 2022
        },
        {
            "authors": [
                "Emily Atkinson",
                "Matthew W. Wagers",
                "Jeffrey Lidz",
                "Colin Phillips",
                "Akira Omaki."
            ],
            "title": "Developing incrementality in filler-gap dependency processing",
            "venue": "Cognition, 179:132\u2013149.",
            "year": 2018
        },
        {
            "authors": [
                "Enes Avcu",
                "Chihiro Shibata",
                "Jeffrey Heinz."
            ],
            "title": "Subregular complexity and deep learning",
            "venue": "CLASP Papers in Computational Linguistics, page 20.",
            "year": 2017
        },
        {
            "authors": [
                "Thea Cameron-Faulkner",
                "Elena Lieven",
                "Michael Tomasello."
            ],
            "title": "A construction based analysis of child directed speech",
            "venue": "Cognitive science, 27(6):843\u2013 873.",
            "year": 2003
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "Syntactic Structures",
            "venue": "Janua linguarum (Mouton, Paris).: Series Minor. Mouton.",
            "year": 1957
        },
        {
            "authors": [
                "Morten H. Christiansen",
                "Nick Chater."
            ],
            "title": "Toward a connectionist model of recursion in human linguistic performance",
            "venue": "Cognitive Science, 23(2):157\u2013205.",
            "year": 1999
        },
        {
            "authors": [
                "R\u00f3bert Csord\u00e1s",
                "Kazuki Irie",
                "Juergen Schmidhuber."
            ],
            "title": "The devil is in the detail: Simple tricks improve systematic generalization of transformers",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 619\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Nathanael Sch\u00e4rli",
                "Ekin Aky\u00fcrek",
                "Nathan Scales",
                "Xinying Song",
                "Xinyun Chen",
                "Olivier Bousquet",
                "Denny Zhou."
            ],
            "title": "Compositional semantic parsing with large language models",
            "venue": "arXiv:2209.15003.",
            "year": 2022
        },
        {
            "authors": [
                "W Tecumseh Fitch",
                "Marc D Hauser."
            ],
            "title": "Computational constraints on syntactic processing in a nonhuman primate",
            "venue": "Science, 303(5656):377\u2013380.",
            "year": 2004
        },
        {
            "authors": [
                "Jerry A Fodor",
                "Zenon W Pylyshyn."
            ],
            "title": "Connectionism and cognitive architecture: A critical analysis",
            "venue": "Cognition, 28(1-2):3\u201371.",
            "year": 1988
        },
        {
            "authors": [
                "Edward Gibson",
                "James Thomas."
            ],
            "title": "Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical",
            "venue": "Language and Cognitive Processes, 14(3):225\u2013248.",
            "year": 1999
        },
        {
            "authors": [
                "Johannes Gontrum",
                "Jonas Groschwitz",
                "Alexander Koller",
                "Christoph Teichmann."
            ],
            "title": "Alto: Rapid prototyping for parsing and translation",
            "venue": "Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Com-",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Groschwitz",
                "Meaghan Fowlie",
                "Alexander Koller."
            ],
            "title": "Learning compositional structures for semantic graph parsing",
            "venue": "Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021), pages 22\u201336, Online. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Groschwitz",
                "Matthias Lindemann",
                "Meaghan Fowlie",
                "Mark Johnson",
                "Alexander Koller."
            ],
            "title": "AMR dependency parsing with a typed semantic algebra",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2018
        },
        {
            "authors": [
                "Jiaqi Guo",
                "Zecheng Zhan",
                "Yan Gao",
                "Yan Xiao",
                "JianGuang Lou",
                "Ting Liu",
                "Dongmei Zhang."
            ],
            "title": "Towards complex text-to-SQL in cross-domain database with intermediate representation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034.",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kelvin Guu",
                "Panupong Pasupat",
                "Yuan Zhang."
            ],
            "title": "Unlocking compositional generalization in pre-trained models using intermediate representations",
            "venue": "arXiv preprint arXiv:2104.07478.",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Verna Dankers",
                "Mathijs Mul",
                "Elia Bruni"
            ],
            "title": "Compositionality decomposed: How do neural networks generalise",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2020
        },
        {
            "authors": [
                "Fred Karlsson."
            ],
            "title": "Constraints on multiple centerembedding of clauses",
            "venue": "Journal of Linguistics, 43(2):365\u2013392.",
            "year": 2007
        },
        {
            "authors": [
                "Fred Karlsson."
            ],
            "title": "Syntactic recursion and iteration",
            "venue": "Recursion and human language, pages 43\u201367.",
            "year": 2010
        },
        {
            "authors": [
                "Najoung Kim",
                "Tal Linzen."
            ],
            "title": "COGS: A compositional generalization challenge based on semantic interpretation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9087\u20139105, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Najoung Kim",
                "Tal Linzen",
                "Paul Smolensky."
            ],
            "title": "Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models",
            "venue": "arXiv:2212.10769.",
            "year": 2022
        },
        {
            "authors": [
                "Brenden Lake",
                "Marco Baroni."
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "International conference on machine learning, pages 2873\u20132882. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Yair Lakretz",
                "Th\u00e9o Desbordes",
                "Jean-R\u00e9mi King",
                "Beno\u00eet Crabb\u00e9",
                "Maxime Oquab",
                "Stanislas Dehaene"
            ],
            "title": "Can rnns learn recursive nested subject-verb agreements? arXiv preprint arXiv:2101.02258",
            "year": 2021
        },
        {
            "authors": [
                "Bingzhi Li",
                "Guillaume Wisniewski",
                "Beno\u00eet Crabb\u00e9."
            ],
            "title": "Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2023
        },
        {
            "authors": [
                "Matthias Lindemann",
                "Jonas Groschwitz",
                "Alexander Koller."
            ],
            "title": "Fast semantic parsing with welltypedness guarantees",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3929\u20133951, On-",
            "year": 2020
        },
        {
            "authors": [
                "Chenyao Liu",
                "Shengnan An",
                "Zeqi Lin",
                "Qian Liu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Lijie Wen",
                "Nanning Zheng",
                "Dongmei Zhang."
            ],
            "title": "Learning algebraic recombination for compositional generalization",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Puyuan Liu",
                "Chenyang Huang",
                "Lili Mou."
            ],
            "title": "Learning non-autoregressive models from search for unsupervised sentence summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Abhijit Mahalunkar",
                "John Kelleher."
            ],
            "title": "Multielement long distance dependencies: Using SPk languages to explore the characteristics of long-distance dependencies",
            "venue": "Proceedings of the Workshop on Deep Learning and Formal Languages: Building",
            "year": 2019
        },
        {
            "authors": [
                "Rebecca Marvin",
                "Tal Linzen."
            ],
            "title": "Targeted syntactic evaluation of language models",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192\u20131202, Brussels, Belgium. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Richard Thomas McCoy",
                "Jennifer Culbertson",
                "Paul Smolensky",
                "G\u00e9raldine Legendre."
            ],
            "title": "Infinite use of finite means? evaluating the generalization of center embedding learned from an artificial grammar",
            "venue": "Proceedings of the Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Richard Montague."
            ],
            "title": "English as a formal language",
            "venue": "Richmond H. Thomason, editor, Formal Philosophy: Selected Papers of Richard Montague, pages",
            "year": 1974
        },
        {
            "authors": [
                "Barbara H. Partee."
            ],
            "title": "Compositionality",
            "venue": "F. Landman and F. Veltman, editors, Varieties of Formal Semantics, pages 281\u2013311. Dordrecht: Foris.",
            "year": 1984
        },
        {
            "authors": [
                "Amy Perfors",
                "Joshua B. Tenenbaum",
                "Terry Regier."
            ],
            "title": "The learnability of abstract syntactic principles",
            "venue": "Cognition, 118(3):306\u2013338.",
            "year": 2011
        },
        {
            "authors": [
                "Linlu Qiu",
                "Peter Shaw",
                "Panupong Pasupat",
                "Pawel Nowak",
                "Tal Linzen",
                "Fei Sha",
                "Kristina Toutanova."
            ],
            "title": "Improving compositional generalization with latent structure and data augmentation",
            "venue": "Proceedings of the 2022 Conference of the North Ameri-",
            "year": 2022
        },
        {
            "authors": [
                "Linlu Qiu",
                "Peter Shaw",
                "Panupong Pasupat",
                "Tianze Shi",
                "Jonathan Herzig",
                "Emily Pitler",
                "Fei Sha",
                "Kristina Toutanova."
            ],
            "title": "Evaluating the impact of model scale for compositional generalization in semantic parsing",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Siva Reddy",
                "Oscar T\u00e4ckstr\u00f6m",
                "Slav Petrov",
                "Mark Steedman",
                "Mirella Lapata."
            ],
            "title": "Universal semantic parsing",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 89\u2013101, Copenhagen, Denmark. Association",
            "year": 2017
        },
        {
            "authors": [
                "Douglas Roland",
                "Frederic Dick",
                "Jeffrey L Elman."
            ],
            "title": "Frequency of basic english grammatical structures: A corpus analysis",
            "venue": "Journal of memory and language, 57(3):348\u2013379.",
            "year": 2007
        },
        {
            "authors": [
                "Ivan A Sag."
            ],
            "title": "English filler-gap constructions",
            "venue": "Language, pages 486\u2013545.",
            "year": 2010
        },
        {
            "authors": [
                "Michael Tomasello",
                "Raquel Olguin."
            ],
            "title": "Twentythree-month-old children have a grammatical category of noun",
            "venue": "Cognitive Development, 8(4):451\u2013464.",
            "year": 1993
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Pia Wei\u00dfenhorn",
                "Lucia Donatelli",
                "Alexander Koller."
            ],
            "title": "Compositional generalization with a broadcoverage semantic parser",
            "venue": "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 44\u201354, Seattle, Washington. Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Gordon Wells",
                "Allayne Bridges."
            ],
            "title": "Learning through interaction: volume 1: the study of language development, volume 1",
            "venue": "Cambridge University Press.",
            "year": 1981
        },
        {
            "authors": [
                "Ethan Wilcox",
                "Roger Levy",
                "Takashi Morita",
                "Richard Futrell"
            ],
            "title": "What do RNN language models learn about filler\u2013gap dependencies",
            "venue": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2018
        },
        {
            "authors": [
                "Yuk Wah Wong",
                "Raymond Mooney."
            ],
            "title": "Learning synchronous grammars for semantic parsing with lambda calculus",
            "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960\u2013967, Prague, Czech Republic.",
            "year": 2007
        },
        {
            "authors": [
                "Zhengxuan Wu",
                "Christopher D Manning",
                "Christopher Potts."
            ],
            "title": "Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation",
            "venue": "arXiv preprint arXiv:2303.13716.",
            "year": 2023
        },
        {
            "authors": [
                "Yuekun Yao",
                "Alexander Koller."
            ],
            "title": "Structural generalization is hard for sequence-to-sequence models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5048\u20135062, Abu Dhabi, United Arab Emirates. As-",
            "year": 2022
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Disentangled sequence to sequence learning for compositional generalization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4256\u20134268, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "2023); Yao and Koller (2022), who noted that the decoder of Transformer models tends to exhibit a heavy bias towards generating observed n-grams",
            "year": 2022
        },
        {
            "authors": [
                "Lindemann"
            ],
            "title": "2020) can also predict non-projective trees, but uses a different probability model that is incompatible with the training algorithm of Groschwitz et al",
            "year": 2021
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "2023), exhibits certain limitations and ambiguities which render direct comparisons with variable-based LF results inappropriate",
            "venue": "The variable-free LF,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Compositional generalization benchmarks that test the ability to understand novel utterances based on composition of known parts (Montague, 1974; Partee, 1984; Fodor and Pylyshyn, 1988) have emerged as a useful tool for model evaluation in semantic parsing. COGS (Kim and Linzen, 2020) in particular has become a widely-used benchmark, as it is designed to expose a generalization gap between training and testing data that many recent semantic parsers still struggle with.\nCOGS distinguishes two distinct types of generalization challenges: lexical generalization tests a model\u2019s ability to interpret novel combinations of known lexical items and known linguistic structures (Figure 1a), whereas structural generalization\n\u2217This work was conducted during Bingzhi Li\u2019s visit to NYU. The middle authors are listed in alphabetical order.\nS\nNP\nEmma\nVP\nV\nsaw\nNP\nthe dog\nS\nNP\nThe cat\nVP\nV\nran\nS\nNP\nThe dog\nVP\nV\nran\n+ ;\n(a) Lexical generalization: object \u2192 subject (COGS)\nS\nNP\nEmma\nVP\nV\nsaw\nNP\nNP\nthe dog\nRC\nthat Max held _\nS\nNP\nThe cat\nVP\nV\nran\nS\nNP\nNP\nThe dog\nRC\nthat Max saw _\nVP\nV\nran\n+ ;\n(b) Structural generalization: RC object\u2192RC subject (SLOG)\nFigure 1: Examples of lexical generalization in COGS (a), and structural generalization in SLOG (b). The SLOG task requires mapping the generalization examples to their logical forms; the corresponding logical forms are shown in Table 1.\ntests the ability to combine known structures into a novel structure (Figure 1b). Importantly, most of the generalization types in COGS are lexical generalization (18 out of 21 generalization types, 86% of the dataset). As lexical generalization is arguably easier than structural generalization (e.g., solvable by simple slot-filling), this imbalance may lead to overall performance numbers that are overly optimistic with regard to a model\u2019s capacity to generalize compositionally (Yao and Koller, 2022).\nTo facilitate a more comprehensive evaluation of structural generalization, we introduce SLOG, a Structural LOng-distance dependencies Generalization benchmark. SLOG extends COGS to include 17 cases of structural generalization in total (14 new cases and 3 existing cases from COGS) (\u00a72). The novel generalizations we introduce target two key structural features of human language (\u00a73): recursion and filler-gap dependencies.\nWe use SLOG to evaluate a sequence-tosequence (seq2seq) Transformer model trained\nfrom scratch (Vaswani et al., 2017), two pretrained Transformers (T5-base; Raffel et al. 2020 and LLaMA; Touvron et al. 2023), and a structureaware1 model (AM-Parser; Wei\u00dfenhorn et al. 2022). In comparison to their overall performance on COGS, all models exhibit considerably lower performance on SLOG (\u00a75). An error analysis reveals that the structure-aware AM-Parser generalizes well on the existing structural generalization cases in COGS but struggles with the gap constructions introduced in SLOG due to inherent structural limitations, which we discuss in \u00a75.3. Transformers tend to erroneously repeat frequent meaning representation subsequences observed during training. Even with pretraining, they struggle with unseen long-distance dependencies, which we attribute to their bias towards shorter predicate-argument dependencies. Overall, the discrepancy in performance between SLOG and COGS demonstrates the utility of SLOG in exposing the overall limitations of current semantic parsing models shown to achieve high performance on existing generalization benchmarks, as well as highlighting the different weaknesses of these models."
        },
        {
            "heading": "2 The SLOG Benchmark",
            "text": "SLOG follows the semantic parsing format used in COGS, where the task is to translate English expressions into logic-based meaning representations. As in COGS, there is a systematic gap between the training set and the generalization set: there are constructions in the generalization set that are not included in the training set, but pieces of constructions included in the training set can be recombined to arrive at their correct meanings. For example, as illustrated in Table 1, noun phrases that appear\n1In this paper, \u2018structure-aware\u2019 refers specifically to models that incorporate explicit representations of linguistic structure.\nonly in object position during training must be reinterpreted in subject position during generalization.\nSLOG2 is generated using manually specified rules (\u00a73), adopting the same meaning representation as COGS. The COGS logical form (LF), derived from Reddy et al. (2017), uses indexed constants to represent entities or events. For example, in the first example of Table 1, x3 denotes an entity that is both a dog and the theme of a seeing event, while x1 denotes the seeing event. The constant names are determined by the linear position of the phrasal head in the input sentence.\nSLOG contains 17 structural generalization cases grouped into four categories. These generalization cases are primarily motivated by frequency asymmetries in natural language, where simpler structures are more common than complex ones; in other words, SLOG assesses whether NLP models can extrapolate from frequent patterns to rare ones.\nWe describe the four categories below; see Table 2 for the full list of generalization cases."
        },
        {
            "heading": "2.1 Novel Recursion Depth",
            "text": "Recursion allows smaller phrases to be combined to create larger phrases. This combination process can be repeated an unbounded number of times. COGS tests a model\u2019s ability to apply recursion in two cases: sentential complements (tail CP recursion)3 and nominal prepositional phrase modifiers (tail PP recursion). For both cases, the training set contains recursive depths of 0\u20132, where 0 indicates the absence of any PP or CP, and the generalization set contains the strictly greater depths 3\u201312.\nBy contrast, the SLOG training set includes recursion of depths 0\u20132 and 4, and the generalization\n2The generation code and SLOG dataset are available at https://github.com/bingzhilee/SLOG.\n3Nested clauses with right-branch embeddings like [Max knows that [Mary knows [that Emma cooks]CP ]CP ]CP\nset contains both the intermediate depth 3 and the greater depths 5\u201312. Including both shallower and deeper embeddings in the generalization set allows us to determine if any difficulty in generalizing to an unseen embedding depth is a consequence of the model\u2019s more general difficulty in processing longer sequences than observed in training (Lake and Baroni, 2018; Herzig et al., 2021; Anil et al., 2022) rather than a more specific issue with applying recursion to generate novel constructions.\nIn addition to this new depth split, SLOG introduces a new recursion construction. COGS involves only tail recursion, which features recursive PPs and CPs with right-branch embeddings. SLOG extends this with center embedding, where a phrase is embedded in the middle of another of the same type, leaving elements on both sides of the embedded component and producing well-parenthesized long-distance dependencies, as denoted by the subscript numbers:\n(1) Eva saw the mouse [that the cat1 [ that the dog2 chased2 ] held1 ].\nAt the same recursion depths, the average LF length increases from PP recursion to tail CP recursion to center embedding.\nIn natural language, recursion depth is rarely greater than five, and center embedding is generally limited to two levels (Karlsson, 2007, 2010). By contrast, SLOG includes recursion up to depth 12. While this may surpass human processing abilities for reasons presumed to be linked to memory constraints (Gibson and Thomas, 1999; Karlsson, 2007), deeper embedding depth remains grammatical, echoing Chomsky\u2019s competence versus performance distinction. Importantly, we also note that our goal with SLOG is to evaluate the linguistic competence of NLP models, whose goal is not to simulate human performance limitations."
        },
        {
            "heading": "2.2 Novel Combination of Modified Phrases and Grammatical Roles",
            "text": "SLOG also tests the capacity to interpret complex noun phrases (NPs) in new positions. In addition to PP modifiers included in COGS, we introduce relative clause modifiers."
        },
        {
            "heading": "2.2.1 Prepositional Phrase Modifiers",
            "text": "In COGS, NPs modified by PPs are seen only as direct objects (2), and need to be interpreted as subjects during generalization (3). SLOG adds generalization cases targeting indirect object modification (4).\n(2) Noah saw [a cat on the table]dobj . (3) [The cat on the mat]subj ran. (4) Emma gave [a cat on the mat]iobj a fish.\nWe expect sub-cases of indirect object modification to pose challenges of varying difficulty, depending on the length of the predicate-argument dependency. In particular, generalization to indirect object modification in active oblique datives (4) introduces a dependency between the verb gave and the direct object a fish across the non-argument NP the mat.4 In contrast, sub-cases like (5a) and (5b), where the non-argument NP occurs at the end of the sentence, do not include a dependency across an intervening NP; we therefore expect them to be relatively easier.\n(5) a. Emma gave a fish to [a cat on the mat]iobj .\nb. A fish was given to [a cat on the mat]iobj .\nSLOG\u2019s training set additionally includes standalone PP-modified NPs (e.g., the NP the cat on the table on its own5) to prevent modifiers from being associated with only a particular range of token indices, as pointed out by Wu et al. (2023).6 Such standalone NPs, which are common in child-directed speech (Wells and Bridges, 1981; Cameron-Faulkner et al., 2003) but were not a part of COGS, serve as a signal that the distribution of PP-modified NPs is not restricted to the object position."
        },
        {
            "heading": "2.2.2 Relative Clause Modifiers",
            "text": "Similar to PP modifiers, NPs with relative clause (RC) modifiers, as in (6), can occupy any position that an unmodified NP can fill. We expect RC modifiers to pose a greater challenge compared to PP modifiers, as they involve gap constructions, in which a phrase needs to be interpreted in a position other than its canonical position in a declarative clause. We refer to this as extraction (Sag, 2010), and we mark gap positions with an underscore. In (6), the dog should be interpreted as if it occupies the gap position as the direct object of held; in the logical form, this is represented by the fact that x3 is filling both see.theme and hold.theme.\n4This observation also holds true for the generalization to subject modification shown in (3).\n5the cat on the table ; *cat(x1); *table(x4); cat.nmod.on(x1, x4)\n6PPs in COGS were restricted to the object position, so models never observed the association of modifiers with linearly-earlier indices, which makes it difficult to isolate this effect from structural generalization.\n(6) Emma saw the dog that Max held __. ; *dog(x3); see.agent(x1, Emma)\n\u2227 see.theme(x1, x3) \u2227 dog.nmod(x3, x6) \u2227 hold.agent(x6, Max) \u2227 hold.theme(x6, x3)\nSimilar to the case of the PP modifiers (\u00a72.2.1), the training set contains direct object NPs modified by RCs as well as standalone RC-modified NPs, as in (7). The generalization set contains RC modifiers for subject NPs, as in (8a), and indirect object NPs, as in (8b):\n(7) TRAINING a. Liam saw [the cat that Emma held\n__]dobj . b. the cat that Liam fed __\n(8) GENERALIZATION a. [The cat that Emma found __]subj\nsmiled. b. Liam gave [a cat that Emma held\n__]iobj a fish."
        },
        {
            "heading": "2.3 Novel Gap Positions",
            "text": "The SLOG training set contains both subject and direct object extraction in RCs (9); these are the most frequent extraction positions in both written and spoken English corpora (Roland et al., 2007). The generalization set includes extraction of indirect objects (10), a less frequent construction.\n(9) TRAINING a. Liam saw the boy that ate a cake. b. Liam saw the boy that Emma loved __\n(10) GENERALIZATION a. Liam saw the boy that Emma gave a\ncake to __ .\nSLOG also tests for the interpretation of novel gap positions in wh-questions. As with RCs, the training set includes questions with either subject or direct object extraction (11), and the generalization set contains questions with indirect object extraction (12).\n(11) TRAINING a. Who did Emma love __? b. Who ate a cake?\n(12) GENERALIZATION a. Who did Emma give a cake to __?.\nIn a wh-question (11a), a wh-filler (who) in the initial position of the clause is interpreted as if it occupied the gap (again indicated with an underscore) in the direct object position of love.\n2.4 Novel Wh-questions\nNext, we evaluate generalization to extraction cases that involve familiar gap positions\u2014subject and direct object\u2014paired with verb types that have never been observed in wh-questions during training. For this case, the training set contains wh-questions with simple transitive verbs (13) and declarative sentences with various verb types: transitive, intransitive and ditransitive. The generalization set includes five novel types of wh-questions that have not been observed during training, though their declarative counterparts have.\nThe novel wh-questions have varying distance between the wh-filler and the gap. Subject whquestions, which maintain the same word order as their declarative counterparts, exhibit no gap (14a, 14b). Questions about direct objects of ditransitive verbs (14c), as well as questions with NPs modified by either a PP or an RC (14d),7 have moderately long filler-gap distances. The filler-gap distance is longest for object extraction out of embedded clauses (14e).\n(13) TRAINING (The training set also includes the declarative counterparts of (14).)\na. Who saw a cat ? b. What did Emma see __?\n(14) GENERALIZATION\na. Who froze ?\nb. What was frozen ?\nc. What did the boy give __ to Liam?\nd. What did Max give a cat that slept __?\ne. What did a boy say that Max believed that the cat saw __?"
        },
        {
            "heading": "3 Dataset Generation",
            "text": "Grammar SLOG is generated from a probabilistic Synchronous Context-Free Grammar (SCFG) implemented in Alto (Gontrum et al., 2017). This grammar simultaneously generates the English expressions and their corresponding meaning representations (see Appendix B for more details).\nTraining and generalization sets We follow a similar sampling procedure to COGS. A total of\n7Wh-questions with PP- or RC-modified NPs include various constructions where modifiers appear in subjects, direct objects, or indirect objects, exhibiting an average filler-gap distance similar to ditransitive verb wh-questions.\n10,607 sentences are sampled from the probabilistic SCFG and then split into training, in-domain validation and in-domain test sets with an 8:1:1 ratio. The splits are then merged with the corresponding COGS splits. We then add 100 standalone PP-modified NPs and 100 standalone RC-modified NPs to the training set, as discussed in Section 2.2.\nWe also include what we refer to as primitive exposure examples for each ditransitive verb and verb accepting CP arguments,8 totaling 40 primitives. These are standalone verb lexical meanings, such as, hope ; \u03bba.\u03bbb.\u03bbe.hope.agent(e,b) \u2227 hope.ccomp(e,a). This results in a final training set of 32,755 examples and 4,046 examples in both validation and in-distribution test sets.\nFor the generalization set, we use separate grammars for each generalization case. We sample 1000 examples from each of the 17 cases, yielding a total of 17,000 examples. For the training set and the generalization set, the maximum lengths of the input English sentences are 28 and 61 tokens, respectively. The maximum lengths of the corresponding output logic forms are 229 and 599 tokens. See Appendix B for more details."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Models We evaluate the performance of seq2seq, autoregressive, and structure-aware models on SLOG. The seq2seq models we evaluate are a Transformer we train on SLOG from scratch (vanilla Transformer henceforth; Vaswani et al. 2017), and a finetuned pretrained Transformer (T5; Raffel et al. 2020) that has demonstrated strong performance on multiple compositional generalization tasks (Herzig et al., 2021). The autoregressive Transformer model we evaluate is LLaMa (Touvron et al., 2023). Finally, the structure-aware model we evaluate is the AM-Parser (Groschwitz et al., 2018), which achieves near-perfect accuracy on COGS (Wei\u00dfenhorn et al., 2022). Previous work has shown that structure-aware models perform well on compositional generalization tasks, specifically those involving structural generalization (Yao and Koller, 2022). Following Wei\u00dfenhorn et al. (2022), we first have the AM-Parser predict an intermediate dependency tree, and then convert it to a graph-based representation of the SLOG logical form. We use the A* AM-parser from Lindemann\n8Primitive examples of these two verb types let us incorporate their infinitive forms, used in wh-questions, into SLOG\u2019s vocabulary.\net al. (2020) for our experiments, as it yields the best overall results compared to alternative versions of AM-parser, such as the one in Groschwitz et al. (2018).9 We run each experiment with five different random seeds. See Appendix A for more details.\nEvaluation metric Most studies report exact match accuracy on COGS. This metric has two limitations that may lead to an underestimation of a model\u2019s generalization capacity. First, because the COGS LF is conjunctive, reorderings of the conjuncts are semantically equivalent; yet, under exact match accuracy, only a single order is considered correct. Second, the COGS LF uses Skolem constants with a naming scheme tied to the linear indices of phrasal heads in the input. While a commitment to a systematic naming scheme is necessary for consistent evaluation, different naming schemes up to the renaming of the constants in the gold LF yield equivalent LFs (e.g., (15a) vs. (15b)). Such LFs would be considered incorrect under exact match.\nTo incorporate semantic equivalence up to conjunct reordering and constant renaming, at evaluation time, we alphabetically sort the conjuncts of the gold LFs, and subsequently index variables based on their appearance order in the sorted LFs. The same modifications are applied to the model outputs. This process results in the reformatted output as shown in (16); applying these modifications to (15a) and (15b) yields the same outcome. Then, computing exact match on these postprocessed LFs captures the targeted semantic equivalence.\n(15) Gold LF and model-predicted LF for What did the baby eat?: a. Gold: eat.theme(x4, ?) \u2227\neat.agent(x4, x3) \u2227 baby(x3) b. Out: eat.agent(x3, x6) \u2227\neat.theme(x3,?) \u2227 baby(x6) (16) Reordered and reindexed version:\na. baby(y2) \u2227 eat.agent(y1, y2) \u2227 eat.theme(y1, ?)\nThis reformatted exact-match metric is used for all results reported in the main text; see Appendix C.1 and Table 5 for more details."
        },
        {
            "heading": "5 Results",
            "text": "Overall, seq2seq Transformers, both trained from scratch and pretrained, display low accuracy on\n9For a detailed discussion, please refer to Appendix D.\nSLOG (Figure 2), in line with earlier studies on structural generalization in seq2seq models (Yao and Koller, 2022). This is also the case for the more recent autoregressive Transformer LLaMa, whose performance is similar to that of T5. As Figure 2 shows, high accuracy on the full COGS dataset, where 86% of the generalization cases are lexical, can obscure low performance on structural generalization, highlighting the need for the expanded structural generalization tests included in SLOG.\nSLOG additionally reveals weaknesses in the AM-Parser that COGS did not. While the AMParser achieves 90% accuracy on the structural generalization subset of COGS (Figure 2), it faces systematic difficulties with several generalization types introduced in SLOG (Figure 3).\nPerformance varied substantially across generalization categories (Figure 3); in particular, all models achieved near-perfect accuracy on Active subject wh-questions and Shallower PP recursion. These cases were the least structurally complex in their respective categories (\u00a72.3 and \u00a72.1).We highlight specific error types in the rest of this section; see Appendix C for full results and additional error analysis."
        },
        {
            "heading": "5.1 Unobserved Depth and Length Both Affect Depth Generalization",
            "text": "The maximum depth observed in training was four levels of embedding for all three recursive structures tested. All models achieve greater than 90% accuracy on unseen shallower PP recursion (three\nlevels of embedding). A considerably lower performance is observed for seq2seq models with shallower tail CP recursion (<61%); in particular, the Transformer trained from scratch consistently fails to generalize to shallower center embedding, with zero accuracy overall. Transformer models show systematically lower performance on deeper recursions (5-12 levels of embedding), whereas the structure-aware model is robust to depth variation.\nWe investigate the relation between length and depth generalization further by dividing the deeper depth generalization cases into examples that are shorter vs. longer than the maximum output length observed in training (229 output tokens). Results are shown in Table 3. All tested Transformer models are unable to generalize to examples longer than the maximum output length observed in training;\nthis result is consistent with the difficulty of length extrapolation observed in the literature (Hupkes et al., 2020; Anil et al., 2022). Length extrapolation does not capture the full story, however: the model\u2019s performance is limited even when the length of the generalization examples falls within the range of observed output lengths. This indicates that unobserved depth indeed plays a role in these models\u2019 poor generalization to deeper structures, in addition to known difficulties in length generalization."
        },
        {
            "heading": "5.2 Unobserved Long-distance Dependencies Make Generalization Difficult",
            "text": "Generalizing to subject modification (both PP and RC) is one of the most challenging cases. Seq2seq models achieve near-zero accuracy, even with the additional cue from the standalone modified NPs that modification can appear outside of object positions. This challenge echoes previous findings on COGS (Akyurek and Andreas, 2021; Zheng and Lapata, 2022; Yao and Koller, 2022). The remainder of this section focuses on the analysis of PP modification cases, but similar patterns are observed for RC modifiers, which we discuss in Appendix C.3.\nCommon error patterns across Transformer models reveal a bias towards shorter predicate-argument dependencies. For instance, in sentences like A cat on the mat froze, models often misinterpret the closer NP the mat as the subject.\nA further breakdown of the modifier generalization performance by construction shows that examples involving longer predicate-argument dependency (i.e., there is an intervening non-argument NP between the predicate and the argument) tend to be more difficult for all models (Table 4). However, the Transformer-based models show a stronger bias towards linearly adjacent predicate-argument structures. Further analysis (Appendix C.2) shows that seq2seq models additionally fall prey to inference patterns akin to a modification rule \u201cattach PPs to NPs in immediate post-verb position\u201d, which is compatible with the training data but leads to incorrect generalization."
        },
        {
            "heading": "5.3 Gap Generalizations Are Challenging for All Tested Models",
            "text": "For gap generalization cases, all models display low accuracy and considerable variability across different runs as shown in Figure 3. While Transformer models are biased towards more frequent subsequences of output LFs observed during train-\ning (see Appendix C.4), the structure-aware AMParser demonstrates different generalization difficulties.\nThe AM-Parser systematically fails on every instance of wh-questions involving long movement (e.g. What did Ava say that the cat saw __?). This issue arises from its internal prediction of dependency trees, which represent how meaning representations are compositionally constructed. For these wh-questions, the required dependency trees are nonprojective since the edge from the embedded verb to the wh-pronoun crosses the matrix verb. However, the AM-Parser used in our study only supports projective dependency trees, leading to incorrect prediction of sentence structure.10 This issue with projectivity can serve as a diagnostic for structural limitations of similar structure-aware parsers (Liu et al., 2022; Qiu et al., 2022a).\nFurthermore, on the indirect and direct object wh-questions, the AM-Parser performs very unpredictably, with accuracies ranging from 0 to 80 depending on the random seed. This is because at the bottom of its compositional process, the AMParser predicts the lexical meaning for each token in the sentence (supertag). In these generalization types, the gold meaning representations in the test set require supertags that are infrequent in training. Thus, while the AM-Parser can compensate the distribution shift of the meaning representations as a whole, SLOG exposes its weakness to distribution shifts in the lexical supertags. A more detailed discussion is provided in Appendix D."
        },
        {
            "heading": "6 Related Work",
            "text": "Previous research has shown that recurrent neural network (RNN) architectures often struggle with learning complex long-range relations from simpler formal languages (Avcu et al., 2017; Mahalunkar and Kelleher, 2019). Our results on SLOG reveal that unseen long-distance predicateargument dependencies pose considerable difficulty for Transformer-based models as well (\u00a75.2). For filler-gap dependencies, prior work has centered on syntactic tasks involving wh-questions or relative clauses (Wilcox et al., 2018; Marvin and Linzen, 2018; Li et al., 2023; i.a.). These studies primarily use language modeling as the task and do not require mapping to semantic representations. SLOG incorporates both long-distance predicate-\n10Alternative versions of the AM-Parser that can handle non-projective trees exist and are discussed in Appendix D.\nargument and filler-gap dependencies within a semantic parsing setup.\nGeneralizing recursive patterns to deeper structures has been investigated in both artificial neural networks and humans using artificial languages (Christiansen and Chater, 1999; Lakretz et al., 2021; McCoy et al., 2021). Our findings underscore Transformer-based models\u2019 limitations with deeper recursive structures, corroborating the observations of Hupkes et al. (2020); Lakretz et al. (2021). In contrast, human studies have shown that they can learn and extrapolate center-embedding patterns to greater depth in artificial languages (Fitch and Hauser, 2004; McCoy et al., 2021).\nGeneralization cases in SLOG draw inspiration from the frequency gaps in natural language, where common patterns serve as a foundation for generalizing to rarer structures. This has connections to language acquisition in children, who have limited exposure to complex, less frequent structures, yet need to generalize to novel complex utterances by extrapolating from familiar linguistic elements (Perfors et al., 2011; Tomasello and Olguin, 1993; Atkinson et al., 2018). Human proficiency in such generalizations is attributed to inductive biases rooted in systematic compositional rules. However, the Transformer-based models we tested, despite excelling in lexical generalization scenarios, face challenges when presented with unfamiliar linguistic structures requiring such rule induction, hinting at potentially different or inadequate underlying mechanisms. More broadly, how the compositional generalization cases proposed in this work can be connected to human language acquisition is an interesting area of future study."
        },
        {
            "heading": "7 Conclusions",
            "text": "We introduce SLOG, a semantic parsing dataset that extends the COGS benchmark with a focus on structural generalization, which is often underrepresented in current benchmarks for compositional generalization. Using SLOG, we assess the structural generalization capacity of Transformer models (both pretrained and trained from scratch), as well as AM-Parser, a structure-aware model. While all models achieve good overall accuracy on COGS (\u2265 78%), their performance on SLOG is substantially lower, especially for Transformer models (\u2264 41%). Furthermore, even the structure-aware AMParser, which achieved strong performance on all structural generalization cases of COGS, performs poorly on several of the newly introduced generalization types in SLOG. Our error analysis shows that all Transformer models tested struggle with interpreting unseen long-distance dependencies and deeper recursive constructions than observed in training. On the other hand, the AM-Parser, despite its stronger overall performance (71%), displays categorical failures on gap generalization due to its inherent parser design limitations. Overall, SLOG exposes the limitations of a range of models that have previously been claimed to achieve good compositional generalization, and can serve as a useful analytic tool for guiding future improvements.\nLimitations\nSLOG is a synthetic corpus and covers only a fraction of the diverse structures in English. Furthermore, previous research has demonstrated that the design of meaning representations (MR) can have a nontrivial effect on model performance in semantic parsing tasks (Guo et al., 2019; Herzig et al., 2021; Qiu et al., 2022b). For example, as noted by Wu et al. (2023), the variable indexing scheme\nmay introduce additional semantically irrelevant challenges when assessing structural generalization. SLOG\u2019s reformatted exact-match evaluation metric partially addresses this concern by taking into consideration several variations of MRs that are semantically equivalent, including MRs that are equivalent up to constant renaming. However, a more comprehensive study of the effect of artifacts from the formalism is left to future work.\nThere also exist challenges specific to the evaluation of pretrained models. That is, distributional shift between training and generalization sets intended by SLOG, such as withholding the constructions PPs modifying subject NPs from training, is difficult to strictly enforce when pretraining is involved (Kim et al., 2022). This potential violation of distributional control makes the interpretation of the obtained results difficult; we cannot disentangle whether generalization success in pretrained models derives from genuine compositional capabilities or simply exposure during pretraining to the target constructions meant to be withheld from the evaluated models. Still, corpus analyses such as Karlsson (2007) suggest that deep center embedding beyond three levels is very rare in naturally occurring data, so it is possible that very deep embedded structures are withheld as intended even from models exposed to large amounts of pretraining data. We hope the additional structural generalization cases that SLOG offers can also help with future work investigating the interaction between structures available in pretraining data and structural generalization."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Zhengxuan Wu, Christopher Manning, Christopher Potts and all members of the NYU Computation and Psycholinguistics Lab for helpful discussion. This work was supported in part through the NYU IT HPC resources, services, and staff expertise, and was funded by Labex EFL ANR-10-LABX-0083, the laboratory LLF of Universit\u00e9 Paris Cit\u00e9, the DFG through project KO 2916/2-2, and the National Science Foundation (NSF) under Grants No. BCS-204122, BCS2114505 and IIS-2239862."
        },
        {
            "heading": "A Training details",
            "text": "Hyperparameters The architecture of the Transformer trained from scratch is the same as in original COGS, which consists of 2 encoder and 2 decoder layers, 4 attention heads per layer, and a feedforward dimension of 512. We use the best combination of hyperparameters from Csord\u00e1s et al. (2021) on COGS: a learning rate of 0.0001 with no label smoothing, warmup, or early stopping. Absolute positional embeddings with down scaling scheme (He et al., 2015; Csord\u00e1s et al., 2021) is used due to stability issues observed with relative positional embeddings in recursive depth generalization cases, a similar phenomenon also noted in\nCsordas and colleagues\u2019 experiments. Models are trained for 50k steps with a batch size of 128.\nFor the T5 experiments, we finetune T5-base11 using a learning rate of 1.5e-5 and no label smoothing, warmup or early stopping. We finetune the model for 50k steps using a batch size of 2048.\nFor the LLaMA experiments, we finetune llama-7b-hf12 with LoRA (Hu et al., 2021).13 We set the learning rate to 3e-4, LoRA rank to 8, alpha to 32 and dropout to 0.1. We finetune the model for 5K steps with a batch size of 64, with 100 warmup steps and no label smoothing or early stopping. We apply LoRA to Wq and Wv weight matrices in the model.\nAll our experiments were run five times, using different random seeds. The final checkpoints from each run were used for evaluation on both in-domain test and out-of-domain generalization sets."
        },
        {
            "heading": "B Data generation",
            "text": "B.1 Meaning representations We use Alto (Gontrum et al., 2017) to implement a probabilistic Synchronous Context-Free Grammar (SCFG), which simultaneously generates pairs of English expressions and their corresponding meaning representations. Since SCFG cannot handle logical variables (Wong and Mooney, 2007), we use a variable-free representation proposed by Qiu et al. (2022a) (17a) as an intermediate representation during generation. The variable-free logical form (LF) can be deterministically postprocessed into the original COGS LF (17b) with additional information and specific constraints: (i) We rely on the word order in the input sentence to label the Skolem constants (i.e. variables); (ii) While the variable-free LF is unable to represent binding relations correctly as pointed out by Wu et al. (2023), an additional constraint that disallows duplicate nouns enables the intended binding relations to be identified unambiguously.\n(17) A cat slept. ; a. Variable-free LF:\nsleep(agent=cat)\nb. COGS LF: cat(x1) \u2227 sleep.agent(x2, x1)\n11https://huggingface.co/t5-base 12https://huggingface.co/spaces/tloen/ alpaca-lora 13https://github.com/tloen/alpaca-lora\n(18) A cat wanted to sleep. ;\na. Variable-free LF: want(agent=cat, xcomp=sleep(agent=cat))\nb. COGS LF: cat(x1) \u2227 want.agent(x2,x1) \u2227 want.xcomp(x2, x4) \u2227 sleep.agent(x4,x1)\nIn the original COGS LF, entities or events specified by the predicates are represented by indexed constants (17b). In its variable-free counterpart (17a), sleep denotes the sleeping event, cat expresses the existence of a cat entity and fills the agent role of the sleeping event. In this way, each predicate in the LF has a set of arguments directly connected to their thematic roles without using variables.\nSince the variable-free LF often results in a more compact LF, it has been adopted as the primary meaning representation in several prior work (Qiu et al., 2022b; Drozdov et al., 2022). We move away from this practice and keep the original COGS LF as the main meaning representation\u2014as briefly mentioned above, the variable-free LF cannot represent binding relations accurately unless some external heuristic or constraint is introduced for disambiguation. For example, the variable-free LF in (18a) is ambiguous between the meaning of A cat wanted to sleep and A cat wanted a (different) cat to sleep, whereas the COGS LF in (18b) unambiguously represents the meaning of A cat wanted to sleep.\nWhile we release the SLOG dataset in both LFs and report the results using the variable-free LF in Appendix E to enable comparison with existing work, we strongly recommend using the original COGS LF for evaluation on SLOG in future work.\nB.2 Grammar and sampling details\nSLOG expands upon the COGS vocabulary, which consists of 503 nouns and 113 verbs, to additionally include wh-words (who, what) and that used as a relative pronoun. In SLOG, for the sake of simplicity, we only consider restrictive relative clauses introduced by that regardless of the animacy of the head NPs. For indirect object-extracted instances, we use the preposition stranding structure, such as the boy that Emma give a cake to, rather than the boy to whom Emma gave a cake.\nThe dataset includes the 30,000 examples from the initial COGS training set, and new examples that fall into one of the following categories:\n\u2022 Relative clauses within object NPs, equal in number to instances with PP modifications\n\u2022 Subject and object wh-questions matching the quantity of their corresponding declarative sentences\n\u2022 An equal number of four-level-nesting recursion constructions as the depth-2 instances in initial COGS\n\u2022 A primitive example for each ditransitive verbs and verbs accepting complement clause (CP) arguments\nFinally, the SLOG sampling process excludes sentences with duplicate nouns (e.g. Emma saw Emma.), as mentioned in Section B.1.\nSemantic plausibility Following COGS, our grammar implements simplified selectional restrictions, focusing mainly on animacy constraints. For instance, the subjects of unergative verbs are limited to animate entities, as in the cat smiled. As a result, our generated sentences may include semantically odd but syntactically well-formed sentences, such as non-edible object being the theme of eat or spatial incongruities like a house in a bottle. While these semantic limitations are unlikely to affect models trained from scratch, they may influence the performance of models that have been pretrained on naturalistic language data. It\u2019s important to note that our primary aim is to assess the extent to which models rely on compositional structural generalization to derive meaning. In line with the classic example \u201ccolorless green ideas sleep furiously\u201d (Chomsky, 1957), which demonstrates that syntactic structure can be independent of semantic coherence, we argue that a model capable of compositional generalization should be able to map such sentences to an appropriate logical form as long as they are structurally well-formed.\nStructural disambiguation choice In SLOG, mappings to logical forms are designed to be unambiguous, particularly for sentences that are inherently ambiguous due to prepositional phrase attachment ambiguity, such as Ava saw the ball in the bottle on the mat. This design choice, following COGS, is to use right-branching disambiguation for\nall meaning representations. Consequently, SLOG ensures that PP modifiers are consistently interpreted as nested NP-attachments\u2014Ava saw [the ball [in the bottle [on the mat]]], although a VPattachment might sometimes seem more intuitive depending on the context. This approach ensures that there exists an unambiguous target meaning representation for each expression in the dataset (and this is clearly signaled by the training data), rather than preserving the ambiguity which may complicate the evaluation protocol."
        },
        {
            "heading": "C Full results and additional analyses",
            "text": "All models perform very well on the in-domain test set (accuracy over 99%). All experiments in this work were conducted on the out-of-domain generalization set, and we report the full results of the experiments discussed in Section 5 in Table 5.\nC.1 Effect of the reformatted exact-match metric\nAll models exhibit higher overall accuracy with the reformatted exact-match evaluation compared to the initial metric, notably pretrained models with an increase of over 10 percentage points (Table 5). This suggests that the initial exact-match metric may have underestimated model performance.\nC.2 PP Modifiers in unseen positions\nAs discussed in Section 5.2, generalization to PP modification involving unseen long predicateargument dependencies is challenging for all evaluated models. Among such constructions, PP modification in the indirect object position (20a) is less challenging than subject position (19). A possible explanation is that the former has a closer surface resemblance to direct object modification\u2014 modifiers attach to an immediate post-verb NP. Indeed, we observe that a higher proportion of indirect object modifications are partially correct; models correctly predicted the PP-modified NP, but erred in the argument structure.\nTable 4 also shows that Transformers perform worse on Indirect object in PP datives (20c) compared to Passive indirect objects (20b), although neither subcase introduces long predicate-argument dependencies.\n(19) PP within subject NPs: [A cat on a mat]subj ate a fish.\n(20) Sub-cases of PP within indirect object NPs:\na. Indirect object in double object datives: Emma gave [ a cat on the mat ]iobj a fish.\nb. Passive indirect objects: A fish was given to [ a cat on the mat ]iobj.\nc. Indirect object in PP datives: Emma gave a fish to [ a cat on the mat ]iobj.\nThe predominant error pattern in the former subcase was incorrect attachment of PP modifiers to the direct object NP. For example (21b), modifier the mat denoted by x9 was attached to a fish instead of the cat. This suggests that Transformers additionally fall prey to inference patterns akin to a modification rule \u201cattach PPs to NPs in immediate post-verb position\u201d, which is compatible with the training data but does not lead to correct generalization.\n(21) Gold LF and model-predicted LF for Emma gave a fish to the cat on the mat: a. Gold: *cat (x6); *mat(x9);\ngive.agent (x1,Emma) \u2227 give.theme (x4, x3) \u2227 give.recipient (x1, x6)\u2227 fish(x3) \u2227 cat.nmod.on (x6, x9)\nb. Out: *cat (x6); *mat(x9); give.agent (x1,Emma) \u2227 give.theme (x4, x3) \u2227 give.recipient (x1, x6)\u2227 fish(x3) \u2227 fish.nmod.on (x3, x9)\nC.3 RC Modifiers in unseen positions\nGeneralizing RC modifiers to unseen positions presents a similar challenge as PP modification cases, due to unobserved long-distance dependencies. As shown in Table 6, all models exhibit a significant performance discrepancy between constructions involving unseen long predicateargument dependencies and those that do not.\nFor novel positions that introduce long predicateargument dependencies, RC modification in the indirect object appears more difficult than in the subject position, contrary to the case with PP modifiers. The primary error pattern (22) demonstrates that models struggle to detect the RC boundary when the relative clause ends with a verb. They systematically misinterpret the indirect object a fish of the main verb gave as the direct object of the adjacent embedded verb slept.\n(22) Gold LF and model-predicted LF for Emma gave a cat that slept a fish: a. Gold: give.agent (x1,Emma)\n\u2227 give.recipient (x1, x3) \u2227 give.theme (x1, x7)\u2227 cat(x3) \u2227 cat.nmod (x3, x5) \u2227 sleep.agent(x5, x3) \u2227 fish(x7)\nb. Out: give.agent (x1,Emma) \u2227 give.theme (x1, x3) \u2227 cat(x3) \u2227 cat.nmod (x3, x5) sleep.agent(x5, x3) \u2227 sleep.theme(x5, x7) \u2227fish(x7)\nC.4 Gap constructions\nWhile performing poorly on indirect objectextracted relative clauses (23), all tested models systematically mirror the direct object-extracted RC pattern in training, as demonstrated by the incorrect output (23b). They furthermore show distinct difficulties when handling wh-questions cases, as will be discussed in the remainder of this section.\n(23) Input: Ella cooked the servant that Emma gave a tool to __ . a. Gold: *servant(x3);cook.agent\n(x1, Ella) \u2227 cook.theme(x1, x3) \u2227 servant.nmod( x3, x6) \u2227 give.agent(x6, Emma) \u2227 give.theme (x6, x8) \u2227 give.recipient(x6, x3) \u2227 tool (x8)\nb. Models output: *servant(x3);cook.agent(x1, Ella) \u2227 cook.theme(x1, x3) \u2227 servant.nmod( x3, x6) \u2227 give.agent(x6, Emma) \u2227 give.theme (x6, x3) \u2227 give.recipient(x6, x8) \u2227 tool (x8)\nC.4.1 Direct and indirect wh-questions The Transformer trained from scratch and LLaMa frequently misinterpret the theme role in direct object wh-questions. For example, they often fail to map wh-words to \u2018?\u2019 as illustrated in (24b):\n(24) Input: What did Emma sell to Liam ? a. Gold:sell.theme (x3, ?)\u2227\nsell.agent (x3, Emma) \u2227 sell.recipient(x3,Liam)\nb. Output of Vanilla Transformer and LLaMa: sell.theme (x3, x5) \u2227 sell.agent (x3, Emma)\u2227 sell.recipient(x3,Liam) c. AM-Parser\u2019s output: sell.agent (x3, ?) \u2227 sell.theme (x3, Emma)\u2227 sell.recipient(x3,Liam)\nThis error pattern can be traced back to frequency of the subsequences in the training data. Three\ntypes of tokens can appear post-comma in the output LF space: x, ? denoting wh-words, or a proper noun (PropN), such as Emma. The subsequence theme(xi, xj) is 20 times more frequent than theme(xi,?) and theme(xi,PropN). This discrepancy does not affect all models equally; in fact, T5 can generalize correctly for some constructions despite this skewed label distribution, achieving near-perfect accuracy for direct object wh-questions. However, when it comes to less frequent constructions\u2014indirect object wh-questions, T5 overgeneralizes. In 94.6% of these cases, it erroneously produces the observed direct object wh-questions pattern theme(xi,?), instead of the correct but unseen recipient(xi,?). This observation aligns with the findings of Wu et al. (2023); Yao and Koller (2022), who noted that the decoder of Transformer models tends to exhibit a heavy bias towards generating observed n-grams.\nC.4.2 Wh-questions with long-distance movement\nAll models achieve very low accuracy when generalizing to longer filler-gap dependency across CPs, but an error analysis shows that Transformer and structure-aware models face distinct challenges. As shown in (25b), the Transformer trained from scratch commonly misinterprets the complementizer that (corresponding to ccomp in LF) as a relative pronoun (nmod). Additionally, it tends to interpret the wh-word as the direct object of the CP verb, e.g., say. In the most common errors for T5 and LLaMa (25c), the whole gap conjunct (paint.theme(x7, ?)) is missing, revealing their difficulties in establishing long-range filler-gap dependencies between the initial whword and the embedded gap position. On the other hand, AM-Parser cannot decode non-projective dependencies, thus has 0% accuracy (see more detailed discussion of the issue in \u00a7D).\n(25) Input: What did Liam say that the bear painted __ ? a. Gold: *bear(x6); say.agent\n(x3,Liam) \u2227 say.ccomp (x3,x7) \u2227 paint.agent (x7,x6) \u2227 paint.theme (x7,?)\nb. Output of vanilla Transformer: *bear(x6); say.agent (x3,Liam) \u2227 say.theme (x3,?) \u2227 say.nmod (x3,x7) \u2227 paint.agent (x7,x6) \u2227 paint.theme (x7,?) c. Output of T5 and LLaMa: *bear(x5); say.agent (x3,Liam) \u2227 say.ccomp (x3,x7) \u2227 paint.agent (x7,x5)\nC.4.3 Wh-questions with modified NPs In wh-questions with PP and RC modifiers, even though the SLOG training set only contains whquestions with unmodified NPs, all models generalize well (accuracy > 80%) to direct object NPs with modifiers (e.g., Who ate a cake on the table?). These are cases where the modification pattern is observed in training as a part of declarative sentences. In contrast, performance declines when models encounter wh-questions with modifiers in the indirect object position (i.e., modification structure not observed as part of declarative sentences). Similarly, for wh-questions with subject position modifiers, the performance is very low: both T5 and vanilla Transformer achieve near-zero accuracy, and LLaMa achieves around 5%.\nThis observation mirrors the patterns discussed in \u00a75.2, attributed to difficulties introduced by unseen subject-verb dependencies across PPs or RCs. In contrast, the structure-aware model exhibits significantly better performance in wh-question with subject modification.\nC.4.4 Passive subject wh-questions For subject wh-questions, which exhibit no gap, T5 and AM-Parser perform near-perfectly on both active and passive subject questions. Transformer trained from scratch and LLaMa also perform well on active subject questions, but achieve much lower performance on passive subject questions. This performance discrepancy is the most evident in sub-cases where passive subjects function as theme (e.g., (26))\u2014the Transformer trained from scratch has near-zero accuracy for these sub-cases, systematically failing to map wh-words to \u2018?\u2019 as in (26b):\n(26) Input: What was eaten by Emma ?\na. Gold: eat.theme (x2, ?) \u2227 eat.agent (x2, Emma)\nb. Output of Vanilla Transformer and LLaMa: eat.theme (x2, x4) \u2227 eat.agent (x2, Emma)\nAs discussed in Section C.4, this error pattern may result from the highly imbalanced label distribution in training output space. Both LLaMa and Transformer trained from scratch are inclined to repeat the substantially more common subsequence theme(xi, xj) over theme(xi,?)."
        },
        {
            "heading": "D AM-Parser-specific issues",
            "text": "While the AM-Parser achieves strong performance on most generalization types, it faces systematic difficulties in handling novel gap structures. In particular, its accuracy on wh-questions involving long-distance movement and indirect objectextracted relative clauses is always 0. Additionally, its accuracy significantly fluctuates across runs for both direct and indirect object wh-questions. Here, we give a detailed explanation of error patterns for these challenging types.\nBackground The AM-Parser maps input sentences to graphs by parsing each input sentence to an AM dependency tree, which is then deterministically evaluated to a graph (Groschwitz et al., 2018). In the AM dependency tree, each token is labeled with a supertag\u2014a small graph illustrated in Figure 4\u2014that captures the lexical meaning of the token. The tree\u2019s edges represent the compositional structure of the sentence, which specifies how the meaning of the sentence is recursively computed from the supertags. The supertag in Figure 4 represents the meaning of cooked in the sentence Ella cooked the servant that Emma gave a tool to. The blue markers \u201cS1\u201d and \u201cS2\u201d indicate that two arguments are still needed to fill the agent and theme roles of cook.\nWh-questions with long movement We show an example of a predicted AM dependency tree for a wh-question with long movement in Figure 5\nand the corresponding gold AM dependency tree in Figure 6. As discussed in Section 5.3, the parser used in this paper is limited to predicting projective AM dependency trees, but the gold AM dependency tree in Figure 6 is non-projective (the edge snapped -> Who crosses the edge root -> appreciate). Thus it is impossible for the AMParser to predict the correct compositional structure.\nInstead of the A* parser, one could instead use the fixed-tree decoder of Groschwitz et al. (2018), which is capable of predicting non-projective AM dependency trees. This parser achieves nonzero accuracy (36%) on wh-questions with long movement, confirming our hypothesis that the projectivity is the issue. However, the A* parser outperforms the fixed-tree decoder on most other generalization types, which is why we only report its results in the main body of the paper. The transitionbased AM-Parser of Lindemann et al. (2020) can also predict non-projective trees, but uses a different probability model that is incompatible with the training algorithm of Groschwitz et al. (2021) that we use here.\nNote that the A* AM-Parser shares its limitation to projective structures with many other structureaware models. For instance, the LeAR model of Liu et al. (2021) uses phrase-structure trees as compositional structures, and the CSL-T5 parser of Qiu et al. (2022a) uses phrase-structure trees during the data augmentation process. Because phrase structure trees are equivalent to projective dependency trees, they are likely to encounter similar difficulties on SLOG.\nDirect & indirect wh-questions and indirect object-extracted RC The AM-Parser consistently shows zero accuracy for indirect objectextracted RCs and exhibits big performance fluctuation across different runs for direct and indirect wh-questions. This is because in these generalization types, the gold meaning representations in the test set require supertags that are infrequent in training.\nWe show an example of AM dependency trees for a direct object wh-question in Figure 7, with gold supertags in Figure 7a and predicated supertags in Figure 7b. The issue here is that the model predicts the wrong supertag for sell, treating What as its agent instead of theme, and Emma as its theme rather than agent, which results in the erroneous output LF as shown in (24c). The\nAM-Parser is limited to using supertags that it observed during training (possibly with different node labels to accommodate novel lexical material). For the direct wh-question case, the correct supertag was actually present in the training data, but was much less frequent than the erroneous one in Figure 7b. We observe a similar discrepancy in the frequency distribution between predicted and gold supertags for indirect object-extracted RCs and indirect wh-questions.\nWe conjecture that the AM-Parser was overly sensitive to the supertag distribution in the training data, pointing to a further architectural limitation. Thus, while the AM-Parser can compensate the distribution shift of the meaning representations as a whole, SLOG exposes its weakness to distribution shifts in the lexical supertags."
        },
        {
            "heading": "E Results with variable-free LFs",
            "text": "Table 7 reports the accuracy on SLOG using variable-free logical forms. The AM-Parser is unable to handle the variable-free format and therefore is omitted. The hyperparameters for the three tested models are the same as the experiments described in Appendix A.\nThe variable-free LF, as discussed in Appendix B and Wu et al. (2023), exhibits certain limitations and ambiguities which render direct comparisons with variable-based LF results inappropriate. Regardless, all three models achieve higher accuracy scores on the variable-free LFs compared to the COGS LFs, with pretrained models experiencing a particularly significant boost. This aligns with the observations of Qiu et al. 2022b.\nDespite the change in LF, the overall trends and challenges remain consistent. The Transformer trained from scratch struggles with the same generalization cases, failing to extrapolate to deeper recursion depths and struggling with cases involving unseen long-distance dependencies. Pretrained models, while exhibiting better overall performance, continue to struggle with more structurally complex generalization cases in their respective categories. These include deeper center embedding, indirect object-extracted RC and whquestions with long movement."
        }
    ],
    "title": "SLOG: A Structural Generalization Benchmark for Semantic Parsing",
    "year": 2023
}