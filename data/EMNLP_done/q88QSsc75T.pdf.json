{
    "abstractText": "Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with ground-truth at low latency may introduce forced anticipations, whereas utilizing reference consistent with the source word order at high latency results in performance degradation. Consequently, it is crucial to train the SiMT model with appropriate reference that avoids forced anticipations during training while maintaining high quality. In this paper, we propose a novel method that provides tailored reference for the SiMT models trained at different latency by rephrasing the ground-truth. Specifically, we introduce the tailor, induced by reinforcement learning, to modify ground-truth to the tailored reference. The SiMT model is trained with the tailored reference and jointly optimized with the tailor to enhance performance. Importantly, our method is applicable to a wide range of current SiMT approaches. Experiments on three translation tasks demonstrate that our method achieves state-of-the-art performance in both fixed and adaptive policies1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shoutao Guo"
        },
        {
            "affiliations": [],
            "name": "Shaolei Zhang"
        },
        {
            "affiliations": [],
            "name": "Yang Feng"
        }
    ],
    "id": "SP:034647a15e4dc0760969c9cd8598a5a0f71afb15",
    "references": [
        {
            "authors": [
                "Mauro Cettolo",
                "Jan Niehues",
                "Sebastian St\u00fcker",
                "Luisa Bentivogli",
                "Roldano Cattoni",
                "Marcello Federico."
            ],
            "title": "The IWSLT 2015 evaluation campaign",
            "venue": "Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Cam-",
            "year": 2015
        },
        {
            "authors": [
                "Junkun Chen",
                "Renjie Zheng",
                "Atsuhito Kita",
                "Mingbo Ma",
                "Liang Huang."
            ],
            "title": "Improving simultaneous translation by incorporating pseudo-references with fewer reorderings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Maha Elbayad",
                "Laurent Besacier",
                "Jakob Verbeek."
            ],
            "title": "Efficient wait-k models for simultaneous machine translation",
            "venue": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino J. Gomez",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Machine Learning, Proceedings of the Twenty-Third Interna-",
            "year": 2006
        },
        {
            "authors": [
                "Jiatao Gu",
                "James Bradbury",
                "Caiming Xiong",
                "Victor O.K. Li",
                "Richard Socher."
            ],
            "title": "Non-autoregressive neural machine translation",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Jiatao Gu",
                "Graham Neubig",
                "Kyunghyun Cho",
                "Victor O.K. Li."
            ],
            "title": "Learning to translate in real-time with neural machine translation",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume",
            "year": 2017
        },
        {
            "authors": [
                "Shoutao Guo",
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Turning fixed to adaptive: Integrating post-evaluation into simultaneous machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates,",
            "year": 2022
        },
        {
            "authors": [
                "Shoutao Guo",
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Learning optimal policy for simultaneous machine translation via binary search",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2023
        },
        {
            "authors": [
                "Fei Huang",
                "Hao Zhou",
                "Yang Liu",
                "Hang Li",
                "Minlie Huang"
            ],
            "title": "Directed acyclic transformer for nonautoregressive machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Jind\u0159ich Libovick\u00fd",
                "Jind\u0159ich Helcl."
            ],
            "title": "End-toend non-autoregressive neural machine translation with connectionist temporal classification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3016\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Manuel Lopes",
                "Tobias Lang",
                "Marc Toussaint",
                "Pierreyves Oudeyer."
            ],
            "title": "Exploration in model-based reinforcement learning by empirically estimating learning progress",
            "venue": "Advances in Neural Information Processing Systems, volume 25. Curran Associates,",
            "year": 2012
        },
        {
            "authors": [
                "Mingbo Ma",
                "Liang Huang",
                "Hao Xiong",
                "Renjie Zheng",
                "Kaibo Liu",
                "Baigong Zheng",
                "Chuanqiang Zhang",
                "Zhongjun He",
                "Hairong Liu",
                "Xing Li",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "STACL: simultaneous translation with implicit anticipation and controllable",
            "year": 2019
        },
        {
            "authors": [
                "Xutai Ma",
                "Juan Miguel Pino",
                "James Cross",
                "Liezl Puzon",
                "Jiatao Gu."
            ],
            "title": "Monotonic multihead attention",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Zhengrui Ma",
                "Shaolei Zhang",
                "Shoutao Guo",
                "Chenze Shao",
                "Min Zhang",
                "Yang Feng."
            ],
            "title": "Nonautoregressive streaming transformer for simultaneous translation",
            "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language",
            "year": 2023
        },
        {
            "authors": [
                "Yishu Miao",
                "Phil Blunsom",
                "Lucia Specia."
            ],
            "title": "A generative framework for simultaneous machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Lihua Qian",
                "Hao Zhou",
                "Yu Bao",
                "Mingxuan Wang",
                "Lin Qiu",
                "Weinan Zhang",
                "Yong Yu",
                "Lei Li."
            ],
            "title": "Glancing transformer for non-autoregressive neural machine translation",
            "venue": "arXiv preprint arXiv:2008.07905.",
            "year": 2020
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725,",
            "year": 2016
        },
        {
            "authors": [
                "Chenze Shao",
                "Jinchao Zhang",
                "Jie Zhou",
                "Yang Feng."
            ],
            "title": "Rephrasing the reference for non-autoregressive machine translation",
            "venue": "CoRR, abs/2211.16863.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Lex Weaver",
                "Nigel Tao."
            ],
            "title": "The optimal reward baseline for gradient-based reinforcement learning",
            "venue": "UAI \u201901: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5,",
            "year": 2001
        },
        {
            "authors": [
                "Ronald J. Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Mach. Learn., 8:229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Ruiqing Zhang",
                "Chuanqiang Zhang",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Learning adaptive segmentation policy for simultaneous translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Universal simultaneous machine translation with mixture-of-experts wait-k policy",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng"
            ],
            "title": "2022a. Gaussian multihead attention for simultaneous machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Informationtransport-based policy for simultaneous translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 992\u2013 1013, Abu Dhabi, United Arab Emirates. Association",
            "year": 2022
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Modeling dual read/write paths for simultaneous machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2461\u20132477, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "End-to-end simultaneous speech translation with differentiable segmentation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Hidden markov transformer for simultaneous machine translation",
            "venue": "CoRR, abs/2303.00257.",
            "year": 2023
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng",
                "Liangyou Li."
            ],
            "title": "Future-guided incremental transformer for simultaneous translation",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Shoutao Guo",
                "Yang Feng."
            ],
            "title": "Wait-info policy: Balancing source and target at information level for simultaneous machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2249\u20132263, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Baigong Zheng",
                "Kaibo Liu",
                "Renjie Zheng",
                "Mingbo Ma",
                "Hairong Liu",
                "Liang Huang."
            ],
            "title": "Simultaneous translation policies: From fixed to adaptive",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Simultaneous machine translation (SiMT) (Gu et al., 2017; Ma et al., 2019, 2020) generates the target sentence while reading in the source sentence. Compared to Full-sentence translation, it faces a greater challenge because it has to make trade-offs between latency and translation quality (Zhang and Feng, 2022a). In applications, it needs to meet the requirements of different latency tolerances, such as online conferences and real-time subtitles. Therefore, the SiMT models trained at\n\u2217Corresponding author: Yang Feng. 1Code is available at https://github.com/ictnlp/\nTailored-Ref\ndifferent latency should exhibit excellent translation performance.\nUsing an inappropriate reference to train the SiMT model can significantly impact its performance. The optimal reference for the SiMT model trained at different latency varies. Under high latency, it is reasonable to train the SiMT model with ground-truth since the model can leverage sufficient source information (Zhang and Feng, 2022c). However, under low latency, the model is constrained by limited source information and thus requires reference consistent with the source word order (Chen et al., 2021). Therefore, the SiMT model should be trained with corresponding appropriate reference under different latency.\nHowever, the existing SiMT methods, which employ fixed or adaptive policy, commonly utilize only ground-truth for training across different latency settings. For fixed policy (Ma et al., 2019; Elbayad et al., 2020; Zhang and Feng, 2021), the model generates translations based on the predefined rules. The SiMT models are often forced to anticipate target tokens with insufficient information or wait for unnecessary source tokens. For adaptive policy (Ma et al., 2020; Miao et al., 2021; Zhang and Feng, 2022b), the model can adjust its translation policy based on translation status. Nevertheless, the policy learning of SiMT model will gradually adapt to the given reference (Zhang et al., 2020). Consequently, employing only ground-truth\nfor the SiMT models trained at varying latency levels can negatively impact overall performance, as it forces them to learn the identical policy. Furthermore, Chen et al. (2021) adopts an offline approach to generate reference using the Full-sentence model for training the SiMT model at different latency, but this approach also imposes an upper bound on the performance of the SiMT model. Therefore, it is necessary to provide high-quality and appropriate reference for the models with different latency.\nUnder these grounds, we aim to dynamically provide an appropriate reference for training the SiMT models at different latency. In SiMT, the source information available to the translation model varies with latency (Ma et al., 2019). Therefore, the appropriate reference should allow the model to utilize the available information for predicting target tokens accurately. Otherwise, it will result in forced anticipations, where the model predicts target tokens in reference using insufficient source information (Guo et al., 2023). To explore the extent of forced anticipations when training the SiMT model with ground-truth at different latency, we introduce anticipation rate (AR) (Chen et al., 2021). As shown in Table 1, the anticipation rate decreases as the SiMT is trained with higher latency. Consequently, the reference requirements of the SiMT model vary at different latency. To meet the requirements, the appropriate reference should avoid forced anticipations during training and maintain high quality. Therefore, we propose to dynamically tailor reference, called tailored reference, for the training of SiMT model according to the latency, thereby reducing forced anticipations. We present an intuitive example of tailored reference in Figure 1. It can avoid forced predictions during training while maintaining the semantics consistent with the original sentence.\nTherefore, we propose a new method for providing tailored reference to SiMT models at different latency. To accomplish this, we introduce the tailor, a shallow non-autoregressive Transformer Decoder (Gu et al., 2018), to modify ground-truth to the tailored reference. Since there is no explicit supervision to train the tailor, we quantify the requirements for the tailored reference as two reward functions and optimize them using reinforcement learning (RL). On the one hand, tailored reference should avoid forced anticipations, ensuring that the word reorderings between it and the source sentence can be handled by the SiMT model trained\nat that latency. To achieve this, the tailor learns from non-anticipatory reference corresponding to that latency, which can be generated by applying Wait-k policy to Full-sentence model (Chen et al., 2021). On the other hand, tailored reference should maintain high quality, which can be achieved by encouraging the tailor to learn from ground-truth. Therefore, we measure the similarity between the output of tailor and both non-anticipatory reference and ground-truth, assigning them as separate rewards. The tailor can be optimized by striking a balance between these two rewards. During training, the SiMT model takes the output of the tailor as the objective and is jointly optimized with the tailor. Additionally, our method is applicable to a wide range of SiMT approaches. Experiments on three translation tasks demonstrate that our method achieves state-of-the-art performance in both fixed and adaptive policies."
        },
        {
            "heading": "2 Background",
            "text": "For a SiMT task, the model reads in the source sentence x = (x1, ..., xJ) with length J and generates the target sentence y = (y1, ..., yI) with length I based on the policy. To describe the policy, we define the number of source tokens read in when translating yi as gi. Then the policy can be represented as g = (g1, ..., gI). Therefore, the SiMT model can be trained by minimizing the cross-entropy loss:\nLsimt = \u2212 I\u2211\ni=1\nlog p(y\u22c6i | x\u2264gi ,y<i), (1)\nwhere y\u22c6i represents the ground-truth token. Our approach involves Wait-k (Ma et al., 2019), HMT (Zhang and Feng, 2023b) and CTC training (Libovick\u00fd and Helcl, 2018), so we briefly introduce them.\nWait-k policy As the most widely used fixed policy, the model reads in k tokens first and then alternates writing and reading a token. It can be formalized as:\ngki = min{k + i\u2212 1, J}, (2)\nwhere J indicates the length of the source sentence.\nHMT Hidden Markov Transformer (HMT), which derives from the Hidden Markov Model, is the current state-of-the-art SiMT model. It treats the translation policy g as hidden events and the target sentence y as observed events. During training, HMT learns when to generate translation by minimizing the negative log-likelihood of observed events over all possible policies:\nLhmt = \u2212 log \u2211 g p(y | x,g)\u00d7 p(g) . (3)\nCTC CTC (Graves et al., 2006) is applied in nonautoregressive translation (NAT) (Gu et al., 2018) due to its remarkable performance and no need for length predictor. CTC-based NAT will generate a sequence containing repetitive and blank tokens first, and then reduce it to a normal sentence based on the collapse function \u0393\u22121. During training, CTC will consider all possible sequences a, which can be reduced to y using function \u0393\u22121:\nLctc = \u2212 log \u2211\na\u2208\u0393(y)\np(a | x) , (4)\nwhere p(a | x) is modeled by NAT architecture."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we introduce the architecture of our model, which incorporates tailor into the SiMT model. To train the SiMT model with the tailor, we present a three-stage training method, in which the SiMT model benefits from training with tailored reference and is optimized together with the tailor. During inference, the SiMT model generates translation according to the policy. The details are introduced in the following subsections."
        },
        {
            "heading": "3.1 Model Architecture",
            "text": "We present the architecture of our method in Figure 2. Alongside the encoder and decoder, our method introduces the tailor module, which is responsible for generating a tailored reference for the SiMT model, utilizing the ground-truth as its input. Considering the efficiency of generating tailored reference, the tailor module adopts the structure of the non-autoregressive Transformer decoder (Vaswani et al., 2017). To enable the tailor to generate a tailored reference that is not limited by the length of ground-truth, it initially upsamples the groundtruth. Subsequently, it cross-attends to the output\nof the encoder and modifies ground-truth while considering the word order of the source sentence. Finally, it transforms the output of tailor into the tailored reference by eliminating repetitive and blank tokens (Libovick\u00fd and Helcl, 2018). The tailored reference replaces the ground-truth as the training objective for the SiMT model.\nGiven the lack of explicit supervision for training the tailor, we quantify the requirements for tailored reference into two rewards and optimize the model through reinforcement learning. We propose a three-stage training method for the SiMT model with the tailor, the details of which will be presented in the next subsection."
        },
        {
            "heading": "3.2 Training Method",
            "text": "After incorporating tailor into the SiMT model, it is essential to train the SiMT model with the assistance of tailor to get better performance. In light of this, we quantify the requirements of the tailored reference into two rewards and propose a novel three-stage training method for the training of our method. First, we train the SiMT model using ground-truth and equip the SiMT model with good translation capability. Subsequently, we use a pre-training strategy to train the tailor, enabling it to establish a favorable initial state and converge faster. Finally, we fine-tune the tailor by optimiz-\ning the two rewards using reinforcement learning, where the output of the tailor serves as the training target for the SiMT model after being reduced. In the third stage, the tailor and SiMT model are jointly optimized and share the output the of the encoder. Next, we describe our three-stage training method in detail.\nTraining the Base Model In our architecture, the tailor cross-attends to the output of the encoder to adjust ground-truth based on source information. As a result, before training the tailor module, we need a well-trained SiMT model as the base model. In our method, we choose the Wait-k policy (Ma et al., 2019) and HMT model (Zhang and Feng, 2023b) as the base model for fixed policy and adaptive policy, respectively. The base model is trained using the cross-entropy loss. Once the training of the base model is completed, we optimize the tailor module, which can provide the tailored reference for the SiMT models trained across different latency settings.\nPre-training Tailor The tailor adopts the architecture of a non-autoregressive decoder (Gu et al., 2018). The non-autoregressive architecture has demonstrated excellent performance (Qian et al., 2020; Huang et al., 2022). Importantly, it enables the simultaneous generation of target tokens across all positions, making it highly efficient for reinforcement learning. However, if we train the tailor using reinforcement learning directly, it will converge to a suboptimal state in which the tokens at each position are some frequent tokens (Shao et al., 2022). This behavior is attributed to the exploration-based nature of reinforcement learning, highlighting the need for a favorable initial state for the model (Lopes et al., 2012). Since the tailored reference is modified from ground-truth, we let it learn from ground-truth during pre-training and then fine-tune it using reinforcement learning. The details of pre-training stage are introduced below.\nTo keep the output of the tailor from being limited by the length of ground-truth, the tailor upsamples ground-truth to get the input of the tailor, denoted as y\u2032. During training, CTC loss (Libovick\u00fd and Helcl, 2018) is used to optimize the tailor.\nDenoting the output of the tailor as a = (a1, ..., aT ), the probability distribution modeled by the tailor can be presented as:\npa(a | x,y\u2032) = T\u220f t=1 pa(at | x,y\u2032), (5)\nwhere T is the output length of tailor and is a multiple of the length of y. Subsequently, we can get the normal sequence s by applying collapse function \u0393\u22121 to a and the distribution of s is calculated by considering all possible a:\nps(s | x,y\u2032) = \u2211\na\u2208\u0393(s)\npa(a | x,y\u2032). (6)\nTo make the tailor learn from ground-truth, the tailor is optimized by minimizing the negative loglikelihood:\nLpt = \u2212 log ps(y | x,y\u2032), (7)\nwhich can be efficiently calculated through dynamic programming (Graves et al., 2006).\nRL Fine-tuning After completing the pretraining, the tailor is already in a favorable initial state. We quantify the requirements for tailored reference as two rewards and fine-tune the tailor using reinforcement learning. We then introduce the two reward functions.\nOn the one hand, the tailored reference should not force the model to predict the target tokens before reading corresponding source information, which means the SiMT model can handle the word reorderings between the tailored reference and the source sentence at that latency (Zhang et al., 2022). Therefore, we make the tailor learn from nonanticipatory reference yna, which is generated by applying the corresponding Wait-k policy to the Full-sentence model. It has the word order that matches the latency and maintains the original semantics (Chen et al., 2021). We employ reward Rna to measure the similarity between the output of tailor and non-anticipatory reference. On the other hand, the tailored reference should remain faithful to ground-truth. We introduce the reward Rgt to measure the similarity between ground-truth and the output of the tailor. By striking an appropriate balance between Rna and Rgt, we can obtain the tailored reference.\nGiven the output a of tailor, we can obtain the normal sentence s by removing the repetitive and blank tokens (Libovick\u00fd and Helcl, 2018). We use BLEU (Papineni et al., 2002) to measure the similarity between two sequences. Therefore, Rna and Rgt for the output of tailor is calculated as:\nRna(s) = BLEU(s,yna), (8)\nRgt(s) = BLEU(s,y). (9)\nBased on these two rewards, we can obtain the final reward R by balancing Rna and Rgt:\nR(s) = (1\u2212 \u03b1)Rna(s) + \u03b1Rgt(s), (10)\nwhere \u03b1 \u2208 [0, 1] is a hyperparameter. Subsequently, we use REINFORCE algorithm (Williams, 1992) to optimize the final reward R to obtain the tailored reference:\n\u2207\u03b8J (\u03b8) = \u2207\u03b8 \u2211 s ps(s | x,y\u2032, \u03b8)R(s)\n= E s\u223cps\n[\u2207\u03b8 log ps(s | x,y\u2032, \u03b8)R(s)]\n= E a\u223cpa [\u2207\u03b8 log ps(\u0393\u22121(a) | x,y\u2032, \u03b8)R(\u0393\u22121(a)]. (11)\nwhere \u0393\u22121 represents the collapse function and \u03b8 denotes the parameter of tailor. During training, we sample the sequence a from the distribution pa(a | x,y\u2032, \u03b8) using Monte Carlo method. As the tailor adopts a non-autoregressive structure where all positions are independent of each other, we can concurrently sample tokens for all positions from the distribution. We then apply collapse function to sequence a to obtain the normal sequence s, which is used to compute the reward\nR(s) and update the tailor with estimated gradient \u2207\u03b8 log ps(s | x,y\u2032, \u03b8)R(s). In the calculation of ps(s | x,y\u2032, \u03b8), we use dynamic programming to accelerate the process. Additionally, we adopt the baseline reward strategy to reduce the variance of the estimated gradient (Weaver and Tao, 2001).\nIn this stage, we utilize reinforcement learning to optimize the final reward R(s) and train the SiMT model with tailored reference using Lt_simt. As a result, the SiMT model and the tailor are jointly optimized to enhance performance."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate our method on three translation tasks.\nIWSLT152 English\u2192Vietnamese (En\u2192Vi) (Cettolo et al., 2015) We use TED tst2012 as the development set and TED tst2013 as the test set. In line with Ma et al. (2020), we replace the tokens occurring less than 5 with \u27e8unk\u27e9. Consequently, the vocabulary sizes of English and Vietnamese are 17K and 7.7K, respectively.\nWMT163 English\u2192Romanian (En\u2192Ro) We 2https://nlp.stanford.edu/projects/nmt/ 3www.statmt.org/wmt16/\nuse newsdev-2016 as the development set and newstest-2016 as the test set. The source and target languages employ a shared vocabulary. Other settings are consistent with Gu et al. (2018).\nWMT154 German\u2192English (De\u2192En) Following Ma et al. (2020), we use newstest2013 as development set and newstest2015 as test set. We apply BPE (Sennrich et al., 2016) with 32K subword units and use a shared vocabulary between source and target."
        },
        {
            "heading": "4.2 System Settings",
            "text": "Our experiments involve the following methods and we briefly introduce them.\nFull-sentence model is the conventional fullsentence machine translation model.\nWait-k policy (Ma et al., 2019) initially reads k tokens and then alternates between writing and reading a source token.\nMulti-path (Elbayad et al., 2020) introduces the unidirectional encoder and trains the model by sampling the latency k.\nAdaptive Wait-k (Zheng et al., 2020) employs multiple Wait-k models through heuristic method to achieve adaptive policy.\nMMA (Ma et al., 2020) makes each head determine the translation policy by predicting the Bernoulli variable.\nMoE Wait-k (Zhang and Feng, 2021), the current state-of-the-art fixed policy, treats each head as an expert and integrates the decisions of all experts.\nPED (Guo et al., 2022) implements the adaptive policy via integrating post-evaluation into the fixed translation policy.\nBS-SiMT (Guo et al., 2023) constructs the optimal policy online via binary search.\nITST (Zhang and Feng, 2022b) treats the translation as information transport from source to target.\nHMT (Zhang and Feng, 2023b) models simultaneous machine translation as a Hidden Markov Model, and achieves the current state-of-the-art performance in SiMT.\n*+100% Pseudo-Refs (Chen et al., 2021) trains the Wait-k model with ground-truth and pseudo reference, which is generated by applying Wait-k policy to the Full-sentence model.\n*+Top 40% Pseudo-Refs (Chen et al., 2021) filters out pseudo references in the top 40% of quality to train the model with ground-truth.\nWait-k + Tailor applies our method on Wait-k. 4www.statmt.org/wmt15/\nHMT + Tailor applies our method on HMT. All systems are based on Transformer architecture (Vaswani et al., 2017) and adapted from Fairseq Library (Ott et al., 2019). We apply Transformer-Small (6 layers, 4 heads) for En\u2192Vi task and Transform-Base (6 layers, 8 heads) for En\u2192Ro and De\u2192En tasks. Since PED and Adaptive Wait-k do not report the results on the En\u2192Ro task, we do not compare them in the experiment. For our method, we adopt the non-regressive decoder structure with 2 layers for the tailor. We empirically set the hyperparameter \u03b1 as 0.2. The non-anticipatory reference used for RL Fine-tuning of SiMT model is generated by Test-time Wait-k method (Ma et al., 2019) with corresponding latency. Other system settings are consistent with Ma et al. (2020) and Zhang and Feng (2023b). The detailed settings are shown in Appendix C. We use greedy search during inference and evaluate all methods with translation quality estimated by BLEU (Papineni et al., 2002) and latency measured by Average Lagging (AL) (Ma et al., 2019)."
        },
        {
            "heading": "4.3 Main Results",
            "text": "The performance comparison between our method and other SiMT approaches on three translation tasks is illustrated in Figure 3 and Figure 4. Our method achieves state-of-the-art translation performance in both fixed and adaptive policies. When comparing with other training methods in Figure 5, our approach also achieves superior performance.\nWhen selecting the most commonly used Waitk policy as the base model, our method outperforms MoE Wait-k, which is the current state-ofthe-art fixed policy. Compared to Wait-k policy,\nour method brings significant improvement, especially under low latency. Wait-k policy is trained on ground-truth and cannot be adjusted, which may force the model to predict tokens before reading corresponding source information (Ma et al., 2019). In contrast, our method provides a tailored reference for the SiMT model, thereby alleviating the issue of forced anticipations. Our method also exceeds Multi-path and MoE Wait-k. These two methods are trained using multiple Wait-k policies (Elbayad et al., 2020) and gain the ability to translate under multiple latency (Zhang and Feng, 2021), but they still utilize ground-truth at all latency, leading to lower performance.\nOur method can further enhance the SiMT performance by selecting adaptive policy as the base model. As the current state-of-the-art adaptive policy, HMT possesses the ability to dynamically adjust policy to balance latency and translation quality (Zhang and Feng, 2023b). However, it still relies on ground-truth for training SiMT models across different latency settings. By providing a tailored reference that matches the latency, our method can alleviate the latency burden of the SiMT model, resulting in state-of-the-art performance.\nOur method also surpasses other training approaches. Ground-truth is not suitable for incremental input due to word reorderings, resulting in performance degradation (Zhang and Feng, 2022b). On the contrary, pseudo reference can avoid forced anticipations during training (Chen et al., 2021). However, it is constructed offline by applying the Wait-k policy on the Full-sentence model. It imposes an upper bound on the performance of the SiMT model. The tailored reference avoids forced anticipations while maintaining high quality, leading to the best performance.\nIn addition to enhancing translation performance, our method effectively narrows the gap between fixed and adaptive policies. By leveraging our method, the SiMT model can achieve comparable performance to Full-sentence translation with lower latency on En\u2192Vi and De\u2192En tasks."
        },
        {
            "heading": "5 Analysis",
            "text": "To gain a comprehensive understanding of our method, we conducted multiple analyses. All of the following results are reported on De\u2192En task."
        },
        {
            "heading": "5.1 Ablation Study",
            "text": "We conduct ablation studies on the structure and training method of tailor to investigate the influence of different settings. The experiments all use Wait-k model as the base model with k set to 3. Table 2 presents a comparison of different structures. The best performance is achieved when the tailor has 2 layers. The performance can be negatively affected by both excessive layers and insufficient layers. Table 3 illustrates the results of the ablation study on the training method. Each stage of the training method contributes to the performance of the SiMT model and the training stage of the base model has the most significant impact on the performance. This can be attributed to the fact that a well-trained encoder can provide accurate source information to the tailor, enabling the generation of appropriate tailored references. Additionally, when \u03b1 is selected as 0.2, our method yields the best performance, indicating an optimal balance between word order and quality for the tailor."
        },
        {
            "heading": "5.2 Analysis of Tailored Reference",
            "text": "Anticipation Rate Furthermore, we conduct an analysis of the tailored reference to assess its influence. We first explore the rate of forced anticipations caused by using different references during training. Using the anticipation rate (AR) (Chen et al., 2021) as the metric, the results in Table 4 show that the tailored reference can effectively re-\nduce the forced anticipations during the training of the SiMT model under all latency. This implies that, compared to ground-truth, the word reorderings between the tailored reference and the source sentence can be more effectively handled by the SiMT model at different latency.\nQuality However, one concern is whether the quality of tailored reference will deteriorate like non-anticipatory reference after adjusting the word order. To assess this, we compare different references with ground-truth to measure their quality. As shown in Table 5, we observe that the tailored reference exhibits significantly higher quality than the non-anticipatory reference. Therefore, our method successfully reduces the rate of forced anticipations during training while remaining faithful to ground-truth. To provide a better understanding of the tailored reference, we include several illustrative cases in Appendix B."
        },
        {
            "heading": "5.3 Hallucination in Translation",
            "text": "If the SiMT model is forced to predict target tokens before reading corresponding source information during training, there is a high likelihood of generating hallucinations during inference (Ma et al., 2019). To quantify the presence of hallucinations in the translation, we introduce hallucination rate (HR) (Chen et al., 2021) for evaluation. Figure 6 illustrates that the SiMT model trained with the tailored reference demonstrates a reduced probability of generating hallucinations. Moreover, even though the adaptive policy can adjust its behavior based on the translation status, our approach still ef-\nfectively mitigates the hallucinations by alleviating the burden of latency. This signifies that minimizing forced predictions during training can enhance the faithfulness of the translation to the source sentence, thereby improving translation quality (Ma et al., 2023)."
        },
        {
            "heading": "6 Related Work",
            "text": "Simultaneous machine translation (SiMT) generates translation while reading the source sentence. It requires a policy to determine the source information read when translating each target token, thus striking a balance between latency and translation quality. Current research on SiMT mainly focuses on two areas: policy improvement and adjustment of the training method.\nFor policy improvement, it aims to provide sufficient source information for translation while avoiding unnecessary latency. Ma et al. (2019) propose Wait-k policy, which initially reads k tokens and alternates between writing and reading one token. Zhang and Feng (2021) enable each head to obtain the information with a different fixed latency and integrate the decisions of multiple heads for translation. However, the fixed policy cannot be flexibly adjusted based on context, resulting in suboptimal performance. Ma et al. (2020) allow each head to determine its own policy and make all heads decide on the translation. Miao et al. (2021) propose a generative framework, which uses a re-parameterized Poisson prior to regularising the policy. Zhang and Feng (2023a) propose a segmentation policy for the source input. Zhang and Feng (2023b) model the simultaneous machine translation as a Hidden\nMarkov Model and achieve state-of-the-art performance. However, these methods are all trained with ground-truth, leading to forced predictions at low latency.\nFor the adjustment of the training method, it wants to supplement the missing full-sentence information or cater to the requirements of latency. Zhang et al. (2021) shorten the distance of source hidden states between the SiMT model and the Full-sentence model. This makes the source hidden states implicitly embed future information, but encourages data-driven prediction. On the other hand, Chen et al. (2021) try to train the model with non-anticipatory reference, which can be effectively handled by the SiMT model at that latency. However, while non-anticipatory reference can alleviate forced predictions at low latency, it hinders performance improvement at high latency.\nTherefore, we want to provide a tailored reference for the SiMT models trained at different latency. The tailored reference should avoid forced anticipations and exhibit high quality. In view of the good structure and superior performance of the non-autoregressive model (Gu et al., 2018; Libovick\u00fd and Helcl, 2018), we utilize it to modify the ground-truth to the tailored reference."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose a novel method to provide a tailored reference for the training of SiMT model. Experiments and extensive analyses demonstrate that our method achieves state-of-the-art performance in both fixed and adaptive policies and effectively reduces the hallucinations in translation.\nLimitations\nRegarding the system settings, we investigate the impact of the number of layers and training methods on performance. We think that further exploration of system settings could potentially yield even better results. Additionally, the tailor module aims to avoid forced anticipations and maintain faithfulness to ground-truth. If we can add language-related features to the SiMT model using a heuristic method, it may produce more suitable references for the SiMT model. We leave it for future work."
        },
        {
            "heading": "Acknowledgment",
            "text": "We thank all anonymous reviewers for their valuable suggestions."
        },
        {
            "heading": "A Anticipation & Hallucination Rate",
            "text": "In our analyses, we evaluate the reference and translation using anticipation rate (AR) and hallucination rate (HR), respectively. We then introduce the calculation method of these two evaluation metrics in detail.\nGiven a sentence pair (x, y), there exists an alignment set h, which is a set of source-target index pairs (j, i) where jth source token xj aligns to the ith target token yi. If we apply the policy g = (g1, ..., gI) on this sentence pair, the target token yi is forcibly anticipated (A(i,h,g) = 1) if it aligns to least one source token xj where j > gi:\nA(i,h,g)=1[{(j, i) \u2208 h |j > gi} =\u0338 \u2205]. (12)\nTherefore, we can define the anticipation rate (AR) of (x,y,h) under the policy g:\nAR(x,y,h,g)= 1\n|y| |y|\u2211 i=1 A(i,h,g). (13)\nThe anticipation rate is a metric used to quantify the degree to which the target token is predicted before reading all relevant source tokens.\nWe then introduce the hallucination rate (HR). We first define the translation as y\u0302. A target token y\u0302i in translation y\u0302 is a hallucination (H(i,h)=1) if it can not be aligned to any source token:\nH(i,h)=1[{(j, i) \u2208 h} = \u2205]. (14)\nTherefore, the hallucination rate can be defined as:\nHR(x, y\u0302,h)= 1\n|y\u0302| |y\u0302|\u2211 i=1 H(i,h). (15)"
        },
        {
            "heading": "B Case Study",
            "text": "We also provide two cases in the De\u2192En test set to understand our method. The cases are shown in Figure 7 and Figure 8. It presents that the tailored reference is more consistent with the word order requirements of the specific latency.\nIn Figure 7, if we train the SiMT model on Wait1 policy with the ground-truth, it will be forced to predict \u2018with him\u2019 before reading \u2018mit ihm\u2019 during training. However, training the SiMT model with tailored reference will eliminate forced predictions by adjusting \u2018like that\u2019 to \u2018such\u2019 and positioning it forward. Importantly, the tailored reference also maintains the original semantics. This shows that\nthe order of the source sentence and the tailored sentence is consistent, which makes it suitable for Wait-1 policy.\nIn Figure 8, using ground-truth as the training target of Wait-1 policy also forces the model to predict \u2018ordering online\u2019 before reading \u2018Online\u2019 and \u2018Bestellung\u2019. In contrast, by replacing \u2018ordering online\u2019 with \u2018an online order\u2019, the word order of tailored reference is the same as the source sentence, thereby avoiding forced anticipations during the training of Wait-1 policy."
        },
        {
            "heading": "C Hyperparameters",
            "text": "The system settings on three translation tasks are shown in Table 6. For more detailed implementation issues, please refer to our publicly available code."
        },
        {
            "heading": "D Numerical Results",
            "text": "In addition to the translation performance comparison in Figure 3 and Figure 4, we also provide corresponding numerical results for reference. Table 7, 8, 9 respectively report the numerical results on IWSLT15 En\u2192Vi, WMT16 En\u2192Ro and WMT15 De\u2192En measured by AL (Ma et al., 2019) and BLEU (Papineni et al., 2002).\nWMT16 En\u2192Ro"
        }
    ],
    "title": "Simultaneous Machine Translation with Tailored Reference",
    "year": 2023
}