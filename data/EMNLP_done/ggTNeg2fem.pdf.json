{
    "abstractText": "Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned image. Multimodal misinformation is perceived as more credible by humans, and spreads faster than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on text. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terms used in different communities and map them to our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mubashara Akhtar"
        },
        {
            "affiliations": [],
            "name": "Michael Schlichtkrull"
        },
        {
            "affiliations": [],
            "name": "Zhijiang Guo"
        },
        {
            "affiliations": [],
            "name": "Oana Cocarascu"
        },
        {
            "affiliations": [],
            "name": "Elena Simperl"
        },
        {
            "affiliations": [],
            "name": "Andreas Vlachos"
        }
    ],
    "id": "SP:6ac78047b9a3c0b46bb89485a446d3edc9dc5e73",
    "references": [
        {
            "authors": [
                "Sara Abdali."
            ],
            "title": "Multi-modal misinformation detection: Approaches, challenges and opportunities",
            "venue": "CoRR, abs/2203.13883.",
            "year": 2022
        },
        {
            "authors": [
                "Sahar Abdelnabi",
                "Rakibul Hasan",
                "Mario Fritz."
            ],
            "title": "Open-domain, content-based, multi-modal fact-checking of out-of-context images via online resources",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Shruti Agarwal",
                "Hany Farid",
                "Yuming Gu",
                "Mingming He",
                "Koki Nagano",
                "Hao Li."
            ],
            "title": "Protecting world leaders against deep fakes",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2019, Long Beach, CA, USA, June",
            "year": 2019
        },
        {
            "authors": [
                "Mubashara Akhtar",
                "Oana Cocarascu",
                "Elena Simperl."
            ],
            "title": "PubHealthTab: A public health table-based dataset for evidence-based fact checking",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1\u201316, Seattle, United States. As-",
            "year": 2022
        },
        {
            "authors": [
                "Mubashara Akhtar",
                "Oana Cocarascu",
                "Elena Simperl."
            ],
            "title": "Reading and reasoning over chart images for evidence-based automated fact-checking",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 399\u2013414, Dubrovnik,",
            "year": 2023
        },
        {
            "authors": [
                "Firoj Alam",
                "Stefano Cresci",
                "Tanmoy Chakraborty",
                "Fabrizio Silvestri",
                "Dimiter Dimitrov",
                "Giovanni Da San Martino",
                "Shaden Shaar",
                "Hamed Firooz",
                "Preslav Nakov."
            ],
            "title": "A survey on multimodal disinformation detection",
            "venue": "Proceedings of the 29th Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Rami Aly",
                "Zhijiang Guo",
                "Michael Sejr Schlichtkrull",
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Oana Cocarascu",
                "Arpit Mittal"
            ],
            "title": "FEVEROUS: fact extraction and verification over unstructured and structured information",
            "year": 2021
        },
        {
            "authors": [
                "Irene Amerini",
                "Leonardo Galteri",
                "Roberto Caldelli",
                "Alberto Del Bimbo."
            ],
            "title": "Deepfake video detection through optical flow based CNN",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshops, ICCV Workshops 2019, Seoul, Korea (South),",
            "year": 2019
        },
        {
            "authors": [
                "Sabrine Amri",
                "Dorsaf Sallami",
                "Esma A\u00efmeur."
            ],
            "title": "EXMULF: an explainable multimodal content-based fake news detection system",
            "venue": "Foundations and Practice of Security - 14th International Symposium, FPS 2021, Paris, France, December 7-10, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Shivangi Aneja",
                "Christoph Bregler",
                "Matthias Nie\u00dfner"
            ],
            "title": "Catching out-of-context misin",
            "year": 2021
        },
        {
            "authors": [
                "Pepa Atanasova",
                "Jakob Grue Simonsen",
                "Christina Lioma",
                "Isabelle Augenstein."
            ],
            "title": "Generating fact checking explanations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352\u20137364, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Tadas Baltrusaitis",
                "Chaitanya Ahuja",
                "Louis-Philippe Morency."
            ],
            "title": "Multimodal machine learning: A survey and taxonomy",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 41(2):423\u2013443.",
            "year": 2019
        },
        {
            "authors": [
                "Giscard Biamby",
                "Grace Luo",
                "Trevor Darrell",
                "Anna Rohrbach."
            ],
            "title": "Twitter-COMMs: Detecting climate, COVID, and military multimodal misinformation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Christina Boididou",
                "Symeon Papadopoulos",
                "Yiannis Kompatsiaris",
                "Steve Schifferes",
                "Nic Newman."
            ],
            "title": "Challenges of computational verification in social multimedia",
            "venue": "Proceedings of the 23rd International Conference on World Wide Web, pages",
            "year": 2014
        },
        {
            "authors": [
                "Kurt D. Bollacker",
                "Colin Evans",
                "Praveen K. Paritosh",
                "Tim Sturge",
                "Jamie Taylor."
            ],
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "venue": "Proceedings of the ACM SIGMOD International Conference on Management of",
            "year": 2008
        },
        {
            "authors": [
                "Nicol\u00f2 Bonettini",
                "Edoardo Daniele Cannas",
                "Sara Mandelli",
                "Luca Bondi",
                "Paolo Bestagini",
                "Stefano Tubaro."
            ],
            "title": "Video face manipulation detection through ensemble of cnns",
            "venue": "25th International Conference on Pattern Recognition, ICPR 2020, Vir-",
            "year": 2020
        },
        {
            "authors": [
                "Juan Cao",
                "Peng Qi",
                "Qiang Sheng",
                "Tianyun Yang",
                "Junbo Guo",
                "Jintao Li."
            ],
            "title": "Exploring the role of visual content in fake news detection",
            "venue": "CoRR, abs/2003.05096.",
            "year": 2020
        },
        {
            "authors": [
                "Gullal Singh Cheema",
                "Sherzod Hakimov",
                "Abdul Sittar",
                "Eric M\u00fcller-Budack",
                "Christian Otto",
                "Ralph Ewerth."
            ],
            "title": "MM-claims: A dataset for multimodal claim detection in social media",
            "venue": "Findings of the Association for Computational Linguistics: NAACL",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang."
            ],
            "title": "Tabfact: A large-scale dataset for table-based fact verification",
            "venue": "8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Cooper."
            ],
            "title": "A concise history of the fauxtography blogstorm in the 2006 lebanon war",
            "venue": "American Communication Journal, 9.",
            "year": 2007
        },
        {
            "authors": [
                "Davide Cozzolino",
                "Andreas R\u00f6ssler",
                "Justus Thies",
                "Matthias Nie\u00dfner",
                "Luisa Verdoliva."
            ],
            "title": "Idreveal: Identity-aware deepfake video detection",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Dang",
                "Feng Liu",
                "Joel Stehouwer",
                "Xiaoming Liu",
                "Anil K. Jain."
            ],
            "title": "On the detection of digital face manipulation",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Brian Dolhansky",
                "Russ Howes",
                "Ben Pflaum",
                "Nicole Baram",
                "Cristian Canton-Ferrer."
            ],
            "title": "The deepfake detection challenge (DFDC) preview dataset",
            "venue": "CoRR, abs/1910.08854.",
            "year": 2019
        },
        {
            "authors": [
                "Florian Eyben",
                "Felix Weninger",
                "Florian Gro\u00df",
                "Bj\u00f6rn W. Schuller."
            ],
            "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
            "venue": "ACM Multimedia Conference, MM \u201913, Barcelona, Spain, October 21-25, 2013, pages",
            "year": 2013
        },
        {
            "authors": [
                "Florian Eyben",
                "Martin W\u00f6llmer",
                "Bj\u00f6rn W. Schuller."
            ],
            "title": "Openear - introducing the munich open-source emotion and affect recognition toolkit",
            "venue": "Affective Computing and Intelligent Interaction, Third International Conference and Workshops, ACII 2009, Am-",
            "year": 2009
        },
        {
            "authors": [
                "Yi Fung",
                "Christopher Thomas",
                "Revanth Gangi Reddy",
                "Sandeep Polisetty",
                "Heng Ji",
                "Shih-Fu Chang",
                "Kathleen McKeown",
                "Mohit Bansal",
                "Avi Sil."
            ],
            "title": "InfoSurgeon: Cross-media fine-grained information consistency checking for fake news detection",
            "venue": "Proceed-",
            "year": 2021
        },
        {
            "authors": [
                "Kiran Garimella",
                "Dean Eckles."
            ],
            "title": "Images and misinformation in political groups: Evidence from whatsapp in india",
            "venue": "CoRR, abs/2005.09784.",
            "year": 2020
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron C. Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial networks",
            "venue": "Commun. ACM, 63(11):139\u2013 144.",
            "year": 2020
        },
        {
            "authors": [
                "D Graves"
            ],
            "title": "Understanding the promise and limits of automated fact-checking",
            "year": 2018
        },
        {
            "authors": [
                "Luca Guarnera",
                "Oliver Giudice",
                "Sebastiano Battiato."
            ],
            "title": "Level up the deepfake detection: a method to effectively discriminate images generated by GAN architectures and diffusion models",
            "venue": "CoRR, abs/2303.00608.",
            "year": 2023
        },
        {
            "authors": [
                "David Guera",
                "Edward J. Delp."
            ],
            "title": "Deepfake video detection using recurrent neural networks",
            "venue": "15th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2018, Auckland, New Zealand, November 27-30, 2018, pages 1\u20136.",
            "year": 2018
        },
        {
            "authors": [
                "Wenzhong Guo",
                "Jianwen Wang",
                "Shiping Wang."
            ],
            "title": "Deep multimodal representation learning: A survey",
            "venue": "IEEE Access, 7:63373\u201363394.",
            "year": 2019
        },
        {
            "authors": [
                "Zhijiang Guo",
                "Michael Schlichtkrull",
                "Andreas Vlachos."
            ],
            "title": "A survey on automated fact-checking",
            "venue": "Transactions of the Association for Computational Linguistics, 10:178\u2013206.",
            "year": 2022
        },
        {
            "authors": [
                "Aditi Gupta",
                "Hemank Lamba",
                "Ponnurangam Kumaraguru",
                "Anupam Joshi."
            ],
            "title": "Faking sandy: characterizing and identifying fake images on twitter during hurricane sandy",
            "venue": "22nd International World Wide Web Conference, WWW \u201913, Rio de Janeiro,",
            "year": 2013
        },
        {
            "authors": [
                "Brazil",
                "May"
            ],
            "title": "Companion Volume, pages 729\u2013736",
            "venue": "International World Wide Web Conferences Steering Committee / ACM",
            "year": 2013
        },
        {
            "authors": [
                "Ashim Gupta",
                "Vivek Srikumar."
            ],
            "title": "X-fact: A new benchmark dataset for multilingual fact checking",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Vipin Gupta",
                "Rina Kumari",
                "Nischal Ashok",
                "Tirthankar Ghosal",
                "Asif Ekbal"
            ],
            "title": "MMM: an emotion",
            "year": 2022
        },
        {
            "authors": [
                "Michael Hameleers",
                "Thomas E Powell",
                "Toni GLA Van Der Meer",
                "Lieke Bos."
            ],
            "title": "A picture paints a thousand lies? the effects and mechanisms of multimodal disinformation and rebuttals disseminated via social media",
            "venue": "Political Communication, 37(2):281\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Hicham Hammouchi",
                "Mounir Ghogho."
            ],
            "title": "Evidence-aware multilingual fake news detection",
            "venue": "IEEE Access, 10:116808\u2013116818.",
            "year": 2022
        },
        {
            "authors": [
                "Naeemul Hassan",
                "Bill Adair",
                "James T Hamilton",
                "Chengkai Li",
                "Mark Tremayne",
                "Jun Yang",
                "Cong Yu."
            ],
            "title": "The quest to automate fact-checking",
            "venue": "Proceedings of the 2015 computation+ journalism symposium. Citeseer.",
            "year": 2015
        },
        {
            "authors": [
                "Naeemul Hassan",
                "Chengkai Li",
                "Mark Tremayne."
            ],
            "title": "Detecting check-worthy factual claims in presidential debates",
            "venue": "Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick."
            ],
            "title": "Mask r-cnn",
            "venue": "2017 IEEE International Conference on Computer Vision (ICCV), pages 2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Silvan Heller",
                "Luca Rossetto",
                "Heiko Schuldt."
            ],
            "title": "The ps-battles dataset - an image collection for image manipulation detection",
            "venue": "CoRR, abs/1804.04866.",
            "year": 2018
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput., 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Rui Hou",
                "Ver\u00f3nica P\u00e9rez-Rosas",
                "Stacy L. Loeb",
                "Rada Mihalcea."
            ],
            "title": "Towards automatic detection of misinformation in online medical videos",
            "venue": "International Conference on Multimodal Interaction, ICMI 2019, Suzhou, China, October 14-18, 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Xuming Hu",
                "Zhijiang Guo",
                "Junzhe Chen",
                "Lijie Wen",
                "Philip S. Yu."
            ],
            "title": "MR2: A benchmark for multimodal retrieval-augmented rumor detection in social media",
            "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in",
            "year": 2023
        },
        {
            "authors": [
                "Minyoung Huh",
                "Andrew Liu",
                "Andrew Owens",
                "Alexei A. Efros."
            ],
            "title": "Fighting fake news: Image splice detection via learned self-consistency",
            "venue": "Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Pro-",
            "year": 2018
        },
        {
            "authors": [
                "Cherilyn Ireton",
                "Julie Posetti."
            ],
            "title": "Journalism, fake news & disinformation: handbook for journalism education and training",
            "venue": "Unesco Publishing.",
            "year": 2018
        },
        {
            "authors": [
                "Omar Ismael Al-Sanjary",
                "Ahmed Abdullah Ahmed",
                "Ghazali Sulong."
            ],
            "title": "Development of a video tampering dataset for forensic investigation",
            "venue": "Forensic Science International, 266:565\u2013572.",
            "year": 2016
        },
        {
            "authors": [
                "Ayush Jaiswal",
                "Ekraam Sabir",
                "Wael Abd-Almageed",
                "Premkumar Natarajan."
            ],
            "title": "Multimedia semantic integrity assessment using joint embedding of images and text",
            "venue": "Proceedings of the 2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA,",
            "year": 2017
        },
        {
            "authors": [
                "Liming Jiang",
                "Ren Li",
                "Wayne Wu",
                "Chen Qian",
                "Chen Change Loy"
            ],
            "title": "Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhijing Jin",
                "Abhinav Lalwani",
                "Tejas Vaidhya",
                "Xiaoyu Shen",
                "Yiwen Ding",
                "Zhiheng Lyu",
                "Mrinmaya Sachan",
                "Rada Mihalcea",
                "Bernhard Schoelkopf."
            ],
            "title": "Logical fallacy detection",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiwei Jin",
                "Juan Cao",
                "Han Guo",
                "Yongdong Zhang",
                "Jiebo Luo."
            ],
            "title": "Multimodal fusion with recurrent neural networks for rumor detection on microblogs",
            "venue": "Proceedings of the 2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA, USA, Octo-",
            "year": 2017
        },
        {
            "authors": [
                "Marvin Kalb",
                "Carol Saivetz."
            ],
            "title": "The israeli\u2014hezbollah war of 2006: The media as a weapon in asymmetrical conflict",
            "venue": "Harvard International Journal of Press/Politics, 12(3):43\u201366.",
            "year": 2007
        },
        {
            "authors": [
                "Manvi Kamboj",
                "Christian Hessler",
                "Priyanka Asnani",
                "Kais Riani",
                "Mohamed Abouelenien."
            ],
            "title": "Multimodal political deception detection",
            "venue": "IEEE Multim., 28(1):94\u2013102.",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila."
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4401\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Hasam Khalid",
                "Shahroz Tariq",
                "Minha Kim",
                "Simon S. Woo."
            ],
            "title": "Fakeavceleb: A novel audio-video multimodal deepfake dataset",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Bench-",
            "year": 2021
        },
        {
            "authors": [
                "Dhruv Khattar",
                "Jaipal Singh Goud",
                "Manish Gupta",
                "Vasudeva Varma."
            ],
            "title": "MVAE: multimodal variational autoencoder for fake news detection",
            "venue": "The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pages 2915\u20132921.",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling."
            ],
            "title": "Autoencoding variational bayes",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
            "year": 2014
        },
        {
            "authors": [
                "Tomi Kinnunen",
                "Md. Sahidullah",
                "H\u00e9ctor Delgado",
                "Massimiliano Todisco",
                "Nicholas W.D. Evans",
                "Junichi Yamagishi",
                "Kong-Aik Lee."
            ],
            "title": "The asvspoof 2017 challenge: Assessing the limits of replay spoofing attack detection",
            "venue": "Interspeech 2017, 18th An-",
            "year": 2017
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Kopev",
                "Ahmed Ali",
                "Ivan Koychev",
                "Preslav Nakov."
            ],
            "title": "Detecting deception in political debates using acoustic and textual features",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore, December 14-18,",
            "year": 2019
        },
        {
            "authors": [
                "Neema Kotonya",
                "Francesca Toni."
            ],
            "title": "Explainable automated fact-checking: A survey",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5430\u20135443, Barcelona, Spain (Online). International Committee on Compu-",
            "year": 2020
        },
        {
            "authors": [
                "Neema Kotonya",
                "Francesca Toni."
            ],
            "title": "Explainable automated fact-checking for public health claims",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7740\u20137754, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Ziyi Kou",
                "Daniel Yue Zhang",
                "Lanyu Shang",
                "Dong Wang."
            ],
            "title": "Exfaux: A weakly supervised approach to explainable fauxtography detection",
            "venue": "2020 IEEE International Conference on Big Data (IEEE BigData 2020), Atlanta, GA, USA, December 10-13,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Kwon",
                "Jaeseong You",
                "Gyuhyeon Nam",
                "Sungwoo Park",
                "Gyeongsu Chae."
            ],
            "title": "Kodf: A largescale korean deepfake detection dataset",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October",
            "year": 2021
        },
        {
            "authors": [
                "Tuan-Vinh La",
                "Quang-Tien Tran",
                "Thanh-Phuc Tran",
                "Anh-Duy Tran",
                "Duc-Tien Dang-Nguyen",
                "MinhSon Dao."
            ],
            "title": "Multimodal cheapfakes detection by utilizing image captioning for global context",
            "venue": "ICDAR@ICMR 2022: Proceedings of the 3rd ACM",
            "year": 2022
        },
        {
            "authors": [
                "Claire Lauer."
            ],
            "title": "Contending with terms: \u201cmultimodal\u201d and \u201cmultimedia\u201d in the academic and public spheres",
            "venue": "Computers and Composition, 26(4):225\u2013 239.",
            "year": 2009
        },
        {
            "authors": [
                "Galina Lavrentyeva",
                "Sergey Novoselov",
                "Marina Volkova",
                "Yuri Matveev",
                "Maria De Marsico."
            ],
            "title": "Phonespoof: A new dataset for spoofing attack detection in telephone channel",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Mandar Joshi",
                "Iulia Turc",
                "Hexiang Hu",
                "Fangyu Liu",
                "Julian Eisenschlos",
                "Urvashi Khandelwal",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Stephan Lewandowsky",
                "John Cook",
                "Doug Lombardi"
            ],
            "title": "Debunking Handbook 2020",
            "year": 2020
        },
        {
            "authors": [
                "Manling Li",
                "Alireza Zareian",
                "Qi Zeng",
                "Spencer Whitehead",
                "Di Lu",
                "Heng Ji",
                "Shih-Fu Chang."
            ],
            "title": "Cross-media structured common space for multimedia event extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Yiyi Li",
                "Ying Xie."
            ],
            "title": "Is a picture worth a thousand words? an empirical study of image content and social media engagement",
            "venue": "Journal of Marketing Research, 57(1):1\u201319.",
            "year": 2020
        },
        {
            "authors": [
                "Yuezun Li",
                "Xin Yang",
                "Pu Sun",
                "Honggang Qi",
                "Siwei Lyu."
            ],
            "title": "Celeb-df: A large-scale challenging dataset for deepfake forensics",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Amir Zadeh",
                "Louis-Philippe Morency."
            ],
            "title": "Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions",
            "venue": "CoRR, abs/2209.03430.",
            "year": 2022
        },
        {
            "authors": [
                "Ying Lin",
                "Heng Ji",
                "Fei Huang",
                "Lingfei Wu."
            ],
            "title": "A joint neural model for information extraction with global features",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7999\u20138009, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Fangyu Liu",
                "Emanuele Bugliarello",
                "Edoardo Maria Ponti",
                "Siva Reddy",
                "Nigel Collier",
                "Desmond Elliott."
            ],
            "title": "Visually grounded reasoning across languages and cultures",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Fuxiao Liu",
                "Yaser Yacoob",
                "Abhinav Shrivastava."
            ],
            "title": "COVID-VTS: fact extraction and verification on short video platforms",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Hui Liu",
                "Wenya Wang",
                "Haoliang Li."
            ],
            "title": "Interpretable multimodal misinformation detection with logic reasoning",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 9781\u20139796. Associa-",
            "year": 2023
        },
        {
            "authors": [
                "V\u00edtor Louren\u00e7o",
                "Aline Paes."
            ],
            "title": "A modality-level explainable framework for misinformation checking in social networks",
            "venue": "CoRR, abs/2212.04272.",
            "year": 2022
        },
        {
            "authors": [
                "Grace Luo",
                "Trevor Darrell",
                "Anna Rohrbach."
            ],
            "title": "NewsCLIPpings: Automatic Generation of Out-ofContext Multimodal Media",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6801\u20136817, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Marie-Helen Maras",
                "Alex Alexandrou."
            ],
            "title": "Determining authenticity of video evidence in the age of artificial intelligence and in the wake of deepfake videos",
            "venue": "The International Journal of Evidence & Proof, 23(3):255\u2013262.",
            "year": 2019
        },
        {
            "authors": [
                "Alexandre Maros",
                "Jussara M. Almeida",
                "Marisa Vasconcelos."
            ],
            "title": "A study of misinformation in audio messages shared in whatsapp groups",
            "venue": "Disinformation in Open Online Media - Third Multidisciplinary International Symposium, MISDOOM 2021, Virtual",
            "year": 2021
        },
        {
            "authors": [
                "Priyanka Meel",
                "Dinesh Kumar Vishwakarma."
            ],
            "title": "Han, image captioning, and forensics ensemble multimodal fake news detection",
            "venue": "Information Sciences, 567:23\u201341.",
            "year": 2021
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "1st International Conference",
            "year": 2013
        },
        {
            "authors": [
                "Shreyash Mishra",
                "Suryavardan S",
                "Amrit Bhaskar",
                "Parul Chopra",
                "Aishwarya N. Reganti",
                "Parth Patwa",
                "Amitava Das",
                "Tanmoy Chakraborty",
                "Amit P. Sheth",
                "Asif Ekbal."
            ],
            "title": "FACTIFY: A multi-modal fact verification dataset",
            "venue": "Proceedings of the Workshop on",
            "year": 2022
        },
        {
            "authors": [
                "Eric M\u00fcller-Budack",
                "Jonas Theiner",
                "Sebastian Diering",
                "Maximilian Idahl",
                "Ralph Ewerth."
            ],
            "title": "Multimodal analytics for real-world news using measures of cross-modal entity consistency",
            "venue": "Proceedings of the 2020 on International Conference on Multimedia",
            "year": 2020
        },
        {
            "authors": [
                "Kai Nakamura",
                "Sharon Levy",
                "William Yang Wang."
            ],
            "title": "Fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France,",
            "year": 2020
        },
        {
            "authors": [
                "Preslav Nakov",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Tamer Elsayed",
                "Reem Suwaileh",
                "Llu\u00eds M\u00e0rquez",
                "Wajdi Zaghouani",
                "Pepa Atanasova",
                "Spas Kyuchukov",
                "Giovanni Da San Martino"
            ],
            "title": "Overview of the CLEF2018 checkthat! lab on automatic identification",
            "year": 2018
        },
        {
            "authors": [
                "Preslav Nakov",
                "David P.A. Corney",
                "Maram Hasanain",
                "Firoj Alam",
                "Tamer Elsayed",
                "Alberto Barr\u00f3n-Cede\u00f1o",
                "Paolo Papotti",
                "Shaden Shaar",
                "Giovanni Da San Martino."
            ],
            "title": "Automated fact-checking for assisting human fact-checkers",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Kartik Narayan",
                "Harsh Agarwal",
                "Kartik Thakral",
                "Surbhi Mittal",
                "Mayank Vatsa",
                "Richa Singh."
            ],
            "title": "Dfplatter: Multi-face heterogeneous deepfake dataset",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages",
            "year": 2023
        },
        {
            "authors": [
                "Eryn Newman",
                "Lynn Zhang"
            ],
            "title": "Truthiness: How Non-Probative Photos Shape Belief",
            "year": 2020
        },
        {
            "authors": [
                "Eryn J Newman",
                "Maryanne Garry",
                "Daniel M Bernstein",
                "Justin Kantner",
                "D Stephen Lindsay."
            ],
            "title": "Nonprobative photographs (or words) inflate truthiness",
            "venue": "Psychonomic Bulletin & Review, 19(5):969\u2013974.",
            "year": 2012
        },
        {
            "authors": [
                "Matthew L Newman",
                "James W Pennebaker",
                "Diane S Berry",
                "Jane M Richards."
            ],
            "title": "Lying words: Predicting deception from linguistic styles",
            "venue": "Personality and social psychology bulletin, 29(5):665\u2013675.",
            "year": 2003
        },
        {
            "authors": [
                "Dan Saattrup Nielsen",
                "Ryan McConville."
            ],
            "title": "Mumin: A large-scale multilingual multimodal factchecked misinformation social network dataset",
            "venue": "SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information",
            "year": 2022
        },
        {
            "authors": [
                "Yuval Nirkin",
                "Yosi Keller",
                "Tal Hassner."
            ],
            "title": "FSGAN: subject agnostic face swapping and reenactment",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages",
            "year": 2019
        },
        {
            "authors": [
                "Britt Paris",
                "Joan Donovan."
            ],
            "title": "Deepfakes and cheap fakes",
            "venue": "United States of America: Data & Society, 1.",
            "year": 2019
        },
        {
            "authors": [
                "Parth Patwa",
                "Shreyash Mishra",
                "Suryavardan S",
                "Amrit Bhaskar",
                "Parul Chopra",
                "Aishwarya Reganti",
                "Amitava Das",
                "Tanmoy Chakraborty",
                "Amit Sheth",
                "Asif Ekbal",
                "Chaitanya Ahuja"
            ],
            "title": "Benchmarking multimodal entailment for fact verification",
            "year": 2022
        },
        {
            "authors": [
                "Ver\u00f3nica P\u00e9rez-Rosas",
                "Mohamed Abouelenien",
                "Rada Mihalcea",
                "Mihai Burzo."
            ],
            "title": "Deception detection using real-life trial data",
            "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, Seattle, WA, USA, November 09 -",
            "year": 2015
        },
        {
            "authors": [
                "Tarunima Prabhakar",
                "Anushree Gupta",
                "Kruttika Nadig",
                "Denny George."
            ],
            "title": "Check mate: Prioritizing user generated multi-media content for fact-checking",
            "venue": "Proceedings of the Fifteenth International AAAI Conference on Web and Social Media, ICWSM 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Nathaniel Purwanto",
                "Joan Santoso",
                "Po-Ruey Lei",
                "Hui-Kuo Yang",
                "Wen-Chih Peng."
            ],
            "title": "Fakeclip: Multimodal fake caption detection with mixed languages for explainable visualization",
            "venue": "2021 International Conference on Technologies and Applica-",
            "year": 2021
        },
        {
            "authors": [
                "Peng Qi",
                "Yuyan Bu",
                "Juan Cao",
                "Wei Ji",
                "Ruihao Shui",
                "Junbin Xiao",
                "Danding Wang",
                "Tat-Seng Chua."
            ],
            "title": "Fakesv: A multimodal benchmark with rich social context for fake news detection on short video platforms",
            "venue": "CoRR, abs/2211.10973.",
            "year": 2022
        },
        {
            "authors": [
                "Peng Qi",
                "Juan Cao",
                "Tianyun Yang",
                "Junbo Guo",
                "Jintao Li."
            ],
            "title": "Exploiting multi-domain visual information for fake news detection",
            "venue": "2019 IEEE International Conference on Data Mining, ICDM 2019, Beijing, China, November 8-11, 2019, pages",
            "year": 2019
        },
        {
            "authors": [
                "Peng Qi",
                "Yuyang Zhao",
                "Yufeng Shen",
                "Wei Ji",
                "Juan Cao",
                "Tat-Seng Chua."
            ],
            "title": "Two heads are better than one: Improving fake news video detection by correlating with neighbors",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Shengsheng Qian",
                "Jinguang Wang",
                "Jun Hu",
                "Quan Fang",
                "Changsheng Xu."
            ],
            "title": "Hierarchical multi-modal contextual attention network for fake news detection",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in",
            "year": 2021
        },
        {
            "authors": [
                "Jingnong Qu",
                "Liunian Harold Li",
                "Jieyu Zhao",
                "Sunipa Dev",
                "Kai-Wei Chang."
            ],
            "title": "Disinfomeme: A multimodal dataset for detecting meme intentionally spreading out disinformation",
            "venue": "CoRR, abs/2205.12617.",
            "year": 2022
        },
        {
            "authors": [
                "Jingnong Qu",
                "Liunian Harold Li",
                "Jieyu Zhao",
                "Sunipa Dev",
                "Kai-Wei Chang."
            ],
            "title": "Disinfomeme: A multimodal dataset for detecting meme intentionally spreading out disinformation",
            "venue": "CoRR, abs/2205.12617.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Ricardo Reimao",
                "Vassilios Tzerpos."
            ],
            "title": "For: A dataset for synthetic speech detection",
            "venue": "2019 International Conference on Speech Technology and Human-Computer Dialogue, SpeD 2019, Timisoara, Romania, October 10-12, 2019, pages 1\u201310. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Gustavo Resende",
                "Philipe F. Melo",
                "Hugo Sousa",
                "Johnnatan Messias",
                "Marisa Vasconcelos",
                "Jussara M. Almeida",
                "Fabr\u00edcio Benevenuto."
            ],
            "title": "mis)information dissemination in whatsapp: Gathering, analyzing and countermeasures",
            "venue": "The World",
            "year": 2019
        },
        {
            "authors": [
                "Jonas Ricker",
                "Simon Damm",
                "Thorsten Holz",
                "Asja Fischer."
            ],
            "title": "Towards the detection of diffusion model deepfakes",
            "venue": "CoRR, abs/2210.14571.",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "Proceedings of the IEEE/CVF Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Andreas R\u00f6ssler",
                "Davide Cozzolino",
                "Luisa Verdoliva",
                "Christian Riess",
                "Justus Thies",
                "Matthias Nie\u00dfner."
            ],
            "title": "Faceforensics: A large-scale video dataset for forgery detection in human faces",
            "venue": "CoRR, abs/1803.09179.",
            "year": 2018
        },
        {
            "authors": [
                "Arjun Roy",
                "Asif Ekbal."
            ],
            "title": "Mulcob-mulfav: Multimodal content based multilingual fact verification",
            "venue": "International Joint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021, pages 1\u20138. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Anders S\u00f8gaard."
            ],
            "title": "Square one bias in NLP: Towards a multidimensional exploration of the research manifold",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2340\u20132354, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Ekraam Sabir",
                "Wael AbdAlmageed",
                "Yue Wu",
                "Prem Natarajan."
            ],
            "title": "Deep multimodal imagerepurposing detection",
            "venue": "2018 ACM Multimedia Conference on Multimedia Conference, MM 2018, Seoul, Republic of Korea, October 22-26, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Ekraam Sabir",
                "Jiaxin Cheng",
                "Ayush Jaiswal",
                "Wael AbdAlmageed",
                "Iacopo Masi",
                "Prem Natarajan."
            ],
            "title": "Recurrent convolutional strategies for face manipulation detection in videos",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Sejr Schlichtkrull",
                "Zhijiang Guo",
                "Andreas Vlachos."
            ],
            "title": "Averitec: A dataset for real-world claim verification with evidence from the web",
            "venue": "CoRR, abs/2305.13117.",
            "year": 2023
        },
        {
            "authors": [
                "Gautam Kishore Shahi",
                "Durgesh Nandini."
            ],
            "title": "Fakecovid- A multilingual cross-domain fact check news dataset for COVID-19",
            "venue": "Workshop Proceedings of the 14th International AAAI Conference on Web and Social Media, ICWSM 2020 Workshops, At-",
            "year": 2020
        },
        {
            "authors": [
                "Lanyu Shang",
                "Ziyi Kou",
                "Yang Zhang",
                "Dong Wang."
            ],
            "title": "A multimodal misinformation detector for COVID-19 short videos on tiktok",
            "venue": "2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, December 15-18, 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Lanyu Shang",
                "Ziyi Kou",
                "Yang Zhang",
                "Dong Wang."
            ],
            "title": "A duo-generative approach to explainable multimodal COVID-19 misinformation detection",
            "venue": "WWW \u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pages 3623\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Rui Shao",
                "Tianxing Wu",
                "Ziwei Liu."
            ],
            "title": "Detecting and grounding multi-modal media manipulation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6904\u20136913.",
            "year": 2023
        },
        {
            "authors": [
                "Craig Silverman"
            ],
            "title": "Verification handbook",
            "year": 2013
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman."
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-",
            "year": 2015
        },
        {
            "authors": [
                "Shivangi Singhal",
                "Rajiv Ratn Shah",
                "Ponnurangam Kumaraguru."
            ],
            "title": "Factdrill: A data repository of fact-checked social media content to study fake news incidents in india",
            "venue": "Proceedings of the Sixteenth International AAAI Conference on Web and Social",
            "year": 2022
        },
        {
            "authors": [
                "Tiening Sun",
                "Zhong Qian",
                "Peifeng Li",
                "Qiaoming Zhu."
            ],
            "title": "Graph interactive network with adaptive gradient for multi-modal rumor detection",
            "venue": "Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, ICMR 2023, Thessaloniki,",
            "year": 2023
        },
        {
            "authors": [
                "S. Suryavardan",
                "Shreyash Mishra",
                "Megha Chakraborty",
                "Parth Patwa",
                "Anku Rani",
                "Aman Chadha",
                "Aishwarya Reganti",
                "Amitava Das",
                "Amit P. Sheth",
                "Manoj Chinnakotla",
                "Asif Ekbal",
                "Srijan Kumar"
            ],
            "title": "Findings of factify 2: Multimodal fake news detection",
            "year": 2023
        },
        {
            "authors": [
                "S Suryavardan",
                "Shreyash Mishra",
                "Parth Patwa",
                "Megha Chakraborty",
                "Anku Rani",
                "Aishwarya Reganti",
                "Aman Chadha",
                "Amitava Das",
                "Amit P. Sheth",
                "Manoj Chinnakotla",
                "Asif Ekbal",
                "Srijan Kumar"
            ],
            "title": "Factify 2: A multimodal fake news and satire news",
            "year": 2023
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich."
            ],
            "title": "Going deeper with convolutions",
            "venue": "2015 IEEE Conference on Computer Vision and Pattern",
            "year": 2015
        },
        {
            "authors": [
                "Reuben Tan",
                "Bryan A. Plummer",
                "Kate Saenko."
            ],
            "title": "Detecting cross-modal inconsistency to defend against neural fake news",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, Novem-",
            "year": 2020
        },
        {
            "authors": [
                "Vidhu Tanwar",
                "Kapil Sharma."
            ],
            "title": "Multi-model fake news detection based on concatenation of visual latent features",
            "venue": "2020 International Conference on Communication and Signal Processing (ICCSP), pages 1344\u20131348.",
            "year": 2020
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos."
            ],
            "title": "Automated fact checking: Task formulations, methods and future directions",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3346\u20133359, Santa Fe, New Mexico, USA. As-",
            "year": 2018
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Oana Cocarascu",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "The fact extraction and VERification (FEVER) shared task",
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 1\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Junke Wang",
                "Zuxuan Wu",
                "Wenhao Ouyang",
                "Xintong Han",
                "Jingjing Chen",
                "Yu-Gang Jiang",
                "Ser-Nam Li."
            ],
            "title": "M2TR: multi-modal multi-scale transformers for deepfake detection",
            "venue": "ICMR \u201922: International Conference on Multimedia Retrieval, Newark, NJ,",
            "year": 2022
        },
        {
            "authors": [
                "Run Wang",
                "Felix Juefei-Xu",
                "Yihao Huang",
                "Qing Guo",
                "Xiaofei Xie",
                "Lei Ma",
                "Yang Liu."
            ],
            "title": "Deepsonar: Towards effective and robust detection of ai-synthesized fake voices",
            "venue": "MM \u201920: The 28th ACM International Conference on Multimedia, Vir-",
            "year": 2020
        },
        {
            "authors": [
                "Yaqing Wang",
                "Fenglong Ma",
                "Zhiwei Jin",
                "Ye Yuan",
                "Guangxu Xun",
                "Kishlay Jha",
                "Lu Su",
                "Jing Gao."
            ],
            "title": "EANN: event adversarial neural networks for multi-modal fake news detection",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Deressa Wodajo",
                "Solomon Atnafu."
            ],
            "title": "Deepfake video detection using convolutional vision transformer",
            "venue": "CoRR, abs/2102.11126.",
            "year": 2021
        },
        {
            "authors": [
                "Haiwei Wu",
                "Jiantao Zhou",
                "Shile Zhang."
            ],
            "title": "Generalizable synthetic image detection via languageguided contrastive learning",
            "venue": "CoRR, abs/2305.13800.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoshuai Wu",
                "Xin Liao",
                "Bo Ou."
            ],
            "title": "Sepmark: Deep separable watermarking for unified source tracing and deepfake detection",
            "venue": "CoRR, abs/2305.06321.",
            "year": 2023
        },
        {
            "authors": [
                "Yang Wu",
                "Pengwei Zhan",
                "Yunjian Zhang",
                "Liming Wang",
                "Zhen Xu."
            ],
            "title": "Multimodal fusion with coattention networks for fake news detection",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2560\u20132569, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wu",
                "Wael AbdAlmageed",
                "Premkumar Natarajan."
            ],
            "title": "Mantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Zhizheng Wu",
                "Tomi Kinnunen",
                "Nicholas W.D. Evans",
                "Junichi Yamagishi",
                "Cemal Hanil\u00e7i",
                "Md. Sahidullah",
                "Aleksandr Sizov."
            ],
            "title": "Asvspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
            "venue": "INTERSPEECH 2015, 16th",
            "year": 2015
        },
        {
            "authors": [
                "Barry Menglong Yao",
                "Aditya Shah",
                "Lichao Sun",
                "Jin-Hee Cho",
                "Lifu Huang."
            ],
            "title": "End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models",
            "venue": "CoRR, abs/2205.12487.",
            "year": 2022
        },
        {
            "authors": [
                "Jiangyan Yi",
                "Ye Bai",
                "Jianhua Tao",
                "Haoxin Ma",
                "Zhengkun Tian",
                "Chenglong Wang",
                "Tao Wang",
                "Ruibo Fu."
            ],
            "title": "Half-truth: A partially fake audio detection dataset",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communi-",
            "year": 2021
        },
        {
            "authors": [
                "Egor Zakharov",
                "Aliaksandra Shysheya",
                "Egor Burkov",
                "Victor S. Lempitsky."
            ],
            "title": "Few-shot adversarial learning of realistic neural talking head models",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), Octo-",
            "year": 2019
        },
        {
            "authors": [
                "Markos Zampoglou",
                "Symeon Papadopoulos",
                "Yiannis Kompatsiaris."
            ],
            "title": "Detecting image splicing in the wild (WEB)",
            "venue": "2015 IEEE International Conference on Multimedia & Expo Workshops, ICME Workshops 2015, Turin, Italy, June 29 - July 3, 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Xia Zeng",
                "Amani S. Abumansour",
                "Arkaitz Zubiaga."
            ],
            "title": "Automated fact-checking: A survey",
            "venue": "Lang. Linguistics Compass, 15(10).",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Yue Zhang",
                "Lanyu Shang",
                "Biao Geng",
                "Shuyue Lai",
                "Ke Li",
                "Hongmin Zhu",
                "Md. Tanvir Al Amin",
                "Dong Wang."
            ],
            "title": "Fauxbuster: A content-free fauxtography detector using social media comments",
            "venue": "IEEE International Conference on Big Data (IEEE",
            "year": 2018
        },
        {
            "authors": [
                "Yizhou Zhang",
                "Loc Trinh",
                "Defu Cao",
                "Zijun Cui",
                "Yan Liu."
            ],
            "title": "Detecting out-of-context multimodal misinformation with interpretable neural-symbolic model",
            "venue": "CoRR, abs/2304.07633.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaqi Zheng",
                "Xi Zhang",
                "Sanchuan Guo",
                "Quan Wang",
                "Wenyu Zang",
                "Yongdong Zhang."
            ],
            "title": "MFAN: multi-modal feature-enhanced attention networks for rumor detection",
            "venue": "Proceedings of the Thirty-First",
            "year": 2022
        },
        {
            "authors": [
                "Yinglin Zheng",
                "Jianmin Bao",
                "Dong Chen",
                "Ming Zeng",
                "Fang Wen."
            ],
            "title": "Exploring temporal coherence for more general video face forgery detection",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada,",
            "year": 2021
        },
        {
            "authors": [
                "Peng Zhou",
                "Xintong Han",
                "Vlad I. Morariu",
                "Larry S. Davis."
            ],
            "title": "Learning rich features for image manipulation detection",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2018
        },
        {
            "authors": [
                "Dimitrina Zlatkova",
                "Preslav Nakov",
                "Ivan Koychev."
            ],
            "title": "Fact-checking meets fauxtography: Verifying claims about images",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Hong Kong",
                "China"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2099
        },
        {
            "authors": [
                "Ruohan Zong",
                "Yang Zhang",
                "Lanyu Shang",
                "Dong Wang."
            ],
            "title": "Contrastfaux: Sparse semi-supervised fauxtography detection on the web using multi-view contrastive learning",
            "venue": "Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Motivated by the challenges presented by misinformation in the modern media ecosystem, previous research has commonly modelled automated factchecking (AFC) as a pipeline consisting of different stages, surveyed in a variety of axes (Thorne and Vlachos, 2018; Kotonya and Toni, 2020a; Zeng et al., 2021; Nakov et al., 2021; Guo et al., 2022). However, these surveys focus on a single modality, text. This is different to real-world misinformation that often occurs via several modalities.\nIn AFC, the term multimodal has been used to refer to cases where the claim and/or evidence are expressed through different or multiple modalities (Hameleers et al., 2020; Alam et al., 2022; Biamby et al., 2022). Examples of multimodal misinformation include: (i) claims about digitally manipulated content (Agarwal et al., 2019; R\u00f6ssler et al., 2018) such as photos depicting former US president Trump\u2019s arrest (Figure 1); (ii) combining\n* This work was partially done during Mubashara\u2019s research visit at Cambridge.\n1https://www.bbc.com/news/ world-us-canada-65069316\ncontent from different modalities and contexts, e.g. using video footage in a misleading context (Aneja et al., 2021; Biamby et al., 2022; Abdelnabi et al., 2022); (iii) embedding a claim in another modality, e.g. a meme, an image with embedded text (Qu et al., 2022a), with notable real-world examples including a Brexit Vote Leave poster2 and TikTok videos with COVID misinformation (Shang et al., 2021); (iv) verifying a claim with evidence from a different modality than the input claim, e.g. verifying images against text (Shao et al., 2023), audio against textual metadata (Kopev et al., 2019), and text against images (Akhtar et al., 2023).\nFact-checking multimodal misinformation is important for a number of reasons. First, multimodal content is perceived as more credible compared to text containing a similar claim (Newman et al., 2012). For example, previous research shows that visual content exhibits a \u201cphoto truthiness\u201deffect (Newman and Zhang, 2020), biasing readers to believe a claim is true. Second, multimodal content spreads faster and has a higher engagement than text-only posts (Li and Xie, 2020). Third, with recent advances in generative machine learning models (Rombach et al., 2022), the generation of multimodal misinformation has been simplified.\nTo validate the importance of multimodal fact-\n2https://www.itv.com/news/2019-01-18/ boris-johnson-under-attack-over-turkey-claim/\nchecking, we manually annotated 9,255 claims from the AVeriTeC dataset (Schlichtkrull et al., 2023), which were collected with the Google FactCheck ClaimReview API3. For each claim, we identified the modalities present in it and evidence strategies (e.g. identification of geolocation) used for fact-checking. We find that more than 2, 600 (28.68%) claims either contain multimodal data or require multimodal reasoning for verification, with 20.07% involving images, 8.06% videos, and 0.55% audios (see Table 1).4 These claims can neither be fact-checked by a text-only model, nor by a model with no text capabilities.\nIn this survey, we introduce a three-stage framework for multimodal automated fact-checking: claim detection and extraction, evidence retrieval, and verdict prediction encompassing veracity, manipulation and out-of-context classification, as well as justification production. The input and output data of each stage can have different or multiple modalities. For each stage, we discuss related terms and definitions developed in different research communities. In contrast to previous surveys on multimodal fact-checking that focus on individual subtasks (Cao et al., 2020; Alam et al., 2022; Abdali, 2022), we consider all subtasks surveying benchmarks and modeling approaches for them.\nWe focus on four prevalent modalities of realworld fact-checking identified in our annotations: text, image, audio, and video. While tables and knowledge graphs are increasingly used as evidence for benchmarks (Chen et al., 2020; Aly et al., 2021; Akhtar et al., 2022), they have been covered in previous surveys (Thorne and Vlachos, 2018; Zeng et al., 2021; Guo et al., 2022). Finally, we discuss the extent to which current approaches to AFC work for multimodal data, and promising directions for further research (Section 4). We accompany the\n3https://toolbox.google.com/factcheck/apis 4Annotations at http://github.com/MichSchli/\nAVeriTeC.\nsurvey with a repository,5 which lists the resources mentioned in our survey."
        },
        {
            "heading": "2 Task Formulation",
            "text": "This section introduces a conceptualisation of multimodal AFC as a three-stage process, including claim detection and extraction, evidence retrieval, and production of verdicts and justifications for various types of misinformation (Figure 2). Compared to the text-only pipeline presented in Guo et al. (2022), our framework extends their first stage with a claim extraction stage, and generalises their third stage to cover tasks that fall under multimodal AFC, thus accounting for its particular challenges.\nTerminology. A number of works (Singhal et al., 2022; Fung et al., 2021) use the term multimedia, which is also more common in public discussions instead of multimodal (Lauer, 2009). However in in this survey we adopt the latter, following other surveys that use multimodal data (Liang et al., 2022; Guo et al., 2019). Adopting the terminology of previous surveys (Thorne and Vlachos, 2018; Alam et al., 2022) and following advice from institutions such as the UNO (Ireton and Posetti, 2018), we avoid multimodal fake news (Meel and Vishwakarma, 2021; Amri et al., 2021; Patwa et al., 2022) due to the term\u2019s ambiguous use.\nStage 1: Claim Detection and Extraction. The first pipeline stage aims to find checkable (i.e. factually-verifiable) and check-worthy (i.e. important factual claims (Hassan et al., 2015b)) claims. Debunking a typical claim and writing the factchecking article takes approximately one day for a human fact-checker (Hassan et al., 2015a). This stage aims to focus the AFC process on claims which are verifiable and most impactful. Multimodal claims can be diverse and include: (1) a written claim embedded in another modality (Prabhakar et al., 2021; Maros et al., 2021) such as an image or a spoken claim in an audio or video; (2) a claim that a piece of content is authentic, e.g. that a video footage is from a specific geographic location (Zhang et al., 2018; Heller et al., 2018); (3) a claim for which the evidence is manipulated to support it, e.g. through lip-syncing (R\u00f6ssler et al., 2018). While in some cases the claim is clearly specified (e.g. in form of a headline), in often multiple modalities are required to understand and ex-\n5https://github.com/Cartus/ Automated-Fact-Checking-Resources\ntract a claim at this stage. Simply detecting potentially misleading content is often not enough \u2013 it is necessary to extract the claim before fact-checking it in the subsequent stages. For example, detecting text in images or videos and understanding it given the context (Qu et al., 2022b) or verifying audios by transcribing and extracting claims (Maros et al., 2021).\nStage 2: Evidence Retrieval. Similarly to factchecking with text, multimodal fact-checking often relies on evidence to make judgments, similar to the process followed by human fact-checkers (Silverman, 2013; Nakov et al., 2021). Two main approaches have been used in the past: (i) using the claim to-be-checked as evidence itself, e.g. to detect manipulation (Qi et al., 2019; Bonettini et al., 2020); this can be seen as the multimodal version of evidence-free fact-checking of text claims by checking logical fallacies in the text (Jin et al., 2022), and (ii) retrieving additional evidence (Abdelnabi et al., 2022). In multimodal fact-checking, the evidence modality can be different from the claim modality. For example, to retrieve evidence for image or audio fact-checking, previous works have also used text e.g. metadata, social media comments, or captions (Gupta et al., 2013; Huh et al., 2018; M\u00fcller-Budack et al., 2020; Kopev et al., 2019).\nStage 3: Verdict Prediction and Justification Production. Following the fact-checking process of professional fact-checkers, the final stage comprises verdict prediction and the production of justification that explains the fact-check to humans (Graves, 2018). Verdict prediction is decomposed into three tasks considering prevalent multimodal misinformation types: manipulation, using content out-of-context, and veracity classification.\nStage 3.1: Manipulation Classification. Manipulation classification commonly addresses (i) misinformative claims with manipulated content; (ii) correct claims accompanied by manipulated content (e.g. to increase credibility). Many meth-\nods exist to manipulate text, visual and audio content. While some require more knowledge to use (e.g. speech synthesis), other manipulations can be achieved with simple tools (e.g. changing speed of videos) (Paris and Donovan, 2019). Different terms have been used for manipulated content in recent years. A deepfake is commonly defined as \u201cthe product of artificial intelligence (AI) applications that [...] create fake videos that appear authentic\u201d (Maras and Alexandrou, 2019), with popular examples including realistic-looking videos where the speaker\u2019s voice or face is modified (Paris and Donovan, 2019). On the other hand, cheap fake defines manipulated content created through more accessible methods (Paris and Donovan, 2019), e.g. changing captions or speed of videos (La et al., 2022). The term fauxtography was first coined in journalism for images manipulated to \u201cconvey a questionable (or outright false) sense of the events they seem to depict\u201d (Cooper, 2007; Kalb and Saivetz, 2007). Other terms used in the literature for manipulated content are fake (Cheema et al., 2022), forgery (Cozzolino et al., 2021), and splice (Zampoglou et al., 2015).\nStage 3.2: Out-of-context Classification. Using unchanged content out-of-context is one of the most common and easiest methods to create multimodal misinformation (Luo et al., 2021; Aneja et al., 2021), and involves (possibly misinformative) textual claims paired with content (e.g. a video) taken out of context (Zhang et al., 2018; Abdelnabi et al., 2022; Garimella and Eckles, 2020). Recent work has also studied the applicability of traditional multimodal misinformation detection methods to identify out-of-context content (Zhang et al., 2023). Other terms used for combining multimodal content in a misleading way include cross-modal (in-) consistency (M\u00fcllerBudack et al., 2020) and repurposing (Luo et al., 2021).\nStage 3.3: Veracity Classification. This task is the multimodal counterpart to classifying the veracity of textual claims given retrieved evidence\n(Thorne and Vlachos, 2018). Veracity classification of claims embedded in audio is also commonly referred to as deception detection (Kopev et al., 2019; Kamboj et al., 2021). While earlier work considered mostly claims recorded in staged setups (Newman et al., 2003) or from court trials (P\u00e9rez-Rosas et al., 2015), more recently real-world political debates have become popular. (Kopev et al., 2019; Kamboj et al., 2021).\nStage 3.4: Justification Production. Different to previous research on automated justification production (Kotonya and Toni, 2020a), human factcheckers also give justifications for fact-checks involving images, audios, or videos (Silverman, 2013). Justifications for multimodal misinformation can be grouped in three categories: (i) identifying which part of the claim input is misleading (e.g. specific areas in a visual claim or words in a textual one) (Kou et al., 2020; Purwanto et al., 2021; Louren\u00e7o and Paes, 2022); (ii) providing natural language justifications following human fact-checkers (Yao et al., 2022); (iii) selecting and highlighting evidence parts used for verification (Atanasova et al., 2020; Shang et al., 2022). Justifications serve purposes beyond explaining veracity classification, e.g. human fact-checkers also use them to discuss uncertainties and potential errors \u2013 especially needed in fact-checking for rapidly developing events (Silverman, 2013)."
        },
        {
            "heading": "3 Datasets and Modeling Approaches",
            "text": ""
        },
        {
            "heading": "3.1 Stage 1: Claim Detection and Extraction",
            "text": "Input. Typical inputs to claim detection are unimodal, including image (Garimella and Eckles, 2020; Qu et al., 2022a), audio (Maros et al., 2021), and video (Shang et al., 2021; Qi et al., 2022), which are collected from social media platforms such as WhatsApp and TikTok (see Table 2). The written or spoken claim is extracted from the input at this stage before fact-checking it. Output. Claim detection is typically framed as a classification task. Models predict if a claim is checkable or check-worthy (Prabhakar et al., 2021; Cheema et al., 2022; Barr\u00f3n-Cede\u00f1o et al., 2023). The verdict for factual-verifiability is often binary (Jin et al., 2017; Shang et al., 2021). For check-worthiness, Prabhakar et al. (2021) defines three categories of multimodal claims: statistical/numerical claims, claims about world events/places/noteworthy individuals, and other fac-\ntual claims. Cheema et al. (2022) extend the binary labels for textual check-worthiness (Hassan et al., 2015b) with images to be considered as well. A tweet is considered check-worthy if it is potentially harmful, breaking news, or up-to-date. Modeling Approaches. Detecting claims is a challenging task due to the vast number of posts that are published every day. Existing claim detection methods primarily rely on input content since the large volume of potentially check-worthy inputs makes it difficult to retrieve and use evidence. The early multimodal method directly concatenated visual and textual features for detection (Jin et al., 2017; Wang et al., 2018). However, simple modality fusion may not be sufficient to capture the complex relationships among multimodal information. As a result, later efforts focused on jointly learning representations across modalities. For instance, Khattar et al. (2019) leverage a variational autoencoder (Kingma and Welling, 2014) to learn a shared representation of visual and textual content. Various attention mechanisms have also been developed to fuse multimodal representations (Qian et al., 2021; Wu et al., 2021; Liu et al., 2023b; Qi et al., 2023). Another popular approach is to use graph neural networks (Kipf and Welling, 2017) to model the interactions among different modalities (Zheng et al., 2022; Sun et al., 2023).\nMultimodal content can implicitly provide claims, as seen in images and videos on social media that often have accompanying text. To extract claims from visual input, OCR systems are commonly used (Garimella and Eckles, 2020; Prabhakar et al., 2021). Qu et al. (2022b) use Google Vision API to identify text in memes. Claim extraction becomes more challenging when dealing with video inputs. Shang et al. (2021) address this challenge by extracting captions and audio chunks after sampling video frames. These captions and audio chunks were then encoded into representations to guide the visual feature extraction process. For audio inputs, Maros et al. (2021) use Google\u2019s Speech-to-Text API to produce transcripts."
        },
        {
            "heading": "3.2 Stage 2: Evidence Retrieval",
            "text": "Previous work uses different types of evidence and retrieval methods given the modalities involved. Evidence data and retrieval approaches can be grouped into (i) content-based and (ii) retrievalbased (see column evidence in Table 3). Content-based. Content-based approaches use the\nclaim and its context (i.e. the same information that is used for claim detection and extraction) as evidence instead of retrieving additional data. This is particularly common for audio and video misinformation (Table 3). Acoustic or visual features extracted from the input are used as evidence for verdict prediction (Wu et al., 2015; Yi et al., 2021; Ismael Al-Sanjary et al., 2016; Jiang et al., 2020). Most approaches use audio (or video) features and accompanying data (e.g. metadata, transcripts if available) as evidence to identify inconsistencies (Kopev et al., 2019; R\u00f6ssler et al., 2018; Li et al., 2020b). Several datasets with image/text claims (Tan et al., 2020; Luo et al., 2021; Aneja et al., 2021) also do not retrieve additional evidence (Table 3) but rely on the given claim input or use accompanying metadata (Jaiswal et al., 2017; Sabir et al., 2018). Metadata is also often used as evidence for verdict prediction with images as input (Table 3). Jaiswal et al. (2017) and Sabir et al. (2018) use metadata (e.g. image timestamps) to provide additional information. Similarly, Huh et al. (2018) incorporate EXIF metadata (e.g. camera version, focal length, resolution settings) to detect manipulation. Image captions are also used as evidence sometimes (Shao et al., 2023).\nRetrieval-based. Retrieved evidence external to the claim is mostly used for fact-checking text claims, text/image and image claims while audio and video fact-checks often don\u2019t retrieve additional evidence data (Table 3) but rely on the content of the video/audio input. Fung et al. (2021) leverage a knowledge base for additional background knowledge. They first construct a knowledge graph of the input news article using its text\nand images. They extract entities/relations from this knowledge graph with an Information Extraction system (Li et al., 2020a; Lin et al., 2020) and map the entities to Freebase (Bollacker et al., 2008) as their background knowledge base. Two recent datasets scrape claims from fact-checking websites, and include text/image/video from those articles as evidence (Singhal et al., 2022; Yao et al., 2022). Akhtar et al. (2023) used chart images as evidence to verify textual claims. To determine if an image is used out-of-context, previous works also use (reverse) image search (M\u00fcller-Budack et al., 2020; Abdelnabi et al., 2022), to find evidence sources with images similar to or same as the claim image. M\u00fcller-Budack et al. (2020) query search engines and the WikiData knowledge graph using named entities from the claim text. Abdelnabi et al. (2022) use the claim image caption and the image itself as query."
        },
        {
            "heading": "3.3 Stage 3: Verdict Prediction",
            "text": "As introduced in Section 2, the verdict prediction stage includes manipulation, out-of-context, and veracity classification as sub-tasks. Input. As shown in Table 3, inputs of manipulation classification datasets usually focus on one modality. For dataset creation, manipulated images are often collected from social media platforms such as Twitter, Reddit, and YouTube (Gupta et al., 2013; Heller et al., 2018). For verdict prediction datasets with videos, in addition to social media (Ismael Al-Sanjary et al., 2016), film clips (Guera and Delp, 2018), facial expressions (R\u00f6ssler et al., 2018), and interviews (Li et al., 2020b) are used. Some works record videos to simulate real-world\nscenarios (Dolhansky et al., 2019; Jiang et al., 2020; Kwon et al., 2021). To create datasets of manipulated content, altering methods based on GANs have also been applied in earlier works (Zakharov et al., 2019; Nirkin et al., 2019; Karras et al., 2019). For audio manipulations, most benchmarks (Wu et al., 2015; Kinnunen et al., 2017; Reimao and Tzerpos, 2019; Wang et al., 2020; Yi et al., 2021) use speech synthesis and voice conversion algorithms to collect manipulated audios. To assess real-world audio manipulations, Lavrentyeva et al. (2019) emulate realistic telephone channels.\nMost out-of-context classification datasets have image-caption pairs as input (Table 3). Jaiswal et al. (2017) replace captions of Flickr images to get mismatched pairs. As replacing the entire caption can be easy to detect, later efforts (Sabir et al., 2018; M\u00fcller-Budack et al., 2020) propose to change specific entities in them. Luo et al. (2021) show that such text manipulations introduce linguistic biases and can be solved without the images. They use CLIP (Radford et al., 2021) to filter out pairs that do not require multimodal modeling. Popular sources for out of context datasets with text and image\nclaims include Flickr and news/fact-checking websites (Aneja et al., 2021; Jaiswal et al., 2017; Sabir et al., 2018).\nThe primary input to multimodal veracity classification is the content-to-be-checked itself \u2013 typically text, audio or video in past benchmarks. Kopev et al. (2019) include verified speeches from the CLEF-2018 Task 2 (Nakov et al., 2018) while Hou et al. (2019) collect videos about prostate cancer verified by urologists. Zlatkova et al. (2019) and Yao et al. (2022) collect viral images with texts verified by dedicated agencies. Nakamura et al. (2020) collect image-text pairs from Reddit via distant supervision, e.g. labeling a post from the subreddit \u201cfakefacts\u201d as misleading and from \u201cphotoshopbattles\u201d as manipulated. For veracity classification of spoken claims, real-world political debates are popular sources for claims (Kopev et al., 2019; Kamboj et al., 2021). For example, Kopev et al. (2019) and Kamboj et al. (2021) use claims labelled by fact checking organizations, and video recordings as well as transcripts of the respective political debates.\nOutput. Most manipulation and out-of-context\nclassification datasets use binary labels: \u201cout-ofcontext/not out-of-context\u201d (M\u00fcller-Budack et al., 2020; Luo et al., 2021), \u201cpristine/falsified\u201d (Boididou et al., 2014; Heller et al., 2018), \u201cmanipulation/no manipulation\u201d (Dolhansky et al., 2019; Li et al., 2020b). Following fact-checkers, veracity classification datasets (Singhal et al., 2022; Nakamura et al., 2020) sometimes employ multi-class labels to represent degrees of truthfulness (e.g. true, mostly-true, half-true) (see Table 3). Mishra et al. (2022) adopt labels to denote the entailment between different claim and evidence modalities, e.g. the label support text denotes that only the textual part of the evidence supports the claim but not the accompanying image while support multimodal includes both modalities.\nModeling Approaches. To detect visual manipulations, early approaches mostly use CNN models, such as VGG16 (Amerini et al., 2019; Dang et al., 2020), ResNet (Amerini et al., 2019; Sabir et al., 2019), and InceptionV3 (Guera and Delp, 2018). Some works extend them to capture temporal aspects of video manipulation classification. Amerini et al. (2019) adopt optical flow fields to capture the correlation between consequent video frames and detect dissimilarities caused by manipulation. Guera and Delp (2018) model temporal information with an LSTM model and a sequence of features vectors per video frame to classify manipulated videos. Sabir et al. (2019) similarly extract features for video frames and detect discrepancies between frames using a recurrent convolution network. Some recent models also integrate transformer-based components (Vaswani et al., 2017; Zheng et al., 2021). For example, Wang et al. (2022) combine CNNs and vision transformers (ViTs) (Dosovitskiy et al., 2021) while Wodajo and Atnafu (2021) introduce a multi-scale ViT with variable patch sizes.\nModels for out-of-context and veracity classification typically consist of unimodal encoders, a fusion component to obtain joint, multimodal representations, and a classification component. To obtain text representations, early approaches used combinations of word2vec models (Mikolov et al., 2013), LSTMs (Hochreiter and Schmidhuber, 1997), and TF-IDF scores for n-grams (Jin et al., 2017; Tanwar and Sharma, 2020; Hou et al., 2019). More recent efforts use pretrained language models (Fung et al., 2021; Aneja et al., 2021; La et al., 2022). To encode visual data, many approaches\nfirst detect objects in visual content using a Mask R-CNN model (He et al., 2017) before extracting visual features (Aneja et al., 2021; La et al., 2022; Shang et al., 2022). Visual representations for images and videos are commonly obtained using CNN models such as ResNet (He et al., 2016; Garimella and Eckles, 2020; Abdelnabi et al., 2022), VGG (Simonyan and Zisserman, 2015; Jin et al., 2017; Sabir et al., 2018), and Inception (Szegedy et al., 2015; Guera and Delp, 2018; Roy and Ekbal, 2021). To obtain audio features for voice quality, loudness, and tonality, Shang et al. (2021) extract the Melfrequency cepstral coefficient, Kopev et al. (2019) use the INTERSPEECH 2013 ComParE feature set (Eyben et al., 2013), and Hou et al. (2019) use the openEAR toolkit (Eyben et al., 2009). Various approaches have been used to obtain multimodal representations. Early fusion, which joins representations immediately after the encoding step (Baltrusaitis et al., 2019) is more common (Aneja et al., 2021; Tanwar and Sharma, 2020; La et al., 2022) than late fusion (Yao et al., 2022). Moreover, model-agnostic methods (e.g. concatenation and dot product) are more prevalent (Aneja et al., 2021; Kopev et al., 2019; Jin et al., 2017; La et al., 2022) than model-based approaches (e.g. neural networks) (Jaiswal et al., 2017; Shang et al., 2022). Also popular for out-of-context classification are cross-modality checks that compare modalities present in a claim to each other, e.g. a video and its caption (M\u00fcller-Budack et al., 2020; Fung et al., 2021)."
        },
        {
            "heading": "3.4 Stage 3: Justification Production",
            "text": "A small number of datasets is available for multimodal justification production. Previous work can be grouped into two categories: (1) highlighting parts of the input, and (2) generating natural language justifications. Highlighting Input. The first category highlights input parts as justification which contribute to models\u2019 results. A popular approach for this are Graph Neural Networks (Kipf and Welling, 2017). Several papers encode multimodal data as graph elements, combining entities and their relations in and between modalities. Models are trained to detect inconsistencies between different modalities, or to detect relations (i.e., between entities) that may be misinformative. This detection could be based on the local graph structure, or on an external knowledge base (Fung et al., 2021; Shang et al., 2022;\nKou et al., 2020). Highlighted entities and relations serve as explanations for the potential misinformativeness of the entire graph. Conversely, Zhou et al. (2018) and Wu et al. (2019) use a multitask model for manipulation classification and identification of manipulated regions. Rather than labeled data, some papers rely on attention mechanisms to highlight areas as explanations. Bonettini et al. (2020); Dang et al. (2020) use this approach to highlight manipulated image regions; Purwanto et al. (2021) also include captions. Natural Language Justifications. Yao et al. (2022) recently introduced a multimodal dataset with natural language justifications. They scrape text and visual content from web pages referenced by fact-checking articles. The dataset includes summaries in the fact-checking articles as gold justifications for the verdicts. However, such a setting is not realistic, as fact-checking articles are not available when verifying a new claim."
        },
        {
            "heading": "4 Challenges and Future Directions",
            "text": "Claim extraction from multimodal content. Multimodal claims, e.g. manipulated videos, are often embedded in specific contexts and framed as (part of) larger stories. For example, countering the misinformation in Figure 1 requires not only classifying if the image is manipulated, but understanding that it depicts the arrest of the former president in one of the cases he is being charged in. Only then can relevant evidence data be extracted and used to verify the story of Trump\u2019s arrest.To determine what is being claimed is a challenging first step in multimodal automated fact-checking. However, current efforts for multimodal claim extraction are limited to text extraction from visual content or transcribing audios and videos (Qu et al., 2022b; Garimella and Eckles, 2020; Maros et al., 2021). Addressing this challenge will require modeling approaches to effectively align and integrate all modalities present in and around the claim. For example, methods for pixel-based language modeling have recently been introduced to better align visually situated language with image content (Lee et al., 2022). Such approaches considering modalities beyond text and vision for multimodal data alignment can be useful for claim extracting from multimodal input.\nMultimodal evidence retrieval. Evidence retrieval for audio and video fact-checking remains a major challenge. Different to other modalities,\nthey cannot be easily searched on the web or social media networks (Silverman, 2013). Fact-checkers often use text accompanying the videos to find evidence (Silverman, 2013). Reverse image search engines, e.g. Google Lens or TinEye, require screenshots from the video as input \u2013 and thus require the correct timeframe, which can be challenging to extract. A dedicated adversary can render current tools very difficult to use. Very often evidence for image or audio fact-checking is retrieved using text accompanying them , e.g. metadata, social media comments, or captions (Gupta et al., 2013; Huh et al., 2018; M\u00fcller-Budack et al., 2020; Kopev et al., 2019). While incorporating the textual information and the other modality (e.g. audio/image) in retrieval would provide more information, this is missing currently. How to best retrieve evidence data that is non-textual or has a different modality than the claim, also remains a challenge.\nMultilinguality and multimodality. While there is increasing work on multilingual factchecking (Gupta and Srikumar, 2021; Shahi and Nandini, 2020; Hammouchi and Ghogho, 2022), it is mostly limited to text-only benchmarks and models. Surveying benchmarks for different pipeline stages (Figure 2), we found limited multimodal datasets for non-English languages (see Table 3). Previous work on multilingual multimodality shows that training and testing on English data alone introduces biases, as models fail to capture concepts and images prevalent in other languages and cultures (Liu et al., 2021). Moreover, some types of multimodal misinformation exploit cross-lingual sources to mislead, e.g. images or videos from non-English newspapers appearing as out-of-context data for English multimodal misinformation (Silverman, 2013). To prevent false conclusions and biases, it is thus necessary to take approaches that are both multimodal and multilingual (Ruder et al., 2022). Construction of large-scale multimodal, multilingual AFC datasets would facilitate futures research in this direction, similar to benchmarks and shared tasks created for automated fact-checking tasks in English (Thorne et al., 2018; Suryavardan et al., 2023a).\nGeneralizing detection of visual manipulations. The recent popularity of diffusion models (DMs) for visual manipulation have raised questions regarding the generalizability of manipulation detectors developed for earlier models (e.g.\nGANs (Goodfellow et al., 2020)). Detection models are biased towards specific manipulation models and struggle to generalize (Wu et al., 2023a; Ricker et al., 2022). A recent study (Ricker et al., 2022) shows that detectors initially developed for GANs, have average performance drops of around 15% for image by DMs. While new detection approaches for DM manipulations are already being developed (Guarnera et al., 2023; Wu et al., 2023b), the question how to generalize and increase robustness of manipulation detectors for potential future manipulation models remains open. Potential solutions can include evidence-based approaches, where the manipulated content is used to retrieve evidence data (e.g. the original video or counterfactual evidence) to prove the manipulation.\nJustifications for multimodal fact-checking. While explainable fact-checking has received attention recently (Kotonya and Toni, 2020b; Atanasova et al., 2020), there is limited work on producing justifications for multimodal content. Previous efforts on multimodal justification production have mostly focused on highlighting parts of the input to increase interpretability (Kou et al., 2020; Shang et al., 2022). Natural language justifications that explain the fact-check of multimodal claims so that it is accessible to non-technical have not been developed yet. To develop solutions, we first need appropriate benchmarks to measure progress. Moreover, with the recent advances of neural models for visual and audio generation and editing, another so far unexplored direction presents itself: editing input images/videos/audios or generating entirely content to explain fact-checking results. This could include, for example, the generation of infographics or video clips to explanation factchecks. Such a system, especially if guided by human fact-checkers (Nakov et al., 2021), would be a potent tool. As noted in Lewandowsky et al. (2020), \u201cwell-designed graphs, videos, photos, and other semantic aids can be helpful to convey corrections involving complex or statistical information clearly and concisely\u201d."
        },
        {
            "heading": "5 Conclusion",
            "text": "We survey research on multimodal automated factchecking and introduce a framework that combines and organizes tasks introduced in various communities studying misinformation. We discuss common terms and definitions in context of our framework. We further study popular benchmarks and model-\ning approaches, and discuss promising directions for future research.\nLimitations\nWhile we cite many datasets and modeling approaches for multimodal fact-checking, we describe most of them only briefly due to space constraints. Our aim was to provide an overview of multimodal fact-checking and organise previous works in a framework. Moreover, the presented survey focuses primarily on four modalities. While there are other modalities we could have included, we concentrated on those prevalent in real-world fact-checking that have not been discussed as part of a fact-checking framework in previous surveys.\nEthics Statement\nAs we mention in Section 4, most datasets for multimodal fact-checking tasks are available only in English. Thus, models are evaluated based on their performance on English benchmarks only. This can lead to a distorted view about advancements on multimodal automated fact-checking as it is limited to a single language out of more than 7000 world languages. While we call for future work on a variety of languages, this survey provides an overview on the state-of-the-art of mostly-English research efforts. Finally, we want to point out that multimodal fact-checking works we cite in this survey might include misleading statements or images given as examples."
        },
        {
            "heading": "Acknowledgements",
            "text": "Zhijiang Guo, Michael Schlichtkrull and Andreas Vlachos are supported by the ERC grant AVeriTeC (GA 865958). This paper is produced as part of the MuseIT project which has been co-funded by the EU under the Grant Agreement number 101061441. MuseIT has supported the work of Mubashara Akhtar. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Executive Agency, REA. Neither the EU nor the granting authority can be held responsible for them."
        },
        {
            "heading": "A Methodology",
            "text": "We applied the following methodological approach to find and select relevant research papers for the survey.\nFirst, after defining the research scope, we collected pivotal, highly-cited work (e.g. Nakamura et al. (2020)) and related surveys (e.g. (Alam et al., 2022)), resulting in 25 papers, as well as papers citing or cited by these works. We collected further works using the scholarly search engines Google Scholar6, Semantic Scholar7, DBLP8 and ACL Anthology9, and keyword-based search with Cartesian products of following keyword sets: {\u201cfact checking\u201d, \u201cfact verification\u201d, \u201cmisinformation\u201d, \u201cdisinformation\u201d, \u201cfake news\u201d}, {\u201cmultimodal\u201d, \u201ctext\u201d, \u201cimage\u201d, \u201caudio\u201d, \u201cvideo\u201d}, and {\u201cmachine learning\u201d, \u201cautomated\u201d}. The databases were queried primarily during the time frame July 26, 2022 and August 10, 2022. This step resulted in a collection of 123 papers.\n6https://scholar.google.com/ 7https://www.semanticscholar.org/ 8https://dblp.org/ 9https://aclanthology.org/\nWe manually screened and filtered the papers based on abstracts and introduction sections, before creating an overview of papers across the following dimensions: (1) modality; (2) fact-checking task; (3) contribution type (i.e. dataset, modeling approach, demo); (4) paper type (i.e. survey, position paper, solution paper (e.g. introducing a new benchmark or modeling approach), or evaluation paper (e.g. investigating previously proposed approaches)). Papers were mostly excluded because they focused on other tasks than fact-checking (e.g. hate speech detection) or used modalities out of our scope (e.g. tables). Moreover, during the screening process we found and added further related works, and concluded the screening with 84 unique papers.\nThe taxonomy of tasks (Section 2) was created in an iterative manner starting with the task labels we assigned to works during screening. As a starting point we also used taxonomies of text-only factchecking surveys (Guo et al., 2022; Thorne and Vlachos, 2018) and adapted them for multimodal fact-checking works.\nB Examples: multimodal misinformation"
        }
    ],
    "title": "Multimodal Automated Fact-Checking: A Survey",
    "year": 2023
}