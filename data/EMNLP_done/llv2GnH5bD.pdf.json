{
    "abstractText": "We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pretrained ones in few-shot learning setting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sapan Shah"
        },
        {
            "affiliations": [],
            "name": "Sreedhar Reddy"
        },
        {
            "affiliations": [],
            "name": "Pushpak Bhattacharyya"
        }
    ],
    "id": "SP:dad2e4bd8c79ea2f00d08d31faa3a35e1daeff41",
    "references": [
        {
            "authors": [
                "Olanrewaju Tahir Aduragba",
                "Jialin Yu",
                "Alexandra Ioana Cristea",
                "Lei Shi."
            ],
            "title": "Detecting fine-grained emotions on social media during major disease outbreaks: Health and well-being before and during the covid-19 pandemic",
            "venue": "AMIA ...",
            "year": 2021
        },
        {
            "authors": [
                "Nastaran Babanejad",
                "Heidar Davoudi",
                "Aijun An",
                "Manos Papagelis"
            ],
            "title": "Affective and contextual",
            "year": 2020
        },
        {
            "authors": [
                "Laura-Ana-Maria Bostan",
                "Roman Klinger."
            ],
            "title": "An analysis of annotated corpora for emotion classification in text",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2104\u20132119, Santa Fe, New Mexico, USA. As-",
            "year": 2018
        },
        {
            "authors": [
                "Sven Buechel",
                "Udo Hahn"
            ],
            "title": "Emotion embeddings- learning stable and homogeneous abstractions from heterogeneous affective datasets",
            "year": 2023
        },
        {
            "authors": [
                "Deng Cai",
                "Xin Li",
                "Jackie Chun-Sing Ho",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Retrofitting multilingual sentence embeddings with Abstract Meaning Representation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Dana Movshovitz-Attias",
                "Jeongwoo Ko",
                "Alan Cowen",
                "Gaurav Nemade",
                "Sujith Ravi."
            ],
            "title": "GoEmotions: A dataset of fine-grained emotions",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Deng",
                "Vasilisa Bashlovkina",
                "Feng Han",
                "Simon Baumgartner",
                "Michael Bendersky."
            ],
            "title": "Llms to the moon? reddit market sentiment analysis with large language models",
            "venue": "Companion Proceedings of the ACM Web Conference 2023, WWW \u201923 Com-",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Hongchao Fang",
                "Sicheng Wang",
                "Meng Zhou",
                "Jiayuan Ding",
                "Pengtao Xie."
            ],
            "title": "Cert: Contrastive self-supervised learning for language understanding",
            "venue": "ArXiv, abs/2005.12766.",
            "year": 2020
        },
        {
            "authors": [
                "Hao Fei",
                "Yafeng Ren",
                "Donghong Ji."
            ],
            "title": "Retrofitting structure-aware transformer language model for end tasks",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2151\u20132161, On-",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Beliz Gunel",
                "Jingfei Du",
                "Alexis Conneau",
                "Veselin Stoyanov."
            ],
            "title": "Supervised contrastive learning for pre-trained language model fine-tuning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What does bert learn about the structure of language? pages 3651\u20133657, Florence, Italy",
            "venue": "Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yongjie Lin",
                "Yi Chern Tan",
                "Robert Frank."
            ],
            "title": "Open Sesame: Getting inside BERT\u2019s Linguistic Knowledge",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241\u2013253, Stroudsburg,",
            "year": 2019
        },
        {
            "authors": [
                "Weiyang Liu",
                "Yandong Wen",
                "Zhiding Yu",
                "Meng Yang."
            ],
            "title": "Large-margin softmax loss for convolutional neural networks",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916,",
            "year": 2016
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "L. McInnes",
                "J. Healy",
                "J. Melville."
            ],
            "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
            "venue": "ArXiv e-prints.",
            "year": 2018
        },
        {
            "authors": [
                "Gon\u00e7alo Azevedo Mendes",
                "Bruno Martins."
            ],
            "title": "Quantifying valence and arousal in text with multilingual pre-trained transformers",
            "venue": "Advances in Information Retrieval, pages 84\u2013100, Cham. Springer Nature Switzerland.",
            "year": 2023
        },
        {
            "authors": [
                "Saif M. Mohammad."
            ],
            "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
            "venue": "Proceedings of The Annual Conference of the Association for Computational Linguistics (ACL), Melbourne, Australia.",
            "year": 2018
        },
        {
            "authors": [
                "Saif M. Mohammad",
                "Peter D. Turney."
            ],
            "title": "Crowdsourcing a word-emotion association lexicon",
            "venue": "Computational Intelligence, 29(3):436\u2013465.",
            "year": 2013
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Ivan Vuli\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Ira Leviant",
                "Roi Reichart",
                "Milica Ga\u0161i\u0107",
                "Anna Korhonen",
                "Steve Young."
            ],
            "title": "Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints",
            "venue": "Transactions",
            "year": 2017
        },
        {
            "authors": [
                "Nikola Mrk\u0161i\u0107",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Blaise Thomson",
                "Milica Ga\u0161i\u0107",
                "Lina M. Rojas-Barahona",
                "PeiHao Su",
                "David Vandyke",
                "Tsung-Hsien Wen",
                "Steve Young."
            ],
            "title": "Counter-fitting word vectors to linguistic constraints",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Kevin Musgrave",
                "Serge Belongie",
                "Ser-Nam Lim."
            ],
            "title": "A metric learning reality check",
            "venue": "Computer Vision \u2013 ECCV 2020, pages 681\u2013699, Cham. Springer International Publishing.",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Musgrave",
                "Serge Belongie",
                "Ser-Nam Lim."
            ],
            "title": "Pytorch metric learning",
            "venue": "ArXiv, abs/2008.09164.",
            "year": 2020
        },
        {
            "authors": [
                "Kosmas Pinitas",
                "Konstantinos Makantasis",
                "Antonios Liapis",
                "Georgios N. Yannakakis."
            ],
            "title": "Supervised contrastive learning for affect modelling",
            "venue": "Proceedings of the 2022 International Conference on Multimodal Interaction, ICMI \u201922, page",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Anupama Ray",
                "Apoorva Nunna",
                "Pushpak Bhattacharyya."
            ],
            "title": "A multimodal corpus for emotion recognition in sarcasm",
            "venue": "Proceedings of the 13th Edition of the Language Resources and Evaluation Conference (LREC-2022), Marseille, France.",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Sara Rosenthal",
                "Noura Farra",
                "Preslav Nakov."
            ],
            "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502\u2013518, Vancouver, Canada. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Elvis Saravia",
                "Hsien-Chi Toby Liu",
                "Yen-Hao Huang",
                "Junlin Wu",
                "Yi-Shin Chen."
            ],
            "title": "CARER: Contextualized affect representations for emotion recognition",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Hooman Sedghamiz",
                "Shivam Raval",
                "Enrico Santus",
                "Tuka Alhanai",
                "Mohammad Ghassemi."
            ],
            "title": "SupCL-Seq: Supervised Contrastive Learning for downstream optimized sequence representations",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Sapan Shah",
                "Sreedhar Reddy",
                "Pushpak Bhattacharyya."
            ],
            "title": "A retrofitting model for incorporating semantic relations into word embeddings",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1292\u20131298,",
            "year": 2020
        },
        {
            "authors": [
                "Sapan Shah",
                "Sreedhar Reddy",
                "Pushpak Bhattacharyya."
            ],
            "title": "Affective retrofitted word embeddings",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint",
            "year": 2022
        },
        {
            "authors": [
                "Sapan Shah",
                "Sreedhar Reddy",
                "Pushpak Bhattacharyya."
            ],
            "title": "Emotion enriched retrofitted word embeddings",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 4136\u20134148, Gyeongju, Republic of Ko-",
            "year": 2022
        },
        {
            "authors": [
                "Dinghan Shen",
                "Mingzhi Zheng",
                "Yelong Shen",
                "Yanru Qu",
                "Weizhu Chen."
            ],
            "title": "A simple but tough-to-beat data augmentation approach for natural language understanding and generation",
            "venue": "CoRR, abs/2009.13818.",
            "year": 2020
        },
        {
            "authors": [
                "Weijia Shi",
                "Muhao Chen",
                "Pei Zhou",
                "Kai-Wei Chang."
            ],
            "title": "Retrofitting contextualized word embeddings with paraphrases",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on",
            "year": 2013
        },
        {
            "authors": [
                "Xiaohui Song",
                "Longtao Huang",
                "Hui Xue",
                "Songlin Hu."
            ],
            "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5197\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Varsha Suresh",
                "Desmond C. Ong."
            ],
            "title": "Using knowledge-embedded attention to augment pretrained language models for fine-grained emotion recognition",
            "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction",
            "year": 2021
        },
        {
            "authors": [
                "Xun Wang",
                "Haozhi Zhang",
                "Weilin Huang",
                "Matthew R Scott."
            ],
            "title": "Cross-batch memory for embedding learning",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Zengzhi Wang",
                "Qiming Xie",
                "Zixiang Ding",
                "Yi Feng",
                "Rui Xia."
            ],
            "title": "Is chatgpt a good sentiment analyzer? A preliminary study",
            "venue": "CoRR, abs/2304.04339.",
            "year": 2023
        },
        {
            "authors": [
                "Xing Wu",
                "Shangwen Lv",
                "Liangjun Zang",
                "Jizhong Han",
                "Songlin Hu."
            ],
            "title": "Conditional bert contextual augmentation",
            "venue": "International Conference on Conceptual Structures.",
            "year": 2018
        },
        {
            "authors": [
                "Da Yin",
                "Tao Meng",
                "Kai-Wei Chang."
            ],
            "title": "SentiBERT: A transferable transformer-based architecture for compositional sentiment semantics",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3695\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Yue Deng",
                "Bing-Quan Liu",
                "Sinno Jialin Pan",
                "Lidong Bing."
            ],
            "title": "Sentiment analysis in the era of large language models: A reality check",
            "venue": "ArXiv, abs/2305.15005.",
            "year": 2023
        },
        {
            "authors": [
                "Zhilu Zhang",
                "Mert R. Sabuncu."
            ],
            "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
            "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, page 8792\u20138802, Red Hook,",
            "year": 2018
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao."
            ],
            "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
            "venue": "ArXiv, abs/2302.10198.",
            "year": 2023
        },
        {
            "authors": [
                "Jie Zhou",
                "Junfeng Tian",
                "Rui Wang",
                "Yuanbin Wu",
                "Wenming Xiao",
                "Liang He."
            ],
            "title": "SentiX: A sentiment-aware pre-trained model for cross-domain sentiment analysis",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Hutter"
            ],
            "title": "2019) optimizer with Cosine decay and",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Despite the emergence of powerful models like GPT4 and LaMDA, lightweight pre-trained language models (PLMs) such as BERT and RoBERTa remain relevant since they are computationally less expensive and open. Being trained on massive amount of data available over the web, these models are extremely good at encoding general language properties, resulting in highly accurate word, sentence, and paragraph representations. These representations are then typically fine-tuned for given tasks using task-specific datasets. However, the representations learned using these PLMs are general purpose, and they do not capture the affective aspects (such as emotions, affects, etc.) of human communication well. Consider a few exemplar pairs of text fragments, along with the emotion they evoke, in Table 1. We expect the pairs of frag-\nments with the same emotion label to have similar embeddings if the PLMs are cognizant of emotion content. However, as evident from Table 1, the cosine distance (computed using BERT [CLS] embeddings) between pairs exhibiting the same emotion is anomalously higher than those between different emotion categories. This observation suggests that PLMs do not adequately capture emotion aspects that are inherently present in natural language text. Incorporating them into PLM representations can significantly benefit NLP applications that are affective in nature, such as sentiment analysis, sarcasm detection, empathetic agents, etc.\nOne straightforward way to inject emotions into PLMs is through transfer learning by fine-tuning them on emotion recognition task. Multi-task learning is also a potential alternative wherein the affective task of interest is paired with the emotion recognition task and optimized jointly. However, these approaches are not that effective in capturing emotion aspects (see section 5). The community has focussed on using resources such as sentiment lexicons, emoticons, etc., to impart affective aspects into PLMs, by modifying pre-training objectives such as masked language modeling (Zhou et al., 2020; Aduragba et al., 2021) and next sentence prediction (Babanejad et al., 2020). These approaches either learn PLMs from scratch or continually pre-train them on domain-specific corpora. A few works (Suresh and Ong, 2021; Yin et al., 2020) have also explored attention-based network modules over contextualized embeddings to capture affective semantics. The approaches mentioned above, however, are highly sensitive to the training corpus and the lexical resources used, and do not generalize well across tasks. Retrofitting methods that use external knowledge to improve representations have been well explored in the community for static embeddings (Mrk\u0161ic\u0301 et al., 2016; Shah et al., 2020). There are also attempts to retrofit PLMs to learn robust contextualized word embeddings\n(Shi et al., 2019), sentence embeddings (Cai et al., 2022), etc. However, PLM retrofitting is relatively less explored for human affects domains.\nRecently, contrastive learning has been actively explored in self-supervised setting to learn better sentence representations using data augmentation (Gao et al., 2021; Fang et al., 2020). Supervised contrastive learning (SCL) (Khosla et al., 2020) improves it further by generating informed training data using label information. SCL has been shown to learn robust sentence classification models (Gunel et al., 2021; Sedghamiz et al., 2021). It has also been applied to affective tasks such as arousal classification (Pinitas et al., 2022), emotion recognition in conversation (Song et al., 2022), and so on. A common theme across all these methods is that SCL has been applied only in a single task setting. Despite being fundamentally a representation learning tool, it has neither been explored in transfer learning nor in retrofitting settings.\nIn this work, we present a novel retrofitting method to learn emotion-aware PLMs using supervised contrastive learning as a transfer learning tool. We use go_emotions (Demszky et al., 2020), the largest publicly available emotion recognition dataset, as a retrofitting corpus. Our method updates PLM network weights such that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge originally present in the PLM is preserved. We refer to the resulting emotion-aware models as \u2217Emo1. Our contributions are:\n1. A novel retrofitting method to learn emotionaware PLMs using supervised contrastive learning (section 4). The \u2217Emo models produce emotion-aware sentence representations\n1We use \u2217Emo to refer to both BERTEmo and RoBERTaEmo combinedly.\nensuring that sentences with similar emotions have high cosine similarity and sentences with different emotions have low similarity. The sentence embeddings from BERT and RoBERTa do not have this property (quantitatively shown in Table 3).\n2. A detailed evaluation (Table 6) showing that the \u2217Emo models perform better than their pre-trained counterparts (about 1% statistically significant improvement in F1-score) and other approaches, such as transfer learning and multi-task learning, on sentiment analysis and sarcasm detection tasks. In limited data setting, they perform exceedingly better than BERT and RoBERTa, as exemplified by the few-shot learning experiments on sentiment analysis task (Figure 2)."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Affect-aware PLMs",
            "text": "Resources such as sentiment lexicons, emoticons, etc., have been actively used to learn affect-aware PLMs by updating masked language modeling (MLM) objective. For instance, SentiX (Zhou et al., 2020) learns the BERT model from scratch using reviews/ratings (Yelp, Amazon) dataset by increasing the masking probability for sentiment words and emoticons; EmoBERT (Aduragba et al., 2021) continually pre-trains by masking emotionbearing words in tweet dataset; and so on. Unlike MLM, Babanejad et al. (2020) generate emotionfeature vector using EmoLex (Mohammad and Turney, 2013) and use it for the next sentence prediction (NSP) task in continual pre-training setting. CARER (Saravia et al., 2018) is a graph-based approach for learning emotion-aware contextualized representations. SentiBERT (Yin et al., 2020) adds an attention network based semantic composition module over BERT representations, and fine-tunes\nit on sentiment analysis task using constituency trees. A few approaches use affective resources directly during end-task fine-tuning. For instance, KEA (Suresh and Ong, 2021) uses token-level valence, arousal, and dominance scores to learn attention weighted sentence representations. The affect-aware approaches described above, however, are highly sensitive to the training corpus and the lexical resources used, and may not generalize well across tasks.\nRetrofitting is a post-processing method that updates pre-trained network weights to respect constraints extracted from external knowledge resources. It is well explored for static embeddings, e.g., retrofitting for relations such as synonymy, and hypernymy (Mrk\u0161ic\u0301 et al., 2016; Shah et al., 2020), for affective lexicons (Shah et al., 2022b,a), and so on. Shi et al. (2019) retrofit PLM in ELMo using paraphrase context to learn robust contextualized word embeddings. Other notable approaches include retrofitting sentence embeddings with abstract meaning representations in multilingual setting (Cai et al., 2022), learning label-aware conditional mask language model for contextual data augmentation (Wu et al., 2018), and so on. PLM retrofitting, however, is relatively less explored for the human affects domain."
        },
        {
            "heading": "2.2 Large Language Models",
            "text": "The research community has recently been actively applying large language models (LLMs), especially ChatGPT, to sentiment analysis tasks (Zhong et al., 2023; Wang et al., 2023). These works have explored a variety of approaches, including zeroshot and in-context learning, as well as prompting methods such as chain-of-thought (CoT). Although these works have considered aspects such as polarity shift detection, aspect-based analysis, and sentiment inference, for the standard sentiment classification task, they experiment only with the Stanford sentiment treebank binary classification (SST2 dataset). While the reported results are comparable to fine-tuned BERT and RoBERTa, the SST2 dataset is a more straightforward, coarsegrained classification task with a SOTA accuracy of 97.5%. A more detailed evaluation with finegrained sentiment classes, non-standard English, etc., is required before establishing the efficacy of LLMs. To this end, Zhang et al. (2023) perform an exhaustive set of experiments with ChatGPT using zero-shot and in-context learning on 13 sentiment\nanalysis tasks. Unlike using LLMs during inference, Deng et al. (2023) use ChatGPT with CoT prompt to obtain labeled data for sentiment analysis and then use the weakly labeled data to learn an accurate student model. This approach, however, has only been tested on social media content in the finance domain."
        },
        {
            "heading": "2.3 Contrastive Learning",
            "text": "Contrastive learning (CL) aims to learn an embedding space such that similar data points are mapped close to each other while dissimilar points are pushed apart. Self-supervised contrastive learning uses data augmentation techniques such as lexical editing (Wu et al., 2018), back translation (Fei et al., 2020; Fang et al., 2020), dropout (Gao et al., 2021), cut-off (Shen et al., 2020), etc., to generate similar (positive) data points for the given anchor. The dissimilar (negative) points for the anchor are then selected randomly from in-batch examples. Supervised contrastive learning (SCL) (Khosla et al., 2020) takes this idea further by using labeled data to generate positives from the same class as the anchor, thereby providing more variability than data augmentation. SCL has been recently applied to text classification problems, e.g., joint learning with SCL and cross-entropy (CE) loss (Gunel et al., 2021), SCL followed by standard fine-tuning using CE in a pipeline fashion (Sedghamiz et al., 2021). It has also been applied for emotion recognition in conversations (Song et al., 2022), affect-infused representations for arousal classification (Pinitas et al., 2022), and so on. However, the approaches described above have applied SCL only in a single task setting. In contrast, we explore SCL as a transfer learning tool to induce emotion aspects into PLMs in retrofitting setting."
        },
        {
            "heading": "3 Retrofitting Corpus: go_emotions",
            "text": "To retrofit PLMs for emotions, we need a corpus that provides emotion annotations for text fragments, i.e., emotion recognition datasets. Such datasets, varying in size, labeling scheme, domain, etc., have been proposed in the field (see Bostan and Klinger 2018 for review). Being the largest publicly available dataset, we use go_emotions (Demszky et al., 2020) in this work. It provides fine-grained emotion annotations (27 categories) for 54,263 English Reddit comments. The dataset has been carefully created to avoid offensive, identity and religion terms. Though the text fragments\nin the dataset are marked with multiple emotion categories, we only use a subset of 45,446 examples that are annotated with a single emotion label. Table 2 shows the summary statistics for go_emotions."
        },
        {
            "heading": "4 Retrofitting Method",
            "text": "The application of supervised contrastive learning in our retrofitting method is inspired by (Khosla et al., 2020; Sedghamiz et al., 2021), albeit applied in a transfer learning setting. Our method also ensures that the linguistic knowledge already present in PLM network weights is not inadvertently perturbed. Figure 1 shows the architecture for our retrofitting method. 1. Training data generation: We sample training examples from go_emotions to create a minibatch B of size N (i.e., {xi, yi}i=1..N;xi = text fragment; yi = label). We consider two sampling alternatives: (1) MPerClassSampler samples an equal number of examples from each emotion category; (2) StratifiedSampler samples examples from an emotion category proportional to the total number of examples of that category in go_emotions. 2. Encoder: The PLM under consideration is treated as an encoder function. It takes text fragments xi from batch B as input and returns the [CLS]i embeddings as output, i.e., Enc(xi) = [CLS]i. 3. Projection head: It maps [CLS]i to\nan l2-normalized vector zi \u2208 Rd, i.e., Proj([CLS]i) = zi. We consider two variants for Proj: (1) Linear: [CLS]i projected linearly to R64; (2) MLP: [CLS]i projected non-linearly to R64 using a single hidden layer of size 768. 4. Loss function: In self-supervised contrastive learning, the normalized temperature-scaled cross entropy (NT-Xent) loss proposed by Chen et al. (2020) has been shown to learn robust latent representations by implicitly providing hard positive/negative mining. Khosla et al. (2020) extended it further for SCL by incorporating label information into the loss function. We use the supervised version of the loss function2 which is as follows,\nLscl = N\u2211 i=1 \u22121 |P (i)| \u2211 p\u2208P (i) log e(zi\u00b7zp)/\u03c4\u2211\nb\u2208B(i) e(zi\u00b7zb)/\u03c4\n(1)\nHere, B(i): b \u2208 B\\{i}; P (i): {p \u2208 B; yp = yi}; and \u03c4 : temperature scaling factor. Setting \u03c4 to a low value gives more importance to hard positives/negatives, whereas a high value weighs all pairs nearly equally. Vector space preservation (VSP): Being trained on a vast amount of data, PLMs are extremely good at encoding lexical, syntactic, and semantic relations, concept similarities, and so on (Jawahar et al., 2019; Lin et al., 2019). We do not want to lose such valuable information while retrofitting them for emotions. While retrofitting literature on static embeddings has recognized and addressed this issue by introducing a regularization term that preserves the topology of pre-trained vector space (Mrk\u0161ic\u0301 et al., 2016; Mrk\u0161ic\u0301 et al., 2017), such regularization has not been considered by retrofitting methods for PLMs. To address this, we use the following regularization term,\nLvsp = N\u2211 i=1 \u2016Enc(xi)\u2212 Encfixed(xi)\u20162 (2)\n2Contrastive learning on deep networks generally requires large mini-batch sizes for training stability (Radford et al., 2021; Chen et al., 2020). To support this, we use cross-batch memory (XBM) (Wang et al., 2020) with the SCL loss.\nWhile Enc(\u00b7) comes from step 2, Encfixed(\u00b7) is a fixed version where the network weights from PLM are frozen, providing the snapshot of the PLM before the application of contrastive learning. With this regularization, the network weights in Enc(\u00b7) are updated such that the sentence embeddings in [CLS] do not deviate much from their pre-trained version. The final loss function used by our method is then: L = Lscl + \u03bbLvsp, where \u03bb is a hyperparameter that determines how strictly the original pre-trained vector space is preserved.\nPost training, we discard the projection head. While the encoder Enc(\u00b7), with its updated weights, provides us with emotion-aware PLM."
        },
        {
            "heading": "5 Experiments",
            "text": "We evaluate our retrofitting method on two PLMs in BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) {base, uncased versions}. The emotion-aware PLMs retrofitted by our method are referred to as \u2217Emo, i.e., BERTEmo and RoBERTaEmo. To select the best hyper-parameter configuration, we consider two aspects: (1) quality of \u2217Emo in learning emotion-aware embeddings, measured using clustering and retrieval metrics (refer section 5.2 for details) (2) vector space preservation: mean cosine distance between sentence embeddings obtained from \u2217Emo and its pre-trained version (cosine distance = 1 - cosine similarity; Range=[0, 2]). We first filter vector space preserving configurations. From the filtered set, we then choose the configuration with the highest AMI (clustering quality metric) as the best configuration. We detail this process and report the complete hyper-parameter grid search in Appendix A."
        },
        {
            "heading": "5.1 Compared Work",
            "text": "The BERT and RoBERTa models are considered as pre-trained baseline. For downstream tasks, we add a linear classification head over the [CLS] embeddings and jointly fine-tune both the PLM network weights and the classification head using cross-entropy (CE) loss (referred to as standard fine-tuning).\nFor transfer learning (TLearn), we first add a classification head over the [CLS] embeddings and perform fine-tuning on the emotion recognition task using the go_emotions dataset. After training for emotions, the updated PLM is further fine-tuned on end tasks using a linear classification head. For multi-task learning (MTL), we add two classifi-\ncation heads, one for the end task under consideration and the other for the emotion recognition task. While training, both tasks are given equal weightage by sampling their mini-batches in equal proportions. Contrastive methods: We compare our approach with Sentence-BERT/RoBERTa (SNT) (Reimers and Gurevych, 2019). It fine-tunes BERT and RoBERTa on natural language inference task using Siamese network architecture, with the objective to learn semantically meaningful sentence representations. Next, we compare our approach with two SCL methods: (1) SCL-Joint (Gunel et al., 2021): proposed for sequence classification tasks, it defines the loss function as an affine combination of CE and SCL loss. (2) SupCLSeq (Sedghamiz et al., 2021): a pipelined approach that first updates PLM network weights using SCL loss and dropoutbased data augmentation. The updated PLM is then further fine-tuned on end tasks using CE loss. Both methods apply the CE and SCL losses only for the task under consideration, i.e., single-task setting. They do not take any explicit affective signals into account. Affect-aware methods: We compare the \u2217Emo models with two affect-aware PLMs: (1) KEA (Suresh and Ong, 2021): enriches contextualized word embeddings using valence, arousal, and dominance scores in the NRC VAD lexicon (Mohammad, 2018). The [CLS] embeddings are first used as a query vector to learn sentence embeddings in terms of attention weighted VAD-enriched contextualized embeddings. A classification head over the sentence embeddings is then used to fine-tune end tasks; (2) SentiX (Zhou et al., 2020): learns sentiment-aware BERT from scratch using largescale review datasets. In addition to MLM and NSP, it adds additional pre-training objectives at token and sentence level using emoticons, sentiment lexicons, and review ratings."
        },
        {
            "heading": "5.2 Evaluating Emotion-awareness",
            "text": "The question we posed to evaluate language models for their emotion content is: Do text fragments that evoke the same emotion have similar sentence embeddings? In other words, are fragments with similar emotion content clustered together in the embedding space? Our retrofitting corpus (i.e., unseen test set in go_emotions) provides the required test bed for this study. We perform K-means clustering (#means = 28), considering [CLS] embeddings\nof text fragments as features. Since the emotion labels are available, we apply external cluster validity indices such as adjusted mutual information (AMI), adjusted rand index (ARI), and Fowlkes Mallows score (FMS) (refer to Scikit-learn user guide) to measure clustering quality. In addition to clustering, we also consider retrieval-based metrics3 such as mean reciprocal rank (MRR), precision@1 (P@1), and mean average precision at r (MAP@r). These metrics directly probe the nearest neighbors of given text fragments for their consistency in emotion labeling. While retrofitting PLMs for emotions, we want to preserve the lexical, syntactic, and semantic knowledge already contained in them. To quantify this property, we compute the mean cosine distance between sentence embeddings obtained from emotion-aware language models and their pre-trained counterparts (referred to as \u2206emb; lower values are better).\nWe investigate4 \u2217Emo, SNT, TLearn, SentiX, and their pre-trained counterparts. As shown in Table 3, the BERT and RoBERTa baselines have extremely low scores across all metrics. This is because emotion aspects have not been explicitly taken into account during their pre-training phase. The sentence embeddings in SNT slightly improve the clustering and retrieval metrics. Sur-\n3refer to Appendix C for details on metrics. 4Not applicable for remaining methods: MTL and KEA include emotion signals only during end task fine-tuning; SCL-Joint and SupCLSeq do not consider emotion signals at all.\nprisingly, even though SentiX is trained on sentiment data, which has some affective aspects, it is not good at capturing emotion aspects. The straightforward way of incorporating emotions into PLMs by fine-tuning them on emotion recognition task, i.e., TLearn, drastically improves the clustering and retrieval metrics. However, it excessively alters the topology of sentence embedding space (very high \u2206emb) and may end up overfitting the embeddings for emotions. The \u2217Emo models retrofitted by our method are not only emotion-aware (high values for clustering and retrieval metrics) but also preserve the topology of the embedding space (low values for \u2206emb).\nAppendix D shows 2-dim UMAP plots for text fragments in the go_emotions test set. The fragments from all emotion categories are completely interleaved for BERT and RoBERTa. On the other hand, BERTEmo and RoBERTaEmo provide a good separation between different emotion categories. The cosine distances computed using BERTEmo are well calibrated for emotion content, as evident from the exemplar pairs in Table 1."
        },
        {
            "heading": "5.3 Evaluation on Downstream Tasks",
            "text": "We evaluate our method on two affective downstream tasks: (1) Sentiment analysis on Stanford sentiment treebank (sentence level) with both the graded (SST5) and binary (SST2) variants; and SemEval 2017 task 4A (SE) containing tweet messages; (2) Sarcasm detection using Mustard++ dataset (Mus) that contains sit-com utterances.\nTable 4 details the statistics of these datasets. While we showed the intrinsic efficacy of emotion-aware sentence representations in section 5.2, are they effective for downstream tasks? To study this, we learn KNN classifier5 for end tasks, treating [CLS] embeddings as features. Being emotion-aware, the retrofitted \u2217Emo models achieve significantly better results (\u2248 10% improvements in F1-score) than baselines on both tasks (refer to Table 5). This suggests that incorporating emotion content into PLMs is beneficial for end tasks that are affective in nature. Next, we perform fine-tuning experiments where we update all network parameters during training.\nTable 6 reports the micro F1-scores (averaged over 30 runs) on the downstream tasks for the finetuning experiments. The standard fine-tuning of BERT and RoBERTa using CE loss seems to be a hard baseline to beat for both tasks. The sentence embeddings in SNT improves it slightly for the sentiment analysis task. Though ubiquitous, the CE loss sometimes leads to poor generalization (Liu et al., 2016) and may not be robust to noisy labels (Zhang and Sabuncu, 2018). Therefore it has recently been supplemented with the SCL loss either in a joint or pipeline architecture. Though the joint learning approach in SCL-Joint could not perform well, we observed that the pipelined architecture in\n5The network weights are fixed only for the KNN classification experiments. The rest of the experiments in this section jointly update parameters of both the PLM and classification head (i.e., standard fine-tuning)\nSupCLSeq led to slightly improved F1-scores on both tasks. It should be noted that these approaches have not taken any emotion signals into account during fine-tuning.\nBeing pre-trained on large review datasets such as Yelp and Amazon that mainly contain sentiment signals, SentiX attains the highest F1-score on sentiment analysis task (for BERT). However, it does not generalize to other tasks (e.g., inferior results on sarcasm detection). The knowledge (valence, arousal, dominance) embedded approach in KEA unexpectedly does not perform well on any task. The PLMs retrofitted for emotions using transfer learning (TLearn) and multi-task learning (MTL) perform better than the pre-trained baselines on both tasks, exhibiting the positive impact the external emotion signals provide. In this work, we seek to learn emotion-aware PLMs, but not at the expense of displacing existing linguistic knowledge (by overfitting them for emotions). While the VSP loss helps in preserving the topology of the sentence embedding space, the contrastive loss brings robustness to our method. This helps \u2217Emo models achieve better results than other approaches, with \u2248 1% statistically significant improvements (p-val < 0.01 for one-tailed student\u2019s t-test with sample size 30) over pre-trained baselines. Comparison with ChatGPT: Zhang et al. (2023) have reported results on gpt-3.5-turbo with zeroshot and in-context learning for 13 sentiment analysis tasks. Table 7 compares these results with the fine-tuned RoBERTaEmo (our method). As we can see, RoBERTaEmo performs significantly better than gpt-3.5-turbo on the sentence-level sentiment classification datasets considered in this work. It will be interesting to compare RoBERTaEmo with a fine-tuned gpt-3.5-turbo. We will leave this for future work."
        },
        {
            "heading": "5.3.1 Ablation Study",
            "text": "Vector space preservation: Retrofitting methods for static embeddings have consistently used a reg-"
        },
        {
            "heading": "I 0.5677 0.9425 0.7055 0.6059",
            "text": "ularization term to preserve the topology of pretrained vector space. However, such a term has not been considered in the context of PLMs. By constraining sentence representations to be closer to their pre-trained version, The Lvsp term in our loss function guides weight updates for emotions such that the linguistic knowledge already present in PLMs is not distorted. When we retrofitted BERT and RoBERTa without vector space preservation (\u2217Emo\u03bb=0 in Table 6), the performance on both the end tasks consistently deteriorated. On projection head: The SCL methods for text\n(Gunel et al., 2021; Sedghamiz et al., 2021) apply contrastive loss directly on the encoder output, i.e., [CLS] embeddings (or Identity (I) projection head). However, for images, as suggested by Chen et al. (2020), applying contrastive loss directly on the encoder (ResNet) output inadvertently results in a loss of information that may be useful for downstream tasks. To avoid such unintended effects, we first map encoder output to a 64-dim vector space using two projection head variants, i.e., Linear and MLP, as described in section 4. Table 8 shows the F1scores on end tasks with RoBERTaEmo variants that are learned using different projection heads. The results indicate that the Linear and MLP projection heads perform better than directly using the encoder output6."
        },
        {
            "heading": "5.3.2 Few-shot Learning Experiments",
            "text": "We perform few-shot learning experiments on the sentiment analysis task. For all datasets, we first sample train data of various sizes from the original set such that the #training examples are in [20, 50, 100, 500], keeping the original label distribution intact. We then fine-tune BERTEmo and BERT on these data sizes and compare their micro-F1 scores on the original test set. As shown in Figure 2, the emotion-aware BERTEmo outperforms its pre-trained counterpart BERT across\n6The MLP head is more robust. The variance in F1-scores across multiple runs is relatively lower with MLP than Linear.\nall datasets by a significant margin (similar observation for RoBERTaEmo; refer to Figure 3 in Appendix B). The difference in performance gradually decreases with an increase in #training examples. This suggests that the affective knowledge on emotions, as captured by our retrofitting method, helps improve end tasks, especially in few-shot learning scenario."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We present a novel retrofitting method to learn emotion-aware PLMs in a contrastive learning setting using emotion labeling in the go_emotions (Demszky et al., 2020) corpus. It updates PLM network weights such that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and fragments with different emotions are pushed apart while preserving the linguistic knowledge originally captured during the PLM pre-training phase. The emotion-aware models (\u2217Emo) learned by our method perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other benchmarks, with significant gains in few-shot learning setting.\nGoing forward, we want to extend our retrofitting method to other affective resources such as the NRC VAD lexicon, emoticons, etc. We also plan to investigate affective content in large language models such as GPT4.\nLimitations\nIn this work, we have used the go_emotions dataset primarily due to its large size. The dataset has been created from English Reddit comments. Being retrofitted solely on go_emotions, \u2217Emo may contain Reddit-specific nuances. It will be interesting to learn jointly from multiple emotion recognition datasets spanning different genres and domains such as news, tweets, blogs, health, politics, etc. However, this may also bring in additional complexities. For instance, we do not know which domain or genre should be given more importance, how to handle datasets that vary a lot in size, etc.\nWe demonstrated the effectiveness of \u2217Emo models on two downstream tasks. We believe these models can generalize to other affective tasks such as hate speech detection, bias detection, and empathetic agents. However, it is difficult to comment on their effectiveness on general NLP tasks such as entity extraction, grammatical error correction, etc. Though the vector space preservation term in our loss function keeps a check on the PLM network weights so that the existing linguistic knowledge is not inadvertently perturbed, a few changes are bound to happen to accommodate the emotion aspects. It will be interesting to compare \u2217Emo with their pre-trained counterparts on general benchmarks such as GLUE and SuperGLUE.\nEthics Statement\nThe go_emotions dataset has been created from English Reddit comments which are known to contain toxic/offensive language. These comments are also demographically biased toward young male users. Though the creators of go_emotions have taken extensive care in data filtering, pre-processing, and masking steps to address the bias and offenserelated issues, the dataset might still inadvertently contain inappropriate content. This might then flow directly into \u2217Emo models, degrading their quality."
        },
        {
            "heading": "A Training Details",
            "text": "In this section, we provide the complete hyperparameter grid search details and the best combinations selected thereof. As described in section 4, we treat the PLMs in BERT and RoBERTa as encoding function Enc(\u00b7). The [CLS] embeddings obtained from the Enc(\u00b7) function are then passed to a projection head Proj(.). We consider three variants for Proj(.): {Identity, Linear, MLP}. For both Enc(.) and Proj(.), we set weight-decay to 0.01 and dropout to 0.1. The temperature factor \u03c4 in NT-Xent loss in eq. 1 is varied as {0.05, 0.1, 0.2}. We set the batch size B to 64, with crossbatch memory in XBM (Wang et al., 2020) varied as {512, 1024, 2048}. The regularizer for vector space preservation loss, \u03bb, is varied as {0.01, 0.05, 0.1, 0.5}. The hyper-parameters relevant for training are: ReLU activation; AdamW (Loshchilov and Hutter, 2019) optimizer with Cosine decay and warm-up (3 steps); learning rate varied as {5e-06, 1e-05}; and 30 epochs with early stopping using official validation set. The training batches are generated using two sampler variants: MPerClassSampler and StratifiedSampler.\nFor experimentation, we used Nvidia DGX A100 GPUs with a memory size of 20GB RAM. Each configuration, on average, took 17 minutes to run.\nWe find the best hyper-parameter configuration setting in the following way. Post training, for\neach combination, we compute two metrics: (1) the clustering metric in AMI to measure the emotionawareness of the retrofitted PLM; (2) the mean cosine distance between sentence embeddings obtained from the retrofitted PLM and its pre-trained version, i.e., \u2206emb , to measure the vector space preservation quality. We first filter configurations for which \u2206emb is less than 0.1 for BERT and 0.2 for RoBERTa. We then choose the configuration with the highest AMI from the filtered set. We consider the performance on the SST5 dataset (sentiment analysis task) for tie-breaking. Table 9 reports the best hyper-parameter configurations for BERTEmo and RoBERTaEmo. To implement our retrofitting method, we used Pytorch Metric Learning library (Musgrave et al., 2020b)."
        },
        {
            "heading": "B Training Details: Downstream tasks",
            "text": "For downstream tasks, we add a linear classification head over the sentence embeddings in [CLS] and jointly fine-tune both the PLM network weights and the classification head using cross-entropy loss. We implement this using Auto Classes7 from the huggingface library. The relevant hyper-parameters are: AdamW optimizer with learning rate in {1e-05, 2e-05, 3e-05}, dropout=0.1, and batch size=32.\nWhile section 5.3.2 compares BERT with the emotion-aware BERTEmo on few-shot learning experiments for the sentiment analysis task, here we report results for RoBERTa and its emotion-aware version in RoBERTaEmo. As can be seen from Figure 3, RoBERTaEmo performs significantly better than its pre-trained version RoBERTa across all datasets in the limited data setting."
        },
        {
            "heading": "C Evaluating Emotion-awareness: Metrics",
            "text": "As described in section 5.2, we use clustering and retrieval metrics for evaluating emotion awareness. The existing labeling of text fragments in the go_emotions test set provides us with true clustering. Whereas the clustering induced by the K-means algorithm gives the predicted clustering. The partition of text fragments provided by the true and predicted clustering is then used to compute the clustering validity indices such as adjusted mutual information (AMI), adjusted rand index (ARI), and Fowlkes Mallows score (FMS) (refer to Scikitlearn user guide). Unlike clustering, the retrievalbased metrics directly probe the neighborhood of\n7sequence classification - huggingface\ntext fragments for their consistency in emotion labeling. The precision@1 metric checks the immediate neighbor of the queried text fragment. The mean reciprocal rank (MRR) metric considers the reciprocal rank of the closest fragment with the same emotion label as the query fragment. The mean average precision at r (MAP@r) metric considers average precision till rank r, where r is the number of text fragments with the same emotion label as the query fragment. Refer (Musgrave et al., 2020a) for details."
        },
        {
            "heading": "D Evaluating Emotion-awareness: UMAP plots",
            "text": "We visualize 2-dim UMAP (McInnes et al., 2018) plots for text fragments in the go_emotions test set, comparing \u2217Emo with their pre-trained counterparts. We learn the 2-dim embeddings using the umap-learn library8 with the following hyperparameter setting: #neighbors=15; min_dist=0.1; distance metric=Euclidean. As we can see from Figure 4 and Figure 5, The text fragments from all emotion categories are completely interleaved for BERT and RoBERTa. On the other hand, BERTEmo and RoBERTaEmo provide a good separation between different emotion categories.\n8https://pypi.org/project/umap-learn/"
        }
    ],
    "title": "Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning",
    "year": 2023
}