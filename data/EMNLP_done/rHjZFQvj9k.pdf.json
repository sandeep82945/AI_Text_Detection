{
    "abstractText": "Distributed representations of words encode lexical semantic information, but what type of information is encoded and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. This theory also extends to contextualized word embeddings in language models or any neural networks with the softmax output layer. We also demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of the informativeness of a word in tasks such as keyword extraction, proper-noun discrimination, and hypernym discrimination.",
    "authors": [
        {
            "affiliations": [],
            "name": "Momose Oyama"
        },
        {
            "affiliations": [],
            "name": "Sho Yokoi"
        },
        {
            "affiliations": [],
            "name": "Hidetoshi Shimodaira"
        }
    ],
    "id": "SP:d17567d6cd7b4b344151606653eee2ed23352839",
    "references": [
        {
            "authors": [
                "Alan Agresti."
            ],
            "title": "Categorical Data Analysis, 3rd edition",
            "venue": "John Wiley & Sons.",
            "year": 2013
        },
        {
            "authors": [
                "Shun-Ichi Amari."
            ],
            "title": "Differential Geometry of Curved Exponential Families-Curvatures and Information Loss",
            "venue": "The Annals of Statistics, 10(2):357 \u2013 385.",
            "year": 1982
        },
        {
            "authors": [
                "Shun-Ichi Amari."
            ],
            "title": "Natural gradient works efficiently in learning",
            "venue": "Neural computation, 10:251\u2013276.",
            "year": 1998
        },
        {
            "authors": [
                "Makoto Aoshima",
                "Dan Shen",
                "Haipeng Shen",
                "Kazuyoshi Yata",
                "Yi-Hui Zhou",
                "James S Marron."
            ],
            "title": "A survey of high dimension low sample size asymptotics",
            "venue": "Australian & New Zealand journal of statistics, 60:4\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Nikolay Arefyev",
                "Pavel Ermolaev",
                "Alexander Panchenko"
            ],
            "title": "How much does a word weigh? weighting word embeddings for word sense induction",
            "year": 2018
        },
        {
            "authors": [
                "A.R. Aronson",
                "O. Bodenreider",
                "H.F. Chang",
                "S.M. Humphrey",
                "J.G. Mork",
                "S.J. Nelson",
                "T.C. Rindflesch",
                "W.J. Wilbur."
            ],
            "title": "The NLM indexing initiative",
            "venue": "Proceedings of the AMIA Symposium, pages 17\u2013",
            "year": 2000
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Wei Hu",
                "Yuping Luo."
            ],
            "title": "Implicit regularization in deep matrix factorization",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Giusepppe Attardi."
            ],
            "title": "Wikiextractor",
            "venue": "https:// github.com/attardi/wikiextractor.",
            "year": 2015
        },
        {
            "authors": [
                "Isabelle Augenstein",
                "Mrinal Das",
                "Sebastian Riedel",
                "Lakshmi Vikraman",
                "Andrew McCallum."
            ],
            "title": "SemEval 2017 task 10: ScienceIE - extracting keyphrases and relations from scientific publications",
            "venue": "Proceedings of the 11th International Workshop",
            "year": 2017
        },
        {
            "authors": [
                "Ole Barndorff-Nielsen."
            ],
            "title": "Information and exponential families: in statistical theory",
            "venue": "John Wiley & Sons.",
            "year": 2014
        },
        {
            "authors": [
                "Marco Baroni",
                "Alessandro Lenci."
            ],
            "title": "How we BLESSed distributional semantic evaluation",
            "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1\u201310, Edinburgh, UK. Association for Computational",
            "year": 2011
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the Association for Computational Linguistics, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Bott",
                "Dominik Schlechtweg",
                "Sabine Schulte im Walde."
            ],
            "title": "More than just frequency? demasking unsupervised hypernymy prediction methods",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 186\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Mikael Brunila",
                "Jack LaViolette"
            ],
            "title": "What company do words keep? revisiting the distributional semantics of J.R. firth & zellig Harris",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Ciprian Chelba",
                "Tom\u00e1s Mikolov",
                "Mike Schuster",
                "Qi Ge",
                "Thorsten Brants",
                "Phillipp Koehn",
                "Tony Robinson."
            ],
            "title": "One billion word benchmark for measuring progress in statistical language modeling",
            "venue": "INTERSPEECH 2014, 15th Annual Conference of the In-",
            "year": 2014
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Bradley Efron."
            ],
            "title": "The geometry of exponential families",
            "venue": "The Annals of Statistics, 6:362\u2013376.",
            "year": 1978
        },
        {
            "authors": [
                "Bradley Efron."
            ],
            "title": "Exponential Families in Theory and Practice",
            "venue": "Cambridge University Press.",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Evert."
            ],
            "title": "The statistics of word cooccurrences: word pairs and collocations",
            "venue": "Ph.D. thesis, University of Stuttgart.",
            "year": 2005
        },
        {
            "authors": [
                "J.R. Firth"
            ],
            "title": "A synopsis of linguistic theory 1930",
            "year": 1957
        },
        {
            "authors": [
                "Sujatha Das Gollapalli",
                "Cornelia Caragea."
            ],
            "title": "Extracting keyphrases from research papers using citation networks",
            "venue": "Proceedings of AAAI Conference on Artificial Intelligence, 28(1).",
            "year": 2014
        },
        {
            "authors": [
                "Michael Gutmann",
                "Aapo Hyv\u00e4rinen."
            ],
            "title": "Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics",
            "venue": "J. Mach. Learn. Res., 13:307\u2013361.",
            "year": 2012
        },
        {
            "authors": [
                "Zellig Harris."
            ],
            "title": "Distributional structure",
            "venue": "Word, 10(2-3):146\u2013162.",
            "year": 1954
        },
        {
            "authors": [
                "Aur\u00e9lie Herbelot",
                "Mohan Ganesalingam."
            ],
            "title": "Measuring semantic content in distributional vectors",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 440\u2013445, Sofia, Bulgaria. Association",
            "year": 2013
        },
        {
            "authors": [
                "Anette Hulth."
            ],
            "title": "Improved automatic keyword extraction given more linguistic knowledge",
            "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216\u2013223.",
            "year": 2003
        },
        {
            "authors": [
                "Sungkyu Jung",
                "J Stephen Marron."
            ],
            "title": "PCA consistency in high dimension, low sample size context",
            "venue": "The Annals of Statistics, 37:4104 \u2013 4130.",
            "year": 2009
        },
        {
            "authors": [
                "Mikhail Khodak",
                "Nikunj Saunshi",
                "Yingyu Liang",
                "Tengyu Ma",
                "Brandon Stewart",
                "Sanjeev Arora."
            ],
            "title": "A la carte embedding: Cheap but effective induction of semantic feature vectors",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Su Nam Kim",
                "Olena Medelyan",
                "Min-Yen Kan",
                "Timothy Baldwin."
            ],
            "title": "SemEval-2010 task 5 : Automatic keyphrase extraction from scientific articles",
            "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21\u201326, Uppsala, Sweden.",
            "year": 2010
        },
        {
            "authors": [
                "Goro Kobayashi",
                "Tatsuki Kuribayashi",
                "Sho Yokoi",
                "Kentaro Inui."
            ],
            "title": "Attention is not only a weight: Analyzing transformers with vector norms",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Mikalai Krapivin",
                "Aliaksandr Autaeu",
                "Maurizio Marchese."
            ],
            "title": "Large dataset for keyphrases extraction",
            "venue": "Technical Report DISI-09-055, University of Trento.",
            "year": 2009
        },
        {
            "authors": [
                "Erich L Lehmann",
                "George Casella."
            ],
            "title": "Theory of point estimation",
            "venue": "Springer New York, NY.",
            "year": 1998
        },
        {
            "authors": [
                "Alessandro Lenci",
                "Giulia Benotto."
            ],
            "title": "Identifying hypernyms in distributional semantic spaces",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task,",
            "year": 2012
        },
        {
            "authors": [
                "Bofang Li",
                "Tao Liu",
                "Zhe Zhao",
                "Buzhou Tang",
                "Aleksandr Drozd",
                "Anna Rogers",
                "Xiaoyong Du."
            ],
            "title": "Investigating different syntactic context types and context representations for learning word embeddings",
            "venue": "Proceedings of the 2017 Conference on Empiri-",
            "year": 2017
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Matt Mahoney."
            ],
            "title": "About the test data",
            "venue": "http:// mattmahoney.net/dc/textdata.html.",
            "year": 2011
        },
        {
            "authors": [
                "Lu\u00eds Marujo",
                "M\u00e1rcio Viveiros",
                "Jo\u00e3o Paulo da Silva Neto."
            ],
            "title": "Keyphrase cloud generation of broadcast news",
            "venue": "Proceedings of Annual Conference of the International Speech Communication Association, pages 2393\u20132396.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Matsuo",
                "M. Ishizuka."
            ],
            "title": "Keyword extraction from a single document using word co-occurrence statistical information",
            "venue": "International Journal on Artificial Intelligence Tools, 13(01):157\u2013169.",
            "year": 2004
        },
        {
            "authors": [
                "Alyona Medelyan."
            ],
            "title": "Keyword extraction datasets",
            "venue": "https://github.com/zelandiya/ keyword-extraction-datasets.",
            "year": 2015
        },
        {
            "authors": [
                "Olena Medelyan",
                "Eibe Frank",
                "Ian H. Witten."
            ],
            "title": "Human-competitive tagging using automatic keyphrase extraction",
            "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318\u20131327, Singapore. As-",
            "year": 2009
        },
        {
            "authors": [
                "Olena Medelyan",
                "Ian H. Witten."
            ],
            "title": "Domainindependent automatic keyphrase indexing with small training sets",
            "venue": "Journal of the American Society for Information Science and Technology, 59(7):1026\u2013 1040.",
            "year": 2008
        },
        {
            "authors": [
                "Olena Medelyan",
                "Ian H Witten",
                "David Milne."
            ],
            "title": "Topic indexing with Wikipedia",
            "venue": "Proceedings of the AAAI WikiAI workshop, volume 1, pages 19\u201324.",
            "year": 2008
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "Jeff Mitchell",
                "Mirella Lapata."
            ],
            "title": "Composition in distributional models of semantics",
            "venue": "Cognitive Science, 34(8):1388\u20131429.",
            "year": 2010
        },
        {
            "authors": [
                "Marcelo A. Montemurro",
                "Dami\u00e1 n H. Zanette."
            ],
            "title": "Towards the quantification of the semantic information encoded in written language",
            "venue": "Advances in Complex Systems, 13(2):135\u2013153.",
            "year": 2010
        },
        {
            "authors": [
                "Thuy Dung Nguyen",
                "Min-Yen Kan."
            ],
            "title": "Keyphrase extraction in scientific publications",
            "venue": "Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers, pages 317\u2013326, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2007
        },
        {
            "authors": [
                "Sergey Oladyshkin",
                "Wolfgang Nowak."
            ],
            "title": "The connection between bayesian inference and information theory for model selection, information gain and experimental design",
            "venue": "Entropy, 21(11):1081.",
            "year": 2019
        },
        {
            "authors": [
                "Matteo Pagliardini",
                "Prakhar Gupta",
                "Martin Jaggi."
            ],
            "title": "Unsupervised learning of sentence embeddings using compositional n-gram features",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Enrico Santus",
                "Alessandro Lenci",
                "Qin Lu",
                "Sabine Schulte im Walde."
            ],
            "title": "Chasing hypernyms in vector spaces with entropy",
            "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short",
            "year": 2014
        },
        {
            "authors": [
                "Enrico Santus",
                "Frances Yung",
                "Alessandro Lenci",
                "Chu-Ren Huang"
            ],
            "title": "EVALution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models",
            "venue": "In Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources",
            "year": 2015
        },
        {
            "authors": [
                "Adriaan M.J. Schakel",
                "Benjamin J. Wilson."
            ],
            "title": "Measuring word significance using distributed representations of words",
            "venue": "ArXiv 1508.02297.",
            "year": 2015
        },
        {
            "authors": [
                "Tobias Schnabel",
                "Igor Labutov",
                "David Mimno",
                "Thorsten Joachims."
            ],
            "title": "Evaluation methods for unsupervised word embeddings",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 298\u2013307, Lisbon,",
            "year": 2015
        },
        {
            "authors": [
                "A.T. Schutz."
            ],
            "title": "Keyphrase extraction from single documents in the open domain exploiting linguistic and statistical methods",
            "venue": "Master\u2019s thesis, National University of Ireland.",
            "year": 2008
        },
        {
            "authors": [
                "Vered Shwartz",
                "Enrico Santus",
                "Dominik Schlechtweg."
            ],
            "title": "Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for",
            "year": 2017
        },
        {
            "authors": [
                "Kumiko Tanaka-Ishii."
            ],
            "title": "Statistical Universals of Language",
            "venue": "Springer.",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Christian Wartena",
                "Rogier Brussee",
                "Wout Slakhorst."
            ],
            "title": "Keyword extraction using word co-occurrence",
            "venue": "2010 Workshops on Database and Expert Systems Applications, pages 54\u201358.",
            "year": 2010
        },
        {
            "authors": [
                "Julie Weeds",
                "Daoud Clarke",
                "Jeremy Reffin",
                "David Weir",
                "Bill Keller."
            ],
            "title": "Learning to distinguish hypernyms and co-hyponyms",
            "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2249\u2013",
            "year": 2014
        },
        {
            "authors": [
                "Julie Weeds",
                "David Weir."
            ],
            "title": "A general framework for distributional similarity",
            "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 81\u201388.",
            "year": 2003
        },
        {
            "authors": [
                "Julie Weeds",
                "David Weir",
                "Diana McCarthy."
            ],
            "title": "Characterising measures of lexical distributional similarity",
            "venue": "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 1015\u20131021, Geneva, Switzerland. COL-",
            "year": 2004
        },
        {
            "authors": [
                "Wikimedia Foundation."
            ],
            "title": "English wikipedia dump data",
            "venue": "Accessed on: 15-June-2021.",
            "year": 2021
        },
        {
            "authors": [
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Sho Yokoi",
                "Ryo Takahashi",
                "Reina Akama",
                "Jun Suzuki",
                "Kentaro Inui."
            ],
            "title": "Word rotator\u2019s distance",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2944\u20132960, Online. Association for Computa-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The strong connection between natural language processing and deep learning began with word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017; Schnabel et al., 2015). Even in today\u2019s complex models, each word is initially converted into a vector in the first layer. One of the particularly interesting empirical findings about word embeddings is that the norm represents the relative importance of the word while the direction represents the meaning of the word (Schakel and Wilson, 2015; Khodak et al., 2018; Arefyev et al., 2018; Pagliardini et al., 2018; Yokoi et al., 2020).\nThis study focuses on the word embeddings obtained by the skip-gram with negative sampling (SGNS) model (Mikolov et al., 2013). We show theoretically and experimentally that the Euclidean\nnorm of embedding for word w, denoted as \u2225uw\u2225, is closely related to the Kullback-Leibler (KL) divergence of the co-occurrence distribution p(\u00b7|w) of a word w for a fixed-width window to the unigram distribution p(\u00b7) of the corpus, denoted as\nKL(w) := KL(p(\u00b7|w) \u2225 p(\u00b7)).\nIn Bayesian inference, the expected KL divergence is called information gain. In this context, the prior distribution is p(\u00b7), and the posterior distribution is p(\u00b7|w). The information gain represents how much information we obtain about the context word distribution when observing w. Table 1 shows that the 10 highest values of KL(w) are given by contextspecific informative words, while the 10 lowest values are given by context-independent words.\nFig. 1 shows that \u2225uw\u22252 is almost linearly related to KL(w); this relationship holds also for a larger corpus of Wikipedia dump as shown in Appendix G. We prove in Section 4 that the square of the norm of the word embedding with a whitening-like transformation approximates the KL divergence1. The main results are explained\n1Readers who are interested in information-theoretic mea-\nby the theory of the exponential family of distributions (Barndorff-Nielsen, 2014; Efron, 1978, 2022; Amari, 1982).\nEmpirically, the KL divergence, and thus the norm of word embedding, are helpful for some NLP tasks. In other words, the notion of information gain, which is defined in terms of statistics and information theory, can be used directly as a metric of informativeness in language. We show this through experiments on the tasks of keyword extraction, proper-noun discrimination, and hypernym discrimination in Section 7.\nIn addition, we perform controlled experiments that correct for word frequency bias to strengthen the claim. The KL divergence is heavily influenced by the word frequency nw, the number of times that word w appears in the corpus. Since the corpus size is finite, although often very large, the KL divergence calculated from the co-occurrence matrix of the corpus is influenced by the quantization error and the sampling error, especially for low-frequency words. The same is also true for the norm of word embedding. This results in bias due to word frequency, and a spurious relationship is observed between word frequency and other quantities. Therefore, in the experiments, we correct the word frequency bias of the KL divergence and the norm of word embedding.\nThe contributions of this paper are as follows:\nsures other than KL divergence are referred to Appendix B. The KL divergence is more strongly related to the norm of word embedding than the Shannon entropy of the cooccurrence distribution (Fig. 7) and the self-information \u2212 log p(w) (Fig. 8).\n\u2022 We showed theoretically and empirically that the squared norm of word embedding obtained by the SGNS model approximates the information gain of a word defined by the KL divergence. Furthermore, we have extended this theory to encompass contextualized embeddings in language models.\n\u2022 We empirically showed that the bias-corrected KL divergence and the norm of word embedding are similarly good as a metric of word informativeness.\nAfter providing related work (Section 2) and theoretical background (Section 3), we prove the theoretical main results in Section 4. In Section 5, we extend this theory to contextualized embeddings. We then explain the word frequency bias (Section 6) and evaluate KL(w) and \u2225uw\u22252 as a metric of word informativeness in the experiments of Section 7."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Norm of word embedding",
            "text": "Several studies empirically suggest that the norm of word embedding encodes the word informativeness. According to the additive compositionality of word vectors (Mitchell and Lapata, 2010), the norm of word embedding is considered to represent the importance of the word in a sentence because longer vectors have a larger influence on the vector sum. Moreover, it has been shown in Yokoi et al. (2020) that good performance of word mover\u2019s distance is achieved in semantic textual similarity (STS) task when the word weights are set to the norm of word embedding, while the transport costs are set to the cosine similarity. Schakel and Wilson (2015) claimed that the norm of word embedding and the word frequency represent word significance and showed experimentally that proper nouns have embeddings with larger norms than function words. Also, it has been experimentally shown that the norm of word embedding is smaller for less informative tokens (Arefyev et al., 2018; Kobayashi et al., 2020)."
        },
        {
            "heading": "2.2 Metrics of word informativeness",
            "text": "Keyword extraction. Keywords are expected to have relatively large amounts of information. Keyword extraction algorithms often use a metric of the \u201cimportance of words in a document\u201d calculated by some methods, such as TF-IDF or word\nco-occurrence (Wartena et al., 2010). Matsuo and Ishizuka (2004) showed that the \u03c72 statistics computed from the word co-occurrence are useful for keyword extraction. The \u03c72 statistic is closely related to the KL divergence (Agresti, 2013) since \u03c72 statistic approximates the likelihood-ratio chisquared statistic G2 = 2nwKL(w) when each document is treated as a corpus.\nHypernym discrimination. The identification of hypernyms (superordinate words) and hyponyms (subordinate words) in word pairs, e.g., cat and munchkin, has been actively studied. Recent unsupervised hypernym discrimination methods are based on the idea that hyponyms are more informative than hypernyms and make discriminations by comparing a metric of the informativeness of words. Several metrics have been proposed, including the KL divergence of the co-occurrence distribution to the unigram distribution (Herbelot and Ganesalingam, 2013), the Shannon entropy (Shwartz et al., 2017), and the median entropy of context words (Santus et al., 2014).\nWord frequency bias. Word frequency is a strong baseline metric for unsupervised hypernym discrimination. Discriminations based on several unsupervised methods with good task performance are highly correlated with those based simply on word frequency (Bott et al., 2021). KL divergence achieved 80% precision but did not outperform the word frequency (Herbelot and Ganesalingam, 2013). WeedsPrec (Weeds et al., 2004) and SLQS Row (Shwartz et al., 2017) correlate strongly with frequency-based predictions, calling for the need to examine the frequency bias in these methods."
        },
        {
            "heading": "3 Theoretical background",
            "text": "In this section, we describe the KL divergence (Section 3.2), the probability model of SGNS (Section 3.3), and the exponential family of distributions (Section 3.4) that are the background of our theoretical argument in the next section."
        },
        {
            "heading": "3.1 Preliminary",
            "text": "Probability distributions. We denote the probability of a word w in the corpus as p(w) and the unigram distribution of the corpus as p(\u00b7). Also, we denote the conditional probability of a word w\u2032 co-occurring with w within a fixed-width window as p(w\u2032|w), and the co-occurrence distribution as\np(\u00b7|w). Since these are probability distributions,\u2211 w\u2208V p(w) = \u2211 w\u2032\u2208V p(w\n\u2032|w) = 1, where V is the vocabulary set of the corpus. The frequencyweighted average of p(\u00b7|w) is again the unigram distribution p(\u00b7), that is,\np(\u00b7) = \u2211 w\u2208V p(w)p(\u00b7|w). (1)\nEmbeddings. SGNS learns two different embeddings with dimensions d for each word in V : word embedding uw \u2208 Rd for w \u2208 V and context embedding vw\u2032 \u2208 Rd for w\u2032 \u2208 V . We denote the frequency-weighted averages of uw and vw\u2032 as\nu\u0304 = \u2211 w\u2208V p(w)uw, v\u0304 = \u2211 w\u2032\u2208V p(w\u2032)vw\u2032 . (2)\nWe also use the centered vectors\nu\u0302w := uw \u2212 u\u0304, v\u0302w\u2032 := vw\u2032 \u2212 v\u0304."
        },
        {
            "heading": "3.2 KL divergence measures information gain",
            "text": "The distributional semantics (Harris, 1954; Firth, 1957) suggests that \u201csimilar words will appear in similar contexts\u201d (Brunila and LaViolette, 2022). This implies that the conditional probability distribution p(\u00b7|w) represents the meaning of a word w. The difference between p(\u00b7|w) and the marginal distribution p(\u00b7) can therefore capture the additional information obtained by observing w in a corpus.\nA metric for such discrepancies of information is the KL divergence of p(\u00b7|w) to p(\u00b7), defined as\nKL(p(\u00b7|w) \u2225 p(\u00b7)) = \u2211 w\u2032\u2208V p(w\u2032|w) log p(w \u2032|w) p(w\u2032) .\nIn this paper, we denote it by KL(w) and call it the KL divergence of word w. Since p(\u00b7) is the prior distribution and p(\u00b7|w) is the posterior distribution given the word w, KL(w) can be interpreted as the information gain of word w (Oladyshkin and Nowak, 2019). Since the joint distribution of w\u2032 and w is p(w\u2032, w) = p(w\u2032|w)p(w), the expected value of KL(w) is expressed as\u2211\nw\u2208V p(w)KL(w)\n= \u2211 w\u2208V \u2211 w\u2032\u2208V p(w\u2032, w) log p(w\u2032, w) p(w\u2032)p(w) .\nThis is the mutual information I(W \u2032,W ) of the two random variablesW \u2032 andW that correspond to w\u2032 and w, respectively2. I(W \u2032,W ) is often called information gain in the literature.\n2In the following, w\u2032 and w represent W \u2032 and W by abuse of notation."
        },
        {
            "heading": "3.3 The probability model of SGNS",
            "text": "The SGNS training utilizes the Noise Contrastive Estimation (NCE) (Gutmann and Hyv\u00e4rinen, 2012) to distinguish between p(\u00b7|w) and the negative sampling distribution q(\u00b7) \u221d p(\u00b7)3/4. For each co-occurring word pair (w,w\u2032) in the corpus, \u03bd negative samples {w\u2032\u2032i }\u03bdi=1 are generated, and we aim to classify the \u03bd+1 samples {w\u2032, w\u2032\u20321 , . . . , w\u2032\u2032\u03bd} as either a positive sample generated from w\u2032 \u223c p(w\u2032|w) or a negative sample generated fromw\u2032\u2032 \u223c q(w\u2032\u2032). The objective of SGNS (Mikolov et al., 2013) involves computing the probability of w\u2032 being a positive sample using a kind of logistic regression model, which is expressed as follows (Gutmann and Hyv\u00e4rinen, 2012):\np(w\u2032|w) p(w\u2032|w) + \u03bdq(w\u2032) = 1 1 + e\u2212\u27e8uw,vw\u2032 \u27e9 . (3)\nTo gain a better understanding of this formula, we can cross-multiply both sides of (3) by the denominators:\np(w\u2032|w)(1 + e\u2212\u27e8uw,vw\u2032 \u27e9) = p(w\u2032|w) + \u03bdq(w\u2032),\nand rearrange it to obtain:\np(w\u2032|w) = \u03bdq(w\u2032)e\u27e8uw,vw\u2032 \u27e9. (4)\nWe assume that the co-occurrence distribution satisfies the probability model (4). This is achieved when the word embeddings {uw} and {vw\u2032} perfectly optimize the SGNS\u2019s objective, whereas it holds only approximately in reality."
        },
        {
            "heading": "3.4 Exponential family of distributions",
            "text": "We can generalize (4) by considering an instance of the exponential family of distributions (Lehmann and Casella, 1998; Barndorff-Nielsen, 2014; Efron, 2022), given by\np(w\u2032|u) := q(w\u2032) exp(\u27e8u, vw\u2032\u27e9 \u2212 \u03c8(u)), (5)\nwhere u \u2208 Rd is referred to as the natural parameter vector, vw\u2032 \u2208 Rd represents the sufficient statistics (treated as constant vectors here, while tunable parameters in SGNS model), and the normalizing function is defined as\n\u03c8(u) := log \u2211 w\u2032\u2208V q(w\u2032) exp(\u27e8u, vw\u2032\u27e9),\nensuring that \u2211\nw\u2032\u2208V p(w \u2032|u) = 1 for any u \u2208 Rd.\nThe SGNS model (4) is interpreted as a special case of the exponential family\np(w\u2032|w) = p(w\u2032|uw)\nfor u = uw with constraints \u03c8(uw) = \u2212 log \u03bd for w \u2208 V ; the model (5) is a curved exponential family when the parameter value u is constrained as \u03c8(u) = \u2212 log \u03bd, but we do not assume it in the following argument.\nThis section outlines some well-known basic properties of the exponential family of distributions, which have been established in the literature (Barndorff-Nielsen, 2014; Efron, 1978, 2022; Amari, 1982). For ease of reference, we provide the derivations of these basic properties in Appendix J.\nThe expectation and the covariance matrix of vw\u2032 with respect to w\u2032 \u223c p(w\u2032|u) are calculated as the first and second derivatives of \u03c8(u), respectively. Specifically, we have\n\u03b7(u) := \u2202\u03c8(u)\n\u2202u = \u2211 w\u2032\u2208V p(w\u2032|u)vw\u2032 , (6)\nG(u) := \u22022\u03c8(u)\n\u2202u\u2202u\u22a4 =\u2211\nw\u2032\u2208V p(w\u2032|u)(vw\u2032 \u2212 \u03b7(u))(vw\u2032 \u2212 \u03b7(u))\u22a4. (7)\nThe KL divergence of p(\u00b7|u1) to p(\u00b7|u2) for two parameter values u1, u2 \u2208 Rd is expressed as\nKL(p(\u00b7|u1) \u2225 p(\u00b7|u2)) = \u27e8u1 \u2212 u2, \u03b7(u1)\u27e9 \u2212 \u03c8(u1) + \u03c8(u2). (8)\nThe KL divergence is interpreted as the squared distance between two parameter values when they are not very far from each other. In fact, the KL divergence (8) is expressed approximately as\n2KL(p(\u00b7|u1) \u2225 p(\u00b7|u2)) \u2243 (u1 \u2212 u2)\u22a4G(ui) (u1 \u2212 u2) (9)\nfor i = 1, 2. Here, the equation holds approximately by ignoring higher order terms of O(\u2225u1 \u2212 u2\u22253). For more details, refer to Amari (1982, p. 369), Efron (2022, p. 35). More generally, G(u) is the Fisher information metric, and (9) holds for a wide class of probability models (Amari, 1998)."
        },
        {
            "heading": "4 Squared norm of word embedding approximates KL divergence",
            "text": "In this section, we theoretically explain the linear relationship between KL(w) and \u2225uw\u22252 observed in Fig. 1 by elaborating on additional details of the exponential family of distributions (Section 4.1) and experimentally confirm our theoretical results (Section 4.2)."
        },
        {
            "heading": "4.1 Derivation of theoretical results",
            "text": "We assume that the unigram distribution is represented by a parameter vector u0 \u2208 Rd and\np(w\u2032) = p(w\u2032|u0). (10)\nBy substituting u1 and u2 with uw and u0 respectively in (9), we obtain\n2KL(w) \u2243 (uw \u2212 u0)\u22a4G (uw \u2212 u0). (11)\nHere G := G(u0) is the covariance matrix of vw\u2032 with respect to w\u2032 \u223c p(w\u2032), and we can easily compute it from (7) as\nG = \u2211 w\u2032\u2208V p(w\u2032)(vw\u2032 \u2212 v\u0304)(vw\u2032 \u2212 v\u0304)\u22a4,\nbecause \u03b7(u0) = v\u0304 from (2) and (6). However, it is important to note that the value of u0 is not trained in practice, and thus we need an estimate of u0 to compute uw \u2212 u0 on the right-hand side of (11).\nWe argue that uw \u2212 u0 in (11) can be replaced by uw \u2212 u\u0304 = u\u0302w so that\n2KL(w) \u2243 u\u0302\u22a4wG u\u0302w. (12)\nFor a formal derivation of (12), see Appendix K. Intuitively speaking, u\u0304 approximates u0, because u\u0304 corresponds to p(\u00b7) in the sense that u\u0304 is the weighted average of uw as seen in (2), while p(\u00b7) is the weighted average of p(\u00b7|uw) as seen in (1).\nTo approximate u0, we could also use uw of some representative words instead of using u\u0304. We expect u0 to be very close to some uw of stopwords such as \u2018a\u2019 and \u2018the\u2019 since their p(\u00b7|uw) are expected to be very close to p(\u00b7).\nLet us define a linear transform of the centered embedding as\nu\u0303w := G 1 2 u\u0302w, (13)\ni.e., the whitening of uw with the context embedding3 , then (12) is now expressed4 as\n2KL(w) \u2243 \u2225u\u0303w\u22252. (14)\nTherefore, the square of the norm of the word embedding with the whitening-like transformation in (13) approximates the KL divergence.\n3Note that the usual whightening is Cov(u)\u2212 1 2 u\u0302w, but we call (13) as \u201cwhitening\u201d for convenience in this paper. 4(12) and (14) are equivalent, because \u2225u\u0303w\u22252 = u\u0303\u22a4w u\u0303w = (G 1 2 u\u0302w) \u22a4G 1 2 u\u0302w = u\u0302 \u22a4 wG 1 2 \u22a4G 1 2 u\u0302w = u\u0302 \u22a4 wGu\u0302w."
        },
        {
            "heading": "4.2 Experimental confirmation of theory",
            "text": "The theory explained so far was confirmed by an experiment on real data.\nSettings. We used the text8 corpus (Mahoney, 2011) with the size of N = 17.0\u00d7 106 tokens and |V | = 254 \u00d7 103 vocabulary words. We trained 300-dimensional word embeddings (uw)w\u2208V and (vw\u2032)w\u2032\u2208V by optimizing the objective of SGNS model (Mikolov et al., 2013). We also computed the KL divergence (KL(w))w\u2208V from the co-occurrence matrix. These embeddings and KL divergence are used throughout the paper. See Appendix A for the details of the settings.\nDetails of Fig. 1. First, look at the plot of KL(w) and \u2225uw\u22252 in Fig. 1 again. Although uw are raw word embeddings without the transformation (13), we confirm good linearity \u2225uw\u22252 \u221d KL(w). A regression line was fitted to words with nw > 103, where low-frequency words were not very stable and ignored. The coefficient of determination R2 = 0.831 indicates a very good fitting.\nAdequacy of theoretical assumptions. In Fig. 1, the minimum value of KL(w) is observed to be very close to zero. This indicates that p(\u00b7|w) for the most frequent w is very close to p(\u00b7) in the corpus, and that the assumption (10) in Section 4.1 is adequate.\nConfirmation of the theoretical results. To confirm the theory stated in (11), we thus estimated u0 as the frequency-weighted average of word vectors corresponding to the words {the, of, and}. These three words were selected as they are the top three words in the word frequency nw. Then the correctness of (11) was verified in Fig. 2, where the slope coefficient is much closer to 1 than 0.048 of Fig. 1. Similarly, the fitting in Fig. 3 confirmed the theory stated in (12) and (14), where we replaced u0 by u\u0304.\nExperiments on other embeddings. In Appendix G, the theory was verified by performing experiments using a larger corpus of Wikipedia dump (Wikimedia Foundation, 2021). In Appendix H, we also confirmed similar results using pre-trained fastText (Bojanowski et al., 2017) and SGNS (Li et al., 2017) embeddings."
        },
        {
            "heading": "5 Contextualized embeddings",
            "text": "The theory developed for static embeddings of the SGNS model is extended to contextualized embeddings in language models, or any neural networks with the softmax output layer."
        },
        {
            "heading": "5.1 Theory for language models",
            "text": "The final layer of language models with weights vw\u2032 \u2208 Rd and bias bw\u2032 \u2208 R is expressed for contextualized embedding u \u2208 Rd as\nyw\u2032 = \u27e8u, vw\u2032\u27e9+ bw\u2032 ,\nand the probability of choosing the word w\u2032 \u2208 V is calculated by the softmax function\npsoftmax(w \u2032|u) = e yw\u2032\u2211 w\u2208V e yw . (15)\nComparing (15) with (5), the final layer is actually interpreted as the exponential family of distributions with q(w\u2032) = ebw\u2032/ \u2211 w\u2208V e\nbw so that psoftmax(w\n\u2032|u) = p(w\u2032|u). Thus, the theory for SGNS based on the exponential family of distributions should hold for language models.\nHowever, we need the following modifications to interpret the theory. Rather than representing\nthe co-occurrence distribution, p(\u00b7|u) now signifies the word distribution at a specific token position provided with the contextualized embedding u. Instead of the frequency-weighted average u\u0304 =\u2211\nw\u2208V p(w)uw, we redefine u\u0304 := \u2211N\ni=1 ui/N as the average over the contextualized embeddings {ui}Ni=1 calculated from the training corpus of the language model. Here, ui denotes the contextualized embedding computed for the i-th token of the training set of size N . The information gain of contextualized embedding u is\nKL(u) := KL(p(\u00b7|u) \u2225 p(\u00b7)).\nWith these modifications, all the arguments presented in Sections 3.4 and 4.1, along with their respective proofs, remain applicable in the same manner (Appendix L), and we have the main result (14) extended to contextualized embeddings as\n2KL(u) \u2243 \u2225u\u0303\u22252, (16)\nwhere the contextualized version of the centering and whitening are expressed as u\u0302 := u \u2212 u\u0304 and u\u0303 := G 1 2 u\u0302, respectively."
        },
        {
            "heading": "5.2 Experimental confirmation of theory",
            "text": "We have tested four pre-trained language models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT-2 (Radford et al., 2019), and Llama 2 (Touvron et al., 2023) from Hugging Face transformers library (Wolf et al., 2020). Since the assumption (10) may not be appropriate for these models, we first computed u0 = argminu\u2208{u1,...,uN}KL(u), and used p(\u00b7|u0) as a substitute for p(\u00b7) when verifying the linear relationship between KL(u) and \u2225u \u2212 u0\u22252. Fig. 4 demonstrates that the linear relationship holds approximately for RoBERTa and Llama 2. All results,\nincluding those for BERT and GPT-2, as well as additional details, are described in Appendix I. While not as distinct as the result from SGNS in Fig. 1, it was observed that the theory suggested by (16) approximately holds true in the case of contextualized embeddings from language models."
        },
        {
            "heading": "6 Word frequency bias in KL divergence",
            "text": "The KL divergence is highly correlated with word frequency. In Fig. 5, \u2018raw\u2019 shows the plot of KL(w) against nw. The KL divergence tends to be larger for less frequent words. A part of this tendency represents the true relationship that rarer words are more informative and thus tend to shift the co-occurrence distribution from the corpus distribution. However, a large part of the tendency, particularly for low-frequency words, comes from the error caused by the finite size N of the corpus. This introduces a spurious relationship between KL(w) and nw, causing a direct influence of word frequency. The word informativeness can be better measured by using the KL divergence when this error is adequately corrected."
        },
        {
            "heading": "6.1 Estimation of word frequency bias",
            "text": "Preliminary. The word distributions p(\u00b7) and p(\u00b7|w) are calculated from a finite-length corpus. The observed probability of a word w is p(w) = nw/N , whereN = \u2211 w\u2208V nw. The observed probability of a context word w\u2032 co-occurring with w is p(w\u2032|w) = nw,w\u2032/ \u2211 w\u2032\u2032\u2208V nw,w\u2032\u2032 , where (nw,w\u2032)w,w\u2032\u2208V is the co-occurrence matrix. We computed nw,w\u2032 as the number of times that w\u2032 appears within a window of \u00b1h around w in the\ncorpus. Note that the denominator of p(w\u2032|w) is\u2211 w\u2032\u2032\u2208V nw,w\u2032\u2032 = 2hnw if the endpoints of the corpus are ignored.\nSampling error (\u2018shuffle\u2019). Now we explain how word frequency directly influences the KL divergence. Consider a randomly shuffled corpus, i.e., words are randomly reordered from the original corpus (Montemurro and Zanette, 2010; TanakaIshii, 2021). The unigram information, i.e., nw and p(\u00b7), remains unchanged after shuffling the corpus. On the other hand, the bigram information, i.e., nw,w\u2032 and p(\u00b7|w), computed for the shuffled corpus is independent of the co-occurrence of words in the original corpus. In the limit of N \u2192 \u221e, p(\u00b7|w) = p(\u00b7) holds and KL(w) = 0 for all w \u2208 V in the shuffled corpus. For finite corpus size N , however, p(\u00b7|w) deviates from p(\u00b7) because (nw,w\u2032)w\u2032\u2208V is approximately interpreted as a sample from the multinomial distribution with parameter p(\u00b7) and 2hnw.\nIn order to estimate the error caused by the direct influence of word frequency, we generated 10 sets of randomly shuffled corpus and computed the average of KL(w), denoted as KL(w), which is shown as \u2018shuffle\u2019 in Fig. 5. KL(w) does not convey the bigram information of the original corpus but does represent the sampling error of the multinomial distribution. For sufficiently large N , we expect KL(w) \u2248 0 for all w \u2208 V . However, KL(w) is very large for small nw in Fig. 5.\nSampling error (\u2018lower 3 percentile\u2019). Another computation of KL(w) faster than \u2018shuffle\u2019 was also attempted as indicated as \u2018lower 3 percentile\u2019 in Fig. 5. This represents the lower 3-percentile point of KL(w) in a narrow bin of word frequency nw. First, 200 bins were equally spaced on a logarithmic scale in the interval from 1 to max(nw). Next, each bin was checked in order of decreasing nw and merged so that each bin had at least 50 data points. This method allows for faster and more robust computation of KL(w) directly from KL(w) of the original corpus without the need for shuffling.\nQuantization error (\u2018round\u2019). There is another word frequency bias due to the fact that the cooccurrence matrix only takes integer values; it is indicated as \u2018round\u2019 in Fig. 5. This quantization error is included in the sampling error estimated by KL(w), so there is no need for further correction. See Appendix C for details."
        },
        {
            "heading": "6.2 Correcting word frequency bias",
            "text": "We simply subtracted KL(w) from KL(w). The sampling error KL(w) was estimated by either \u2018shuffle\u2019 or \u2018lower 3 percentile\u2019. We call\n\u2206KL(w) := KL(w)\u2212KL(w) (17)\nas the bias-corrected KL divergence. The same idea using the random word shuffling has been applied to an entropy-like word statistic in an existing study (Montemurro and Zanette, 2010)."
        },
        {
            "heading": "7 Experiments",
            "text": "In the experiments, we first confirmed that the KL divergence is indeed a good metric of the word informativeness (Section 7.1). Then we confirmed that the norm of word embedding encodes the word informativeness as well as the KL divergence (Section 7.2). Details of the experiments are given in Appenices D, E, and F.\nAs one of the baseline methods, we used the Shannon entropy of p(\u00b7|w), defined as\nH(w) = \u2212 \u2211 w\u2032\u2208V p(w\u2032|w) log p(w\u2032|w).\nIt also represents the information conveyed by w as explained in Appendix B."
        },
        {
            "heading": "7.1 KL divergence represents the word informativeness",
            "text": "Through keyword extraction tasks, we confirmed that the KL divergence is indeed a good metric of the word informativeness.\nSettings. We used 15 public datasets for keyword extraction for English documents. Treating each document as a \u201ccorpus\u201d, vocabulary words were ordered by a measure of informativeness, and Mean Reciprocal Rank (MRR) was computed as an evaluation metric. When a keyword consists of two or more words, the worst value of rank was used. We used specific metrics, namely \u2018random\u2019, nw, nwH(w) and nwKL(w), as our baselines. These metrics are computed only from each document without relying on external knowledge, such as a dictionary of stopwords or a set of other documents. For this reason, we did not use other metrics, such as TF-IDF, as our baselines. Note that \u2225uw\u22252 was not included in this experiment because embeddings cannot be trained from a very short \u201ccorpus\u201d.\nResults and discussions. Table 2 shows that nwKL(w) performed best in many datasets. Therefore, keywords tend to have a large value of nwKL(w), and thus p(\u00b7|w) is significantly different from p(\u00b7). This result verifies the idea that keywords have significantly large information gain."
        },
        {
            "heading": "7.2 Norm of word embedding encodes the word informativeness",
            "text": "We confirmed through proper-noun discrimination tasks (Section 7.2.1) and hypernym discrimination tasks (Section 7.2.2) that the norm of word embedding, as well as the KL divergence, encodes the word informativeness, and also confirmed that correcting the word frequency bias improves it.\nIn these experiments, we examined the properties of the raw word embedding uw instead of the whitening-like transformed word embedding u\u0303w. From a practical standpoint, we used uw, but experiments using u\u0303w exhibited a similar trend.\nCorrecting word frequency bias. In the same way as (17), we correct the bias of embedding norm and denote the bias-corrected squared norm as \u2206\u2225uw\u22252 := \u2225uw\u22252 \u2212 \u2225uw\u22252. We used the \u2018lower 3 percentile\u2019 method of Section 6.1 for \u2206\u2225uw\u22252, because the recomputation of embeddings for the shuffled corpus is prohibitive. Other bias-corrected quantities, such as \u2206KL(w) and \u2206H(w), were computed from 10 sets of randomly shuffled corpus."
        },
        {
            "heading": "7.2.1 Proper-noun discrimination",
            "text": "Settings. We used 10561 proper nouns, 123 function words, 4771 verbs, and 2695 adjectives that appeared in the text8 corpus not less than 10 times.\nWe used nw, H(w), KL(w), and \u2225uw\u22252 as a measure for discrimination. The performance of binary classification was evaluated by ROC-AUC.\nResults and discussions. Table 3 shows that \u2206KL(w) and \u2206\u2225uw\u22252 can discriminate proper nouns from other parts of speech more effectively than alternative measures. A larger value of \u2206KL(w) and \u2206\u2225uw\u22252 indicates that words appear in a more limited context. Fig. 6 illustrates that proper nouns tend to have larger \u2206KL(w) and \u2206\u2225uw\u22252 values when compared to verbs and function words."
        },
        {
            "heading": "7.2.2 Hypernym discrimination",
            "text": "Settings. We used English hypernym-hyponym pairs extracted from four benchmark datasets for hypernym discrimination: BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Lenci and Benotto, 2012), and Weeds (Weeds et al., 2014). Each dataset was divided into two parts by comparing nw of hypernym and hyponym to remove the effect of word frequency. In addition to \u2018random\u2019 and nw, we used WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), SLQS Row (Shwartz et al., 2017) and SLQS (Santus et al., 2014) as baselines.\nResults and discussions. Table 4 shows that \u2206\u2225uw\u22252 and \u2206KL(w) were the best and the second best, respectively, for predicting hypernym in hypernym-hyponym pairs. Correcting frequency bias remedies the difficulty of discrimination for the nhyper < nhypo part, resulting in an improvement in the average accuracy."
        },
        {
            "heading": "8 Conclusion",
            "text": "We showed theoretically and empirically that the KL divergence, i.e., the information gain of the word, is encoded in the norm of word embedding. The KL divergence and, thus, the norm of word embedding has the word frequency bias, which was corrected in the experiments. We then confirmed that the KL divergence and the norm of word embedding work as a metric of informativeness in NLP tasks.\nLimitations\n\u2022 The important limitation of the paper is that the theory assumes the skip-gram with negative sampling (SGNS) model for static word embeddings or the softmax function in the final layer of language models for contextualized word embeddings.\n\u2022 The theory also assumes that the model is trained perfectly, as mentioned in Section 3.3. When the assumption is violated, the theory may not hold. For example, the training is not perfect when the number of epochs is insufficient, as illustrated in Appendix G.\nEthics Statement\nThis study complies with the ACL Ethics Policy5."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Junya Honda and Yoichi Ishibashi for the discussion and the anonymous reviewers for their helpful advice. This study was partially supported by JSPS KAKENHI 22H05106, 23H03355, JST ACT-X JPMJAX200S, and JST CREST JPMJCR21N3."
        },
        {
            "heading": "A Settings for computation of word embeddings and KL divergence",
            "text": "Corpus. We used the text8 (Mahoney, 2011), which is an English corpus data with the size of N = 17.0\u00d7106 tokens and |V | = 254\u00d7103 vocabulary words. We used all the tokens6 separated by spaces for word embeddings and KL divergence.\nTraining of the SGNS model. Word embeddings were trained7 by optimizing the same objective function used in Mikolov et al. (2013). Parameters used to train SGNS are summarized in Table 5. The learning rate shown is the initial value, which we decreased linearly to the minimum value of 1.0\u00d7 10\u22124 during the learning process. The negative sampling distribution was specified as\nq(w) \u221d (nw) 3 4 .\nThe elements of uw were initialized by the uniform distribution over [\u22120.5, 0.5] divided by the dimensionality of the embedding, and the elements of vw were initialized by zero.\n6We manually checked that the words used in Table 1 and Table 8 were not personally identifiable or offensive.\n7We used AMD EPYC 7702 64-Core Processor (64 cores \u00d7 2). In this setting, the CPU time is estimated at about 12 hours.\nComputation of KL divergence. The value of KL(w) was computed from p(\u00b7|w) and p(\u00b7) using the definition in Section 3.2 with the convention that 0 log 0 = 0. The word probability p(w\u2032) and the co-occurrence probability p(w\u2032|w) were computed from the word frequency nw and the cooccurrence matrix (nw,w\u2032)w,w\u2032\u2208V , respectively, as described in Section 6. The co-occurrence matrix was computed with the window size h = 10.\nWord set for visualization. We have used 47\u00d7 103 words with nw \u2265 101 for the plots of Figs. 1 to 5. Except for Fig. 5, extreme points, up to 0.5% for each axis, were truncated to set the plot range. Word embeddings and KL divergence are not very stable for low-frequency words. For this reason, we used 1820 words with nw > 103 to fit the simple linear regression model using the least squares method."
        },
        {
            "heading": "B Other quantities of information theory",
            "text": "In addition to KL divergence, two other information theoretic quantities are discussed here.\nB.1 Shannon entropy The Shannon entropy of p(\u00b7|w), defined as\nH(w) = \u2212 \u2211 w\u2032\u2208V p(w\u2032|w) log p(w\u2032|w),\nalso represents information conveyed by w. In this paper, we call it the Shannon entropy of word w. H(w) is closely related to KL(w). The Shannon entropy of p(\u00b7|w) can be written as\nH(w) = log |V | \u2212KL(p(\u00b7|w) \u2225 unif(\u00b7)),\nmeaning that \u2212H(w) measures how much the cooccurrence distribution shifts from the uniform distribution (i.e., unif(w\u2032) = 1/|V |). Thus, H(w) and KL(w) have different reference distributions.\nB.2 Self-information A much naive way of measuring the information of a word is the self-information of the event that the word w is sampled from p(\u00b7), defined as\nI(w) = \u2212 log p(w).\nThe expected value \u2211\nw\u2208V p(w)I(w) is the Shannon entropy of p(\u00b7). Since p(w) was computed as p(w) = nw/N ,\nI(w) = logN \u2212 log nw\nactually looks at the word frequency nw in the log scale.\nB.3 Relation to word embedding H(w) and I(w) were computed with the same settings as in Section 4.2 and Appendix A. They were plotted along with \u2225uw\u22252 as shown in Fig. 7 and Fig. 8, respectively. Compared with KL(w), the relationships are less clear with R2 \u2248 0.4. From this experiment, we see that KL(w) better represents \u2225uw\u22252 than H(w) and I(w)."
        },
        {
            "heading": "C Quantization error",
            "text": "The co-occurrence matrix (nw,w\u2032)w,w\u2032\u2208V is sparse with many zero values at rows of w with small nw. The effect of quantization error caused by nw,w\u2032 taking only integer values cannot be ignored for low-frequency words. This effect is part of the sampling error, but we try to isolate the quantization error here. Let us redefine nw,w\u2032 := round(2hnwp(w\n\u2032)) and compute the KL divergence, denoted as KL0(w), which is shown\nas \u2018round\u2019 in Fig. 5. If there is no rounding errors, p(w\u2032|w) = p(w\u2032) so that KL0(w) = 0. In reality, however, KL0(w) is non-negligible for words with small nw, and this effect can be corrected by KL(w)\u2212KL0(w)."
        },
        {
            "heading": "D Details of experiment in Section 7.1",
            "text": "In this experiment, we confirmed that humanannotated keywords of documents were observed at the top of the ranking calculated by the discrepancy between p(\u00b7|w) and p(\u00b7).\nDatasets. For the experiment of keyword extraction, we used 15 datasets in English8. Each entry consists of a pair of document and gold keywords. Table 6 includes information on the size (the number of documents) and the type of documents.\nPreparation. Each document in the datasets was tokenized by NLTK\u2019s word_tokenize function. Then, each word was stemmed using NLTK\u2019s PorterStemmer, and all characters were converted to lowercase. The same preprocessing of stemming and lowercase was also applied to the gold keywords. However, we did not remove stopwords in preprocessing to see if the informativeness measures could remove unnecessary stopwords by themselves. The co-occurrence matrix for each document was computed with the window size h = 10. Note that only a subset V \u2032 \u2282 V of the vocabulary set described below was used for stable computation of p(w\u2032|w), w\u2032 \u2208 V \u2032, w \u2208 V . For constructing V \u2032, all the words w \u2208 V were sorted in decreasing order of nw, and the cumulative frequency ci = \u2211i j=1 nwj up to the i-th frequent word were computed for i = 1, 2, . . . , |V |. Then V \u2032 = {w1, . . . , wi} was defined with the smallest i such that ci \u2265 N/3.\nMethods. In each document, word ranking lists were created by sorting its vocabulary words using the informativeness measures. For \u2018random\u2019, the\n8Datasets for the keyword extraction experiment were obtained from a public repository https://github. com/LIAAD/KeywordExtractor-Datasets which includes Krapivin2009 (Krapivin et al., 2009), theses100 (Medelyan, 2015), fao780 and fao30 (Medelyan and Witten, 2008), SemEval2010 (Kim et al., 2010), Nguyen2007 (Nguyen and Kan, 2007), PubMed (Aronson et al., 2000), citeulike180 (Medelyan et al., 2009), wiki20 (Medelyan et al., 2008), Schutz2008 (Schutz, 2008), kdd (Gollapalli and Caragea, 2014), Inspec (Hulth, 2003), www (Gollapalli and Caragea, 2014), SemEval2017 (Augenstein et al., 2017), and KPCrowd (Marujo et al., 2011).\nranking list is simply a random shuffle of the vocabulary words. For nwH(w), words were ranked in increasing order. For other measures, words were ranked in decreasing order. We multiply nw to KL(w) because G2 = 2nwKL(w) is appropriate for testing the null hypothesis that p(\u00b7|w) = p(\u00b7). nwH(w) is also interpreted as a test statistic for testing the null hypothesis that p(\u00b7|w) = unif(\u00b7). We also included the \u03c72 statistic (Matsuo and Ishizuka, 2004), which is related to KL(w) as \u03c72 \u2248 G2 for sufficiently large nw.\nEvaluation metrics. We used MRR and P@5 as evaluation metrics for the keyword prediction task.\nMRR is the average of the reciprocals of gold keywords\u2019 ranks. The numbers in the tables were multiplied by 100. For each document, we used the best-ranked keyword, i.e., the minimum value of the ranks of correct answers. If a keyword is given as a phrase consisting of two or more words, the rank of the keyword is defined by the worstranked word. For example, the rank of \"New York\" is 10 if the ranks of \"new\" and \"york\" are 3 and 10, respectively.\nP@5 is the average percentage of correct answers that appear in the top five words of the ranked list. For each document, the number of gold keywords in the top five words was computed and divided by 5. For a keyword consisting of two or more words, it is regarded as a correct answer only when all the words are included in the top five words. Thus the percentage can be larger than 100\nif several gold keywords share the same words.\nResults. Table 6 shows MRR, and Table 7 shows P@5 of the experiment. Datasets were sorted in the increasing order of MRR of the random baseline in both tables. Table 2 in Section 7.1 is a summary of Table 6. Small values of MRR or P@5 of the random baseline indicate the extent of difficulty of the keyword extraction. Datasets with the article type are difficult, and the dataset with the news type is the easiest. In the difficult datasets, nwKL(w) performed best in almost all datasets."
        },
        {
            "heading": "E Details of experiment in Section 7.2.1",
            "text": "In this experiment, we confirmed that proper nouns tend to have larger values of \u2206KL(w) and \u2206\u2225uw\u2225 compared to other parts of speech.\nDatasets. We used 10561 proper nouns, 123 function words, 4771 verbs, and 2695 adjectives that appeared in the text8 corpus not less than 10 times (nw \u2265 10). The parts of speech of these words were identified by NLTK\u2019s POS tagger. Proper nouns are tagged as {NN, NNS}, verbs are tagged as {VB, VBD, VBG, VBN, VBP, VBZ}, adjectives are tagged as {JJ, JJS, JJR}, and function words are tagged as {IN, PRP, PRP$, WP, WP$, DT, PDT, WDT, CC, MD, RP}. Proper nouns were restricted to those found in the 61711 words of the English Proper nouns database9.\n9https://github.com/jxlwqq/ english-proper-nouns/\nPreparation. We computed nw, KL(w) and \u2225uw\u22252 from the text8 corpus as described in Appendix A. H(w) was also computed in the same way as KL(w). For their bias-corrected versions, we used the \u2018shuffle\u2019 method in Section 6.1 for \u2206KL(w) and \u2206H(w), and the \u2018lower 3 percentile\u2019 method for \u2206\u2225uw\u22252. We used these measures for the binary classification of part-of-speech.\nMethods. Proper nouns tend to have large values of nw, KL(w) and \u2225uw\u22252, or small values ofH(w) as seen in Fig. 9. Therefore, each word is classified as a proper noun if a measure is larger (or smaller) than a threshold value. We performed two sets of binary classification experiments: proper nouns vs. verbs, and proper nouns vs. adjectives.\nEvaluation metrics. Since the classification depends on the threshold value, we used ROC-AUC to evaluate the classification performance. ROCAUC was computed by Scipy\u2019s roc_curve function.\nResults. Table 3 in Section 7.2.1 shows the ROCAUC of the classification task, confirming the good performance of \u2206KL(w) and \u2206\u2225uw\u22252.\nTable 8 shows randomly sampled proper nouns with 101 \u2264 nw \u2264 103 and specific ranges of \u2206KL(w); since our experiment is case-insensitive, some selected words were actually considered as common nouns, such as storm and haven. We observed that common nouns tend to have small KL values. On the other hand, words with large KL values include context-specific nouns, such as company names, suggesting that they are more informative."
        },
        {
            "heading": "F Details of experiment in Section 7.2.2",
            "text": "In this experiment, we confirmed that \u2206KL(w) and \u2206\u2225uw\u22252 tend to have a smaller value for hypernym in hypernym-hyponym pairs.\nDatasets. Among the hypernym-hyponym pairs in each dataset, we used those consisting of words that appear in the text8 corpus. Specifically, we used 1336 pairs from the 1337 pairs of the BLESS dataset (Baroni and Lenci, 2011), 3635 pairs from the 3637 pairs of the EVALution dataset (Santus et al., 2015), 1760 pairs from the 1933 pairs of the Lenci/Benotto dataset (Lenci and Benotto,\n2012), 1427 pairs from the 1427 pairs of the Weeds dataset (Weeds et al., 2014). Each dataset was divided into two parts: the nhyper > nhypo part and the nhyper < nhypo part.\nPreparation. We computed nw, nw,w\u2032 , H(w), KL(w), \u2225uw\u22252, \u2206H(w), \u2206KL(w), and \u2206\u2225uw\u22252 from the text8 corpus as described in Appendices A and E.\nMethods. We considered the binary classification of hypernym given a hypernym-hyponym pair. Using KL(w), \u2225uw\u22252, \u2206KL(w), or \u2206\u2225uw\u22252 as a measure of informativeness, the word with a smaller value of the measure was predicted as hypernym.\nBaseline methods to predict hypernym given a word pair (w1, w2) are described below.\n\u2022 Random is the random classification. The accuracy is 50%.\n\u2022 Word Frequency chooses the word with larger nw as hypernym.\n\u2022 WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004) is based on the distributional inclusion hypothesis that the context of hyponym is included in the context of its hypernym. The weighted inclusion of word w2 in the context of word w1 is formulated as\nWeedsPrec(w1, w2) =\n\u2211 w\u2032\u2208Vw1\u2229w2\nnw1,w\u2032\u2211 w\u2032\u2208V nw1,w\u2032 ,\nwhere Vw1\u2229w2 = {w\u2032 \u2208 V | nw1,w\u2032 > 0 \u2227 nw2,w\u2032 > 0}. w1 is predicted as hypernym if\nWeedsPrec(w1, w2) <WeedsPrec(w2, w1).\n\u2022 SLQS Row (Shwartz et al., 2017) compares the Shannon entropy. w1 is predicted as hypernym if\nSLQSRow(w1, w2) := 1\u2212 H(w1)\nH(w2) < 0,\nor equivalently H(w1) > H(w2).\n\u2022 SLQS (Santus et al., 2014) compares the median entropy of context words defined as\nE(w) = Medianc\u2208CwH(c).\nw1 is predicted as hypernym if\nSLQS(w1, w2) := 1\u2212 E(w1)\nE(w2) < 0,\nor equivalently E(w1) > E(w2). Note that Cw is the set of most strongly associated context words of w, as determined by positive local mutual information (Evert, 2005). We used |Cw| = 50.\n\u2022 \u2206WeedsPrec is the bias-corrected version of WeedsPrec computed by the method in Section 6.2. WeedsPrec(w1, w2) is the average of WeedsPrec(w1, w2) for 10 randomly shuffled corpora, and \u2206WeedsPrec(w1, w2) =\nWeedsPrec(w1, w2) \u2212 WeedsPrec(w1, w2). w1 is predicted as hypernym if\n\u2206WeedsPrec(w1, w2)\n< \u2206WeedsPrec(w2, w1).\n\u2022 \u2206SLQS Row is the bias-corrected version of SLQS Row. w1 is predicted as hypernym if \u2206H(w1) > \u2206H(w2).\n\u2022 \u2206SLQS is the bias-corrected version of SLQS. w1 is predicted as hypernym if \u2206E(w1) > \u2206E(w2), where\n\u2206E(w) = Medianc\u2208Cw\u2206H(c).\nEvaluation metrics. The classification accuracy of each method was computed separately for the nhyper > nhypo part and for the nhyper < nhypo part of each dataset. Then, we calculated the unweighted average of the accuracy over the four datasets for each part and for both parts.\nResults. Table 9 shows the classification accuracy. Table 4 in Section 7.2.2 is a summary of Table 9. Looking at the overall accuracy, \u2206\u2225uw\u22252 and \u2206KL(w) were the best and the second best, respectively, for predicting hypernym in hypernymhyponym pairs."
        },
        {
            "heading": "G Results on Wikipedia dump",
            "text": "We used the Wikipedia dump (Wikimedia Foundation, 2021)10 with the size of N = 24.0 \u00d7 108 tokens and |V | = 645 \u00d7 104 vocabulary words, which was preprocessed by Wikiextractor (Attardi, 2015). The training of the SGNS model and the computation of KL divergence were performed as in Appendix A using the same setting11. For plotting the results, we used 50,000 words randomly sampled from the 1,114,207 vocabulary words with nw \u2265 10. For fitting the regression line, we used 2,662 words with nw > 103.\nFig. 10 shows the word embeddings of the Wikipedia dump computed with the same setting as that of the text8 corpus. The left panel of Fig. 10 is very similar to Fig. 1, confirming that the result for the text8 corpus is reproduced for the Wikipedia dump. The right panel of Fig. 10 corresponds to Fig. 8 with the axes exchanged and the log10 nw axis rescaled. Again, the two plots are very similar.\nHowever, the result changes when the epoch of training is reduced, thus the optimization is insufficient. Fig. 11 shows the word embeddings of the Wikipedia dump, but the epoch was reduced to 10.\n10Wikipedia dump dataset is licensed under the GFDL and the CC BY-SA 3.0.\n11We used AMD EPYC 7763 (64 cores). For 10 epochs of training, the CPU time is estimated at about 20 hours, and for 100 epochs of training, the CPU time is estimated at about 8 days.\nIn the left panel, the linear relationship was not reproduced. Looking at the right panel, the norm of embedding reduces for low-frequency words with nw < 100; plots of the same shape are also found in the literature (Schakel and Wilson, 2015; Arefyev et al., 2018; Pagliardini et al., 2018; Khodak et al., 2018). This is considered a consequence of insufficient optimization epochs; the norm of parameters tends to be smaller due to the implicit regularization (Arora et al., 2019), thus the trained parameters do not satisfy the ideal SGNS model (4) very well, particularly for low-frequency words."
        },
        {
            "heading": "H Results on pre-trained word embeddings",
            "text": "In this section, we show that the linear relationship between the KL divergence and the squared norm of word embedding holds also for pre-trained word embeddings.\nH.1 Pre-trained fastText embeddings\nWe used Wiki word vectors provided by Bojanowski et al. (2017). These 300-dimensional embeddings are trained for 5 epochs on Wikipedia with the fastText model. We used the same KL divergence as in Appendix G, which was calculated on the Wikipedia dump corpus. Results are shown in the left panel of Figure 12, where we randomly selected 10,000 words that appeared not less than 104 times in the Wikipedia dump.\nH.2 Pre-trained SGNS embeddings\nWe used pre-trained SGNS vectors provided by Li et al. (2017). These 500-dimensional embeddings are trained for 2 epochs on Wikipedia with the SGNS model. We used the same KL divergence as in Appendix G, which was calculated on the Wikipedia dump corpus. Results are shown in the right panel of Figure 12, where we randomly selected 10,000 words that appeared not less than 104 times in the Wikipedia dump."
        },
        {
            "heading": "I Results on contextualized embeddings",
            "text": "Settings. For the experiment of contextualized word embeddings, we used embeddings obtained from the final layer of BERT, RoBERTa, GPT-2, and Llama 2. We obtained 2000 sentences from One Billion Word Benchmark (Chelba et al., 2014) and input them into each language model to get contextualized embeddings of all tokens. Special\ntokens at the beginning and end of tokenized inputs, if any, were excluded.\nResults. Looking at the scatterplots in Fig. 13, approximate linear relationships can be observed in BERT, RoBERTa, and Llama 2, but in GPT-2, the linear relationship is somewhat weaker. According to the values in Table 10, whitening improves the linear relationship for GPT-2 and Llama 2, but it worsens for BERT and RoBERTa, and the effect of whitening is not clear-cut. While there is still room for discussion, overall, an approximate linear relationship between KL divergence and the squared norm of contextual embeddings appears to hold."
        },
        {
            "heading": "J Basic properties of the exponential family of distributions",
            "text": "The expectation and covariance matrix. The first and second derivatives of \u03c8(u) are computed as\n\u2202\u03c8(u)\n\u2202u = e\u2212\u03c8(u)\n\u2202\n\u2202u \u2211 w\u2032\u2208V q(w\u2032)e\u27e8u,vw\u2032 \u27e9\n= e\u2212\u03c8(u) \u2211 w\u2032\u2208V q(w\u2032)vw\u2032e \u27e8u,vw\u2032 \u27e9\n= \u2211 w\u2032\u2208V p(w\u2032|u)vw\u2032 ,\n\u22022\u03c8(u) \u2202u\u2202u\u22a4 = \u2202 \u2202u\n( e\u2212\u03c8(u) \u2211 w\u2032\u2208V q(w\u2032)vw\u2032e \u27e8u,vw\u2032 \u27e9 )\u22a4 = e\u2212\u03c8(u) \u2202\n\u2202u \u2211 w\u2032\u2208V q(w\u2032)v\u22a4w\u2032e \u27e8u,vw\u2032 \u27e9\n+ \u2202e\u2212\u03c8(u)\n\u2202u \u2211 w\u2032\u2208V q(w\u2032)v\u22a4w\u2032e \u27e8u,vw\u2032 \u27e9\n= e\u2212\u03c8(u) \u2211 w\u2032\u2208V q(w\u2032)vw\u2032v \u22a4 w\u2032e \u27e8u,vw\u2032 \u27e9\n\u2212 \u03c8(u) \u2202u e\u2212\u03c8(u) \u2211 w\u2032\u2208V q(w\u2032)v\u22a4w\u2032e \u27e8u,vw\u2032 \u27e9\n= \u2211 w\u2032\u2208V p(w\u2032|u)vw\u2032v\u22a4w\u2032 \u2212 \u03b7(u)\u03b7(u)\u22a4\n= \u2211 w\u2032\u2208V p(w\u2032|u)(vw\u2032 \u2212 \u03b7(u))(vw\u2032 \u2212 \u03b7(u))\u22a4,\nshowing (6) and (7), respectively.\nKL divergence. For computing the KL divergence, first note that\nlog p(w\u2032|u1) p(w\u2032|u2) = \u27e8u1 \u2212 u2, vw\u2032\u27e9 \u2212 \u03c8(u1) + \u03c8(u2)\nfrom (4). Thus, the KL divergence is\nKL(p(\u00b7|u1) \u2225 p(\u00b7|u2)) =\u2211 w\u2032\u2208V p(w\u2032|u1) ( \u27e8u1 \u2212 u2, vw\u2032\u27e9 \u2212 \u03c8(u1) + \u03c8(u2) ) = \u27e8u1 \u2212 u2, \u03b7(u1)\u27e9 \u2212 \u03c8(u1) + \u03c8(u2), (18)\nshowing (8).\nApproximation of KL divergence. Next, we consider the Taylor expansion of \u03c8(u) at u = u1. By ignoring higher order terms of O(\u2225u\u2212 u1\u22253), we have\n\u03c8(u) \u2243 \u03c8(u1) + \u2202\u03c8(u)\n\u2202u\u22a4 \u2223\u2223\u2223\u2223 u1 (u\u2212 u1)\n+ 1\n2 (u\u2212 u1)\u22a4\n\u22022\u03c8(u) \u2202u\u2202u\u22a4 \u2223\u2223\u2223\u2223 u1 (u\u2212 u1).\nUsing (6) and (7), we can rewrite this expression for u = u2 as\n\u03c8(u2) \u2243 \u03c8(u1) + \u27e8u2 \u2212 u1, \u03b7(u1)\u27e9\n+ 1\n2 (u2 \u2212 u1)\u22a4G(u1) (u2 \u2212 u1), (19)\nand substituting it into (18), we obtain\n2KL(p(\u00b7|u1) \u2225 p(\u00b7|u2)) \u2243 (u1 \u2212 u2)\u22a4G(u1) (u1 \u2212 u2), (20)\nshowing (9) for i = 1. Considering the Taylor expansion of G(u) at u = u2, each element of G(u1) is Gij(u1) = Gij(u2) + O(\u2225u1 \u2212 u2\u2225). Thus we can rewrite the right hand side of (20) as (u1 \u2212 u2)\u22a4(G(u2) +O(\u2225u1 \u2212 u2\u2225)) (u1 \u2212 u2) \u2243 (u1\u2212u2)\u22a4G(u2) (u1\u2212u2)+O(\u2225u1\u2212u2\u22253). Therefore, we have shown that (9) holds for both i = 1 and i = 2."
        },
        {
            "heading": "K High-dimensional random vectors",
            "text": "Random vector setting. In this section, we adopt a probabilistic viewpoint and treat the elements of vectors u and v as random variables denoted by ui and vi for i = 1, . . . , d to estimate the orders of magnitude of various quantities, such as vector norms. Although the embedding vectors {uw}w\u2208V , {vw\u2032}w\u2032\u2208V are not random variables, the random variable setting is justified when we randomly sample words w and w\u2032 from a large corpus and set u = uw and v = vw\u2032 . To simplify the analysis, we assume that the vector elements are distributed independently. While we could relax this assumption by imposing the spherical condition (Jung and Marron, 2009; Aoshima et al., 2018), we leave this extension for future work.\nWe aim to discuss the relative magnitudes of vectors, so rescaling the vectors does not affect the argument. Therefore, we assume that each element is proportional to d\u22121/2, i.e., ui = Op(d\u22121/2), vi = Op(d\n\u22121/2). The squared norm of u is \u2225u\u22252 =\u2211d i=1(u\ni)2 = Op(d \u00b7 (d\u22121/2)2) = Op(1), and the norm itself is also \u2225u\u2225 = (\u2225u\u22252)1/2 = Op(1). Here Op(1) means that the magnitude of the vector remains bounded even if the dimension d increases. The same applies to v, i.e., \u2225v\u2225 = Op(1). The inner product of u and v is also \u27e8u, v\u27e9 = \u2211d i=1 u\nivi = Op(d \u00b7 (d\u22121/2)2) = Op(1). Throughout this section, we consider magnitudes up to O(d\u22121) and ignore higher order terms of O(d\u22123/2) for sufficiently large d.\nInner product with centered vector. Each element of centered vector u \u2212 u\u0304 is ui \u2212 u\u0304i = Op(d \u22121/2), thus \u2225u \u2212 u\u0304\u22252 = \u2211d i=1(u i \u2212 u\u0304i)2 = Op(d \u00b7 (d\u22121/2)2) = Op(1). However, the inner product\n\u27e8u\u2212 u\u0304, v\u27e9 = Op(d\u22121/2), (21)\ni.e., it tends to zero as d \u2192 \u221e. Similarly, \u27e8u, v \u2212 v\u0304\u27e9 = Op(d\u22121/2). To show (21), note that E(ui \u2212 u\u0304i) = \u2211 w\u2208V p(w)(u i w \u2212 u\u0304iw) = 0. Thus, E((ui \u2212 u\u0304i)vi) = E(ui \u2212 u\u0304i)E(vi) = 0. The variance is E(((ui \u2212 u\u0304i)vi)2) = E((ui \u2212 u\u0304i)2)E((vi)2) = O(d\u22121 \u00b7 d\u22121) = O(d\u22122). Therefore, E(\u27e8u \u2212 u\u0304, v\u27e9) = 0, and E(\u27e8u \u2212 u\u0304, v\u27e92) = E( \u2211d i=1(u i \u2212 u\u0304i)vi)2) = \u2211d\ni=1 E(((ui \u2212 u\u0304i)vi)2)+ \u2211 i \u0338=j E((ui\u2212 u\u0304i)vi)E((uj \u2212 u\u0304j)vj) = O(d \u00b7 d\u22122) + 0 = O(d\u22121). This proves (21).\nu\u0304 approximates u0. Regarding v, we used only the property vi = Op(d\u22121/2) when deriving (21). So, the result does not change if we replace v by v \u2212 v\u0304: \u27e8u\u2212 u\u0304, v \u2212 v\u0304\u27e9 = Op(d\u22121/2). However, the result changes if we further replace u by u0:\n\u27e8u0 \u2212 u\u0304, v \u2212 v\u0304\u27e9 = Op(d\u22121), (22)\nmeaning that u\u0304 approximates u0. To show this, we first prepare another presentation of (5) as follows. Since p(w\u2032) = q(w\u2032) exp(\u27e8u0, vw\u2032\u27e9\u2212\u03c8(u0)), (5) is expressed as p(w\u2032|u) = p(w\u2032) exp(\u27e8u\u2212u0, vw\u2032\u27e9\u2212 \u03c8(u) + \u03c8(u0)) by canceling out q(w\u2032). We substitute \u03c8(u) by (19) with u1 = u0, u2 = u to obtain\np(w\u2032|u) \u2243 p(w\u2032) exp(\u27e8u\u2212 u0, vw\u2032 \u2212 v\u0304\u27e9 \u2212 12(u\u2212 u0) \u22a4G(u\u2212 u0)). (23)\nIn the above, \u27e8u\u2212 u0, vw\u2032 \u2212 v\u0304\u27e9 = Op(d\u22121/2), and (u \u2212 u0)\u22a4G(u \u2212 u0) = \u2211 w\u2032\u2208V (u \u2212 u0)\u22a4(vw\u2032 \u2212\nv\u0304)(vw\u2032 \u2212 v\u0304)\u22a4(u \u2212 u0)p(w\u2032) = \u2211\nw\u2032\u2208V \u27e8u \u2212 u0, vw\u2032 \u2212 v\u0304\u27e92p(w\u2032) = O(d\u22121), because \u27e8u \u2212 u0, vw\u2032 \u2212 v\u0304\u27e9 = Op(d\u22121/2).\nNext, we consider (1) and let p(w\u2032|w) = p(w\u2032|uw) with (23).\np(w\u2032) = \u2211 w\u2208V p(w\u2032|uw)p(w)\n\u2243 p(w\u2032) \u2211 w\u2208V exp [ \u27e8uw \u2212 u0, vw\u2032 \u2212 v\u0304\u27e9\n\u2212 12(uw \u2212 u0) \u22a4G(uw \u2212 u0)\n] p(w).\nThis holds for anyw\u2032, thus \u2211\nw\u2208V exp[\u00b7 \u00b7 \u00b7 ]p(w) \u2243 1. By considering the Taylor expansion of the summand above, we have exp[\u00b7 \u00b7 \u00b7 ] = 1 + \u27e8uw \u2212 u0, vw\u2032 \u2212 v\u0304\u27e9\u2212 12(uw\u2212u0)\n\u22a4G(uw\u2212u0)+ 12\u27e8uw\u2212 u0, vw\u2032 \u2212 v\u0304\u27e92 + Op(d\u22123/2). Therefore, by taking\nthe summation, we have\n\u27e8u\u0304\u2212 u0, v \u2212 v\u0304\u27e9 \u2212 12 \u2211 w\u2208V (uw \u2212 u0)\u22a4G(uw \u2212 u0)p(w)\n+ 12 \u2211 w\u2208V \u27e8uw \u2212 u0, v \u2212 v\u0304\u27e92p(w) \u2243 0, (24)\nwhere we have replaced vw\u2032 by v to clarify that w\u2032 is arbitrary. Here, \u2211 w\u2208V (uw \u2212 u0)\u22a4G(uw \u2212 u0)p(w) = Op(d \u22121) and \u2211 w\u2208V \u27e8uw \u2212 u0, v \u2212 v\u0304\u27e92p(w) = Op(d\u22121), thus we have proved (22). In addition to showing (22), we can also obtain an explicit formula for \u27e8u0 \u2212 u\u0304, v \u2212 v\u0304\u27e9. The second term in (24) is \u2211 w\u2208V (uw \u2212 u0) \u22a4G(uw \u2212 u0)p(w) = \u2211 w\u2208V trG(uw \u2212 u0)(uw \u2212 u0)\u22a4p(w) = trGH , where\nH := \u2211 w\u2208V p(w)(uw \u2212 u0)(uw \u2212 u0)\u22a4. (25)\nThe third term in (24) is \u2211\nw\u2208V \u27e8uw \u2212 u0, v \u2212 v\u0304\u27e92p(w) = \u2211 w\u2208V (v \u2212 v\u0304)\u22a4(uw \u2212 u0)(uw \u2212 u0) \u22a4(v \u2212 v\u0304) = (v \u2212 v\u0304)\u22a4H(v \u2212 v\u0304). Therefore, we obtain\n\u27e8u0 \u2212 u\u0304, v \u2212 v\u0304\u27e9 \u2243 12(v \u2212 v\u0304) \u22a4H(v \u2212 v\u0304)\n\u2212 12trGH. (26)\nInterstingly, (26) shows that all the context embeddings {vw\u2032}w\u2032\u2208V are constrained to a qudractic surface in Rd.\nProof of (12). First note that\n(u\u2212 u0)\u22a4G(u\u2212 u0) = (u\u2212 u\u0304+ u\u0304\u2212 u0)\u22a4G(u\u2212 u\u0304+ u\u0304\u2212 u0) = (u\u2212 u\u0304)\u22a4G(u\u2212 u\u0304) + (u\u0304\u2212 u0)\u22a4G(u\u0304\u2212 u0) + 2(u\u2212 u\u0304)\u22a4G(u\u0304\u2212 u0).\nUsing (22), the magnitude of the remaining terms is obtained as follows. (u\u0304 \u2212 u0)\u22a4G(u\u0304 \u2212 u0) = \u2211 w\u2032\u2208V (u\u0304\u2212u0)\u22a4(vw\u2032 \u2212 v\u0304)(vw\u2032 \u2212 v\u0304)\u22a4(u\u0304\u2212 u0)p(w \u2032) = \u2211 w\u2032\u2208V \u27e8u\u0304 \u2212 u0, vw\u2032 \u2212 v\u0304\u27e92p(w\u2032) = O((d\u22121)2) = O(d\u22122). Similarly, (u\u2212 u\u0304)\u22a4G(u\u0304\u2212 u0) = \u2211 w\u2032\u2208V (u\u2212 u\u0304)\u22a4(vw\u2032 \u2212 v\u0304)(vw\u2032 \u2212 v\u0304)\u22a4(u\u0304\u2212 u0)p(w \u2032) = \u2211 w\u2032\u2208V \u27e8u\u2212 u\u0304, vw\u2032 \u2212 v\u0304\u27e9\u27e8vw\u2032 \u2212 v\u0304, u\u0304\u2212 u0\u27e9p(w\u2032) = O(d\u22121/2 \u00b7 d\u22121) = O(d\u22123/2). Therefore, we have shown that\n(u\u2212 u0)\u22a4G(u\u2212 u0) = (u\u2212 u\u0304)\u22a4G(u\u2212 u\u0304) +Op(d\u22123/2),\nwhere the magnitude of (u\u2212 u0)\u22a4G(u\u2212 u0) and (u \u2212 u\u0304)\u22a4G(u \u2212 u\u0304) is Op(d\u22121), and u is arbitrary uw. Combining this with (11) proves (12)."
        },
        {
            "heading": "L Technical details of the contextualized embeddings",
            "text": "We need only the following additional modifications. The equation (1) for the unigram distribution p(w) is replaced by\np(\u00b7) = N\u2211 i=1 p(\u00b7|ui)/N.\nThe definition (25) for the matrixH in Appendix K is replaced by\nH := N\u2211 i=1 (ui \u2212 u0)(ui \u2212 u0)\u22a4/N.\nThese modifications simply replace the average weighted by word frequency p(w) on the vocabulary set V with the simple average over {ui}Ni=1. For a sufficiently large corpus size N of the training set, the distribution of {ui}Ni=1 is approximated by a density function \u03c0(u) of contextualized embedding u. Therefore, the simple average is interpreted as the expectation with respect to \u03c0(u). Consequently, we can also employ an alternate approach to the definition: u\u0304 = \u222b u\u03c0(u) du,\np(\u00b7) = \u222b p(\u00b7|u)\u03c0(u) du and H = \u222b (u \u2212 u0)(u \u2212 u0) \u22a4\u03c0(u) du."
        }
    ],
    "title": "Norm of Word Embedding Encodes Information Gain",
    "year": 2023
}