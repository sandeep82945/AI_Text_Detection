{
    "abstractText": "Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-ofthe-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vivek Gupta"
        },
        {
            "affiliations": [],
            "name": "Pranshu Kandoi"
        },
        {
            "affiliations": [],
            "name": "Mahek Bhavesh Vora"
        },
        {
            "affiliations": [],
            "name": "Shuo Zhang"
        },
        {
            "affiliations": [],
            "name": "Yujie He"
        },
        {
            "affiliations": [],
            "name": "Ridho Reinanda"
        },
        {
            "affiliations": [],
            "name": "Vivek Srikumar"
        }
    ],
    "id": "SP:d0cf5062cbb139214ee793e5394b9a5fcd8e9176",
    "references": [
        {
            "authors": [
                "Faheem Abbas",
                "M.K. Malik",
                "M. Rashid",
                "Rizwan Zafar."
            ],
            "title": "Wikiqa \u2014 a question answering system on wikipedia using freebase, dbpedia and infobox",
            "venue": "2016 Sixth International Conference on Innovative Computing Technology (INTECH), pages 185\u2013193.",
            "year": 2016
        },
        {
            "authors": [
                "Rami Aly",
                "Zhijiang Guo",
                "Michael Sejr Schlichtkrull",
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Oana Cocarascu",
                "Arpit Mittal"
            ],
            "title": "The fact extraction and VERification over unstructured and structured information",
            "year": 2021
        },
        {
            "authors": [
                "Muhao Chen",
                "Hongming Zhang",
                "Qiang Ning",
                "Manling Li",
                "Heng Ji",
                "Kathleen McKeown",
                "Dan Roth."
            ],
            "title": "Event-centric natural language understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Ming-Wei Chang",
                "Eva Schlinger",
                "William Yang Wang",
                "William W. Cohen."
            ],
            "title": "Open question answering over tables and text",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Jianshu Chen",
                "Yu Su",
                "Zhiyu Chen",
                "William Yang Wang."
            ],
            "title": "Logical natural language generation from open-domain tables",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7929\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang."
            ],
            "title": "Tabfact: A large-scale dataset for table-based fact verification",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xinyi Wang",
                "William Yang Wang."
            ],
            "title": "A dataset for answering time-sensitive questions",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hanwen Zha",
                "Zhiyu Chen",
                "Wenhan Xiong",
                "Hong Wang",
                "William Yang Wang."
            ],
            "title": "HybridQA: A dataset of multi-hop question answering over tabular and textual data",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Wenhu Chen",
                "Charese Smiley",
                "Sameena Shah",
                "Iana Borova",
                "Dylan Langdon",
                "Reema Moussa",
                "Matt Beane",
                "Ting-Hao Huang",
                "Bryan Routledge",
                "William Yang Wang."
            ],
            "title": "FinQA: A dataset of numerical reasoning over financial data",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Li Deng",
                "Shuo Zhang",
                "Krisztian Balog."
            ],
            "title": "Table2vec: Neural word and entity embeddings for table population and retrieval",
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2019
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Jeremy R. Cole",
                "Julian Martin Eisenschlos",
                "Daniel Gillick",
                "Jacob Eisenstein",
                "William W. Cohen."
            ],
            "title": "Time-aware language models as temporal knowledge bases",
            "venue": "Transactions of the Association for Computational Linguistics, 10:257\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Julian Eisenschlos",
                "Syrine Krichene",
                "Thomas M\u00fcller."
            ],
            "title": "Understanding tables with intermediate pre-training",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281\u2013296, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Michael Glass",
                "Mustafa Canim",
                "Alfio Gliozzo",
                "Saneem Chemmengath",
                "Vishwajeet Kumar",
                "Rishav Chakravarti",
                "Avi Sil",
                "Feifei Pan",
                "Samarth Bharadwaj",
                "Nicolas Rodolfo Fauceglia"
            ],
            "title": "Capturing row and column semantics in transformer based question",
            "year": 2021
        },
        {
            "authors": [
                "Vivek Gupta",
                "Maitrey Mehta",
                "Pegah Nokhiz",
                "Vivek Srikumar."
            ],
            "title": "INFOTABS: Inference on tables as semi-structured data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2309\u20132324, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Pawel Krzysztof Nowak",
                "Thomas M\u00fcller",
                "Francesco Piccinno",
                "Julian Eisenschlos."
            ],
            "title": "TaPas: Weakly supervised table parsing via pre-training",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Linmei Hu",
                "Zeyi Liu",
                "Ziwang Zhao",
                "Lei Hou",
                "Liqiang Nie",
                "Juanzi Li."
            ],
            "title": "A survey of knowledge enhanced pre-trained language models",
            "venue": "IEEE Transactions on Knowledge and Data Engineering.",
            "year": 2023
        },
        {
            "authors": [
                "Hiroshi Iida",
                "Dung Thai",
                "Varun Manjunatha",
                "Mohit Iyyer."
            ],
            "title": "TABBIE: Pretrained representations of tabular data",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Robert Iv",
                "Alexandre Passos",
                "Sameer Singh",
                "MingWei Chang."
            ],
            "title": "FRUIT: Faithfully reflecting updated information in text",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Mohit Iyyer",
                "Wen-tau Yih",
                "Ming-Wei Chang."
            ],
            "title": "Search-based neural structured learning for sequential question answering",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1821\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Zhen Jia",
                "Abdalghani Abujabal",
                "Rishiraj Saha Roy",
                "Jannik Str\u00f6tgen",
                "Gerhard Weikum."
            ],
            "title": "Tempquestions: A benchmark for temporal question answering",
            "venue": "Companion Proceedings of the The Web Conference 2018, WWW \u201918, page 1057\u20131062, Re-",
            "year": 2018
        },
        {
            "authors": [
                "Zhen Jia",
                "Abdalghani Abujabal",
                "Rishiraj Saha Roy",
                "Jannik Str\u00f6tgen",
                "Gerhard Weikum."
            ],
            "title": "TEQUILA: Temporal Question Answering over Knowledge Bases",
            "venue": "Proceedings of the 27th ACM International Conference on Information and Knowl-",
            "year": 2018
        },
        {
            "authors": [
                "Zhen Jia",
                "Soumajit Pramanik",
                "Rishiraj Saha Roy",
                "Gerhard Weikum."
            ],
            "title": "Complex temporal question answering on knowledge graphs",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 792\u2013802.",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Jia",
                "Soumajit Pramanik",
                "Rishiraj Saha Roy",
                "Gerhard Weikum."
            ],
            "title": "Complex Temporal Question Answering on Knowledge Graphs, page 792\u2013802",
            "venue": "Association for Computing Machinery, New York, NY, USA.",
            "year": 2021
        },
        {
            "authors": [
                "Nithish Kannen",
                "Udit Sharma",
                "Sumit Neelam",
                "Dinesh Khandelwal",
                "Shajith Ikbal",
                "Hima Karanam",
                "L Venkata Subramaniam"
            ],
            "title": "Targeted extraction of temporal facts from textual resources for improved temporal question answering over knowledge bases",
            "year": 2022
        },
        {
            "authors": [
                "Jayant Krishnamurthy",
                "Pradeep Dasigi",
                "Matt Gardner."
            ],
            "title": "Neural semantic parsing with type constraints for semi-structured tables",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516\u20131526, Copen-",
            "year": 2017
        },
        {
            "authors": [
                "Dibyakanti Kumar",
                "Vivek Gupta",
                "Soumya Sharma",
                "Shuo Zhang."
            ],
            "title": "Realistic data augmentation framework for enhancing tabular reasoning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4411\u20134429, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Tongliang Li",
                "Lei Fang",
                "Jian-Guang Lou",
                "Zhoujun Li."
            ],
            "title": "TWT: Table with written text for controlled data-to-text generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1244\u20131254, Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Xi Victoria Lin",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Bridging textual and tabular data for crossdomain text-to-SQL semantic parsing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870\u20134888, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Pan Lu",
                "Liang Qiu",
                "Kai-Wei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Tanmay Rajpurohit",
                "Peter Clark",
                "Ashwin Kalyan."
            ],
            "title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "venue": "The Eleventh International Conference",
            "year": 2023
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul",
                "Benjamin Bossan."
            ],
            "title": "Peft: State-of-the-art parameter-efficient finetuning methods",
            "venue": "https://github.com/huggingface/ peft.",
            "year": 2022
        },
        {
            "authors": [
                "Costas Mavromatis",
                "Prasanna Lakkur Subramanyam",
                "Vassilis N. Ioannidis",
                "Soji Adeshina",
                "Phillip R. Howard",
                "Tetiana Grinberg",
                "Nagib Hakim",
                "George Karypis"
            ],
            "title": "Tempoqr: Temporal question reasoning over knowledge graphs",
            "year": 2021
        },
        {
            "authors": [
                "Alvaro Morales",
                "Varot Premtoon",
                "Cordelia Avery",
                "Sue Felshin",
                "Boris Katz."
            ],
            "title": "Learning to answer questions from Wikipedia infoboxes",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1930\u20131935, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Julian Eisenschlos",
                "Syrine Krichene."
            ],
            "title": "TAPAS at SemEval-2021 task 9: Reasoning over tables with intermediate pre-training",
            "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 423\u2013430,",
            "year": 2021
        },
        {
            "authors": [
                "Hailey Schoelkopf",
                "Riley Kong",
                "Xiangru Tang",
                "Mutethia Mutuma",
                "Ben Rosand",
                "Isabel Trindade",
                "Renusree Bandaru",
                "Jacob Cunningham",
                "Caiming Xiong",
                "Dragomir Radev",
                "Dragomir Radev."
            ],
            "title": "FeTaQA: Free-form table question answering",
            "venue": "Trans-",
            "year": 2022
        },
        {
            "authors": [
                "Yasin Tarabar",
                "Ankit Gupta",
                "Tao Yu",
                "Yi Chern Tan",
                "Xi Victoria Lin",
                "Caiming Xiong",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "DART: Opendomain structured data record to text generation",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Sumit Neelam",
                "Udit Sharma",
                "Hima Karanam",
                "Shajith Ikbal",
                "Pavan Kapanipathi",
                "Ibrahim Abdelaziz",
                "Nandana Mihindukulasooriya",
                "Young-Suk Lee",
                "Santosh Srivastava",
                "Cezar Pendus"
            ],
            "title": "A benchmark for generalizable and interpretable temporal ques",
            "year": 2022
        },
        {
            "authors": [
                "J. Neeraja",
                "Vivek Gupta",
                "Vivek Srikumar."
            ],
            "title": "Incorporating external knowledge to enhance tabular reasoning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Ning",
                "Hao Wu",
                "Rujun Han",
                "Nanyun Peng",
                "Matt Gardner",
                "Dan Roth."
            ],
            "title": "TORQUE: A reading comprehension dataset of temporal ordering questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Qiang Ning",
                "Ben Zhou",
                "Zhili Feng",
                "Haoruo Peng",
                "Dan Roth."
            ],
            "title": "CogCompTime: A tool for understanding time in natural language",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstra-",
            "year": 2018
        },
        {
            "authors": [
                "Barlas Oguz",
                "Xilun Chen",
                "Vladimir Karpukhin",
                "Stan Peshterliev",
                "Dmytro Okhonko",
                "Michael Schlichtkrull",
                "Sonal Gupta",
                "Yashar Mehdad",
                "Scott Yih."
            ],
            "title": "Unified open-domain question answering with structured and unstructured knowledge",
            "venue": "arXiv preprint",
            "year": 2020
        },
        {
            "authors": [
                "Ankur Parikh",
                "Xuezhi Wang",
                "Sebastian Gehrmann",
                "Manaal Faruqui",
                "Bhuwan Dhingra",
                "Diyi Yang",
                "Dipanjan Das"
            ],
            "title": "ToTTo: A controlled table-to-text",
            "year": 2020
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
            "year": 2015
        },
        {
            "authors": [
                "Aniket Pramanick",
                "Indrajit Bhattacharya."
            ],
            "title": "Joint learning of representations for web-tables, entities and types using graph convolutional network",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Apoorv Saxena",
                "Soumen Chakrabarti",
                "Partha Talukdar."
            ],
            "title": "Question answering over temporal knowledge graphs",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Chao Shang",
                "Peng Qi",
                "Guangtao Wang",
                "Jing Huang",
                "Youzheng Wu",
                "Bowen Zhou."
            ],
            "title": "Open temporal relation extraction for question answering",
            "venue": "3rd Conference on Automated Knowledge Base Construction.",
            "year": 2021
        },
        {
            "authors": [
                "Tianze Shi",
                "Chen Zhao",
                "Jordan Boyd-Graber",
                "Hal Daum\u00e9 III",
                "Lillian Lee."
            ],
            "title": "On the potential of lexico-logical alignments for semantic parsing to SQL queries",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Huan Sun",
                "Hao Ma",
                "Xiaodong He",
                "Scott Wen-tau Yih",
                "Yu Su",
                "Xifeng Yan."
            ],
            "title": "Table cell search for question answering",
            "venue": "Proceedings of the companion publication of the 25th international conference on World Wide Web. ACM - Association for Comput-",
            "year": 2016
        },
        {
            "authors": [
                "Nancy X.R. Wang",
                "Diwakar Mahajan",
                "Marina Danilevsky",
                "Sara Rosenthal."
            ],
            "title": "SemEval2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TABFACTS)",
            "venue": "Proceedings of the 15th International",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Haoyang Wen",
                "Yanru Qu",
                "Heng Ji",
                "Qiang Ning",
                "Jiawei Han",
                "Avi Sil",
                "Hanghang Tong",
                "Dan Roth."
            ],
            "title": "Event time extraction and propagation via graph attention networks",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig",
                "Wen-tau Yih",
                "Sebastian Riedel."
            ],
            "title": "TaBERT: Pretraining for joint understanding of textual and tabular data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413\u20138426, On-",
            "year": 2020
        },
        {
            "authors": [
                "Ori Yoran",
                "Alon Talmor",
                "Jonathan Berant."
            ],
            "title": "Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills",
            "venue": "arXiv preprint arXiv:2107.07261. Version 1.",
            "year": 2021
        },
        {
            "authors": [
                "Tao Yu",
                "Chien-Sheng Wu",
                "Xi Victoria Lin",
                "bailin wang",
                "Yi Chern Tan",
                "Xinyi Yang",
                "Dragomir Radev",
                "richard socher",
                "Caiming Xiong"
            ],
            "title": "Gra{pp}a: Grammar-augmented pre-training for table semantic parsing",
            "venue": "In International Conference on Learning",
            "year": 2021
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman",
                "Zilin Zhang",
                "Dragomir Radev"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic pars",
            "year": 2018
        },
        {
            "authors": [
                "Vicky Zayats",
                "Kristina Toutanova",
                "Mari Ostendorf."
            ],
            "title": "Representations for question answering from documents with tables and text",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Hongzhi Zhang",
                "Yingyao Wang",
                "Sirui Wang",
                "Xuezhi Cao",
                "Fuzheng Zhang",
                "Zhongyuan Wang."
            ],
            "title": "Table fact verification with structure-aware transformer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Michael Zhang",
                "Eunsol Choi."
            ],
            "title": "SituatedQA: Incorporating extra-linguistic contexts into QA",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7371\u2013 7387, Online and Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Shuo Zhang",
                "Krisztian Balog."
            ],
            "title": "Autocompletion for data cells in relational tables",
            "venue": "Proceedings of the 28th ACM International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Zhang",
                "Krisztian Balog."
            ],
            "title": "Web table extraction, retrieval, and augmentation: A survey",
            "venue": "ACM Trans. Intell. Syst. Technol., 11(2):13:1\u201313:35.",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Zhang",
                "Zhuyun Dai",
                "Krisztian Balog",
                "Jamie Callan."
            ],
            "title": "Summarizing and exploring tabular data in conversational search",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "ERNIE: Enhanced language representation with informative entities",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "R. Socher."
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "venue": "ArXiv, abs/1709.00103.",
            "year": 2017
        },
        {
            "authors": [
                "Fengbin Zhu",
                "Wenqiang Lei",
                "Youcheng Huang",
                "Chao Wang",
                "Shuo Zhang",
                "Jiancheng Lv",
                "Fuli Feng",
                "TatSeng Chua."
            ],
            "title": "TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance",
            "venue": "Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Iv"
            ],
            "title": "Assess their performance",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Reasoning about temporal aspects of factual information presents a fundamental challenge for contemporary Natural Language Processing (NLP) systems. Factual information related to an entity often evolves over time, and understanding it requires understanding the scope of knowledge and temporal intervals. Furthermore, this factual information is also scattered across semi-structured data in several different forms. These forms include both implicit and explicit representations (see Figure 1 for an example). The wide prevalence of these characteristics creates major challenges for NLP models. It requires these models to effectively handle changes over time and extract valuable insights from time-dependent data.\nPrevious studies have primarily concentrated on question answering (Pasupat and Liang, 2015; Krishnamurthy et al., 2017) and inference (Gupta\n\u2217Work done during an internship at Bloomberg. \u2020Equal contributions. \u2021Corresponding authors.\net al., 2020; Chen et al., 2020b) concerning numerical aspects of semi-structured tables. They typically examine tables without time-related information and focus on queries with predominantly non-temporal contexts. Research on temporal aspects in entity-centric tables, such as Wikipedia Infoboxes, has been limited (Gupta et al., 2020; Neeraja et al., 2021; Kumar et al., 2022). Morales et al. (2016) introduced question answering in an entity-centric table context. However, the questions in the dataset are simple and non-temporal, and it\u2019s worth noting that the dataset is not open source. Existing studies have only considered a few temporal aspects and addressed a small number of\ntime-related factors. Advances in modeling techniques, including table pre-training and targeted fine-tuning, have substantially improved reasoning on semi-structured tables (M\u00fcller et al., 2021; Eisenschlos et al., 2020). Moreover, large language models (LLMs) have exhibited impressive performance across various domains, such as general, finance, and medical, demonstrating their mathematical, knowledge-based, and common-sense reasoning capabilities (Chen et al., 2021d; Aly et al., 2021; Wang et al., 2021; Lu et al., 2023). However, the effectiveness of these models in handling temporal aspects remains understudied. Consequently, this paper seeks to address the following research question: \u201cCan modern NLP models effectively reason about temporal information in semi-structured tables?\u201d\nTo effectively address the above question, we introduce a new task called temporal question answering on entity-centric semi-structured tables. Figure 1 shows an example. We curate a comprehensive (covering diverse domains), specialized (temporally aligned), and human-verified dataset, TEMPTABQA. It consists of 11,454 questionanswer pairs extracted from 1,208 Wikipedia Infobox tables across more than 90 domains. Both the tables and questions in TEMPTABQA are encompass numerous temporal terms. For example, the Figure 1 table involve multiple dates, age, years, and the corresponding questions also incorporate temporal terms, such as first and last, years (since 2013), ranking, and more. These enhanced tables incorporate temporal information, ensuring that all queries have time-related components. This dataset is the first to explore question-answering and temporal reasoning in semi-structured data.\nWe conduct analysis of temporal reasoning challenges in TEMPTABQA, offering both qualitative and quantitative insights. Most questions in TEMPTABQA are abstractive and necessitate mathematical calculations over temporal concepts to arrive at correct answers. The dataset also encompasses additional test sets to evaluate reasoning in rare domains. Our findings indicate that temporal reasoning in TEMPTABQA poses greater challenges compared to non-temporal reasoning in previous tabular datasets. Our assessment of contemporary NLP systems on the TEMPTABQA benchmark exposes their subpar performance in comparison to humans. Humans excel at temporal reasoning, delivering accurate answers and in-\ndepth explanations. In contrast, our error analysis shows that models frequently make mistakes, particularly when faced with complex temporal reasoning questions. Consequently, our dataset serves as a challenging testbed for investigating effective temporal reasoning within semi-structured information.\nOur paper marks a significant milestone by pioneering the creation of complex temporal question answering datasets, specifically tailored to entity-centric tables. Our primary objective was to introduce a novel challenge \u2013 addressing intricate temporal questions within this context. The TEMPTABQA dataset not only demands sophisticated reasoning but also necessitates a firm grasp of temporal common sense, adept handling of arithmetic and numerical aspects. Our work sheds light on the unique temporal specificity of this dataset, setting it apart from existing models. Furthermore, we delves deep into this differentiation, offering a comprehensive array of statistics and analyses that illuminate the multitude of temporal reasoning challenges posed by the dataset. The findings from above enhance our understanding of temporal reasoning in tables and encourage further research.\nThe TEMPTABQA dataset can be accessed at https://zenodo.org/records/10022927. For relevant analysis and modeling scripts refer at https:// temptabqa.github.io."
        },
        {
            "heading": "2 Motivation",
            "text": "Dynamic Nature of Information. Tables serve as structured representations that organize and record diverse information types, making them highly useful for studying an entity\u2019s timeline. They offer a comprehensive record of events, enabling clear visualization of the evolution of various aspects over time. By capturing a chronological sequence of events, tables allow us to analyze the progression of positions, changes in marital status, and the acquisition of awards, serving as reliable sources for temporal reasoning. Additionally, entity-centric tables, such as Wikipedia Infoboxes, significantly differ from both unstructured and fully structured data (SQL tables and KGs). These tables possess a semi-structured nature and store data in intricate implicit forms, as discussed in the context of semi-structured tables (Gupta et al., 2020).\nTables in the Real World. Complex temporal question answering applied to entity-centric semistructured tables, like Wikipedia Infoboxes, has\nbroad applicability across various fields. In historical research and education, it helps scholars, historians, and students extract precise historical details, while in financial analysis, it empowers analysts with historical financial data for informed investment decisions. In medical research, it aids in accessing historical medical data and clinical trial timelines, and legal professionals use it to review historical legal records. Journalists gain historical context, linguists analyze language dynamics, and businesses optimize supply chains through historical data. Environmental researchers, policy analysts, travelers, software developers, and archivists all benefit from this versatile tool. This underscores the significance of discovering valuable information within tables across a broad spectrum of diverse fields.\nWhy Temporal Questions Answering? Temporal questions require reasoning based on timerelated information, falling into two main categories: explicit and implicit: (a.) Explicit temporal questions directly involve time-specific details like dates, years, or hours, demanding precise knowledge. For example, in Figure 1, question such as \u2018When was the Nedelcheva\u2019s was born?\u2019 is an explicit temporal question. (b.) Implicit temporal questions rely on temporal terms and relationships that indicate the temporal order or context of events or entities. These questions may include terms like \"rank,\" \"before,\" \"after,\" \"predecessor,\" \"successor,\" and similar expressions. In such cases, the temporal dimension isn\u2019t explicitly stated (e.g., mention of year not in the table) but must be inferred (or extracted) from the given context with understanding of the relationships between elements. For instance, in Figure 1 \u2018How many Bronze medals did Nedelcheva\u2019s won before 2013?\u2019 assumes an implicit understanding of the temporal sequence. In essence, addressing temporal questions involves comprehending and manipulating time-related information, whether explicit or implicit. This skill is vital in domains spanning historical research to natural language understanding, enabling effective reasoning about temporal aspects within data.\nWhy a new Table QA dataset? Current datasets such WIKITABLEQUESTIONS (Pasupat and Liang, 2015), SQUALL (Shi et al., 2020), FINQA (Chen et al., 2021d), TAT-QA (Zhu et al., 2021), HYBRIDQA (Chen et al., 2020c), FETAQA (Nan et al.,\n2022), SequentialQA(SQA) (Iyyer et al., 2017), and WIKISQL (Zhong et al., 2017) for questionanswering on table are limited in terms of both quantity and complexity of temporal questions. Table 31 show broad comparison across several tabular datasets. They fail to cover crucial aspects of temporal reasoning, necessitating the creation of a new manually curated dataset that specifically focuses on answering temporal questions related to tables. Our dataset, TEMPTABQA, can serve help train and evaluate models, aiding the development of more accurate and robust systems capable of effectively reasoning with temporal information in table-based contexts.\n3 Our TEMPTABQA Dataset\nWe create a benchmark for answering temporal questions on entity-centric tables across domains."
        },
        {
            "heading": "3.1 Data Creation",
            "text": "Table Domains Selection. TEMPTABQA is built with Infobox tables from various Wikipedia articles across more than 90 categories. We focus on domains with time-related attributes in tables, particularly articles with Infoboxes containing temporal values. We analyze 1,208 Infobox templates1 to compile a varied domain list, prioritizing entity tables with numerous temporal values like dates and times. Our analysis indicates that longer tables from popular and highly viewed articles contain a higher amount of temporal information, including an increased presence of temporal terms. As a result, these longer tables are deemed more appropriate for inclusion in TEMPTABQA. 2\nAnnotation Procedure. To generate questionanswer pairs from selected tables, we engage Amazon Mechanical Turk crowd-workers. MTurk annotators draft both the temporal question and answer based on a provided table. To ensure clarity, we instruct annotators to write clear, unambiguous, pronoun-free questions with grammatically complete answers. We also direct them to avoid yes/no questions and trivial questions that do not require temporal reasoning. This annotation approach ensures that the questions are challenging enough to evaluate models. We advise annotators to avoid\n1https://en.wikipedia.org/wiki/Wikipedia: List_of_infoboxes\n2We extract tables using BeautifulSoup4 and the Wikipedia extraction API.\nrepeating question patterns and instead use different starting tokens like \"What,\" \"When,\" \"Whose,\" \"How,\" \"Where,\" \"Who,\" \"How many,\" etc., in order to prevent biased evaluation results. We instruct them to incorporate multiple unique rows from the table when formulating questions, including logical aspects that require reasoning to enhance complexity and variation. This evaluation approach ensures that the model\u2019s ability to reason across different parts of the table is assessed, and the questions are not overly basic. We encourage annotators to actively create unique questions, avoiding repetition and incorporating linguistic variation. This process fosters the generation of novel and interesting questions.\nTo answer the questions, we asked Turkers to provide brief responses in the form of phrases rather than full sentences. Additional information regarding Turking cost, annotator statistics, bonus and reward criteria, batch splitting, and other details are outlined in the appendix \u00a7D.\nDealing with Annotation Bias. Annotators may rely on personal biases, leading to position, selection, and popularity biases. They might also use repetitive question patterns and similar logical reasoning. To address these biases, we implemented measures such as: (1) Diverse Table Categories: Including tables from various categories in a single batch for diversity, with 12 distinct domains and no more than 3 tables from the same domain. (2) Removal of Popular Rows: Excluding frequent keys in entity tables, such as \"Year Active,\" \"Born,\" \"Died,\" etc. (3) Shuffling and Reordering: Addressing position bias by shuffling table content and reordering subheadings like tournament titles, medal tallies, and awards. (4) Mitigating Selection Bias: Lessening selection bias by removing popular subsections, such as the \"Olympics\" section from an athlete table.\n3.2 TEMPTABQA Statistics and Analysis Dataset. Table 1 presents key metrics, including average row count, total unique tables, total questions, and average questions per table. We provide two test sets instead of one: the Head set with popular frequent domains, and the Tail set with rarer domains. Data split for train, development, head, and tail test sets are shown in Table 2.\nQuestions. Table 3 describes the composition and complexity of questions in our dataset. It presents the percentage of simple and complex\nquestions, taking into account multiple time-frame reasoning, the presence of single or multiple entities, and the inclusion of mathematical operations on temporal aspects. A question is deemed complex if it involves at least two simultaneous temporal reasoning steps from the categories of before-related, after-related, and inbetween/during-related. Further details regarding these analyses, including mathematical operations such as min, max, count, average, difference, and comparison, can be found in Table 5.\nWe examine the required temporal intervals, including before, after, and present, as shown in Table 4. To categorize questions as current, we use keywords such as \"until\", \"in\", \"during\", \"while\", \"at the same time\", \"meanwhile,\" \"when\", \"since\", and soon. Past questions contained keywords such as \"before\", \"previous to\", \"prior to\", \"preceding\", and soon., and future questions contained keywords such as \"after\", \"following\", \"successor\", \"followed by\", and soon.\nIn addition, Table 4 also distinguishes between explicit and implicit temporal questions. Explicit questions mention a specific time or date, while implicit questions do not mention explicitly such temporal references. We also identified questions that used ordinal words or implied ranking or counting operations.\nAnswers. Table 6 breaks down answer types by counting examples whose answers are for several entity types: money, person, organization, location, percentage, and product.\nFurthermore, we also evaluates answer complexity based on types such as count cardinal, ranking ordinal, boolean (Yes/No), temporal (Date/Time/Year), and age-related terms.\n3.3 TEMPTABQA Dataset Validation\nTo ensure answer correctness in TEMPTABQA, we validate the development and two test sets by assigning three annotators to answer the questions based on the table. Annotators are instructed to provide concise yet comprehensive explanations to ensure accuracy and logical reasoning. The given instructions to annotators are as follows: (a.) Use\nTable Information Only: Annotators are instructed to rely solely on the table information, avoiding external knowledge except for common sense reasoning. (b.) Clear, Concise, and Unambiguous Answers: Annotators are asked to clear, concise, complete, and unambiguous answers and explanations, ensuring accuracy and clarity in the validation process. (c.) Avoid Opinions or Assumptions: To maintain objectivity and accuracy, annotators are instructed to refrain from including personal opinions or assumptions in their explanations. (d.) Exclude Acronyms or Abbreviations: To ensure clarity and avoid confusion, annotators are instructed to avoid using acronyms or abbreviations in their explanations. (e.) Current Date and Year: During the annotation process, we instructed annotators to consider December 2022 as the current month when answering questions that involve the present moment.\nTEMPTABQA Filtering. We use pre-processing to refine our training set, removing non-temporal and basic questions. The test and development sets undergo manual reviews by NLP experts after initial script-based filtering to maintain quality, focusing on complex temporal queries. We correct errors and prioritize questions that demand advanced reasoning, excluding those with direct answers or requiring external knowledge. Annotators were instructed to provide clear answers. However, some answers varied in format like \"365 days\" versus \"one year\". We made sure evaluation didn\u2019t penalize format differences, applying regex rules validated by human checks. For more details on filter refer to the appendix \u00a7D.\nIn the development set, less than 10% of questions, under 7% in the Head set, and under 11% in the Tail set were ambiguous, as shown in Table 7. Under 3% of questions were subjective. The most errors came from complex reasoning, whereas datetime errors were typically a year off. Around 82% of the annotated questions reach a clear majority consensus, demonstrating high agreement among annotators. For non-consensus questions, another review boosted agreement by 8-10%. By comparing the majority and gold answers, human accuracy was found to be 86%, as detailed in Table 7. See appendix \u00a7D, table 30 for fine-grained agreement."
        },
        {
            "heading": "4 Experimental Evaluation",
            "text": "We address the following research questions through our experiments: (a.) Is the new dataset TEMPTABQA challenging for existing models? (b.) Does finetuning on TEMPTABQA enhance model performance? (c.) Does providing few-shot examples and chain of thought reasoning benefit them? (d.) Is the performance on the tail domains worse than on the head domains?\nEvaluation: We use the following metrics to evaluate the models: F1 score (F1), Exact Match (EM), Rouge 1 (R1) and 2 (R2), and Meteor (MET). For evaluation purposes, we treat December 2022 as the current month and year. To ensure models are aware of this, in all experiments, we add a new table row \u2018Current Date: December, 2022\u2019.\nModels for Comparison. Since most of the questions in TEMPTABQA require temporal and numerical reasoning to answer and are abstractive in nature, we mostly use decoder-only models (except for BART which are encoder-decoder models). We consider the following models: (a.) Finetuned model: BART-Large, T5-XL, and Flan-T5XL, along with smaller versions, all fine-tuned on TEMPTABQA. (b.) Zero-shot LLM: T5-XXL, Flan-T5-XXL, LLaMA-2, GPT-3.5 and 4, and PaLM along with smaller versions, without finetuning. (c.) Few-shot LLM: Same models as with zero-shot but in few shot settings with three reference examples. (d.) Few-shot LLM with Chain of Thoughts: Similar to the few-shot setup, but with chain-of-thought (Wei et al., 2022) reasoning included with examples. 3\nFor additional details on the these models, including hyperparameter information, please refer to the appendix \u00a7C."
        },
        {
            "heading": "4.1 Our Findings: Results and Analysis",
            "text": "Table 8, 9, 10, 11 show the zero-shot, fine tuned, few-shot w/o chain of thoughts and few-shot with chain of thoughts prompting models performance.\nTEMPTABQA is Challenging. The dataset presents an challenging task, with all models performing significantly worse than the human, refer to Table 8, 9, 10, 11. Even our top-performing model, GPT-4 with Chain of Thought prompting, lags behind humans by 13.19 and 20.61 F1 points on the Head and Tail sets, respectively. Additionally, our best fine-tuned model, Flan-T5-XL, trails even further behind, with a margin of 31.75 and 32.58 F1 points on the Head and Tail sets. 4Furthermore, the GPT model consistently outperforms other models, such as Flan-T5 and T5, in both zero-shot and few-shot settings. Turning tables into knowledge graphs (+KG) 5 results in the model\u2019s superior performance compared to conventional linearization methods.\nFine-tuning Helps. Our findings, in Table 9, highlight the significant advantages of fine-tuning medium-scale models. Remarkably, fine-tuned Flan-T5-XL models outperform the non-fine-tuned\n3We didn\u2019t include TabT5 due to proprietary industry restrictions (not published in the open source dataset).\n4Due to resource limitations we didn\u2019t fine-tuning models larger than XL size.\n5We use GPT-4 with human in the loop to convert table to Knowledge Graph."
        },
        {
            "heading": "Model Size F1 EM R1 R2 MET",
            "text": "Head Domain\nT5 L 35.51 33.93 35.73 35.67 23.97 XL 35.51 33.93 35.73 35.67 27.07 XXL 38.08 36.77 38.08 38.05 25.86 Flan-T5 L 33.81 32.04 33.91 33.87 22.43 XL 41.80 40.72 41.83 41.8 27.17 XXL 43.29 41.87 43.41 43.40 27.78 LLaMA 2 47.90 40.73 48.36 48.28 33.62 GPT 3.5 53.38 49.03 53.64 53.56 39.094 69.97 65.17 70.24 70.22 50.33 +KG 4 72.24 68.02 72.98 72.86 52.10 PaLM 2 69.05 66.82 69.00 68.91 42.32 Human 87.49 86.17 87.61 87.61 58.87\nTail Domain\nFlan-T5-XXL model, which is even larger in size, in various few-shot scenarios, including chain-ofthought prompting, by impressive margins of 13.79 and 17.18 F1 points. However, when compared to the GPT models, particularly in few-shot scenarios with chain-of-thought prompting, the fine-tuned models fall short by 18.56 and 11.97 on the Head and Tail sets respectively."
        },
        {
            "heading": "Tail Domain",
            "text": ""
        },
        {
            "heading": "Model Size F1 EM R1 R2 MET Head Domain",
            "text": "Few-shot (w CoT) > few-shot (w/o CoT) > zeroshot). Tables 10 and 11 shows that few-shot models outperform their zero-shot counterparts. For instance, GPT-4 shows a gain of 2.0 and 2.23 F1 points on the Head and Tail sets, respectively, in the few-shot version compared to the zero-shot version. This trend is consistent across models like Flan-T5 and T5, regardless of model size. Notably, larger model sizes (L to XL to XXL) yield improved performance. Furthermore, incorporating chain-of-thought prompting provides an additional boost to the model\u2019s performance. Furthermore, linearization outperforms knowledge graphs.\nHead vs.Tail domain. Our observations reveal that the tail set posed greater challenges for all models across various settings, while humans achieved similar performance on both sets. Models face greater challenges with tail tables in contrast to head tables. For instance, even the top-performing model, GPT-4, showed a difference of around 9.20 F1 points, performing better on the Head set in zero-shot scenarios. However, this performance gap diminished with few-shot learning and chainof-thought reasoning. In few-shot scenarios with chain-of-thought prompting, the gap reduced to 7.09 F1 points This phenomenon mainly results from knowledge transfer between less common and widely recognized sports tables. The head tables exhibit many common attributes and pose similar types of questions, in contrast to the rare tables"
        },
        {
            "heading": "5 Analysis Breakdown of Performance",
            "text": "In our analysis, we examine the results (exact match) of our best model, GPT-4 few-shot with chain of thought, alongside human performance.\nQuestion Types. We categorize questions based on their types: starting with \"what,\" \"where,\" \"when,\" \"how,\" or \"quantity\" (also known as \"how many\"). The evaluation of the GPT-4 model\u2019s performance (exact match) compared to humans is presented in Table 12.\nAnalysis. Humans consistently outperform the model in all scenarios, with a notable performance disparity in the tail domain. The model demonstrates relatively stronger performance in answering \"Where\" and \"How Much\" questions compared to other types. However, it faces challenges in tackling \"What,\" \"Who,\" and \"When\" questions, resulting in lower performance. We observe that humans handle \"Where\" questions with the least difficulty and struggle the most with \"How Many\" questions. Conversely, the model encounters significant challenges with \"Who\" questions and performs relatively better with \"Where\" question types.\nReasoning Operation. To answer the questions, various analytical reasoning operations are involved, such as maximum, minimum, counting, summation, average, difference, and comparison. Table 13 provides a evaluation of the GPT-4 model\u2019s performance (exact match) compared to human performance, focusing on these operations.\nAnalysis. Once again, it is evident that humans consistently outperform the model in all types of operations, particularly in challenging tasks. Furthermore, our observations reveal that the model demonstrates relatively stronger performance in analytical reasoning tasks like \"maximum\" and \"counting\" compared to other types of tasks. However, it faces significant challenges in tasks such as \"minimum,\" \"difference,\" and \"comparison,\" resulting in lower performance levels. Overall, both humans and the model excel in \"maximum\" tasks while struggling with \"difference\" and \"summation\" tasks. Additionally, the model\u2019s performance in \"minimum\" and \"comparison\" tasks falls short compared to human performance, indicating its limitations in these areas.\nExplicit or Implicit. Our analysis compares the performance of humans and the best model in answering explicit and implicit time-related questions. Explicit questions directly mention time and can be found in the table, while implicit questions require inferring the time from the table information. Table 15 showcases the model\u2019s performance on both question types.\nAnalysis. The model demonstrates better performance in implicit temporal reasoning compared to explicit temporal reasoning. As earlier model struggles more with rare and infrequent questions in the tail domain. Implicit temporal reasoning questions\nare more prevalent, with a greater performance difference between the two types observed in the tail set. Notably, humans also struggle more with explicit questions compared to implicit ones, likely due to increased complexity and advanced mathematical reasoning requirements. Explicit questions demand deeper understanding and precise reasoning, explicitly stating specific temporal information, while implicit questions rely more on contextual reasoning and inference, allowing the model to leverage broader table information.\nAnswer Types. We analyze the entity or common noun type of the answer. Answer categories include age (gap or sum), count, monetary terms, ordinal numbers, organization names, percentages, person names, place names, product specifics, temporal entities (date, time, day), Boolean (yes/no, true/false, comparison), or unknown (not any specific type). Table 15 presents the model\u2019s performance based on the type of answer entity.\nAnalysis. Our analysis reveals that the model struggles with calculating age gaps, boolean, place, and person-related questions, in contrast to countrelated questions. Similar to previous findings, both the model and humans perform better on frequent head domain tables compared to tail domain tables. However, regardless of table type, both humans and the model encounter difficulties with percentages and ordinals. The model\u2019s performance is notably weaker in age gap calculations, boolean, place, and person-related questions, while exhibiting better performance in count-related questions. Additionally, both humans and the model face challenges with percentages and ordinals across table domains. For GPT-3.5 analysis, refer to appendix \u00a7A. Category-specific analysis based on table domain is in appendix \u00a7B."
        },
        {
            "heading": "6 Comparison with Related Work",
            "text": "Tabular Datasets and Models. Recent studies have explored various NLP tasks on semistructured tabular data, including tabular natural language inference, fact verification (Chen et al., 2020b; Gupta et al., 2020; Zhang and Balog, 2019), question answering, semantic parsing (Zhang and Balog, 2020; Zhang et al., 2020b; Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020c; Lin et al., 2020; Zayats et al., 2021; Oguz et al., 2020; Chen et al., 2021b; Iyyer et al., 2017), and table-totext generation (Parikh et al., 2020; Li et al., 2021; Nan et al., 2021; Yoran et al., 2021; Chen et al., 2020a).\nVarious strategies have been proposed to represent Wikipedia relational tables, including Table2vec (Deng et al., 2019), TAPAS (Herzig et al., 2020), TaBERT (Yin et al., 2020), TabStruc (Zhang et al., 2020a), TABBIE (Iida et al., 2021), TabGCN (Pramanick and Bhattacharya, 2021), and RCI (Glass et al., 2021). Pre-training methods have also been studied to improve tabular inference (Yu et al., 2018, 2021; Eisenschlos et al., 2020; Neeraja et al., 2021). Recent shared tasks like SemEval\u201921 Task 9 (Wang et al., 2021) and FEVEROUS\u201921 shared task (Aly et al., 2021) have further explored these areas.\nIn comparision to prior work, TEMPTABQA centers on temporal question answering within entitycentric tables, an untapped domain. While most datasets lean towards non-temporal queries, they seldom address temporal aspects and lack a grounding in the common sense and the necessary world knowledge. These datasets predominantly emphasize arithmetic reasoning using SQL in structured formats, overlooking the nuanced semi-structured Infobox-style tables rich in common sense.\nTemporal Datasets and Models. Several temporal question answering datasets have been introduced recently. These include TIME-SENSITIVEQA (Chen et al., 2021c) and TORQUE (Ning et al., 2020), which are entity-specific reading comprehension datasets with time-sensitive questions derived from Wikipedia paragraphs. TEMPQA-WD (Neelam et al., 2022), CRONQUESTIONS (Saxena et al., 2021), and TEMPQUESTIONS (Jia et al., 2018a) are question answering datasets focusing on knowledge graph embeddings with temporal links. Additionally, there are open-domain (Zhang and\nChoi, 2021) and cloze-form (Dhingra et al., 2022) question answering tasks, as well as event-centric datasets (Ning et al., 2018; Wen et al., 2021; Chen et al., 2021a) that explore temporal QA.\nIn terms of modeling, there are temporally tuned language models trained on knowledge-based question answering datasets such as CRONKBQA (Saxena et al., 2021), TEQUILA (Jia et al., 2018b), EXAQT (Jia et al., 2021a), OTR-QA (Shang et al., 2021), and TEMPOQR (Mavromatis et al., 2021), among others. (Kannen et al., 2022) suggest a targeted approach to extract temporal facts when traditional KBQA methods fail to retrieve them from the knowledge base. Some methods incorporate temporal aspects during masked language model pre-training (Dhingra et al., 2022; Iv et al., 2022), rather than fine-tuning on downstream NLI tasks. In comparison to prior work, TEMPTABQA focuses on temporal question answering specifically on entity-centric tables, while most existing studies address non-tabular datasets."
        },
        {
            "heading": "7 Conclusion",
            "text": "In conclusion, this study addresses the effectiveness of current NLP systems in reasoning about temporal information in semi-structured data, specifically Infobox tables. We introduce the task of temporal question answering on semi-structured tables and present the TEMPTABQA dataset, consisting of 11,454 question-answer pairs from 1,208 Wikipedia Infobox tables across varied domains. Evaluating state-of-the-art models on this dataset reveals significant gaps compared to human performance, exceeding 13.5 F1 points. These findings emphasize the need for advancements in temporal reasoning capabilities of NLP models. The TEMPTABQA dataset serves as a challenging benchmark to enhance temporal reasoning in NLP models.\nFuture Directions. From our analysis, we suggest future avenues in temporal query answering: (a) Diverse Structures: Expand temporal queries to various table structures, like hybrid compositions (text, table, image). (b) Dynamic Queries: Examine evolving tables across a consistent timeline. (c) Open Domain Queries: Merge retrieval, extraction, understanding, and temporal reasoning into one framework. (d) Reasoning with LLMs: Tailor large language models (LLMs) for tablespecific temporal logic, with more on this in Appendix H. Advanced prompts remain a potential area of exploration."
        },
        {
            "heading": "Limitations",
            "text": "First, it focuses solely on entity-centric tables from Wikipedia, excluding non-Infobox tables and other relevant sources. Exploring a broader range of table types would be valuable. Second, despite our efforts to ensure unbiased table selection and dataset annotation, inadvertent bias leakage is possible, potentially affecting results.\nThird, due to limited computational capacity, we could only fine-tune models using large sizes (XL), not extra-large (XXL). One idea that could be explore here is using Parameter Efficient Fine tuning (PEFT) (Mangrulkar et al., 2022). Incorporating more open-source Language Models (LLMs) would enhance our understanding of temporal reasoning capabilities. Lastly, our work primarily targets the English language, while exploring multilingual settings would increase applicability and generalizability. These limitations present opportunities for future research and expansion in this field."
        },
        {
            "heading": "Aknowledgement",
            "text": "The authors express their gratitude to Bloomberg\u2019s AI Engineering team, particularly Edgar Meij and Prabhanjan Kambadur, for their invaluable feedback and guidance. We are also thankful for the valuable insights provided by Ellen Riloff, Dan Roth, and the Utah NLP group. Special thanks to Dibyakanti Kumar and Manasvi Kundalia for their contributions. Vivek Gupta acknowledges the support received from Bloomberg\u2019s Data Science Ph.D. Fellowship and the Cognitive Computation Group at the University of Pennsylvania. This work is partially supported by NSF grants #1801446, #1822877, #2007398 and #2129111. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies of any government agency. Lastly, we extend our appreciation to the reviewing team for their insightful comments."
        },
        {
            "heading": "Ethics Statement",
            "text": "The dataset in this study is designed for research on temporal question answering with entity-centric tables. It should be strictly used for research purposes, not for other applications. We have diligently created the dataset to minimize bias during table selection and question-answer generation. However, inadvertent bias leakage is still possible,\nso thorough examination is crucial for uses beyond the intended research scope.\nTo ensure fairness, we provided fair wages to MTurk crowd workers and conducted three pilot studies to estimate task completion time accurately. We also plan to release a datasheet, full annotation template, and other resources for data and model openness. Emphasizing openness enables issue identification and correction, allowing continuous improvement based on community feedback. These measures promote transparency and facilitate further advancements in the field."
        },
        {
            "heading": "A Analysis Breakdown: GPT-3.5 model",
            "text": "GPT-3.5 Model: Table 16, 17, 18, 19, 20, 21 represent analysis of GPT-3.5 on various aspects such as question type, reasoning operation, explicit and implicit , entity type and category wise (tail and head), respectively."
        },
        {
            "heading": "B Analysis Breakdown: GPT-4 vs Human",
            "text": "GPT-4 vs. Human: Table 22, 23, 24, 25 represent analysis of GPT-4 and Human on various aspects such as question type, reasoning operation, explicit and implicit , entity type.\nWe also consider what the performance of model as comapred to Human on examples on particular table/article domain. We consider coarse grained categories for comparison. Table 26, 27 and 28, 29 shows the model head and tail set performance based on table domains for coarse and fine-gained setting."
        },
        {
            "heading": "Domain # EM Domain # EM",
            "text": ""
        },
        {
            "heading": "Op. F1 EM R1 R2 MET",
            "text": ""
        },
        {
            "heading": "Type # F1 EM R1 R2 MET",
            "text": ""
        },
        {
            "heading": "Type # F1 EM R1 R2 MET",
            "text": ""
        },
        {
            "heading": "C Models and Hyperparameters Details",
            "text": "In our research, we embarked on a series of training experiments utilizing several models such as BART, Flan-T5, and T5, each with base, large, and XL variants. Training was conducted over 1-3 epochs, incorporating input sequence lengths from 1024 to 4096 tokens. To foster efficient convergence,"
        },
        {
            "heading": "Op. Human GPT-4 Human GPT-4 Human GPT-4 Human GPT-4",
            "text": ""
        },
        {
            "heading": "Type Human GPT-4 Human GPT-4 Human GPT-4 Human GPT-4",
            "text": "we introduced a warm-up period of 500 steps and applied weight decay at a rate of 0.01 during the optimization phase. We implemented logging and evaluation at every 100-step interval. The learning rate was designated at 2e-5, and a gradient accumulation process of 8 steps was used to optimize memory resources.\nOur study further expanded to encompass zeroshot and few-shot experiments. This included, but was not limited to, chain-of-thought prompting on cutting-edge models like GPT-3.5 and GPT-4. We delved into various variants of Flan-T5 and T5 models, such as large L, XL, and XXL. A thorough analysis was undertaken to compare the performance of these models against human performance benchmarks."
        },
        {
            "heading": "Type Human GPT-4 Human GPT-4 Human GPT-4 Human GPT-4",
            "text": "Flan-T5 is an instruction fine-tuned derivative of the T5 language model, purposefully crafted to excel in a wide array of natural language processing tasks. These tasks include, among others, text generation, summarization, and translation. With the integration of instruction-based finetuning, Flan-T5 boosts its competency in handling zero-shot NLP tasks while also facilitating fewshot in-context learning. Thanks to its advanced encoder-decoder architecture and attention mech-\nanisms, Flan-T5 efficiently leverages contextual information to generate high-quality output. This capability promotes opportunities for enhanced performance and adaptability across various language processing applications.\nOur study involved fine-tuning the Flan-T5 model across its different variants: Flan-T5-Base, Flan-T5-Large, and Flan-T5-XL. Alongside this, we also carried out zero-shot, few-shot (with and without chain of thought prompting) experiments on Flan-T5-Large, Flan-T5-XL, and Flan-T5-XXL.\nBART (Bidirectional and AutoRegressive Transformers) is a robust sequence-to-sequence model architecture widely adopted in various natural language processing tasks. By fusing bidirectional and autoregressive training objectives, BART is capable of exploiting the context of both the input and target sequences. Given that BART features an autoregressive decoder, it can be directly fine-tuned for sequence generation tasks, such as abstractive question answering.\nWe fine-tuned the BART-Large model, consisting of 12 encoder-decoder layers with 440 million parameters, and BART-Base model, comprising 6 encoder-decoder layers and 140 million parameters. The performance analysis and outcomes of these fine-tuned models can be found in the main paper."
        },
        {
            "heading": "T5 (Text-To-Text Transfer Transformer) is a",
            "text": "versatile language model architecture. Based on the transformer model, T5 is equipped to handle various natural language processing tasks. By leveraging a \"text-to-text\" training approach, T5 learns to transform input text into target text, thus enabling it to manage a wide variety of tasks. These tasks include text classification, summarization, translation, and question answering. The T5 model\nincorporates an encoder-decoder structure with several layers of self-attention mechanisms and uses a shared vocabulary and tokenization scheme, thereby ensuring a consistent representation and efficient processing of text data.\nWe carried out fine-tuning on different variants of the T5 model: T5-Base, T5-Large, and T5-XL. We also conducted zero-shot experiments on T5Large, T5-XL, and T5-XXL.\nGPT-3.5-turbo and GPT-4 are the latest developments in the distinguished GPT series of language models. GPT-3.5 Turbo is an enhanced variant of GPT-3, boasting approximately 154 billion parameters and demonstrating superior language processing capabilities. It is particularly adept at text generation, comprehension, summarization, among other tasks. Conversely, GPT-4 signifies the next step in language modeling with an expected model size of around 1 trillion parameters and improved language understanding and generation capacities. These models rely on vast pretraining data for superior generalization and exhibit excellent performance in both zero-shot and few-shot learning scenarios.\nWe conducted a suite of experiments on GPT3.5 Turbo and GPT-4, focusing on zero-shot and few-shot learning scenarios, examining their performance with and without reasoning capabilities.\nTable Representation. Firstly, each table is transformed from HTML into a JSON representation, containing subheadings, rows with keys and their respective values, as well as the table title and category as distinct keys. We employed a linearization process akin to INFOTABS (Gupta et al., 2020), using delimiters such as \"tab\" or \":\" to separate keys, and \"newline\" or \";\" for rows. Subsections are partitioned by double \"new lines\" or \"#\". For instance, in Table 1, the representation is: Title: Petya Nedelcheva # Personal Information # Country: Bulgaria; Born: July 30, 1983 (age 38), and soon."
        },
        {
            "heading": "D Crowdsourcing Details",
            "text": "To construct TEMPTABQA, we divided the task into 80 batches, each consisting of three questionanswer pair generations per HIT6. We assigned\n6A Human Intelligence Task, or HIT, is a question that needs an answer. A HIT represents a single, self-contained, virtual task that a Worker can work on, submit an answer, and collect a reward for completing. HITs are created by Requester customers in order to be completed by Worker customers.\neach HIT to three distinct annotators, resulting in an average of 9 QA pairs generated per table. The wage for each HIT, which involved generating three question-answer pairs for a given table, was set at 0.75 cents based on the average completion time observed during three pilot studies.\nAll our annotators were proficient English speakers from countries where English is spoken. They possessed master-level qualifications and maintained a HIT acceptance rate of 95% and above. We occasionally rewarded frequent and exceptional annotators with a bonus of 3 times the cost of the HIT. To ensure task quality, we implemented temporary blocking and rewarding mechanisms for annotators.\nFor verification purposes, each HIT required answering three questions and providing a brief explanation. An annotator received 0.15 cents per HIT for this task. If consensus was not reached among the initial three annotators, we reassigned the HIT to another set of three annotators. Here two we start with three pilot study to decide the cost of the annotation. Notably, we observed that the top 50 annotators were responsible for annotating approximately 90% of the dataset. This observation aligns with other crowdsourced data annotation projects such as SNLI and MultiNLI.\nValidation Details. We employ straightforward pre-processing scripts to remove non-temporal and basic extractive questions from the training set prior to fine-tuning. For the test and development sets, we enforce rigorous quality control by manually reviewing each Table QA, with input from three experts who are NLP researchers. This process follows the initial automated script-based filtering and is aimed at ensuring high-quality complex temporal questions. Additionally, we address answer\nunits and correct spelling errors during our quality filtering. We prioritize questions involving intricate temporal reasoning and abstract concepts while filtering out questions with answers directly present in the question or associated tables. Questions that required external knowledge beyond common sense are also filtered.\nOur annotators were directed to offer concise and pertinent answers. While the majority adhered to the instructions, a few instances deviated, leading to occasional ambiguities. These ambiguities typically emerged when multiple answer forms conveyed the same meaning but in different units or formats, such as \u2019365 days\u2019 \u201912 months\u2019 or \u2019one year.\u2019 We ensured our assessment script didn\u2019t penalize models or human verifiers for unit or format conversion issues. We established regex rules that encompassed various forms, and these were further validated through human verification across numerous samples. Figure 30 shows the exact agreement between across several annotators.\nE More Examples from TEMPTABQA\nFigures 2, 3, 4, 5, 6 show some examples of tabular question answers from TEMPTABQA.\nAnswering these questions demands from language models an understanding of temporal relationships to correctly connect time frames to pertinent events, as well as numerical reasoning to perform calculations, comparisons, and quantitative analyses based on temporal data. This includes both basic arithmetic and complex numerical reasoning like identifying trends or evaluating numerical changes over time.\nThese questions present challenges for language models due to the multi-faceted nature of the information required to answer them. First, they demand a deep understanding of temporal relationships, encompassing the ability to interpret and analyze time frames accurately. The language model must effectively connect these time frames to specific events, albums, or other relevant entities mentioned in the context.\nFurthermore, numerical reasoning plays a crucial role in successfully addressing these questions. The language model needs to perform calculations, comparisons, and quantitative analysis based on temporal data to arrive at the correct answers. This entails not only basic arithmetic operations but also more sophisticated numerical reasoning, such as identifying trends, computing durations, or evaluat-\ning numerical changes over time."
        },
        {
            "heading": "F Further Discussion",
            "text": "Key Findings. Based on our experimental analysis in \u00a74, we conclude that even state-of-the-art large language models like GPT-4 struggle with temporal question answering on entity-centric tables within TEMPTABQA, despite humans\u2019 high performance. Fine-tuning and few-shot learning techniques have a positive impact on the model\u2019s performance. The model encounters more difficulties in the tail domain, comprising rare occurrences, compared to the head domain with more frequent instances. Techniques involving step-by-step explanations, such as chain of thought prompting, further enhance the model\u2019s performance.\nIn our breakdown in \u00a75, we discovered inconsistent performance of the model and humans across\nvarious question types, answer entity types, reasoning operations, answer positions, and table domains. Both the model and humans demonstrate varying levels of proficiency across different categories. The analysis helps identify weaknesses and areas for improvement in future temporal reasoning models on semi-structured tabular data.\nSemi-structured Tables. Semi-structured data lies in a realm between raw, unstructured text and rigidly structured content such as Knowledge Grpah. This data landscape, where structured frameworks interweave with free-form text, spans the gamut from extensive verbosity like web pages, to succinct instances such as fact sheets, information tables, and technical specifications. Unlike databases, this type of data isn\u2019t uniformly structured; it can be a heterogeneous assortment without preset schemas. Adding to the complexity, explana-\ntory text that imparts context isn\u2019t always at hand. Nonetheless, we frequently deduce insights from such diverse and incomplete data, bridging information gaps based on our expectations about relationships within.\nReasoning Requirements. Navigating semistructured information necessitates a broad range of reasoning skills. We\u2019re tasked with comprehending a makeshift layout composed of elements like text snippets, form fields, or even sub structured components like lists. Querying this data calls for various levels of inference, ranging from straightforward lookups e.g. in Figure 1 querying Petya born place, to lexical deductions, such as understanding in same table single (WS) and double game (WD) format, junior championship vs. senior events of badminton, to grasping the nature of content within cells, the structure of the various events, the tournament names, tournament years and places, the total and specific medals tally, and the tournament types. Additionally, we might find ourselves aggregating insights across multiple rows, such as understanding that Dressage is a non-contact sport in which both genders compete, or even conducting intricate reasoning that melds temporal details with general knowledge.\nSimilarity with Knowledge Graph. However, it\u2019s important to note that Infoboxes exhibit a high degree of similarity with standard knowledge bases, particularly when compared to Wikidata. Wikidata generally surpasses Infoboxes in terms of comprehensiveness. When contrasting Wikidata with the\nInfobox style, we observe significant distinctions in how information is structured. Wikidata adopts a more organized and structured approach, resembling a knowledge graph. For example, when dealing with a person\u2019s birth details, Wikidata neatly separates the information into distinct categories like \"birth date,\" \"birth place,\" and \"birth name.\" In semi-structured Infoboxes , on the other hand, these details are often combined under a single heading, such as \"Born.\" Furthermore, there is a noticeable contrast in how relationships are presented. In Wikidata, relationships are systematically categorized. For instance, instead of using a generic \"spouse\" label, Wikidata provides separate entries for \"husband\" and \"wife,\" resulting in a more precise representation. In contrast, an Infobox might consolidate such information under a single \"spouse\" entry without specifying the gender.\nFrom a Temporal Perspective Temporal details find distinct treatment as well. Wikidata distinctly separates \"start date\" and \"end date,\" yielding precise timeline information. This stands in contrast to Infoboxes , where these details could be condensed into single terms like \"service,\" potentially necessitating further interpretation. Wikidata\u2019s penchant for hierarchy is evident in how complex terms are\nbroken down. For instance, a \"government official\" could be subcategorized as \"president,\" \"prime minister,\" and more. In contrast, Infoboxes might lack this hierarchical clarity, opting for more generalized terms. Granular attributes shine in Wikidata, with individual specifications for attributes like \"awards,\" enabling a detailed breakdown of accolades. Conversely, Infoboxes could consolidate these attributes, obscuring the specifics of received awards. When it comes to event descriptions, Wikidata adopts a distinction between \"start time\" and \"end time,\" leading to lucid event elucidations. In Infoboxes , these might be captured by a singular term, potentially devoid of temporal context. Lastly, Wikidata\u2019s categorization of properties imparts a structured approach to data.\nIn contrast, Infoboxes may not adhere to a similar systematic categorization, potentially leading to ambiguity. Collectively, these instances highlight the structured nature of Wikidata in contrast to the more succinct, semi-structured implicit knowledge of Infoboxes ."
        },
        {
            "heading": "Entity Table conversion\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Knowledge Graph. As",
            "text": "Infoboxes are highly structured (compared to Web tables), we could translate them to Wikidata and apply existing datasets and algorithms. Despite, this approach holds some promise, it\u2019s worth noting that transforming them into a clean and fully structured Wikidata format is in itself a challenging task, as highlighted earlier. Nevertheless, it presents an interesting opportunity to explore the capabilities of state-of-the-art language models in achieving this conversion. However, it\u2019s important to acknowledge that tables not found on Wikipedia, such as those containing e-commerce attribute values, research grants, medical reports, financial company data, etc., pose their own challenges when it comes to transitioning them into structured knowledge formats like Wikidata."
        },
        {
            "heading": "G Table Representation for LLMs",
            "text": "We experimented with three prompts, each featuring detailed instructions similar to those given to human verifiers. These prompts were based on three distinct table representations using different delimiters. Our selection process involved choosing the prompt that yielded the best performance. We present the table input in a linear format, akin to the approach adopted in TABFACT (Chen et al., 2020b) and INFOTABS (Gupta et al., 2020). Here, we employ a distinctive denominator token to demarcate rows using \";\" and columns using \":\". We also explored alternative delimiters such as \"|\" and \"#\" as well, the performance was similar.\nWe also experimented with an approach involving attempted table-to-paragraph conversion, but it caused models to include unwanted external information. LLM parametric knowledge lead to out of table unwanted hallucination in the paragraph. The performance variation across these representations were marginal <1% in the F1-score, and <0.75% in the exact match."
        },
        {
            "heading": "H Future Directions: Other Modeling Techniques",
            "text": "Based on our observations and discussions, we have identified several promising future directions for enhancing models performance on TEMPTABQA:\n1. LLM Pre-trained with Temporal Knowledge: Explore techniques incorporating temporal aspects during pre-training for masked language models (e.g., Dhingra et al. (2022); Iv et al. (2022)). Assess their performance in temporal tabular tasks using auxiliary tasks from temporal question-answering datasets in open domains (Jia et al., 2021b), cloze-form, or event-centric settings (Dhingra et al., 2022; Chen et al., 2021a; Ning et al., 2018; Wen et al., 2021).\n2. Temporal-Aligned Models for EntityCentric Tabular Data: Utilize temporally tuned language models (e.g., TEQUILA, EXAQT, OTR-QA, TempoQR) on temporal knowledge-based question-answering datasets (e.g., CRONQUESTIONS (Saxena et al., 2021), TEMPQA-WD (Neelam et al., 2022)) for answering questions related to temporal events (Jia et al., 2018a,b; Shang et al., 2021; Mavromatis et al., 2021; Saxena et al., 2021; Neelam et al., 2022).\n3. Integrating External Temporal Knowledge: Incorporate knowledge base questionanswering datasets like CRONKBQA(Saxena et al., 2021), nto LLM models during pretraining (e.g., ERNIE (Zhang et al., 2019), WKLM, KECP, ERICA, DKPLM) or structural adaptation (e.g., ERNIE-THU, KnowBert, EaE, JAKET). Explore the use of non-entity temporal relations (e.g., ERICA, KEPLER, DKPLM, KP-PLM) through pretraining objectives or structural adaptation methods (e.g., FaE, K-adapter, KB-adapter,\nKLMO, KERM, JointLK, GreaseLM, JAKET, KnowPrompt, OntoPrompt) as described in detail in (Hu et al., 2023).\n4. Fine-Tuning on Other Temporal Knowledge: Investigate benefits of training on synthetic and counterfactual temporal data (implicit knowledge addition) to enhance model performance, similar to AUTOTNLI (Kumar et al., 2022) and (Eisenschlos et al., 2020). Consider using simple temporal data from unstructured text sources like Time-SensitiveQA (Chen et al., 2021c) and CogCompTime (Ning et al., 2018), or structured text datasets like TempQA-WD, CronQUESTIONS, and TempQuestions (Saxena et al., 2021; Neelam et al., 2022; Jia et al., 2018b; Shang et al., 2021), which feature question-answering over knowledge graph embeddings with temporal links."
        }
    ],
    "title": "TEMPTABQA: Temporal Question Answering for Semi-Structured Tables",
    "year": 2023
}