{
    "abstractText": "Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledgeintensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like \u201cAs far as I know\u201d to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks demonstrate the effectiveness of our proposed approach, which proves that the knowledge stored in PLMs can be better exploited to enhance performance1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yunzhi Yao"
        },
        {
            "affiliations": [],
            "name": "Peng Wang"
        },
        {
            "affiliations": [],
            "name": "Shengyu Mao"
        },
        {
            "affiliations": [],
            "name": "Chuanqi Tan"
        },
        {
            "affiliations": [],
            "name": "Fei Huang"
        },
        {
            "affiliations": [],
            "name": "Huajun Chen"
        },
        {
            "affiliations": [],
            "name": "Ningyu Zhang"
        }
    ],
    "id": "SP:92255e1a0794065411ba19ee32e67cb5ef982120",
    "references": [
        {
            "authors": [
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Chaitanya Malaviya",
                "Keisuke Sakaguchi",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Doug Downey",
                "Wen-tau Yih",
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "8th International Conference on Learning Represen-",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Jianfeng Gao",
                "Yejin Choi."
            ],
            "title": "PIQA: reasoning about physical commonsense in natural language",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Ap-",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi."
            ],
            "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Huajun Chen"
            ],
            "title": "2022a. Knowprompt: Knowledge",
            "year": 2022
        },
        {
            "authors": [
                "Chang",
                "Furu Wei"
            ],
            "title": "2022. Knowledge neurons",
            "year": 2022
        },
        {
            "authors": [
                "Tushar Khot"
            ],
            "title": "Chain-of-thought hub: A",
            "year": 2023
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: decoding-enhanced bert with disentangled attention",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Trans. Big Data, 7(3):535\u2013547.",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Yanyang Li",
                "Jianqiao Zhao",
                "Michael R. Lyu",
                "Liwei Wang."
            ],
            "title": "Eliciting knowledge from large pre-trained models for unsupervised knowledgegrounded conversation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Xinyue Chen",
                "Jamin Chen",
                "Xiang Ren."
            ],
            "title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma",
                "Sheng-Chieh Lin",
                "JhengHong Yang",
                "Ronak Pradeep",
                "Rodrigo Nogueira."
            ],
            "title": "Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations",
            "venue": "SIGIR \u201921: The 44th Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Skyler Hallinan",
                "Ximing Lu",
                "Pengfei He",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Yejin Choi."
            ],
            "title": "Rainier: Reinforced knowledge introspector for commonsense question answering",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Generated knowledge prompting for commonsense reasoning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Wenya Wang",
                "Dianzhuo Wang",
                "Noah A. Smith",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Vera: A general-purpose plausibility estimation model for commonsense statements",
            "venue": "CoRR, abs/2305.03695.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv., 55(9):195:1\u2013195:35.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Shangwen Lv",
                "Daya Guo",
                "Jingjing Xu",
                "Duyu Tang",
                "Nan Duan",
                "Ming Gong",
                "Linjun Shou",
                "Daxin Jiang",
                "Guihong Cao",
                "Songlin Hu."
            ],
            "title": "Graphbased reasoning over heterogeneous external knowledge for commonsense question answering",
            "venue": "The",
            "year": 2020
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Anette Frank."
            ],
            "title": "Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2018
        },
        {
            "authors": [
                "David N. Milne",
                "Ian H. Witten."
            ],
            "title": "Learning to link with wikipedia",
            "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008, Napa Valley, California, USA, October 26-30, 2008, pages 509\u2013518. ACM.",
            "year": 2008
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick S.H. Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard",
                "Vassilis Plachouras",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel"
            ],
            "title": "KILT: a benchmark",
            "year": 2021
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Shuofei Qiao",
                "Yixin Ou",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yunzhi Yao",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Huajun Chen."
            ],
            "title": "Reasoning with language model prompting: A survey",
            "venue": "CoRR, abs/2212.09597.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "CoRR, abs/1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social IQa: Commonsense reasoning about social interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Vered Shwartz",
                "Peter West",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Unsupervised commonsense question answering with self-talk",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Haitian Sun",
                "Tania Bedrax-Weiss",
                "William Cohen."
            ],
            "title": "PullNet: Open domain question answering with iterative retrieval on knowledge bases and text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Mozes van de Kar",
                "Mengzhou Xia",
                "Danqi Chen",
                "Mikel Artetxe."
            ],
            "title": "Don\u2019t prompt, search! miningbased zero-shot learning with language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop Black-",
            "year": 2018
        },
        {
            "authors": [
                "PeiFeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren."
            ],
            "title": "PINTO: Faithful language reasoning using prompt-generated rationales",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Kaiyue Wen",
                "Zhengyan Zhang",
                "Lei Hou",
                "Zhiyuan Liu",
                "Juanzi Li."
            ],
            "title": "Finding skill neurons in pre-trained transformer-based language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Sean Welleck",
                "Jiacheng Liu",
                "Ronan Le Bras",
                "Hanna Hajishirzi",
                "Yejin Choi",
                "Kyunghyun Cho."
            ],
            "title": "Naturalproofs: Mathematical theorem proving in natural language",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and",
            "year": 2021
        },
        {
            "authors": [
                "Sean Welleck",
                "Jiacheng Liu",
                "Ximing Lu",
                "Hannaneh Hajishirzi",
                "Yejin Choi."
            ],
            "title": "Naturalprover: Grounded mathematical proof generation with language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi."
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2020
        },
        {
            "authors": [
                "Yi Yang",
                "Wen-tau Yih",
                "Christopher Meek."
            ],
            "title": "Wikiqa: A challenge dataset for open-domain question answering",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal,",
            "year": 2015
        },
        {
            "authors": [
                "Yunzhi Yao",
                "Shaohan Huang",
                "Li Dong",
                "Furu Wei",
                "Huajun Chen",
                "Ningyu Zhang."
            ],
            "title": "Kformer: Knowledge injection in transformer feed-forward layers",
            "venue": "Natural Language Processing and Chinese Computing: 11th CCF International Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Antoine Bosselut",
                "Hongyu Ren",
                "Xikun Zhang",
                "Christopher D. Manning",
                "Percy Liang",
                "Jure Leskovec."
            ],
            "title": "Deep bidirectional language-knowledge graph pretraining",
            "venue": "Neural Information Processing Systems (NeurIPS).",
            "year": 2022
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Hongyu Ren",
                "Antoine Bosselut",
                "Percy Liang",
                "Jure Leskovec."
            ],
            "title": "QA-GNN: Reasoning with language models and knowledge graphs for question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "STar: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "HellaSwag: Can a machine really finish your sentence",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Lei Li",
                "Xiang Chen",
                "Xiaozhuan Liang",
                "Shumin Deng",
                "Huajun Chen."
            ],
            "title": "Multimodal analogical reasoning over knowledge graphs",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Xikun Zhang",
                "Antoine Bosselut",
                "Michihiro Yasunaga",
                "Hongyu Ren",
                "Percy Liang",
                "Christopher D Manning",
                "Jure Leskovec."
            ],
            "title": "GreaseLM: Graph REASoning enhanced language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen."
            ],
            "title": "A survey of large language models",
            "venue": "CoRR, abs/2303.18223.",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledgeintensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like \u201cAs far as I know\u201d to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks demonstrate the effectiveness of our proposed approach, which proves that the knowledge stored in PLMs can be better exploited to enhance performance1."
        },
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (PLMs) have waved the NLP community as fundamental infrastructure by demonstrating remarkable abilities with the \u201cpre-train, prompt, and predict\u201d paradigm (Liu et al., 2023b; Zhao et al., 2023). The mere PLMs, however, lack the capacity to handle knowledgeintensive tasks with advanced functionalities like commonsense reasoning (Lin et al., 2019; Qiao et al., 2022; Liu et al., 2023a) and open-domain question answering (Yang et al., 2015). This necessitates a boosting trend for research focusing\n\u2217Corresponding author. 1Code is available in https://github.com/zjunlp/\nknowledge-rumination.\nQ: If a bird is a carnivore, then it is likely a(n) A). prey B). predator C). herbivore D). canary\nDirectly finetuning answer is A: prey\nAbout carnivore, as far as I know,\nProbed answer is B: predator\na carnivore is a predator\non augmenting PLMs with external knowledge sources (Chen et al., 2017, 2022a; Welleck et al., 2021, 2022; Zhang et al., 2022, 2023).\nHowever, despite the empirical success, we observe that PLMs can often encode extensive knowledge within their parameters yet fail to utilize this effectively for knowledge-intensive tasks. Taking pilot experiments as an example, we use knowledge probing (Petroni et al., 2019) to the PLM as shown in Figure 1. Given a question \u201cIf a bird is a carnivore, then it is likely a(n) what?\u201d, we notice that the PLM has known the knowledge \u201ca carnivore is likely a(n) predator\u201d in its parameters; however, we surprisingly find that the finetuned PLM chose the wrong answer despite the model knowing the related knowledge. Interestingly, this phenomenon mirrors human behavior. As an example, in the cognitive reflection test (CRT) (Frederick, 2005), participants have posed a series of straightforward questions (already learned), yet they often initially fail in their intuitive reason-\ning. Upon reflection, however, individuals typically identify their erroneous responses and correct them. Consequently, we conjecture that the prominent PLMs of today have flaws as humans and we still have the following problem: are we fully exploiting the potential of the PLMs?\nSome pioneering researchers have attempted to unravel this enigma. For instance, Chen et al. (2022b) and van de Kar et al. (2022) propose to utilize the knowledge in the pre-traning corpus by retrieve-then-fine-tuning method. Likewise, Bhagavatula et al. (2020) capitalizes on the implicit knowledge within large language models (>10B) by retrieving from model weights with recitationaugmented generation. These studies affirm that PLMs encapsulate a vast body of knowledge, with untapped potential, while in our paper, we pursue a more universally applicable, yet simple solution to fully harness knowledge in PLMs for NLP.\nTo address this need, we introduce Knowledge Rumination to assist the model in thinking thoughtfully in handling knowledge-intensive tasks. Analogous to how animals ruminate food for better digestion and absorptionby regurgitating it from the stomach back to the mouth for additional chewingwe aim to mimic this process by having the model first review the relevant knowledge stored in its parameters and then consolidate this knowledge to better tackle associated tasks. In detail, we propose knowledge reviewing with a task-guided prompt by simply adding \u201cAs far as I know\u201d to stimulate the model to recall latent knowledge. Subsequently, we consolidate knowledge via FFN to explicitly leverage latent knowledge to help address downstream tasks since FFN plays a crucial role in PLMs (Wang et al., 2022).\nWe apply the proposed knowledge rumination to various PLMs, including RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021). We also transfer knowledge rumination to large language GPT3 (175B) (Brown et al., 2020). Experimental results on six commonsense reasoning tasks and the GLUE benchmark demonstrate that the proposed simple method can obtain performance gain and even outperform baselines of retrieving external knowledge. To conclude, we summarize the contributions of this work as follows:\n\u2022 We propose a novel approach of Knowledge Rumination to better utilize the knowledge stored in the parameters, which is model agnostic and can be applied to any PLMs\n\u2022 Experimental results demonstrate that the proposed approach can successfully elicit related knowledge for both small and large PLMs, yielding better performance on six commonsense tasks and GLUE benchmarks.\n\u2022 Comprehensive empirical analysis indicates that still a large underestimated amount of knowledge can be retrieved from PLM\u2019s model weights, and our work takes a small step in this direction."
        },
        {
            "heading": "2 Related Work and Background",
            "text": "Extracting Knowledge from PLMs Previous studies have shown that PLMs implicitly contain a large amount of knowledge. Petroni et al. (2019) have shown that such language models can be used in a Knowledge Base (KB) completion task by converting KB relations into natural language templates. Based on this finding, researchers attempt to treat the PLM as a knowledge base. Some studies (Bosselut et al., 2019; West et al., 2022; Hao et al., 2022) employ PLMs to construct knowledge graphs automatically. Meanwhile, some others (Shwartz et al., 2020; Li et al., 2022) find that the knowledge possessed by the PLMs can be used to enhance the model\u2019s performance in downstream tasks. To date, several work (Wang et al., 2023; Zelikman et al., 2022; Bhagavatula et al., 2020) attempt to utilize PLMs to generate free-text rationales for reasoning. Our approach differs from previous works in that we aim to enhance the model\u2019s understanding of what it already knows in order to improve performance.\nKnowledge-Enhanced Models Researchers resort to external sources to facilitate the model\u2019s ability to deal with knowledge-intensive situations. One direction is to ground the question in a KB and conduct inference with both the question and the retrieved knowledge (Yasunaga et al., 2022, 2021; Zhang et al., 2022; Sun et al., 2019; Yao et al., 2022; Lv et al., 2020; Lin et al., 2019). Since the pre-trained model can also be viewed as a knowledge store, several recent studies including Self-talk (Shwartz et al., 2020), Rainier (Liu et al., 2022a), GKP (Liu et al., 2022b), ElicitKnowledge (Li et al., 2022) propose to treat the large language model (e.g., GPT-3) as an external source to elicit knowledge for downstream tasks. In contrast, our approach diverges from relying on external sources such as knowledge bases (KB)\nor language models (LM). Instead, we concentrate on fully leveraging the latent knowledge acquired by the model itself. There are also some kinds of work that decompose the question into subquestions and ask the model to answer each subquestion such as least-to-most prompt (Zhou et al., 2023). However, even these approaches encounter the issue we proposed where the model may possess the answer to the sub-question within its parameters but fails to provide the correct response. The underlying intuition behind our method is that current methods for harnessing the power of pretrained language models (PLMs) have not fully tapped into the knowledge residing within the model\u2019s parameters.\nNote that our approach most closely aligns with Self-talk (Shwartz et al., 2020), but with an additional capability to manage parametric knowledge (such as embeddings in Feed-Forward Networks). This capability broadens the spectrum of the academic idea to a certain extent."
        },
        {
            "heading": "3 Knowledge Rumination",
            "text": "In this section, we introduce technical details of knowledge rumination to tap into the potential of PLMs (\u00a73.1). Given a PLM G, we first freeze the model parameters and design a task-specific prompt (\u00a73.2) to guide the model in reviewing its stored knowledge regarding the task and input (knowledge reviewing). We then consolidate the model\u2019s latent knowledge (\u00a73.3) during tuning downstream tasks (knowledge consolidation)."
        },
        {
            "heading": "3.1 Model Architecture",
            "text": "We take a representative task, multiple-choice commonsense reasoning, as an example to elucidate the details of knowledge rumination, which can be simply adapted to any other tasks in NLP. Given a question q, multiple-choice commonsense reasoning aims to selecting the correct answer ak \u2208 A provided with an optional context c. The set of possible answers A is finite and varies for each question. In the vanilla setting, the PLM is used to directly answer the question by selecting the answer choice a\u0302 with the highest score, based on the concatenation of the question q, context c, and one possible answer choice ai as:\na\u0302 = argmax ai\u2208A\nP (ai | c, q) (1)\nHere, before making a prediction, we ask the model to carefully consider the question and re-\nview its prior knowledge. We freeze the PLM G\u03b8 to probe the knowledge it has stored (\u03b8 represents the model\u2019s parameter) and prepend trainable continuous tokens to each layer. For each question q, we create a unique prompt pq to guide the model in reflecting on its knowledge:\nr = G\u03b8 ([q; pq]) (2)\nThen, the PLM will reinforce its knowledge r of the problem and infer the answer augmented with r. Ideally, the model is supposed to generate helpful knowledge texts for the question. However, training the model requires expensive knowledge annotations for all training instances. To handle this problem, we use the model\u2019s contextualized representation output as the knowledge for rumination and leverage it as a latent variable. The model will answer the question based on both the question q and the vectorized knowledge r:\na\u0302 = argmax ai\u2208A\nP (ai | c, q, r) (3)\nThen, the cross-entropy loss is used to train the whole model. Assuming the answer ak is correct, the loss can be obtained as follows:\nLce = \u2212 \u2211 ai\u2208A Q(ai | c, q) logP (ai | c, q, r) (4)\nwhere Q(ai | c, q) is 1 if ai = ak and 0 otherwise. During training, the gradient flows back into the model, which helps it learn to review and consolidate useful information."
        },
        {
            "heading": "3.2 Knowledge Reviewing with Task-guided Prompting",
            "text": "Analogically, animals return partially digested food from the stomach to the mouth for rechewing; we design specific prompts for each question to probe the latent knowledge for rumination. As shown in Figure 2, we begin by using the background prompt: \u201cAs far as I know, [MASK]\u201d. Note that humans consider mentions in the descriptions to better understand the question. For example, when answering the question \u201c If a bird is a carnivore, then it is likely a(n) what?\u201d, humans would consider the mentions of bird and carnivore to better comprehend the question. We further introduce the mention prompt to review knowledge of mentions. Specifically, we extract mentions M from the questions using off-the-shelf tools2 and\n2https://github.com/marcocor/tagme-python\n(a) Fine-Tuning for PLM\n(b) Prompt Learning for PLM\nengland mountains hen house english hunt california\nAbout commonsense reasoning, I know, [MASK]\u2026[SEP]\nbuild prompts to elicit memories about these mentions. However, it should be noted that some mentions contain unrelated information and may divert attention. To address this, we propose \u201cmention relevance scoring,\u201d where we utilize an encoder model Genc to evaluate the relevance of each mention in relation to the question context. Specifically, we compute the relevance score for each mention m \u2208 M by concatenating the text with the question q and using the output of \u201c[CLS]\u201d as the score (fcls in the following equation):\n\u03c1m = fcls (Genc([q;m])) (5)\nWe sample mentions with the Top-2 relevance scores \u03c1 as the target mentions. The details of \u201cmention relevance scoring\u201d can be found in Appendix A.5. Actually, apart from this commonsense or context knowledge, it is important to note that PLMs also store other types of knowledge, including skill knowledge and task knowledge (Wang et al., 2022). Hence, we assume that the model may acquire latent knowledge regarding the task itself, so we further build task prompts to encourage the model to reflect on the skill knowledge encoded within its parameters.\nOverall, we probe the PLM using three different types of task-guided prompts, along with three areas of interest:\n\u2022 Background Prompt: is designed to help the model to think about the background, the prompt is As far as I know [MASK].\n\u2022 Mention Prompt: is used to elicit memories of mentions, the formation is About <Mention>, I know [MASK].\n\u2022 Task Prompt: is designed to help the model reminisce memories of the task. For example, for the sentiment analysis, the prompt is About sentiment analysis, I know [MASK].\nWe put several \u2018[MASK]\u2019 in the task-guided prompt, and the length of the \u2018[MASK]\u2019 is a hyperparameter for different tasks.\nOur approach can be applied to different PLMs. For the encoder-style model like RoBERTa, we utilize the hidden states h[MASK] of the \u2018[MASK]\u2019 as the latent knowledge for rumination. fmask in the following equation means taking the h[MASK] from the model\u2019s output.\nr = fmask(G\u03b8 ([q; pq])) (6)\nThe following section will explain how to utilize this elicited knowledge."
        },
        {
            "heading": "3.3 Knowledge Consolidation with FFN Neuron Augmentation",
            "text": "To reinforce its understanding, the model should re-digest (inject) the elicited knowledge r of the q, similar to how animals chew their food again. However, where to inject the PLMs remains a challenging issue, indicating potential work to investigate how the model\u2019s knowledge is kept. Previous studies (Dai et al., 2022; Wang et al., 2022) have discovered that the Feed Forward Network works (FFN) as the knowledge neuron or skill neuron, illustrating that FFN may store factual information and encode task-specific skills. Inspired by these findings, we incorporate r into the FFNs, as previous work (Yao et al., 2022) does. Here, we select the Top-1 layer to re-diest (inject) the knowledge. Suppose the two linear layers in FFN emulate as a key-value network K and V , we employ two distinct linear layers to project the information r to the vector space of the matching layer:\n\u03d5k = Wk \u00b7 r (7) \u03d5v = Wv \u00b7 r (8)\nwhere Wk and Wv represents the weights of the two linear layers (Wk,Wv \u2208 Rd\u00d7d, d is the intermediate size of the PLM). The two matrices, Wk and Wv, are initialized randomly and will be updated during training. We expand the FFN by concatenating the projected knowledge to the end of the linear layer and obtain the expanded KE ,V E . The computing can be described as follows:\nFFN(H) = f(H \u00b7KE) \u00b7 V E = f(H \u00b7 [\u03d5k : K]) \u00b7 [\u03d5v : V ]\n(9)\nH denotes the output hidden states of the selfattention module. The model would answer the question with the help of regurgitated knowledge."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We evaluate the proposed approach on six knowledge-intensive tasks of commonsense reasoning benchmarks: CommonsenseQA (CSQA) (Talmor et al., 2019), SocialIQA(Sap et al., 2019), PhysicalQA (PIQA) (Bisk et al., 2020), Openbook QA (OBQA) (Mihaylov et al., 2018), HellaSwag (Zellers et al., 2019) and Abductive Natural Language Inference (aNLI) (Bhagavatula et al., 2020). We follow the data split used by prior works (Yasunaga et al., 2022). Meanwhile, to better understand the effect of the task prompt in our method and the skill knowledge (Wang et al., 2022) learned in the pretrained language model, we consider tasks of the GLUE benchmarks (Wang et al., 2018), including single-sentence tasks (SST, CoLA), inference tasks (QNLI, RTE), and similarity and paraphrase tasks (STS-B, MRPC). We provide dataset details in Appendix A.7."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We choose RoBERTa_large (Liu et al., 2019) and DeBERTa_large (He et al., 2021) as our backbone models for moderately sized language models and compare performance with the following strong external knowledge-enhanced approaches: QAGNN (Yasunaga et al., 2021), GreaseLM (Zhang et al., 2022) and Dragon (Yasunaga et al., 2022). More details about baseline models can be seen in Appendix A.3.2. For the tasks in the GLUE benchmark, in addition to RoBERTa_large, we compare our model with a prompt learning method, LM-BFF (Gao et al., 2021). LM-BFF proposes a prompt-based finetuning method and a refined\nstrategy for dynamically and selectively incorporating demonstrations into each context. In our paper, we simply use the human-curated prompt (provided by LM-BFF) and leverage RoBERTa_large as the backbone."
        },
        {
            "heading": "4.3 Experiment Implementation",
            "text": "In the stage of knowledge reviewing with taskguided prompting, the backbone of the model is frozen, and we only update the prepended trainable continuous tokens (prefix prompt). When implementing the DeBERTa, due to the complex attention mechanism, we simply freeze the whole model and do not add the prefix tokens. For the commonsense reasoning task, we combine the mentioned prompt and background prompt, and for the GLUE benchmarks, we find the task prompt to be more useful. More details can be found in Appendix A.2 and A.3."
        },
        {
            "heading": "4.4 Main Results",
            "text": "We list the results in Table 1 for the commonsense reasoning tasks and Table 2 for the GLUE benchmark. The proposed technique, called knowledge rumination, demonstrates superior performance on a majority of datasets. To be noted, it outperforms naive baselines and obtains better or comparable performance with baselines that incorporate external knowledge. As the Table illustrates, the proposed method of knowledge rumination, RumiRoBERTa, and RumiDeBERTa, shows improved performance on six commonsense reasoning tasks. The results demonstrate that RumiRoBERTa and RumiDeBERTa consistently outperform existing language models (RoBERTa and DeBERTa), with a notable improvement of +2% absolute accuracy on CSQA compared to RoBERTa and DeBERTa. These results indicate that the knowledge stored in the PLM\u2019s parameters can still be further exploited. In addition, it can be observed that RumiRoBERTa and the method that incorporates external knowledge have compa-\nrable results. Notably, on SocialIQA, aNLI, and HellaSwag, RumiRoBERTa even outperforms the pre-trained, knowledge-enhanced model Dragon. On the other hand, RumiDeBERTa performs better than Dragon on most of the tasks. However, the model that uses external knowledge performs best on the CommonsenseQA task. It is hypothesized that some commonsense reasoning datasets are derived from pre-existing knowledge bases. For instance, CommonsenseQA is derived from ConceptNet. The above observations suggest that: 1) the knowledge stored in the parameters is robust and requires explicit activation during finetuning. 2) the performance of the model that retrieves knowledge from external sources is impacted by the quality and relevance of the knowledge sources, while our rumination methods can produce more pertinent knowledge.\nAs shown in Table 2, the results of the GLUE benchmark demonstrate that the knowledge rumination method outperforms the basic finetuning model RoBERTa and the prompt-based method LM-BFF, with an average improvement of +1% for LM-BFF and 3% for RoBERTa. These gains in performance highlight the effectiveness of knowledge rumination methods compared to finetuning and prompt learning."
        },
        {
            "heading": "4.5 Out-of-Distribution (OOD) Performance",
            "text": "To better illustrate the wide applicability and generalization prowess of the knowledge rumination method, we extended our evaluation to incorporate performance on out-of-distribution (OOD) test sets. Table 3 presents a comparative study of the OOD performance of fine-tuning techniques applied to both RoBERTa and RumiRoBERTa models. In general, RumiRoBERTa demonstrates superior performance on OOD tests compared to the conventional fine-tuning model. Notably, when RumiRoBERTa was trained on OBQA and subsequently tested on HellaSwag and PIQA, it achieved a 5% advantage over RoBERTa. This\nenhancement in performance can be attributed to the importance of knowledge rumination in effective problem-solving and knowledge application. Despite exhibiting slightly sub-optimal results on CSQA \u21d2 HellaSwag and PIQA \u21d2 OBQA tests, RumiRoBERTa\u2019s performance still compares favorably with the traditional fine-tuning method."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Impact of Different Knowledge Consolidation Methods",
            "text": "Apart from injecting knowledge in FFN, we compare and evaluate another injection method: Concatenation. Since the knowledge r is a vector, we concatenate it into the sequence after the embedding layer and report the results in Table 4. We notice that both methods benefit from knowledge rumination. Typically, integrating the knowledge through feed-forward networks (FFN) demonstrates better performance than concatenation, with an average improvement of +0.5% in SocialIQA and +2.1% in OBQA. This supports previous research findings that Feed-Forward Networks (FFNs) store some factual knowledge (Dai et al., 2022; Yao et al., 2022) and that our method can ef-\nfectively consolidate this knowledge within FFNs."
        },
        {
            "heading": "5.2 What does the Model Ruminate?",
            "text": "Despite the advantages of knowledge rumination, it is essential to understand the nature of the knowledge generated and the mechanism behind the method. In our model, the model produces a contextualized embedding r as latent knowledge. To make the knowledge more interpretable to humans, we convert the vectorized knowledge to symbolic text. In order to evaluate the effectiveness of the method, we sample successful cases where the simple finetuning model makes the incorrect prediction while our knowledge rumination method provides the correct answer.\nFigure 3 illustrates an example from the CommonsenseQA task. To generate the output words in the vocabulary space, we apply the masked language modeling (MLM) head over the position of the \u2018[MASK]\u2019. We notice that the masked word often includes similar information to the answer. For example, in the question \u201cWho is likely to have a caring heart?\u201d, the \u2018[MASK]\u2019 token contains words such as \u2018people,\u2019 \u2018heart,\u2019 and \u2018like.\u2019 In addition, we map the knowledge rumination output r to the pre-trained corpus space as the memory is constructed during pre-training. We conduct a dense embedding similarity search to identify what our generated contextualized representation is most similar to. In this case, we represent the ruminated knowledge by taking the average of the \u2018[MASK]\u2019 embedding. For each sample from external sources, we add a \u2018[MASK]\u2019 token at the end of the sentence and use the \u2018[MASK]\u2019 to represent the sentence. We use the pre-trained\ncorpus Wikipedia (Milne and Witten, 2008) as the retrieval source and employ FAISS (Johnson et al., 2021) for dense vector search. Interestingly, we notice that the model recalls its memory of a person with a caring heart, \u201ca good person. He helps victims and people.\u201d. This suggests that the model has remembered this information during pre-training, and if it is given a chance to think, the model is aware of what it has learned. Besides, we also map the knowledge into the external knowledge source ConceptNet (Speer et al., 2017) since CommonsenseQA is derived from it. More details can be found in Appendix A.6."
        },
        {
            "heading": "5.3 Transfer to LLMs",
            "text": "In this part, we endeavor to transfer knowledge rumination to LLMs. Compared to small language models, LLMs demonstrate an excellent ability to recall and deal with knowledge by fewshot demonstration as a prompt. GKP (Liu et al., 2022b) notes that pre-appending knowledge retrieved from LLMs can facilitate both the LLMs and other models in effectively handling tasks. As such, we follow suit (Liu et al., 2022b,a) in our knowledge review process, using prompts to recall the memorized knowledge for the target inputs. Nevertheless, simply concatenating the recalled knowledge might not lead to effective utilization by the model. Here, in the knowledge consolidation phase, we explicitly ask the model to attentively consider the recalled knowledge by crafting a stimulus prompt \u201cAccording to the [knowledge], the answer is\u201d or \u201cthink by the [knowledge]\u201d. Furthermore, recent work suggests that Chain-OfThought (COT) (Wei et al., 2022) can elicit language LLMs\u2019 reasoning ability by employing a se-\nries of intermediate reasoning rationales. In contrast, the knowledge generated by the Knowledge Rumination model contains implicit information derived from pre-trained language models during pre-training which may otherwise be overlooked by the reasoning steps. Here, we report the results on GPT-3 Davinci (175B) with knowledge rumination in Figure 4 and compared with original few-shot GPT-3, GKP, and COT. The implementation details can be found in Appendix A.3.2 and the demonstrations can be found in Appendix A.2. Our findings indicate that the performance of GPT3 can be significantly enhanced through knowledge rumination, as evidenced by the 16% improvement in OBQA accuracy, 12% in CSQA, and 11% in SocialIQA. Compared to GKP, it\u2019s evident that merely concatenating the elicited knowledge doesn\u2019t adequately leverage it. In contrast, the knowledge rumination approach surpasses GKP by an average of 6%, demonstrating its efficacy. What\u2019s more, knowledge rumination attains better performance than COT on OBQA and CSQA except for the SocialIQA, demonstrating the effectiveness of the background knowledge. In this preliminary exploration, we discovered that guiding LLMs to deliberate thoroughly on recalled knowledge can augment their understanding and reasoning capabilities. Looking forward, enhancing the utilization and integration of the model\u2019s inherent knowledge for reasoning remains a promising area for future investigation.\nError Analysis We conduct an error analysis on the evaluation examples from the OBQA and CSQA datasets for the GPT-3 model. We categorize the errors into four categories: 1): Failure to Utilize: the model recalls helpful information but does not provide the correct answer. 2): Ineffective Rumination: the rumination information with the highest logprobs is irrelevant to the question, but there are some relevant ones in the remaining. 3): Incorrect Memory: the model\u2019s stored information about the question is incorrect. 4): Missing Information: the model does not have the necessary information about the problem. Examples for each error type can be seen in Appendix A.8.\nThe statistics are presented in Table 5. We observe that the majority of errors are caused by missing information, indicating that large pretrained language models still have difficulty retaining all the knowledge acquired during pre-training. Additionally, our method still has difficulty acti-\nvating all the stored knowledge, as 32% of error cases are caused by ineffective rumination and 18% by failure to utilize retrieved information for the CSQA task. This suggests that there is still room for improvement in this area."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this work, we propose knowledge rumination for PLMs, which can serve as a general solution to exploit latent knowledge for downstream tasks and demonstrate promising results. This concept is akin to humans often erring when answering without thorough thinking. In the future, we plan to apply knowledge rumination to more NLP tasks and more types of models."
        },
        {
            "heading": "Acknowledgment",
            "text": "We would like to express gratitude to the anonymous reviewers for their kind comments. We thank Jiacheng Liu for his kind suggestions. This work was supported by the National Natural Science Foundation of China (No.62206246), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), Yongjiang Talent Introduction Programme (2021A-156-G), CCFBaidu Open Fund, and Information Technology Center and State Key Lab of CAD&CG, Zhejiang University."
        },
        {
            "heading": "Limitations",
            "text": "The proposed work still has some limitations to address in future work.\nMethod. One limitation of knowledge rumination is that it cannot handle incorrect memory as shown in \u00a75.3, and may amplify the effects of those errors (there may exist a tug of war between task data optimization and knowledge consolidation from PLMs). Adatable retrieval-based methods may be a solution for this issue, and we leave this for future work.\nPLMs. We apply knowledge rumination to four PLMs; however, it is still unknown whether the proposed approach works for other language models such as T5 (Raffel et al., 2019), BART (Lewis et al., 2020) and so on. We plan to extend our work in the future to cover more PLMs and multimodal, multilingual scenarios. Besides, knowledge stored in PLMs may have factual errors or severe bias, and knowledge rumination may augment such behavior.\nTasks. We only evaluate text classification and commonsense reasoning tasks. Due to the limited budget and computation resources, we cannot afford evaluation on more tasks. We will plan to evaluate the proposed approach on more NLP benchmark datasets such as KILT (Petroni et al., 2021)."
        },
        {
            "heading": "Ethical Considerations",
            "text": "The model in this paper is indented to be used for exploratory analysis of PLMs. Note that the pre-training corpus contains rich biased data; thus, the proposed knowledge rumination approach may elicit some knowledge with offensive language or discriminatory."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Detailed Comparison with Previous Approaches",
            "text": "Specifically, we note that Self-talk (Shwartz et al., 2020), Rainier (Liu et al., 2022a), GKP (Liu et al., 2022b), and ElicitKnowledge (Li et al., 2022) all harness knowledge, in the form of text sequences, extracted from pre-trained language models to enhance performance in knowledge-intensive tasks.\nSimilarly, the concept of Knowledge Rumination draws from the same inspiration, enabling pre-trained language models to leverage related latent knowledge without the need for retrieval from an external corpus. Among these prior studies, our method bears the closest resemblance to Selftalk (Shwartz et al., 2020), with the added capability of Knowledge Rumination to handle parametric knowledge (e.g., embeddings in Feed-Forward Networks). This extends the scope of the academic concept to a certain degree.\nOur work also shares a connection with COT (Wei et al., 2022). However, while COT generates rationales (texts) and appends them to output sequences to assist reasoning, our Knowledge Rumination model generates implicit knowledge and integrates it with the input sequence to produce desired results. Additionally, COT is primarily focused on reasoning, thus its rationales serve as intermediary steps in the reasoning process. By contrast, the knowledge generated by our Knowledge Rumination model constitutes implicit information derived from pre-trained language models during the pre-training phase."
        },
        {
            "heading": "A.2 Prompts for Knowledge Rumination with LLM",
            "text": "Table 7 through Table 9 shows the full prompts for knowledge rumination that we use for each evaluated task (demonstrations are derived from Liu et al. (2022b,a)): CSQA, OBQA, and SOCIALIQA."
        },
        {
            "heading": "A.3 Experimental Settings",
            "text": "In this section, we describe the implementation of our experiments in detail, including the baseline methods, backbone models, and hyperparameters. Our model is built based on the Huggingface framework (Wolf et al., 2020). Unlike finetuning, which updates all model parameters \u03b8 of a PLM, prefix-tuning freezes all pre-trained Transformer parameters and only optimizes prefix vectors that are prepended to each Transformer layer. We use prefix-tuning (Li and Liang, 2021) to train the knowledge reviewing model to reflect information for each task because: 1) the rumination models for different tasks can share the same backbone Transformer parameters, with only the prefix vectors being different. 2) Prefix-tuning has comparable performance to finetuning but avoids the risk of catastrophic forgetting.\nFor the tasks in the GLUE benchmarks, most of the hyperparameters are the default parameters of LM-BFF. For commonsense reasoning tasks, we follow previous preprocessing from QA-GNN (Yasunaga et al., 2021). We chose RoBERTa (Liu et al., 2019) large and DeBERTa (He et al., 2021) large as our backbone models, and the average training time for each model is 2 to 4 hours. We apply grid search for each hyperparameter tuning."
        },
        {
            "heading": "A.3.1 Hyperparameters",
            "text": "The detailed hyperparameter search space is as follows: (maximum values bolded below)"
        },
        {
            "heading": "CommonsenseQA (CSQA) .",
            "text": "\u2022 epoch: [5, 10, 15]roberta, [5, 10, 15]deberta \u2022 batch size: [8, 16, 32]roberta, [8, 16,\n32]deberta \u2022 learning rate: [5e-6, 1e-5] \u2022 rumi length: [7, 10, 15]roberta, [3, 5, 7]deberta"
        },
        {
            "heading": "OpenbookQA (OBQA) .",
            "text": "\u2022 epoch: [5, 10, 15]roberta, [5, 10, 15]deberta \u2022 batch size: [8, 16, 32]roberta, [8, 16,\n32]deberta \u2022 learning rate: [5e-6, 1e-5] \u2022 rumi length: [7, 10, 15]roberta, [3, 5, 7]deberta Social Interaction QA (SocialIQA) . \u2022 epoch: [5, 10, 15]roberta, [5, 10, 15]deberta \u2022 batch size: [8, 16, 32]roberta, [8, 16,\n32]deberta \u2022 learning rate: [5e-6, 1e-5] \u2022 rumi length: [7, 10, 15]roberta, [3, 5, 7]deberta Physical Interaction QA (PIQA) . \u2022 epoch: [10, 15, 20]roberta, [10, 15, 20]deberta \u2022 batch size: [8, 16, 32]roberta, [8, 16,\n32]deberta \u2022 learning rate: [5e-6, 1e-5] \u2022 rumi length: [3, 5, 10]roberta, [1, 3, 5]deberta"
        },
        {
            "heading": "Abductive Natural Language Inference (aNLI) .",
            "text": "\u2022 epoch: [3, 5, 7]roberta, [3, 5, 7]deberta \u2022 batch size: [8, 16, 32]roberta, [8, 32,\n64]deberta \u2022 learning rate: [5e-6roberta, 8e-6deberta] \u2022 rumi length: [5, 10, 15]roberta, [1, 3, 5]deberta"
        },
        {
            "heading": "HellaSwag .",
            "text": "\u2022 epoch: [1, 3, 5]roberta, [1, 3, 5]deberta \u2022 batch size: [8, 16, 32]roberta, [8, 16,\n32]deberta \u2022 learning rate: [5e-6roberta, 1e-5deberta] \u2022 rumi length: [3, 5, 7]roberta, [3, 5, 7]deberta"
        },
        {
            "heading": "A.3.2 Baselines",
            "text": "QA-GNN (Yasunaga et al., 2021) makes use of the external KG related to the context to enhance the PLMs.\nGreaseLM (Zhang et al., 2022) also employs the well-known ConceptNet and constructs a deep fusion model to incorporate the text and knowledge graph information.\nDragon (Yasunaga et al., 2022) is a deeply joint language-knowledge foundation model pretrained from text and KG at scale, which achieves strong performance on reasoning about language and knowledge.\nGKP (Liu et al., 2022b) prepends the knowledge before the question. The original paper concatenates knowledge before each candidate and compute the probability for each candidate. In our setting, to compare with COT, we provide all the candidates and ask the LLM to obtain the final answer.\nCOT (Wei et al., 2022), we use the chain-ofthought provided by previous work (Fu et al., 2023)3 and utilize the same number of demonstrations for GKP and Knowledge Rumination."
        },
        {
            "heading": "A.4 Evaluation Metrics",
            "text": "For the commonsense reasoning task, we use Accuracy as the evaluation metric. For the GLUE benchmark, we use the same metric in the original paper."
        },
        {
            "heading": "A.5 Mention Relevance Score",
            "text": "To score the relevance of each mention conditioned on the question context (\u00a73.2), we use the sentence embedding model: all-roberta-large-v1 from SentenceBert (Reimers and Gurevych, 2020) for calculating cosine-similarity."
        },
        {
            "heading": "A.6 Retrieval Process from Pre-trained Corpus",
            "text": "To identify what our generated contextualized representation is similar to, we use the pre-trained corpus Wikipedia (Milne and Witten, 2008) and external knowledge source ConceptNet (Speer et al., 2017) as the retrieval sources (Table 6). For efficient similarity search, we use the 1024- dimensional hidden representations to create a FAISS (Johnson et al., 2021) index and search for top-20 similar triples/samples. By the way, the type of faiss-index is indexPQ, which bases on\n3https://github.com/FranxYao/ chain-of-thought-hub\na product quantizer. Stored vectors are approximated by PQ codes.\nFor ConceptNet, we follow KagNet (Lin et al., 2019), which uses sentence template for generating TRIPLESTRING like diamond can be in jewelry store. Additionally, we add a \u2018[MASK]\u2019 token at the end of the TRIPLESTRING and then feed it as text inputs. For Wikipedia, we retrieve 10000 samples for each sentence based on the prebuilt index in Pyserini (Lin et al., 2021) and then use the original text as inputs."
        },
        {
            "heading": "A.7 Downstream Evaluation Datasets",
            "text": "We use the following six commonsense reasoning benchmarks for the experiments in the general domain (\u00a74)\nCommonsenseQA (CSQA) (Talmor et al., 2019) is a 5-way multiple-choice QA task testing commonsense reasoning. The dataset has 12,102 questions. We use the in-house data splits by (Lin et al., 2019).\nOpenbookQA (OBQA) (Mihaylov et al., 2018) is a 4-way multiple-choice QA task containing elementary science questions. It has 5,957 questions. We use the original data splits in (Mihaylov and Frank, 2018).\nSocial Interaction QA (SocialIQA) (Sap et al., 2019) is a 3-way multiple-choice QA task testing social commonsense reasoning. It has 37K questions. We use the original data splits in (Sap et al., 2019).\nPhysical Interaction QA (PIQA) (Bisk et al., 2020) is a 2-way multiple-choice QA task testing physics reasoning about objects. It has 20K questions. We split the dev set in half to make in-house dev/test sets.\nHellaSwag (Zellers et al., 2019) is a 4-way multiple-choice task testing grounded commonsense reasoning about events. It has 70K questions. We split the dev set in half to make in-house dev/test sets.\nAbductive Natural Language Inference (aNLI) (Bhagavatula et al., 2020) is a 2-way multiple-choice task testing abductive commonsense reasoning. It has 170K questions. We use the original data splits in (Bhagavatula et al., 2020)."
        },
        {
            "heading": "A.8 Examples for Different Error Case",
            "text": "In this section, we shows one example for each error type. For each example, we list the knowledge in descending order by the probability score.\nIt only takes the highest-score knowledge (the knowledge in bold) as rumination information in our experiment."
        },
        {
            "heading": "A.8.1 Failure to Utilize Question:",
            "text": "What do people typically do while playing guitar? Answer:\nSinging Knowledge List:\n\u2022 People play guitar while singing. \u2022 Playing guitar is an activity. \u2022 Playing guitar is a hobby. \u2022 People usually play guitar while singing. \u2022 People play guitar to entertain others In this example, \u201cPeople play guitar while singing\u201d has shown the correct answer to the models, but it still remains wrong.\nA.8.2 Ineffective Rumination"
        },
        {
            "heading": "Question:",
            "text": "What do people aim to do at work? Answer:\nComplete Job Knowledge List:\n\u2022 People work to earn money. \u2022 People aim to earn money. \u2022 People aim to get their work done. \u2022 People aim to do their work. \u2022 People aim to do their job well. In this example, \"people work to earn money\" has nothing to do with \"Complete Job\", while the third one in the list, \"People aim to get their work done.\", conveys the meaning of \"Complete Job\".\nA.8.3 Incorrect Memory"
        },
        {
            "heading": "Question:",
            "text": "Where can a human find clothes that aren\u2019t pants? Answer:\nDress Shop Knowledge List:\n\u2022 A human can find clothes that aren\u2019t pants at the beach.\n\u2022 Pants are a type of clothing. \u2022 Clothes that aren\u2019t pants are dresses and\nskirts. \u2022 Pants are the most common type of clothing. \u2022 Pants are not the only type of clothing. In this example, \"A human can find clothes that aren\u2019t pants at the beach\" is the wrong information for the question."
        },
        {
            "heading": "A.8.4 Missing Information Question:",
            "text": "The freeway had no traffic and few buildings, where is it? Answer:\nCountryside Knowledge List:\n\u2022 Freeways are usually in cities. \u2022 Freeways are usually located in urban areas. \u2022 Freeways are in cities. \u2022 Freeways are located in urban areas. \u2022 Freeways are usually in the middle of cities. In this example, all the knowledge in list indicate that mostly freeways can be found in cities, lacking of the information about the freeway had no traffic and few buildings."
        }
    ],
    "title": "Knowledge Rumination for Pre-trained Language Models",
    "year": 2023
}