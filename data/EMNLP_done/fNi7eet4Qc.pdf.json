{
    "abstractText": "Prompting approaches have been recently explored in text style transfer, where a textual prompt is used to query a pretrained language model (PLM) to generate style-transferred texts word by word in an autoregressive manner. However, such a generation process is less controllable and early prediction errors may affect future word predictions. In this paper, we propose a prompt-based editing approach to text style transfer. Specifically, we prompt a PLM for style classification and use the classification probability to compute a style score. Then, we perform discrete search with word-level editing to maximize a comprehensive scoring function for the style-transfer task. In this way, we transform a prompt-based generation problem into a classification one, which does not suffer from the error accumulation problem and is more controllable than the autoregressive generation of sentences. In our experiments, we performed both automatic and human evaluation on three style-transfer benchmark datasets, and show that our approach largely outperforms the existing systems that have 20 times more parameters. Additional empirical analyses further demonstrate the effectiveness of our approach.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Guoqing Luo"
        },
        {
            "affiliations": [],
            "name": "Yu Tong Han"
        },
        {
            "affiliations": [],
            "name": "Lili Mou"
        },
        {
            "affiliations": [],
            "name": "Mauajama Firdaus"
        }
    ],
    "id": "SP:36c3f3862ef2c968c581dc7f018676ab3c6f4982",
    "references": [
        {
            "authors": [
                "Yu Bao",
                "Hao Zhou",
                "Shujian Huang",
                "Lei Li",
                "Lili Mou",
                "Olga Vechtomova",
                "Xin-yu Dai",
                "Jiajun Chen."
            ],
            "title": "Generating sentences from disentangled syntactic and semantic spaces",
            "venue": "ACL, pages 6008\u2013 6019.",
            "year": 2019
        },
        {
            "authors": [
                "Eleftheria Briakou",
                "Sweta Agrawal",
                "Ke Zhang",
                "Joel Tetreault",
                "Marine Carpuat."
            ],
            "title": "A review of human evaluation for style transfer",
            "venue": "Workshop on Natural Language Generation, Evaluation, and Metrics, pages 58\u201367.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Chengzhang Dong",
                "Chenyang Huang",
                "Osmar Za\u00efane",
                "Lili Mou."
            ],
            "title": "Simulated annealing for emotional dialogue systems",
            "venue": "CIKM, pages 2984\u20132988.",
            "year": 2021
        },
        {
            "authors": [
                "Yue Dong",
                "Zichao Li",
                "Mehdi Rezagholizadeh",
                "Jackie Chi Kit Cheung."
            ],
            "title": "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
            "venue": "ACL, pages 3393\u20133402.",
            "year": 2019
        },
        {
            "authors": [
                "Joseph L Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological Bulletin, 76(5):378\u2013382.",
            "year": 1971
        },
        {
            "authors": [
                "Zhenxin Fu",
                "Xiaoye Tan",
                "Nanyun Peng",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Style transfer in text: Exploration and evaluation",
            "venue": "AAAI, pages 664\u2013670.",
            "year": 2018
        },
        {
            "authors": [
                "Navita Goyal",
                "Balaji Vasan Srinivasan",
                "N Anandhavelu",
                "Abhilasha Sancheti."
            ],
            "title": "Multi-style transfer with discriminative feedback on disjoint corpus",
            "venue": "NAACL, pages 3500\u20133510.",
            "year": 2021
        },
        {
            "authors": [
                "Jochen Hartmann",
                "Mark Heitmann",
                "Christian Siebert",
                "Christina Schamp."
            ],
            "title": "More than a feeling: Accuracy and application of sentiment analysis",
            "venue": "International Journal of Research in Marketing, 40(1):75\u201387.",
            "year": 2022
        },
        {
            "authors": [
                "Ruining He",
                "Julian McAuley."
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "WWW, pages 507\u2013517.",
            "year": 2016
        },
        {
            "authors": [
                "Zhiting Hu",
                "Zichao Yang",
                "Xiaodan Liang",
                "Ruslan Salakhutdinov",
                "Eric P Xing."
            ],
            "title": "Toward controlled generation of text",
            "venue": "ICML, pages 1587\u2013 1596.",
            "year": 2017
        },
        {
            "authors": [
                "Parag Jain",
                "Abhijit Mishra",
                "Amar Prakash Azad",
                "Karthik Sankaranarayanan."
            ],
            "title": "Unsupervised controllable text formalization",
            "venue": "AAAI, pages 6554\u2013 6561.",
            "year": 2019
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Lisa Orii",
                "Peter Szolovits."
            ],
            "title": "Hooks in the headline: Learning to generate headlines with controlled styles",
            "venue": "ACL, pages 5082\u20135093.",
            "year": 2020
        },
        {
            "authors": [
                "Vineet John",
                "Lili Mou",
                "Hareesh Bahuleyan",
                "Olga Vechtomova."
            ],
            "title": "Disentangled representation learning for non-parallel text style transfer",
            "venue": "ACL, pages 424\u2013434.",
            "year": 2019
        },
        {
            "authors": [
                "Shailza Jolly",
                "Zi Xuan Zhang",
                "Andreas Dengel",
                "Lili Mou."
            ],
            "title": "Search and learn: Improving semantic coverage for data-to-text generation",
            "venue": "AAAI, pages 10858\u201310866.",
            "year": 2022
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "John Wieting",
                "Mohit Iyyer."
            ],
            "title": "Reformulating unsupervised style transfer as paraphrase generation",
            "venue": "EMNLP, pages 737\u2013762.",
            "year": 2020
        },
        {
            "authors": [
                "Dhruv Kumar",
                "Lili Mou",
                "Lukasz Golab",
                "Olga Vechtomova."
            ],
            "title": "Iterative edit-based unsupervised sentence simplification",
            "venue": "ACL, pages 7918\u20137928.",
            "year": 2020
        },
        {
            "authors": [
                "Huiyuan Lai",
                "Antonio Toral",
                "Malvina Nissim."
            ],
            "title": "Thank you BART! Rewarding pre-trained models improves formality style transfer",
            "venue": "ACL-IJCNLP, pages 484\u2013494.",
            "year": 2021
        },
        {
            "authors": [
                "Guillaume Lample",
                "Sandeep Subramanian",
                "Eric Smith",
                "Ludovic Denoyer",
                "Marc\u2019Aurelio Ranzato",
                "Y-Lan Boureau"
            ],
            "title": "Multiple-attribute text rewriting",
            "venue": "In ICLR",
            "year": 2018
        },
        {
            "authors": [
                "Jingjing Li",
                "Zichao Li",
                "Lili Mou",
                "Xin Jiang",
                "Michael Lyu",
                "Irwin King."
            ],
            "title": "Unsupervised text generation by learning from search",
            "venue": "NeurIPS, pages 10820\u201310831.",
            "year": 2020
        },
        {
            "authors": [
                "Juncen Li",
                "Robin Jia",
                "He He",
                "Percy Liang."
            ],
            "title": "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
            "venue": "NAACL, pages 1865\u2013 1874.",
            "year": 2018
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "ACL-IJCNLP, pages 4582\u20134597.",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "arXiv preprint arXiv:2107.13586.",
            "year": 2021
        },
        {
            "authors": [
                "Puyuan Liu",
                "Chenyang Huang",
                "Lili Mou."
            ],
            "title": "Learning non-autoregressive models from search for unsupervised sentence summarization",
            "venue": "ACL, pages 7916\u20137929.",
            "year": 2022
        },
        {
            "authors": [
                "Xianggen Liu",
                "Lili Mou",
                "Fandong Meng",
                "Hao Zhou",
                "Jie Zhou",
                "Sen Song."
            ],
            "title": "Unsupervised paraphrasing by simulated annealing",
            "venue": "ACL, pages 302\u2013312.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Fuli Luo",
                "Peng Li",
                "Jie Zhou",
                "Pengcheng Yang",
                "Baobao Chang",
                "Zhifang Sui",
                "Xu Sun."
            ],
            "title": "A dual reinforcement learning framework for unsupervised text style transfer",
            "venue": "IJCAI, pages 5116\u20135122.",
            "year": 2019
        },
        {
            "authors": [
                "Xinyao Ma",
                "Maarten Sap",
                "Hannah Rashkin",
                "Yejin Choi."
            ],
            "title": "PowerTransformer: Unsupervised controllable revision for biased language correction",
            "venue": "EMNLP, pages 7426\u20137441.",
            "year": 2020
        },
        {
            "authors": [
                "Ning Miao",
                "Hao Zhou",
                "Lili Mou",
                "Rui Yan",
                "Lei Li."
            ],
            "title": "CGMH: Constrained sentence generation by Metropolis-Hastings sampling",
            "venue": "AAAI, pages 6834\u20136842.",
            "year": 2019
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Noisy channel language model prompting for few-shot text classification",
            "venue": "ACL, pages 5316\u20135330.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Blog.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "JMLR, 21(140):1\u201367.",
            "year": 2020
        },
        {
            "authors": [
                "Sudha Rao",
                "Joel Tetreault."
            ],
            "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
            "venue": "NAACL, pages 129\u2013140.",
            "year": 2018
        },
        {
            "authors": [
                "Machel Reid",
                "Victor Zhong."
            ],
            "title": "LEWIS: Levenshtein editing for unsupervised text style transfer",
            "venue": "Findings of ACL-IJCNLP, pages 3932\u20133944.",
            "year": 2021
        },
        {
            "authors": [
                "Emily Reif",
                "Daphne Ippolito",
                "Ann Yuan",
                "Andy Coenen",
                "Chris Callison-Burch",
                "Jason Wei."
            ],
            "title": "A recipe for arbitrary text style transfer with large language models",
            "venue": "ACL, pages 837\u2013848.",
            "year": 2022
        },
        {
            "authors": [
                "Stuart Rose",
                "Dave Engel",
                "Nick Cramer",
                "Wendy Cowley."
            ],
            "title": "Automatic keyword extraction from individual documents",
            "venue": "Text Mining: Applications and Theory, pages 1\u201320.",
            "year": 2010
        },
        {
            "authors": [
                "Stuart J Russell",
                "Peter Norvig."
            ],
            "title": "Artificial Intelligence: A Modern Approach",
            "venue": "Pearson Education Limited.",
            "year": 2010
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Few-shot text generation with natural language instructions",
            "venue": "EMNLP, pages 390\u2013402.",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "It\u2019s not just size that matters: Small language models are also few-shot learners",
            "venue": "NAACL, pages 2339\u20132352.",
            "year": 2021
        },
        {
            "authors": [
                "Raphael Schumann",
                "Lili Mou",
                "Yao Lu",
                "Olga Vechtomova",
                "Katja Markert."
            ],
            "title": "Discrete optimization for unsupervised sentence summarization with word-level extraction",
            "venue": "ACL, pages 5032\u20135042.",
            "year": 2020
        },
        {
            "authors": [
                "Tianxiao Shen",
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Style transfer from non-parallel text by cross-alignment",
            "venue": "NIPS, pages 6833\u20136844.",
            "year": 2017
        },
        {
            "authors": [
                "Amanda Stent",
                "Matthew Marge",
                "Mohit Singhai."
            ],
            "title": "Evaluating evaluation methods for generation in the presence of variation",
            "venue": "CICLing, pages 341\u2013 351.",
            "year": 2005
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Luke Melas-Kyriazi",
                "Dan Jurafsky."
            ],
            "title": "Prompt-and-Rerank: A method for zeroshot and few-shot arbitrary textual style transfer with small language models",
            "venue": "EMNLP, pages 2195\u2013 2222.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS, pages 24824\u201324837.",
            "year": 2022
        },
        {
            "authors": [
                "Guang Xiang",
                "Bin Fan",
                "Ling Wang",
                "Jason Hong",
                "Carolyn Rose."
            ],
            "title": "Detecting offensive tweets via topical feature discovery over a large scale twitter corpus",
            "venue": "CIKM, pages 1980\u20131984.",
            "year": 2012
        },
        {
            "authors": [
                "Peng Xu",
                "Jackie Chi Kit Cheung",
                "Yanshuai Cao."
            ],
            "title": "On variational learning of controllable representations for text without supervision",
            "venue": "ICML, pages 10534\u201310543.",
            "year": 2020
        },
        {
            "authors": [
                "Wei Xu",
                "Alan Ritter",
                "Bill Dolan",
                "Ralph Grishman",
                "Colin Cherry."
            ],
            "title": "Paraphrasing for style",
            "venue": "COLING, pages 2899\u20132914.",
            "year": 2012
        },
        {
            "authors": [
                "Min Yang",
                "Zhou Zhao",
                "Wei Zhao",
                "Xiaojun Chen",
                "Jia Zhu",
                "Lianqiang Zhou",
                "Zigang Cao."
            ],
            "title": "Personalized response generation via domain adaptation",
            "venue": "SIGIR, pages 1021\u20131024.",
            "year": 2017
        },
        {
            "authors": [
                "Jiaao Zhan",
                "Yang Gao",
                "Yu Bai",
                "Qianhui Liu."
            ],
            "title": "Stage-wise stylistic headline generation: Style generation and summarized content insertion",
            "venue": "IJCAI, pages 4489\u20134495.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "NIPS, pages 649\u2013657.",
            "year": 2015
        },
        {
            "authors": [
                "Yi Zhang",
                "Tao Ge",
                "Xu Sun."
            ],
            "title": "Parallel data augmentation for formality style transfer",
            "venue": "ACL, pages 3221\u20133228.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhe Zheng",
                "Zikai Chen",
                "Rongsheng Zhang",
                "Shilei Huang",
                "Xiaoxi Mao",
                "Minlie Huang."
            ],
            "title": "Stylized dialogue response generation using stylized unpaired texts",
            "venue": "AAAI, pages 14558\u201314567.",
            "year": 2021
        },
        {
            "authors": [
                "Zhemin Zhu",
                "Delphine Bernhard",
                "Iryna Gurevych."
            ],
            "title": "A monolingual tree-based translation model for sentence simplification",
            "venue": "COLING, pages 1353\u2013 1361.",
            "year": 2010
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text style transfer aims to automatically rewrite a sentence by changing it from one style to another (John et al., 2019), such as transferring the positive-sentiment sentence \u201cHe loves eating sandwiches\u201d into a negative one \u201cHe hates eating sandwiches\u201d. During the transfer, the style of the sentence must be changed, whereas the styleindependent content should be preserved. Text style transfer has a wide range of real-world applications, such as personalized response generation (Yang et al., 2017; Zheng et al., 2021), text debiasing (Xiang et al., 2012; Ma et al., 2020),\n1Our code and resources are available at: https://github. com/MANGA-UOFA/Prompt-Edit\ntext simplification (Dong et al., 2019; Kumar et al., 2020), and headline generation (Jin et al., 2020; Zhan et al., 2022).\nEarly work on text style transfer falls mainly into three categories: 1) Parallel supervision with labeled source\u2013target sentence pairs in a sequenceto-sequence manner (Zhu et al., 2010; Rao and Tetreault, 2018; Zhang et al., 2020), 2) Non-parallel supervision with style labels only, including learning latent representations of style and content separately (Shen et al., 2017; John et al., 2019; Goyal et al., 2021) and constructing pseudo-parallel training data for learning (Luo et al., 2019; Krishna et al., 2020; Reid and Zhong, 2021), and 3) Unsupervised learning methods that do not require style labels (Jain et al., 2019; Xu et al., 2020).\nVery recently, prompting methods have been explored in text style transfer (Reif et al., 2022; Suzgun et al., 2022), as large-scale pretrained language models (PLMs) enable us to perform various natural language generation tasks in a zero-shot (Wei et al., 2022a; Sanh et al., 2022) or exemplar-based manner (Brown et al., 2020; Schick and Sch\u00fctze, 2021a). In this paper, we also follow the promptbased setting. This does not require any training samples or labels, but directly performs inference with PLMs; thus, it is more challenging than the above three settings.\nIn previous work, a prompt (e.g., a piece of text \u201cRewrite the text to be positive:\u201d) is used to query a PLM, which will then generate a style-transferred sentence in an autoregressive manner (Reif et al., 2022; Suzgun et al., 2022). However, such autoregressive generation is less controllable as words are generated one after another by the PLM. It has the error accumulation problem where early errors of the PLM will affect its future predictions, leading to less satisfactory performance in general.\nTo this end, we propose a prompt-based editing approach to unsupervised style transfer. We first design a PLM-based style scorer. Specifically,\nwe prompt a PLM for style classification and use the classification probability to compute a style score. Then, we perform steepest-ascent hill climbing (SAHC; Russell and Norvig, 2010) for discrete search with word-level editing (such as replacement, insertion, and deletion) to maximize a heuristically defined scoring function for style transfer. In this way, we transform a prompt-based generation problem into a classification one, which involves only a style-word prediction and is generally believed to be easier than multiple-word predictions for sentence generation.\nOur approach provides several additional advantages. First, it does not suffer from the error accumulation problem, because it performs word edits scattered throughout the entire sentence instead of generating a sentence word by word. Further, we design a discrete search algorithm that combines the PLM-based style score with other scoring functions, including fluency and semantic similarity. Consequently, the search algorithm contributes to a more controllable and refined generation of sentences.\nWe used Eleuther AI\u2019s GPT-J-6B (an off-theshelf PLM)2 and conducted both automatic and human evaluations on three style-transfer benchmark datasets. Results show that our prompt-based editing approach largely outperforms existing prompting systems that have 20 times more parameters. Additional empirical analysis shows the effectiveness of different scoring components and the search algorithm proposed in our approach."
        },
        {
            "heading": "2 Related Work",
            "text": "Prompting. Prompting methods use a piece of text to query a PLM to provide desired outputs (Liu et al., 2021). The simplest prompting method, perhaps, is zero-shot prompting (Wei et al., 2022a; Sanh et al., 2022; Suzgun et al., 2022), which directly queries a PLM to perform a natural language processing task, but this may result in less wellformatted or logical sentences (Reif et al., 2022). Another method is few-shot prompting (Brown et al., 2020; Schick and Sch\u00fctze, 2021a,b; Wei et al., 2022b). It requires several task-specific examples for PLM, but can achieve higher performance than zero shot prompting, and is thus more widely adopted in NLP tasks (Schick and Sch\u00fctze, 2021a; Brown et al., 2020; Wei et al., 2022b).\nPrompting methods were initially applied to 2https://github.com/kingoflolz/mesh-transformer-jax\nnatural language classification tasks (Schick and Sch\u00fctze, 2021a,b; Min et al., 2022), where PLMs are asked to predict the masked word given a piece of text containing the token \u201c[MASK]\u201d, and the predicted word is then projected to a label by a predefined verbalizer. With the emergence of various PLMs (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020; Wei et al., 2022a), prompting methods have recently been widely applied to natural language generation tasks (Liu et al., 2021), such as text style transfer (Reif et al., 2022; Suzgun et al., 2022) and machine translation (Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020).\nText style transfer. Traditionally, style-transfer generation can be accomplished by supervised methods with parallel training data (Xu et al., 2012; Zhang et al., 2015; Rao and Tetreault, 2018). However, obtaining parallel data is labor-intensive and time-consuming, which remains a significant challenge for this task.\nTo mitigate the need for parallel data, one line of research is non-parallel supervision, where it trains the model on a non-parallel but style-labeled corpus (Shen et al., 2017). Early work focuses on learning latent representations of content and style separately (Hu et al., 2017; Fu et al., 2018; John et al., 2019; Bao et al., 2019). Goyal et al. (2021) train multiple language models as discriminators for each of the target styles given the content representation. However, explicit separation of content and style is not always possible, because style can only be conveyed holistically for some sentences.\nOn the other hand, researchers construct pseudoparallel training data for training the model in a supervised manner (Lample et al., 2018; Li et al., 2018; Luo et al., 2019; Krishna et al., 2020). Reid and Zhong (2021) first train an attentive style classifier to synthesize source\u2013target style pairs, which are then used to train a Levenshtein editor and perform multi-span edits. However, the process of constructing pseudo-parallel data can sometimes yield poor-quality data, which would lead to suboptimal training and low model performance.\nAnother line of research is devoted to unsupervised learning methods, where the training samples contain no style labels (Jain et al., 2019; Xu et al., 2020). Jain et al. (2019) train an encoder\u2013decoder model with unlabeled texts and multiple controlling scorers to perform formal style transformation. However, these unsupervised learning methods re-\nquire a complicated training process, which is not efficient.\nRecently, researchers have developed several prompt-based approaches that generate styletransferred texts in a zero-shot (Suzgun et al., 2022) or exemplar-based manner (Reif et al., 2022). Such methods do not require a learning process or any training labels. Reif et al. (2022) prompt largescale PLMs to generate sentences in various styles. Suzgun et al. (2022) generate multiple candidate sentences and then use a re-ranking mechanism to choose one with the highest score as the final output.\nOur approach follows the prompt-based setting and directly performs style transfer without any training procedure. However, unlike other work that mainly performs autoregressive generation, our approach proposes a new prompt-based editing paradigm for text generation, where we not only design a PLM-based scoring function but also develop a discrete search algorithm that is particularly suited to our scenario."
        },
        {
            "heading": "3 Approach",
            "text": "Given an input sentence x = (x1, \u00b7 \u00b7 \u00b7 , xm), our goal is to generate a sentence y = (y1, \u00b7 \u00b7 \u00b7 , yn) that transfers the style of x. Figure 1b depicts the framework of our prompt-based editing approach, where we propose to prompt a pretrained language model (PLM) to predict the style of a candidate sentence. Then, we perform discrete search and iteratively edit the candidate sentence to maximize a scoring objective that involves the PLM\u2019s classification probability. Finally, the highest-scored candidate is taken as the style-transferred sentence."
        },
        {
            "heading": "3.1 Prompt-Based Classifier",
            "text": "In previous work, researchers directly prompt a PLM to obtain style-transferred sentences (Figure 1a; Reif et al., 2022; Suzgun et al., 2022). However, this could be a challenging process, as the PLM has to generate the sentence in a zero-shot or exemplarbased manner; such a process is autoregressive and less controllable.\nTo address this, we design a prompt-based classifier, transforming style-transfer text generation into a classification problem. Specifically, we prompt a PLM for a style word, which in turn yields a style score. This involves only one single-step prediction and is much simpler than generating the entire sentence.\nGiven a candidate sentence [y], we intuitively design the prompt as\npromptcls(y) \u2261 The [t] of the text { [y] } is : (1)\nwhere [t] is the style-transfer task, i.e., sentiment or formality in our experiments, and \u201c{\u201d and \u201c}\u201d are text boundary markers (Reif et al., 2022). Notice that we have not performed prompt engineering, which is beyond the scope of this paper. Instead, our focus is to develop a prompt-based editing approach for text style transfer.\nBased on the above prompt, we perform the next-word prediction to obtain a style probability. Specifically, the PLM computes the conditional probability of the next word w in the vocabulary given the prompt, denoted by PPLM(w | promptcls(y)).\nWe denote si by the representative word of the ith style. Here, si is simply chosen to be the most intuitive style word, namely,\npositive and negative for sentiment transfer and formal and informal for formality transfer. In general, the predicted probabilities of the two styles are PPLM(s1 | promptcls(y)) and PPLM(s2 | promptcls(y)).\nTo compute the style score, we consider the ratio of the two styles. Suppose a sentence in style s1 is to be transferred to s2, we design the style score as:\nfsty(y) = PPLM(s2 | promptcls(y)) PPLM(s1 | promptcls(y))\n(2)\nSuch a ratio measures the candidate\u2019s relative affiliation with different styles.3 It is more robust than the predicted target-style probability PPLM(s2| promptcls(y)), which could be affected by the data sample per se."
        },
        {
            "heading": "3.2 Search Objective",
            "text": "We apply an edit-based search for unsupervised style transfer. This follows the recent development of search-based text generation (Li et al., 2020; Kumar et al., 2020; Liu et al., 2022; Dong et al., 2021; Jolly et al., 2022), where local edits (e.g., word changes) are performed to maximize a heuristically defined objective function. However, different from previous search-based work, we propose to prompt an off-the-shelf PLM to compute a style score and do not require any task-specific training procedure. Overall, our objective function involves three aspects:\nf(y;x) = fsty(y) \u00b7 fflu(y) \u00b7 fsem(y,x) (3)\nwhere the style scorer fsty is designed in Section 3.1; fflu and fsem are fluency and semantic similarity scorers, mostly adopted from previous work and explained below.\nLanguage Fluency. A language model scorer provides an approximation of how fluent a candidate sentence y is. We follow Suzgun et al. (2022) and use GPT-2 (Radford et al., 2019) to obtain the fluency score of the candidate y by the geometric mean of predicted probabilities:\nfflu(y) = [ t\u220f i=1 PGPT-2(yi|y<i) ] 1 t \u03b1 (4) 3While our datasets only consider the transfer between two styles, our approach can be extended to multiple styles in a one-vs-one or one-vs-all manner.\nAlgorithm 1 Prompt-Based Editing 1: Input: Original sentence x, iterative steps T 2: y(0) = x 3: for t \u2208 {1, \u00b7 \u00b7 \u00b7 , T} do 4: Enumerate all edit positions and operations 5: Obtain the highest-scored candidate y\u2217 by Eqn. (3) 6: if fsty(y\u2217) > 1 \u25b7 PLM believes style transferred 7: then: return y\u2217\n8: if f(y(t\u22121),x) \u2265 f(y\u2217,x) \u25b7 Local optimum found 9: then: return y(t\u22121)\n10: else: y(t) = y\u2217 11: return y(T )\nwhere \u03b1 is a hyperparameter balancing fflu with other scoring functions4.\nSemantic Similarity. The semantic similarity scorer evaluates how an output y captures the semantics of an input x. In our work, we adopt wordand sentence-level semantic similarities as in Li et al. (2020).\nA word-level scorer focuses on keyword information, where the keywords in the input sentence x are extracted by the Rake system (Rose et al., 2010). Then, the RoBERTa model (Liu et al., 2019) is adopted to compute the contextualized representation, denoted by RBT(w, s), for a word w in some sentence s. The word-level semantic score is defined as the lowest similarity among all the keywords, given by\nfword(y,x) = min k\u2208keyword(x) max yi\u2208y cos(RBT(k,x),RBT(yi,y))\n(5)\nA sentence-level scorer computes the cosine similarity of two sentence vectors as fsent(y,x) = cos(y,x) = y\n\u22a4x ||y||\u00b7||x|| , where the sentence vectors\ny and x are also encoded by RoBERTa. Finally, the semantic similarity score is computed as the product of word- and sentence-level scores:\nfsem(y,x) = fword(y,x) \u03b2 \u00b7 fsent(y,x)\u03b3 (6)\nwhere \u03b2 and \u03b3 are the weighting hyperparameters."
        },
        {
            "heading": "3.3 Discrete Search Algorithm",
            "text": "We perform style-transfer generation by discrete local search using editing operations, such as word insertion, deletion, and replacement, following previous work (Miao et al., 2019; Li et al., 2020). However, we propose to use steepest-ascent hill\n4Notice that a weighting hyperparameter is not needed for the style scorer fsty because the relative weight of different scorers are given in fflu and fsem.\nclimbing (SAHC; Russell and Norvig, 2010) as our search algorithm.\nDuring development, we measured the edit distance between the input sentences and the reference outputs for sentiment and formality transfer tasks. Our observation is that the average edit distance is 2.9 steps for sentiment transfer and 4.7 steps for formality transfer. Therefore, we set the maximum number of edit steps to 5 to maintain their resemblance. This, unfortunately, makes previous search algorithms\u2014such as simulated annealing (SA; Liu et al., 2020) and first-choice hill climbing (FCHC; Schumann et al., 2020)\u2014ineffective, as they cannot fully make use of the limited search steps.\nIn our work, we use the SAHC algorithm: in a search step t, SAHC enumerates every editing position and performs every editing operation (namely, word deletion, replacement, and insertion)5. Then it selects the highest-scored candidate sentence y\u2217 if the score f(y\u2217,x) is higher than f(y(t\u22121),x) before it reaches the maximum edit steps. Otherwise, SAHC terminates and takes the candidate y(t\u22121) as the style-transferred output. In this way, our SAHC greedily finds the best edit for every search step and is more powerful than SA and FCHC in our scenario, as will be shown in Section 4.6.\nMoreover, we design an additional stopping criterion such that the search terminates when the prompted PLM predicts that the source style has changed into the target one even if it has not reached the maximum edit steps. This not only improves time efficiency but also encourages content preservation.\nOur approach is summarized in Algorithm 1."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we will present empirical evaluation of our proposed prompt-based editing approach. First, we will introduce our datasets and setups. Then, we will show our main results, followed by detailed analyses."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluated our approach on two standard styletransfer tasks: sentiment and formality.\nWe used Yelp reviews (YELP; Zhang et al., 2015) and Amazon reviews (AMAZON; He and McAuley, 2016) for sentiment transfer. These two datasets\n5For replacement and insertion, we follow Li et al. (2020) and choose top-k candidate words predicted by RoBERTa due to efficiency concerns.\nhave been widely used in previous work (Luo et al., 2019; John et al., 2019; Suzgun et al., 2022). YELP contains reviews for restaurants and other businesses (Zhang et al., 2015), and AMAZON contains product reviews. Both the YELP and AMAZON datasets contain 500 positive and 500 negative sentences in the test set.\nFor formality transfer, we used Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC; Rao and Tetreault, 2018). GYAFC consists of sentences that were extracted from the Yahoo Answers forum. We chose the Family & Relationships domain following Luo et al. (2019) and Suzgun et al. (2022). The test set contains 500 formal and 500 informal sentences."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We used Eleuther AI\u2019s off-the-shelf GPT-J-6B as the prompt-based classifier for computing the style score. We also used the off-the-shelf pretrained language model RoBERTa-Large (Liu et al., 2019) to encode sentences (Section 3.2) and to predict top-k words as candidate edits (Section 3.3). We set k = 50 for all sentiment and formality transfer datasets.\nFor the weighting hyperparameters \u03b1, \u03b2, and \u03b3 of the search objective f(y) in Eqn. (3), they were 14 , 1 6 , and 1 6 for both YELP and AMAZON datasets, and 14 , 3 8 , and 3 8 for the GYAFC dataset. This shows that the style scorer is the most important one among all the scorers.\nWe developed our prompt-based editing approach with Python 3.7 and Pytorch 1.11.0. The experiments were conducted on NVIDIA A100 SXM4 GPUs."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "We adopted the following automatic evaluation metrics: \u2022 Style transfer accuracy. This measures whether\na generated output is correctly transferred. Following the practice in Reif et al. (2022) and Lai et al. (2021), we used SiEBERT (Hartmann et al., 2022) for sentiment classification, and finetuned a RoBERTa-Large (Liu et al., 2019) for formality classification. \u2022 BLEU. The BLEU score measures the semantic similarity between generated outputs and human-written references. Following Luo et al. (2019) and Reif et al. (2022), we used multi-bleu.perl to obtain the BLEU-4 score.\n\u2022 Geometric mean (GM) and harmonic mean (HM). They are the average of the abovementioned metrics, evaluating the overall performance of text style transfer. Again, this follows the standard practice in previous work (Luo et al., 2019; Li et al., 2020). We also performed human evaluation on selected style-transfer systems, detailed in Section 4.6."
        },
        {
            "heading": "4.4 Baselines",
            "text": "Since our approach is based on prompting and does not require a training process, we compare our approach with the following existing prompting systems: \u2022 Vanilla prompting. This baseline method\nprompts a PLM with \u201cHere is some text: { [x] }. Here is a rewrite of the text, which is more [s]: {\u201d where [x] is the input and [s] is the style word. This baseline directly obtains a style-transferred sentence, shown in Figure 1a. No exemplars are used here. \u2022 Distant-exemplar prompting. We adopted the approach in Reif et al. (2022), which queries a large PLM (such as the LLM, LLM-dialog, and 175B-parameter GPT-36) with several style6We use the same prompt provided by Reif et al. (2022) to\ntransfer exemplars in a few-shot manner. However, their exemplars have different target styles from the test cases in the inference task, and thus we call it distant-exemplar prompting. \u2022 Prompt & Rerank. Suzgun et al. (2022) propose a method that generates multiple candidate outputs from different manually designed prompts; then, they rerank the outputs by a heuristically defined scoring function. It should be mentioned that the paper (Suzgun et al., 2022) adopts a setting that is non-compatible with other work: they report different directions of sentiment transfer separately, while excluding informal-to-formal transfer in the formality experiment. Therefore, we replicated their work under the standard settings (Luo et al., 2019; Reif et al., 2022).\nTo the best of our knowledge, Reif et al. (2022) and Suzgun et al. (2022) are the only prior studies of prompting methods on text style transfer."
        },
        {
            "heading": "4.5 Main Results",
            "text": "Table 1 shows the performance of different prompting systems on YELP and AMAZON datasets. Com-\nobtain results on the off-the-shelf GPT-3 babbage and GPT-J6B for the YELP, AMAZON, and GYAFC datasets.\npared with the recently proposed Prompt & Rerank system (Suzgun et al., 2022), our approach achieves a performance improvement of 14 and 3 points for GM, as well as 15 and 5 points for HM in the zero- and few-shot settings, respectively, averaged across the two datasets. Further, compared with 175B-parameter GPT-3 with distant exemplars (i.e., style-transfer exemplars containing source texts and outputs written in non-target styles), our approach yields higher GM and HM by more than 3 and 5 points, respectively, also averaged across the two datasets. This is a compelling result, as our approach yields a better balance between content preservation and style transfer strength while using a 20x smaller PLM.\nTable 2 shows the results of different prompting systems on the GYAFC dataset, where both informal-to-formal and formal-to-informal directions are considered (Luo et al., 2019; Reif et al., 2022). For a fair comparison with previous prompting systems, we followed Suzgun et al. (2022) and conducted experiments in a four-shot setting. As seen, our method outperforms previous approaches in GM and HM scores, which is consistent with the results in Table 1. It is also noticed that our approach achieves less improvement on GYAFC than on YELP and AMAZON, as formality transfer is more challenging than sentiment transfer."
        },
        {
            "heading": "4.6 Detailed Analyses",
            "text": "In this subsection, we conduct in-depth analyses to assess the effectiveness of our prompt-based editing approach. Due to the limit of time and resources, we chose the sentiment transfer datasets (YELP and AMAZON) as our testbed.\nHuman Evaluation. We conducted human evaluation via pairwise comparison of system outputs to further confirm the superiority of our approach. Specifically, we randomly selected 100 outputs from the recently proposed Prompt-andRerank (P&R) system (Suzgun et al., 2022) and\nour approach is based on the same GPT-J-6B model. Following Luo et al. (2019) and Krishna et al. (2020), we asked three human annotators, who were instructed to rate each sentence based on a 1\u20135 Likert scale (Stent et al., 2005) in terms of style transfer strength, content preservation, and fluency (Briakou et al., 2021). Our annotations were strictly blind; the samples from the two prompting approaches were randomly shuffled and the annotators did not know which approach generated the sample.\nWe measured the inter-rater agreement by Fleiss\u2019 Kappa score (1971) for the Likert scale ratings. They are 0.37, 0.42, and 0.39 for style transfer strength, content preservation, and fluency, respectively. These scores are considered fair correlation7.\nTable 3 presents the results of human evaluation. We observe that our prompt-based editing approach outperforms P&R in all three aspects, particularly in terms of content preservation. This is because with the proposed stopping criterion and discrete\n7https://en.wikipedia.org/wiki/Fleiss%27_kappa\nsearch, we avoid unnecessary edits and preserve the original content well. Our approach also achieves a higher average score, which is consistent with the automatic evaluation results in Table 1, further demonstrating the effectiveness of our approach.\nAblation Study. To evaluate the contribution of key components in our model, we conducted an ablation study of different scoring functions and our proposed stopping criterion.\nTable 4 shows that all the scorers play a role in our approach, and that the prompt-based style scorer is the most important one. This makes sense, as it is the only signal of the style, without which we would not be able to perform meaningful style transfer. Moreover, we find that the fluency scorer slightly hurts style accuracy and BLEU scores, which are the standard metrics in Luo et al. (2019). However, it significantly improves language model probability (i.e., lower perplexity), which roughly estimates the fluency of text (John et al., 2019). Therefore, we deem the fluency scorer fflu essential to our text style transfer model.\nIn addition, our approach involves a stopping criterion that terminates the search process if the PLM believes the style is successfully transferred. As seen from the last row of Table 4, more edit steps (w/o stop criterion) improve the style accuracy but drastically hurt BLEU scores. This shows that our stopping criterion is able to seek a balance between style transfer accuracy and content preservation.\nDiscrete Search Algorithms. Our steepestascent hill climbing (SAHC) algorithm enumerates candidate edits, including word deletion, insertion,\nand replacement (where top-50 candidate words are considered for efficiency concerns). Then, SAHC selects the best one for the next round of editing, shown in Algorithm 1.\nWe compare our SAHC with two stochastic optimization algorithms, first-choice hill climbing (FCHC; Schumann et al., 2020) and simulated annealing (SA; Liu et al., 2020), which are used in previous search-based text generation. Both FCHC and SA perform stochastic local changes to obtain a candidate sentence. If the proposed sentence is better than the current one, the algorithms will accept the new candidate. Otherwise, FCHC will keep the current candidate, while SA may still accept the candidate with a small probability.\nFrom Table 5, we observe that our SAHC algorithm significantly outperforms FCHC and SA in both style-transfer accuracy and the BLEU score. This is likely due to the limited number of edit steps, requiring that the algorithm should make an effective edit at every search step. The results confirm that SAHC is more suited in our scenario than other discrete search algorithms\nCase Study. We show in Table 6 that our method is able to avoid issues that arise from the error accumulation problem in an autoregressive generation. This is observed through several example outputs by P&R and our approach for YELP, AMAZON, and GYAFC datasets. We see that the previous approach, which performs autoregressive generation, yields less controllable and satisfactory sentences. For example, given the source input \u201cfor my purpose this is the perfect item\u201d in the\npositive-to-negative sentiment transfer of the AMAZON dataset, P&R generates an unrelated sentence starting with \u201cSo this text has\u201d, leading to the subsequent improper word predictions \u201ca text and to be a rewrite\u201d.\nHowever, our prompt-based editing approach transfers the sentiment of a source sentence from positive to negative by inserting the words but and not, while maintaining other semantic content. This shows that our approach is capable of generating more sensible and controllable sentences.\nIn addition, we find that our approach is able to convert the style of source input with multiple edits. For example, given the source sentence \u201ci\u2019m unsure concerning what i should do\u201d in formalto-informal transfer, our approach inserts multiple tokens (yeah, lol, really, and \u201c...\u201d) at the beginning and replaces should with \u2019ll at the end, and the sentence is transferred to an informal one. By allowing iterative edits and examining all possible positions and editing operations, we are able to have multiple word edits scattered throughout the sentence and experience a gradual transfer of style."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a novel prompt-based editing approach to text style transfer that turns a prompt-based generation problem into a classification one. It does not suffer from the issue of error accumulation and is more controllable than autoregressive generation. Our experiments on three benchmark datasets show that the proposed approach significantly outperforms the existing prompting systems while using 20x fewer parameters. Additional empirical analysis shows the effectiveness of different scorers and the discrete search algorithm in our approach."
        },
        {
            "heading": "6 Limitation",
            "text": "Our paper transforms a generation problem into a classification one in text style transfer, but it comes with a trade-off between output quality and inference efficiency. Nevertheless, our algorithm can be implemented in a highly parallel manner when evaluating different candidates, and we only need five iterations. Therefore, the efficiency of our SAHC is already much higher than other search algorithms (such as SA) which requires several hundred search steps (Liu et al., 2020). Further, the efficiency can be improved by learning from the search results (Li et al., 2020), i.e., fine-tuning a PLM based on our\nsearch outputs. In this way, our approach can be more computationally efficient.\nAnother limitation is the need for manually designed prompts, which is inevitable in zero-shot prompting. Our current work adopts the most intuitive prompt and has not performed prompt engineering. In the future, we would like to investigate prompt tuning (Schick and Sch\u00fctze, 2021b; Li and Liang, 2021; Wei et al., 2022a) to mitigate the reliance on designing prompts."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank all reviewers and chairs for their valuable comments. This research is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant No. RGPIN2020-04465, the Amii Fellow Program, the Canada CIFAR AI Chair Program, the Alberta Innovates Program, a UAHJIC project, a donation from DeepMind, and the Digital Research Alliance of Canada (alliancecan.ca)."
        }
    ],
    "title": "Prompt-Based Editing for Text Style Transfer",
    "year": 2023
}